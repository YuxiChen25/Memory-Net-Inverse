{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_64x64_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10641706721650229\n",
      "Average test loss: 0.00481374772472514\n",
      "Epoch 2/300\n",
      "Average training loss: 0.027218736234638427\n",
      "Average test loss: 0.004473803512338135\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02386380257540279\n",
      "Average test loss: 0.004489010292208857\n",
      "Epoch 4/300\n",
      "Average training loss: 0.022653225814302763\n",
      "Average test loss: 0.004118203583276934\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022027290264765423\n",
      "Average test loss: 0.004086458264332678\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021621337282988762\n",
      "Average test loss: 0.0040368094945119485\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021327080173624887\n",
      "Average test loss: 0.004005931794229481\n",
      "Epoch 8/300\n",
      "Average training loss: 0.021025365738405122\n",
      "Average test loss: 0.003984615670517087\n",
      "Epoch 9/300\n",
      "Average training loss: 0.020858050568236246\n",
      "Average test loss: 0.003989629129775696\n",
      "Epoch 10/300\n",
      "Average training loss: 0.020678447772231368\n",
      "Average test loss: 0.003937037494033575\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02051018359677659\n",
      "Average test loss: 0.003917364618016614\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02037311101290915\n",
      "Average test loss: 0.0038952846055229505\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02024645681348112\n",
      "Average test loss: 0.0038808637282086744\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020124422169393964\n",
      "Average test loss: 0.0038796952126754656\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020019534435537126\n",
      "Average test loss: 0.003860115721200903\n",
      "Epoch 16/300\n",
      "Average training loss: 0.019908490346537695\n",
      "Average test loss: 0.0038688371078007752\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019809573822551302\n",
      "Average test loss: 0.0038511463931451243\n",
      "Epoch 18/300\n",
      "Average training loss: 0.019724583195315466\n",
      "Average test loss: 0.003798572275373671\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01961887780825297\n",
      "Average test loss: 0.003801913955973254\n",
      "Epoch 20/300\n",
      "Average training loss: 0.019531927201482984\n",
      "Average test loss: 0.0037978024321297806\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01945720407863458\n",
      "Average test loss: 0.003783092409165369\n",
      "Epoch 22/300\n",
      "Average training loss: 0.019374626096751956\n",
      "Average test loss: 0.0037974694727195635\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01931777441336049\n",
      "Average test loss: 0.003771917125831048\n",
      "Epoch 24/300\n",
      "Average training loss: 0.019252739023831157\n",
      "Average test loss: 0.003730890427612596\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01916832488278548\n",
      "Average test loss: 0.0037238160038573875\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01911378232969178\n",
      "Average test loss: 0.00371885600220412\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019060692373249265\n",
      "Average test loss: 0.0037419349869920147\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019007417256633442\n",
      "Average test loss: 0.003720052271046572\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01895976252026028\n",
      "Average test loss: 0.0037302498591856824\n",
      "Epoch 30/300\n",
      "Average training loss: 0.018915182539158398\n",
      "Average test loss: 0.003702870476163096\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0188537504043844\n",
      "Average test loss: 0.003698779914735092\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018799621370931465\n",
      "Average test loss: 0.0036921295204924215\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0187715848568413\n",
      "Average test loss: 0.0036909171785745355\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01872166001962291\n",
      "Average test loss: 0.0036996787273221544\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018695370886060925\n",
      "Average test loss: 0.003695937808810009\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018643217803703413\n",
      "Average test loss: 0.0037045561683674653\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01860042312575711\n",
      "Average test loss: 0.003692808202571339\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018561744378672706\n",
      "Average test loss: 0.0036823624533911547\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01852527904510498\n",
      "Average test loss: 0.003675388991832733\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018497309396664302\n",
      "Average test loss: 0.0036693587276256747\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018452384228507677\n",
      "Average test loss: 0.0036858951242433654\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018422924169235758\n",
      "Average test loss: 0.003672929945919249\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018380918997029462\n",
      "Average test loss: 0.0036767911459836696\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01835364396042294\n",
      "Average test loss: 0.0036922910784681637\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01831328443520599\n",
      "Average test loss: 0.003661703097737498\n",
      "Epoch 46/300\n",
      "Average training loss: 0.018296222219864527\n",
      "Average test loss: 0.0036743173661331337\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0182451677934991\n",
      "Average test loss: 0.0036773679989079633\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018217379699150722\n",
      "Average test loss: 0.0036917375969803996\n",
      "Epoch 49/300\n",
      "Average training loss: 0.018191857545740073\n",
      "Average test loss: 0.003672100257438918\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01817021597094006\n",
      "Average test loss: 0.0036705473924262657\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01811405824787087\n",
      "Average test loss: 0.0037035900861438777\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018092111920317015\n",
      "Average test loss: 0.003661388548091054\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01805538325260083\n",
      "Average test loss: 0.0036990030676954323\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01802275186777115\n",
      "Average test loss: 0.003666264735162258\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017995428192946646\n",
      "Average test loss: 0.0036850570080180964\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017970725917153887\n",
      "Average test loss: 0.0036996541774521273\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01793064557843738\n",
      "Average test loss: 0.0036729087668160598\n",
      "Epoch 58/300\n",
      "Average training loss: 0.017892016597919995\n",
      "Average test loss: 0.00368028083567818\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01786741638183594\n",
      "Average test loss: 0.0037132678449981744\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01785344027645058\n",
      "Average test loss: 0.003705069522683819\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017822600846489272\n",
      "Average test loss: 0.003745348783623841\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01778638189451562\n",
      "Average test loss: 0.003702922091508905\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01775694286160999\n",
      "Average test loss: 0.0037118511642846796\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01771496139301194\n",
      "Average test loss: 0.0036958710961043834\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017695122223761347\n",
      "Average test loss: 0.003678182111432155\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017668416996796926\n",
      "Average test loss: 0.0036936657821966543\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017643443627489937\n",
      "Average test loss: 0.00375424331964718\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017633589633637006\n",
      "Average test loss: 0.0037074734912150437\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017592182618048458\n",
      "Average test loss: 0.003812620715548595\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017560686657826105\n",
      "Average test loss: 0.0036946852904640964\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01754236797988415\n",
      "Average test loss: 0.0037164734428127605\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017516436859137483\n",
      "Average test loss: 0.0037315009579890306\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01748099724120564\n",
      "Average test loss: 0.003763091164227161\n",
      "Epoch 74/300\n",
      "Average training loss: 0.017470407631662157\n",
      "Average test loss: 0.0037429022205372653\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01742648488945431\n",
      "Average test loss: 0.0037093635230428645\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017413551926612855\n",
      "Average test loss: 0.0037342547049952876\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01739686065912247\n",
      "Average test loss: 0.003817936208513048\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01735532266398271\n",
      "Average test loss: 0.003750315278354618\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017348573439651065\n",
      "Average test loss: 0.0037265284705079262\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01730834938916895\n",
      "Average test loss: 0.0037067467355065874\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017295807446042696\n",
      "Average test loss: 0.003735264838569694\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01727181247373422\n",
      "Average test loss: 0.003705099309277203\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01724174056450526\n",
      "Average test loss: 0.0037100286098817985\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017236152874926726\n",
      "Average test loss: 0.0037532376286884147\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017205063096351093\n",
      "Average test loss: 0.003761376169613666\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0171882826305098\n",
      "Average test loss: 0.0037245490875922973\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017167161269320382\n",
      "Average test loss: 0.0037484727911651135\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01713471341629823\n",
      "Average test loss: 0.0037888743221345875\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017120747263232868\n",
      "Average test loss: 0.0037205863433579602\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017094815125068028\n",
      "Average test loss: 0.0038193706528594096\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01707691714250379\n",
      "Average test loss: 0.00382416676171124\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017066706183883878\n",
      "Average test loss: 0.003752971007178227\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017041273535953628\n",
      "Average test loss: 0.0037356839475946296\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01702611496878995\n",
      "Average test loss: 0.003760251403475801\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01700959260099464\n",
      "Average test loss: 0.0037926222301191753\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016981873128149246\n",
      "Average test loss: 0.003761539741936657\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016956302946640387\n",
      "Average test loss: 0.003786670791606108\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016948397883110575\n",
      "Average test loss: 0.0037321781886534558\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016930267242093882\n",
      "Average test loss: 0.0037682893743945494\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016905061465998492\n",
      "Average test loss: 0.0038620420133488047\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0168950531217787\n",
      "Average test loss: 0.003795551164696614\n",
      "Epoch 102/300\n",
      "Average training loss: 0.016881833132770325\n",
      "Average test loss: 0.0038781143747684027\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01685485558046235\n",
      "Average test loss: 0.0037553401115453904\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01685226499868764\n",
      "Average test loss: 0.0037892122937159406\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01681946837902069\n",
      "Average test loss: 0.0038272491523789033\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0168050104263756\n",
      "Average test loss: 0.003853071914985776\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016812800619337293\n",
      "Average test loss: 0.003815148442983627\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01678028646608194\n",
      "Average test loss: 0.0037880571600463655\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01677720037764973\n",
      "Average test loss: 0.0038611734689523777\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016744174405932427\n",
      "Average test loss: 0.00383711037846903\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016723091731468836\n",
      "Average test loss: 0.003915643651866251\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016708812082807223\n",
      "Average test loss: 0.0037538909471283356\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01670025636918015\n",
      "Average test loss: 0.0038974041543487045\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01667324678103129\n",
      "Average test loss: 0.0037781877554953097\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016673200602332752\n",
      "Average test loss: 0.003836386998494466\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016663327377703454\n",
      "Average test loss: 0.0038330337065789434\n",
      "Epoch 117/300\n",
      "Average training loss: 0.016631052861611047\n",
      "Average test loss: 0.003905156024628215\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016632495967050392\n",
      "Average test loss: 0.00379142134802209\n",
      "Epoch 119/300\n",
      "Average training loss: 0.016610624945826002\n",
      "Average test loss: 0.0038036956652585004\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01661030210968521\n",
      "Average test loss: 0.003869488266400165\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016594921497835053\n",
      "Average test loss: 0.0037804355327453877\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016562413786848387\n",
      "Average test loss: 0.00396232277361883\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01655820258706808\n",
      "Average test loss: 0.0037561945281922816\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01654881405664815\n",
      "Average test loss: 0.0038195517199734845\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016532966747879983\n",
      "Average test loss: 0.0039155401386734515\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016530138163102996\n",
      "Average test loss: 0.003864495843028029\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016511114188366465\n",
      "Average test loss: 0.003832713594039281\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016499425974571044\n",
      "Average test loss: 0.003865855275342862\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016488358815511066\n",
      "Average test loss: 0.003953606233828597\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01646647969053851\n",
      "Average test loss: 0.003806428293387095\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016462136818303004\n",
      "Average test loss: 0.0038069276279873317\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016446203716927104\n",
      "Average test loss: 0.003831931449472904\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01642437917408016\n",
      "Average test loss: 0.0038579281551970376\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016419664264553123\n",
      "Average test loss: 0.0038594859128610955\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016414602799548043\n",
      "Average test loss: 0.00385984884078304\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01641288419895702\n",
      "Average test loss: 0.003938339435184996\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01638825459778309\n",
      "Average test loss: 0.003773149851916565\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01637231748468346\n",
      "Average test loss: 0.003954374349365632\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016373659032914375\n",
      "Average test loss: 0.0038668242879211904\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01634690369831191\n",
      "Average test loss: 0.0038650533710088996\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016335213916997115\n",
      "Average test loss: 0.0038807083273099527\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016334822214312025\n",
      "Average test loss: 0.0038124191378139786\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01631542515423563\n",
      "Average test loss: 0.003833121375905143\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016308683758808507\n",
      "Average test loss: 0.003846475517998139\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016303470520509615\n",
      "Average test loss: 0.0038303149841311906\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016278336936400996\n",
      "Average test loss: 0.004052330851554871\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016281568148897755\n",
      "Average test loss: 0.00385766381998029\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0162590374648571\n",
      "Average test loss: 0.0038379109795722696\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016255087088379597\n",
      "Average test loss: 0.003916778465939893\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016245932199060918\n",
      "Average test loss: 0.003906811711481876\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016235671409302287\n",
      "Average test loss: 0.003970903174330791\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016219039693474768\n",
      "Average test loss: 0.003974150560796261\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016227999602754912\n",
      "Average test loss: 0.0038720687006910644\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016208589918083615\n",
      "Average test loss: 0.0038416549108094638\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01619572891212172\n",
      "Average test loss: 0.003886380102692379\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016203307868705854\n",
      "Average test loss: 0.0038641472359498344\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016170065994891854\n",
      "Average test loss: 0.004039989736138118\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016166088672147857\n",
      "Average test loss: 0.0038763492368161677\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016148428118891184\n",
      "Average test loss: 0.0038936415267073446\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016158385650979146\n",
      "Average test loss: 0.003825485976619853\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01614036368081967\n",
      "Average test loss: 0.003955013791720073\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01613986007703675\n",
      "Average test loss: 0.003866671634837985\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016120009513364897\n",
      "Average test loss: 0.003933795709990793\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016104151288668316\n",
      "Average test loss: 0.0038642022042638725\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01611352762579918\n",
      "Average test loss: 0.003872363700427943\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016090733832783168\n",
      "Average test loss: 0.0039207657439013326\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016108379513025285\n",
      "Average test loss: 0.00394160235962934\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016073246589137447\n",
      "Average test loss: 0.004158785101854139\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01606653317477968\n",
      "Average test loss: 0.003923933693518241\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016062796217699845\n",
      "Average test loss: 0.0038829664395501214\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01605738126900461\n",
      "Average test loss: 0.003960719435993168\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016050393485360675\n",
      "Average test loss: 0.004005713123828173\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01603802110503117\n",
      "Average test loss: 0.0038300903952784007\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01602784587442875\n",
      "Average test loss: 0.0038653042014274333\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016017431277367804\n",
      "Average test loss: 0.003977522265166044\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01601896412256691\n",
      "Average test loss: 0.00395673964648611\n",
      "Epoch 177/300\n",
      "Average training loss: 0.016009428141017756\n",
      "Average test loss: 0.003939894333067868\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01597559720195002\n",
      "Average test loss: 0.0038826863449066876\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015992924860782094\n",
      "Average test loss: 0.003909933932953411\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015996787403192787\n",
      "Average test loss: 0.003959327791714006\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015983669590618876\n",
      "Average test loss: 0.003973518145374126\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015964968801372582\n",
      "Average test loss: 0.0039084569826308225\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015958245442973244\n",
      "Average test loss: 0.00392324362653825\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015945374138653278\n",
      "Average test loss: 0.0039559108685288166\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015946541817651854\n",
      "Average test loss: 0.003895162450356616\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015952467401822407\n",
      "Average test loss: 0.0038901634263909524\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01593227695922057\n",
      "Average test loss: 0.003946052510291338\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015926923209594355\n",
      "Average test loss: 0.003948866514282094\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01591906386944983\n",
      "Average test loss: 0.003991901787204875\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015908021718263627\n",
      "Average test loss: 0.003907018767255876\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015912674583494664\n",
      "Average test loss: 0.0038976642137600317\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015881731771760516\n",
      "Average test loss: 0.0038504910415245426\n",
      "Epoch 193/300\n",
      "Average training loss: 0.015894680119223063\n",
      "Average test loss: 0.0038702665799193914\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01589493620561229\n",
      "Average test loss: 0.004028777183757888\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015889626206623184\n",
      "Average test loss: 0.0038646419605033264\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015854459997680453\n",
      "Average test loss: 0.003939141566140784\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015866896531648106\n",
      "Average test loss: 0.0038854963620089823\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015851194601919916\n",
      "Average test loss: 0.003945286175753507\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015855878996352355\n",
      "Average test loss: 0.003951477808877826\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015837518281406827\n",
      "Average test loss: 0.003887627552366919\n",
      "Epoch 201/300\n",
      "Average training loss: 0.015840118684702448\n",
      "Average test loss: 0.003999648307346636\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01583364332632886\n",
      "Average test loss: 0.0038585464109977085\n",
      "Epoch 203/300\n",
      "Average training loss: 0.015827982471221025\n",
      "Average test loss: 0.0038983619577354855\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015814607633484735\n",
      "Average test loss: 0.0039018873965574637\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015809912915031116\n",
      "Average test loss: 0.003989928861252136\n",
      "Epoch 206/300\n",
      "Average training loss: 0.015805395333303347\n",
      "Average test loss: 0.003925301202883323\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015786978041960133\n",
      "Average test loss: 0.003930944110163384\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01578612955411275\n",
      "Average test loss: 0.003929545087532865\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015791015597681205\n",
      "Average test loss: 0.003914077644960748\n",
      "Epoch 210/300\n",
      "Average training loss: 0.015781176528169048\n",
      "Average test loss: 0.0038816308228092062\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015772880151867865\n",
      "Average test loss: 0.003995272245258093\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01575715912381808\n",
      "Average test loss: 0.003942415892663929\n",
      "Epoch 213/300\n",
      "Average training loss: 0.015756682353715103\n",
      "Average test loss: 0.003912460464570258\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01573986605885956\n",
      "Average test loss: 0.0040095919939792815\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015737358644604682\n",
      "Average test loss: 0.003922293709384071\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015746233113937906\n",
      "Average test loss: 0.003909067989844415\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015740400553577477\n",
      "Average test loss: 0.0040084235523309975\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015725240093138484\n",
      "Average test loss: 0.00390635002280275\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01572511452353663\n",
      "Average test loss: 0.004009422570880917\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015715551325016553\n",
      "Average test loss: 0.003928014557394717\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01572056110451619\n",
      "Average test loss: 0.003981672148323722\n",
      "Epoch 222/300\n",
      "Average training loss: 0.015712300188839436\n",
      "Average test loss: 0.0038972694381243654\n",
      "Epoch 223/300\n",
      "Average training loss: 0.015701363850798872\n",
      "Average test loss: 0.0039806685528407494\n",
      "Epoch 224/300\n",
      "Average training loss: 0.015698825312157473\n",
      "Average test loss: 0.004025165899553233\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01569045918352074\n",
      "Average test loss: 0.003949888107677301\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015678221315145492\n",
      "Average test loss: 0.00394990847652985\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015668878042035633\n",
      "Average test loss: 0.004080303449597623\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015674917490945923\n",
      "Average test loss: 0.003948237751507096\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015671189127696886\n",
      "Average test loss: 0.00389399508262674\n",
      "Epoch 230/300\n",
      "Average training loss: 0.015659844380286003\n",
      "Average test loss: 0.00400984970956213\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015664740809963808\n",
      "Average test loss: 0.00411750947849618\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01565969186110629\n",
      "Average test loss: 0.004050335307502084\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015643379802505176\n",
      "Average test loss: 0.004087755072034067\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01564936309059461\n",
      "Average test loss: 0.003931762826732463\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01563943871524599\n",
      "Average test loss: 0.003907369229942561\n",
      "Epoch 236/300\n",
      "Average training loss: 0.015620578101939625\n",
      "Average test loss: 0.00391103321665691\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015630486119124626\n",
      "Average test loss: 0.004038963129123052\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015616084299981594\n",
      "Average test loss: 0.003993058437688483\n",
      "Epoch 239/300\n",
      "Average training loss: 0.015621792676548164\n",
      "Average test loss: 0.003965663842442963\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01561242275685072\n",
      "Average test loss: 0.003937482668707768\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015609499543077415\n",
      "Average test loss: 0.004029668353291022\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015598822889228661\n",
      "Average test loss: 0.0040084131407654945\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01559285103281339\n",
      "Average test loss: 0.00394003695456518\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01559077417353789\n",
      "Average test loss: 0.004015998662759861\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01558042148583465\n",
      "Average test loss: 0.003941871127941542\n",
      "Epoch 246/300\n",
      "Average training loss: 0.015573010231885644\n",
      "Average test loss: 0.003951200479227635\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015580421422918638\n",
      "Average test loss: 0.004052760063360135\n",
      "Epoch 248/300\n",
      "Average training loss: 0.015565538734197616\n",
      "Average test loss: 0.0039320646478898\n",
      "Epoch 249/300\n",
      "Average training loss: 0.015583252587252194\n",
      "Average test loss: 0.00397746831840939\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015558552069796457\n",
      "Average test loss: 0.003988520819900764\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01555932843933503\n",
      "Average test loss: 0.00397401249781251\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015561851127280129\n",
      "Average test loss: 0.003951798008547889\n",
      "Epoch 253/300\n",
      "Average training loss: 0.015542776722047064\n",
      "Average test loss: 0.003880981324861447\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015542708943287532\n",
      "Average test loss: 0.0039244513480613625\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015538836258980964\n",
      "Average test loss: 0.003957019081442721\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015527859203517437\n",
      "Average test loss: 0.0039348918170564705\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01551778317325645\n",
      "Average test loss: 0.004070446066558361\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015531254551476902\n",
      "Average test loss: 0.003983837160799238\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015529450169040096\n",
      "Average test loss: 0.004036006995994184\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015516478952434328\n",
      "Average test loss: 0.003992411869681544\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015512196050749884\n",
      "Average test loss: 0.0040642999230573575\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015507010521988074\n",
      "Average test loss: 0.004019920946409305\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015491794730226199\n",
      "Average test loss: 0.004133211228996516\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015500908977455563\n",
      "Average test loss: 0.004089431682808532\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015496439279781447\n",
      "Average test loss: 0.0041421275238196055\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015489045356710751\n",
      "Average test loss: 0.003980192166649633\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015482586772077613\n",
      "Average test loss: 0.0039071311780975925\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015477800445424185\n",
      "Average test loss: 0.003952365229320195\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015472053973211183\n",
      "Average test loss: 0.003954369182801909\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01547513338095612\n",
      "Average test loss: 0.003944256996942891\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01546892323758867\n",
      "Average test loss: 0.003914239387959242\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01545342833134863\n",
      "Average test loss: 0.004090379690958394\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015446931512819397\n",
      "Average test loss: 0.003978292087506917\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015464387632078594\n",
      "Average test loss: 0.004016500333117114\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015443220558265846\n",
      "Average test loss: 0.0039346388044456644\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015442371642010079\n",
      "Average test loss: 0.0040507336722479925\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0154279715915521\n",
      "Average test loss: 0.003940608693493737\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015447892681592041\n",
      "Average test loss: 0.004028029319105877\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015430556612710158\n",
      "Average test loss: 0.003971445527962513\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015421809476282862\n",
      "Average test loss: 0.004004169540272819\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015433839946985245\n",
      "Average test loss: 0.004007019759466251\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015416216819650597\n",
      "Average test loss: 0.004006546785641048\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015420056474705537\n",
      "Average test loss: 0.0039664344390233355\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015408474382427003\n",
      "Average test loss: 0.004033719571100341\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015405866738822726\n",
      "Average test loss: 0.003980085250818067\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015406486209068034\n",
      "Average test loss: 0.003985959388936559\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015392479060424699\n",
      "Average test loss: 0.003990592692254318\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015384067015515434\n",
      "Average test loss: 0.004040057549667027\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015386577688157559\n",
      "Average test loss: 0.004052333901325862\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01538944773707125\n",
      "Average test loss: 0.004049041302046842\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015389574025240209\n",
      "Average test loss: 0.004013168607734972\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015382664412260055\n",
      "Average test loss: 0.004114143026785718\n",
      "Epoch 293/300\n",
      "Average training loss: 0.015387579222520192\n",
      "Average test loss: 0.004000617114620076\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015365581644078096\n",
      "Average test loss: 0.0039655590922468235\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015369947320885128\n",
      "Average test loss: 0.004002617404071821\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015375236102276379\n",
      "Average test loss: 0.004044154933343331\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015363980851239628\n",
      "Average test loss: 0.004016074815971984\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01536806307070785\n",
      "Average test loss: 0.0040349902560313546\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015351319124301275\n",
      "Average test loss: 0.004008499063965347\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015351706172029177\n",
      "Average test loss: 0.003971741983253095\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10051793945497936\n",
      "Average test loss: 0.004317894671940142\n",
      "Epoch 2/300\n",
      "Average training loss: 0.024457613225612376\n",
      "Average test loss: 0.00377139574330714\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020928669560286734\n",
      "Average test loss: 0.003725112976299392\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019641793173220422\n",
      "Average test loss: 0.003460319413906998\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01894888425204489\n",
      "Average test loss: 0.0033877078913566137\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018487582641343276\n",
      "Average test loss: 0.0032940792333748603\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01811129295412037\n",
      "Average test loss: 0.0032745350872476894\n",
      "Epoch 8/300\n",
      "Average training loss: 0.017812667803631888\n",
      "Average test loss: 0.0032060876451432707\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01755294815202554\n",
      "Average test loss: 0.0031768821188145214\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01732352062811454\n",
      "Average test loss: 0.003137907543943988\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01710870960354805\n",
      "Average test loss: 0.00305168312560353\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01690568205383089\n",
      "Average test loss: 0.003021853018345104\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016714167752199704\n",
      "Average test loss: 0.002998459045878715\n",
      "Epoch 14/300\n",
      "Average training loss: 0.016550822600722312\n",
      "Average test loss: 0.0030812169547296234\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016367291084594195\n",
      "Average test loss: 0.00294688119366765\n",
      "Epoch 16/300\n",
      "Average training loss: 0.016195243179798126\n",
      "Average test loss: 0.002906943018030789\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01602403269873725\n",
      "Average test loss: 0.0029443623568448755\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015859058919052284\n",
      "Average test loss: 0.003024794680169887\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015709273235665427\n",
      "Average test loss: 0.0028537320558809573\n",
      "Epoch 20/300\n",
      "Average training loss: 0.015547422562208441\n",
      "Average test loss: 0.002817237946515282\n",
      "Epoch 21/300\n",
      "Average training loss: 0.015401100356545714\n",
      "Average test loss: 0.002804685337468982\n",
      "Epoch 22/300\n",
      "Average training loss: 0.015259589987496535\n",
      "Average test loss: 0.002776737241902285\n",
      "Epoch 23/300\n",
      "Average training loss: 0.015125635273754597\n",
      "Average test loss: 0.002797367155137989\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014981335477696525\n",
      "Average test loss: 0.0027562278678847684\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014867279609044394\n",
      "Average test loss: 0.002744197924517923\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014764821570780542\n",
      "Average test loss: 0.0027050090183814365\n",
      "Epoch 27/300\n",
      "Average training loss: 0.014647999688155122\n",
      "Average test loss: 0.0027075609436465636\n",
      "Epoch 28/300\n",
      "Average training loss: 0.014571461652716\n",
      "Average test loss: 0.00274041584175494\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014476473816566996\n",
      "Average test loss: 0.0027066246169722744\n",
      "Epoch 30/300\n",
      "Average training loss: 0.014372975844475958\n",
      "Average test loss: 0.0026666498991350335\n",
      "Epoch 31/300\n",
      "Average training loss: 0.014298994079232215\n",
      "Average test loss: 0.002664293062148823\n",
      "Epoch 32/300\n",
      "Average training loss: 0.014226352138651742\n",
      "Average test loss: 0.0026824603047635822\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01414790233141846\n",
      "Average test loss: 0.0026943861059844495\n",
      "Epoch 34/300\n",
      "Average training loss: 0.014075073704951339\n",
      "Average test loss: 0.0026579491160809996\n",
      "Epoch 35/300\n",
      "Average training loss: 0.014027530391183164\n",
      "Average test loss: 0.0026408261402199665\n",
      "Epoch 36/300\n",
      "Average training loss: 0.013963830006619294\n",
      "Average test loss: 0.0026346036456525324\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013908801308936543\n",
      "Average test loss: 0.0026534122592873043\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01385420484137204\n",
      "Average test loss: 0.002652423594664368\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013776816693445047\n",
      "Average test loss: 0.0026169093466467327\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013735070690512658\n",
      "Average test loss: 0.002623738364626964\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013680044473459323\n",
      "Average test loss: 0.002699934397927589\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013627261882854833\n",
      "Average test loss: 0.0026256474918789335\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013592000012596448\n",
      "Average test loss: 0.0026044302597228023\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013533557467990452\n",
      "Average test loss: 0.002617979122015337\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013487218344377147\n",
      "Average test loss: 0.0026577269147253697\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013432244334783818\n",
      "Average test loss: 0.002709050960217913\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013433706792692344\n",
      "Average test loss: 0.002626057596877217\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013359674075411426\n",
      "Average test loss: 0.0026549269635644224\n",
      "Epoch 49/300\n",
      "Average training loss: 0.013313170599440734\n",
      "Average test loss: 0.0026786252889368267\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013283653704656496\n",
      "Average test loss: 0.0026889417146643\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013254019444187482\n",
      "Average test loss: 0.002616774200151364\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013187201021446122\n",
      "Average test loss: 0.002616738928378456\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013136275481018755\n",
      "Average test loss: 0.0026664409500857193\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013115572762986024\n",
      "Average test loss: 0.002598866494062046\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01310611557132668\n",
      "Average test loss: 0.002594469063397911\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013034111464189159\n",
      "Average test loss: 0.0026223666444420814\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013019461346997155\n",
      "Average test loss: 0.0026051980377071434\n",
      "Epoch 58/300\n",
      "Average training loss: 0.012981955878436565\n",
      "Average test loss: 0.002720767255458567\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01293830617931154\n",
      "Average test loss: 0.002640673076733947\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012901757270925575\n",
      "Average test loss: 0.002657408120110631\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012840738339556589\n",
      "Average test loss: 0.0026100164430422914\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012837955066727268\n",
      "Average test loss: 0.0026660076189372276\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012819454675747289\n",
      "Average test loss: 0.00261080960639649\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012778000688387288\n",
      "Average test loss: 0.002629664619349771\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012706619820661015\n",
      "Average test loss: 0.0026159402618391647\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012693426940176222\n",
      "Average test loss: 0.0026223243857837384\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01265877282122771\n",
      "Average test loss: 0.0026573669572050374\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01264270456135273\n",
      "Average test loss: 0.0026705394194771847\n",
      "Epoch 69/300\n",
      "Average training loss: 0.012610725161102083\n",
      "Average test loss: 0.002630274560302496\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012570632496641742\n",
      "Average test loss: 0.002682173523844944\n",
      "Epoch 71/300\n",
      "Average training loss: 0.012579570910996861\n",
      "Average test loss: 0.002635039658389158\n",
      "Epoch 72/300\n",
      "Average training loss: 0.012528031250668897\n",
      "Average test loss: 0.002745060704027613\n",
      "Epoch 73/300\n",
      "Average training loss: 0.012494178704089589\n",
      "Average test loss: 0.002777701125997636\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012473512143724495\n",
      "Average test loss: 0.0026471639912989406\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012444094293647342\n",
      "Average test loss: 0.0026516590841735403\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012411508187651634\n",
      "Average test loss: 0.0026977972330318556\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012383490922550361\n",
      "Average test loss: 0.002717005186403791\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012414325370556778\n",
      "Average test loss: 0.002667492971238163\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012339938808646467\n",
      "Average test loss: 0.0026973977836055887\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012330791350868012\n",
      "Average test loss: 0.0026971933622327118\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012282128300103876\n",
      "Average test loss: 0.0027157518662926225\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012270169590082433\n",
      "Average test loss: 0.002701560056457917\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01224409931484196\n",
      "Average test loss: 0.002617646182369855\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012228709749877453\n",
      "Average test loss: 0.0027592126385619243\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01218722624828418\n",
      "Average test loss: 0.0028665232790840997\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012184081706735822\n",
      "Average test loss: 0.0026705214981403618\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012161440141085122\n",
      "Average test loss: 0.002720136021470858\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012141006579001745\n",
      "Average test loss: 0.002649595814032687\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0121213125259512\n",
      "Average test loss: 0.0027796219260328345\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012104863372941812\n",
      "Average test loss: 0.002822721069264743\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01208666115750869\n",
      "Average test loss: 0.002687349867903524\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012062137769328223\n",
      "Average test loss: 0.0027004982485539382\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012061469740337796\n",
      "Average test loss: 0.0026646953771511713\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012039611409935687\n",
      "Average test loss: 0.0029301994231839975\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012009829111811187\n",
      "Average test loss: 0.0027176907691690657\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012010952704482608\n",
      "Average test loss: 0.0027406346533033585\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011969441925485928\n",
      "Average test loss: 0.0027703759734415347\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011963666030102306\n",
      "Average test loss: 0.002661592880781326\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011942858030812608\n",
      "Average test loss: 0.0027070754383587176\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01192622161242697\n",
      "Average test loss: 0.0027289859975377717\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011904250680572456\n",
      "Average test loss: 0.002725583992898464\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011886010874476698\n",
      "Average test loss: 0.0026955482359561655\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01189791877484984\n",
      "Average test loss: 0.002781168723064992\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01190770826737086\n",
      "Average test loss: 0.002838351245969534\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011841636690828536\n",
      "Average test loss: 0.0028144565015617343\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0118215348886119\n",
      "Average test loss: 0.0027171163250588708\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011838939426673783\n",
      "Average test loss: 0.0027692328213403624\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011797081669171651\n",
      "Average test loss: 0.0026657615962127844\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011805593774550491\n",
      "Average test loss: 0.0027887165714055298\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011770084901816314\n",
      "Average test loss: 0.002755944062645237\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011770350384215514\n",
      "Average test loss: 0.002792713212470214\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011751395954440037\n",
      "Average test loss: 0.002794020803852214\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011741423468622897\n",
      "Average test loss: 0.0027637640579293173\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011763177277313339\n",
      "Average test loss: 0.0027943627052009106\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011723232693142361\n",
      "Average test loss: 0.0028376739310721558\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011713575652076138\n",
      "Average test loss: 0.0027735200516051718\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011699215181171894\n",
      "Average test loss: 0.002740061384522253\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01168916652434402\n",
      "Average test loss: 0.0027775015065239534\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011683019511401653\n",
      "Average test loss: 0.0027144990358501674\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011642724017302195\n",
      "Average test loss: 0.0027900209501385688\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011647321349216831\n",
      "Average test loss: 0.0027129933403597936\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011657798159453604\n",
      "Average test loss: 0.0027305562320268815\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011637205261323186\n",
      "Average test loss: 0.0028366633475654657\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011613813238011465\n",
      "Average test loss: 0.002748465393566423\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011588968560099602\n",
      "Average test loss: 0.0027713943777812854\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011628590933978557\n",
      "Average test loss: 0.002727549972012639\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011581096129284965\n",
      "Average test loss: 0.0027517968714237213\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011556140993204381\n",
      "Average test loss: 0.0027239002088705697\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011572655945188469\n",
      "Average test loss: 0.0027668842153830663\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011564991869860226\n",
      "Average test loss: 0.002876012802744905\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01154409038937754\n",
      "Average test loss: 0.002768285602124201\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011521003496315744\n",
      "Average test loss: 0.002782721179847916\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0115212911301189\n",
      "Average test loss: 0.0027494340501725673\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011520434213181337\n",
      "Average test loss: 0.002859733001846406\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011491925554143058\n",
      "Average test loss: 0.002761176442934407\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011502993951241176\n",
      "Average test loss: 0.002792230957187712\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011502286043432024\n",
      "Average test loss: 0.0027046397402882578\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01147252508252859\n",
      "Average test loss: 0.002766046671403779\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01148405714912547\n",
      "Average test loss: 0.002881002773737742\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011478993424938785\n",
      "Average test loss: 0.0028478292398568658\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011457476160592502\n",
      "Average test loss: 0.0027454585019085144\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011452547561791208\n",
      "Average test loss: 0.0028034447201838097\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011426694321963523\n",
      "Average test loss: 0.002726473148705231\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011420782119863563\n",
      "Average test loss: 0.002814134353564845\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011416798828376665\n",
      "Average test loss: 0.0027914243074547915\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011404796010090245\n",
      "Average test loss: 0.002756562995000018\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01142467659463485\n",
      "Average test loss: 0.002744543361167113\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011391974433428712\n",
      "Average test loss: 0.0027897395309474733\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01138073582905862\n",
      "Average test loss: 0.0027308911548720467\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011374596499320534\n",
      "Average test loss: 0.002763804523067342\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011364086925155586\n",
      "Average test loss: 0.003072420797828171\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011396874639723036\n",
      "Average test loss: 0.0028626874801185396\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011350672062072489\n",
      "Average test loss: 0.002820482076973551\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011370085701346398\n",
      "Average test loss: 0.0028400797500378557\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011340623674293358\n",
      "Average test loss: 0.0027351600515345734\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011338746518724494\n",
      "Average test loss: 0.0028262265488091442\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011328127819630834\n",
      "Average test loss: 0.002821280034155481\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011328334567447503\n",
      "Average test loss: 0.002832834788080719\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011306840375893646\n",
      "Average test loss: 0.0028021859816379016\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011301046466661824\n",
      "Average test loss: 0.002841911534468333\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011311606862478786\n",
      "Average test loss: 0.0028170500352150863\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011299838362468614\n",
      "Average test loss: 0.0028403356545087365\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011294451151457097\n",
      "Average test loss: 0.002912508117241992\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01127465078069104\n",
      "Average test loss: 0.0028158492907467817\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011254375270671314\n",
      "Average test loss: 0.002788246569327182\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01124190694755978\n",
      "Average test loss: 0.0028410131291796764\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011259345984707276\n",
      "Average test loss: 0.002838859992929631\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011239194175435436\n",
      "Average test loss: 0.002958621611094309\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011243202881266673\n",
      "Average test loss: 0.002781046962365508\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011245068040986855\n",
      "Average test loss: 0.0028174905828717684\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011238133696218332\n",
      "Average test loss: 0.0028916013460192416\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011204717583954334\n",
      "Average test loss: 0.002847970093703932\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011228762132426103\n",
      "Average test loss: 0.00294003814438151\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011217851824230619\n",
      "Average test loss: 0.0027794320854461854\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011212555988795227\n",
      "Average test loss: 0.002804071750698818\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011196340858108467\n",
      "Average test loss: 0.0028002419842200147\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011197815372712082\n",
      "Average test loss: 0.002845815800751249\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011203003377550178\n",
      "Average test loss: 0.0027698675585496758\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011198797990878423\n",
      "Average test loss: 0.0028306526583102014\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011162903003394604\n",
      "Average test loss: 0.0027887899545538755\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0111856075666017\n",
      "Average test loss: 0.002815629994082782\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01117614610079262\n",
      "Average test loss: 0.0028918782037993273\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011165263855622875\n",
      "Average test loss: 0.002892943184201916\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01114255592558119\n",
      "Average test loss: 0.0029163259282294247\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011171147243016296\n",
      "Average test loss: 0.0028042709982643525\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011164606452816063\n",
      "Average test loss: 0.0027867689095437526\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011180504327019055\n",
      "Average test loss: 0.0029095239771736994\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011124470943378078\n",
      "Average test loss: 0.0030098163667652343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011115223120484087\n",
      "Average test loss: 0.0027560115340683196\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011119060681511958\n",
      "Average test loss: 0.002858548508750068\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011113907650113105\n",
      "Average test loss: 0.002856291750859883\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011116412591603068\n",
      "Average test loss: 0.002930008657897512\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011078887343406677\n",
      "Average test loss: 0.002902655531755752\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011098286041782961\n",
      "Average test loss: 0.0028816451935304538\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0111135038236777\n",
      "Average test loss: 0.002847257942996091\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011080014982157283\n",
      "Average test loss: 0.002825113310996029\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011087783453365167\n",
      "Average test loss: 0.0027915223294662107\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011092232128812208\n",
      "Average test loss: 0.0029477959935449894\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01108271188951201\n",
      "Average test loss: 0.002808196087057392\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011067597813076443\n",
      "Average test loss: 0.002954668721701536\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011079817234641976\n",
      "Average test loss: 0.0028871367851065264\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011057045311149623\n",
      "Average test loss: 0.002909657075794207\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011042935520410538\n",
      "Average test loss: 0.002826347088855174\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011064018035100567\n",
      "Average test loss: 0.0028654774186822277\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011072090710202853\n",
      "Average test loss: 0.0027959089517179464\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011049598677290811\n",
      "Average test loss: 0.002952102405950427\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011042875949707296\n",
      "Average test loss: 0.002853502075498303\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011027359002994166\n",
      "Average test loss: 0.002845915612868137\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011026258875926336\n",
      "Average test loss: 0.0030184338121778437\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011019815331531896\n",
      "Average test loss: 0.002753391000131766\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011024359320600828\n",
      "Average test loss: 0.0028165552674068346\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0110261226122578\n",
      "Average test loss: 0.00284468405528201\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011010148347251945\n",
      "Average test loss: 0.002844439539851414\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011023573742144637\n",
      "Average test loss: 0.002908890868640608\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011003571638630496\n",
      "Average test loss: 0.0029162951053844557\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010981937288410133\n",
      "Average test loss: 0.002885620130639937\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010983752334283458\n",
      "Average test loss: 0.002848653620936804\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010984115216467116\n",
      "Average test loss: 0.0030765823283129268\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010995813459985786\n",
      "Average test loss: 0.0028406340362918045\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010983951155510213\n",
      "Average test loss: 0.0029101610670073164\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010984610072440571\n",
      "Average test loss: 0.0027930617317971254\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010966099631041289\n",
      "Average test loss: 0.0028372392900702027\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010971075653202003\n",
      "Average test loss: 0.002809411025295655\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01097176513241397\n",
      "Average test loss: 0.002984379380941391\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010970182049605582\n",
      "Average test loss: 0.0028557962900958955\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010975995873825417\n",
      "Average test loss: 0.0031262807591507834\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010950333361824353\n",
      "Average test loss: 0.0028600507533798614\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010934379716714224\n",
      "Average test loss: 0.002873469417500827\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010945113785564899\n",
      "Average test loss: 0.0028463369119498465\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01093883111824592\n",
      "Average test loss: 0.0029025706316654883\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010936753457619084\n",
      "Average test loss: 0.0029995079044666554\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01094584863136212\n",
      "Average test loss: 0.002927320858567125\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010942069762282902\n",
      "Average test loss: 0.002848946696354283\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010929535718427764\n",
      "Average test loss: 0.002902775331917736\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010917630009353162\n",
      "Average test loss: 0.002844265178673797\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010927973834176858\n",
      "Average test loss: 0.0028932103665752543\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010918376674254736\n",
      "Average test loss: 0.0028589731856352753\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010918026592996385\n",
      "Average test loss: 0.0028865546851108473\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010900056555039353\n",
      "Average test loss: 0.0029422003318452174\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010895230208834013\n",
      "Average test loss: 0.003016993536510401\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010940853342827824\n",
      "Average test loss: 0.0029980857144627305\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010909684778087668\n",
      "Average test loss: 0.0028343051713373927\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01089847130742338\n",
      "Average test loss: 0.002873733122729593\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010884366461800205\n",
      "Average test loss: 0.002961098766471777\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010897717398073938\n",
      "Average test loss: 0.0027948153811610406\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01087900568710433\n",
      "Average test loss: 0.002830227817926142\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01087395182169146\n",
      "Average test loss: 0.0029134683596591156\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010876920787824524\n",
      "Average test loss: 0.0028269351231348184\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010883731359408962\n",
      "Average test loss: 0.002937781123444438\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01086956536769867\n",
      "Average test loss: 0.002944023717401756\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010863836074868839\n",
      "Average test loss: 0.0029957520148406426\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010866369630727502\n",
      "Average test loss: 0.0028898563399497004\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01085076784922017\n",
      "Average test loss: 0.003021667423968514\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010867586101922724\n",
      "Average test loss: 0.00288570215097732\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010853108192483585\n",
      "Average test loss: 0.0028705942376206317\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010856407256176074\n",
      "Average test loss: 0.002911092538697024\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010854558686829276\n",
      "Average test loss: 0.0030082135724110734\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010854220644467406\n",
      "Average test loss: 0.002902458960397376\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010852958490451177\n",
      "Average test loss: 0.003025036563889848\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010846677534282208\n",
      "Average test loss: 0.0029805562529298993\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010840351623793444\n",
      "Average test loss: 0.0028517989094058675\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01084242601527108\n",
      "Average test loss: 0.002941537303229173\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010818499110639096\n",
      "Average test loss: 0.0029758371733542947\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010844267900619242\n",
      "Average test loss: 0.002948939827374286\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01081678305649095\n",
      "Average test loss: 0.0029770225503792367\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01083424845834573\n",
      "Average test loss: 0.002959529104228649\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010819085529281033\n",
      "Average test loss: 0.0029130741322620046\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010812832368744744\n",
      "Average test loss: 0.0029398256509254378\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010812012900908788\n",
      "Average test loss: 0.0029199701655242176\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010807731462021668\n",
      "Average test loss: 0.002960382541641593\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010799769297242164\n",
      "Average test loss: 0.0028337845810585552\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01078648638808065\n",
      "Average test loss: 0.002887823906002773\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010791022871931394\n",
      "Average test loss: 0.0028330753576010464\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010798874407178825\n",
      "Average test loss: 0.002954323376218478\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010781482102970282\n",
      "Average test loss: 0.0029649264230910276\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010789574811028109\n",
      "Average test loss: 0.0029143192917108537\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010785819937785466\n",
      "Average test loss: 0.0028889126792136168\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01080466736935907\n",
      "Average test loss: 0.0028586703483015298\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010785708227919208\n",
      "Average test loss: 0.0029537631227738326\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01076216590901216\n",
      "Average test loss: 0.002932855931835042\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010774079947008026\n",
      "Average test loss: 0.002890911894126071\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010790570799675252\n",
      "Average test loss: 0.002876993843147324\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010779477237827248\n",
      "Average test loss: 0.002966353763929672\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0107682842777835\n",
      "Average test loss: 0.0029165480592184597\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010774122052308585\n",
      "Average test loss: 0.002988212423813012\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0107610631601678\n",
      "Average test loss: 0.0028286718223243953\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010756030081046952\n",
      "Average test loss: 0.002851254932375418\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010765865790347258\n",
      "Average test loss: 0.0028928822773612206\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01074252602789137\n",
      "Average test loss: 0.002990110758278105\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01075226151694854\n",
      "Average test loss: 0.002907794814556837\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010747990089158218\n",
      "Average test loss: 0.0028991276514199046\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010743793701132138\n",
      "Average test loss: 0.0028805675943278605\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010746392451226711\n",
      "Average test loss: 0.002934689958476358\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010737461155487432\n",
      "Average test loss: 0.002963324553022782\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01073364447719521\n",
      "Average test loss: 0.002889069060070647\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010737289275560113\n",
      "Average test loss: 0.002910088075324893\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010737528728942076\n",
      "Average test loss: 0.0028753034081310034\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010731471080746915\n",
      "Average test loss: 0.002916494232498937\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010740443139440484\n",
      "Average test loss: 0.0029637457326882415\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010730004746880796\n",
      "Average test loss: 0.0028639780162937113\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.09929449674487113\n",
      "Average test loss: 0.004737609144093262\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02264859433968862\n",
      "Average test loss: 0.0033338693421747947\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019037170338961813\n",
      "Average test loss: 0.00312534869938261\n",
      "Epoch 4/300\n",
      "Average training loss: 0.017593202821082538\n",
      "Average test loss: 0.002966185353489386\n",
      "Epoch 5/300\n",
      "Average training loss: 0.016785757546623547\n",
      "Average test loss: 0.0028658414581376646\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016227818865742948\n",
      "Average test loss: 0.0027556118437399467\n",
      "Epoch 7/300\n",
      "Average training loss: 0.015793826772107018\n",
      "Average test loss: 0.0027171653387033277\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015431531515386369\n",
      "Average test loss: 0.002586721563504802\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015111544913715786\n",
      "Average test loss: 0.0025265747625380754\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014836217682394716\n",
      "Average test loss: 0.0025699455984350707\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014553628539873494\n",
      "Average test loss: 0.002444700385754307\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014304953412049346\n",
      "Average test loss: 0.0023897009500198894\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01406854526533021\n",
      "Average test loss: 0.00239251589257684\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013835519060906437\n",
      "Average test loss: 0.0023411092356675203\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013595211525758108\n",
      "Average test loss: 0.002331990954362684\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013383691256244978\n",
      "Average test loss: 0.002255866204802361\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013174554848836529\n",
      "Average test loss: 0.0022490390894106694\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012966448477572865\n",
      "Average test loss: 0.0022109043853771357\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012777336419456536\n",
      "Average test loss: 0.0021814729943871497\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012576194945308898\n",
      "Average test loss: 0.002172064202320245\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012394265753527482\n",
      "Average test loss: 0.002189373708019654\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012210518327438169\n",
      "Average test loss: 0.002187718815377189\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012058511598242654\n",
      "Average test loss: 0.002539668703658713\n",
      "Epoch 24/300\n",
      "Average training loss: 0.011906364122198688\n",
      "Average test loss: 0.0021209122304701143\n",
      "Epoch 25/300\n",
      "Average training loss: 0.011767347156173653\n",
      "Average test loss: 0.0020576880623896917\n",
      "Epoch 26/300\n",
      "Average training loss: 0.011635285339421696\n",
      "Average test loss: 0.0020501764567775857\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011535900192128287\n",
      "Average test loss: 0.0020436458632143006\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01141860098640124\n",
      "Average test loss: 0.0020122705225108397\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01132810222854217\n",
      "Average test loss: 0.002007269392410914\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011247998304665089\n",
      "Average test loss: 0.0019840315779050192\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011134266841742728\n",
      "Average test loss: 0.001973843443310923\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01107082070182595\n",
      "Average test loss: 0.0020845292676240205\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01099879202991724\n",
      "Average test loss: 0.001972559489413268\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010906206502682633\n",
      "Average test loss: 0.0019592345700495772\n",
      "Epoch 35/300\n",
      "Average training loss: 0.010875545502536827\n",
      "Average test loss: 0.0019589082571781343\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010794073448412949\n",
      "Average test loss: 0.002012898800066776\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010751870022879707\n",
      "Average test loss: 0.0019467615878416432\n",
      "Epoch 38/300\n",
      "Average training loss: 0.010688811877535448\n",
      "Average test loss: 0.0019876353771736224\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01064657773408625\n",
      "Average test loss: 0.0019374287229859167\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01058407975981633\n",
      "Average test loss: 0.0020037630400102997\n",
      "Epoch 41/300\n",
      "Average training loss: 0.010544394459989335\n",
      "Average test loss: 0.0019503329650809368\n",
      "Epoch 42/300\n",
      "Average training loss: 0.010489201547371017\n",
      "Average test loss: 0.001977848985956775\n",
      "Epoch 43/300\n",
      "Average training loss: 0.010456445947289467\n",
      "Average test loss: 0.0019181247567757963\n",
      "Epoch 44/300\n",
      "Average training loss: 0.010407498808370696\n",
      "Average test loss: 0.001980932112265792\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010374203919122617\n",
      "Average test loss: 0.0019989602133217784\n",
      "Epoch 46/300\n",
      "Average training loss: 0.010333188426577383\n",
      "Average test loss: 0.0019519398141031464\n",
      "Epoch 47/300\n",
      "Average training loss: 0.010282114097641574\n",
      "Average test loss: 0.0019382763403571314\n",
      "Epoch 48/300\n",
      "Average training loss: 0.010243622046791846\n",
      "Average test loss: 0.0019682295121666456\n",
      "Epoch 49/300\n",
      "Average training loss: 0.010201256131960285\n",
      "Average test loss: 0.0019362075326757298\n",
      "Epoch 50/300\n",
      "Average training loss: 0.010164665883613958\n",
      "Average test loss: 0.0019855641131806706\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010149012020892568\n",
      "Average test loss: 0.0019688768003963764\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010095190188950963\n",
      "Average test loss: 0.0019399831611663103\n",
      "Epoch 53/300\n",
      "Average training loss: 0.010050764990763532\n",
      "Average test loss: 0.0019190606266881029\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010032496772706508\n",
      "Average test loss: 0.0019241687253945404\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010000969591240088\n",
      "Average test loss: 0.0019482588526896305\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009977824949142006\n",
      "Average test loss: 0.001938183320686221\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009948625509109762\n",
      "Average test loss: 0.0020098059586145813\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009909079585638312\n",
      "Average test loss: 0.0019754611843576034\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009869338682128323\n",
      "Average test loss: 0.001920693875497414\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009841501850634813\n",
      "Average test loss: 0.0019691155311755007\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009823153445704115\n",
      "Average test loss: 0.001919463036995795\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009802560022307767\n",
      "Average test loss: 0.0019243480643878381\n",
      "Epoch 63/300\n",
      "Average training loss: 0.00975815002868573\n",
      "Average test loss: 0.0019453166837079658\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009771526764664385\n",
      "Average test loss: 0.0019372416525665257\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009723167035314772\n",
      "Average test loss: 0.0019135433677583934\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009681448600358434\n",
      "Average test loss: 0.001985900772942437\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00966343455016613\n",
      "Average test loss: 0.001908347776884006\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009639157972402043\n",
      "Average test loss: 0.0019925064560439854\n",
      "Epoch 69/300\n",
      "Average training loss: 0.009599395733740594\n",
      "Average test loss: 0.0019589692459752164\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009589257874422602\n",
      "Average test loss: 0.0019251017816778687\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009575798204375638\n",
      "Average test loss: 0.0019446469145930475\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009533582908825743\n",
      "Average test loss: 0.0019894519532099365\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009522127577414115\n",
      "Average test loss: 0.0019578399871372516\n",
      "Epoch 74/300\n",
      "Average training loss: 0.009493855739633243\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_64_Depth10/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_64_Depth10/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 31.96649715338813\n",
      "Average test loss: 0.047179391310446794\n",
      "Epoch 2/300\n",
      "Average training loss: 12.61924392361111\n",
      "Average test loss: 0.13452022591895527\n",
      "Epoch 3/300\n",
      "Average training loss: 9.361921060350205\n",
      "Average test loss: 0.004632598211367925\n",
      "Epoch 4/300\n",
      "Average training loss: 7.393131803724501\n",
      "Average test loss: 0.004556335165682766\n",
      "Epoch 5/300\n",
      "Average training loss: 5.088184977001614\n",
      "Average test loss: 0.005590287084173825\n",
      "Epoch 6/300\n",
      "Average training loss: 3.200823096593221\n",
      "Average test loss: 0.008661017233712806\n",
      "Epoch 7/300\n",
      "Average training loss: 2.6987247053782144\n",
      "Average test loss: 0.03586840388696227\n",
      "Epoch 8/300\n",
      "Average training loss: 2.063296583175659\n",
      "Average test loss: 0.10739847055988179\n",
      "Epoch 9/300\n",
      "Average training loss: 1.7044853920406766\n",
      "Average test loss: 0.26803768113172716\n",
      "Epoch 10/300\n",
      "Average training loss: 1.417381526099311\n",
      "Average test loss: 0.004375744263538056\n",
      "Epoch 11/300\n",
      "Average training loss: 1.1750477452807957\n",
      "Average test loss: 0.004004340999035371\n",
      "Epoch 12/300\n",
      "Average training loss: 1.0180221559736464\n",
      "Average test loss: 0.01100064281332824\n",
      "Epoch 13/300\n",
      "Average training loss: 0.8705189799732632\n",
      "Average test loss: 0.052262355835487447\n",
      "Epoch 14/300\n",
      "Average training loss: 0.7243272267447578\n",
      "Average test loss: 0.6450899111231169\n",
      "Epoch 15/300\n",
      "Average training loss: 0.5935171383221944\n",
      "Average test loss: 0.9316623190906312\n",
      "Epoch 16/300\n",
      "Average training loss: 0.5102067159281837\n",
      "Average test loss: 0.013190651285979483\n",
      "Epoch 17/300\n",
      "Average training loss: 0.44155570758713614\n",
      "Average test loss: 1.3999893499215443\n",
      "Epoch 18/300\n",
      "Average training loss: 0.376745553361045\n",
      "Average test loss: 0.17878512207667033\n",
      "Epoch 19/300\n",
      "Average training loss: 0.30911826933754816\n",
      "Average test loss: 0.004098658132884237\n",
      "Epoch 20/300\n",
      "Average training loss: 0.265361805624432\n",
      "Average test loss: 0.015087926240430937\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2356648539967007\n",
      "Average test loss: 0.16069365369445748\n",
      "Epoch 22/300\n",
      "Average training loss: 0.21311756891674466\n",
      "Average test loss: 0.02037969999656909\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1944154986275567\n",
      "Average test loss: 0.00379303228110075\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1800197985039817\n",
      "Average test loss: 0.024770750938604275\n",
      "Epoch 25/300\n",
      "Average training loss: 0.17972016010019515\n",
      "Average test loss: 0.011457892542084059\n",
      "Epoch 26/300\n",
      "Average training loss: 0.16025795136557686\n",
      "Average test loss: 0.5801485182974073\n",
      "Epoch 27/300\n",
      "Average training loss: 0.15167613801691268\n",
      "Average test loss: 0.0037887417491939334\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14512974103291829\n",
      "Average test loss: 0.24950566606389152\n",
      "Epoch 29/300\n",
      "Average training loss: 0.141203603360388\n",
      "Average test loss: 0.003746144790202379\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13647707041104634\n",
      "Average test loss: 0.0037514859040578205\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13211823330322903\n",
      "Average test loss: 0.0037087857437630494\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1292080581055747\n",
      "Average test loss: 0.0037082343445056015\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1283596652812428\n",
      "Average test loss: 0.004190544337448147\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12491870151625739\n",
      "Average test loss: 0.0036835381481796505\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12288685295979182\n",
      "Average test loss: 0.0037045336057328518\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12132718071672652\n",
      "Average test loss: 0.0037522667544997402\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12020767020516926\n",
      "Average test loss: 0.0036729558505531816\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11904482508367963\n",
      "Average test loss: 0.003690567640380727\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11945410709248648\n",
      "Average test loss: 0.00366674642254495\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11762654032972124\n",
      "Average test loss: 0.003655618464367257\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11661056904660332\n",
      "Average test loss: 0.0036651715758360096\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11600584101014667\n",
      "Average test loss: 0.0036706781879895264\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1155317167242368\n",
      "Average test loss: 0.003635647358165847\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11498487061924405\n",
      "Average test loss: 0.0036494668192333644\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1144988266825676\n",
      "Average test loss: 0.003655930448323488\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11423867357439466\n",
      "Average test loss: 0.0036311707260708015\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11379787690440814\n",
      "Average test loss: 0.0036366205914980837\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11305992862913343\n",
      "Average test loss: 0.003688992050786813\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11280499692095651\n",
      "Average test loss: 0.0036342345863166784\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11239306516779794\n",
      "Average test loss: 0.003654691261963712\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11199286343653997\n",
      "Average test loss: 0.0036122760077317555\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11174970338079665\n",
      "Average test loss: 0.003643236364134484\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11116090151998732\n",
      "Average test loss: 0.003646553986188438\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11102249633603625\n",
      "Average test loss: 0.00363563602252139\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11089819335275226\n",
      "Average test loss: 0.0036173248800138633\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1103429077135192\n",
      "Average test loss: 0.003610181391032206\n",
      "Epoch 57/300\n",
      "Average training loss: 0.109901284204589\n",
      "Average test loss: 0.0036214673125909433\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1095473579035865\n",
      "Average test loss: 0.0037484300122078923\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10927513176202774\n",
      "Average test loss: 0.0036330915105839572\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10942594730854034\n",
      "Average test loss: 0.003634957435644335\n",
      "Epoch 61/300\n",
      "Average training loss: 0.10861881237559848\n",
      "Average test loss: 0.003622976617887616\n",
      "Epoch 62/300\n",
      "Average training loss: 0.10835084279378256\n",
      "Average test loss: 0.0036522049349215296\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1080126289791531\n",
      "Average test loss: 0.003623135026751293\n",
      "Epoch 64/300\n",
      "Average training loss: 0.10797869528664483\n",
      "Average test loss: 0.0036211871177785925\n",
      "Epoch 65/300\n",
      "Average training loss: 0.10729761019680235\n",
      "Average test loss: 0.003632424678860439\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10686456816063987\n",
      "Average test loss: 0.003650840600952506\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10687771275308397\n",
      "Average test loss: 0.0036352421171549293\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10637972652249866\n",
      "Average test loss: 0.0036142272748466995\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10596717770232095\n",
      "Average test loss: 0.003617500617893206\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10572907785243459\n",
      "Average test loss: 0.0036499149373008144\n",
      "Epoch 71/300\n",
      "Average training loss: 0.10530081065495808\n",
      "Average test loss: 0.0036080296515590615\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10489954793453217\n",
      "Average test loss: 0.00364025624613795\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10488504993253284\n",
      "Average test loss: 0.003691163072776463\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10434105524089601\n",
      "Average test loss: 0.003660038714814517\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10398001129759682\n",
      "Average test loss: 0.003692011136561632\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10367602239714728\n",
      "Average test loss: 0.0036913307507832845\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10339908232953814\n",
      "Average test loss: 0.0037704809670233066\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10315784493419859\n",
      "Average test loss: 0.0036907966488765346\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10284152372015846\n",
      "Average test loss: 0.00368293245571355\n",
      "Epoch 80/300\n",
      "Average training loss: 0.102303849293126\n",
      "Average test loss: 0.003767269877509938\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10222860213120778\n",
      "Average test loss: 0.0037053306938873397\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10175397803386052\n",
      "Average test loss: 0.0037380187089244526\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10155436693297493\n",
      "Average test loss: 0.0037640065283825\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10122221522198784\n",
      "Average test loss: 0.0037328556567016577\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10103847612937292\n",
      "Average test loss: 0.003651161222615176\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10048951319853465\n",
      "Average test loss: 0.0036709991200930543\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10030908736917708\n",
      "Average test loss: 0.003684972377907899\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10006905010673735\n",
      "Average test loss: 0.003754973986496528\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09979277427991232\n",
      "Average test loss: 0.003687043448082275\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09956640105777316\n",
      "Average test loss: 0.0037619144088692135\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09912829561034839\n",
      "Average test loss: 0.0037164716747485928\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09902955850958824\n",
      "Average test loss: 0.00374939479223556\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09855530635515848\n",
      "Average test loss: 0.00378649238207274\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09842135037647354\n",
      "Average test loss: 0.003794741041544411\n",
      "Epoch 95/300\n",
      "Average training loss: 0.09824953228235245\n",
      "Average test loss: 0.003779870896289746\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0979201529290941\n",
      "Average test loss: 0.0037207353508306876\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09763750208086437\n",
      "Average test loss: 0.0037561528130124013\n",
      "Epoch 98/300\n",
      "Average training loss: 0.09735925149917603\n",
      "Average test loss: 0.00378913811304503\n",
      "Epoch 99/300\n",
      "Average training loss: 0.09711196282174853\n",
      "Average test loss: 0.003784693190207084\n",
      "Epoch 100/300\n",
      "Average training loss: 0.09695181100898319\n",
      "Average test loss: 0.0038042616717931296\n",
      "Epoch 101/300\n",
      "Average training loss: 0.09668891427252027\n",
      "Average test loss: 0.003768034408489863\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09643458802170224\n",
      "Average test loss: 0.0038020825524710945\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09629461405674616\n",
      "Average test loss: 0.003738544713291857\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09607313561439514\n",
      "Average test loss: 0.003752080244736539\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0957103223601977\n",
      "Average test loss: 0.0037564194781912696\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09552358411418067\n",
      "Average test loss: 0.0038928623164279593\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09533904666370815\n",
      "Average test loss: 0.003941220274608996\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0952915413512124\n",
      "Average test loss: 0.003856642685416672\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09491871066557037\n",
      "Average test loss: 0.003953753192805581\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09461834592289395\n",
      "Average test loss: 0.0037959537121156853\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09439418248666658\n",
      "Average test loss: 0.0038794681599570646\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09426742285490036\n",
      "Average test loss: 0.003771830565813515\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0939567922618654\n",
      "Average test loss: 0.003763618717590968\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0939195948905415\n",
      "Average test loss: 0.0037809922190176117\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09376982080274159\n",
      "Average test loss: 0.003747365789074037\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09349203388558494\n",
      "Average test loss: 0.0038560817458977302\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0933073347012202\n",
      "Average test loss: 0.0038430090151313277\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0930361312230428\n",
      "Average test loss: 0.0038665452763024305\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09287195894453261\n",
      "Average test loss: 0.0039114983417093754\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0926207450363371\n",
      "Average test loss: 0.0038450053768853344\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09249287964238061\n",
      "Average test loss: 0.0038598349382066065\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09235586112737655\n",
      "Average test loss: 0.003908164523128007\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09233431997564104\n",
      "Average test loss: 0.0038479121555056836\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09198906863398022\n",
      "Average test loss: 0.0038317373690919744\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09180479745732413\n",
      "Average test loss: 0.0038912642376704347\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09161251238981882\n",
      "Average test loss: 0.0038712300531980066\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09143091773986817\n",
      "Average test loss: 0.003871252004885011\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09133300893836552\n",
      "Average test loss: 0.003841596903693345\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09104786193039682\n",
      "Average test loss: 0.003951549733264579\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09101267545753056\n",
      "Average test loss: 0.003818255942935745\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0907583943274286\n",
      "Average test loss: 0.003777515857376986\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09066769332355923\n",
      "Average test loss: 0.0038078940376225444\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09056482866075304\n",
      "Average test loss: 0.0038348087805012863\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09040887182950974\n",
      "Average test loss: 0.0038073873851034376\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09041421698530515\n",
      "Average test loss: 0.003897481560293171\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09014669689204957\n",
      "Average test loss: 0.003942117181089189\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08981100513537725\n",
      "Average test loss: 0.0038895847238600255\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08982343193557528\n",
      "Average test loss: 0.0037970163457923464\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08958227062225342\n",
      "Average test loss: 0.00394234824180603\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08941400435235765\n",
      "Average test loss: 0.003897870378775729\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08924725918637381\n",
      "Average test loss: 0.0039145635190523335\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08919036096334457\n",
      "Average test loss: 0.003866152216783828\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0890303287771013\n",
      "Average test loss: 0.003836737870135241\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08901685700813929\n",
      "Average test loss: 0.003860940411272976\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0887087580230501\n",
      "Average test loss: 0.0038403289073871243\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08854290856255426\n",
      "Average test loss: 0.003946970729364289\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08846006387141016\n",
      "Average test loss: 0.0038990487816433113\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08840435859892104\n",
      "Average test loss: 0.0038902635514322255\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0882150285575125\n",
      "Average test loss: 0.003900481485244301\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08824054984913932\n",
      "Average test loss: 0.003970884819411569\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08795219356483883\n",
      "Average test loss: 0.003889591751413213\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08779446677366892\n",
      "Average test loss: 0.003920455543738272\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08754815163877275\n",
      "Average test loss: 0.003901650400625335\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08751788210206561\n",
      "Average test loss: 0.003830709404208594\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08749748578336504\n",
      "Average test loss: 0.0038816449573884407\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08718571095334159\n",
      "Average test loss: 0.003926398740874397\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08720915922853682\n",
      "Average test loss: 0.003873733489463727\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08754063354598152\n",
      "Average test loss: 0.0041948460969660015\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08694429485499859\n",
      "Average test loss: 0.003946564874301354\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08676528964440028\n",
      "Average test loss: 0.0039010226263975103\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08666595727205277\n",
      "Average test loss: 0.003910917871528202\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08668189787864686\n",
      "Average test loss: 0.004044575728062126\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08655768897136053\n",
      "Average test loss: 0.003905030702965127\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08640077936318186\n",
      "Average test loss: 0.003947658500530653\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08634873139858246\n",
      "Average test loss: 0.00413507667515013\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08614212297399838\n",
      "Average test loss: 0.00403164359782305\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08621010418070688\n",
      "Average test loss: 0.00393571270402107\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08585286749733818\n",
      "Average test loss: 0.00394933902968963\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08586631988154517\n",
      "Average test loss: 0.003904962480896049\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08573868829674192\n",
      "Average test loss: 0.003884424142539501\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08556935121615727\n",
      "Average test loss: 0.003865548343087236\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0855852022336589\n",
      "Average test loss: 0.0039035680604477723\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08535189533895916\n",
      "Average test loss: 0.003818718812531895\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08531817520989313\n",
      "Average test loss: 0.003932152952171034\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08518244092994266\n",
      "Average test loss: 0.003961472387943003\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08493267820278803\n",
      "Average test loss: 0.003871261637657881\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08509808980756335\n",
      "Average test loss: 0.003917885688030058\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08498995352453656\n",
      "Average test loss: 0.003863677347699801\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0846843969159656\n",
      "Average test loss: 0.003974027932311098\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08494775634672906\n",
      "Average test loss: 0.003914696949844559\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08464307853248385\n",
      "Average test loss: 0.003942757223008408\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08455120488007864\n",
      "Average test loss: 0.0040874002691772245\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08430936627255546\n",
      "Average test loss: 0.003879611935466528\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08430490265289943\n",
      "Average test loss: 0.003983878773326675\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08429143708282047\n",
      "Average test loss: 0.0039167850481139285\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08398908270729913\n",
      "Average test loss: 0.003902877571475175\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08392676342195934\n",
      "Average test loss: 0.004101652842015028\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0839356276790301\n",
      "Average test loss: 0.003936484023721681\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08399141558011373\n",
      "Average test loss: 0.003926674825863705\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0836724749273724\n",
      "Average test loss: 0.00401244164754947\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08364051209555731\n",
      "Average test loss: 0.003907179880680309\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08353659113579326\n",
      "Average test loss: 0.004136656124144792\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08343900201055739\n",
      "Average test loss: 0.003888764437288046\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08346511199076971\n",
      "Average test loss: 0.0039055341051684486\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08324670323067242\n",
      "Average test loss: 0.003993411914962861\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08332360806067785\n",
      "Average test loss: 0.003862650998764568\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08317791756656434\n",
      "Average test loss: 0.00400042835664418\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0828741483423445\n",
      "Average test loss: 0.004029551943971051\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08281302458047866\n",
      "Average test loss: 0.0039005363653931355\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08274019760886828\n",
      "Average test loss: 0.004067481577396393\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08285028106636472\n",
      "Average test loss: 0.003930335485065977\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0825880997048484\n",
      "Average test loss: 0.0038006954164140753\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08265103003051547\n",
      "Average test loss: 0.004033334486393465\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08255862923463185\n",
      "Average test loss: 0.0039026517702473533\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08250419459078047\n",
      "Average test loss: 0.003920265990826819\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0821877414451705\n",
      "Average test loss: 0.003882960209416019\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08241895364059343\n",
      "Average test loss: 0.0038568885216696395\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08220726528432634\n",
      "Average test loss: 0.003988648088028034\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08229076082838906\n",
      "Average test loss: 0.003907296605201231\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08207577590147654\n",
      "Average test loss: 0.003918245303548044\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08184190917677349\n",
      "Average test loss: 0.003945338243204686\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0816854255464342\n",
      "Average test loss: 0.004041256412863731\n",
      "Epoch 213/300\n",
      "Average training loss: 0.081793452070819\n",
      "Average test loss: 0.0038523838638017576\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08169008948074447\n",
      "Average test loss: 0.004034408604933156\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08169948784510295\n",
      "Average test loss: 0.004087085589352581\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08156085087855657\n",
      "Average test loss: 0.003957708468039831\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08146705748637517\n",
      "Average test loss: 0.003932928045590718\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08132519728607601\n",
      "Average test loss: 0.00396243382183214\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08128046103980806\n",
      "Average test loss: 0.003912344620252649\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08136035270161099\n",
      "Average test loss: 0.004042676003028949\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08122997664080726\n",
      "Average test loss: 0.004083648732552926\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08115190949704912\n",
      "Average test loss: 0.003963563438504934\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08095972046587202\n",
      "Average test loss: 0.0039734400539762444\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08082099164194531\n",
      "Average test loss: 0.0039430091318984825\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08104529631137848\n",
      "Average test loss: 0.0039791852318578295\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08087169016069837\n",
      "Average test loss: 0.003905253290509184\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08070978768004311\n",
      "Average test loss: 0.004085148858527342\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08064348395003212\n",
      "Average test loss: 0.0038782515707943173\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08067686706119113\n",
      "Average test loss: 0.00402852413451506\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0803941581580374\n",
      "Average test loss: 0.00402589609949953\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08043284340368377\n",
      "Average test loss: 0.004118289704745015\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08035651578174698\n",
      "Average test loss: 0.003986068909987807\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08044166619910134\n",
      "Average test loss: 0.003970276950134171\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08028112732039558\n",
      "Average test loss: 0.003956215894470612\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08007336627774768\n",
      "Average test loss: 0.003947812196695142\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07994746298922432\n",
      "Average test loss: 0.00399991700425744\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07997119060489867\n",
      "Average test loss: 0.004058999486888449\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07996323844128185\n",
      "Average test loss: 0.00391038416698575\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0800558318429523\n",
      "Average test loss: 0.004023112486220068\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0797704498966535\n",
      "Average test loss: 0.004085522341852387\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0796907129155265\n",
      "Average test loss: 0.003962496129588948\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07974292571677102\n",
      "Average test loss: 0.0040403305577735105\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0797235739297337\n",
      "Average test loss: 0.003939658511843946\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07948028168744511\n",
      "Average test loss: 0.004060636422907313\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0795248833861616\n",
      "Average test loss: 0.0039787863737179175\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07939616468879912\n",
      "Average test loss: 0.003930117186986738\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07934148871236377\n",
      "Average test loss: 0.0040775705960889655\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07937760691510307\n",
      "Average test loss: 0.00392386335051722\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07948193324936761\n",
      "Average test loss: 0.003879342297299041\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0791494727730751\n",
      "Average test loss: 0.003952622145828273\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07914059644937516\n",
      "Average test loss: 0.004018430599528882\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07921967795160081\n",
      "Average test loss: 0.003923198284374343\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0790664907362726\n",
      "Average test loss: 0.004061977442974846\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07886842855479982\n",
      "Average test loss: 0.0040608832544336716\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07882450485229492\n",
      "Average test loss: 0.00408960078139272\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07878395324614312\n",
      "Average test loss: 0.004037556493861808\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07861888575222757\n",
      "Average test loss: 0.004011206366121769\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07867203804850578\n",
      "Average test loss: 0.003942159935625063\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07865255880355836\n",
      "Average test loss: 0.004001354546596607\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0788141146434678\n",
      "Average test loss: 0.003970397528260946\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07849132961034774\n",
      "Average test loss: 0.003968526431669792\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07848918439282311\n",
      "Average test loss: 0.004033744204375479\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07844219595193863\n",
      "Average test loss: 0.0041625334454907315\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0782556261519591\n",
      "Average test loss: 0.004108567375068864\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07826475326220195\n",
      "Average test loss: 0.003949122566936744\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07874594573179881\n",
      "Average test loss: 0.0040250577498227354\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07809582806295819\n",
      "Average test loss: 0.003930449394716157\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07803297443853484\n",
      "Average test loss: 0.004073387254857354\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07783312249845929\n",
      "Average test loss: 0.003999879005468554\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07782490507761637\n",
      "Average test loss: 0.004170528976867596\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0783246372838815\n",
      "Average test loss: 0.004046810350277358\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07802863016062313\n",
      "Average test loss: 0.003958076703051726\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07790443141592873\n",
      "Average test loss: 0.0038786070317857794\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07772996250788371\n",
      "Average test loss: 0.0041029971132261886\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07781471427281697\n",
      "Average test loss: 0.00399598152976897\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07765207446946037\n",
      "Average test loss: 0.004011182822080122\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0774602102306154\n",
      "Average test loss: 0.003958277506960763\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07778707182076242\n",
      "Average test loss: 0.004023852260576354\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07762083803945118\n",
      "Average test loss: 0.003940110855425397\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07732982473240958\n",
      "Average test loss: 0.003906055443816715\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07746202052964105\n",
      "Average test loss: 0.003977455486646957\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07736961011754141\n",
      "Average test loss: 0.0040614754166454076\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07744039839506149\n",
      "Average test loss: 0.004135519241293272\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07726041756735907\n",
      "Average test loss: 0.003957885859327184\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07716896539264255\n",
      "Average test loss: 0.003952281643533045\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07720507851574156\n",
      "Average test loss: 0.003938813135855728\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07707830025090112\n",
      "Average test loss: 0.004022063250756926\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07695925148990419\n",
      "Average test loss: 0.003978125061839819\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07685700652996699\n",
      "Average test loss: 0.004084368622551362\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07690852972533968\n",
      "Average test loss: 0.003985084693051047\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07691487405035231\n",
      "Average test loss: 0.0041525926554782525\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07734896281692717\n",
      "Average test loss: 0.003942645526801547\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07669472085767322\n",
      "Average test loss: 0.003996692730734746\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07664476996660233\n",
      "Average test loss: 0.004137214194155402\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07664114246765773\n",
      "Average test loss: 0.003928281171661285\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07667285833093855\n",
      "Average test loss: 0.004044613248772091\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07663918807440334\n",
      "Average test loss: 0.003979498658536209\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07651476453410254\n",
      "Average test loss: 0.003975230624692307\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07640527073542278\n",
      "Average test loss: 0.003987075961919295\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07646238789293501\n",
      "Average test loss: 0.003973981802662213\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 27.98341186862522\n",
      "Average test loss: 0.005578805755409929\n",
      "Epoch 2/300\n",
      "Average training loss: 15.379193184746637\n",
      "Average test loss: 0.004405863619926903\n",
      "Epoch 3/300\n",
      "Average training loss: 11.097603584289551\n",
      "Average test loss: 0.017701944465438525\n",
      "Epoch 4/300\n",
      "Average training loss: 9.240853625827365\n",
      "Average test loss: 0.0038248224893791807\n",
      "Epoch 5/300\n",
      "Average training loss: 7.946634400685628\n",
      "Average test loss: 0.0038110942660520473\n",
      "Epoch 6/300\n",
      "Average training loss: 7.450121310763889\n",
      "Average test loss: 0.0036151830322212644\n",
      "Epoch 7/300\n",
      "Average training loss: 6.778978146447075\n",
      "Average test loss: 0.022114580479347043\n",
      "Epoch 8/300\n",
      "Average training loss: 5.572272033267551\n",
      "Average test loss: 0.009052692765990893\n",
      "Epoch 9/300\n",
      "Average training loss: 5.1977738249037\n",
      "Average test loss: 0.0035009624840070803\n",
      "Epoch 10/300\n",
      "Average training loss: 5.097294927385119\n",
      "Average test loss: 3.53856907033258\n",
      "Epoch 11/300\n",
      "Average training loss: 4.136251206715902\n",
      "Average test loss: 0.03889984704181552\n",
      "Epoch 12/300\n",
      "Average training loss: 3.3848390144772\n",
      "Average test loss: 0.0031927845142781736\n",
      "Epoch 13/300\n",
      "Average training loss: 2.918379528257582\n",
      "Average test loss: 0.11111607680552535\n",
      "Epoch 14/300\n",
      "Average training loss: 2.4727435927920873\n",
      "Average test loss: 0.0034082900436802044\n",
      "Epoch 15/300\n",
      "Average training loss: 2.2495033457014295\n",
      "Average test loss: 0.03240388700428108\n",
      "Epoch 16/300\n",
      "Average training loss: 2.019744115193685\n",
      "Average test loss: 0.003044553190883663\n",
      "Epoch 17/300\n",
      "Average training loss: 1.7106822547912597\n",
      "Average test loss: 0.0030429178027229177\n",
      "Epoch 18/300\n",
      "Average training loss: 1.4561260476642184\n",
      "Average test loss: 0.0040987404425525\n",
      "Epoch 19/300\n",
      "Average training loss: 1.279071588728163\n",
      "Average test loss: 0.0038902896576457555\n",
      "Epoch 20/300\n",
      "Average training loss: 1.1550050098631117\n",
      "Average test loss: 0.1172865733795075\n",
      "Epoch 21/300\n",
      "Average training loss: 1.033262764930725\n",
      "Average test loss: 0.002878053727456265\n",
      "Epoch 22/300\n",
      "Average training loss: 0.9169592110845778\n",
      "Average test loss: 5.781936080639354\n",
      "Epoch 23/300\n",
      "Average training loss: 0.8102268240716722\n",
      "Average test loss: 0.0027949635800388123\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6977620120578342\n",
      "Average test loss: 0.0027485040902263586\n",
      "Epoch 25/300\n",
      "Average training loss: 0.6007699601915147\n",
      "Average test loss: 0.05579668780912955\n",
      "Epoch 26/300\n",
      "Average training loss: 0.5204976414839426\n",
      "Average test loss: 0.03040281239979797\n",
      "Epoch 27/300\n",
      "Average training loss: 0.45130072763231066\n",
      "Average test loss: 0.0027350633398940164\n",
      "Epoch 28/300\n",
      "Average training loss: 0.395097661919064\n",
      "Average test loss: 0.0027081055731202164\n",
      "Epoch 29/300\n",
      "Average training loss: 0.3463152555094825\n",
      "Average test loss: 0.0026571410352157223\n",
      "Epoch 30/300\n",
      "Average training loss: 0.30655615533722774\n",
      "Average test loss: 0.002720714279346996\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2716546895371543\n",
      "Average test loss: 0.0026693781529449753\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2421190643840366\n",
      "Average test loss: 0.0026213360539534027\n",
      "Epoch 33/300\n",
      "Average training loss: 0.2163945209980011\n",
      "Average test loss: 0.0027520212361382113\n",
      "Epoch 34/300\n",
      "Average training loss: 0.19492398165331948\n",
      "Average test loss: 0.0025917300217681463\n",
      "Epoch 35/300\n",
      "Average training loss: 0.17646554199854533\n",
      "Average test loss: 0.002609690703244673\n",
      "Epoch 36/300\n",
      "Average training loss: 0.16118773277600607\n",
      "Average test loss: 0.0025845330452753438\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1479671972990036\n",
      "Average test loss: 0.002563657640789946\n",
      "Epoch 38/300\n",
      "Average training loss: 0.13712469805611505\n",
      "Average test loss: 0.002571564770407147\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12799996930360794\n",
      "Average test loss: 0.0025870553087443114\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12031433282295863\n",
      "Average test loss: 0.0025536877072105807\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11410531240039401\n",
      "Average test loss: 0.002541452482963602\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10909787760840522\n",
      "Average test loss: 0.0026429629321727487\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10489575786060758\n",
      "Average test loss: 0.002576629559095535\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10128271919488907\n",
      "Average test loss: 0.002550188264085187\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09849092278877894\n",
      "Average test loss: 0.0025174271961053213\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09599552057849037\n",
      "Average test loss: 0.002517907174511088\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09395885935094621\n",
      "Average test loss: 0.002514528309926391\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09225738573736615\n",
      "Average test loss: 0.0026067263755119507\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09064204248454835\n",
      "Average test loss: 0.00252356061567035\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08955717170900769\n",
      "Average test loss: 0.0025200029275276596\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08857275242937936\n",
      "Average test loss: 0.0025611132904887197\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08755225123299493\n",
      "Average test loss: 0.002494785234125124\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08626051711373858\n",
      "Average test loss: 0.0025273716318317584\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0854989683230718\n",
      "Average test loss: 0.0025188443263371788\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08454381560617023\n",
      "Average test loss: 0.002500488275869025\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08365265104505751\n",
      "Average test loss: 0.0024977500602189036\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08313266983628273\n",
      "Average test loss: 0.0024954824960894055\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08228844410181045\n",
      "Average test loss: 0.0024974474782745045\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08154029728968938\n",
      "Average test loss: 0.002518494395953086\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0810210353533427\n",
      "Average test loss: 0.0024924331143912344\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08049521697892083\n",
      "Average test loss: 0.002495940735356675\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07987308180994458\n",
      "Average test loss: 0.0024860585667192936\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07904771052135362\n",
      "Average test loss: 0.0025169882025155754\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07872000195582708\n",
      "Average test loss: 0.0024893333421399195\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07808320964044994\n",
      "Average test loss: 0.0026283949407645397\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07753707477450371\n",
      "Average test loss: 0.0025139548578816985\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07694115430116653\n",
      "Average test loss: 0.0025064843826823765\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07678163674473762\n",
      "Average test loss: 0.0025372026262597907\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0757907118929757\n",
      "Average test loss: 0.0025421691770768826\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07548615123828252\n",
      "Average test loss: 0.0024949266843290793\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07503936345378558\n",
      "Average test loss: 0.002513988228721751\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07425337202681435\n",
      "Average test loss: 0.002486589132911629\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07389787236187192\n",
      "Average test loss: 0.0025187332280394103\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07365214285916752\n",
      "Average test loss: 0.0025334271228768758\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07326466456055641\n",
      "Average test loss: 0.0025455297983975876\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07266150673892763\n",
      "Average test loss: 0.002537385653704405\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07227760910987854\n",
      "Average test loss: 0.00258445670745439\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07184248635503981\n",
      "Average test loss: 0.002504690340616637\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07128841711415185\n",
      "Average test loss: 0.0025217445831901084\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07125458946492937\n",
      "Average test loss: 0.0025083626624610687\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07068380530012978\n",
      "Average test loss: 0.0025544681599777605\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07032534361216757\n",
      "Average test loss: 0.00260885953468581\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06995024380750127\n",
      "Average test loss: 0.002593843818642199\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0696051167315907\n",
      "Average test loss: 0.0026026156724741063\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06922752165132098\n",
      "Average test loss: 0.002546205541739861\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06878267191184892\n",
      "Average test loss: 0.002532698673920499\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06858162542184194\n",
      "Average test loss: 0.002590338045834667\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06819189559088813\n",
      "Average test loss: 0.0025932949139840073\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06773980879121357\n",
      "Average test loss: 0.0025339697142028146\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06756308295991685\n",
      "Average test loss: 0.0025516872790952525\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06740105625987053\n",
      "Average test loss: 0.002580358944212397\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06679136164983114\n",
      "Average test loss: 0.002638055777384175\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0668948385682371\n",
      "Average test loss: 0.0026575051847224436\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06637289118104511\n",
      "Average test loss: 0.0026022780472412703\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06617962889538871\n",
      "Average test loss: 0.0025790505131913556\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06602533735169305\n",
      "Average test loss: 0.0025432746993998686\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06566145392921235\n",
      "Average test loss: 0.002641916820365522\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0654433995683988\n",
      "Average test loss: 0.0025338970716628764\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06527020447784\n",
      "Average test loss: 0.0026484949259708325\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06484118201997545\n",
      "Average test loss: 0.002562736943674584\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06469015506241056\n",
      "Average test loss: 0.0026950239543285636\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06471539296044243\n",
      "Average test loss: 0.0025797060759117207\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06454366403155856\n",
      "Average test loss: 0.002571514577915271\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06409957205255826\n",
      "Average test loss: 0.00255791339961191\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06369880628916952\n",
      "Average test loss: 0.0026822105115279554\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06352062795890702\n",
      "Average test loss: 0.0026272015834434164\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06340842933456103\n",
      "Average test loss: 0.00259518744962083\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06342327238453759\n",
      "Average test loss: 0.002652041557555397\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06295467008484734\n",
      "Average test loss: 0.0025613719449482032\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06265857629312409\n",
      "Average test loss: 0.0025637210768957933\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06265628574291865\n",
      "Average test loss: 0.002664268101048138\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06250153065390057\n",
      "Average test loss: 0.002606872965892156\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06229448979761865\n",
      "Average test loss: 0.002659724782117539\n",
      "Epoch 114/300\n",
      "Average training loss: 0.062118017663558324\n",
      "Average test loss: 0.002630392837959031\n",
      "Epoch 115/300\n",
      "Average training loss: 0.062071483734581205\n",
      "Average test loss: 0.0027339906541423665\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06233619401190016\n",
      "Average test loss: 0.0026910750946650902\n",
      "Epoch 117/300\n",
      "Average training loss: 0.061777638382381865\n",
      "Average test loss: 0.002575072709057066\n",
      "Epoch 118/300\n",
      "Average training loss: 0.061490699847539264\n",
      "Average test loss: 0.002646216542563505\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06119725709822443\n",
      "Average test loss: 0.0026653833167834414\n",
      "Epoch 120/300\n",
      "Average training loss: 0.060966131495104894\n",
      "Average test loss: 0.002584345712636908\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06087869745824072\n",
      "Average test loss: 0.0025875849367843736\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06065060112211439\n",
      "Average test loss: 0.002618576674825615\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06091890711916818\n",
      "Average test loss: 0.002634203899341325\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06048267063829634\n",
      "Average test loss: 0.0026867981559286513\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06030816435813904\n",
      "Average test loss: 0.002654739410926898\n",
      "Epoch 126/300\n",
      "Average training loss: 0.060387441684802375\n",
      "Average test loss: 0.002680896846163604\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06012998997833994\n",
      "Average test loss: 0.0026205459121200776\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05985452588399251\n",
      "Average test loss: 0.0027037919285810654\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05970658687088225\n",
      "Average test loss: 0.0026480603182895317\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05978307117025058\n",
      "Average test loss: 0.0026339609538101488\n",
      "Epoch 131/300\n",
      "Average training loss: 0.059542963710096146\n",
      "Average test loss: 0.0026441186387091875\n",
      "Epoch 132/300\n",
      "Average training loss: 0.059378093434704675\n",
      "Average test loss: 0.002702019697572622\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05921360551317533\n",
      "Average test loss: 0.002676810974876086\n",
      "Epoch 134/300\n",
      "Average training loss: 0.059244845979743534\n",
      "Average test loss: 0.002693204692668385\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05899330335524347\n",
      "Average test loss: 0.0026593377208337186\n",
      "Epoch 136/300\n",
      "Average training loss: 0.058649824675586486\n",
      "Average test loss: 0.0025865543784780636\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05875898140337732\n",
      "Average test loss: 0.002674805734306574\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05862162268161774\n",
      "Average test loss: 0.002683517559948895\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05862330572803815\n",
      "Average test loss: 0.0026334796100854875\n",
      "Epoch 140/300\n",
      "Average training loss: 0.058443420049217015\n",
      "Average test loss: 0.0027011565612629056\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05834660969840156\n",
      "Average test loss: 0.0026920934925890633\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05831582601202859\n",
      "Average test loss: 0.0026772400937560533\n",
      "Epoch 143/300\n",
      "Average training loss: 0.057994097855356\n",
      "Average test loss: 0.0026364105846732855\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05786278894543648\n",
      "Average test loss: 0.0026836324863963655\n",
      "Epoch 145/300\n",
      "Average training loss: 0.057728884389003116\n",
      "Average test loss: 0.0028816928909056717\n",
      "Epoch 146/300\n",
      "Average training loss: 0.057693358533912234\n",
      "Average test loss: 0.0026200026739388706\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05761745209826363\n",
      "Average test loss: 0.002672175931226876\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05759343929754363\n",
      "Average test loss: 0.0026097369473427534\n",
      "Epoch 149/300\n",
      "Average training loss: 0.057340057985650165\n",
      "Average test loss: 0.0026908515008787315\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0579451836016443\n",
      "Average test loss: 0.0026923095035470196\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05706223479906718\n",
      "Average test loss: 0.0027617739331391123\n",
      "Epoch 152/300\n",
      "Average training loss: 0.056984625034862096\n",
      "Average test loss: 0.0026919787385397487\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05700967658890618\n",
      "Average test loss: 0.002694311734702852\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05691372849212752\n",
      "Average test loss: 0.0026801525116380716\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05671349862880177\n",
      "Average test loss: 0.0026403626814070673\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05674638586905267\n",
      "Average test loss: 0.002707867162509097\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05673783383435673\n",
      "Average test loss: 0.0027146482980913585\n",
      "Epoch 158/300\n",
      "Average training loss: 0.056545202963882026\n",
      "Average test loss: 0.0026714504241115516\n",
      "Epoch 159/300\n",
      "Average training loss: 0.056364466461870405\n",
      "Average test loss: 0.0026845212328351206\n",
      "Epoch 160/300\n",
      "Average training loss: 0.056302604188521704\n",
      "Average test loss: 0.0026930890346152916\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05642960779865583\n",
      "Average test loss: 0.0027126432098448275\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0566846744120121\n",
      "Average test loss: 0.0026849220020489562\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05614713548289405\n",
      "Average test loss: 0.002641815939090318\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05588598243395487\n",
      "Average test loss: 0.002741548287268314\n",
      "Epoch 165/300\n",
      "Average training loss: 0.055812034755945206\n",
      "Average test loss: 0.0026163576708899605\n",
      "Epoch 166/300\n",
      "Average training loss: 0.055866063091490004\n",
      "Average test loss: 0.0026759117630620797\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05573637747764587\n",
      "Average test loss: 0.002629469860552086\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05571330816878213\n",
      "Average test loss: 0.002646384830897053\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05562320799297757\n",
      "Average test loss: 0.0027039067896289957\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0556475603348679\n",
      "Average test loss: 0.002654212465716733\n",
      "Epoch 171/300\n",
      "Average training loss: 0.055428303440411886\n",
      "Average test loss: 0.002814426875155833\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0553995796640714\n",
      "Average test loss: 0.002687706955605083\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05524546664290958\n",
      "Average test loss: 0.0027109445414195458\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0552723704179128\n",
      "Average test loss: 0.0027383269270261127\n",
      "Epoch 175/300\n",
      "Average training loss: 0.055347690562407174\n",
      "Average test loss: 0.0026688155813349617\n",
      "Epoch 176/300\n",
      "Average training loss: 0.055004261351293986\n",
      "Average test loss: 0.0025980571260054907\n",
      "Epoch 177/300\n",
      "Average training loss: 0.055055967549482984\n",
      "Average test loss: 0.00275908689511319\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05489653271436691\n",
      "Average test loss: 0.002739536020076937\n",
      "Epoch 179/300\n",
      "Average training loss: 0.054783746825324164\n",
      "Average test loss: 0.0026683241344160505\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05473562548889054\n",
      "Average test loss: 0.0026790792575726906\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05481933621565501\n",
      "Average test loss: 0.0026809564835081497\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05478848011957275\n",
      "Average test loss: 0.002720230529912644\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05447182556324535\n",
      "Average test loss: 0.002750118285417557\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05437180241280132\n",
      "Average test loss: 0.0027339651227618255\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05466339988178677\n",
      "Average test loss: 0.002767452675435278\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05425570382012261\n",
      "Average test loss: 0.0027817086651921274\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05466445070505142\n",
      "Average test loss: 0.002660919770494931\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05417665397458606\n",
      "Average test loss: 0.0026618949554653632\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05406557790769471\n",
      "Average test loss: 0.0027189450703768267\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05403240424394608\n",
      "Average test loss: 0.0027713160972214408\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05410133110152351\n",
      "Average test loss: 0.0027057255384408765\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05400868581732114\n",
      "Average test loss: 0.0028529423740175034\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05388818683889177\n",
      "Average test loss: 0.0027230666526075866\n",
      "Epoch 194/300\n",
      "Average training loss: 0.053799326519171395\n",
      "Average test loss: 0.002660199972904391\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05375808743304677\n",
      "Average test loss: 0.0026988886476804813\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05391584221190877\n",
      "Average test loss: 0.002774491531981362\n",
      "Epoch 197/300\n",
      "Average training loss: 0.053582963195112014\n",
      "Average test loss: 0.002714277854603198\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05374621770448155\n",
      "Average test loss: 0.002738009450957179\n",
      "Epoch 199/300\n",
      "Average training loss: 0.053564766880538725\n",
      "Average test loss: 0.0027946468426121606\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05347984439465735\n",
      "Average test loss: 0.0026989541991303363\n",
      "Epoch 201/300\n",
      "Average training loss: 0.053420999560091234\n",
      "Average test loss: 0.0027409005930854216\n",
      "Epoch 202/300\n",
      "Average training loss: 0.053317123221026524\n",
      "Average test loss: 0.0027657476341765787\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05325458225607872\n",
      "Average test loss: 0.0026802395465266374\n",
      "Epoch 204/300\n",
      "Average training loss: 0.053198360833856796\n",
      "Average test loss: 0.0027881461586803198\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05314957719047864\n",
      "Average test loss: 0.002758578525442216\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0533409547176626\n",
      "Average test loss: 0.002740262071084645\n",
      "Epoch 207/300\n",
      "Average training loss: 0.052950881861978105\n",
      "Average test loss: 0.0027055226222922404\n",
      "Epoch 208/300\n",
      "Average training loss: 0.053114168968465596\n",
      "Average test loss: 0.0027353996070515777\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05289130617843734\n",
      "Average test loss: 0.0027086034920066593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.052923918108145396\n",
      "Average test loss: 0.0027722166402058468\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0528170070581966\n",
      "Average test loss: 0.002795936928027206\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05284204371107949\n",
      "Average test loss: 0.0026936081759631634\n",
      "Epoch 213/300\n",
      "Average training loss: 0.052783983889553285\n",
      "Average test loss: 0.0027558125363041958\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05263655484053824\n",
      "Average test loss: 0.0027133857230138446\n",
      "Epoch 215/300\n",
      "Average training loss: 0.052659398115343516\n",
      "Average test loss: 0.002783100609357158\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0525370937552717\n",
      "Average test loss: 0.002778047474308146\n",
      "Epoch 217/300\n",
      "Average training loss: 0.052466978665855195\n",
      "Average test loss: 0.0027372485347506074\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05249387022852898\n",
      "Average test loss: 0.00269218086451292\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05242573040392664\n",
      "Average test loss: 0.0027641760841425923\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05232374143931601\n",
      "Average test loss: 0.0027731187807189094\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05227302133705881\n",
      "Average test loss: 0.0027743374597695137\n",
      "Epoch 222/300\n",
      "Average training loss: 0.052339182982842125\n",
      "Average test loss: 0.0027285486968855062\n",
      "Epoch 223/300\n",
      "Average training loss: 0.052211651623249054\n",
      "Average test loss: 0.00281537514542126\n",
      "Epoch 224/300\n",
      "Average training loss: 0.052107468095090655\n",
      "Average test loss: 0.0027715106414010126\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05214112967252731\n",
      "Average test loss: 0.002729817125325402\n",
      "Epoch 226/300\n",
      "Average training loss: 0.052184213495916794\n",
      "Average test loss: 0.0027254526163968773\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05226797679728932\n",
      "Average test loss: 0.0027431041734913984\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05209799711571799\n",
      "Average test loss: 0.002741087397767438\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05197271837128533\n",
      "Average test loss: 0.0027867169897589418\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05191532854239146\n",
      "Average test loss: 0.0027703202174355584\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05192181498474545\n",
      "Average test loss: 0.002808669687145286\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05184083417720265\n",
      "Average test loss: 0.0028114886314918597\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05197306033968926\n",
      "Average test loss: 0.0027898920730998118\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05173065641522408\n",
      "Average test loss: 0.0026920083715683885\n",
      "Epoch 235/300\n",
      "Average training loss: 0.051595910107096035\n",
      "Average test loss: 0.002786454011996587\n",
      "Epoch 236/300\n",
      "Average training loss: 0.051493487222327126\n",
      "Average test loss: 0.002711309568956494\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05151135286026531\n",
      "Average test loss: 0.0027709974348545073\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05149492097894351\n",
      "Average test loss: 0.0027489900400655136\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05155628138449457\n",
      "Average test loss: 0.0027249981549878916\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05140164461069637\n",
      "Average test loss: 0.0027443533327637447\n",
      "Epoch 241/300\n",
      "Average training loss: 0.051247765176826054\n",
      "Average test loss: 0.0027977122738957407\n",
      "Epoch 242/300\n",
      "Average training loss: 0.051493803868691124\n",
      "Average test loss: 0.002715280629797942\n",
      "Epoch 243/300\n",
      "Average training loss: 0.051244473540120655\n",
      "Average test loss: 0.00276660712228881\n",
      "Epoch 244/300\n",
      "Average training loss: 0.051238832549916374\n",
      "Average test loss: 0.0027423028335389166\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05116216452585326\n",
      "Average test loss: 0.0027174108210537167\n",
      "Epoch 246/300\n",
      "Average training loss: 0.051169736878739464\n",
      "Average test loss: 0.0027229776505587827\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05113039597206646\n",
      "Average test loss: 0.0027757359846598572\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05098915717668003\n",
      "Average test loss: 0.0027089333629442586\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05097985949781206\n",
      "Average test loss: 0.00281527700016482\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05106245760454072\n",
      "Average test loss: 0.002691518994151718\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05104909232258797\n",
      "Average test loss: 0.0027929463128869734\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05098038894931475\n",
      "Average test loss: 0.002755083185931047\n",
      "Epoch 253/300\n",
      "Average training loss: 0.050895534687572054\n",
      "Average test loss: 0.0027684178729024196\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05082547384169367\n",
      "Average test loss: 0.002753160031305419\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05084881366292636\n",
      "Average test loss: 0.0026916220980799863\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05072430173224873\n",
      "Average test loss: 0.0027208288125693796\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05068829077482224\n",
      "Average test loss: 0.0027769727017730476\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05058185430367788\n",
      "Average test loss: 0.002794355594035652\n",
      "Epoch 259/300\n",
      "Average training loss: 0.050611700339449776\n",
      "Average test loss: 0.002726611876239379\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05065432030293677\n",
      "Average test loss: 0.0027705317723254364\n",
      "Epoch 261/300\n",
      "Average training loss: 0.050614278952280684\n",
      "Average test loss: 0.00275080700384246\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0506166792081462\n",
      "Average test loss: 0.002708738904653324\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05034558996889326\n",
      "Average test loss: 0.002795365110867553\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05034720398651229\n",
      "Average test loss: 0.0027172590392745203\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05037725809878773\n",
      "Average test loss: 0.0027801130848626294\n",
      "Epoch 266/300\n",
      "Average training loss: 0.050208130809995866\n",
      "Average test loss: 0.00276227957672543\n",
      "Epoch 267/300\n",
      "Average training loss: 0.050252066589064066\n",
      "Average test loss: 0.0027853688574913473\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0503396800590886\n",
      "Average test loss: 0.0027845839195781283\n",
      "Epoch 269/300\n",
      "Average training loss: 0.050254036674896876\n",
      "Average test loss: 0.0027804236109885906\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05019569303260909\n",
      "Average test loss: 0.0027702384278592137\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05035186661945449\n",
      "Average test loss: 0.00283065569628444\n",
      "Epoch 272/300\n",
      "Average training loss: 0.050325934810770884\n",
      "Average test loss: 0.0027206607988725104\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05019660763939222\n",
      "Average test loss: 0.002724567550751898\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05001836953891648\n",
      "Average test loss: 0.00276853468724423\n",
      "Epoch 275/300\n",
      "Average training loss: 0.050073885490496954\n",
      "Average test loss: 0.0027827391537527243\n",
      "Epoch 276/300\n",
      "Average training loss: 0.049973622878392535\n",
      "Average test loss: 0.0027837684795053467\n",
      "Epoch 277/300\n",
      "Average training loss: 0.049991864456070796\n",
      "Average test loss: 0.0029174731930510867\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04997119312816196\n",
      "Average test loss: 0.0027941523765524228\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04980092347330517\n",
      "Average test loss: 0.002783886373663942\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04976953208115366\n",
      "Average test loss: 0.002863739517827829\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04977727961871359\n",
      "Average test loss: 0.0027785914569265313\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04970713932315508\n",
      "Average test loss: 0.002748235296147565\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04985314237740305\n",
      "Average test loss: 0.002791105258382029\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04979790611399545\n",
      "Average test loss: 0.0027632291213505796\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04959126141998503\n",
      "Average test loss: 0.0027779965003331504\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04966528527273072\n",
      "Average test loss: 0.002776030680992537\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04966411363416248\n",
      "Average test loss: 0.0027913702720155317\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04968660367859734\n",
      "Average test loss: 0.0027679923959076404\n",
      "Epoch 289/300\n",
      "Average training loss: 0.049515724301338195\n",
      "Average test loss: 0.0027711803428828717\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0495587164428499\n",
      "Average test loss: 0.0028385883087499275\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04938452929755052\n",
      "Average test loss: 0.002738653954325451\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0494691663980484\n",
      "Average test loss: 0.0027687065166731675\n",
      "Epoch 293/300\n",
      "Average training loss: 0.049328797145022286\n",
      "Average test loss: 0.0028308409419324663\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04931097924378183\n",
      "Average test loss: 0.0027638828967594437\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04935565113359027\n",
      "Average test loss: 0.0027742289232297076\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0492480270365874\n",
      "Average test loss: 0.0027721504972626765\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04933064452144835\n",
      "Average test loss: 0.0028368921619322564\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04944361467493905\n",
      "Average test loss: 0.002840453668187062\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04917131313019329\n",
      "Average test loss: 0.002747221264160342\n",
      "Epoch 300/300\n",
      "Average training loss: 0.049706546584765116\n",
      "Average test loss: 0.0027726540540655453\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 29.129018914116752\n",
      "Average test loss: 7.835919048508008\n",
      "Epoch 2/300\n",
      "Average training loss: 16.886485439724392\n",
      "Average test loss: 0.005253311132805215\n",
      "Epoch 3/300\n",
      "Average training loss: 12.11200330013699\n",
      "Average test loss: 2.3251348985516365\n",
      "Epoch 4/300\n",
      "Average training loss: 10.224581115722657\n",
      "Average test loss: 0.01399014146419035\n",
      "Epoch 5/300\n",
      "Average training loss: 9.586446173773872\n",
      "Average test loss: 0.003240131333263384\n",
      "Epoch 6/300\n",
      "Average training loss: 8.88937547132704\n",
      "Average test loss: 0.020105372307201226\n",
      "Epoch 7/300\n",
      "Average training loss: 8.525754314846463\n",
      "Average test loss: 0.003258038358969821\n",
      "Epoch 8/300\n",
      "Average training loss: 8.469888363308376\n",
      "Average test loss: 0.003035049131554034\n",
      "Epoch 9/300\n",
      "Average training loss: 6.718861484951443\n",
      "Average test loss: 0.43282771974388096\n",
      "Epoch 10/300\n",
      "Average training loss: 5.528867359585232\n",
      "Average test loss: 0.023228883747218383\n",
      "Epoch 11/300\n",
      "Average training loss: 5.180931880103217\n",
      "Average test loss: 0.9271652975976468\n",
      "Epoch 12/300\n",
      "Average training loss: 4.377186844719781\n",
      "Average test loss: 0.002675519350812667\n",
      "Epoch 13/300\n",
      "Average training loss: 3.695497962527805\n",
      "Average test loss: 0.003078149852446384\n",
      "Epoch 14/300\n",
      "Average training loss: 3.450583801269531\n",
      "Average test loss: 0.13267327399469084\n",
      "Epoch 15/300\n",
      "Average training loss: 3.1355357337527803\n",
      "Average test loss: 0.0656358068290477\n",
      "Epoch 16/300\n",
      "Average training loss: 2.6684991334279378\n",
      "Average test loss: 3.7016360193358526\n",
      "Epoch 17/300\n",
      "Average training loss: 2.2361630316840277\n",
      "Average test loss: 0.0911348394498022\n",
      "Epoch 18/300\n",
      "Average training loss: 1.9690303849114312\n",
      "Average test loss: 0.01173002640157938\n",
      "Epoch 19/300\n",
      "Average training loss: 1.7852773589028252\n",
      "Average test loss: 0.01349586185399029\n",
      "Epoch 20/300\n",
      "Average training loss: 1.625852238231235\n",
      "Average test loss: 2.2581965477185117\n",
      "Epoch 21/300\n",
      "Average training loss: 1.4406183553271823\n",
      "Average test loss: 0.013621621862260831\n",
      "Epoch 22/300\n",
      "Average training loss: 1.2736435476938883\n",
      "Average test loss: 0.007258459815341565\n",
      "Epoch 23/300\n",
      "Average training loss: 1.1409263610839844\n",
      "Average test loss: 0.011910319903658496\n",
      "Epoch 24/300\n",
      "Average training loss: 1.025895774682363\n",
      "Average test loss: 0.0038896571656482086\n",
      "Epoch 25/300\n",
      "Average training loss: 0.9180461115837097\n",
      "Average test loss: 0.002115503520083924\n",
      "Epoch 26/300\n",
      "Average training loss: 0.8185727319187588\n",
      "Average test loss: 0.0020486736490080756\n",
      "Epoch 27/300\n",
      "Average training loss: 0.7306941310564677\n",
      "Average test loss: 0.0020254876714510222\n",
      "Epoch 28/300\n",
      "Average training loss: 0.6498123958375719\n",
      "Average test loss: 0.0022758163686634764\n",
      "Epoch 29/300\n",
      "Average training loss: 0.5763982962237464\n",
      "Average test loss: 0.0019770456509043775\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5081846304999458\n",
      "Average test loss: 0.0019722783237488732\n",
      "Epoch 31/300\n",
      "Average training loss: 0.44800787647565204\n",
      "Average test loss: 0.0019521591930339733\n",
      "Epoch 32/300\n",
      "Average training loss: 0.39240860912534925\n",
      "Average test loss: 0.0019603547015123895\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3424466125435299\n",
      "Average test loss: 0.0019999705691718392\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2986364981068505\n",
      "Average test loss: 0.0019137407721330723\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2617195559872521\n",
      "Average test loss: 0.0019153198230390748\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2292054807080163\n",
      "Average test loss: 0.0018939254822002516\n",
      "Epoch 37/300\n",
      "Average training loss: 0.20210510712199742\n",
      "Average test loss: 0.0018848624566776885\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1786687532928255\n",
      "Average test loss: 0.0019580540649799835\n",
      "Epoch 39/300\n",
      "Average training loss: 0.15849642003907097\n",
      "Average test loss: 0.0019394258882643447\n",
      "Epoch 40/300\n",
      "Average training loss: 0.14158911234140395\n",
      "Average test loss: 0.00187520292116743\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12764637721909416\n",
      "Average test loss: 0.0018716788129467103\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11607821905612946\n",
      "Average test loss: 0.0018435468945859207\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1065945155620575\n",
      "Average test loss: 0.001844933858865665\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09894964924785826\n",
      "Average test loss: 0.001841236462092234\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09287046719259685\n",
      "Average test loss: 0.0019018443423426813\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08781368611256282\n",
      "Average test loss: 0.0018596124466922549\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0837050676147143\n",
      "Average test loss: 0.0018423849726095795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08027279753817453\n",
      "Average test loss: 0.0018198109606487884\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07754911108149423\n",
      "Average test loss: 0.0018694998793717889\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07511255264613363\n",
      "Average test loss: 0.001884288928988907\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07302348427640067\n",
      "Average test loss: 0.0018214067300885088\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0711716301076942\n",
      "Average test loss: 0.0018783502849853701\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06969193421138657\n",
      "Average test loss: 0.0018346757677694162\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06824073249763912\n",
      "Average test loss: 0.001824566418512\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06721582922008303\n",
      "Average test loss: 0.001825854616239667\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06612086617946625\n",
      "Average test loss: 0.00181153277018004\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06517554287778006\n",
      "Average test loss: 0.001853608456750711\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0643406040834056\n",
      "Average test loss: 0.001820104231747488\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06346287306480937\n",
      "Average test loss: 0.0017986294731704726\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06274399343132973\n",
      "Average test loss: 0.0018094806718743509\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06190440610051155\n",
      "Average test loss: 0.001778849769073228\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06131814479496744\n",
      "Average test loss: 0.001832042216633757\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06074115096529325\n",
      "Average test loss: 0.001807717586039669\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06006251368257735\n",
      "Average test loss: 0.001826006318339043\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05937253297368685\n",
      "Average test loss: 0.001856899960587422\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05878446005119218\n",
      "Average test loss: 0.0018068928489875463\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05804546042283376\n",
      "Average test loss: 0.001821531225513253\n",
      "Epoch 68/300\n",
      "Average training loss: 0.057722282297081415\n",
      "Average test loss: 0.00180008756607357\n",
      "Epoch 69/300\n",
      "Average training loss: 0.056966006600194505\n",
      "Average test loss: 0.001824237834351758\n",
      "Epoch 70/300\n",
      "Average training loss: 0.056358477003044555\n",
      "Average test loss: 0.0018137808821888433\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05596447045935525\n",
      "Average test loss: 0.0018404826108987132\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05534945347242885\n",
      "Average test loss: 0.001822059911572271\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05498724004626274\n",
      "Average test loss: 0.001984095114179783\n",
      "Epoch 74/300\n",
      "Average training loss: 0.054535527600182425\n",
      "Average test loss: 0.0018099737796518538\n",
      "Epoch 75/300\n",
      "Average training loss: 0.054035316255357534\n",
      "Average test loss: 0.0018030714589274592\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05379231379098363\n",
      "Average test loss: 0.001840683835455113\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05306771139303843\n",
      "Average test loss: 0.001836560140674313\n",
      "Epoch 78/300\n",
      "Average training loss: 0.052586696598264905\n",
      "Average test loss: 0.0018191793883840244\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05237655258178711\n",
      "Average test loss: 0.0018506156229931448\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05175938217176331\n",
      "Average test loss: 0.0018664877099088496\n",
      "Epoch 81/300\n",
      "Average training loss: 0.051576278815666836\n",
      "Average test loss: 0.0019102436407572693\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05137501669592327\n",
      "Average test loss: 0.0018284111448253195\n",
      "Epoch 83/300\n",
      "Average training loss: 0.050850714613993965\n",
      "Average test loss: 0.0018091489110762873\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05057246192296346\n",
      "Average test loss: 0.001859582508707212\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05008482772111893\n",
      "Average test loss: 0.0018441585601410933\n",
      "Epoch 86/300\n",
      "Average training loss: 0.049732975913418666\n",
      "Average test loss: 0.001846800949010584\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04943565395805571\n",
      "Average test loss: 0.0018474506737871303\n",
      "Epoch 88/300\n",
      "Average training loss: 0.048972543289264044\n",
      "Average test loss: 0.001790579011870755\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04865502067075835\n",
      "Average test loss: 0.0018332862945066558\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04855041300588184\n",
      "Average test loss: 0.001838055758840508\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0484265195114745\n",
      "Average test loss: 0.001884765521519714\n",
      "Epoch 92/300\n",
      "Average training loss: 0.048278139690558115\n",
      "Average test loss: 0.0018485805853787395\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04774889550440841\n",
      "Average test loss: 0.001845647358439035\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04740997300214238\n",
      "Average test loss: 0.0018998413798916672\n",
      "Epoch 95/300\n",
      "Average training loss: 0.047146718912654456\n",
      "Average test loss: 0.0018189376805805497\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0469234107500977\n",
      "Average test loss: 0.0018639500323269103\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04684181171324518\n",
      "Average test loss: 0.001868761645992183\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04675660192800893\n",
      "Average test loss: 0.0018951838285558753\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0463436701728238\n",
      "Average test loss: 0.0018661901404460272\n",
      "Epoch 100/300\n",
      "Average training loss: 0.046177943140268324\n",
      "Average test loss: 0.001876294282058047\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04595375646154086\n",
      "Average test loss: 0.001857102178865009\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04576158330506749\n",
      "Average test loss: 0.001895779649520086\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04551598279343711\n",
      "Average test loss: 0.001856509091746476\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04530378408895599\n",
      "Average test loss: 0.0018984798561367724\n",
      "Epoch 105/300\n",
      "Average training loss: 0.045130132344033985\n",
      "Average test loss: 0.001853839289293521\n",
      "Epoch 106/300\n",
      "Average training loss: 0.044969851467344495\n",
      "Average test loss: 0.0018770179350136055\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04470507931378152\n",
      "Average test loss: 0.0018649045580791102\n",
      "Epoch 108/300\n",
      "Average training loss: 0.044613811201519436\n",
      "Average test loss: 0.0018372806165781286\n",
      "Epoch 109/300\n",
      "Average training loss: 0.044427120341195\n",
      "Average test loss: 0.001974287105620735\n",
      "Epoch 110/300\n",
      "Average training loss: 0.044435977223846644\n",
      "Average test loss: 0.0018646893159796794\n",
      "Epoch 111/300\n",
      "Average training loss: 0.044171345459090336\n",
      "Average test loss: 0.0018420156928607159\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04395154262251324\n",
      "Average test loss: 0.0018425052961748506\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04392355876498752\n",
      "Average test loss: 0.0018645479585975408\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04359549069073465\n",
      "Average test loss: 0.0018946024803444743\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04338365821374787\n",
      "Average test loss: 0.0019062404706039363\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04363729688856337\n",
      "Average test loss: 0.001973372466655241\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04317380036248101\n",
      "Average test loss: 0.0019434213612952993\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04301199287176132\n",
      "Average test loss: 0.001856978346904119\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0429617316491074\n",
      "Average test loss: 0.0019163886265208324\n",
      "Epoch 120/300\n",
      "Average training loss: 0.042742510828706955\n",
      "Average test loss: 0.0018889578233162562\n",
      "Epoch 121/300\n",
      "Average training loss: 0.042725916723410286\n",
      "Average test loss: 0.0019226027187994785\n",
      "Epoch 122/300\n",
      "Average training loss: 0.042519914055864014\n",
      "Average test loss: 0.001884805137808952\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04233641267485089\n",
      "Average test loss: 0.0018648997010249232\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04226947288711866\n",
      "Average test loss: 0.001888635171422114\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04211900397141775\n",
      "Average test loss: 0.0018515415944986874\n",
      "Epoch 126/300\n",
      "Average training loss: 0.042054255089826055\n",
      "Average test loss: 0.002005520913956894\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04199706063502365\n",
      "Average test loss: 0.0019225484412163495\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04178648042347696\n",
      "Average test loss: 0.0019322697338130738\n",
      "Epoch 129/300\n",
      "Average training loss: 0.041817261836595\n",
      "Average test loss: 0.0019151696249221763\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04161697529256344\n",
      "Average test loss: 0.001965290043503046\n",
      "Epoch 131/300\n",
      "Average training loss: 0.041397851574752065\n",
      "Average test loss: 0.0019380394016496009\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04141393405530188\n",
      "Average test loss: 0.0019154011798608634\n",
      "Epoch 133/300\n",
      "Average training loss: 0.041170960191223356\n",
      "Average test loss: 0.0018706150848625434\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0411349501742257\n",
      "Average test loss: 0.0019299609357905057\n",
      "Epoch 135/300\n",
      "Average training loss: 0.041138339393668705\n",
      "Average test loss: 0.0019905124629537264\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04091177475121286\n",
      "Average test loss: 0.0019315522407285042\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0408470148311721\n",
      "Average test loss: 0.0019321317033221324\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04078453439805243\n",
      "Average test loss: 0.0019417486890322632\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04072743461860551\n",
      "Average test loss: 0.0019110063385839263\n",
      "Epoch 140/300\n",
      "Average training loss: 0.040881958352194894\n",
      "Average test loss: 0.0019027148613499272\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04059245817363262\n",
      "Average test loss: 0.0019114900953136386\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04041251487864388\n",
      "Average test loss: 0.001945144488890138\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04031745883325736\n",
      "Average test loss: 0.0019418164335915612\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0401961733367708\n",
      "Average test loss: 0.0019575344713197813\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04004690302908421\n",
      "Average test loss: 0.0019556893148045575\n",
      "Epoch 146/300\n",
      "Average training loss: 0.040028922014766266\n",
      "Average test loss: 0.0019595751797573434\n",
      "Epoch 147/300\n",
      "Average training loss: 0.040051230675644346\n",
      "Average test loss: 0.0019400142435398366\n",
      "Epoch 148/300\n",
      "Average training loss: 0.039992593967252306\n",
      "Average test loss: 0.00195495015124066\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03984685715370708\n",
      "Average test loss: 0.0019344189590257074\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03973171365923352\n",
      "Average test loss: 0.0019859647021318477\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04023548554380735\n",
      "Average test loss: 0.0019368260687010156\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03954833669132656\n",
      "Average test loss: 0.0019527210756722423\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03944000958734088\n",
      "Average test loss: 0.002000700849211878\n",
      "Epoch 154/300\n",
      "Average training loss: 0.039414661132627066\n",
      "Average test loss: 0.001889951393732594\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03943679082724783\n",
      "Average test loss: 0.0018854395003161497\n",
      "Epoch 156/300\n",
      "Average training loss: 0.039341732245352534\n",
      "Average test loss: 0.001968836943825914\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03936841422650549\n",
      "Average test loss: 0.001975371120083663\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03908346997698148\n",
      "Average test loss: 0.001984829060319397\n",
      "Epoch 159/300\n",
      "Average training loss: 0.039127036859591804\n",
      "Average test loss: 0.001933460793664886\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03896245167321629\n",
      "Average test loss: 0.001935222224642833\n",
      "Epoch 161/300\n",
      "Average training loss: 0.039086336609390045\n",
      "Average test loss: 0.0019087739559925266\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03883363849752479\n",
      "Average test loss: 0.0019790766038414506\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03900902294119199\n",
      "Average test loss: 0.0019421475352719426\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0387579051338964\n",
      "Average test loss: 0.0019436002371625769\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0385878836148315\n",
      "Average test loss: 0.0018953175993843211\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03867883378432857\n",
      "Average test loss: 0.0019260420438109173\n",
      "Epoch 167/300\n",
      "Average training loss: 0.038503259824381936\n",
      "Average test loss: 0.001931315411710077\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03852078215943443\n",
      "Average test loss: 0.0020895393192768095\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03842383941014608\n",
      "Average test loss: 0.0019282063206450806\n",
      "Epoch 170/300\n",
      "Average training loss: 0.038476120786534417\n",
      "Average test loss: 0.0020078140837657783\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03831272341807683\n",
      "Average test loss: 0.0019948240283152296\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0382477288974656\n",
      "Average test loss: 0.0019426851060448422\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03826576416525576\n",
      "Average test loss: 0.002011713764112857\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03813777180843883\n",
      "Average test loss: 0.00199447235196001\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03816776457760069\n",
      "Average test loss: 0.0019478630500121248\n",
      "Epoch 176/300\n",
      "Average training loss: 0.038071885456641516\n",
      "Average test loss: 0.001971837068700956\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03804039275315073\n",
      "Average test loss: 0.0021332983964433274\n",
      "Epoch 178/300\n",
      "Average training loss: 0.037863240141007636\n",
      "Average test loss: 0.001977394176233146\n",
      "Epoch 179/300\n",
      "Average training loss: 0.038036332905292514\n",
      "Average test loss: 0.0019855089206248524\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03781579953100946\n",
      "Average test loss: 0.0019652152307745485\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03778859126733409\n",
      "Average test loss: 0.001982744334679511\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03775698004166285\n",
      "Average test loss: 0.0019608103172439667\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03772560200757451\n",
      "Average test loss: 0.001994536679858963\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0376313083279464\n",
      "Average test loss: 0.0020470428507154185\n",
      "Epoch 185/300\n",
      "Average training loss: 0.037526508407460316\n",
      "Average test loss: 0.0027664096359577444\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03769260591599676\n",
      "Average test loss: 0.0019371026344597339\n",
      "Epoch 187/300\n",
      "Average training loss: 0.037347515333029956\n",
      "Average test loss: 0.001961700609160794\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03737900727325016\n",
      "Average test loss: 0.0020739766110976538\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03738630313343472\n",
      "Average test loss: 0.001994860637156914\n",
      "Epoch 190/300\n",
      "Average training loss: 0.037394053942627375\n",
      "Average test loss: 0.001981921323419859\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03730028766062524\n",
      "Average test loss: 0.0020319267525855038\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03715419553551409\n",
      "Average test loss: 0.0019818578598399957\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03707746133208275\n",
      "Average test loss: 0.00196459166933265\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03715037586622768\n",
      "Average test loss: 0.0019509664823611577\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03716276401943631\n",
      "Average test loss: 0.0019899415727704763\n",
      "Epoch 196/300\n",
      "Average training loss: 0.037117122189866174\n",
      "Average test loss: 0.001956434045607845\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03690550939987103\n",
      "Average test loss: 0.002086281820924746\n",
      "Epoch 198/300\n",
      "Average training loss: 0.036958258551028036\n",
      "Average test loss: 0.001991964427754283\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03694143646624353\n",
      "Average test loss: 0.00196857110886938\n",
      "Epoch 200/300\n",
      "Average training loss: 0.036881378991736304\n",
      "Average test loss: 0.002020783090239598\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03683031477365229\n",
      "Average test loss: 0.001949080455220408\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0367085347407394\n",
      "Average test loss: 0.001981214960002237\n",
      "Epoch 203/300\n",
      "Average training loss: 0.036741610447565715\n",
      "Average test loss: 0.0019643968463771873\n",
      "Epoch 204/300\n",
      "Average training loss: 0.036671422094106676\n",
      "Average test loss: 0.0019524920287852486\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03660546464721362\n",
      "Average test loss: 0.001985080850103663\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03661440670821402\n",
      "Average test loss: 0.0019723249558462864\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03660127040247122\n",
      "Average test loss: 0.0019867539565182396\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03646886575553152\n",
      "Average test loss: 0.001962366469204426\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03658880532781283\n",
      "Average test loss: 0.00202024284688135\n",
      "Epoch 210/300\n",
      "Average training loss: 0.036527388923698\n",
      "Average test loss: 0.002086561027293404\n",
      "Epoch 211/300\n",
      "Average training loss: 0.036397533097200924\n",
      "Average test loss: 0.0020017776852473616\n",
      "Epoch 212/300\n",
      "Average training loss: 0.036398703227440515\n",
      "Average test loss: 0.0019232541238889099\n",
      "Epoch 213/300\n",
      "Average training loss: 0.036309472521146136\n",
      "Average test loss: 0.001961779764956898\n",
      "Epoch 214/300\n",
      "Average training loss: 0.036285103331009545\n",
      "Average test loss: 0.0020313686220906676\n",
      "Epoch 215/300\n",
      "Average training loss: 0.036286378055810926\n",
      "Average test loss: 0.0019253870939008064\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03618087828821606\n",
      "Average test loss: 0.0019490398639399145\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03617652185757955\n",
      "Average test loss: 0.0020325851485961015\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03618243128226863\n",
      "Average test loss: 0.0022331353425979615\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03615992686483595\n",
      "Average test loss: 0.00202662042931964\n",
      "Epoch 220/300\n",
      "Average training loss: 0.035992034329308406\n",
      "Average test loss: 0.0019830097967965736\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03594600706299146\n",
      "Average test loss: 0.002003988441028115\n",
      "Epoch 222/300\n",
      "Average training loss: 0.035980350000990764\n",
      "Average test loss: 0.0019928237064224153\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03592414928144879\n",
      "Average test loss: 0.0019727760505759053\n",
      "Epoch 224/300\n",
      "Average training loss: 0.035987545591261653\n",
      "Average test loss: 0.0020068676742828555\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03590473920603593\n",
      "Average test loss: 0.0020096325965391266\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03597467239532206\n",
      "Average test loss: 0.0019722936012678675\n",
      "Epoch 227/300\n",
      "Average training loss: 0.035756542624698746\n",
      "Average test loss: 0.0019911003100375334\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03574385525286198\n",
      "Average test loss: 0.001985073766567641\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03571923776798778\n",
      "Average test loss: 0.002011067218457659\n",
      "Epoch 230/300\n",
      "Average training loss: 0.035654564071032735\n",
      "Average test loss: 0.001982860737066302\n",
      "Epoch 231/300\n",
      "Average training loss: 0.035672906383872036\n",
      "Average test loss: 0.0019443199844616983\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03570435106092029\n",
      "Average test loss: 0.001994939200787081\n",
      "Epoch 233/300\n",
      "Average training loss: 0.035656481951475145\n",
      "Average test loss: 0.0019887122423905466\n",
      "Epoch 234/300\n",
      "Average training loss: 0.035646283745765686\n",
      "Average test loss: 0.0020919651083855166\n",
      "Epoch 235/300\n",
      "Average training loss: 0.035559216140045063\n",
      "Average test loss: 0.001993422232982185\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03545344700747066\n",
      "Average test loss: 0.0019659910357246796\n",
      "Epoch 237/300\n",
      "Average training loss: 0.035449413124057985\n",
      "Average test loss: 0.002017497202795413\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03548204897178544\n",
      "Average test loss: 0.001995160201357471\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03543267447749774\n",
      "Average test loss: 0.0020153761760642133\n",
      "Epoch 240/300\n",
      "Average training loss: 0.035323645434445806\n",
      "Average test loss: 0.0019737943221504487\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03534441076550219\n",
      "Average test loss: 0.0019654003424156045\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03534828306072288\n",
      "Average test loss: 0.00199288721046307\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03527873659796185\n",
      "Average test loss: 0.001967503859558039\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03523368825183974\n",
      "Average test loss: 0.0020121587589900527\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03516426864597533\n",
      "Average test loss: 0.00200830503884289\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03524295055203967\n",
      "Average test loss: 0.002014968506474462\n",
      "Epoch 247/300\n",
      "Average training loss: 0.035161075433095296\n",
      "Average test loss: 0.002004605691259106\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03511964546640714\n",
      "Average test loss: 0.0020299623523735337\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03510237020088567\n",
      "Average test loss: 0.0020157708181585705\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03507755968968074\n",
      "Average test loss: 0.002028644551316069\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0350376424756315\n",
      "Average test loss: 0.002017006380793949\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03505749868684345\n",
      "Average test loss: 0.0020346431362753114\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03497861996624205\n",
      "Average test loss: 0.002020083398454719\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0349563818871975\n",
      "Average test loss: 0.0021280943517469697\n",
      "Epoch 255/300\n",
      "Average training loss: 0.034951207823223536\n",
      "Average test loss: 0.002024870797888272\n",
      "Epoch 256/300\n",
      "Average training loss: 0.034951776795917086\n",
      "Average test loss: 0.0021307603137360677\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03489402291840977\n",
      "Average test loss: 0.00205713366655012\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03484900810983446\n",
      "Average test loss: 0.002077460975696643\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03475276771518919\n",
      "Average test loss: 0.002009710691144897\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03475929537415504\n",
      "Average test loss: 0.001969830342464977\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03472511170638932\n",
      "Average test loss: 0.00206274237173299\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03483926333321465\n",
      "Average test loss: 0.002000551462173462\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03468567626343833\n",
      "Average test loss: 0.0020366905884196362\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0347091972331206\n",
      "Average test loss: 0.0020291395568185384\n",
      "Epoch 265/300\n",
      "Average training loss: 0.034622493472364214\n",
      "Average test loss: 0.0020376514576168525\n",
      "Epoch 266/300\n",
      "Average training loss: 0.034572475088967214\n",
      "Average test loss: 0.001984695764258504\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034588077144490345\n",
      "Average test loss: 0.002081067497945494\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03461208572818173\n",
      "Average test loss: 0.002204761312653621\n",
      "Epoch 269/300\n",
      "Average training loss: 0.034606815169254936\n",
      "Average test loss: 0.002011153161008325\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03448343093693256\n",
      "Average test loss: 0.002069677672245436\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03449104292194049\n",
      "Average test loss: 0.0020663594890178907\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03446301913261413\n",
      "Average test loss: 0.00198321250681248\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03446323850088649\n",
      "Average test loss: 0.0020366987145195403\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03444187749094433\n",
      "Average test loss: 0.0020363114598310654\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03438897114661005\n",
      "Average test loss: 0.001996370449869169\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03438550503883097\n",
      "Average test loss: 0.0020712825923951137\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03425375610258844\n",
      "Average test loss: 0.002003753535863426\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03426588088770707\n",
      "Average test loss: 0.0020973099635707006\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03422705514232317\n",
      "Average test loss: 0.0020251350230019952\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03430854159924719\n",
      "Average test loss: 0.0020560526309741866\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03427059818307559\n",
      "Average test loss: 0.0019976946101006535\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03427429261141353\n",
      "Average test loss: 0.0020880687321639725\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03410780795746379\n",
      "Average test loss: 0.002038054750818345\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03416298500696818\n",
      "Average test loss: 0.002030695451837447\n",
      "Epoch 285/300\n",
      "Average training loss: 0.034221511498093604\n",
      "Average test loss: 0.002049037938627104\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03412965153654417\n",
      "Average test loss: 0.001999704688321799\n",
      "Epoch 287/300\n",
      "Average training loss: 0.034049622466166815\n",
      "Average test loss: 0.001991630736634963\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03409179012808535\n",
      "Average test loss: 0.0020644933702424167\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0340596110953225\n",
      "Average test loss: 0.0020137094038849075\n",
      "Epoch 290/300\n",
      "Average training loss: 0.034077866239680184\n",
      "Average test loss: 0.0020937120620575216\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03397935408850511\n",
      "Average test loss: 0.002059469727281895\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03404316619369719\n",
      "Average test loss: 0.002076916134812766\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0338995230793953\n",
      "Average test loss: 0.0020994745226990846\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03390581073032485\n",
      "Average test loss: 0.002055025655983223\n",
      "Epoch 295/300\n",
      "Average training loss: 0.033977582352028954\n",
      "Average test loss: 0.0020350574616135823\n",
      "Epoch 296/300\n",
      "Average training loss: 0.033967977984084026\n",
      "Average test loss: 0.0020328832269749706\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03387449140018887\n",
      "Average test loss: 0.0020612348055260047\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03382733031279511\n",
      "Average test loss: 0.0020107744611385795\n",
      "Epoch 299/300\n",
      "Average training loss: 0.033809680175450114\n",
      "Average test loss: 0.0020766704701301124\n",
      "Epoch 300/300\n",
      "Average training loss: 0.033858332402176325\n",
      "Average test loss: 0.0020705313512848485\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 22.19443303595649\n",
      "Average test loss: 358.69590937886306\n",
      "Epoch 2/300\n",
      "Average training loss: 14.832212716844348\n",
      "Average test loss: 0.0036533511432094708\n",
      "Epoch 3/300\n",
      "Average training loss: 10.096302274068197\n",
      "Average test loss: 3.1588093242746673\n",
      "Epoch 4/300\n",
      "Average training loss: 8.234197238074408\n",
      "Average test loss: 52.617742321401835\n",
      "Epoch 5/300\n",
      "Average training loss: 7.010565141889784\n",
      "Average test loss: 0.21605028012394906\n",
      "Epoch 6/300\n",
      "Average training loss: 6.011607086605496\n",
      "Average test loss: 574.6006491156684\n",
      "Epoch 7/300\n",
      "Average training loss: 6.113007971021864\n",
      "Average test loss: 0.007815002513428529\n",
      "Epoch 8/300\n",
      "Average training loss: 5.236927502102322\n",
      "Average test loss: 0.002290155845383803\n",
      "Epoch 9/300\n",
      "Average training loss: 4.825495282067193\n",
      "Average test loss: 4.832510662888487\n",
      "Epoch 10/300\n",
      "Average training loss: 4.884620736440023\n",
      "Average test loss: 0.0022195857525285747\n",
      "Epoch 11/300\n",
      "Average training loss: 3.5719970241122776\n",
      "Average test loss: 0.023329467402771115\n",
      "Epoch 12/300\n",
      "Average training loss: 2.942615963406033\n",
      "Average test loss: 0.6263714173947358\n",
      "Epoch 13/300\n",
      "Average training loss: 2.6441291222042507\n",
      "Average test loss: 2.18103126325955\n",
      "Epoch 14/300\n",
      "Average training loss: 2.400901626586914\n",
      "Average test loss: 1.427657646564146\n",
      "Epoch 15/300\n",
      "Average training loss: 2.197930198881361\n",
      "Average test loss: 0.013211095027108159\n",
      "Epoch 16/300\n",
      "Average training loss: 2.06574434015486\n",
      "Average test loss: 0.002055119311850932\n",
      "Epoch 17/300\n",
      "Average training loss: 1.833514900525411\n",
      "Average test loss: 0.8866743912481599\n",
      "Epoch 18/300\n",
      "Average training loss: 1.6236893770429823\n",
      "Average test loss: 0.0031430339581953984\n",
      "Epoch 19/300\n",
      "Average training loss: 1.447651275952657\n",
      "Average test loss: 0.018017803347979984\n",
      "Epoch 20/300\n",
      "Average training loss: 1.3074978261523778\n",
      "Average test loss: 0.0035858671736593048\n",
      "Epoch 21/300\n",
      "Average training loss: 1.1908996402952405\n",
      "Average test loss: 0.014700865760445594\n",
      "Epoch 22/300\n",
      "Average training loss: 1.074554869863722\n",
      "Average test loss: 0.002368610954584761\n",
      "Epoch 23/300\n",
      "Average training loss: 0.9569180218378702\n",
      "Average test loss: 0.0015721437202559578\n",
      "Epoch 24/300\n",
      "Average training loss: 0.8517002283202277\n",
      "Average test loss: 0.001780240439085497\n",
      "Epoch 25/300\n",
      "Average training loss: 0.7567882563802931\n",
      "Average test loss: 0.0030306563013129764\n",
      "Epoch 26/300\n",
      "Average training loss: 0.6684732780986362\n",
      "Average test loss: 0.0014539014117585288\n",
      "Epoch 27/300\n",
      "Average training loss: 0.5872283167839051\n",
      "Average test loss: 0.0015030086699148848\n",
      "Epoch 28/300\n",
      "Average training loss: 0.5164639534420438\n",
      "Average test loss: 0.0014361553721957737\n",
      "Epoch 29/300\n",
      "Average training loss: 0.4521714632246229\n",
      "Average test loss: 0.0014558598519199425\n",
      "Epoch 30/300\n",
      "Average training loss: 0.39437129566404555\n",
      "Average test loss: 0.003789647390238113\n",
      "Epoch 31/300\n",
      "Average training loss: 0.3421803514427609\n",
      "Average test loss: 0.0017289861761447456\n",
      "Epoch 32/300\n",
      "Average training loss: 0.29795738026830887\n",
      "Average test loss: 0.0013839448973950413\n",
      "Epoch 33/300\n",
      "Average training loss: 0.25986152419779035\n",
      "Average test loss: 0.001372004700307217\n",
      "Epoch 34/300\n",
      "Average training loss: 0.22609867236349318\n",
      "Average test loss: 0.0013541715558514826\n",
      "Epoch 35/300\n",
      "Average training loss: 0.19752268201775022\n",
      "Average test loss: 0.0013412432302203443\n",
      "Epoch 36/300\n",
      "Average training loss: 0.17267461986011928\n",
      "Average test loss: 0.0013405513283279208\n",
      "Epoch 37/300\n",
      "Average training loss: 0.15153659228483835\n",
      "Average test loss: 0.0013269558149493403\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1336036724117067\n",
      "Average test loss: 0.001311615541577339\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11823465545972188\n",
      "Average test loss: 0.001323612901278668\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1052642517818345\n",
      "Average test loss: 0.0013078359131080408\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09458856659465366\n",
      "Average test loss: 0.0012880588554673724\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08605663618776534\n",
      "Average test loss: 0.0013142013731929992\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07887952284680473\n",
      "Average test loss: 0.0013020867565646768\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07315534308552742\n",
      "Average test loss: 0.0013293884366543756\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0686171221865548\n",
      "Average test loss: 0.0012831578446138236\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06483168345689773\n",
      "Average test loss: 0.0013205235180341534\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0616584604912334\n",
      "Average test loss: 0.001288257374195382\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05910889240437084\n",
      "Average test loss: 0.0013230324167137345\n",
      "Epoch 49/300\n",
      "Average training loss: 0.057138682627015644\n",
      "Average test loss: 0.0013009794997051358\n",
      "Epoch 50/300\n",
      "Average training loss: 0.055158445758952034\n",
      "Average test loss: 0.0012565041007991467\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05382937235964669\n",
      "Average test loss: 0.0012702017271270355\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05235606267386013\n",
      "Average test loss: 0.001289183053601947\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05125205886363983\n",
      "Average test loss: 0.001279425321922948\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05013076169954406\n",
      "Average test loss: 0.0012680278707088695\n",
      "Epoch 55/300\n",
      "Average training loss: 0.049304117225938374\n",
      "Average test loss: 0.0012444376640228761\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0485463566382726\n",
      "Average test loss: 0.0012478230044556161\n",
      "Epoch 57/300\n",
      "Average training loss: 0.047780314438872866\n",
      "Average test loss: 0.0012461112032954892\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04723217429551813\n",
      "Average test loss: 0.0012667104506658184\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04636769939793481\n",
      "Average test loss: 0.0012554050673627192\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04568388808435864\n",
      "Average test loss: 0.001273651066287938\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04501713519626194\n",
      "Average test loss: 0.001251731913536787\n",
      "Epoch 62/300\n",
      "Average training loss: 0.044442146566179064\n",
      "Average test loss: 0.0012711691655632523\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0438273013929526\n",
      "Average test loss: 0.0012423326113364763\n",
      "Epoch 64/300\n",
      "Average training loss: 0.043303850216997994\n",
      "Average test loss: 0.0012373722780806323\n",
      "Epoch 65/300\n",
      "Average training loss: 0.042703276263342964\n",
      "Average test loss: 0.0012493031243793666\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04231017951832877\n",
      "Average test loss: 0.0012495915051549673\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04190001428789562\n",
      "Average test loss: 0.0012640990576603347\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04105338208211793\n",
      "Average test loss: 0.0012624853609336747\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04067910117904345\n",
      "Average test loss: 0.001248926367196772\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04029008297291067\n",
      "Average test loss: 0.0013021195721812547\n",
      "Epoch 71/300\n",
      "Average training loss: 0.040103552672598096\n",
      "Average test loss: 0.001276505713402811\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03949072704381413\n",
      "Average test loss: 0.0012427494156484803\n",
      "Epoch 73/300\n",
      "Average training loss: 0.038846183266904616\n",
      "Average test loss: 0.0012736285242459012\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03846234542628129\n",
      "Average test loss: 0.0012460252422218522\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03807611724734306\n",
      "Average test loss: 0.0012442911053076387\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03793370704021719\n",
      "Average test loss: 0.0012338905533154805\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03792434085574415\n",
      "Average test loss: 0.0013111436350478067\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03713070575065083\n",
      "Average test loss: 0.0012922479226771329\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03672096182074812\n",
      "Average test loss: 0.0012965579768642783\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03628299823568927\n",
      "Average test loss: 0.0013327479899550479\n",
      "Epoch 81/300\n",
      "Average training loss: 0.036175507651435006\n",
      "Average test loss: 0.0013383227576398188\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03578064961234728\n",
      "Average test loss: 0.0012326752281644278\n",
      "Epoch 83/300\n",
      "Average training loss: 0.035419219768709605\n",
      "Average test loss: 0.0012707193410024047\n",
      "Epoch 84/300\n",
      "Average training loss: 0.035172520026564597\n",
      "Average test loss: 0.0012845507556986477\n",
      "Epoch 85/300\n",
      "Average training loss: 0.034919417770372496\n",
      "Average test loss: 0.0012820233136622442\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03465507442586952\n",
      "Average test loss: 0.0013027843441814184\n",
      "Epoch 87/300\n",
      "Average training loss: 0.034462515181965296\n",
      "Average test loss: 0.001287442922902604\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03422704902291298\n",
      "Average test loss: 0.001276365975331929\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03397616507278548\n",
      "Average test loss: 0.0013646686482760642\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03365709206296338\n",
      "Average test loss: 0.0013008125471986002\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03365068258676264\n",
      "Average test loss: 0.0013056705429011748\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03331776159008344\n",
      "Average test loss: 0.0012593177685307133\n",
      "Epoch 93/300\n",
      "Average training loss: 0.032962849262687895\n",
      "Average test loss: 0.0012756470295911034\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03281916624307633\n",
      "Average test loss: 0.0013318625050079492\n",
      "Epoch 95/300\n",
      "Average training loss: 0.032669081759121685\n",
      "Average test loss: 0.0013031357819628384\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03246794525947836\n",
      "Average test loss: 0.0012801730608981517\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03217760389712122\n",
      "Average test loss: 0.0012887345898068613\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03210886801282565\n",
      "Average test loss: 0.0012795952363974519\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03193161020345158\n",
      "Average test loss: 0.0012589524605621893\n",
      "Epoch 100/300\n",
      "Average training loss: 0.031817581529418625\n",
      "Average test loss: 0.0013220831518992783\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03164414879017406\n",
      "Average test loss: 0.001345244627032015\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03150828573273288\n",
      "Average test loss: 0.0012916377578965491\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03144917036261823\n",
      "Average test loss: 0.0013170692661466697\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03106319118373924\n",
      "Average test loss: 0.0013608392272144556\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030991487214962643\n",
      "Average test loss: 0.0013027943141965402\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03080927231742276\n",
      "Average test loss: 0.0013293124055489898\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03063268509010474\n",
      "Average test loss: 0.0012959966087299915\n",
      "Epoch 108/300\n",
      "Average training loss: 0.030569386318325998\n",
      "Average test loss: 0.001297031221186949\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03044567662311925\n",
      "Average test loss: 0.0013186563375509447\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0303325334439675\n",
      "Average test loss: 0.0013202534557009737\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030090029055873553\n",
      "Average test loss: 0.00132484435579843\n",
      "Epoch 112/300\n",
      "Average training loss: 0.030007231099738015\n",
      "Average test loss: 0.0013280698712915182\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02992118473433786\n",
      "Average test loss: 0.0013513935060343808\n",
      "Epoch 114/300\n",
      "Average training loss: 0.029853340215153165\n",
      "Average test loss: 0.0012894172833508087\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02975750377939807\n",
      "Average test loss: 0.0013319128229179317\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02948683010455635\n",
      "Average test loss: 0.0013746163209693298\n",
      "Epoch 117/300\n",
      "Average training loss: 0.029451326409147844\n",
      "Average test loss: 0.0013330581040742497\n",
      "Epoch 118/300\n",
      "Average training loss: 0.029397421086827913\n",
      "Average test loss: 0.0013194175631635718\n",
      "Epoch 119/300\n",
      "Average training loss: 0.029204841478003396\n",
      "Average test loss: 0.0013078493347598447\n",
      "Epoch 120/300\n",
      "Average training loss: 0.029180642478995853\n",
      "Average test loss: 0.0013534253671144446\n",
      "Epoch 121/300\n",
      "Average training loss: 0.029213525970776876\n",
      "Average test loss: 0.001320522437699967\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02897245386408435\n",
      "Average test loss: 0.0013275908171716663\n",
      "Epoch 123/300\n",
      "Average training loss: 0.028860709938738082\n",
      "Average test loss: 0.0013030015082202025\n",
      "Epoch 124/300\n",
      "Average training loss: 0.028804855520526567\n",
      "Average test loss: 0.0013117761756293476\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028734270034564868\n",
      "Average test loss: 0.00130140314809978\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02860015957388613\n",
      "Average test loss: 0.0013513494672046768\n",
      "Epoch 127/300\n",
      "Average training loss: 0.028608522499601045\n",
      "Average test loss: 0.0013234204820150302\n",
      "Epoch 128/300\n",
      "Average training loss: 0.028334774262375302\n",
      "Average test loss: 0.0013484249841421842\n",
      "Epoch 129/300\n",
      "Average training loss: 0.028341478852762118\n",
      "Average test loss: 0.0013513712958536214\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028277709189388486\n",
      "Average test loss: 0.001341993925989502\n",
      "Epoch 131/300\n",
      "Average training loss: 0.028174263945884175\n",
      "Average test loss: 0.0013402006258256733\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02806246473060714\n",
      "Average test loss: 0.0013971560441164506\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028082868016428418\n",
      "Average test loss: 0.001317660383776658\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02798862934112549\n",
      "Average test loss: 0.0013639110947648685\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0278421497527096\n",
      "Average test loss: 0.0013994890074763033\n",
      "Epoch 136/300\n",
      "Average training loss: 0.027759874944885573\n",
      "Average test loss: 0.001374366683471534\n",
      "Epoch 137/300\n",
      "Average training loss: 0.027689648565318848\n",
      "Average test loss: 0.001357651997771528\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02757444204721186\n",
      "Average test loss: 0.0013516076441026396\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02758508936729696\n",
      "Average test loss: 0.0013622628168927298\n",
      "Epoch 140/300\n",
      "Average training loss: 0.027606986946529813\n",
      "Average test loss: 0.001365905294660479\n",
      "Epoch 141/300\n",
      "Average training loss: 0.027424013033509255\n",
      "Average test loss: 0.0013413532026526\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027319186550047662\n",
      "Average test loss: 0.0013455311542169915\n",
      "Epoch 143/300\n",
      "Average training loss: 0.027359985656208462\n",
      "Average test loss: 0.0013497550405251483\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02737095982167456\n",
      "Average test loss: 0.0013325662988031076\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02715011529624462\n",
      "Average test loss: 0.0013835772127947874\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02714596848686536\n",
      "Average test loss: 0.0013603098606690765\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02700520783993933\n",
      "Average test loss: 0.001357554117942022\n",
      "Epoch 148/300\n",
      "Average training loss: 0.027013523360093435\n",
      "Average test loss: 0.0013567525901728207\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026946903988718986\n",
      "Average test loss: 0.0013636984722688794\n",
      "Epoch 150/300\n",
      "Average training loss: 0.026995990397201644\n",
      "Average test loss: 0.001324338293944796\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02675215617650085\n",
      "Average test loss: 0.0013805294243308406\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02674766938885053\n",
      "Average test loss: 0.0013775070783578686\n",
      "Epoch 153/300\n",
      "Average training loss: 0.026752176793085204\n",
      "Average test loss: 0.0013674068637192249\n",
      "Epoch 154/300\n",
      "Average training loss: 0.026713658610979716\n",
      "Average test loss: 0.0013455669651221897\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02666376827326086\n",
      "Average test loss: 0.0014047537081771427\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02660074860188696\n",
      "Average test loss: 0.0013432392600095935\n",
      "Epoch 157/300\n",
      "Average training loss: 0.026510395548409886\n",
      "Average test loss: 0.0013797220802969402\n",
      "Epoch 158/300\n",
      "Average training loss: 0.026440775563319523\n",
      "Average test loss: 0.0014018724064549638\n",
      "Epoch 159/300\n",
      "Average training loss: 0.026422806883851686\n",
      "Average test loss: 0.0014790563941415814\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02638974648548497\n",
      "Average test loss: 0.0013685908826171524\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0264018078578843\n",
      "Average test loss: 0.0013642556653875443\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02631692686345842\n",
      "Average test loss: 0.0013724226378318337\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0261611710406012\n",
      "Average test loss: 0.001370440561634799\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026211600561936697\n",
      "Average test loss: 0.0013572334341911806\n",
      "Epoch 165/300\n",
      "Average training loss: 0.026132303171687656\n",
      "Average test loss: 0.0013785585591362581\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026066414102084108\n",
      "Average test loss: 0.0013684190376144316\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0260772900995281\n",
      "Average test loss: 0.0014260283943472638\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026025142474306956\n",
      "Average test loss: 0.0013885631980374456\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026011222976777288\n",
      "Average test loss: 0.0014196897150638202\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0259466140502029\n",
      "Average test loss: 0.001385486077931192\n",
      "Epoch 171/300\n",
      "Average training loss: 0.025832581624388694\n",
      "Average test loss: 0.0013910985871528586\n",
      "Epoch 172/300\n",
      "Average training loss: 0.025807827653156385\n",
      "Average test loss: 0.0013943173902937107\n",
      "Epoch 173/300\n",
      "Average training loss: 0.025795749493771128\n",
      "Average test loss: 0.0013448758301221663\n",
      "Epoch 174/300\n",
      "Average training loss: 0.025762195428212484\n",
      "Average test loss: 0.0014013942994901703\n",
      "Epoch 175/300\n",
      "Average training loss: 0.025704647045996454\n",
      "Average test loss: 0.001368909936501748\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02559839525156551\n",
      "Average test loss: 0.0013507679593231943\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02559312253528171\n",
      "Average test loss: 0.0013627273554189339\n",
      "Epoch 178/300\n",
      "Average training loss: 0.025804947237173716\n",
      "Average test loss: 0.0013976946465878023\n",
      "Epoch 179/300\n",
      "Average training loss: 0.025634881815976566\n",
      "Average test loss: 0.001389462615052859\n",
      "Epoch 180/300\n",
      "Average training loss: 0.025432642373773788\n",
      "Average test loss: 0.0013905775425955653\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02550993030104372\n",
      "Average test loss: 0.0013423332805331382\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02546495351029767\n",
      "Average test loss: 0.0013747018815742599\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02545989935265647\n",
      "Average test loss: 0.0014003865090716217\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02542291521694925\n",
      "Average test loss: 0.0014326978196493453\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0253184412419796\n",
      "Average test loss: 0.001348458221906589\n",
      "Epoch 186/300\n",
      "Average training loss: 0.025340952393081453\n",
      "Average test loss: 0.001408184574296077\n",
      "Epoch 187/300\n",
      "Average training loss: 0.025310349757472674\n",
      "Average test loss: 0.0013589441914421817\n",
      "Epoch 188/300\n",
      "Average training loss: 0.025190747189852928\n",
      "Average test loss: 0.0014383340377567542\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0252510433892409\n",
      "Average test loss: 0.0014027457871577805\n",
      "Epoch 190/300\n",
      "Average training loss: 0.025175048611230322\n",
      "Average test loss: 0.001386635064561334\n",
      "Epoch 191/300\n",
      "Average training loss: 0.025121297326352862\n",
      "Average test loss: 0.0014051634731392065\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02512989094853401\n",
      "Average test loss: 0.0014267665939405561\n",
      "Epoch 193/300\n",
      "Average training loss: 0.025113527137372228\n",
      "Average test loss: 0.0013549272374560435\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02497949042585161\n",
      "Average test loss: 0.0013901134993260106\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02493129376239247\n",
      "Average test loss: 0.0014040677489505875\n",
      "Epoch 196/300\n",
      "Average training loss: 0.024969099033210013\n",
      "Average test loss: 0.0014016818238629235\n",
      "Epoch 197/300\n",
      "Average training loss: 0.024975759961538845\n",
      "Average test loss: 0.001374147576176458\n",
      "Epoch 198/300\n",
      "Average training loss: 0.024880771314104398\n",
      "Average test loss: 0.0014298938577994704\n",
      "Epoch 199/300\n",
      "Average training loss: 0.024857458100550706\n",
      "Average test loss: 0.0014395312916280496\n",
      "Epoch 200/300\n",
      "Average training loss: 0.024907615196373727\n",
      "Average test loss: 0.001832894544220633\n",
      "Epoch 201/300\n",
      "Average training loss: 0.024862204632825322\n",
      "Average test loss: 0.0014080621911626724\n",
      "Epoch 202/300\n",
      "Average training loss: 0.024809109825226997\n",
      "Average test loss: 0.0014010972674522136\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02477131316396925\n",
      "Average test loss: 0.001393574842562278\n",
      "Epoch 204/300\n",
      "Average training loss: 0.024702632326218818\n",
      "Average test loss: 0.0014624828591735827\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02466596705218156\n",
      "Average test loss: 0.0013762924098927114\n",
      "Epoch 206/300\n",
      "Average training loss: 0.024731062975194718\n",
      "Average test loss: 0.001400506740798139\n",
      "Epoch 207/300\n",
      "Average training loss: 0.024590470562378567\n",
      "Average test loss: 0.001441577863569061\n",
      "Epoch 208/300\n",
      "Average training loss: 0.024619202070766025\n",
      "Average test loss: 0.001389194303502639\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02460360804862446\n",
      "Average test loss: 0.0014051786333115564\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02464198419948419\n",
      "Average test loss: 0.0013912799514623152\n",
      "Epoch 211/300\n",
      "Average training loss: 0.024513907288511595\n",
      "Average test loss: 0.0013900816739122902\n",
      "Epoch 212/300\n",
      "Average training loss: 0.024536001905798913\n",
      "Average test loss: 0.0013994291190885836\n",
      "Epoch 213/300\n",
      "Average training loss: 0.024534557812743718\n",
      "Average test loss: 0.002931278470282753\n",
      "Epoch 214/300\n",
      "Average training loss: 0.024481730884975857\n",
      "Average test loss: 0.001385103616760009\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02438843921489186\n",
      "Average test loss: 0.0014093217723485495\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02442407284345892\n",
      "Average test loss: 0.0014140556550377773\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02438401693602403\n",
      "Average test loss: 0.0014265173359049692\n",
      "Epoch 218/300\n",
      "Average training loss: 0.024372331496742037\n",
      "Average test loss: 0.0013855165517888963\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02428905288543966\n",
      "Average test loss: 0.0013619386814534664\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02437027274734444\n",
      "Average test loss: 0.0014014957415767841\n",
      "Epoch 221/300\n",
      "Average training loss: 0.024232449010014534\n",
      "Average test loss: 0.0014023721909357442\n",
      "Epoch 222/300\n",
      "Average training loss: 0.024171871572732925\n",
      "Average test loss: 0.0013924918455175228\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02418850030667252\n",
      "Average test loss: 0.0014973082343737285\n",
      "Epoch 224/300\n",
      "Average training loss: 0.024131839892930454\n",
      "Average test loss: 0.0014407313897584876\n",
      "Epoch 225/300\n",
      "Average training loss: 0.024196400721867878\n",
      "Average test loss: 0.0014130170794410838\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0242206305977371\n",
      "Average test loss: 0.0013832587396932973\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02411256316304207\n",
      "Average test loss: 0.0014519775798544287\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02411786015994019\n",
      "Average test loss: 0.0014099408274309503\n",
      "Epoch 229/300\n",
      "Average training loss: 0.024110898794399368\n",
      "Average test loss: 0.001384936068000065\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02407872792912854\n",
      "Average test loss: 0.0014293518683148754\n",
      "Epoch 231/300\n",
      "Average training loss: 0.024052476482258903\n",
      "Average test loss: 0.0013789462188465728\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024011370258198845\n",
      "Average test loss: 0.0014568760582866769\n",
      "Epoch 233/300\n",
      "Average training loss: 0.023961752398146522\n",
      "Average test loss: 0.00140899683524751\n",
      "Epoch 234/300\n",
      "Average training loss: 0.024063934697873064\n",
      "Average test loss: 0.001484233864169154\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023928908434179094\n",
      "Average test loss: 0.0014095143485917813\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02398888279000918\n",
      "Average test loss: 0.0013990769911971356\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023900617776645556\n",
      "Average test loss: 0.0014542571542681092\n",
      "Epoch 238/300\n",
      "Average training loss: 0.023835829267899195\n",
      "Average test loss: 0.0014359958805143832\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023831856664684084\n",
      "Average test loss: 0.0014504188265547983\n",
      "Epoch 240/300\n",
      "Average training loss: 0.023850709680053924\n",
      "Average test loss: 0.0013643810208369461\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02380714369316896\n",
      "Average test loss: 0.0014293624799077709\n",
      "Epoch 242/300\n",
      "Average training loss: 0.023828964695334433\n",
      "Average test loss: 0.001405149459373206\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023753352327479257\n",
      "Average test loss: 0.0014268165927173364\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023854473057720395\n",
      "Average test loss: 0.001404246849525306\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02393682185643249\n",
      "Average test loss: 0.001442584088175661\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02370804403556718\n",
      "Average test loss: 0.0013973147208905883\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02370749907526705\n",
      "Average test loss: 0.0013986263998473684\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023625947083036104\n",
      "Average test loss: 0.0014284631733058226\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023567836276359028\n",
      "Average test loss: 0.0014201740920543671\n",
      "Epoch 250/300\n",
      "Average training loss: 0.023609965400563346\n",
      "Average test loss: 0.0014309339401208693\n",
      "Epoch 251/300\n",
      "Average training loss: 0.023631075715025266\n",
      "Average test loss: 0.0014124975845010745\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023660002218352422\n",
      "Average test loss: 0.0014442847492173315\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023542207626832855\n",
      "Average test loss: 0.0014374609084592926\n",
      "Epoch 254/300\n",
      "Average training loss: 0.023561901539564133\n",
      "Average test loss: 0.0013914190083742142\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0234802146040731\n",
      "Average test loss: 0.0013887931301982865\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023566817257139418\n",
      "Average test loss: 0.001415374289887647\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023489457291033532\n",
      "Average test loss: 0.0014303741487157013\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02349015961918566\n",
      "Average test loss: 0.001373592769106229\n",
      "Epoch 259/300\n",
      "Average training loss: 0.023560876845485634\n",
      "Average test loss: 0.0014776081424206495\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023398105776972242\n",
      "Average test loss: 0.001438308505858812\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02344589167005486\n",
      "Average test loss: 0.001428484587930143\n",
      "Epoch 262/300\n",
      "Average training loss: 0.023442874145176675\n",
      "Average test loss: 0.0014499036262018814\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023425487936370903\n",
      "Average test loss: 0.001443151487606681\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023335568567117056\n",
      "Average test loss: 0.0014508096483639545\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023348470686210526\n",
      "Average test loss: 0.0014008136249871717\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023392609390947554\n",
      "Average test loss: 0.001423759496356878\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023243534437484212\n",
      "Average test loss: 0.0014668863165295787\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02331455930074056\n",
      "Average test loss: 0.0014327458328463965\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02334317876895269\n",
      "Average test loss: 0.001447516213171184\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02328184054295222\n",
      "Average test loss: 0.0014920500536552733\n",
      "Epoch 271/300\n",
      "Average training loss: 0.023203973352909087\n",
      "Average test loss: 0.0014590763929817412\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02321089424358474\n",
      "Average test loss: 0.0013871343643921945\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02317768727408515\n",
      "Average test loss: 0.001443141749025219\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023166175623734794\n",
      "Average test loss: 0.0013914472977113392\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023206006194154423\n",
      "Average test loss: 0.0014157292925649218\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02313520192934407\n",
      "Average test loss: 0.0014078973419446913\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023227998425563178\n",
      "Average test loss: 0.0014410861427895725\n",
      "Epoch 278/300\n",
      "Average training loss: 0.023108117459548844\n",
      "Average test loss: 0.00141543156415638\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023097195885247654\n",
      "Average test loss: 0.001435948874697917\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02304245708220535\n",
      "Average test loss: 0.0014562250984211762\n",
      "Epoch 281/300\n",
      "Average training loss: 0.023045991251866023\n",
      "Average test loss: 0.0014290555502391525\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023066171007023916\n",
      "Average test loss: 0.0014656669394009644\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02309933217200968\n",
      "Average test loss: 0.0014948357105669047\n",
      "Epoch 284/300\n",
      "Average training loss: 0.023044660818245674\n",
      "Average test loss: 0.001406656339764595\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02302541835606098\n",
      "Average test loss: 0.0014387257765564653\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02297719992366102\n",
      "Average test loss: 0.0013876166325062513\n",
      "Epoch 287/300\n",
      "Average training loss: 0.023017898961901664\n",
      "Average test loss: 0.0014424636018358999\n",
      "Epoch 288/300\n",
      "Average training loss: 0.022939390238788392\n",
      "Average test loss: 0.0014169338409685426\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022902366085184946\n",
      "Average test loss: 0.0014518102255339424\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022990008587638538\n",
      "Average test loss: 0.0013972492364959584\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022910403963592316\n",
      "Average test loss: 0.0014234424634681393\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02297541874812709\n",
      "Average test loss: 0.0014067166782915591\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02287986949334542\n",
      "Average test loss: 0.00143788230439855\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022898942743738493\n",
      "Average test loss: 0.0014005503241593638\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022842083621356223\n",
      "Average test loss: 0.0014179652963971926\n",
      "Epoch 296/300\n",
      "Average training loss: 0.022865778803825378\n",
      "Average test loss: 0.0014970933558005426\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02280948586265246\n",
      "Average test loss: 0.0014277906174668007\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022743335232138635\n",
      "Average test loss: 0.0014304997999635008\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022788000093566047\n",
      "Average test loss: 0.001397904729988012\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022774340179231432\n",
      "Average test loss: 0.0014070785471962557\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_64_Depth10/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.23\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.74\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.22\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.52\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.70\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.61\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.99\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.03\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.98\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.44\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.65\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.67\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.74\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.68\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.14\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.36\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.27\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.42\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.44\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.80\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.84\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.85\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.99\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 34.08\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.73\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 34.23\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 34.18\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 34.25\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.37\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.51\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
