{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_64x64_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.27251538530985514\n",
      "Average test loss: 0.01398533218436771\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1047720955212911\n",
      "Average test loss: 0.013154399481912453\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08572882686720953\n",
      "Average test loss: 0.014168768777615495\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0755241859820154\n",
      "Average test loss: 0.00892478929211696\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06936741818322076\n",
      "Average test loss: 0.009098661965380112\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06337562696139018\n",
      "Average test loss: 0.008273617283337645\n",
      "Epoch 7/300\n",
      "Average training loss: 0.061098614308569166\n",
      "Average test loss: 0.008171904559764596\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05743417316012912\n",
      "Average test loss: 0.008445019433067904\n",
      "Epoch 9/300\n",
      "Average training loss: 0.055689240084754094\n",
      "Average test loss: 0.009239569962852532\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05304775627785259\n",
      "Average test loss: 0.009590774229831166\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05099098132054011\n",
      "Average test loss: 0.016763484926273425\n",
      "Epoch 12/300\n",
      "Average training loss: 0.049494085321823755\n",
      "Average test loss: 0.008828408707347182\n",
      "Epoch 13/300\n",
      "Average training loss: 0.047684640907579\n",
      "Average test loss: 0.007113171368009514\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0464508395658599\n",
      "Average test loss: 0.007815128411683771\n",
      "Epoch 15/300\n",
      "Average training loss: 0.044975501322084006\n",
      "Average test loss: 0.007611495470835103\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04334731014238463\n",
      "Average test loss: 0.0071598472437924806\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04258839123116599\n",
      "Average test loss: 0.007561581018484301\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04133795215023889\n",
      "Average test loss: 0.006910182576212618\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04095730626417531\n",
      "Average test loss: 0.006754322738283211\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03980826304687394\n",
      "Average test loss: 0.006400476010309325\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03909348795149061\n",
      "Average test loss: 0.0076573801135851275\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0384202087521553\n",
      "Average test loss: 0.006537775785144832\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03781297147605154\n",
      "Average test loss: 0.006089390536149343\n",
      "Epoch 24/300\n",
      "Average training loss: 0.037238638013601305\n",
      "Average test loss: 0.00617787081334326\n",
      "Epoch 25/300\n",
      "Average training loss: 0.036726692464616566\n",
      "Average test loss: 0.005994888990910517\n",
      "Epoch 26/300\n",
      "Average training loss: 0.036463837754395274\n",
      "Average test loss: 0.005947492732149031\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0360000580449899\n",
      "Average test loss: 0.006165523536917236\n",
      "Epoch 28/300\n",
      "Average training loss: 0.035752753837241066\n",
      "Average test loss: 0.006021909161574311\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03528859865334299\n",
      "Average test loss: 0.007885154033286703\n",
      "Epoch 30/300\n",
      "Average training loss: 0.034988477488358816\n",
      "Average test loss: 0.0059130457105735935\n",
      "Epoch 31/300\n",
      "Average training loss: 0.034787423153718314\n",
      "Average test loss: 0.0056388742869926825\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03431261932849884\n",
      "Average test loss: 0.005633313132243024\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03440234009093709\n",
      "Average test loss: 0.005706961227373944\n",
      "Epoch 34/300\n",
      "Average training loss: 0.033747711117068924\n",
      "Average test loss: 0.007023996769968006\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0336608688433965\n",
      "Average test loss: 0.0057434356843845714\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03352933310800128\n",
      "Average test loss: 0.005703952054596609\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033245059496826594\n",
      "Average test loss: 0.005545826471100251\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03312409966190656\n",
      "Average test loss: 0.00631311722430918\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03286916012896432\n",
      "Average test loss: 0.005499237318833669\n",
      "Epoch 40/300\n",
      "Average training loss: 0.032650835517379975\n",
      "Average test loss: 0.005678896953869197\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03251988482475281\n",
      "Average test loss: 0.006211772728711366\n",
      "Epoch 42/300\n",
      "Average training loss: 0.032332000896334645\n",
      "Average test loss: 0.00823796792080005\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0322643572340409\n",
      "Average test loss: 0.005430814115537538\n",
      "Epoch 44/300\n",
      "Average training loss: 0.032038670559724175\n",
      "Average test loss: 0.006102525436215931\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03189782822794385\n",
      "Average test loss: 0.005451593006236686\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03179684753219287\n",
      "Average test loss: 0.007896226804289553\n",
      "Epoch 47/300\n",
      "Average training loss: 0.031681653804249234\n",
      "Average test loss: 0.005541736674391561\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03179073072969914\n",
      "Average test loss: 0.005748083253701528\n",
      "Epoch 49/300\n",
      "Average training loss: 0.031553156634171804\n",
      "Average test loss: 0.0054868921774129074\n",
      "Epoch 50/300\n",
      "Average training loss: 0.031279996554056805\n",
      "Average test loss: 0.0053857860035366485\n",
      "Epoch 51/300\n",
      "Average training loss: 0.031218757894304065\n",
      "Average test loss: 0.006080689266531004\n",
      "Epoch 52/300\n",
      "Average training loss: 0.031210458043548796\n",
      "Average test loss: 0.005879433252331283\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03100421163936456\n",
      "Average test loss: 0.005658385357509056\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03095056595404943\n",
      "Average test loss: 0.017364147682156827\n",
      "Epoch 55/300\n",
      "Average training loss: 0.030770421677165562\n",
      "Average test loss: 0.005431049966977702\n",
      "Epoch 56/300\n",
      "Average training loss: 0.030781930453247494\n",
      "Average test loss: 0.006801867011106677\n",
      "Epoch 57/300\n",
      "Average training loss: 0.030628159922030235\n",
      "Average test loss: 0.005910865296092299\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030550444834762148\n",
      "Average test loss: 0.005627820583681265\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0304888327833679\n",
      "Average test loss: 0.005358844863043891\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03042990297410223\n",
      "Average test loss: 0.005412540665931172\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030337056219577788\n",
      "Average test loss: 0.005600381480736865\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03015243937075138\n",
      "Average test loss: 0.005599956371304062\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03017741903497113\n",
      "Average test loss: 0.0053708966469599145\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03005692900220553\n",
      "Average test loss: 0.005471930527024799\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03001322830716769\n",
      "Average test loss: 0.005717090715550714\n",
      "Epoch 66/300\n",
      "Average training loss: 0.030054106877909766\n",
      "Average test loss: 0.005337492715981272\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029890960837403932\n",
      "Average test loss: 0.005332197694314851\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02978427321049902\n",
      "Average test loss: 0.006360649814208349\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02976500032676591\n",
      "Average test loss: 0.01033790929781066\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02974416962265968\n",
      "Average test loss: 0.0059847885631024835\n",
      "Epoch 71/300\n",
      "Average training loss: 0.029626800540420745\n",
      "Average test loss: 0.00537145485687587\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02954325695335865\n",
      "Average test loss: 0.005373990754286448\n",
      "Epoch 73/300\n",
      "Average training loss: 0.029571393431888686\n",
      "Average test loss: 0.005845083887999257\n",
      "Epoch 74/300\n",
      "Average training loss: 0.029475080841117433\n",
      "Average test loss: 0.006843687131173081\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02947861659857962\n",
      "Average test loss: 0.005280404554473029\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02936256100402938\n",
      "Average test loss: 0.0054972927706937\n",
      "Epoch 77/300\n",
      "Average training loss: 0.029380598894423907\n",
      "Average test loss: 0.005294538928402794\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029185872511731253\n",
      "Average test loss: 0.0060512341815564365\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029174131779207125\n",
      "Average test loss: 0.0059883235572940775\n",
      "Epoch 80/300\n",
      "Average training loss: 0.029171330706940758\n",
      "Average test loss: 0.005395269766449928\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02915027098523246\n",
      "Average test loss: 0.005739955023758942\n",
      "Epoch 82/300\n",
      "Average training loss: 0.029105575000246367\n",
      "Average test loss: 0.005623699014385542\n",
      "Epoch 83/300\n",
      "Average training loss: 0.028974097364478642\n",
      "Average test loss: 0.005442191324300236\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02905241519378291\n",
      "Average test loss: 0.005310266590366761\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02890773185259766\n",
      "Average test loss: 0.005716044279850192\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028995875106917486\n",
      "Average test loss: 0.006110270623945527\n",
      "Epoch 87/300\n",
      "Average training loss: 0.028804320931434632\n",
      "Average test loss: 0.005741731007066038\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028771873354911805\n",
      "Average test loss: 0.005838339952545034\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028728284160296123\n",
      "Average test loss: 0.00559667508966393\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02869223459892803\n",
      "Average test loss: 0.005396865110844373\n",
      "Epoch 91/300\n",
      "Average training loss: 0.028687535651856\n",
      "Average test loss: 0.006693497407767508\n",
      "Epoch 92/300\n",
      "Average training loss: 0.028717614303032556\n",
      "Average test loss: 0.005454193722042772\n",
      "Epoch 93/300\n",
      "Average training loss: 0.028690004451407326\n",
      "Average test loss: 0.005292170798199045\n",
      "Epoch 94/300\n",
      "Average training loss: 0.028460798563228713\n",
      "Average test loss: 0.005471655841088957\n",
      "Epoch 95/300\n",
      "Average training loss: 0.028441544704967073\n",
      "Average test loss: 0.0055650852748917205\n",
      "Epoch 96/300\n",
      "Average training loss: 0.028593379514084923\n",
      "Average test loss: 0.0055278919123940996\n",
      "Epoch 97/300\n",
      "Average training loss: 0.028382501522699992\n",
      "Average test loss: 0.00560445760935545\n",
      "Epoch 98/300\n",
      "Average training loss: 0.028366079153286088\n",
      "Average test loss: 0.005402923607163959\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028416091526548067\n",
      "Average test loss: 0.005449786489208539\n",
      "Epoch 100/300\n",
      "Average training loss: 0.028386832853158316\n",
      "Average test loss: 0.005415980305108759\n",
      "Epoch 101/300\n",
      "Average training loss: 0.028251482506593067\n",
      "Average test loss: 0.005230233511163129\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02833168850508001\n",
      "Average test loss: 0.005699629942162169\n",
      "Epoch 103/300\n",
      "Average training loss: 0.028221583919392692\n",
      "Average test loss: 0.0053270721642507445\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02819363311926524\n",
      "Average test loss: 0.00540190679869718\n",
      "Epoch 105/300\n",
      "Average training loss: 0.028172186429301898\n",
      "Average test loss: 0.005423766831970877\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02818938607805305\n",
      "Average test loss: 0.005608901924557156\n",
      "Epoch 107/300\n",
      "Average training loss: 0.028117229888836543\n",
      "Average test loss: 0.005368568621161912\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02808533669014772\n",
      "Average test loss: 0.005472546031905545\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02807662178079287\n",
      "Average test loss: 0.014182864373756779\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027974037220080693\n",
      "Average test loss: 0.005874428754465448\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02803291813035806\n",
      "Average test loss: 0.006710199512541294\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027967850340737236\n",
      "Average test loss: 0.006265691209998396\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027920887490113577\n",
      "Average test loss: 0.043871413997064033\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028019874023066625\n",
      "Average test loss: 0.005547921302831835\n",
      "Epoch 115/300\n",
      "Average training loss: 0.027875311311748294\n",
      "Average test loss: 0.006191358399473958\n",
      "Epoch 116/300\n",
      "Average training loss: 0.027867755558755664\n",
      "Average test loss: 0.005318928987615638\n",
      "Epoch 117/300\n",
      "Average training loss: 0.027849337024821175\n",
      "Average test loss: 0.005361426329033242\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027829326432612208\n",
      "Average test loss: 0.005284047507163551\n",
      "Epoch 119/300\n",
      "Average training loss: 0.027818177277843158\n",
      "Average test loss: 0.006010554919226302\n",
      "Epoch 120/300\n",
      "Average training loss: 0.027714862356583277\n",
      "Average test loss: 0.005765113583455483\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027719789970252248\n",
      "Average test loss: 0.005373779689272245\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02768529950413439\n",
      "Average test loss: 0.005723600298166275\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02772432872818576\n",
      "Average test loss: 0.005653710784183608\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027683172962731786\n",
      "Average test loss: 0.005363331275681654\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02756553148229917\n",
      "Average test loss: 0.005527633398771286\n",
      "Epoch 126/300\n",
      "Average training loss: 0.027632668241858484\n",
      "Average test loss: 0.005655519120809104\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027567786455154418\n",
      "Average test loss: 0.005263406100786395\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027593239364524683\n",
      "Average test loss: 0.0056583842591693\n",
      "Epoch 129/300\n",
      "Average training loss: 0.027580652882655463\n",
      "Average test loss: 0.005933886447714434\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027502345967623923\n",
      "Average test loss: 0.005308617132819361\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02750543649494648\n",
      "Average test loss: 0.007199624853415622\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02741842665606075\n",
      "Average test loss: 0.005768599055293534\n",
      "Epoch 133/300\n",
      "Average training loss: 0.027448844116595057\n",
      "Average test loss: 0.009817335763739214\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02743323402603467\n",
      "Average test loss: 0.005701721349938048\n",
      "Epoch 135/300\n",
      "Average training loss: 0.027374769378039572\n",
      "Average test loss: 0.005730224287758271\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02732889951268832\n",
      "Average test loss: 0.006029764486269819\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02737360652950075\n",
      "Average test loss: 0.00526318955483536\n",
      "Epoch 138/300\n",
      "Average training loss: 0.027345092601246304\n",
      "Average test loss: 0.005422375212526984\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02731256172226535\n",
      "Average test loss: 0.005365245351153943\n",
      "Epoch 140/300\n",
      "Average training loss: 0.027269358192880948\n",
      "Average test loss: 0.005776664832399951\n",
      "Epoch 141/300\n",
      "Average training loss: 0.027278184173835648\n",
      "Average test loss: 0.35059816575050357\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027273379327522385\n",
      "Average test loss: 0.005372710962262418\n",
      "Epoch 143/300\n",
      "Average training loss: 0.027175996098253462\n",
      "Average test loss: 0.0052799725296596685\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02721769933402538\n",
      "Average test loss: 0.005537299197167158\n",
      "Epoch 145/300\n",
      "Average training loss: 0.027196167959107294\n",
      "Average test loss: 0.005491148083988163\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027179109202490913\n",
      "Average test loss: 0.005588176713221603\n",
      "Epoch 147/300\n",
      "Average training loss: 0.027124524606598747\n",
      "Average test loss: 0.005543311460771494\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02712168088886473\n",
      "Average test loss: 0.0052437510734630955\n",
      "Epoch 149/300\n",
      "Average training loss: 0.027097575401266415\n",
      "Average test loss: 0.005813486301236683\n",
      "Epoch 150/300\n",
      "Average training loss: 0.027103356833259266\n",
      "Average test loss: 0.0056599809154868124\n",
      "Epoch 151/300\n",
      "Average training loss: 0.027163432503739992\n",
      "Average test loss: 0.005432808270470963\n",
      "Epoch 152/300\n",
      "Average training loss: 0.027025962092810208\n",
      "Average test loss: 0.00643174281799131\n",
      "Epoch 153/300\n",
      "Average training loss: 0.027056588464313084\n",
      "Average test loss: 0.005277967665344476\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0269850112663375\n",
      "Average test loss: 0.005943988103833463\n",
      "Epoch 155/300\n",
      "Average training loss: 0.027031850091285177\n",
      "Average test loss: 0.005601744141015741\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02695230228536659\n",
      "Average test loss: 0.005478401647259792\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0269627978189124\n",
      "Average test loss: 0.008794029869139195\n",
      "Epoch 158/300\n",
      "Average training loss: 0.026939230460259648\n",
      "Average test loss: 0.006212348873002662\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02688926575581233\n",
      "Average test loss: 0.005585127965443664\n",
      "Epoch 160/300\n",
      "Average training loss: 0.026969962336950832\n",
      "Average test loss: 0.005291829773535331\n",
      "Epoch 161/300\n",
      "Average training loss: 0.026937739587492415\n",
      "Average test loss: 0.005353763835918572\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02691365799307823\n",
      "Average test loss: 0.005635735096616877\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026861951066388026\n",
      "Average test loss: 0.0053209050405356615\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026865352246496412\n",
      "Average test loss: 0.005656130603618092\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02681591724190447\n",
      "Average test loss: 0.005347139642470413\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026789582133293154\n",
      "Average test loss: 0.005471198246710831\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026850731251140434\n",
      "Average test loss: 0.005535012025800016\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026752579256892204\n",
      "Average test loss: 0.005495805364929968\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02680218650566207\n",
      "Average test loss: 0.005402744229882956\n",
      "Epoch 170/300\n",
      "Average training loss: 0.026763541825943524\n",
      "Average test loss: 0.008573848249597681\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026746974186764822\n",
      "Average test loss: 0.005731640711012813\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026690930374794537\n",
      "Average test loss: 0.0055844915422300495\n",
      "Epoch 173/300\n",
      "Average training loss: 0.026750086408522392\n",
      "Average test loss: 0.0066090384105013476\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026694420450263552\n",
      "Average test loss: 0.005296288683596584\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02674065535929468\n",
      "Average test loss: 0.005381850493864881\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026659194938838483\n",
      "Average test loss: 0.006409728973690006\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02666355383892854\n",
      "Average test loss: 0.005558613283352719\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02668565926121341\n",
      "Average test loss: 0.005501957267729772\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02662261167830891\n",
      "Average test loss: 0.005307445653196838\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02660249772336748\n",
      "Average test loss: 0.0054585491331915065\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026610339485936694\n",
      "Average test loss: 0.015166713395880329\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02680603280166785\n",
      "Average test loss: 0.005319902041720019\n",
      "Epoch 183/300\n",
      "Average training loss: 0.026538419139054085\n",
      "Average test loss: 0.005326243292954233\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02657751471797625\n",
      "Average test loss: 0.005497059916870462\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026526184539000193\n",
      "Average test loss: 0.005381036223429773\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026547116299470267\n",
      "Average test loss: 0.005484206627640459\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02649853611820274\n",
      "Average test loss: 0.005363366528103749\n",
      "Epoch 188/300\n",
      "Average training loss: 0.026518011619647345\n",
      "Average test loss: 0.005309306356641981\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026538136819998424\n",
      "Average test loss: 0.008882203934921158\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026442536955078443\n",
      "Average test loss: 0.0054421292704840505\n",
      "Epoch 191/300\n",
      "Average training loss: 0.026411834547917047\n",
      "Average test loss: 0.005415982847826349\n",
      "Epoch 192/300\n",
      "Average training loss: 0.026431976965732046\n",
      "Average test loss: 0.005480937270240651\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026470083377427525\n",
      "Average test loss: 0.005641253091808822\n",
      "Epoch 194/300\n",
      "Average training loss: 0.026457496762275694\n",
      "Average test loss: 0.005661539896494813\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026373600348830222\n",
      "Average test loss: 0.005742402219523986\n",
      "Epoch 196/300\n",
      "Average training loss: 0.026406033428178894\n",
      "Average test loss: 0.005395062517374754\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026439456701278688\n",
      "Average test loss: 0.005546908046636317\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02642668062945207\n",
      "Average test loss: 0.005546957511144379\n",
      "Epoch 199/300\n",
      "Average training loss: 0.026355719927284454\n",
      "Average test loss: 0.005539693624195125\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026355404193202654\n",
      "Average test loss: 0.005510857061379486\n",
      "Epoch 201/300\n",
      "Average training loss: 0.026383764929241604\n",
      "Average test loss: 0.005589779889418019\n",
      "Epoch 202/300\n",
      "Average training loss: 0.026345918938517572\n",
      "Average test loss: 0.00525932538178232\n",
      "Epoch 203/300\n",
      "Average training loss: 0.026325987492998443\n",
      "Average test loss: 0.006218700310422314\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026350569595893224\n",
      "Average test loss: 0.005300651342918476\n",
      "Epoch 205/300\n",
      "Average training loss: 0.026305190025104418\n",
      "Average test loss: 0.005418482813156314\n",
      "Epoch 206/300\n",
      "Average training loss: 0.026284561446971364\n",
      "Average test loss: 0.00923615815035171\n",
      "Epoch 207/300\n",
      "Average training loss: 0.026292174859179392\n",
      "Average test loss: 0.005596983945618073\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02629401357471943\n",
      "Average test loss: 0.005381268778608905\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02627982141243087\n",
      "Average test loss: 0.005715259772621923\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0262584590613842\n",
      "Average test loss: 0.00606475896636645\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02619497210284074\n",
      "Average test loss: 0.0056675815553300915\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026211776461866166\n",
      "Average test loss: 0.005564463967250453\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026260975341002146\n",
      "Average test loss: 0.0055072033214900225\n",
      "Epoch 214/300\n",
      "Average training loss: 0.026243962642219332\n",
      "Average test loss: 0.005498286583978269\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02614087437755532\n",
      "Average test loss: 0.005316880303952429\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026241419326927928\n",
      "Average test loss: 0.006065916267948018\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026154673480325276\n",
      "Average test loss: 0.0058549981282817\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02615850376089414\n",
      "Average test loss: 0.0069239626137746705\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026164889643589654\n",
      "Average test loss: 0.00536426926487022\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026180153151353202\n",
      "Average test loss: 0.006889467693037457\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02610816371606456\n",
      "Average test loss: 0.005381191528505749\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026132165941927167\n",
      "Average test loss: 0.005300786604897844\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026131965114010704\n",
      "Average test loss: 0.005461796375198497\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026052727939354047\n",
      "Average test loss: 0.006304533178607623\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026074451537595855\n",
      "Average test loss: 0.005461885932419035\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026177891908420455\n",
      "Average test loss: 0.005812466861059269\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0260726404454973\n",
      "Average test loss: 0.005822687415199148\n",
      "Epoch 228/300\n",
      "Average training loss: 0.026031585501299963\n",
      "Average test loss: 0.005404012100150188\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026032636208666696\n",
      "Average test loss: 0.005684685765455166\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026071839794516563\n",
      "Average test loss: 0.00634326702894436\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026094851856430373\n",
      "Average test loss: 0.005699000622249312\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02610086217853758\n",
      "Average test loss: 0.005900607342107429\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026005823052591748\n",
      "Average test loss: 0.005595783737384611\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0259651281370057\n",
      "Average test loss: 0.006277134377923277\n",
      "Epoch 235/300\n",
      "Average training loss: 0.025972418101297485\n",
      "Average test loss: 0.005548386911551157\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02601791547735532\n",
      "Average test loss: 0.005637829023723801\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02601641947693295\n",
      "Average test loss: 0.0057237941034966044\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0259190803402\n",
      "Average test loss: 0.0055751176716552844\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026069438580009673\n",
      "Average test loss: 0.005374747744657927\n",
      "Epoch 240/300\n",
      "Average training loss: 0.025957011540730796\n",
      "Average test loss: 0.005598928931686614\n",
      "Epoch 241/300\n",
      "Average training loss: 0.025987758148875502\n",
      "Average test loss: 0.008221754747753341\n",
      "Epoch 242/300\n",
      "Average training loss: 0.025884365507298047\n",
      "Average test loss: 0.0057174108338852725\n",
      "Epoch 243/300\n",
      "Average training loss: 0.025909857753250335\n",
      "Average test loss: 0.005412633979279134\n",
      "Epoch 244/300\n",
      "Average training loss: 0.025860013917088507\n",
      "Average test loss: 0.005543876738597949\n",
      "Epoch 245/300\n",
      "Average training loss: 0.025872055469287766\n",
      "Average test loss: 0.005570217159473234\n",
      "Epoch 246/300\n",
      "Average training loss: 0.025895018516315354\n",
      "Average test loss: 0.0053855005395081305\n",
      "Epoch 247/300\n",
      "Average training loss: 0.025978640298048656\n",
      "Average test loss: 0.005655000025820401\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02588635655740897\n",
      "Average test loss: 0.005468637163854308\n",
      "Epoch 249/300\n",
      "Average training loss: 0.025894650838441318\n",
      "Average test loss: 0.005291563588298029\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02581836209528976\n",
      "Average test loss: 0.0060295041569819055\n",
      "Epoch 251/300\n",
      "Average training loss: 0.025881833505299358\n",
      "Average test loss: 0.0055562614281144404\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02583564722041289\n",
      "Average test loss: 0.00534795799644457\n",
      "Epoch 253/300\n",
      "Average training loss: 0.025869043240944543\n",
      "Average test loss: 0.005442090742703941\n",
      "Epoch 254/300\n",
      "Average training loss: 0.025890088773436015\n",
      "Average test loss: 0.005536691137899955\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025809208154678343\n",
      "Average test loss: 0.005516645992795627\n",
      "Epoch 256/300\n",
      "Average training loss: 0.025806547911630735\n",
      "Average test loss: 0.006188497771819432\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02580573747224278\n",
      "Average test loss: 0.005659563121282392\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025787291806605127\n",
      "Average test loss: 0.005469244339813789\n",
      "Epoch 259/300\n",
      "Average training loss: 0.025800036400556565\n",
      "Average test loss: 0.005827254653804832\n",
      "Epoch 260/300\n",
      "Average training loss: 0.025761236912674373\n",
      "Average test loss: 0.005754731231679519\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02581359715263049\n",
      "Average test loss: 0.005532491787026326\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025743755160106553\n",
      "Average test loss: 0.007024272175712718\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02575806870063146\n",
      "Average test loss: 0.005403371334075928\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02576693214972814\n",
      "Average test loss: 0.008932771589193079\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026839726014269722\n",
      "Average test loss: 0.005737139621956481\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026310048742426765\n",
      "Average test loss: 0.00548867856276532\n",
      "Epoch 267/300\n",
      "Average training loss: 0.025672666826181943\n",
      "Average test loss: 0.00539283609473043\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025648934814665052\n",
      "Average test loss: 0.005488852274500662\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025659394684765073\n",
      "Average test loss: 0.0054246089913778835\n",
      "Epoch 270/300\n",
      "Average training loss: 0.025683044312728776\n",
      "Average test loss: 0.00564376178301043\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025666284498241213\n",
      "Average test loss: 0.005530292597495847\n",
      "Epoch 272/300\n",
      "Average training loss: 0.025751317900088097\n",
      "Average test loss: 0.006189778147472276\n",
      "Epoch 273/300\n",
      "Average training loss: 0.025711312972837023\n",
      "Average test loss: 0.005587892623204324\n",
      "Epoch 274/300\n",
      "Average training loss: 0.025645855569177204\n",
      "Average test loss: 0.005958217515299718\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02565897862116496\n",
      "Average test loss: 0.005497042324807909\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025653153960903487\n",
      "Average test loss: 0.0060661344486806126\n",
      "Epoch 277/300\n",
      "Average training loss: 0.025648573969801268\n",
      "Average test loss: 0.0054344351324770186\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02570516448550754\n",
      "Average test loss: 0.006351004978641868\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025689481341176563\n",
      "Average test loss: 0.0054290567880703345\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025641079811586275\n",
      "Average test loss: 0.005952524153722657\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025605131899317107\n",
      "Average test loss: 0.00578447473131948\n",
      "Epoch 282/300\n",
      "Average training loss: 0.025632885480920475\n",
      "Average test loss: 0.005587045405473974\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025556866885887253\n",
      "Average test loss: 0.0058805651453634105\n",
      "Epoch 284/300\n",
      "Average training loss: 0.025601463629139794\n",
      "Average test loss: 0.005744730520993471\n",
      "Epoch 285/300\n",
      "Average training loss: 0.025612684865792594\n",
      "Average test loss: 0.005503914975457721\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02558850011891789\n",
      "Average test loss: 0.006162113075041109\n",
      "Epoch 287/300\n",
      "Average training loss: 0.025604430476824442\n",
      "Average test loss: 0.006453300677653816\n",
      "Epoch 288/300\n",
      "Average training loss: 0.025502672655714882\n",
      "Average test loss: 0.005572625315851635\n",
      "Epoch 289/300\n",
      "Average training loss: 0.025560778306590187\n",
      "Average test loss: 0.007596809860318899\n",
      "Epoch 290/300\n",
      "Average training loss: 0.025656229193011918\n",
      "Average test loss: 0.005777218811627891\n",
      "Epoch 291/300\n",
      "Average training loss: 0.025549469451109568\n",
      "Average test loss: 0.0054577124679668085\n",
      "Epoch 292/300\n",
      "Average training loss: 0.025481813020176356\n",
      "Average test loss: 0.005712044597913821\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02555884175664849\n",
      "Average test loss: 0.005803198577215275\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025556817279921637\n",
      "Average test loss: 0.005651350719233354\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02556018525030878\n",
      "Average test loss: 0.005558757122606039\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02549211676253213\n",
      "Average test loss: 0.006036036322928137\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0255209268513653\n",
      "Average test loss: 0.006580287892785338\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025621557962563303\n",
      "Average test loss: 0.0055238367252879676\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025503540544046296\n",
      "Average test loss: 0.00595456182377206\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025451676625344487\n",
      "Average test loss: 0.0077299659873048465\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2362946269578404\n",
      "Average test loss: 0.009329938189023071\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0905247288412518\n",
      "Average test loss: 0.010234700192179945\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07070277221335305\n",
      "Average test loss: 0.008296088923182753\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06105165027578672\n",
      "Average test loss: 0.00668892394254605\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05473071871863471\n",
      "Average test loss: 0.006391297787427902\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05011612821618716\n",
      "Average test loss: 0.005430528899033864\n",
      "Epoch 7/300\n",
      "Average training loss: 0.046928312536742955\n",
      "Average test loss: 0.004986229371279478\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04444273336397277\n",
      "Average test loss: 0.004925733951644765\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04180087098148134\n",
      "Average test loss: 0.02690444898025857\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03978218877646658\n",
      "Average test loss: 0.00474224168020818\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03751639182700051\n",
      "Average test loss: 0.004897688676913579\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03628547418117523\n",
      "Average test loss: 0.004441377809478177\n",
      "Epoch 13/300\n",
      "Average training loss: 0.034914228174421524\n",
      "Average test loss: 0.004209186430192656\n",
      "Epoch 14/300\n",
      "Average training loss: 0.033789925936195585\n",
      "Average test loss: 0.004630752216196722\n",
      "Epoch 15/300\n",
      "Average training loss: 0.032459123518731856\n",
      "Average test loss: 0.004222848560040196\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03159280749824312\n",
      "Average test loss: 0.0041254736135403315\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030237650947438344\n",
      "Average test loss: 0.004559589211725526\n",
      "Epoch 18/300\n",
      "Average training loss: 0.029501421415143544\n",
      "Average test loss: 0.0043259071682890254\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0287747578902377\n",
      "Average test loss: 0.0040524251181632285\n",
      "Epoch 20/300\n",
      "Average training loss: 0.027931788621677292\n",
      "Average test loss: 0.0038826142470869754\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027490855069624055\n",
      "Average test loss: 0.0036466582684467237\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02702707555393378\n",
      "Average test loss: 0.0036064490026070013\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02658905976348453\n",
      "Average test loss: 0.003517314521388875\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02619784397052394\n",
      "Average test loss: 0.0034455034455491435\n",
      "Epoch 25/300\n",
      "Average training loss: 0.025651510472098987\n",
      "Average test loss: 0.004239286087039444\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025384707162777582\n",
      "Average test loss: 0.003371470825539695\n",
      "Epoch 27/300\n",
      "Average training loss: 0.025276346292760637\n",
      "Average test loss: 0.0033460119689504306\n",
      "Epoch 28/300\n",
      "Average training loss: 0.024852274861600663\n",
      "Average test loss: 0.0034278180102507274\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02451314916378922\n",
      "Average test loss: 0.003242119509105881\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024468901225262216\n",
      "Average test loss: 0.003283528096559975\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02411040836572647\n",
      "Average test loss: 0.0033203092813491823\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024000764118300544\n",
      "Average test loss: 0.0032801646689573924\n",
      "Epoch 33/300\n",
      "Average training loss: 0.023914823457598686\n",
      "Average test loss: 0.003403706226704849\n",
      "Epoch 34/300\n",
      "Average training loss: 0.023674203256766002\n",
      "Average test loss: 0.0032943628090951176\n",
      "Epoch 35/300\n",
      "Average training loss: 0.023487146991822454\n",
      "Average test loss: 0.003405048632166452\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02328936346537537\n",
      "Average test loss: 0.0031503179638336103\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02308212851981322\n",
      "Average test loss: 0.0034094711057841777\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02304338872929414\n",
      "Average test loss: 0.003458131268620491\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02288182563086351\n",
      "Average test loss: 0.003706306077953842\n",
      "Epoch 40/300\n",
      "Average training loss: 0.022797045853402878\n",
      "Average test loss: 0.003703981700456805\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02261974254747232\n",
      "Average test loss: 0.003180932377361589\n",
      "Epoch 42/300\n",
      "Average training loss: 0.022578466650512483\n",
      "Average test loss: 0.003226039678272274\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0224884416775571\n",
      "Average test loss: 0.0035164887075208955\n",
      "Epoch 44/300\n",
      "Average training loss: 0.022349468756053183\n",
      "Average test loss: 0.003701067900285125\n",
      "Epoch 45/300\n",
      "Average training loss: 0.022233494902650516\n",
      "Average test loss: 0.003113965829834342\n",
      "Epoch 46/300\n",
      "Average training loss: 0.022213863846328525\n",
      "Average test loss: 0.0033770458770708907\n",
      "Epoch 47/300\n",
      "Average training loss: 0.022194119776288667\n",
      "Average test loss: 0.0030623043905943633\n",
      "Epoch 48/300\n",
      "Average training loss: 0.022036568911539184\n",
      "Average test loss: 0.0030699253254052664\n",
      "Epoch 49/300\n",
      "Average training loss: 0.021929074659943582\n",
      "Average test loss: 0.003287819632018606\n",
      "Epoch 50/300\n",
      "Average training loss: 0.021822044044733046\n",
      "Average test loss: 0.0031305045520679817\n",
      "Epoch 51/300\n",
      "Average training loss: 0.021837513049443563\n",
      "Average test loss: 0.0031456827285389105\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02168246218396558\n",
      "Average test loss: 0.0030705579691049124\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021625379310713875\n",
      "Average test loss: 0.003781256709454788\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02161438532339202\n",
      "Average test loss: 0.003059869489736027\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021497816951738462\n",
      "Average test loss: 0.003337376823855771\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02143248640994231\n",
      "Average test loss: 0.0031497097979817125\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02144240687456396\n",
      "Average test loss: 0.003016246964327163\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02128704072535038\n",
      "Average test loss: 0.0035247531491849156\n",
      "Epoch 59/300\n",
      "Average training loss: 0.021405264743500286\n",
      "Average test loss: 0.0030734146864463885\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021216153198646174\n",
      "Average test loss: 0.0031737099140882494\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021262899592518807\n",
      "Average test loss: 0.003162415326676435\n",
      "Epoch 62/300\n",
      "Average training loss: 0.021130736430486043\n",
      "Average test loss: 0.0033129021846171882\n",
      "Epoch 63/300\n",
      "Average training loss: 0.021095488448937733\n",
      "Average test loss: 0.0030083624116248554\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02099768274360233\n",
      "Average test loss: 0.0030635918436778915\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02100183694726891\n",
      "Average test loss: 0.002993085514133175\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02092901954551538\n",
      "Average test loss: 0.003926913579098052\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02088599088953601\n",
      "Average test loss: 0.0031504868200669684\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02081723375287321\n",
      "Average test loss: 0.003102548061766558\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02079312817255656\n",
      "Average test loss: 0.0033016864098608495\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020748016486565272\n",
      "Average test loss: 0.0032047337531629534\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0206978749781847\n",
      "Average test loss: 0.0030246455748048095\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020676442112359736\n",
      "Average test loss: 0.003102007171760003\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02063650839527448\n",
      "Average test loss: 0.0034241419862955807\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020629798081186083\n",
      "Average test loss: 0.002988418684651454\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020528593166006935\n",
      "Average test loss: 0.0030948728875567514\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020512687587075765\n",
      "Average test loss: 0.003113447037835916\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020458597855435478\n",
      "Average test loss: 0.0033466726390437946\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0206028895676136\n",
      "Average test loss: 0.003172818125122123\n",
      "Epoch 79/300\n",
      "Average training loss: 0.020414589179886714\n",
      "Average test loss: 0.002994602501599325\n",
      "Epoch 80/300\n",
      "Average training loss: 0.020399468382199606\n",
      "Average test loss: 0.0029578970823850898\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020344523679878976\n",
      "Average test loss: 0.0030092018939968613\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02031543285648028\n",
      "Average test loss: 0.003295252731276883\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020303589481446477\n",
      "Average test loss: 0.003105301777521769\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020246227469709183\n",
      "Average test loss: 0.003068924225245913\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020222792566650445\n",
      "Average test loss: 0.0032737824286644657\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020157189240058264\n",
      "Average test loss: 0.003038373205810785\n",
      "Epoch 87/300\n",
      "Average training loss: 0.020250965669751167\n",
      "Average test loss: 0.003492423449953397\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020096709094113773\n",
      "Average test loss: 0.0030280608342339596\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02011373445805576\n",
      "Average test loss: 0.003076922203724583\n",
      "Epoch 90/300\n",
      "Average training loss: 0.020036325835519366\n",
      "Average test loss: 0.0030424565747380256\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020042453348636628\n",
      "Average test loss: 0.002955589390049378\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020013752203848628\n",
      "Average test loss: 0.004751201555546787\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02000307123031881\n",
      "Average test loss: 0.0036226325821545387\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019939124958382713\n",
      "Average test loss: 0.0030183649259722897\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019875324596961338\n",
      "Average test loss: 0.0032872362623198163\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019966940434442627\n",
      "Average test loss: 0.0030996811214006608\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019844783490730655\n",
      "Average test loss: 0.0030061087844272453\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01982106358640724\n",
      "Average test loss: 0.0033047093695236576\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019861417732304997\n",
      "Average test loss: 0.0030602535468836627\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01980934847229057\n",
      "Average test loss: 0.0030231697052303286\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019756143785185285\n",
      "Average test loss: 0.0030901447747730547\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019725331043203672\n",
      "Average test loss: 0.002942159739633401\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019753309385644065\n",
      "Average test loss: 0.0030226750237246354\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019694177767468825\n",
      "Average test loss: 0.0032569584279424613\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019691700738337304\n",
      "Average test loss: 0.0029610150108734766\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019664969755543604\n",
      "Average test loss: 0.0030406346710191833\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01964563519259294\n",
      "Average test loss: 0.0030735637905697027\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01958568193184005\n",
      "Average test loss: 0.0029767016865726976\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019591984992225964\n",
      "Average test loss: 0.0029776750307323203\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019549433901906013\n",
      "Average test loss: 0.0031063303992980057\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019544779679841466\n",
      "Average test loss: 0.002984489387522141\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01948309069292413\n",
      "Average test loss: 0.002968718985302581\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01950815292696158\n",
      "Average test loss: 0.0029603116654066575\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01952862622174952\n",
      "Average test loss: 0.0030087406053725214\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01945826886345943\n",
      "Average test loss: 0.0030935998755610653\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0194336309797234\n",
      "Average test loss: 0.002977817510151201\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01940177934202883\n",
      "Average test loss: 0.003075320421821541\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01943062392539448\n",
      "Average test loss: 0.0030181988196240532\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019378854538003604\n",
      "Average test loss: 0.0030498831605331764\n",
      "Epoch 120/300\n",
      "Average training loss: 0.019325427629881434\n",
      "Average test loss: 0.0030341747984704045\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019343475533856284\n",
      "Average test loss: 0.002971470263683134\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0193154253611962\n",
      "Average test loss: 0.0029717918102526004\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01937464712891314\n",
      "Average test loss: 0.0030097881154053743\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01934396734668149\n",
      "Average test loss: 0.003006525682285428\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01925888384713067\n",
      "Average test loss: 0.003423114959564474\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01927233359217644\n",
      "Average test loss: 0.002946645160516103\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019253544529279073\n",
      "Average test loss: 0.0029439038046532207\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01920953162180053\n",
      "Average test loss: 0.003261470914507906\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019165310194094974\n",
      "Average test loss: 0.0030458479865143697\n",
      "Epoch 130/300\n",
      "Average training loss: 0.019174535519546933\n",
      "Average test loss: 0.0029604326476239495\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019153757125139236\n",
      "Average test loss: 0.003038397397639023\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01914714960422781\n",
      "Average test loss: 0.00299519232308699\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019141847608817947\n",
      "Average test loss: 0.003004588457859225\n",
      "Epoch 134/300\n",
      "Average training loss: 0.019181894504361682\n",
      "Average test loss: 0.003094054763722751\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019096267665425937\n",
      "Average test loss: 0.002993831125398477\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01907655913465553\n",
      "Average test loss: 0.0029966799893106025\n",
      "Epoch 137/300\n",
      "Average training loss: 0.019100944207774268\n",
      "Average test loss: 0.002929601418889231\n",
      "Epoch 138/300\n",
      "Average training loss: 0.019096404103769195\n",
      "Average test loss: 0.0030236706551578308\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019033704333835177\n",
      "Average test loss: 0.003010759007806579\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01903174147837692\n",
      "Average test loss: 0.003308555754315522\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018991498356892005\n",
      "Average test loss: 0.003039342867417468\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018977874856856133\n",
      "Average test loss: 0.0029316199651608864\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019009810087581475\n",
      "Average test loss: 0.003153501704749134\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018949319195416238\n",
      "Average test loss: 0.0029646953439546957\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01893865807188882\n",
      "Average test loss: 0.0034240258776893216\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01895919085873498\n",
      "Average test loss: 0.002938336894743972\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018924579603804483\n",
      "Average test loss: 0.002971129774633381\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01893854124347369\n",
      "Average test loss: 0.00295686932404836\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018911988840334945\n",
      "Average test loss: 0.0030700759693152377\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018872788212365574\n",
      "Average test loss: 0.0029697921276092528\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018874574773841434\n",
      "Average test loss: 0.0030273617162472673\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01886566964454121\n",
      "Average test loss: 0.002984573689599832\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018848336910208065\n",
      "Average test loss: 0.0037485667347080176\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018837216421961785\n",
      "Average test loss: 0.00303913936101728\n",
      "Epoch 155/300\n",
      "Average training loss: 0.018823219180107115\n",
      "Average test loss: 0.0030048164042333764\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018882990650832654\n",
      "Average test loss: 0.0030510609199603396\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018763691584269205\n",
      "Average test loss: 0.002951653508676423\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018754108959601987\n",
      "Average test loss: 0.0031238465768595536\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018744187022248903\n",
      "Average test loss: 0.0030038654423422285\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018876520613829294\n",
      "Average test loss: 0.0029689359857390323\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018725445131460825\n",
      "Average test loss: 0.002988746082203256\n",
      "Epoch 162/300\n",
      "Average training loss: 0.018716337735454242\n",
      "Average test loss: 0.003048125988493363\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018714039395252863\n",
      "Average test loss: 0.0031675742483801314\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018747771403855747\n",
      "Average test loss: 0.0030287292119529513\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018675624859001903\n",
      "Average test loss: 0.0029708447015533847\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018646832785672612\n",
      "Average test loss: 0.002947965261629886\n",
      "Epoch 167/300\n",
      "Average training loss: 0.018677465870976448\n",
      "Average test loss: 0.0030994515932268566\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01865198440021939\n",
      "Average test loss: 0.003071860322418312\n",
      "Epoch 169/300\n",
      "Average training loss: 0.018642331194546488\n",
      "Average test loss: 0.0032342427948282826\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01866894987391101\n",
      "Average test loss: 0.0030365063517043987\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01861261083020104\n",
      "Average test loss: 0.0031361359337137803\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0185808561179373\n",
      "Average test loss: 0.0030373516260749764\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018609871735175452\n",
      "Average test loss: 0.003364639580870668\n",
      "Epoch 174/300\n",
      "Average training loss: 0.018598866153094504\n",
      "Average test loss: 0.0031299101546820667\n",
      "Epoch 175/300\n",
      "Average training loss: 0.018586450409558083\n",
      "Average test loss: 0.0030337232806616357\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01858702450990677\n",
      "Average test loss: 0.0029815045446157456\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01857074577940835\n",
      "Average test loss: 0.0030899830174942813\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01857296500603358\n",
      "Average test loss: 0.003680017306469381\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01858027632617288\n",
      "Average test loss: 0.003177074978335036\n",
      "Epoch 180/300\n",
      "Average training loss: 0.018519589896003406\n",
      "Average test loss: 0.0030228036816956267\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018479419256250063\n",
      "Average test loss: 0.002987672690716055\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018483648301826583\n",
      "Average test loss: 0.0032903060064547593\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018524933621287345\n",
      "Average test loss: 0.0030235994745873743\n",
      "Epoch 184/300\n",
      "Average training loss: 0.018455839138891962\n",
      "Average test loss: 0.0030053287032577726\n",
      "Epoch 185/300\n",
      "Average training loss: 0.018480524291594823\n",
      "Average test loss: 0.0030790184800409607\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01858110008140405\n",
      "Average test loss: 0.003046491482605537\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018456815731194284\n",
      "Average test loss: 0.0029888682410948806\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018470474425289367\n",
      "Average test loss: 0.0031198137979954483\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018413103130128648\n",
      "Average test loss: 0.0029514513129575387\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018420384669469464\n",
      "Average test loss: 0.003337848607657684\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018425817052523295\n",
      "Average test loss: 0.0030203451694299777\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018431307678421337\n",
      "Average test loss: 0.003035550541140967\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01838067055079672\n",
      "Average test loss: 0.0030722298148191636\n",
      "Epoch 194/300\n",
      "Average training loss: 0.018414343221320045\n",
      "Average test loss: 0.0030318063563770717\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01841838534673055\n",
      "Average test loss: 0.0029808988188289934\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018358630432022943\n",
      "Average test loss: 0.0030873153607050576\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018391162359052235\n",
      "Average test loss: 0.0030345905661169027\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01836001908944713\n",
      "Average test loss: 0.003037255864176485\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018354268697400887\n",
      "Average test loss: 0.003010745173320174\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01833436820904414\n",
      "Average test loss: 0.0031808023870819146\n",
      "Epoch 201/300\n",
      "Average training loss: 0.018304289132356644\n",
      "Average test loss: 0.0030681134932157065\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018332063789168993\n",
      "Average test loss: 0.003209089138855537\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018285381356875102\n",
      "Average test loss: 0.003323391702026129\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01827345445752144\n",
      "Average test loss: 0.003093863242564516\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018344227572282157\n",
      "Average test loss: 0.0031201055660429926\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01828598337703281\n",
      "Average test loss: 0.0030951907662674786\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018254502789841757\n",
      "Average test loss: 0.003085507318170534\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018298642117116185\n",
      "Average test loss: 0.003054385217320588\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018258374849955242\n",
      "Average test loss: 0.00329226905769772\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018228055430783166\n",
      "Average test loss: 0.0029618040886190203\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018225680943992403\n",
      "Average test loss: 0.0030986944259040884\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018238476634853416\n",
      "Average test loss: 0.0031197736207395793\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018185776692297723\n",
      "Average test loss: 0.0031504181186772055\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018238870425356758\n",
      "Average test loss: 0.0031903967501388654\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01823242912027571\n",
      "Average test loss: 0.0030046195652749805\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018192872615324127\n",
      "Average test loss: 0.0031002926013122003\n",
      "Epoch 217/300\n",
      "Average training loss: 0.018223671823740007\n",
      "Average test loss: 0.003042823249060247\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018205429823862182\n",
      "Average test loss: 0.0032175365270425875\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018193315178155898\n",
      "Average test loss: 0.0030586584742284483\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018140712570812966\n",
      "Average test loss: 0.0030067222906897464\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018141922435826727\n",
      "Average test loss: 0.0033141282465722825\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018167635414335463\n",
      "Average test loss: 0.00304490476494862\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018175237195359337\n",
      "Average test loss: 0.0032222334469358127\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018151357856889566\n",
      "Average test loss: 0.0033813871275633575\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018148578839169607\n",
      "Average test loss: 0.00323537729980631\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01812416331966718\n",
      "Average test loss: 0.0031425822166105113\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01809366665283839\n",
      "Average test loss: 0.003002312626896633\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018128980379137728\n",
      "Average test loss: 0.0031918052047904995\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018166955166392856\n",
      "Average test loss: 0.0030519217232035266\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018100707929995324\n",
      "Average test loss: 0.003002383140226205\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018070993478099505\n",
      "Average test loss: 0.0033377494876169495\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01808349130137099\n",
      "Average test loss: 0.003210267966095772\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018088415581319066\n",
      "Average test loss: 0.0030857196818623277\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018056440287166173\n",
      "Average test loss: 0.0030271322131156923\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01807735209001435\n",
      "Average test loss: 0.0031020812938610713\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018091908910208277\n",
      "Average test loss: 0.002985972508788109\n",
      "Epoch 237/300\n",
      "Average training loss: 0.018037904482748772\n",
      "Average test loss: 0.0030382014479902056\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01804991587003072\n",
      "Average test loss: 0.0030883678067475558\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018022494435310364\n",
      "Average test loss: 0.0030160990431904794\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018036179160078367\n",
      "Average test loss: 0.0031736729184372557\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01801510664820671\n",
      "Average test loss: 0.003056411579872171\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018051511724789936\n",
      "Average test loss: 0.003049796787608001\n",
      "Epoch 243/300\n",
      "Average training loss: 0.018048094986213577\n",
      "Average test loss: 0.0032031243360704847\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018006057783133453\n",
      "Average test loss: 0.003175231220614579\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0180602256125874\n",
      "Average test loss: 0.0030638839383092193\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017966799853576555\n",
      "Average test loss: 0.0032449048097348875\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018000689496596653\n",
      "Average test loss: 0.0030490390008522405\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017960442490047877\n",
      "Average test loss: 0.0030308135137375857\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01800982555250327\n",
      "Average test loss: 0.003029005970598923\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017955472596817547\n",
      "Average test loss: 0.0030750050764116977\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017968350077668827\n",
      "Average test loss: 0.002989033932901091\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017944718362556563\n",
      "Average test loss: 0.0030675120279192924\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017929883284701242\n",
      "Average test loss: 0.0030789193660020826\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01797209644648764\n",
      "Average test loss: 0.0030951713776836793\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01798452963762813\n",
      "Average test loss: 0.0031624834595455065\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017927815203037527\n",
      "Average test loss: 0.0031101876166131763\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01795831769373682\n",
      "Average test loss: 0.0031700582177274757\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017916019219491215\n",
      "Average test loss: 0.0031731362026184797\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017983277660277155\n",
      "Average test loss: 0.0031701427155898677\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01791984813494815\n",
      "Average test loss: 0.003178751985128555\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017892139433158767\n",
      "Average test loss: 0.0030779094874031016\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01793214656329817\n",
      "Average test loss: 0.003300787076767948\n",
      "Epoch 263/300\n",
      "Average training loss: 0.017924078000916374\n",
      "Average test loss: 0.003034441431570384\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017890676009986135\n",
      "Average test loss: 0.003211797954928544\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01791251496142811\n",
      "Average test loss: 0.0030721595214886796\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018095230004853673\n",
      "Average test loss: 0.0030801935934772094\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01786718486249447\n",
      "Average test loss: 0.0030704565178602933\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017884775640236005\n",
      "Average test loss: 0.00303290052111778\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017852074725760354\n",
      "Average test loss: 0.0032683127050598462\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017858980001674758\n",
      "Average test loss: 0.004277284296850363\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01786878143913216\n",
      "Average test loss: 0.0030830218580861885\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017819361514515346\n",
      "Average test loss: 0.0030654517221781943\n",
      "Epoch 273/300\n",
      "Average training loss: 0.017803422775533463\n",
      "Average test loss: 0.003143139620208078\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01783673213587867\n",
      "Average test loss: 0.003056252799514267\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017811969263686073\n",
      "Average test loss: 0.003203173131785459\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01784916557206048\n",
      "Average test loss: 0.0031284205669330225\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017814669898814625\n",
      "Average test loss: 0.003130810020491481\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0178234885931015\n",
      "Average test loss: 0.0030588546117974653\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017805106611715422\n",
      "Average test loss: 0.003070838165572948\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01782077841626273\n",
      "Average test loss: 0.0031716190756609043\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017798520915210246\n",
      "Average test loss: 0.0030797523342900807\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01779546453969346\n",
      "Average test loss: 0.003081687048077583\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017816156971785756\n",
      "Average test loss: 0.003219819704691569\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01775722388840384\n",
      "Average test loss: 0.003176044954193963\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01784849850833416\n",
      "Average test loss: 0.0031784416004601453\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017762312054634095\n",
      "Average test loss: 0.0030945996946344775\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01775521671440866\n",
      "Average test loss: 0.004051109468771351\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017763217504653667\n",
      "Average test loss: 0.0030464733242988587\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01773706189543009\n",
      "Average test loss: 0.0030889956610691215\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01777988818950123\n",
      "Average test loss: 0.0031074877956675157\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01776905976732572\n",
      "Average test loss: 0.0030646074650188286\n",
      "Epoch 292/300\n",
      "Average training loss: 0.017755422958069377\n",
      "Average test loss: 0.0031035903826769854\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017726224765181542\n",
      "Average test loss: 0.0031568023639006746\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017740488035811318\n",
      "Average test loss: 0.003165994133386347\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01772009234295951\n",
      "Average test loss: 0.0031267182607617643\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017708766584595043\n",
      "Average test loss: 0.003154301719326112\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017722749718361432\n",
      "Average test loss: 0.003272073407553964\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017680302393105293\n",
      "Average test loss: 0.003086382842105296\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01777151650438706\n",
      "Average test loss: 0.0031974614655805957\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017691127793656454\n",
      "Average test loss: 0.003057057116180658\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.24728332744704354\n",
      "Average test loss: 0.007814933287186755\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0889767571091652\n",
      "Average test loss: 0.0052753294280005825\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06790766104062398\n",
      "Average test loss: 0.0058608177863061425\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05702378281619814\n",
      "Average test loss: 0.004289940786444479\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04985392520825068\n",
      "Average test loss: 0.004097972444569071\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04503286008702384\n",
      "Average test loss: 0.009433531533512804\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04133997210197979\n",
      "Average test loss: 0.003932293663215306\n",
      "Epoch 8/300\n",
      "Average training loss: 0.038778916246361206\n",
      "Average test loss: 0.009994294303986762\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03618875919116868\n",
      "Average test loss: 0.003433386469259858\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03413773945967356\n",
      "Average test loss: 0.003676007105865412\n",
      "Epoch 11/300\n",
      "Average training loss: 0.031844982309473885\n",
      "Average test loss: 0.00336787848547101\n",
      "Epoch 12/300\n",
      "Average training loss: 0.030502145931124686\n",
      "Average test loss: 0.00764146362659004\n",
      "Epoch 13/300\n",
      "Average training loss: 0.028850567324293985\n",
      "Average test loss: 0.0038515379656520153\n",
      "Epoch 14/300\n",
      "Average training loss: 0.027601965955562063\n",
      "Average test loss: 0.0028939450850917234\n",
      "Epoch 15/300\n",
      "Average training loss: 0.026294360750251346\n",
      "Average test loss: 0.0030469488391859662\n",
      "Epoch 16/300\n",
      "Average training loss: 0.025486216525236766\n",
      "Average test loss: 0.003115791661871804\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024482836564381917\n",
      "Average test loss: 0.003219064136967063\n",
      "Epoch 18/300\n",
      "Average training loss: 0.023760374433464473\n",
      "Average test loss: 0.0027791164678831895\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0230626399550173\n",
      "Average test loss: 0.0026379992651442688\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022374044944842658\n",
      "Average test loss: 0.0033160795217586888\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02207577786511845\n",
      "Average test loss: 0.0025487641875321667\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021599239019884003\n",
      "Average test loss: 0.0032339869249198173\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02116980986462699\n",
      "Average test loss: 0.008527458974884616\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020929807068573104\n",
      "Average test loss: 0.0030975785586569043\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020493482124474313\n",
      "Average test loss: 0.002321645840174622\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020191667821672227\n",
      "Average test loss: 0.003502583388860027\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019909995277722678\n",
      "Average test loss: 0.0023317789836890166\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01971664712164137\n",
      "Average test loss: 0.002239117919260429\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01954839353925652\n",
      "Average test loss: 0.002675929030610455\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01924507908357514\n",
      "Average test loss: 0.002232946181669831\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01914826285507944\n",
      "Average test loss: 0.0022213175915595557\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018923708665702078\n",
      "Average test loss: 0.0025587864002833763\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018698382001784114\n",
      "Average test loss: 0.0025900953347898193\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018541563737723563\n",
      "Average test loss: 0.002370678142334024\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018494816995329327\n",
      "Average test loss: 0.0021462139640417363\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018328764584329395\n",
      "Average test loss: 0.002261135807674792\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018310151444541085\n",
      "Average test loss: 0.0022152878548949958\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018148356071776815\n",
      "Average test loss: 0.0022537049458672604\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018050625198417238\n",
      "Average test loss: 0.002141130055921773\n",
      "Epoch 40/300\n",
      "Average training loss: 0.017945857574542364\n",
      "Average test loss: 0.002688316520510448\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01777555863890383\n",
      "Average test loss: 0.0023071452675180303\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017761733804312018\n",
      "Average test loss: 0.002216359823528263\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01770817586614026\n",
      "Average test loss: 0.0021328988001785344\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017550945629676185\n",
      "Average test loss: 0.002073452271728052\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01748544596383969\n",
      "Average test loss: 0.0023212500129722886\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0174744416533245\n",
      "Average test loss: 0.0022198638877727918\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017329103355606397\n",
      "Average test loss: 0.0021190781750612788\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01722838885585467\n",
      "Average test loss: 0.002574148784702023\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01721789110534721\n",
      "Average test loss: 0.002169136525442203\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01716447555190987\n",
      "Average test loss: 0.0020373151623126534\n",
      "Epoch 51/300\n",
      "Average training loss: 0.017059274137847955\n",
      "Average test loss: 0.0020991978699134457\n",
      "Epoch 52/300\n",
      "Average training loss: 0.017026308693819575\n",
      "Average test loss: 0.0021521448586136103\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016975062559048334\n",
      "Average test loss: 0.0020156667129033142\n",
      "Epoch 54/300\n",
      "Average training loss: 0.016898079368803238\n",
      "Average test loss: 0.0020728003229531977\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016852189040018454\n",
      "Average test loss: 0.0020308041745382877\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016782550556792154\n",
      "Average test loss: 0.0020384166884339517\n",
      "Epoch 57/300\n",
      "Average training loss: 0.016724237345159054\n",
      "Average test loss: 0.002100930109516614\n",
      "Epoch 58/300\n",
      "Average training loss: 0.016659579081667795\n",
      "Average test loss: 0.0021122737201965517\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01659223564134704\n",
      "Average test loss: 0.0020757752207832205\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016577652228375275\n",
      "Average test loss: 0.0021029037931520078\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016589634967346986\n",
      "Average test loss: 0.0020169364822407564\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016573422234919335\n",
      "Average test loss: 0.002038087871753507\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016426511075761582\n",
      "Average test loss: 0.00199727102348374\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016398173451423644\n",
      "Average test loss: 0.0020689950852344433\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01632904084854656\n",
      "Average test loss: 0.0021088710619757574\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01633311666796605\n",
      "Average test loss: 0.0021333743011992838\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01629450979994403\n",
      "Average test loss: 0.0020068095304692783\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016231935845481025\n",
      "Average test loss: 0.002254749329139789\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016245078374942143\n",
      "Average test loss: 0.0019729857415788704\n",
      "Epoch 70/300\n",
      "Average training loss: 0.016231614781750572\n",
      "Average test loss: 0.002003043629229069\n",
      "Epoch 71/300\n",
      "Average training loss: 0.016095967003868686\n",
      "Average test loss: 0.0020202151618690955\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01607006536093023\n",
      "Average test loss: 0.002046851256862283\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016057795779572594\n",
      "Average test loss: 0.0019912945731646483\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016046573943561977\n",
      "Average test loss: 0.002335176915137304\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01603352323008908\n",
      "Average test loss: 0.002008624931073023\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0159608420100477\n",
      "Average test loss: 0.0020404881652858523\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01596086011495855\n",
      "Average test loss: 0.0020287151456707055\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01596356578833527\n",
      "Average test loss: 0.002062315509447621\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0159683781531122\n",
      "Average test loss: 0.0019619326546995177\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01582480901810858\n",
      "Average test loss: 0.0019969227171192567\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015834540709853174\n",
      "Average test loss: 0.0021298166623132096\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015786937028997475\n",
      "Average test loss: 0.002222288374375138\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01577549807065063\n",
      "Average test loss: 0.0020268975641164516\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01573996155626244\n",
      "Average test loss: 0.001967523862297336\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015720293149352072\n",
      "Average test loss: 0.0019285762121693955\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015668458281291854\n",
      "Average test loss: 0.001968897341233161\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015716589071684415\n",
      "Average test loss: 0.002077299581426713\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015628424876266055\n",
      "Average test loss: 0.001968081972251336\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015627951400975386\n",
      "Average test loss: 0.0019925646297633648\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01557118170708418\n",
      "Average test loss: 0.0019764698875240154\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015542440000507567\n",
      "Average test loss: 0.001972045367066231\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015528782818052504\n",
      "Average test loss: 0.0020650410138898425\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015517739024427202\n",
      "Average test loss: 0.0019374318700283765\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015488727726870113\n",
      "Average test loss: 0.0020395859630985393\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015455707357989417\n",
      "Average test loss: 0.0020139963236740894\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015517015287445651\n",
      "Average test loss: 0.0020168828995277485\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0154192107518514\n",
      "Average test loss: 0.0019386485477702485\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015414582165579001\n",
      "Average test loss: 0.00348055129022234\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015447517363561524\n",
      "Average test loss: 0.0022602193103068405\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015372426273922126\n",
      "Average test loss: 0.002234174173532261\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015395142546130551\n",
      "Average test loss: 0.0022367307890413537\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015343141881128152\n",
      "Average test loss: 0.0019532897704177435\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01528599774837494\n",
      "Average test loss: 0.0020092772908715736\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015289609471129046\n",
      "Average test loss: 0.00198423594986606\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015271219063136313\n",
      "Average test loss: 0.0019615514162513946\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015271308503217167\n",
      "Average test loss: 0.001960716424923804\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015285475744141472\n",
      "Average test loss: 0.0019540230330700676\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015201393104261822\n",
      "Average test loss: 0.001963501219948133\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015249486843744914\n",
      "Average test loss: 0.0019907020389412842\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015170317662258943\n",
      "Average test loss: 0.002366494317849477\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015181817492677106\n",
      "Average test loss: 0.0021347911736617486\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015184702125688393\n",
      "Average test loss: 0.001967940266761515\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015198056474328042\n",
      "Average test loss: 0.002033584628875057\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015129406872722838\n",
      "Average test loss: 0.0020448099865267673\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01507574435737398\n",
      "Average test loss: 0.0019901792488785253\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015100398442811436\n",
      "Average test loss: 0.001970054235516323\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015079836606151528\n",
      "Average test loss: 0.0020869341087010173\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015046302146381802\n",
      "Average test loss: 0.0019878376739927463\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015042277383307616\n",
      "Average test loss: 0.0020216349065303803\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015001307082672914\n",
      "Average test loss: 0.0020978418863895868\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015022490745617284\n",
      "Average test loss: 0.0019893590373297534\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015013398329416911\n",
      "Average test loss: 0.001955768291321066\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015010004139608808\n",
      "Average test loss: 0.00197999588224209\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014958374641007847\n",
      "Average test loss: 0.0019793078700701397\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014916931678851445\n",
      "Average test loss: 0.0020051962722920707\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014950305773152246\n",
      "Average test loss: 0.002082136707380414\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014910648581054476\n",
      "Average test loss: 0.0020266787531889145\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014914235381616486\n",
      "Average test loss: 0.0019682469757066833\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014914698659545846\n",
      "Average test loss: 0.0020167516509277954\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014926507665051355\n",
      "Average test loss: 0.001975608532110022\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014863773754901357\n",
      "Average test loss: 0.0019955614511337543\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014830950429042181\n",
      "Average test loss: 0.0019953040337810915\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0148760495243801\n",
      "Average test loss: 0.002014108631449441\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01486718945701917\n",
      "Average test loss: 0.0020273260666678347\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014821791739927398\n",
      "Average test loss: 0.0020372290244946877\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014813691957129373\n",
      "Average test loss: 0.002013571515161958\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014800699565145705\n",
      "Average test loss: 0.0020124422732947602\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014803437536789312\n",
      "Average test loss: 0.0020259542032662367\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01476873134324948\n",
      "Average test loss: 0.0020399341403196255\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014777594825459851\n",
      "Average test loss: 0.0020488562635663484\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014768117782142428\n",
      "Average test loss: 0.0019944245521393085\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014720575022200743\n",
      "Average test loss: 0.0019411672082626156\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014773333470854494\n",
      "Average test loss: 0.0019218513547546335\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014712509066694312\n",
      "Average test loss: 0.002002408713930183\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014714860817624463\n",
      "Average test loss: 0.002067790144019657\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014715298336413172\n",
      "Average test loss: 0.0019412293365846077\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014681452132347558\n",
      "Average test loss: 0.0020579053099370665\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014696090002854665\n",
      "Average test loss: 0.0021082066522083347\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014656058692269856\n",
      "Average test loss: 0.002018005285619034\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014637073213027584\n",
      "Average test loss: 0.00198313059254239\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014632560441891353\n",
      "Average test loss: 0.002055395572135846\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014648049010998672\n",
      "Average test loss: 0.0038356698842512238\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01462590179923508\n",
      "Average test loss: 0.001955887805360059\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014587479864557584\n",
      "Average test loss: 0.00206223088533928\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01457424695789814\n",
      "Average test loss: 0.001988347573205829\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014606666115423043\n",
      "Average test loss: 0.0019982428374803727\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01454309860120217\n",
      "Average test loss: 0.002027103431108925\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01461016184422705\n",
      "Average test loss: 0.002037512567722135\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014550146022604571\n",
      "Average test loss: 0.0020063362792134285\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014545799573262533\n",
      "Average test loss: 0.0020322605240572656\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01453974846088224\n",
      "Average test loss: 0.002169979720272952\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014667956443296538\n",
      "Average test loss: 0.002036720844399598\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014549088407721785\n",
      "Average test loss: 0.001989849561618434\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014461227867338392\n",
      "Average test loss: 0.002145660922345188\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014523169173134698\n",
      "Average test loss: 0.001971061435010698\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014521060233314832\n",
      "Average test loss: 0.0019898225222714245\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014476529493100113\n",
      "Average test loss: 0.002027343467498819\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01450057456890742\n",
      "Average test loss: 0.0023099276637658477\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01454556175155772\n",
      "Average test loss: 0.002287472505329384\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014440154055754344\n",
      "Average test loss: 0.002006155959020058\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014457672985891502\n",
      "Average test loss: 0.0020417656537352336\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014436888029177983\n",
      "Average test loss: 0.001947074566450384\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014447639383375645\n",
      "Average test loss: 0.001961604217067361\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014419531104465326\n",
      "Average test loss: 0.0020158548878712788\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014400030800037914\n",
      "Average test loss: 0.0021029597005496423\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014444826094640626\n",
      "Average test loss: 0.0019850327555711073\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01438287430256605\n",
      "Average test loss: 0.001959191309960766\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014427583922114637\n",
      "Average test loss: 0.002127329137797157\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0143570730711023\n",
      "Average test loss: 0.002040961405572792\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014368181658287843\n",
      "Average test loss: 0.0021365659076513516\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014380310907959938\n",
      "Average test loss: 0.0019985056964473592\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014359151952796512\n",
      "Average test loss: 0.0020573959269871316\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014330140428410637\n",
      "Average test loss: 0.0021551266704789465\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014380272320575184\n",
      "Average test loss: 0.0020878432544155254\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014356623088320097\n",
      "Average test loss: 0.0019950482230116097\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014317815121677187\n",
      "Average test loss: 0.001998037072415981\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014325200909955633\n",
      "Average test loss: 0.002314147406982051\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01430069793181287\n",
      "Average test loss: 0.002019375369677113\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014330428633424971\n",
      "Average test loss: 0.0022551937788310977\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014286893383496337\n",
      "Average test loss: 0.002010251861479547\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014290993957883781\n",
      "Average test loss: 0.00200370089430362\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014312325875792238\n",
      "Average test loss: 0.001978495325168802\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01427159426940812\n",
      "Average test loss: 0.002122250486372246\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014274226063655483\n",
      "Average test loss: 0.002004203525889251\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014304634355836444\n",
      "Average test loss: 0.0019855947663179703\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014252554641829597\n",
      "Average test loss: 0.0020065746279433368\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01425862684018082\n",
      "Average test loss: 0.002021178388140268\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014226203265289466\n",
      "Average test loss: 0.0020449273727006383\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01421616498215331\n",
      "Average test loss: 0.001974480999012788\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014222643372913202\n",
      "Average test loss: 0.0020458128827934466\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014227434984511799\n",
      "Average test loss: 0.001994804097753432\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01420774207264185\n",
      "Average test loss: 0.002033350217052632\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014186430348290337\n",
      "Average test loss: 0.002034435894754198\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014266672623654206\n",
      "Average test loss: 0.0020228347619995475\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014215975360737907\n",
      "Average test loss: 0.002087030722345743\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014180355745885108\n",
      "Average test loss: 0.0020027728648856283\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014153802393211259\n",
      "Average test loss: 0.0022449165597144102\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014161497371892134\n",
      "Average test loss: 0.0019986530316786636\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0141711420789361\n",
      "Average test loss: 0.0020284316442492935\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014144244563248422\n",
      "Average test loss: 0.0020400869444840482\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014168353610568576\n",
      "Average test loss: 0.001998669381926043\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014197481763031747\n",
      "Average test loss: 0.0019587149924288195\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014151942691041364\n",
      "Average test loss: 0.001993019494641986\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014132565567890804\n",
      "Average test loss: 0.00213140601468169\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014146618330644237\n",
      "Average test loss: 0.0021377681170900663\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014130140456060569\n",
      "Average test loss: 0.002121472479568587\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014217135768797663\n",
      "Average test loss: 0.0019813813778261345\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014110933773219585\n",
      "Average test loss: 0.0019639995396137236\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014089015135334598\n",
      "Average test loss: 0.002083680739108887\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014113470628857613\n",
      "Average test loss: 0.002080813588367568\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01408934385743406\n",
      "Average test loss: 0.0021766655787618626\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014101317102710407\n",
      "Average test loss: 0.0023085095234629182\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014097587179806498\n",
      "Average test loss: 0.0020039318205995692\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014088616749478712\n",
      "Average test loss: 0.002012988640823298\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014062821806304984\n",
      "Average test loss: 0.002050763306621876\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014049795280728075\n",
      "Average test loss: 0.0021304123364388945\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014139960622621907\n",
      "Average test loss: 0.0022009547220336067\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014045260056853295\n",
      "Average test loss: 0.0020742892602251635\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014034947086539533\n",
      "Average test loss: 0.0020206122661216392\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014069896096984546\n",
      "Average test loss: 0.002182335092789597\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014032990893556013\n",
      "Average test loss: 0.0020036832162489493\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01405390199025472\n",
      "Average test loss: 0.0020149064577288096\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014038012663523357\n",
      "Average test loss: 0.002138203272285561\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014003594674997859\n",
      "Average test loss: 0.002173323428051339\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014009300434754955\n",
      "Average test loss: 0.0021975293868324824\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014015326145622465\n",
      "Average test loss: 0.0020000906369338433\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013990982570581966\n",
      "Average test loss: 0.0020642143125749295\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014014271984497706\n",
      "Average test loss: 0.0020132731835668287\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013978156191607317\n",
      "Average test loss: 0.002065528577607539\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01396290056904157\n",
      "Average test loss: 0.0020391127365744777\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01402931098226044\n",
      "Average test loss: 0.002058645428261823\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01396380359182755\n",
      "Average test loss: 0.0020571374013606046\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013974480650491184\n",
      "Average test loss: 0.002020341893347601\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013978352852165699\n",
      "Average test loss: 0.002001751776991619\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013967773116297192\n",
      "Average test loss: 0.002087964478880167\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013977331327067482\n",
      "Average test loss: 0.0021055187763025364\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013972005672752858\n",
      "Average test loss: 0.002000119518281685\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013977094213995669\n",
      "Average test loss: 0.0020687377853319047\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0139496488140689\n",
      "Average test loss: 0.0020176822306174373\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013948352721002366\n",
      "Average test loss: 0.002108587529924181\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01391836729976866\n",
      "Average test loss: 0.002079151474663781\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013907781632410155\n",
      "Average test loss: 0.00208073445275012\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013924737218353484\n",
      "Average test loss: 0.002033864882050289\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013953840532236628\n",
      "Average test loss: 0.0020338755761169724\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013917515008813806\n",
      "Average test loss: 0.0020218997883299987\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01389850232253472\n",
      "Average test loss: 0.002106731419865456\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013920050824681918\n",
      "Average test loss: 0.002052627082810634\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013911684778829416\n",
      "Average test loss: 0.0020599915146206817\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013884701112906139\n",
      "Average test loss: 0.0020587865846852463\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013875096957716677\n",
      "Average test loss: 0.0020729555987442534\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013878151284323799\n",
      "Average test loss: 0.002083714477924837\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01389845629201995\n",
      "Average test loss: 0.002075765471284588\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013893596562246481\n",
      "Average test loss: 0.0020161975911921924\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013867528509762552\n",
      "Average test loss: 0.0020327248193530573\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013851423093842135\n",
      "Average test loss: 0.0020609007146623398\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0138716671615839\n",
      "Average test loss: 0.0020877413526177407\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01383323465867175\n",
      "Average test loss: 0.0021467409752723246\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013862150165769789\n",
      "Average test loss: 0.0020528108031592435\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013836568737195598\n",
      "Average test loss: 0.0021099920119676324\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013898864311476548\n",
      "Average test loss: 0.0020124632661334342\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013855920134319199\n",
      "Average test loss: 0.0019979510764694874\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013866425183912118\n",
      "Average test loss: 0.0020982911828905343\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013796558189723227\n",
      "Average test loss: 0.0020130413243961004\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01382826136963235\n",
      "Average test loss: 0.0020625261639555296\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013862168084416124\n",
      "Average test loss: 0.002172677672778567\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013836767768694296\n",
      "Average test loss: 0.0020272142993700173\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013823707170784473\n",
      "Average test loss: 0.002058842615224421\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013839661055968868\n",
      "Average test loss: 0.001997323460669981\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013768778112199571\n",
      "Average test loss: 0.002093799462645418\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013824276111192173\n",
      "Average test loss: 0.002037682865228918\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013805092030929195\n",
      "Average test loss: 0.002165287488864528\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013813528573347462\n",
      "Average test loss: 0.0021509558393930394\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013818811163306237\n",
      "Average test loss: 0.0020294955575631724\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013755949261287849\n",
      "Average test loss: 0.0020786718900005024\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013811343883474669\n",
      "Average test loss: 0.0020981422452670006\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013803180693752236\n",
      "Average test loss: 0.0022096025093148153\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013763475315438376\n",
      "Average test loss: 0.002046378324015273\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013762786113553577\n",
      "Average test loss: 0.002086392157504128\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013802142374217511\n",
      "Average test loss: 0.002065522054831187\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013768199539846844\n",
      "Average test loss: 0.0020157852036257586\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01377522736042738\n",
      "Average test loss: 0.002215004564780328\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01374777726083994\n",
      "Average test loss: 0.0020938829738232825\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013781012693213092\n",
      "Average test loss: 0.002002466334030032\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01375245576683018\n",
      "Average test loss: 0.0020553111866013045\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013769964622954527\n",
      "Average test loss: 0.002025143757359021\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013754213891095586\n",
      "Average test loss: 0.00225521236182087\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013762258622381422\n",
      "Average test loss: 0.0021655696077893176\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01382595445215702\n",
      "Average test loss: 0.002047385203755564\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013744438895748721\n",
      "Average test loss: 0.0021680543904917106\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013720025166869163\n",
      "Average test loss: 0.002123977792966697\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.23471612254778543\n",
      "Average test loss: 0.007240096952352259\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08313262308306164\n",
      "Average test loss: 0.008114257398578855\n",
      "Epoch 3/300\n",
      "Average training loss: 0.062230538454320694\n",
      "Average test loss: 0.0036570106322566668\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05107721742987633\n",
      "Average test loss: 0.0032417698837816714\n",
      "Epoch 5/300\n",
      "Average training loss: 0.044838906549745136\n",
      "Average test loss: 0.0035549149682952298\n",
      "Epoch 6/300\n",
      "Average training loss: 0.039575510167413286\n",
      "Average test loss: 0.0031315231300476525\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03628848132159975\n",
      "Average test loss: 0.002853814894850883\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0330727293872171\n",
      "Average test loss: 0.11888179131348928\n",
      "Epoch 9/300\n",
      "Average training loss: 0.030559748235676025\n",
      "Average test loss: 0.0026073148085011375\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02917963072988722\n",
      "Average test loss: 0.0038275779547790687\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027271587916546398\n",
      "Average test loss: 0.0024078204315155743\n",
      "Epoch 12/300\n",
      "Average training loss: 0.025935434151026938\n",
      "Average test loss: 0.0023465014691982002\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024600558618704478\n",
      "Average test loss: 0.19459402490986719\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023347432184550498\n",
      "Average test loss: 0.0025392853634224996\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022319944587018754\n",
      "Average test loss: 0.002158497672321068\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021389154235521953\n",
      "Average test loss: 0.0020506892355365886\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02070866252068016\n",
      "Average test loss: 0.0021077314965013\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020025554423530895\n",
      "Average test loss: 0.0023724857390754754\n",
      "Epoch 19/300\n",
      "Average training loss: 0.019391880705952644\n",
      "Average test loss: 0.0019506596084684134\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01910693819158607\n",
      "Average test loss: 0.0018588460348546506\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018458865060574477\n",
      "Average test loss: 0.0028697017156001595\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018226684403088358\n",
      "Average test loss: 0.0018925038785156277\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017715370812349848\n",
      "Average test loss: 0.0022551800400639572\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01735984583861298\n",
      "Average test loss: 0.0019281597860778372\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01713789903289742\n",
      "Average test loss: 0.001741067152056429\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0169457063757711\n",
      "Average test loss: 0.0016707918130689197\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01672705660180913\n",
      "Average test loss: 0.001824644537228677\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016550459146499635\n",
      "Average test loss: 0.0021184630015244088\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016250741784771282\n",
      "Average test loss: 0.0017444821041491297\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01609889565159877\n",
      "Average test loss: 0.001684166652046972\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015947395332985455\n",
      "Average test loss: 0.0019450626667175028\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015826939488450688\n",
      "Average test loss: 0.0019876453678848013\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015654961970945198\n",
      "Average test loss: 0.001588199232911898\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015560433208114571\n",
      "Average test loss: 0.0019126171291702324\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015434614466296301\n",
      "Average test loss: 0.001569427155268689\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015285945714347892\n",
      "Average test loss: 0.0015230358284380702\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015240731383363406\n",
      "Average test loss: 0.0016206697016540501\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015075713449054295\n",
      "Average test loss: 0.001564346432996293\n",
      "Epoch 39/300\n",
      "Average training loss: 0.014979193061590195\n",
      "Average test loss: 0.00165921827716132\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014860885474416944\n",
      "Average test loss: 0.0016907523173011012\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014848115825818645\n",
      "Average test loss: 0.0016974217543481952\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01478866084996197\n",
      "Average test loss: 0.0014696006175234087\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014644749959309896\n",
      "Average test loss: 0.0024985986224686104\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014625551590489016\n",
      "Average test loss: 0.0015160986760424243\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014507300285001596\n",
      "Average test loss: 0.0015981456086867386\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014479224375552602\n",
      "Average test loss: 0.0014905000897124409\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014416421875357627\n",
      "Average test loss: 0.001513114900328219\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01431486304021544\n",
      "Average test loss: 0.0014566743049977554\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014236006818711757\n",
      "Average test loss: 0.001485628381371498\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014198821660545138\n",
      "Average test loss: 0.001644455597528981\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014151479599376519\n",
      "Average test loss: 0.002078508623254796\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0141188625825776\n",
      "Average test loss: 0.0014926293726182647\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014060575523310238\n",
      "Average test loss: 0.00442593181754152\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014000703648560577\n",
      "Average test loss: 0.0014986638777578871\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013935716509819032\n",
      "Average test loss: 0.0016038107539837559\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013860457222494814\n",
      "Average test loss: 0.0017020958406436775\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013838442609541946\n",
      "Average test loss: 0.0014787776921358373\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013817218239936563\n",
      "Average test loss: 0.0015131455650553107\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013740370752910774\n",
      "Average test loss: 0.0015893833724678391\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01373720008548763\n",
      "Average test loss: 0.0014443662511702212\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013651507614387406\n",
      "Average test loss: 0.0017498784959316253\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013629929523501132\n",
      "Average test loss: 0.0015945863319146965\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013600189450714324\n",
      "Average test loss: 0.0014312841517126395\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013539572309288714\n",
      "Average test loss: 0.0016014298677651418\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013514722998771402\n",
      "Average test loss: 0.001418148148390982\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013518202876051266\n",
      "Average test loss: 0.0014272561605191893\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013442748769289917\n",
      "Average test loss: 0.001434258539798773\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013395638996528255\n",
      "Average test loss: 0.0014770999555993411\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013401176316042741\n",
      "Average test loss: 0.0014418702992714113\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013349103163513872\n",
      "Average test loss: 0.0014871566873043776\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013328628310726749\n",
      "Average test loss: 0.0013993659646043346\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013284107198317846\n",
      "Average test loss: 0.0014191910350281332\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013241220716800955\n",
      "Average test loss: 0.001409089544477562\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013241413708362314\n",
      "Average test loss: 0.0014151282909636697\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013211105992396673\n",
      "Average test loss: 0.0015126363892729083\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0131589591105779\n",
      "Average test loss: 0.0017833559822498096\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013141750380396843\n",
      "Average test loss: 0.0014824819437538584\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013105537661247783\n",
      "Average test loss: 0.0014589952723019653\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013121444924838013\n",
      "Average test loss: 0.0014067832785141137\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013062885619699954\n",
      "Average test loss: 0.0014634338081296948\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013084555808040832\n",
      "Average test loss: 0.001522727763487233\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01301900733759006\n",
      "Average test loss: 0.0014639595529685418\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0129996417760849\n",
      "Average test loss: 0.0014405756725205316\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013020917585326565\n",
      "Average test loss: 0.001529218450602558\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012960755059288608\n",
      "Average test loss: 0.00140148309390578\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012937168476482233\n",
      "Average test loss: 0.001458828629925847\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012913033163381948\n",
      "Average test loss: 0.002666583988712066\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012905697598225541\n",
      "Average test loss: 0.0016042345591510336\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012912695632212692\n",
      "Average test loss: 0.001446321134455502\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012862008217308256\n",
      "Average test loss: 0.0014745936513257524\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01282737830032905\n",
      "Average test loss: 0.001445435917418864\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01280187031875054\n",
      "Average test loss: 0.0014225353622912534\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012797562146352397\n",
      "Average test loss: 0.0014007128701648778\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012751578968018294\n",
      "Average test loss: 0.0016461226678349905\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012785857500301466\n",
      "Average test loss: 0.0014202937466195888\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012793597841428386\n",
      "Average test loss: 0.0014405046169542605\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012706418997711605\n",
      "Average test loss: 0.0015818559591554934\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012705804027616978\n",
      "Average test loss: 0.0014136050618253648\n",
      "Epoch 99/300\n",
      "Average training loss: 0.012697848697503409\n",
      "Average test loss: 0.0014229754195031192\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01264712736921178\n",
      "Average test loss: 0.0013972217671366202\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012668830561141172\n",
      "Average test loss: 0.0014081964784612257\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012627103914817174\n",
      "Average test loss: 0.0014607069967314602\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012635736748576164\n",
      "Average test loss: 0.0014141373485326767\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012619559105899599\n",
      "Average test loss: 0.0013863749923184515\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012596999395224783\n",
      "Average test loss: 0.001475394788508614\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012591710049245093\n",
      "Average test loss: 0.0014375966663161914\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012579225570791297\n",
      "Average test loss: 0.0014199146301382118\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012556933443579409\n",
      "Average test loss: 0.001481547379348841\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01255409123086267\n",
      "Average test loss: 0.0014776669567864802\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012512364186346532\n",
      "Average test loss: 0.0013888658260305723\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0125239442297154\n",
      "Average test loss: 0.0014742091797913114\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012730906110670832\n",
      "Average test loss: 0.001450489323068824\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012596457596454355\n",
      "Average test loss: 0.0013913981857606107\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012549957072569264\n",
      "Average test loss: 0.0015154301257183154\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01247265495856603\n",
      "Average test loss: 0.0014236474161346754\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012448069391979111\n",
      "Average test loss: 0.0015772872153255674\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012419880631897185\n",
      "Average test loss: 0.0014491637964836426\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01241080887367328\n",
      "Average test loss: 0.0013990063447919157\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012386206438144047\n",
      "Average test loss: 0.0014025225792494084\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012412881305648221\n",
      "Average test loss: 0.0014251724294283325\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012381361139317353\n",
      "Average test loss: 0.0015000957368562619\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012365537023378743\n",
      "Average test loss: 0.0014346456792619492\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01237228508459197\n",
      "Average test loss: 0.0014216771871886319\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012336150414413875\n",
      "Average test loss: 0.0014169482363180982\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012314605974488788\n",
      "Average test loss: 0.0014395254328846932\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012327008433640003\n",
      "Average test loss: 0.0014163185908045205\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01232938242620892\n",
      "Average test loss: 0.001472789957601991\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012277909671266874\n",
      "Average test loss: 0.0014255371227239568\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012268969217108355\n",
      "Average test loss: 0.00141836994410389\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012274725728564792\n",
      "Average test loss: 0.0015085526030096743\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012270030829641555\n",
      "Average test loss: 0.0014007068729648988\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012273026649322775\n",
      "Average test loss: 0.001397447898466554\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012236343952516715\n",
      "Average test loss: 0.00141093732126885\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012226804224981201\n",
      "Average test loss: 0.0015470806187432673\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01224729033311208\n",
      "Average test loss: 0.0014299341284462975\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012199629405306445\n",
      "Average test loss: 0.0014904199317097664\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012234944106804\n",
      "Average test loss: 0.0016113194182722104\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012185515736540158\n",
      "Average test loss: 0.0015310550138561262\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012165809182657136\n",
      "Average test loss: 0.0014659607171391448\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012178763519558641\n",
      "Average test loss: 0.0016169589545784724\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012187399580246873\n",
      "Average test loss: 0.0014398861236663328\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012140026786261134\n",
      "Average test loss: 0.0014148249837259452\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012130861510833105\n",
      "Average test loss: 0.001500741891355978\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012137728140585953\n",
      "Average test loss: 0.0014276082393609815\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012118035926587051\n",
      "Average test loss: 0.0013853368936106562\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01215161786476771\n",
      "Average test loss: 0.0013822176068400344\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012169115498661995\n",
      "Average test loss: 0.001407215935488542\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01208325266920858\n",
      "Average test loss: 0.0013821531019897924\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012048322713209523\n",
      "Average test loss: 0.00143303537580909\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012050543457269669\n",
      "Average test loss: 0.0014358648760244251\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012063999017079672\n",
      "Average test loss: 0.0014741182061326172\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01207301309125291\n",
      "Average test loss: 0.0013970063844074806\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012023598112165928\n",
      "Average test loss: 0.0014269384557588233\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012047069052855174\n",
      "Average test loss: 0.0014160012466212114\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012028729991780387\n",
      "Average test loss: 0.00139577254290796\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012045575382808845\n",
      "Average test loss: 0.0014226525771535105\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01202282652258873\n",
      "Average test loss: 0.001447814772112502\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01199782412002484\n",
      "Average test loss: 0.00143890484318965\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011979462280869484\n",
      "Average test loss: 0.001433460295200348\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011976184626420338\n",
      "Average test loss: 0.0014411216574824518\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012005253046751022\n",
      "Average test loss: 0.0016724067225845323\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011986311160855823\n",
      "Average test loss: 0.0015549109080392454\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011948450081050397\n",
      "Average test loss: 0.001413124673275484\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011938504756324821\n",
      "Average test loss: 0.0013982012635614309\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01195914533568753\n",
      "Average test loss: 0.001424443980368475\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011939205585254563\n",
      "Average test loss: 0.0014443220023272767\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011943546798494128\n",
      "Average test loss: 0.0014191837077960371\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011934985374410947\n",
      "Average test loss: 0.0015365074175513454\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011911936308774683\n",
      "Average test loss: 0.0016328478338610796\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011918908495042058\n",
      "Average test loss: 0.001424477365385327\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011919893959330187\n",
      "Average test loss: 0.0014364949394431378\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011879799332883623\n",
      "Average test loss: 0.001456322023852004\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011909927354090744\n",
      "Average test loss: 0.0014895118567057782\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011877314902014203\n",
      "Average test loss: 0.0014666395277405779\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011855285076631441\n",
      "Average test loss: 0.0014309350579149193\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011867376891275247\n",
      "Average test loss: 0.001418179063126445\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011876456279721524\n",
      "Average test loss: 0.001466240271512005\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011836536661618286\n",
      "Average test loss: 0.0014421655629864997\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011835514760679669\n",
      "Average test loss: 0.001431734734421803\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011863841257989406\n",
      "Average test loss: 0.001454396680618326\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01188120388570759\n",
      "Average test loss: 0.0015325428445099128\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011798957760963175\n",
      "Average test loss: 0.0014840337241896324\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011838232330977918\n",
      "Average test loss: 0.0014263265564416845\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01181874812559949\n",
      "Average test loss: 0.0014847563815613587\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011808760633071264\n",
      "Average test loss: 0.001475645199418068\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011809056363172001\n",
      "Average test loss: 0.0015835189001841678\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011814349339240127\n",
      "Average test loss: 0.0014556527548573084\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011837523019976086\n",
      "Average test loss: 0.0014046498311476574\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011806609272956849\n",
      "Average test loss: 0.0015120861352317862\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011792939763102266\n",
      "Average test loss: 0.0014869445585128334\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011776447628935179\n",
      "Average test loss: 0.0015249250260078245\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011771497161851989\n",
      "Average test loss: 0.0014746663485550218\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011742985682355033\n",
      "Average test loss: 0.0014729797147835294\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011757908004025618\n",
      "Average test loss: 0.0014940976590539017\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011753400390346846\n",
      "Average test loss: 0.0014338548316930732\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011731883181879918\n",
      "Average test loss: 0.0014576852962167727\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01175524485939079\n",
      "Average test loss: 0.001466684970073402\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011743302767475447\n",
      "Average test loss: 0.001437526934262779\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011729144535130924\n",
      "Average test loss: 0.001415441680337406\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011724245034986073\n",
      "Average test loss: 0.0013966317688011462\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011727346180213823\n",
      "Average test loss: 0.0015210425441360307\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011715390272852448\n",
      "Average test loss: 0.0014689302414448726\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011710427230430974\n",
      "Average test loss: 0.0014314646849201784\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011697380031976436\n",
      "Average test loss: 0.001409696955440773\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02309329648481475\n",
      "Average test loss: 0.0016662577533473571\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014438925202522013\n",
      "Average test loss: 0.0014908575173467398\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01304346176236868\n",
      "Average test loss: 0.001444622648259004\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01242362949417697\n",
      "Average test loss: 0.0014446142913463215\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012103968510197268\n",
      "Average test loss: 0.0014394842233094905\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011911796387698915\n",
      "Average test loss: 0.001451704374100599\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011800990401870675\n",
      "Average test loss: 0.0014801188123722872\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011744422321518263\n",
      "Average test loss: 0.0014303899393934344\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01171814693344964\n",
      "Average test loss: 0.0014524622700280614\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011704703686137994\n",
      "Average test loss: 0.001445320557595955\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011698633074760437\n",
      "Average test loss: 0.001480100249664651\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011677695760296451\n",
      "Average test loss: 0.0014233813801159461\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011681505775286092\n",
      "Average test loss: 0.0014290909739211202\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01165507080240382\n",
      "Average test loss: 0.0014717257702723146\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011642721833454238\n",
      "Average test loss: 0.0014282159444151653\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011663881479038133\n",
      "Average test loss: 0.0014602380119678048\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011660963215761714\n",
      "Average test loss: 0.0014214100234417452\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011643012216521633\n",
      "Average test loss: 0.0014368240841560894\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011658983703288767\n",
      "Average test loss: 0.0014082065337441033\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011646297142737442\n",
      "Average test loss: 0.0014976259146092666\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011651194703247813\n",
      "Average test loss: 0.0014306961480114195\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011626736266745461\n",
      "Average test loss: 0.0014968299318311943\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011598866980936793\n",
      "Average test loss: 0.0014442081225828991\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01162795601785183\n",
      "Average test loss: 0.0014409608885438906\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011614508933491178\n",
      "Average test loss: 0.0015726528217395147\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011619043798082405\n",
      "Average test loss: 0.0014805129077285527\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011618569711844126\n",
      "Average test loss: 0.0014937859044099847\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011600378284023868\n",
      "Average test loss: 0.0014232084474836786\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011601016773117914\n",
      "Average test loss: 0.0014938565362244845\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011589267859856287\n",
      "Average test loss: 0.001443592394184735\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011568303275439474\n",
      "Average test loss: 0.001446144425827596\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011573310491111544\n",
      "Average test loss: 0.001467406055941764\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011594080894357628\n",
      "Average test loss: 0.0014590748469862672\n",
      "Epoch 238/300\n",
      "Average training loss: 0.011578710466623307\n",
      "Average test loss: 0.0014383694768572847\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011595277490301264\n",
      "Average test loss: 0.0015046060999027557\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011542502866023117\n",
      "Average test loss: 0.001443758678001662\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01155497802918156\n",
      "Average test loss: 0.0015954283897040619\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011914296678370899\n",
      "Average test loss: 0.001439648288819525\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011633346959120697\n",
      "Average test loss: 0.0014373507111643752\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011528690241277218\n",
      "Average test loss: 0.0014162383385313054\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011517551193634668\n",
      "Average test loss: 0.001432148441310144\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011534600507054064\n",
      "Average test loss: 0.0014951234126670493\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011570018682214949\n",
      "Average test loss: 0.0014812559135154717\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011519866493841012\n",
      "Average test loss: 0.0014184006748514044\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011515481363568041\n",
      "Average test loss: 0.0015635935056747662\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011519381583564811\n",
      "Average test loss: 0.0015274643858687745\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011514221241076787\n",
      "Average test loss: 0.0016027017571032048\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011509454239573743\n",
      "Average test loss: 0.0014509338264664013\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011502008746067683\n",
      "Average test loss: 0.0014435469566120042\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011500315179841385\n",
      "Average test loss: 0.0017082933145057824\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01151003513402409\n",
      "Average test loss: 0.0015049637827194398\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011498124642504586\n",
      "Average test loss: 0.001436879643239081\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01149020323322879\n",
      "Average test loss: 0.0014586366769961186\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011506187108655771\n",
      "Average test loss: 0.0014632921409275797\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011476399469706747\n",
      "Average test loss: 0.0014660655725747347\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01147235351552566\n",
      "Average test loss: 0.0014477169855187336\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011465019933051534\n",
      "Average test loss: 0.0014684426519605848\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011458445301486386\n",
      "Average test loss: 0.0014240830030499234\n",
      "Epoch 263/300\n",
      "Average training loss: 0.011500598207116127\n",
      "Average test loss: 0.0014595484371400542\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011456879713469082\n",
      "Average test loss: 0.0014447508793738154\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011454930750032266\n",
      "Average test loss: 0.0014617368628581364\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01144323781132698\n",
      "Average test loss: 0.001489125828155213\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0114392976951268\n",
      "Average test loss: 0.0015540615717569988\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011442161979774634\n",
      "Average test loss: 0.001467695815074775\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01143881179806259\n",
      "Average test loss: 0.0014670212271933754\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011439249467518595\n",
      "Average test loss: 0.0014699254224283828\n",
      "Epoch 271/300\n",
      "Average training loss: 0.011438816889292665\n",
      "Average test loss: 0.0015289433414323462\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01144439123570919\n",
      "Average test loss: 0.0015168425107064346\n",
      "Epoch 273/300\n",
      "Average training loss: 0.011413836470908588\n",
      "Average test loss: 0.00145808642419676\n",
      "Epoch 274/300\n",
      "Average training loss: 0.011436278118027582\n",
      "Average test loss: 0.001481518304389384\n",
      "Epoch 275/300\n",
      "Average training loss: 0.011416523254579968\n",
      "Average test loss: 0.0015153018887051279\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01146009923848841\n",
      "Average test loss: 0.0014367224958000912\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0114291368847092\n",
      "Average test loss: 0.0014741230318322778\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011390684319867029\n",
      "Average test loss: 0.0015160640428463618\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011388320239881674\n",
      "Average test loss: 0.001460089867313703\n",
      "Epoch 280/300\n",
      "Average training loss: 0.011400741962095102\n",
      "Average test loss: 0.0015032861669444376\n",
      "Epoch 281/300\n",
      "Average training loss: 0.011412798335982693\n",
      "Average test loss: 0.0015459976051416663\n",
      "Epoch 282/300\n",
      "Average training loss: 0.011445166753398047\n",
      "Average test loss: 0.0014495812110188935\n",
      "Epoch 283/300\n",
      "Average training loss: 0.011422521178093222\n",
      "Average test loss: 0.0015348528663938245\n",
      "Epoch 284/300\n",
      "Average training loss: 0.011424705669283866\n",
      "Average test loss: 0.0014624279652101298\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011391833427879545\n",
      "Average test loss: 0.0014927208699906866\n",
      "Epoch 286/300\n",
      "Average training loss: 0.011403769160310427\n",
      "Average test loss: 0.0015400760351266297\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011364712249073717\n",
      "Average test loss: 0.0014587060726351207\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011368817045456833\n",
      "Average test loss: 0.0014817008510645893\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011377609870500035\n",
      "Average test loss: 0.0014681699635047051\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011351940143439505\n",
      "Average test loss: 0.0014604993483258617\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011375931411981583\n",
      "Average test loss: 0.0014725410912392867\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011379558224644926\n",
      "Average test loss: 0.0015672524578662383\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01135440768715408\n",
      "Average test loss: 0.0014484171034354302\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01135059137062894\n",
      "Average test loss: 0.0015803106997369064\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011360369498531024\n",
      "Average test loss: 0.0015057578744987648\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011362092960211966\n",
      "Average test loss: 0.0015071902597943942\n",
      "Epoch 297/300\n",
      "Average training loss: 0.011377684921854073\n",
      "Average test loss: 0.0014918270047443608\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011347100825773344\n",
      "Average test loss: 0.0014518502419814468\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01132458764149083\n",
      "Average test loss: 0.0015010837562796143\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01133381674107578\n",
      "Average test loss: 0.00149663240255581\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_64_Depth10/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.24\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.33\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.09\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.58\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.65\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 25.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 33.25\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.087814742618137\n",
      "Average test loss: 1.8047495803170734\n",
      "Epoch 2/300\n",
      "Average training loss: 2.580118481318156\n",
      "Average test loss: 0.016635817830761273\n",
      "Epoch 3/300\n",
      "Average training loss: 1.9799453265931872\n",
      "Average test loss: 0.012011947924892108\n",
      "Epoch 4/300\n",
      "Average training loss: 1.653194190449185\n",
      "Average test loss: 0.01915026047329108\n",
      "Epoch 5/300\n",
      "Average training loss: 1.369745182885064\n",
      "Average test loss: 0.011857448058409823\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1489125663969253\n",
      "Average test loss: 0.012614936543007691\n",
      "Epoch 7/300\n",
      "Average training loss: 1.01034703540802\n",
      "Average test loss: 0.0085475811527835\n",
      "Epoch 8/300\n",
      "Average training loss: 0.8792531317075094\n",
      "Average test loss: 0.009204579902191957\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7758377752304078\n",
      "Average test loss: 0.00907951506268647\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6734245564672682\n",
      "Average test loss: 0.007235741601636012\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5840297541618347\n",
      "Average test loss: 0.012544403399030367\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5131436315377553\n",
      "Average test loss: 0.009397781107988622\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4568632771174113\n",
      "Average test loss: 0.0066438022628426555\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4103010600407918\n",
      "Average test loss: 0.006327668272786671\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3689262051317427\n",
      "Average test loss: 0.006612425372832351\n",
      "Epoch 16/300\n",
      "Average training loss: 0.33362961496247184\n",
      "Average test loss: 0.00619603150172366\n",
      "Epoch 17/300\n",
      "Average training loss: 0.30387542099422876\n",
      "Average test loss: 0.008206549594799677\n",
      "Epoch 18/300\n",
      "Average training loss: 0.27666214527024163\n",
      "Average test loss: 0.023593614098098542\n",
      "Epoch 19/300\n",
      "Average training loss: 0.25349470541212293\n",
      "Average test loss: 0.006273499177147945\n",
      "Epoch 20/300\n",
      "Average training loss: 0.23370795840687222\n",
      "Average test loss: 0.007200471040275362\n",
      "Epoch 21/300\n",
      "Average training loss: 0.21679974370532565\n",
      "Average test loss: 0.006660707564817535\n",
      "Epoch 22/300\n",
      "Average training loss: 0.20132459775606792\n",
      "Average test loss: 0.05603574179609617\n",
      "Epoch 23/300\n",
      "Average training loss: 0.18877401785055797\n",
      "Average test loss: 0.3483998789555497\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1782293503549364\n",
      "Average test loss: 0.0063582619871530265\n",
      "Epoch 25/300\n",
      "Average training loss: 0.16952696690294478\n",
      "Average test loss: 0.005198282382554478\n",
      "Epoch 26/300\n",
      "Average training loss: 0.16147839065392813\n",
      "Average test loss: 0.005208266669263442\n",
      "Epoch 27/300\n",
      "Average training loss: 0.15224702449639638\n",
      "Average test loss: 0.18627259942889213\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1454262470404307\n",
      "Average test loss: 0.005325445852345891\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13963034280141196\n",
      "Average test loss: 0.005126031039903561\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1356356921195984\n",
      "Average test loss: 0.006116399062176546\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13038506419128842\n",
      "Average test loss: 0.005103143971413374\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12663751808802287\n",
      "Average test loss: 0.0053650300337208644\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12353758964273664\n",
      "Average test loss: 1.6830956226289273\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11919226372241974\n",
      "Average test loss: 0.004926785069828232\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11725615657038159\n",
      "Average test loss: 0.008494420943988694\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11290885166989433\n",
      "Average test loss: 0.0666305816181832\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1110429399145974\n",
      "Average test loss: 0.005045411078880231\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10811977969275581\n",
      "Average test loss: 0.008518077419035965\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10566646587848663\n",
      "Average test loss: 0.004862107754374544\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10417177349328995\n",
      "Average test loss: 4.0361721548040705\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10156866045130623\n",
      "Average test loss: 0.004716832738990585\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09968839780489604\n",
      "Average test loss: 0.006541777255220546\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09740388244390488\n",
      "Average test loss: 0.004755561718924178\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09633694653378592\n",
      "Average test loss: 0.00566324996534321\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09594283512565825\n",
      "Average test loss: 0.004893707147075071\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09281736302375794\n",
      "Average test loss: 0.004648366231057379\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0915836739871237\n",
      "Average test loss: 0.004963996099929015\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09016694915294647\n",
      "Average test loss: 0.021208461657166482\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0902212638258934\n",
      "Average test loss: 0.26372642570734023\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0881530317929056\n",
      "Average test loss: 0.15836573073764643\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08782633682092031\n",
      "Average test loss: 0.00484546156351765\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0864564567539427\n",
      "Average test loss: 0.005481923305119077\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08589999712838067\n",
      "Average test loss: 0.0960816776851813\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08564978713459438\n",
      "Average test loss: 0.004938787660251061\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08421510587798224\n",
      "Average test loss: 0.004992764121956295\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08400941178533766\n",
      "Average test loss: 9.96161429945628\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08313637596368789\n",
      "Average test loss: 0.004954202306353383\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08220785110526614\n",
      "Average test loss: 0.004825992917435037\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09686781742837694\n",
      "Average test loss: 0.008665197471777597\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09361965861585406\n",
      "Average test loss: 0.0046414384328656725\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08541330406400892\n",
      "Average test loss: 0.005115658743513955\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0829536735283004\n",
      "Average test loss: 0.004746622300396363\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0815847431090143\n",
      "Average test loss: 0.004635328006827169\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08102075871494081\n",
      "Average test loss: 0.004860929595927398\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08062662340535058\n",
      "Average test loss: 0.004643019043736988\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0800082702934742\n",
      "Average test loss: 0.01311343948294719\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07959239294793871\n",
      "Average test loss: 0.005779004371000661\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07953142596615685\n",
      "Average test loss: 0.004679650894469685\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07864097225666046\n",
      "Average test loss: 0.004678176052247484\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0798631464574072\n",
      "Average test loss: 0.07528110807140669\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0780505694117811\n",
      "Average test loss: 0.004652351622366243\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07777316799428728\n",
      "Average test loss: 0.006264664472805129\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07752383583784103\n",
      "Average test loss: 68.86149109665884\n",
      "Epoch 74/300\n",
      "Average training loss: 8.122215451372995\n",
      "Average test loss: 3.792722979810503\n",
      "Epoch 75/300\n",
      "Average training loss: 5.4854869624243845\n",
      "Average test loss: 0.013576832820143965\n",
      "Epoch 76/300\n",
      "Average training loss: 3.242656714121501\n",
      "Average test loss: 0.21522973646057977\n",
      "Epoch 77/300\n",
      "Average training loss: 2.326344611061944\n",
      "Average test loss: 1.2731953710557686\n",
      "Epoch 78/300\n",
      "Average training loss: 1.6762906730439928\n",
      "Average test loss: 0.007720905467867851\n",
      "Epoch 79/300\n",
      "Average training loss: 1.3239890517128838\n",
      "Average test loss: 0.007314962805145316\n",
      "Epoch 80/300\n",
      "Average training loss: 1.0951604063245985\n",
      "Average test loss: 0.012207878758509955\n",
      "Epoch 81/300\n",
      "Average training loss: 0.9159721434381273\n",
      "Average test loss: 0.009112386607461505\n",
      "Epoch 82/300\n",
      "Average training loss: 0.7666185514662001\n",
      "Average test loss: 0.006085223112669256\n",
      "Epoch 83/300\n",
      "Average training loss: 0.6459098067813449\n",
      "Average test loss: 1.5710647009445562\n",
      "Epoch 84/300\n",
      "Average training loss: 0.5451263621118334\n",
      "Average test loss: 26.9874053491652\n",
      "Epoch 85/300\n",
      "Average training loss: 0.4555126442909241\n",
      "Average test loss: 0.025003617220040825\n",
      "Epoch 86/300\n",
      "Average training loss: 0.36741549391216705\n",
      "Average test loss: 0.005466791226218144\n",
      "Epoch 87/300\n",
      "Average training loss: 0.2804349739683999\n",
      "Average test loss: 0.009358039710256788\n",
      "Epoch 88/300\n",
      "Average training loss: 0.2297730506658554\n",
      "Average test loss: 0.008364078165549372\n",
      "Epoch 89/300\n",
      "Average training loss: 0.19879262745380402\n",
      "Average test loss: 0.005130392852342791\n",
      "Epoch 90/300\n",
      "Average training loss: 0.17833447199397617\n",
      "Average test loss: 0.005289458378321595\n",
      "Epoch 91/300\n",
      "Average training loss: 0.16304085286458334\n",
      "Average test loss: 0.0050370306567185455\n",
      "Epoch 92/300\n",
      "Average training loss: 0.15205878851148819\n",
      "Average test loss: 0.0061030867977274784\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1431447258790334\n",
      "Average test loss: 0.005025101823525296\n",
      "Epoch 94/300\n",
      "Average training loss: 0.13672054151031707\n",
      "Average test loss: 0.0048566237447990315\n",
      "Epoch 95/300\n",
      "Average training loss: 0.12955060415797764\n",
      "Average test loss: 0.004798858236935403\n",
      "Epoch 96/300\n",
      "Average training loss: 0.12469421378771464\n",
      "Average test loss: 0.0047799207493662834\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11979466152191162\n",
      "Average test loss: 0.004796076381786002\n",
      "Epoch 98/300\n",
      "Average training loss: 3.7560605942143335\n",
      "Average test loss: 0.010065771971311834\n",
      "Epoch 99/300\n",
      "Average training loss: 1.8141780776977539\n",
      "Average test loss: 0.017191003678573503\n",
      "Epoch 100/300\n",
      "Average training loss: 1.1039373403655157\n",
      "Average test loss: 0.00867466514888737\n",
      "Epoch 101/300\n",
      "Average training loss: 0.7526250403192308\n",
      "Average test loss: 0.013469576828181744\n",
      "Epoch 102/300\n",
      "Average training loss: 0.5532721061706543\n",
      "Average test loss: 0.006138352603134182\n",
      "Epoch 103/300\n",
      "Average training loss: 0.4175594217777252\n",
      "Average test loss: 13.576978069047133\n",
      "Epoch 104/300\n",
      "Average training loss: 0.325623831431071\n",
      "Average test loss: 18.040704279147917\n",
      "Epoch 105/300\n",
      "Average training loss: 0.2664225301477644\n",
      "Average test loss: 0.0056611247712539305\n",
      "Epoch 106/300\n",
      "Average training loss: 0.22183979402648077\n",
      "Average test loss: 0.00900498698051605\n",
      "Epoch 107/300\n",
      "Average training loss: 0.19157157541645897\n",
      "Average test loss: 0.005853093802101082\n",
      "Epoch 108/300\n",
      "Average training loss: 0.1723019040690528\n",
      "Average test loss: 0.006898231981943051\n",
      "Epoch 109/300\n",
      "Average training loss: 0.15786562754048242\n",
      "Average test loss: 0.006969693116015858\n",
      "Epoch 110/300\n",
      "Average training loss: 0.14584350865417056\n",
      "Average test loss: 0.006463026903569698\n",
      "Epoch 111/300\n",
      "Average training loss: 0.13556222865316603\n",
      "Average test loss: 0.010804589668081866\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1269932187596957\n",
      "Average test loss: 0.004902170856793722\n",
      "Epoch 113/300\n",
      "Average training loss: 0.12030576696660783\n",
      "Average test loss: 0.0058305484257224535\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11522410929203034\n",
      "Average test loss: 0.004859642281714413\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11161731204059389\n",
      "Average test loss: 0.005803821129931344\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10740899093283547\n",
      "Average test loss: 0.004871095541450712\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10430363712045881\n",
      "Average test loss: 0.004705403164029121\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10153612577252918\n",
      "Average test loss: 0.005340349367923207\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09900682206286325\n",
      "Average test loss: 0.004657795311262211\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09822790712118148\n",
      "Average test loss: 0.004904750045802858\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09512873224417369\n",
      "Average test loss: 0.004644725317756335\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09294925462537341\n",
      "Average test loss: 0.004787711979200443\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09177383920881484\n",
      "Average test loss: 0.007621213250690036\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08997091595994101\n",
      "Average test loss: 0.004651936585290564\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08846207820706897\n",
      "Average test loss: 0.004555006641894579\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08725158993403116\n",
      "Average test loss: 0.0046045726272794935\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08626656588580874\n",
      "Average test loss: 0.0058009870201349255\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08525852573580212\n",
      "Average test loss: 0.004551396623874704\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0843016289141443\n",
      "Average test loss: 0.004558200285666518\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08385099627574284\n",
      "Average test loss: 0.004628839493211773\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08298350127538046\n",
      "Average test loss: 0.004602610730048683\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08248497298691009\n",
      "Average test loss: 0.004561328753415081\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08191416376829147\n",
      "Average test loss: 0.004535590620504485\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08116416502661175\n",
      "Average test loss: 0.004692039352324274\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08095852779017554\n",
      "Average test loss: 0.004500239486909575\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08028580337100559\n",
      "Average test loss: 0.005480001649508874\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08002038482162688\n",
      "Average test loss: 0.006150579991853899\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0793757173286544\n",
      "Average test loss: 0.0045508913372953735\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07884424367878172\n",
      "Average test loss: 0.004976318075839016\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0784304351343049\n",
      "Average test loss: 0.004817488098103139\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07818876378072633\n",
      "Average test loss: 0.004654541777239905\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0792800262371699\n",
      "Average test loss: 0.004616692140284512\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0774480972621176\n",
      "Average test loss: 0.004514806588490804\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07699582985374663\n",
      "Average test loss: 0.005050272904750373\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07658539861440658\n",
      "Average test loss: 0.004683587430251969\n",
      "Epoch 146/300\n",
      "Average training loss: 9.48411797592375\n",
      "Average test loss: 0.2838311324260301\n",
      "Epoch 147/300\n",
      "Average training loss: 2.2077442577150133\n",
      "Average test loss: 0.008431236443420252\n",
      "Epoch 148/300\n",
      "Average training loss: 1.362160805914137\n",
      "Average test loss: 0.007437838716225492\n",
      "Epoch 149/300\n",
      "Average training loss: 0.9015316936175028\n",
      "Average test loss: 0.4165017113557292\n",
      "Epoch 150/300\n",
      "Average training loss: 0.629189169300927\n",
      "Average test loss: 0.07280046402580208\n",
      "Epoch 151/300\n",
      "Average training loss: 0.4832987569173177\n",
      "Average test loss: 0.005890964000589318\n",
      "Epoch 152/300\n",
      "Average training loss: 0.38311915742026437\n",
      "Average test loss: 0.005797309300551812\n",
      "Epoch 153/300\n",
      "Average training loss: 0.31628183388710024\n",
      "Average test loss: 19.338979212662412\n",
      "Epoch 154/300\n",
      "Average training loss: 0.26639658337169225\n",
      "Average test loss: 74263.9961821484\n",
      "Epoch 155/300\n",
      "Average training loss: 0.22651645633909437\n",
      "Average test loss: 4.39345358226945\n",
      "Epoch 156/300\n",
      "Average training loss: 0.19815158885055117\n",
      "Average test loss: 0.009948838472366333\n",
      "Epoch 157/300\n",
      "Average training loss: 0.1745220132138994\n",
      "Average test loss: 0.0052506143347256715\n",
      "Epoch 158/300\n",
      "Average training loss: 0.1570966840585073\n",
      "Average test loss: 0.17251533889770507\n",
      "Epoch 159/300\n",
      "Average training loss: 0.14411118431886036\n",
      "Average test loss: 0.0051263674002968605\n",
      "Epoch 160/300\n",
      "Average training loss: 0.13459668517775006\n",
      "Average test loss: 0.023736901895453532\n",
      "Epoch 161/300\n",
      "Average training loss: 0.1271819022099177\n",
      "Average test loss: 0.06368099293112754\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1221406649019983\n",
      "Average test loss: 0.004994223251110977\n",
      "Epoch 163/300\n",
      "Average training loss: 0.11753278424342474\n",
      "Average test loss: 4.146567094054901\n",
      "Epoch 164/300\n",
      "Average training loss: 0.1129541300535202\n",
      "Average test loss: 0.021904054457114804\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10872905106014676\n",
      "Average test loss: 0.23068489240606627\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10518756269746357\n",
      "Average test loss: 0.004926212283058299\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10189608987172445\n",
      "Average test loss: 0.0064687055216895205\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09974999370177587\n",
      "Average test loss: 0.0050022836493121255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09655839033590423\n",
      "Average test loss: 0.004729426684271958\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09435387980275683\n",
      "Average test loss: 0.004651197597798374\n",
      "Epoch 171/300\n",
      "Average training loss: 3.538371057404412\n",
      "Average test loss: 0.013654327172372076\n",
      "Epoch 172/300\n",
      "Average training loss: 1.2224703544510735\n",
      "Average test loss: 0.009624397339920203\n",
      "Epoch 173/300\n",
      "Average training loss: 0.5288029203414917\n",
      "Average test loss: 0.02258407591780027\n",
      "Epoch 174/300\n",
      "Average training loss: 0.35748794047037763\n",
      "Average test loss: 65.17501104461319\n",
      "Epoch 175/300\n",
      "Average training loss: 0.3006981862650977\n",
      "Average test loss: 5.694658230695046\n",
      "Epoch 176/300\n",
      "Average training loss: 0.2597774667872323\n",
      "Average test loss: 1167.8342104551527\n",
      "Epoch 177/300\n",
      "Average training loss: 0.2290462117460039\n",
      "Average test loss: 347.6509252648792\n",
      "Epoch 178/300\n",
      "Average training loss: 0.20289217523733774\n",
      "Average test loss: 1316747.7539705217\n",
      "Epoch 179/300\n",
      "Average training loss: 0.18402365716298422\n",
      "Average test loss: 561.2659071960151\n",
      "Epoch 180/300\n",
      "Average training loss: 0.16848146020703847\n",
      "Average test loss: 454.5408823451731\n",
      "Epoch 181/300\n",
      "Average training loss: 0.15708342648877038\n",
      "Average test loss: 15925218.477938334\n",
      "Epoch 182/300\n",
      "Average training loss: 0.14776079176531898\n",
      "Average test loss: 298.91920054871343\n",
      "Epoch 183/300\n",
      "Average training loss: 0.1366930129792955\n",
      "Average test loss: 54841.56725876797\n",
      "Epoch 184/300\n",
      "Average training loss: 0.12812673805819616\n",
      "Average test loss: 877.6629654231639\n",
      "Epoch 185/300\n",
      "Average training loss: 0.12230108767085605\n",
      "Average test loss: 16.55349619927671\n",
      "Epoch 186/300\n",
      "Average training loss: 0.1155300072365337\n",
      "Average test loss: 0.043352762141161495\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1103946491347419\n",
      "Average test loss: 0.009975049405255251\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10941748089260525\n",
      "Average test loss: 0.005416769581950373\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10368998461299472\n",
      "Average test loss: 0.00789582105891572\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10123992389440536\n",
      "Average test loss: 0.00526468689698312\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09819164166847864\n",
      "Average test loss: 0.004754480195956098\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09563301408290863\n",
      "Average test loss: 0.004706358518865373\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09335945577091641\n",
      "Average test loss: 1.9310957859092288\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09144394454028872\n",
      "Average test loss: 0.004751629739792811\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08998625775178273\n",
      "Average test loss: 0.004684505773501263\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08821782788303163\n",
      "Average test loss: 0.004537598987834321\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08722654432720608\n",
      "Average test loss: 0.007145685451312197\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08635429092248281\n",
      "Average test loss: 0.010072451523194711\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10016107046604157\n",
      "Average test loss: 0.004640198456744353\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08681781973441442\n",
      "Average test loss: 0.005382225804444816\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08469135251310136\n",
      "Average test loss: 0.0046276256944984195\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0833311970697509\n",
      "Average test loss: 0.008783601032363043\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08209068011575275\n",
      "Average test loss: 0.004717604149339928\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08176518073346879\n",
      "Average test loss: 0.004557713901003201\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08810391785701116\n",
      "Average test loss: 0.0050998740875058705\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08130054532157051\n",
      "Average test loss: 0.055464483194881016\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08016738977034887\n",
      "Average test loss: 0.0215767139825556\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08196244555049473\n",
      "Average test loss: 0.004674077910267645\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07935433891084459\n",
      "Average test loss: 0.004551419298268027\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0791518375939793\n",
      "Average test loss: 0.024554494250151847\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07848005382882224\n",
      "Average test loss: 0.00549267534001006\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07847799603806602\n",
      "Average test loss: 0.004700819375614325\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07746558857957522\n",
      "Average test loss: 0.00503961027206646\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07713083810276455\n",
      "Average test loss: 0.004531051483419206\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07700074476003647\n",
      "Average test loss: 0.35536585405137805\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07662212612231573\n",
      "Average test loss: 0.0045399154799266\n",
      "Epoch 217/300\n",
      "Average training loss: 0.076471741532286\n",
      "Average test loss: 0.014445322965996134\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07544166754682859\n",
      "Average test loss: 0.004848293168263303\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0758010769089063\n",
      "Average test loss: 0.006197426966908905\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07495857832829157\n",
      "Average test loss: 0.004705763766335117\n",
      "Epoch 221/300\n",
      "Average training loss: 0.5838016864127583\n",
      "Average test loss: 0.07439597523005473\n",
      "Epoch 222/300\n",
      "Average training loss: 0.30766220941808486\n",
      "Average test loss: 9305.280901421442\n",
      "Epoch 223/300\n",
      "Average training loss: 0.15260643421941333\n",
      "Average test loss: 17.39403663443836\n",
      "Epoch 224/300\n",
      "Average training loss: 0.12594104732407463\n",
      "Average test loss: 0.0067410644127262965\n",
      "Epoch 225/300\n",
      "Average training loss: 0.1107973358101315\n",
      "Average test loss: 0.008154652807654606\n",
      "Epoch 226/300\n",
      "Average training loss: 0.10341901557313071\n",
      "Average test loss: 0.10220289553205172\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09786586010456086\n",
      "Average test loss: 0.004637471289270454\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09414161407285267\n",
      "Average test loss: 0.004631400721768538\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09160968254009882\n",
      "Average test loss: 0.00501490034452743\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08895750297440423\n",
      "Average test loss: 0.004635319974687364\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08691893686188591\n",
      "Average test loss: 0.004609066012625893\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08552222429381477\n",
      "Average test loss: 0.004732263075394763\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0835875893202093\n",
      "Average test loss: 0.004664774050315221\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08224297124809689\n",
      "Average test loss: 0.004584732797410753\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08123664449320898\n",
      "Average test loss: 0.00459423610733615\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08023455101251602\n",
      "Average test loss: 0.004924121480021212\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07929466670751571\n",
      "Average test loss: 0.0052505357720785675\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07886103973123762\n",
      "Average test loss: 0.004573511641679538\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07912482033835518\n",
      "Average test loss: 0.004618434355076816\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07742788894971211\n",
      "Average test loss: 0.012432016409105726\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0768923850191964\n",
      "Average test loss: 0.123442769870162\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07862320514520009\n",
      "Average test loss: 0.004580763285772668\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07639957022335794\n",
      "Average test loss: 391.32660883019366\n",
      "Epoch 244/300\n",
      "Average training loss: 62.05477791570955\n",
      "Average test loss: 143.230394573662\n",
      "Epoch 245/300\n",
      "Average training loss: 2.841761810514662\n",
      "Average test loss: 0.32039658143785266\n",
      "Epoch 246/300\n",
      "Average training loss: 1.8157757965723673\n",
      "Average test loss: 0.010098629430764252\n",
      "Epoch 247/300\n",
      "Average training loss: 1.4208537047704062\n",
      "Average test loss: 361.656540678925\n",
      "Epoch 248/300\n",
      "Average training loss: 1.1889472289615208\n",
      "Average test loss: 0.01420365380247434\n",
      "Epoch 249/300\n",
      "Average training loss: 1.0287228581110637\n",
      "Average test loss: 0.5754559925740792\n",
      "Epoch 250/300\n",
      "Average training loss: 0.9010088733037313\n",
      "Average test loss: 0.009830479512612025\n",
      "Epoch 251/300\n",
      "Average training loss: 0.7905720435248481\n",
      "Average test loss: 0.023771887434025606\n",
      "Epoch 252/300\n",
      "Average training loss: 0.7039321329328749\n",
      "Average test loss: 0.006498971160501242\n",
      "Epoch 253/300\n",
      "Average training loss: 0.6283337065378825\n",
      "Average test loss: 0.015075642531116804\n",
      "Epoch 254/300\n",
      "Average training loss: 0.5561884067853292\n",
      "Average test loss: 13152.674221796266\n",
      "Epoch 255/300\n",
      "Average training loss: 0.4792664345370399\n",
      "Average test loss: 64.7708108962112\n",
      "Epoch 256/300\n",
      "Average training loss: 0.39044592401716444\n",
      "Average test loss: 7.301258421086603\n",
      "Epoch 257/300\n",
      "Average training loss: 0.31109656092855664\n",
      "Average test loss: 23.09195423725413\n",
      "Epoch 258/300\n",
      "Average training loss: 0.2517459052933587\n",
      "Average test loss: 0.0432745451318721\n",
      "Epoch 259/300\n",
      "Average training loss: 0.20143669244978163\n",
      "Average test loss: 0.006321276796774732\n",
      "Epoch 260/300\n",
      "Average training loss: 0.16714547663264803\n",
      "Average test loss: 0.005268996714717812\n",
      "Epoch 261/300\n",
      "Average training loss: 0.14593777386347453\n",
      "Average test loss: 769.8435158276028\n",
      "Epoch 262/300\n",
      "Average training loss: 0.13223551282617782\n",
      "Average test loss: 0.005119709307120906\n",
      "Epoch 263/300\n",
      "Average training loss: 0.1228667678170734\n",
      "Average test loss: 0.004930591666036182\n",
      "Epoch 264/300\n",
      "Average training loss: 0.11601018212238948\n",
      "Average test loss: 0.005739475945217742\n",
      "Epoch 265/300\n",
      "Average training loss: 0.11095271562205421\n",
      "Average test loss: 0.0049274838661981955\n",
      "Epoch 266/300\n",
      "Average training loss: 0.10765831665860282\n",
      "Average test loss: 0.004805206957376665\n",
      "Epoch 267/300\n",
      "Average training loss: 0.1035523772570822\n",
      "Average test loss: 4.016296418315834\n",
      "Epoch 268/300\n",
      "Average training loss: 0.10134665297137366\n",
      "Average test loss: 0.005218207357244359\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09863009095854229\n",
      "Average test loss: 0.004683696840372351\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09555310319529639\n",
      "Average test loss: 0.0049072841219604015\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0928656509783533\n",
      "Average test loss: 0.004895829317884313\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0911745964023802\n",
      "Average test loss: 0.004699219999214014\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08936530454291237\n",
      "Average test loss: 0.004732793362604247\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08728189918729994\n",
      "Average test loss: 0.004602129908899466\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08559125761853324\n",
      "Average test loss: 0.004613853564278947\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08396012747950024\n",
      "Average test loss: 0.004597406468250686\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08258790643347634\n",
      "Average test loss: 0.004568218191878663\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08215081687106027\n",
      "Average test loss: 0.004820522660182582\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08049692989720239\n",
      "Average test loss: 0.0053773372756938135\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07971867048078113\n",
      "Average test loss: 0.004587113639546765\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07891644981834624\n",
      "Average test loss: 0.005046627067650358\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07800773535834418\n",
      "Average test loss: 0.008715655079732338\n",
      "Epoch 283/300\n",
      "Average training loss: 0.24888564606507618\n",
      "Average test loss: 92.32373877976627\n",
      "Epoch 284/300\n",
      "Average training loss: 0.16195652533902063\n",
      "Average test loss: 1.570278879834132\n",
      "Epoch 285/300\n",
      "Average training loss: 0.12054062414169311\n",
      "Average test loss: 0.005077726711829504\n",
      "Epoch 286/300\n",
      "Average training loss: 0.10848221357001199\n",
      "Average test loss: 0.004864707158671485\n",
      "Epoch 287/300\n",
      "Average training loss: 0.10225628519720502\n",
      "Average test loss: 0.004729302749865585\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09740770179695553\n",
      "Average test loss: 0.005081688766885135\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09412751705116695\n",
      "Average test loss: 0.004862849646351404\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09127432619200812\n",
      "Average test loss: 0.004677862609840102\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08847780645887057\n",
      "Average test loss: 0.004708657731612524\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08643634721967909\n",
      "Average test loss: 0.0045852567901213965\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08431281425555547\n",
      "Average test loss: 0.004720441940757963\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08262266588873333\n",
      "Average test loss: 3326.002504665799\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08155347387658225\n",
      "Average test loss: 0.004594605381290118\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08027455337180031\n",
      "Average test loss: 0.009328115228977469\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07928673397170173\n",
      "Average test loss: 0.004570182125187582\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07896884781784481\n",
      "Average test loss: 0.004816113441354699\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09681973612308502\n",
      "Average test loss: 0.004723895663602484\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08018373712566164\n",
      "Average test loss: 0.006903804995947414\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.438634221818712\n",
      "Average test loss: 0.16376163531591495\n",
      "Epoch 2/300\n",
      "Average training loss: 2.060010986328125\n",
      "Average test loss: 0.009891967557370663\n",
      "Epoch 3/300\n",
      "Average training loss: 1.5680344163046942\n",
      "Average test loss: 0.007464822838703792\n",
      "Epoch 4/300\n",
      "Average training loss: 1.3117504408094618\n",
      "Average test loss: 0.006750751602980826\n",
      "Epoch 5/300\n",
      "Average training loss: 1.1251210339864095\n",
      "Average test loss: 0.005696502994332049\n",
      "Epoch 6/300\n",
      "Average training loss: 0.9690779433780247\n",
      "Average test loss: 0.005875149562127061\n",
      "Epoch 7/300\n",
      "Average training loss: 0.8508244694073995\n",
      "Average test loss: 0.005521350053863393\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7482526511086358\n",
      "Average test loss: 0.004857429208854834\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6615405609342787\n",
      "Average test loss: 0.0065725112532575925\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5867416679594252\n",
      "Average test loss: 0.004816790237194962\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5233101648754543\n",
      "Average test loss: 0.0067854000917739335\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4680241060256958\n",
      "Average test loss: 0.003943178188469675\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4196275893582238\n",
      "Average test loss: 0.004255035345753034\n",
      "Epoch 14/300\n",
      "Average training loss: 0.3763997152116564\n",
      "Average test loss: 0.004068109593457646\n",
      "Epoch 15/300\n",
      "Average training loss: 0.338721596982744\n",
      "Average test loss: 0.004174434224764506\n",
      "Epoch 16/300\n",
      "Average training loss: 0.3046111780802409\n",
      "Average test loss: 0.004542844022313754\n",
      "Epoch 17/300\n",
      "Average training loss: 0.27440368678834703\n",
      "Average test loss: 0.0036092500533494686\n",
      "Epoch 18/300\n",
      "Average training loss: 0.24908225366804335\n",
      "Average test loss: 0.0036480372531546485\n",
      "Epoch 19/300\n",
      "Average training loss: 0.22636465871334077\n",
      "Average test loss: 0.004029682340721289\n",
      "Epoch 20/300\n",
      "Average training loss: 0.20551147830486297\n",
      "Average test loss: 0.01007615012427171\n",
      "Epoch 21/300\n",
      "Average training loss: 0.18804843197928534\n",
      "Average test loss: 0.005185249926108453\n",
      "Epoch 22/300\n",
      "Average training loss: 0.17436183432737987\n",
      "Average test loss: 0.003299003196052379\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16078703241878087\n",
      "Average test loss: 0.05709872926606072\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15039181087414424\n",
      "Average test loss: 0.006894617973102464\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14047085462676154\n",
      "Average test loss: 0.0029889122155598467\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13175657147831388\n",
      "Average test loss: 0.008224577783710427\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1232865538597107\n",
      "Average test loss: 0.003454494125313229\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11577157939142652\n",
      "Average test loss: 0.0033999918347431555\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10975252365403705\n",
      "Average test loss: 0.0034287246006230515\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10416586951414744\n",
      "Average test loss: 0.0029922884301178984\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09864536422491074\n",
      "Average test loss: 0.003719952774130636\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09446625706222322\n",
      "Average test loss: 0.0032975681103352044\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09081274227963554\n",
      "Average test loss: 0.003872860445537501\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08712539299991395\n",
      "Average test loss: 0.0031778909442946317\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0830601159201728\n",
      "Average test loss: 0.002725351429440909\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08028140851524142\n",
      "Average test loss: 0.002706120408243603\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07798416674137115\n",
      "Average test loss: 0.0028994212874935733\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07523774384127722\n",
      "Average test loss: 0.002798915057339602\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07246113762921758\n",
      "Average test loss: 0.0034066566897349225\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0705217026869456\n",
      "Average test loss: 0.0060415297804607285\n",
      "Epoch 41/300\n",
      "Average training loss: 0.068321256985267\n",
      "Average test loss: 131.29599864853753\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06651417220301098\n",
      "Average test loss: 0.018910390487561622\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06491988232069545\n",
      "Average test loss: 0.0026196678891364073\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06354163486096594\n",
      "Average test loss: 0.0028299707567526235\n",
      "Epoch 45/300\n",
      "Average training loss: 0.13258377961979972\n",
      "Average test loss: 0.004683818094846275\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09708218679825464\n",
      "Average test loss: 0.003162686077877879\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07962375623650021\n",
      "Average test loss: 0.003175944676415788\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0737326469288932\n",
      "Average test loss: 0.0030236884088565906\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06957005347477065\n",
      "Average test loss: 0.0028986980408016176\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06688683119912943\n",
      "Average test loss: 0.003026555834338069\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06448046746518876\n",
      "Average test loss: 0.0027800175537251765\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06294108319282532\n",
      "Average test loss: 0.003731024381808109\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06156946294837528\n",
      "Average test loss: 0.0026880965882705317\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06032042541768816\n",
      "Average test loss: 0.0036441044606682327\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05950471953219837\n",
      "Average test loss: 0.0029060291945934296\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0583081256184313\n",
      "Average test loss: 0.002938867094512615\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05743985852599144\n",
      "Average test loss: 0.0026512732492345904\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05723562933339013\n",
      "Average test loss: 0.002658516087465816\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05631062786777814\n",
      "Average test loss: 0.003233963041462832\n",
      "Epoch 60/300\n",
      "Average training loss: 0.057285949157343974\n",
      "Average test loss: 0.00308492345466382\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0550903261270788\n",
      "Average test loss: 0.0026501856551816067\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0544377283387714\n",
      "Average test loss: 0.0029369499422609805\n",
      "Epoch 63/300\n",
      "Average training loss: 0.054231116854482224\n",
      "Average test loss: 0.002611185764686929\n",
      "Epoch 64/300\n",
      "Average training loss: 0.054270065973202386\n",
      "Average test loss: 0.03407314327524768\n",
      "Epoch 65/300\n",
      "Average training loss: 0.17211825195617145\n",
      "Average test loss: 0.0029537678557551568\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07822055797775586\n",
      "Average test loss: 0.0027537781244350804\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0696582076019711\n",
      "Average test loss: 0.0028755761741970976\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06553583330578275\n",
      "Average test loss: 0.0030644209697428676\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06282941724856694\n",
      "Average test loss: 0.002675972166057262\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06073527893424034\n",
      "Average test loss: 0.0026351569671597746\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05966757669051488\n",
      "Average test loss: 0.002661288928033577\n",
      "Epoch 72/300\n",
      "Average training loss: 0.058135749618212385\n",
      "Average test loss: 0.0036904444264041054\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05704431066579289\n",
      "Average test loss: 0.010689594286183516\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05622616457608011\n",
      "Average test loss: 0.003099170000395841\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05550536384847429\n",
      "Average test loss: 0.002721392689790163\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05570146473911074\n",
      "Average test loss: 0.002597847634512517\n",
      "Epoch 77/300\n",
      "Average training loss: 0.054636625978681776\n",
      "Average test loss: 0.0026089862359480725\n",
      "Epoch 78/300\n",
      "Average training loss: 0.053987603932619095\n",
      "Average test loss: 0.0026130342549747893\n",
      "Epoch 79/300\n",
      "Average training loss: 0.053537376099162634\n",
      "Average test loss: 0.0030067467174182336\n",
      "Epoch 80/300\n",
      "Average training loss: 0.054815705974896746\n",
      "Average test loss: 0.0026886473562982346\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05350681975815031\n",
      "Average test loss: 0.0026201642581986056\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05288232395384047\n",
      "Average test loss: 0.0026049985484116606\n",
      "Epoch 83/300\n",
      "Average training loss: 0.052656558179193076\n",
      "Average test loss: 0.002553500010114577\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05210633709364467\n",
      "Average test loss: 0.002994801793454422\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05181126879983478\n",
      "Average test loss: 0.0027106949363110795\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05305506128735012\n",
      "Average test loss: 0.006308335993025038\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05218341710832384\n",
      "Average test loss: 0.004451521360211902\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0509003250433339\n",
      "Average test loss: 0.0026059659626334907\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05044475719332695\n",
      "Average test loss: 0.0026495886670632493\n",
      "Epoch 90/300\n",
      "Average training loss: 0.050200869189368356\n",
      "Average test loss: 0.0025563144189202124\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04990578866004944\n",
      "Average test loss: 0.0025948260167820585\n",
      "Epoch 92/300\n",
      "Average training loss: 0.049631946765714224\n",
      "Average test loss: 0.013220098829310802\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04968625177443028\n",
      "Average test loss: 0.0035361813517908254\n",
      "Epoch 94/300\n",
      "Average training loss: 0.049068175544341404\n",
      "Average test loss: 0.00257984355588754\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04891339106029934\n",
      "Average test loss: 0.0025344948781033357\n",
      "Epoch 96/300\n",
      "Average training loss: 0.048503081305159464\n",
      "Average test loss: 0.0027927469429042603\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04835174919830428\n",
      "Average test loss: 0.00905842326891919\n",
      "Epoch 98/300\n",
      "Average training loss: 0.048059535135825476\n",
      "Average test loss: 0.0029365295798828204\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04777383643057611\n",
      "Average test loss: 0.004065245913755563\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04772013172176149\n",
      "Average test loss: 0.002518688942409224\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04981040516495704\n",
      "Average test loss: 0.011260955387105545\n",
      "Epoch 102/300\n",
      "Average training loss: 0.2417895850042502\n",
      "Average test loss: 0.0030293087305294144\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09850187975168229\n",
      "Average test loss: 0.0060874122181493375\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08186347915728887\n",
      "Average test loss: 0.0031475404287791913\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07347213933865229\n",
      "Average test loss: 0.007619589942610926\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06913097016347779\n",
      "Average test loss: 0.0026322838782022397\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06546421752373377\n",
      "Average test loss: 0.0027609193587882653\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06346624458498425\n",
      "Average test loss: 0.002747297000967794\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06053482220239109\n",
      "Average test loss: 0.002578302402049303\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05865186881356769\n",
      "Average test loss: 0.002528053713755475\n",
      "Epoch 111/300\n",
      "Average training loss: 0.057154768523242735\n",
      "Average test loss: 0.003798184163454506\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0557763751745224\n",
      "Average test loss: 0.002542825990253025\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05457102057668898\n",
      "Average test loss: 0.0025576701110435855\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05374200910329819\n",
      "Average test loss: 0.002713346803974774\n",
      "Epoch 115/300\n",
      "Average training loss: 0.053056016859081054\n",
      "Average test loss: 0.0036310311535166367\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05198965731263161\n",
      "Average test loss: 0.002898214518299533\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05162731841537688\n",
      "Average test loss: 0.0026316100267900363\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05073029178380966\n",
      "Average test loss: 0.002566141366027296\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05007843968603346\n",
      "Average test loss: 0.003182831737730238\n",
      "Epoch 120/300\n",
      "Average training loss: 0.049754420591725246\n",
      "Average test loss: 0.0026918700013516678\n",
      "Epoch 121/300\n",
      "Average training loss: 0.049288720097806715\n",
      "Average test loss: 0.482536124587059\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04878184208605024\n",
      "Average test loss: 0.004608364236230652\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04848420473602083\n",
      "Average test loss: 0.007837292455550697\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04872404610282845\n",
      "Average test loss: 0.00467548926298817\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04808799116147889\n",
      "Average test loss: 0.0025526928398758174\n",
      "Epoch 126/300\n",
      "Average training loss: 0.047684095233678815\n",
      "Average test loss: 0.005173777006980446\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04744771350092358\n",
      "Average test loss: 0.002529028887136115\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04749593116839727\n",
      "Average test loss: 0.002516502326561345\n",
      "Epoch 129/300\n",
      "Average training loss: 0.047198944465981586\n",
      "Average test loss: 0.005850938809828626\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04682487242420515\n",
      "Average test loss: 0.002685701746079657\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04687062456872728\n",
      "Average test loss: 0.002535215846573313\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04729316223661105\n",
      "Average test loss: 0.0025999799573587046\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04625778792964087\n",
      "Average test loss: 0.002527617788977093\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04653973547948731\n",
      "Average test loss: 0.002597746359805266\n",
      "Epoch 135/300\n",
      "Average training loss: 0.045883231772316825\n",
      "Average test loss: 0.0028084313691490227\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05058351265390714\n",
      "Average test loss: 0.0037637474350631236\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04816582821475135\n",
      "Average test loss: 0.0026406198487513594\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04663686250646909\n",
      "Average test loss: 0.002699792692023847\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04623967135614819\n",
      "Average test loss: 0.0026684191293186612\n",
      "Epoch 140/300\n",
      "Average training loss: 0.15764696309301587\n",
      "Average test loss: 0.003023884875078996\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07466714424557155\n",
      "Average test loss: 0.0027916324542214474\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06531465286678738\n",
      "Average test loss: 0.0027554222639236186\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06075557803445392\n",
      "Average test loss: 0.0026809956878423693\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05784973825679885\n",
      "Average test loss: 0.019728694029980235\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0555193027291033\n",
      "Average test loss: 0.0033106151554319592\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05373856161369218\n",
      "Average test loss: 0.0025681684416615303\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0524024282793204\n",
      "Average test loss: 0.002524779259123736\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05205221441719267\n",
      "Average test loss: 0.0027583520646310513\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04991110737787353\n",
      "Average test loss: 0.005117000423268312\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04886003053519461\n",
      "Average test loss: 0.002483933748263452\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04847474610805511\n",
      "Average test loss: 0.002527148022626837\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04729114952352312\n",
      "Average test loss: 0.0026945908380051455\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04692338832219442\n",
      "Average test loss: 0.0024853335480309194\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0463063847074906\n",
      "Average test loss: 0.0026941626431523924\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04587747375832664\n",
      "Average test loss: 0.00944089245961772\n",
      "Epoch 156/300\n",
      "Average training loss: 0.046022005607684456\n",
      "Average test loss: 0.002925748808309436\n",
      "Epoch 157/300\n",
      "Average training loss: 0.047684217025836306\n",
      "Average test loss: 0.0025309412748449377\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04568160927957959\n",
      "Average test loss: 0.002805337429460552\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0451216852830516\n",
      "Average test loss: 0.002672276778767506\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04497445607185364\n",
      "Average test loss: 0.0025408453517076044\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04493492095337974\n",
      "Average test loss: 0.002585427795743777\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04461879931224717\n",
      "Average test loss: 0.002543802982299692\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0448673895266321\n",
      "Average test loss: 0.005304453443735838\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04435148093932205\n",
      "Average test loss: 0.005991682008736663\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04413488944702678\n",
      "Average test loss: 0.002576486101994912\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04410015619794528\n",
      "Average test loss: 0.002675226026939021\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04425516616304716\n",
      "Average test loss: 0.0029410711789710654\n",
      "Epoch 168/300\n",
      "Average training loss: 0.043760769668552614\n",
      "Average test loss: 0.6205716604602833\n",
      "Epoch 169/300\n",
      "Average training loss: 0.043851863847838506\n",
      "Average test loss: 0.002580754778865311\n",
      "Epoch 170/300\n",
      "Average training loss: 0.8253197511070305\n",
      "Average test loss: 23222123.93908848\n",
      "Epoch 171/300\n",
      "Average training loss: 0.18879305611716377\n",
      "Average test loss: 1256408.4796102892\n",
      "Epoch 172/300\n",
      "Average training loss: 0.14020867488119337\n",
      "Average test loss: 11609.149518472981\n",
      "Epoch 173/300\n",
      "Average training loss: 0.11803005655606588\n",
      "Average test loss: 666141.6708102272\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10398425087663862\n",
      "Average test loss: 9460884.925929504\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09278202382061217\n",
      "Average test loss: 0.025542630340283114\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08404702974028058\n",
      "Average test loss: 27.95692657733957\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07799223571353489\n",
      "Average test loss: 1881.0188539877063\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07294228991534975\n",
      "Average test loss: 0.005625646185957723\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06818534633848403\n",
      "Average test loss: 0.0027320323458148374\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06437742808792327\n",
      "Average test loss: 0.0028890923381679587\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06202914051214854\n",
      "Average test loss: 0.0027043650288962654\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05940676336155997\n",
      "Average test loss: 0.002587228997093108\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05755402257376247\n",
      "Average test loss: 0.002697178460450636\n",
      "Epoch 184/300\n",
      "Average training loss: 0.055795926133791605\n",
      "Average test loss: 4.536730851704876\n",
      "Epoch 185/300\n",
      "Average training loss: 0.054229903717835745\n",
      "Average test loss: 0.0025152660711771913\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05339885616964764\n",
      "Average test loss: 0.007397722487234407\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05210488961802589\n",
      "Average test loss: 15.506499980496036\n",
      "Epoch 188/300\n",
      "Average training loss: 0.051056955218315124\n",
      "Average test loss: 0.0025684747588303352\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04972192915611797\n",
      "Average test loss: 0.0029091265727248456\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04923200444380442\n",
      "Average test loss: 0.007350104638478822\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04830297854211595\n",
      "Average test loss: 0.0027316109887841676\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04758189294901159\n",
      "Average test loss: 0.002680672885746592\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04990173859066433\n",
      "Average test loss: 0.003036333353155189\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04789364320039749\n",
      "Average test loss: 0.0025949587422526544\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04621231871512201\n",
      "Average test loss: 0.002850179700594809\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04574040307270156\n",
      "Average test loss: 0.0025216255496359535\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04531252723601129\n",
      "Average test loss: 0.0028540083648016055\n",
      "Epoch 198/300\n",
      "Average training loss: 0.045134297178851236\n",
      "Average test loss: 0.0025355122617135447\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0448244678609901\n",
      "Average test loss: 0.002980413179637657\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04453590197695626\n",
      "Average test loss: 0.002781919006879131\n",
      "Epoch 201/300\n",
      "Average training loss: 0.044297110242976084\n",
      "Average test loss: 0.002529648564962877\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04400806232955721\n",
      "Average test loss: 0.00284134062524471\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04401721434791883\n",
      "Average test loss: 7.761407954321967\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04393892244829072\n",
      "Average test loss: 0.00256978843971673\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04368843700157272\n",
      "Average test loss: 0.002839236828395062\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04365734207961294\n",
      "Average test loss: 0.0027641288874049983\n",
      "Epoch 207/300\n",
      "Average training loss: 0.043304640077882345\n",
      "Average test loss: 0.003728743721213606\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04341087778078185\n",
      "Average test loss: 0.0026037046547151273\n",
      "Epoch 209/300\n",
      "Average training loss: 0.042949763142400316\n",
      "Average test loss: 0.002862534741974539\n",
      "Epoch 210/300\n",
      "Average training loss: 0.043079313380850685\n",
      "Average test loss: 0.0025397405582997536\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04268836413489448\n",
      "Average test loss: 0.0025665954339007535\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04270632253090541\n",
      "Average test loss: 0.003062664984001054\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04278671121266153\n",
      "Average test loss: 0.0045156323709007766\n",
      "Epoch 214/300\n",
      "Average training loss: 0.042355292196075124\n",
      "Average test loss: 0.0051387155496825775\n",
      "Epoch 215/300\n",
      "Average training loss: 0.042343600981765324\n",
      "Average test loss: 0.002693111268803477\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04217961816986402\n",
      "Average test loss: 0.0026234849457525545\n",
      "Epoch 217/300\n",
      "Average training loss: 0.042246451980537836\n",
      "Average test loss: 0.0027097927749984794\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04200129216909409\n",
      "Average test loss: 0.006254271276709107\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04187864490681224\n",
      "Average test loss: 0.004276177079934213\n",
      "Epoch 220/300\n",
      "Average training loss: 0.041804416103495494\n",
      "Average test loss: 0.00605314417473144\n",
      "Epoch 221/300\n",
      "Average training loss: 1.184391564210256\n",
      "Average test loss: 39.33546260633899\n",
      "Epoch 222/300\n",
      "Average training loss: 0.39011664679315355\n",
      "Average test loss: 0.0031719222341974576\n",
      "Epoch 223/300\n",
      "Average training loss: 0.21018449250857035\n",
      "Average test loss: 0.003083210577360458\n",
      "Epoch 224/300\n",
      "Average training loss: 0.15352706283993192\n",
      "Average test loss: 0.0029536912846896382\n",
      "Epoch 225/300\n",
      "Average training loss: 0.12988477209541532\n",
      "Average test loss: 0.0028592384457588197\n",
      "Epoch 226/300\n",
      "Average training loss: 0.11270172272125879\n",
      "Average test loss: 0.0027924898701409497\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09776374742719862\n",
      "Average test loss: 0.002851656408980489\n",
      "Epoch 228/300\n",
      "Average training loss: 0.084570166779889\n",
      "Average test loss: 0.002787364914599392\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07596192289061017\n",
      "Average test loss: 0.0027349206360263957\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07077100625965331\n",
      "Average test loss: 0.002800833103350467\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06688499830166499\n",
      "Average test loss: 0.003359187236469653\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06406624135043885\n",
      "Average test loss: 0.0027587926928988762\n",
      "Epoch 233/300\n",
      "Average training loss: 0.061553082585334776\n",
      "Average test loss: 0.002609938370063901\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05957462493578593\n",
      "Average test loss: 0.004208857126740946\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05772772142622206\n",
      "Average test loss: 0.00257422760170367\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05614888878663381\n",
      "Average test loss: 0.0031131367861396736\n",
      "Epoch 237/300\n",
      "Average training loss: 0.054619127313296\n",
      "Average test loss: 0.00256056926627126\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05331436973147922\n",
      "Average test loss: 0.0025628335362093317\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05207088437014156\n",
      "Average test loss: 0.002520712960097525\n",
      "Epoch 240/300\n",
      "Average training loss: 0.050973707298437754\n",
      "Average test loss: 0.0032760451233221424\n",
      "Epoch 241/300\n",
      "Average training loss: 0.050125535958343084\n",
      "Average test loss: 0.002650603665349384\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04914126715064049\n",
      "Average test loss: 0.002676383354390661\n",
      "Epoch 243/300\n",
      "Average training loss: 0.048290451099475225\n",
      "Average test loss: 0.002582276556019982\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0475147578616937\n",
      "Average test loss: 0.002548832396666209\n",
      "Epoch 245/300\n",
      "Average training loss: 0.046840512928035524\n",
      "Average test loss: 0.002663783060800698\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04624547368453608\n",
      "Average test loss: 0.003939914860659175\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04709195720156034\n",
      "Average test loss: 0.002529084162062241\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04527300288942125\n",
      "Average test loss: 0.004773273194829623\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04451884213752217\n",
      "Average test loss: 0.0025224515570120678\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04415797255436579\n",
      "Average test loss: 0.0043833196891678704\n",
      "Epoch 251/300\n",
      "Average training loss: 0.043703697618510985\n",
      "Average test loss: 0.0028068391488244135\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04329311066865921\n",
      "Average test loss: 0.0026412543242590293\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04305309525794453\n",
      "Average test loss: 0.0026057910753620995\n",
      "Epoch 254/300\n",
      "Average training loss: 0.042696048822667865\n",
      "Average test loss: 0.002604165898222062\n",
      "Epoch 255/300\n",
      "Average training loss: 0.042463671293523575\n",
      "Average test loss: 0.01612753008471595\n",
      "Epoch 256/300\n",
      "Average training loss: 0.042262131710847216\n",
      "Average test loss: 0.008808981072571543\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04340259678827392\n",
      "Average test loss: 0.006725542141331566\n",
      "Epoch 258/300\n",
      "Average training loss: 0.17791941682828796\n",
      "Average test loss: 0.05498924747047325\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0810352747771475\n",
      "Average test loss: 0.002728066626108355\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06899150311284595\n",
      "Average test loss: 0.004059650206524465\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06418041635221905\n",
      "Average test loss: 0.002589862463581893\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06046368062496185\n",
      "Average test loss: 0.00265284201513148\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05762786277797487\n",
      "Average test loss: 0.002564972918894556\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05500700030393071\n",
      "Average test loss: 0.0027132012500531142\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0527214362124602\n",
      "Average test loss: 0.03415562373855048\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05080162379807896\n",
      "Average test loss: 0.013955803182390002\n",
      "Epoch 267/300\n",
      "Average training loss: 0.049126032925314374\n",
      "Average test loss: 3506.2478715717925\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04757548983560668\n",
      "Average test loss: 0.0025596750697327985\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04640796571638849\n",
      "Average test loss: 0.05735767331471046\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04526645364032851\n",
      "Average test loss: 0.0025869182758033278\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04436033794615004\n",
      "Average test loss: 0.9454010610481103\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04362359095282025\n",
      "Average test loss: 0.002603707294911146\n",
      "Epoch 273/300\n",
      "Average training loss: 0.043183305240339705\n",
      "Average test loss: 0.002601370427550541\n",
      "Epoch 274/300\n",
      "Average training loss: 0.042663162052631376\n",
      "Average test loss: 0.0026516318501283724\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04255592421359486\n",
      "Average test loss: 0.0026092950399551128\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0420923057463434\n",
      "Average test loss: 0.0025633478802111415\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04196860322025087\n",
      "Average test loss: 0.0026708180546346637\n",
      "Epoch 278/300\n",
      "Average training loss: 0.041692013853126104\n",
      "Average test loss: 0.0026306898285531335\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04170023912853665\n",
      "Average test loss: 0.002883882413100865\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04153191020091375\n",
      "Average test loss: 0.008210132698218027\n",
      "Epoch 281/300\n",
      "Average training loss: 0.041279139558474225\n",
      "Average test loss: 0.002736358919698331\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04122054657671187\n",
      "Average test loss: 0.002616640089286698\n",
      "Epoch 283/300\n",
      "Average training loss: 0.041295620388454864\n",
      "Average test loss: 0.0027106086615886955\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04111001117361916\n",
      "Average test loss: 0.00258071656876968\n",
      "Epoch 285/300\n",
      "Average training loss: 0.040899974783261614\n",
      "Average test loss: 0.0029731853384938503\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04086858039928807\n",
      "Average test loss: 0.0028548425193876026\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04079207477635807\n",
      "Average test loss: 0.0027858495343890456\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04073737187849151\n",
      "Average test loss: 0.0038544381207062136\n",
      "Epoch 289/300\n",
      "Average training loss: 0.041344154593017365\n",
      "Average test loss: 0.01912873950931761\n",
      "Epoch 290/300\n",
      "Average training loss: 2.4173362334138817\n",
      "Average test loss: 0.006374749059892363\n",
      "Epoch 291/300\n",
      "Average training loss: 0.8775232107374403\n",
      "Average test loss: 0.00409038947812385\n",
      "Epoch 292/300\n",
      "Average training loss: 0.5893556484116448\n",
      "Average test loss: 0.004536750697427326\n",
      "Epoch 293/300\n",
      "Average training loss: 0.42888756219546\n",
      "Average test loss: 0.0036532448451552127\n",
      "Epoch 294/300\n",
      "Average training loss: 0.30962265843815273\n",
      "Average test loss: 0.003940646007864011\n",
      "Epoch 295/300\n",
      "Average training loss: 0.2078652376068963\n",
      "Average test loss: 0.00542288626357913\n",
      "Epoch 296/300\n",
      "Average training loss: 0.151028659052319\n",
      "Average test loss: 0.009925712135930857\n",
      "Epoch 297/300\n",
      "Average training loss: 0.12834323105547163\n",
      "Average test loss: 0.0035905101021958723\n",
      "Epoch 298/300\n",
      "Average training loss: 0.11232835887538062\n",
      "Average test loss: 0.0029273880078560777\n",
      "Epoch 299/300\n",
      "Average training loss: 0.100181697262658\n",
      "Average test loss: 0.016487468525353405\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09000927589999305\n",
      "Average test loss: 0.003607051738434368\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.409467609829373\n",
      "Average test loss: 0.013608077401088345\n",
      "Epoch 2/300\n",
      "Average training loss: 2.193965487692091\n",
      "Average test loss: 0.021095695317619376\n",
      "Epoch 3/300\n",
      "Average training loss: 1.7114078761206732\n",
      "Average test loss: 1.2985950606597794\n",
      "Epoch 4/300\n",
      "Average training loss: 1.416493817753262\n",
      "Average test loss: 0.004560536278204785\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2224504325654773\n",
      "Average test loss: 0.004255434175746309\n",
      "Epoch 6/300\n",
      "Average training loss: 1.071692344400618\n",
      "Average test loss: 0.00427661830973294\n",
      "Epoch 7/300\n",
      "Average training loss: 0.9463950290150113\n",
      "Average test loss: 0.004054095179670387\n",
      "Epoch 8/300\n",
      "Average training loss: 0.8446553497844272\n",
      "Average test loss: 0.005041810277849436\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7535570427046882\n",
      "Average test loss: 0.00557632904044456\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6740344417889913\n",
      "Average test loss: 0.003343590900922815\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6051193633609347\n",
      "Average test loss: 0.003165635937824845\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5427237167358399\n",
      "Average test loss: 0.004325830547346009\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4879094694720374\n",
      "Average test loss: 0.003026119183128079\n",
      "Epoch 14/300\n",
      "Average training loss: 0.43866045604811776\n",
      "Average test loss: 0.003992052750248048\n",
      "Epoch 15/300\n",
      "Average training loss: 0.39422225075297884\n",
      "Average test loss: 0.002687398829186956\n",
      "Epoch 16/300\n",
      "Average training loss: 0.35296475426356\n",
      "Average test loss: 0.0027735001184046267\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3173835273053911\n",
      "Average test loss: 0.0045572536989218655\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2841501185629103\n",
      "Average test loss: 0.0025104542499822047\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2543381366332372\n",
      "Average test loss: 0.0025033614018725022\n",
      "Epoch 20/300\n",
      "Average training loss: 0.22881590064366658\n",
      "Average test loss: 0.0024529574988409876\n",
      "Epoch 21/300\n",
      "Average training loss: 0.206887718545066\n",
      "Average test loss: 0.002653901650259892\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1873945203754637\n",
      "Average test loss: 0.004683484676087068\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1704448408683141\n",
      "Average test loss: 0.00455836795642972\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15628704966439141\n",
      "Average test loss: 0.002610181674361229\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14270649629831314\n",
      "Average test loss: 0.002363412676172124\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1316713760693868\n",
      "Average test loss: 0.0021835195768831503\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12090492557154761\n",
      "Average test loss: 0.0021162030863472155\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11284793155060874\n",
      "Average test loss: 0.0019599991839172114\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10448494521776835\n",
      "Average test loss: 0.003799142080669602\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09705992474820879\n",
      "Average test loss: 0.0019294539131224156\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0901392681201299\n",
      "Average test loss: 0.9857111463596423\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08487531848086251\n",
      "Average test loss: 0.5143189546780453\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07924797199500931\n",
      "Average test loss: 0.007119411936650674\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07524686330556869\n",
      "Average test loss: 0.0020120525292845236\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07110346745782428\n",
      "Average test loss: 0.06662419488032659\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06771035327514012\n",
      "Average test loss: 0.002013733041369253\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06370070112744967\n",
      "Average test loss: 0.0019197704387932189\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06162717405292723\n",
      "Average test loss: 0.008528764033069213\n",
      "Epoch 39/300\n",
      "Average training loss: 0.058420724040932126\n",
      "Average test loss: 0.0017632570064937075\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05711165593730079\n",
      "Average test loss: 0.0018437846787273883\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05458277026481099\n",
      "Average test loss: 6.893620129991737\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07959453905953301\n",
      "Average test loss: 0.0019363584095198247\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05676123012105624\n",
      "Average test loss: 0.0018772330524192916\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05304903335372607\n",
      "Average test loss: 0.002270137236143152\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05013351080152723\n",
      "Average test loss: 0.014018336640877855\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04841044943862491\n",
      "Average test loss: 0.001756314699124131\n",
      "Epoch 47/300\n",
      "Average training loss: 0.047049350765016346\n",
      "Average test loss: 0.0017204973675931494\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04607176822092798\n",
      "Average test loss: 0.0017092954552628927\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04670880788895819\n",
      "Average test loss: 0.00174855202022526\n",
      "Epoch 50/300\n",
      "Average training loss: 0.057279957854085496\n",
      "Average test loss: 0.00191572980913851\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0539488968749841\n",
      "Average test loss: 0.0018023156631324026\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04574675573739741\n",
      "Average test loss: 0.001811084752695428\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04412729120420085\n",
      "Average test loss: 0.0018802182465377782\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04303403255674574\n",
      "Average test loss: 0.0017262334044401844\n",
      "Epoch 55/300\n",
      "Average training loss: 0.042539136088556716\n",
      "Average test loss: 0.008588681302550766\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04176877894335323\n",
      "Average test loss: 0.0017195617964284287\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0414079151418474\n",
      "Average test loss: 1.080922585275438\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04088287177019649\n",
      "Average test loss: 0.0017300358628854155\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04019926557275984\n",
      "Average test loss: 0.0017438830201410584\n",
      "Epoch 60/300\n",
      "Average training loss: 0.040290811399618785\n",
      "Average test loss: 0.002869603486938609\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05657478972607189\n",
      "Average test loss: 0.061782362792640926\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06648022445374065\n",
      "Average test loss: 0.0017906446235461367\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04687869395481216\n",
      "Average test loss: 0.0019744949753706656\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04348681065440178\n",
      "Average test loss: 0.0029876376073807477\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04190596144398053\n",
      "Average test loss: 0.0018259313504935966\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04091376237736808\n",
      "Average test loss: 0.0017444439973268245\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04038585062490569\n",
      "Average test loss: 0.018382734631498653\n",
      "Epoch 68/300\n",
      "Average training loss: 0.039846652635269694\n",
      "Average test loss: 0.040154288781185946\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03942807991637124\n",
      "Average test loss: 0.008718012202117178\n",
      "Epoch 70/300\n",
      "Average training loss: 0.039034626533587774\n",
      "Average test loss: 0.0024637695574719043\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0385968992014726\n",
      "Average test loss: 0.0033420310030794808\n",
      "Epoch 72/300\n",
      "Average training loss: 0.038436545719703036\n",
      "Average test loss: 0.0019063556893832154\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03824628857771556\n",
      "Average test loss: 0.0016894956685395703\n",
      "Epoch 74/300\n",
      "Average training loss: 0.037960199031564924\n",
      "Average test loss: 0.0016432721151245966\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03803516307473183\n",
      "Average test loss: 0.0017398107117041945\n",
      "Epoch 76/300\n",
      "Average training loss: 0.038898725546068616\n",
      "Average test loss: 0.0016734255994152692\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03773160445690155\n",
      "Average test loss: 0.00460927199323972\n",
      "Epoch 78/300\n",
      "Average training loss: 0.037047933489084243\n",
      "Average test loss: 0.0048750354186114335\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03670759033825662\n",
      "Average test loss: 0.0016702434942126275\n",
      "Epoch 80/300\n",
      "Average training loss: 0.036473338968223995\n",
      "Average test loss: 0.0016480266730715004\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03612565313776334\n",
      "Average test loss: 0.0016830980550083848\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03612132981088426\n",
      "Average test loss: 0.00692650165160497\n",
      "Epoch 83/300\n",
      "Average training loss: 0.035743834485610325\n",
      "Average test loss: 0.016250038923902643\n",
      "Epoch 84/300\n",
      "Average training loss: 0.035476311809486814\n",
      "Average test loss: 0.12455265281846126\n",
      "Epoch 85/300\n",
      "Average training loss: 0.036514163095090126\n",
      "Average test loss: 0.04531307458132505\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05201699332727326\n",
      "Average test loss: 0.0017495077970541186\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04004521838492817\n",
      "Average test loss: 0.0019476686869230534\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03771796886622906\n",
      "Average test loss: 0.07691630420585473\n",
      "Epoch 89/300\n",
      "Average training loss: 0.036624975540571744\n",
      "Average test loss: 0.0030639651181797187\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03583455069197549\n",
      "Average test loss: 0.0017934690739752517\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03585552069544792\n",
      "Average test loss: 0.0027158452729798025\n",
      "Epoch 92/300\n",
      "Average training loss: 0.035219054341316225\n",
      "Average test loss: 0.0017572664320468902\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03501674778262774\n",
      "Average test loss: 0.0016655184361669753\n",
      "Epoch 94/300\n",
      "Average training loss: 0.034753209170367984\n",
      "Average test loss: 0.0016600091469784577\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03462785048948394\n",
      "Average test loss: 0.0017203249869247278\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03449961321221458\n",
      "Average test loss: 0.0017618003324088123\n",
      "Epoch 97/300\n",
      "Average training loss: 0.034911814383334586\n",
      "Average test loss: 0.005160800367179844\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03428583975798554\n",
      "Average test loss: 0.00337192600644711\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03394010550114843\n",
      "Average test loss: 0.0016237248646923238\n",
      "Epoch 100/300\n",
      "Average training loss: 0.033935329467058184\n",
      "Average test loss: 0.0016826447094273236\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03360671658317248\n",
      "Average test loss: 0.0024970757616683843\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03387606213821305\n",
      "Average test loss: 0.0019815587585067583\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03336063536339336\n",
      "Average test loss: 0.004722455096017156\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03398588585688008\n",
      "Average test loss: 0.0016340194774998559\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06758921997083558\n",
      "Average test loss: 0.0017279690232955747\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04538464174336857\n",
      "Average test loss: 0.11656205738998121\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04097835067576832\n",
      "Average test loss: 0.0017380181632729041\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03857809915145238\n",
      "Average test loss: 0.0016179763473984268\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03706231326195929\n",
      "Average test loss: 0.0015857042951716318\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0359108418987857\n",
      "Average test loss: 0.0016119932746514677\n",
      "Epoch 111/300\n",
      "Average training loss: 0.035340285658836366\n",
      "Average test loss: 0.0017104317193023032\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03450689842303594\n",
      "Average test loss: 0.45746755306588277\n",
      "Epoch 113/300\n",
      "Average training loss: 0.034246217674679225\n",
      "Average test loss: 0.0018041763874805637\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03371480153997739\n",
      "Average test loss: 0.0015822916939440702\n",
      "Epoch 115/300\n",
      "Average training loss: 0.035054654409488045\n",
      "Average test loss: 0.005428115836344659\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03333250016636319\n",
      "Average test loss: 0.00310043237834341\n",
      "Epoch 117/300\n",
      "Average training loss: 0.033006268531084064\n",
      "Average test loss: 0.0017099272368165353\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03290884057349629\n",
      "Average test loss: 0.0017269742762049038\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03281805179516475\n",
      "Average test loss: 0.0028219242888606255\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03278386571009954\n",
      "Average test loss: 0.002801677750630511\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0326398816174931\n",
      "Average test loss: 0.00199997208867636\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0333651628676388\n",
      "Average test loss: 0.0017248143371608523\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03254776428805457\n",
      "Average test loss: 0.04446070738385121\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03224121489458614\n",
      "Average test loss: 0.0016259400129525197\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03657148483395577\n",
      "Average test loss: 0.003265248019869129\n",
      "Epoch 126/300\n",
      "Average training loss: 0.033093289012710254\n",
      "Average test loss: 0.001601725405185587\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03248371885882484\n",
      "Average test loss: 0.0016994946002960205\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03225519434114297\n",
      "Average test loss: 0.0017566700418376261\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0321052111685276\n",
      "Average test loss: 0.0016257410458703008\n",
      "Epoch 130/300\n",
      "Average training loss: 0.031918636138240496\n",
      "Average test loss: 0.001611944511015382\n",
      "Epoch 131/300\n",
      "Average training loss: 0.042746226941545805\n",
      "Average test loss: 0.004383306793972022\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03381027680304315\n",
      "Average test loss: 0.001644229659396741\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03243470617135366\n",
      "Average test loss: 0.0016086199281530249\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03199059037036366\n",
      "Average test loss: 0.0017111647104223568\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03174323826531569\n",
      "Average test loss: 0.0016780530599773758\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03172658179203669\n",
      "Average test loss: 0.0021329715988702242\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031805348303582934\n",
      "Average test loss: 0.0020925455065444114\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03164154776599672\n",
      "Average test loss: 0.0016350277844919927\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03150466036465433\n",
      "Average test loss: 0.002062628233494858\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03145721780260404\n",
      "Average test loss: 0.0017397819293869866\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03315607183509403\n",
      "Average test loss: 0.0016419590923728215\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03151110428737269\n",
      "Average test loss: 0.0038180360918243727\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03191942521101899\n",
      "Average test loss: 0.003266994441445503\n",
      "Epoch 144/300\n",
      "Average training loss: 0.031046974203652804\n",
      "Average test loss: 0.001666444088332355\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030971333068278102\n",
      "Average test loss: 0.0033251027994685703\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030891902105675804\n",
      "Average test loss: 8.175451629952423\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03083655235171318\n",
      "Average test loss: 0.0019362175909595357\n",
      "Epoch 148/300\n",
      "Average training loss: 0.030887396302488115\n",
      "Average test loss: 0.001659195598007904\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030783708385295337\n",
      "Average test loss: 0.0017192480413036214\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030516562066144413\n",
      "Average test loss: 0.001770259015262127\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030511329119404157\n",
      "Average test loss: 0.004115798200791081\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03056393504473898\n",
      "Average test loss: 0.001754034010693431\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030499107869135008\n",
      "Average test loss: 0.0018662524501689606\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03072931438518895\n",
      "Average test loss: 0.010690717609392272\n",
      "Epoch 155/300\n",
      "Average training loss: 0.030185826753576596\n",
      "Average test loss: 0.0018281256584450603\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030104574095871712\n",
      "Average test loss: 0.0016293478504392422\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03004535902871026\n",
      "Average test loss: 0.0021217655988617075\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030104399498966004\n",
      "Average test loss: 0.001733911791505913\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02998286830385526\n",
      "Average test loss: 0.001640783858123339\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03017521876593431\n",
      "Average test loss: 0.013471421482248438\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02989279217686918\n",
      "Average test loss: 0.0018666075847836004\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02990705212785138\n",
      "Average test loss: 0.001661835851561692\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029660089732872114\n",
      "Average test loss: 0.6655006915168423\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030858219732840857\n",
      "Average test loss: 0.0017879275222205454\n",
      "Epoch 165/300\n",
      "Average training loss: 0.029782694035106236\n",
      "Average test loss: 0.0017718151946448618\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029472123323215377\n",
      "Average test loss: 0.0016823953251457876\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03296071422927909\n",
      "Average test loss: 0.0016883344173224436\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030664771596590677\n",
      "Average test loss: 0.0016945009540973437\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02964549500743548\n",
      "Average test loss: 0.0016596700095882018\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02963429574502839\n",
      "Average test loss: 0.0016930862174679836\n",
      "Epoch 171/300\n",
      "Average training loss: 0.029232192208369572\n",
      "Average test loss: 0.0017426507255683342\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02934663822253545\n",
      "Average test loss: 0.0016890482015700804\n",
      "Epoch 173/300\n",
      "Average training loss: 0.048658443884717094\n",
      "Average test loss: 0.0016809814038376014\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03301778008871608\n",
      "Average test loss: 0.0016578292621092664\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030815332960751323\n",
      "Average test loss: 0.0016871549722014202\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02995305946469307\n",
      "Average test loss: 0.001662526584851245\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029484904244542122\n",
      "Average test loss: 0.0016397856117950544\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02922599494457245\n",
      "Average test loss: 0.0018274560572786463\n",
      "Epoch 179/300\n",
      "Average training loss: 0.029196544690264597\n",
      "Average test loss: 0.0018004356351577573\n",
      "Epoch 180/300\n",
      "Average training loss: 0.029167498578627903\n",
      "Average test loss: 0.0019389011949921648\n",
      "Epoch 181/300\n",
      "Average training loss: 0.029112693993581667\n",
      "Average test loss: 0.001690408408900516\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02906355533334944\n",
      "Average test loss: 0.0017936904480059942\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029087680442465676\n",
      "Average test loss: 1.9985723452568054\n",
      "Epoch 184/300\n",
      "Average training loss: 0.029357707689205804\n",
      "Average test loss: 0.0019138813498947355\n",
      "Epoch 185/300\n",
      "Average training loss: 0.029245035141706465\n",
      "Average test loss: 0.0017688391467349397\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02896653405825297\n",
      "Average test loss: 0.0020228988393727275\n",
      "Epoch 187/300\n",
      "Average training loss: 0.029021704292959638\n",
      "Average test loss: 0.034370494896339046\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0289264810797241\n",
      "Average test loss: 0.0018823393057617877\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02905700892210007\n",
      "Average test loss: 0.0038416954723911154\n",
      "Epoch 190/300\n",
      "Average training loss: 0.028689488502012358\n",
      "Average test loss: 0.001739078225567937\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02868306416604254\n",
      "Average test loss: 0.0017312441378210981\n",
      "Epoch 192/300\n",
      "Average training loss: 0.028660504839486546\n",
      "Average test loss: 0.0018582359010146723\n",
      "Epoch 193/300\n",
      "Average training loss: 0.028785993203520776\n",
      "Average test loss: 0.0017354711457673046\n",
      "Epoch 194/300\n",
      "Average training loss: 0.028718642012940512\n",
      "Average test loss: 0.002231693525177737\n",
      "Epoch 195/300\n",
      "Average training loss: 0.028542235771814983\n",
      "Average test loss: 0.9214072853351633\n",
      "Epoch 196/300\n",
      "Average training loss: 0.028988949282301796\n",
      "Average test loss: 0.0016596819963306188\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028481463311447037\n",
      "Average test loss: 0.0019957446790196827\n",
      "Epoch 198/300\n",
      "Average training loss: 0.028771218021710714\n",
      "Average test loss: 0.0021057739971826472\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02866875839730104\n",
      "Average test loss: 0.002133919809634487\n",
      "Epoch 200/300\n",
      "Average training loss: 0.028583214751548236\n",
      "Average test loss: 0.0021012194975175793\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02832067280345493\n",
      "Average test loss: 0.0018275769562977884\n",
      "Epoch 202/300\n",
      "Average training loss: 0.028204589621888265\n",
      "Average test loss: 0.0021343509714222618\n",
      "Epoch 203/300\n",
      "Average training loss: 0.028359015867114066\n",
      "Average test loss: 0.0020201726372664174\n",
      "Epoch 204/300\n",
      "Average training loss: 0.028551791081825893\n",
      "Average test loss: 0.008474102683365344\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028218917835917737\n",
      "Average test loss: 0.0017225664243515995\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0282451371120082\n",
      "Average test loss: 0.001720267710586389\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028380909828676117\n",
      "Average test loss: 0.001727600447109176\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02817405930492613\n",
      "Average test loss: 0.0038713801273455224\n",
      "Epoch 209/300\n",
      "Average training loss: 0.028043312004870837\n",
      "Average test loss: 0.111296046183755\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02821948811246289\n",
      "Average test loss: 0.0016878591804868645\n",
      "Epoch 211/300\n",
      "Average training loss: 0.028102291218108602\n",
      "Average test loss: 0.0019922603632633886\n",
      "Epoch 212/300\n",
      "Average training loss: 0.027932684125171768\n",
      "Average test loss: 0.001768560904906028\n",
      "Epoch 213/300\n",
      "Average training loss: 0.028219775802559324\n",
      "Average test loss: 0.0017442548484024073\n",
      "Epoch 214/300\n",
      "Average training loss: 0.028048759089575874\n",
      "Average test loss: 0.0018260388481948111\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02800941444436709\n",
      "Average test loss: 0.001999939251691103\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02797394994397958\n",
      "Average test loss: 0.0049283850350313714\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0278402256452375\n",
      "Average test loss: 0.002092088982048962\n",
      "Epoch 218/300\n",
      "Average training loss: 0.027994283143017027\n",
      "Average test loss: 0.0017320260231693586\n",
      "Epoch 219/300\n",
      "Average training loss: 0.027785791019598643\n",
      "Average test loss: 0.009081166586114301\n",
      "Epoch 220/300\n",
      "Average training loss: 0.027929570097062324\n",
      "Average test loss: 0.0017514991365994016\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028093300950196055\n",
      "Average test loss: 0.23564870717624822\n",
      "Epoch 222/300\n",
      "Average training loss: 0.027975712382131153\n",
      "Average test loss: 0.001745946404006746\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02761840973628892\n",
      "Average test loss: 0.0021262094981761444\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02772754466202524\n",
      "Average test loss: 30.792840340232996\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02786280639635192\n",
      "Average test loss: 0.0022474560692078535\n",
      "Epoch 226/300\n",
      "Average training loss: 0.027740200711621178\n",
      "Average test loss: 0.0018495251488768391\n",
      "Epoch 227/300\n",
      "Average training loss: 0.027529861961801846\n",
      "Average test loss: 0.0017113086504654752\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02771347695754634\n",
      "Average test loss: 0.0017577131870461422\n",
      "Epoch 229/300\n",
      "Average training loss: 0.027512260648939343\n",
      "Average test loss: 0.0018021420554982291\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0277651775230964\n",
      "Average test loss: 0.0018624313624782695\n",
      "Epoch 231/300\n",
      "Average training loss: 0.027739091714223226\n",
      "Average test loss: 0.0019861408712135422\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02760078951054149\n",
      "Average test loss: 0.0018484505169714491\n",
      "Epoch 233/300\n",
      "Average training loss: 0.027601288724276755\n",
      "Average test loss: 0.002145813923743036\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029385686948895453\n",
      "Average test loss: 0.001863339431790842\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02736266967157523\n",
      "Average test loss: 0.002061894808171524\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0273210259526968\n",
      "Average test loss: 0.0017650579369316499\n",
      "Epoch 237/300\n",
      "Average training loss: 0.027321573573682045\n",
      "Average test loss: 0.0017486406707515319\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02738822251558304\n",
      "Average test loss: 0.0018409883397527867\n",
      "Epoch 239/300\n",
      "Average training loss: 0.027573220969902144\n",
      "Average test loss: 0.0017138171853083703\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02760042827659183\n",
      "Average test loss: 0.09608260520547629\n",
      "Epoch 241/300\n",
      "Average training loss: 0.027299965750839976\n",
      "Average test loss: 0.001737992893387046\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027257857766416337\n",
      "Average test loss: 0.0017723222378020485\n",
      "Epoch 243/300\n",
      "Average training loss: 0.027469329744577407\n",
      "Average test loss: 0.0017817617382647262\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02728935039208995\n",
      "Average test loss: 0.0018983392679753402\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02722155161201954\n",
      "Average test loss: 0.0021548687333448066\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02726790731648604\n",
      "Average test loss: 0.002150188075171577\n",
      "Epoch 247/300\n",
      "Average training loss: 0.027433132820659213\n",
      "Average test loss: 0.005154055438107915\n",
      "Epoch 248/300\n",
      "Average training loss: 0.027186286942826376\n",
      "Average test loss: 0.0017998409734831917\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027148311792148484\n",
      "Average test loss: 0.0017759053365637859\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027323836298452482\n",
      "Average test loss: 0.0017375461721482377\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027133994213408893\n",
      "Average test loss: 0.0029442505263206032\n",
      "Epoch 252/300\n",
      "Average training loss: 0.027216329594453175\n",
      "Average test loss: 0.0017121516834530566\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02715811650454998\n",
      "Average test loss: 0.0017815357893705367\n",
      "Epoch 254/300\n",
      "Average training loss: 0.027157115303807787\n",
      "Average test loss: 0.0018384572874961629\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02721461618112193\n",
      "Average test loss: 0.002327552099401752\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027077744482292068\n",
      "Average test loss: 0.0017902920110047691\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02712926172051165\n",
      "Average test loss: 0.002761575462917487\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026989875074889924\n",
      "Average test loss: 0.0018291755680822663\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026993054510818588\n",
      "Average test loss: 0.002866037620231509\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02703469345304701\n",
      "Average test loss: 0.0017709623250282472\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030988048470682567\n",
      "Average test loss: 0.0022797455744196973\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027885242985354528\n",
      "Average test loss: 0.05393529775904284\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027020308138595687\n",
      "Average test loss: 0.05867520555026001\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02686962352693081\n",
      "Average test loss: 0.0017477648478622237\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026842580379711257\n",
      "Average test loss: 0.0017535739197499223\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026895234611299303\n",
      "Average test loss: 0.0018209390429676407\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026893942604462307\n",
      "Average test loss: 0.0017472622125513024\n",
      "Epoch 268/300\n",
      "Average training loss: 0.027253043805559478\n",
      "Average test loss: 0.0017823515005616678\n",
      "Epoch 269/300\n",
      "Average training loss: 0.026866491582658556\n",
      "Average test loss: 0.0017738042994298868\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027036949086520407\n",
      "Average test loss: 0.002183582003538807\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02679532254735629\n",
      "Average test loss: 0.00543772075097594\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026788582076629004\n",
      "Average test loss: 0.001737574864178896\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02697013710770342\n",
      "Average test loss: 0.038700355690697\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02674809294939041\n",
      "Average test loss: 0.0022965327927635774\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02715272849963771\n",
      "Average test loss: 0.0017018843084159825\n",
      "Epoch 276/300\n",
      "Average training loss: 0.031733752893077005\n",
      "Average test loss: 0.0016924962719074555\n",
      "Epoch 277/300\n",
      "Average training loss: 0.027891308698389267\n",
      "Average test loss: 0.0017395820766687392\n",
      "Epoch 278/300\n",
      "Average training loss: 0.026885804967747794\n",
      "Average test loss: 0.0017595140949512522\n",
      "Epoch 279/300\n",
      "Average training loss: 0.026695518487029606\n",
      "Average test loss: 0.001913695711021622\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026624872263934878\n",
      "Average test loss: 0.0019914526119828224\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02664302085340023\n",
      "Average test loss: 0.0018941634618159797\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02786597003042698\n",
      "Average test loss: 53.71021255970581\n",
      "Epoch 283/300\n",
      "Average training loss: 0.028140065749486288\n",
      "Average test loss: 0.0018809505447538362\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026647613583339586\n",
      "Average test loss: 0.0017886655455869105\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026566681896646818\n",
      "Average test loss: 0.0017366309542622832\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02653173547817601\n",
      "Average test loss: 0.0017590676471590995\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026643690519862706\n",
      "Average test loss: 0.0017604467471440633\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026910708352923394\n",
      "Average test loss: 0.00183381827098007\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0266850283096234\n",
      "Average test loss: 0.001806016262413727\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02659501997133096\n",
      "Average test loss: 0.0017528353853979045\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026597117311424678\n",
      "Average test loss: 0.001818491980433464\n",
      "Epoch 292/300\n",
      "Average training loss: 0.027124388322234155\n",
      "Average test loss: 0.0017606209915959172\n",
      "Epoch 293/300\n",
      "Average training loss: 0.026539070723785294\n",
      "Average test loss: 0.001817864375292427\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02643902312219143\n",
      "Average test loss: 0.0017756509286248022\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026808731355600888\n",
      "Average test loss: 0.011247418419354492\n",
      "Epoch 296/300\n",
      "Average training loss: 0.026525767841272883\n",
      "Average test loss: 0.0018706300922979912\n",
      "Epoch 297/300\n",
      "Average training loss: 0.026694508247905306\n",
      "Average test loss: 0.0018408190794806513\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02657203357749515\n",
      "Average test loss: 0.00179470397811383\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02728363500866625\n",
      "Average test loss: 0.004423025504582458\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02654232042696741\n",
      "Average test loss: 0.001971835955046117\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.231701595518324\n",
      "Average test loss: 0.5316495595342583\n",
      "Epoch 2/300\n",
      "Average training loss: 3.1754917015499537\n",
      "Average test loss: 0.005301308649695582\n",
      "Epoch 3/300\n",
      "Average training loss: 2.5199671664767793\n",
      "Average test loss: 0.004427384881923596\n",
      "Epoch 4/300\n",
      "Average training loss: 2.1523610661824546\n",
      "Average test loss: 0.0041496022958308456\n",
      "Epoch 5/300\n",
      "Average training loss: 1.8911006577809653\n",
      "Average test loss: 0.0035573741238978172\n",
      "Epoch 6/300\n",
      "Average training loss: 1.679825134807163\n",
      "Average test loss: 0.2024995357990265\n",
      "Epoch 7/300\n",
      "Average training loss: 1.493375017589993\n",
      "Average test loss: 0.0033192283409751123\n",
      "Epoch 8/300\n",
      "Average training loss: 1.3341635874642266\n",
      "Average test loss: 0.0029994913447234364\n",
      "Epoch 9/300\n",
      "Average training loss: 1.193240229182773\n",
      "Average test loss: 0.002963304876246386\n",
      "Epoch 10/300\n",
      "Average training loss: 1.0710576768451268\n",
      "Average test loss: 0.0026480054000599515\n",
      "Epoch 11/300\n",
      "Average training loss: 0.9703214894400702\n",
      "Average test loss: 0.002684078373325368\n",
      "Epoch 12/300\n",
      "Average training loss: 0.8818711619377136\n",
      "Average test loss: 0.002680322106720673\n",
      "Epoch 13/300\n",
      "Average training loss: 0.8038892631530762\n",
      "Average test loss: 0.002551326568548878\n",
      "Epoch 14/300\n",
      "Average training loss: 0.7322705586751302\n",
      "Average test loss: 0.002308738867234853\n",
      "Epoch 15/300\n",
      "Average training loss: 0.6663547547658285\n",
      "Average test loss: 0.002204307183623314\n",
      "Epoch 16/300\n",
      "Average training loss: 0.605994987487793\n",
      "Average test loss: 0.002915347264872657\n",
      "Epoch 17/300\n",
      "Average training loss: 0.550843127462599\n",
      "Average test loss: 0.0020687033660295936\n",
      "Epoch 18/300\n",
      "Average training loss: 0.49947702439626057\n",
      "Average test loss: 0.0020072945753733316\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4515057543648614\n",
      "Average test loss: 0.0018917186418548227\n",
      "Epoch 20/300\n",
      "Average training loss: 0.40731596620877586\n",
      "Average test loss: 0.0023388955226788916\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3667510185506609\n",
      "Average test loss: 0.001923186911476983\n",
      "Epoch 22/300\n",
      "Average training loss: 0.32910478398534987\n",
      "Average test loss: 0.002050439142311613\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2943601812786526\n",
      "Average test loss: 0.0021505467862718633\n",
      "Epoch 24/300\n",
      "Average training loss: 0.2630953144232432\n",
      "Average test loss: 0.0017597731321843135\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23492973923683166\n",
      "Average test loss: 0.0017319753385252423\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2105000360806783\n",
      "Average test loss: 0.0019453163851673404\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1891407425933414\n",
      "Average test loss: 0.002005261232248611\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1701922222243415\n",
      "Average test loss: 0.0018104699025344518\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1538332333034939\n",
      "Average test loss: 0.0015534084182646539\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13997724344995285\n",
      "Average test loss: 0.00152612052557783\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12745215962992773\n",
      "Average test loss: 0.0018027740090878473\n",
      "Epoch 32/300\n",
      "Average training loss: 0.11579847845766279\n",
      "Average test loss: 0.001791190484435194\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10568852304087745\n",
      "Average test loss: 0.0014824382683986592\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0976143465505706\n",
      "Average test loss: 0.001538218897043003\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0896540755894449\n",
      "Average test loss: 0.006671634202357382\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08240418359968397\n",
      "Average test loss: 0.0031777914892882108\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07593876634041469\n",
      "Average test loss: 0.001930299993397461\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0702712465458446\n",
      "Average test loss: 0.0019710728871739575\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06531135814057457\n",
      "Average test loss: 0.050705592796206476\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06097519703706106\n",
      "Average test loss: 3.0138683446488446\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05834574051035775\n",
      "Average test loss: 0.0016167063850702512\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05322071024444368\n",
      "Average test loss: 0.010435551358800796\n",
      "Epoch 43/300\n",
      "Average training loss: 0.050426431980397965\n",
      "Average test loss: 0.001480585152697232\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04826917304926449\n",
      "Average test loss: 0.002004025965825551\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0539952147603035\n",
      "Average test loss: 0.0013151368702658348\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04409732370244132\n",
      "Average test loss: 0.0012817105391683679\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04191780855589443\n",
      "Average test loss: 0.0018307461969347464\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04021880429652002\n",
      "Average test loss: 0.0011904139392491845\n",
      "Epoch 49/300\n",
      "Average training loss: 0.038745958341492547\n",
      "Average test loss: 0.0012939648309515583\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03759578129980299\n",
      "Average test loss: 0.003587624947954383\n",
      "Epoch 51/300\n",
      "Average training loss: 0.036110756470097434\n",
      "Average test loss: 0.0012070294086717896\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03523498517274857\n",
      "Average test loss: 0.0013618336646921104\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03426831832859251\n",
      "Average test loss: 0.001216969523164961\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03357433187961578\n",
      "Average test loss: 0.0013671802580874\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03266696760058403\n",
      "Average test loss: 0.0011696463818661868\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03214458359115654\n",
      "Average test loss: 0.23467685848888423\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0314075247877174\n",
      "Average test loss: 0.7813785571455956\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030799568701121542\n",
      "Average test loss: 0.0013051798624607424\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03169105490711\n",
      "Average test loss: 0.001228073363761521\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03351749323474036\n",
      "Average test loss: 0.0014757230277690622\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03526075082686212\n",
      "Average test loss: 0.001629636417143047\n",
      "Epoch 62/300\n",
      "Average training loss: 0.030988774228427145\n",
      "Average test loss: 0.001183113952788214\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02994654464059406\n",
      "Average test loss: 0.0012533767188950959\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029730631021989716\n",
      "Average test loss: 0.0012679879734706547\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029104442770282426\n",
      "Average test loss: 0.004028445728019707\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02888182565404309\n",
      "Average test loss: 0.005491141467872593\n",
      "Epoch 67/300\n",
      "Average training loss: 0.028645401671528816\n",
      "Average test loss: 0.0011784281120118167\n",
      "Epoch 68/300\n",
      "Average training loss: 0.028380505601565045\n",
      "Average test loss: 0.0011170935853280954\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02811500488056077\n",
      "Average test loss: 0.0010867111664265394\n",
      "Epoch 70/300\n",
      "Average training loss: 0.027894493255350324\n",
      "Average test loss: 0.0011336950270148616\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02750366779499584\n",
      "Average test loss: 0.002774007651127047\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0273985852599144\n",
      "Average test loss: 0.0012384529760521318\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02713082783917586\n",
      "Average test loss: 0.0013112049739186962\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027394998788833617\n",
      "Average test loss: 0.00283153216023412\n",
      "Epoch 75/300\n",
      "Average training loss: 0.026925002387828297\n",
      "Average test loss: 0.2526544674817059\n",
      "Epoch 76/300\n",
      "Average training loss: 0.026523554118143187\n",
      "Average test loss: 0.0011031784019950362\n",
      "Epoch 77/300\n",
      "Average training loss: 0.026481351756387286\n",
      "Average test loss: 0.01602528047768606\n",
      "Epoch 78/300\n",
      "Average training loss: 0.026844054790834584\n",
      "Average test loss: 0.0011469949626674255\n",
      "Epoch 79/300\n",
      "Average training loss: 0.026718056839373377\n",
      "Average test loss: 0.0015208698684970538\n",
      "Epoch 80/300\n",
      "Average training loss: 0.026073217335674498\n",
      "Average test loss: 0.0010629739592679672\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02589097112086084\n",
      "Average test loss: 0.0013365770126175548\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025774897356828055\n",
      "Average test loss: 3843.706899210612\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0257662294258674\n",
      "Average test loss: 0.004701986401445336\n",
      "Epoch 84/300\n",
      "Average training loss: 0.025555522410406005\n",
      "Average test loss: 0.0011062446422874927\n",
      "Epoch 85/300\n",
      "Average training loss: 0.025428919022281966\n",
      "Average test loss: 0.005917680479379164\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02564540787206756\n",
      "Average test loss: 0.017502306753562555\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025390051633119585\n",
      "Average test loss: 0.0024189674828408495\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02513186439043946\n",
      "Average test loss: 0.0012318838502590856\n",
      "Epoch 89/300\n",
      "Average training loss: 0.024946375278962984\n",
      "Average test loss: 0.003976317590982136\n",
      "Epoch 90/300\n",
      "Average training loss: 0.024857540582617126\n",
      "Average test loss: 0.0012290867577410406\n",
      "Epoch 91/300\n",
      "Average training loss: 0.024794987198379304\n",
      "Average test loss: 0.04668515764632159\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02472285984125402\n",
      "Average test loss: 0.0014688660823222663\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02452590389053027\n",
      "Average test loss: 0.0011040002844399876\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02452646060950226\n",
      "Average test loss: 0.0011086779517225094\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02455978424515989\n",
      "Average test loss: 0.0012259914556311236\n",
      "Epoch 96/300\n",
      "Average training loss: 0.024594472624361517\n",
      "Average test loss: 0.0011074653922890623\n",
      "Epoch 97/300\n",
      "Average training loss: 0.024353806313541202\n",
      "Average test loss: 0.0018360325592673486\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024125468441181713\n",
      "Average test loss: 0.001089959595559372\n",
      "Epoch 99/300\n",
      "Average training loss: 0.024052520477109485\n",
      "Average test loss: 30.186820914904278\n",
      "Epoch 100/300\n",
      "Average training loss: 0.023948896414703793\n",
      "Average test loss: 0.0012719926711999708\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02390448168085681\n",
      "Average test loss: 116.77848052651187\n",
      "Epoch 102/300\n",
      "Average training loss: 0.023835937841898865\n",
      "Average test loss: 0.005032237211552759\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02378426337076558\n",
      "Average test loss: 0.0011192999450075956\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023766213663750226\n",
      "Average test loss: 13.435618189495472\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02369723272985882\n",
      "Average test loss: 0.0018259279617211886\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02361274145791928\n",
      "Average test loss: 0.0011167010859482818\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023561673442522683\n",
      "Average test loss: 0.32828297464839284\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02388537647989061\n",
      "Average test loss: 0.0011234971273483502\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023323991075158118\n",
      "Average test loss: 0.0028242132862408955\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02332586305257347\n",
      "Average test loss: 0.002116701387282875\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02314920414156384\n",
      "Average test loss: 0.001164262017649081\n",
      "Epoch 112/300\n",
      "Average training loss: 0.023103376673327552\n",
      "Average test loss: 0.0011176096278553208\n",
      "Epoch 113/300\n",
      "Average training loss: 0.026060981343189876\n",
      "Average test loss: 0.0014012148622423411\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023520458567473625\n",
      "Average test loss: 0.9576135058266421\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023083747098843258\n",
      "Average test loss: 0.0027058794951687255\n",
      "Epoch 116/300\n",
      "Average training loss: 0.023142004153794714\n",
      "Average test loss: 0.0012827299462838306\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02318672321902381\n",
      "Average test loss: 0.0024720423670692576\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02300140487982167\n",
      "Average test loss: 0.001141375214430607\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02278647705084748\n",
      "Average test loss: 325483.38999739586\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02312119396444824\n",
      "Average test loss: 0.003165961076815923\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02283316061562962\n",
      "Average test loss: 0.0011514517252023022\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022689472060236665\n",
      "Average test loss: 0.0011287119497234623\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05594369013938639\n",
      "Average test loss: 0.0026160702452891404\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0319048754175504\n",
      "Average test loss: 0.0011114072667227852\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028296657130122183\n",
      "Average test loss: 0.0013850615291545788\n",
      "Epoch 126/300\n",
      "Average training loss: 0.026411490708589554\n",
      "Average test loss: 0.0011130536816910737\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025193783826298183\n",
      "Average test loss: 0.09217142856700553\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0243798785077201\n",
      "Average test loss: 0.0013843739540833565\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023819112290938697\n",
      "Average test loss: 0.001151228469175597\n",
      "Epoch 130/300\n",
      "Average training loss: 0.023363546897967657\n",
      "Average test loss: 0.002084671285106904\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023137696852286657\n",
      "Average test loss: 402.9254620806376\n",
      "Epoch 132/300\n",
      "Average training loss: 0.022916054023636712\n",
      "Average test loss: 0.005272977810456521\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02279965673221482\n",
      "Average test loss: 0.0019762564251820246\n",
      "Epoch 134/300\n",
      "Average training loss: 0.022672411867313915\n",
      "Average test loss: 0.001187513222762694\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02261616020070182\n",
      "Average test loss: 0.0014499164968729019\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02257380047771666\n",
      "Average test loss: 0.0012505447247789965\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02261254388259517\n",
      "Average test loss: 0.0011219601545275914\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022530510362651614\n",
      "Average test loss: 0.001272040133157538\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022520437050196858\n",
      "Average test loss: 0.00831565800598926\n",
      "Epoch 140/300\n",
      "Average training loss: 0.023092556327581407\n",
      "Average test loss: 41642.25133588771\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025347541410062048\n",
      "Average test loss: 0.0011664383255669641\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02262634430329005\n",
      "Average test loss: 0.001164120401462747\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022397437892026372\n",
      "Average test loss: 0.001156639023270044\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022381059635016652\n",
      "Average test loss: 0.0011057853394498428\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02226348636051019\n",
      "Average test loss: 0.0011839290564465855\n",
      "Epoch 146/300\n",
      "Average training loss: 0.022273206358154614\n",
      "Average test loss: 0.00557091567494596\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022302598151895735\n",
      "Average test loss: 0.003987426174338907\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022455107210410965\n",
      "Average test loss: 0.004981418821546767\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022619496903485723\n",
      "Average test loss: 0.0011673419238585564\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022036585030456385\n",
      "Average test loss: 0.0011324669699081117\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02213163826862971\n",
      "Average test loss: 0.0010957615253670762\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0220810879882839\n",
      "Average test loss: 0.001406889484367437\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022032987667454613\n",
      "Average test loss: 0.0011044323521976669\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03820426982475652\n",
      "Average test loss: 0.0012449209369305107\n",
      "Epoch 155/300\n",
      "Average training loss: 0.026982527166604996\n",
      "Average test loss: 0.008137501838306586\n",
      "Epoch 156/300\n",
      "Average training loss: 0.024304225406712957\n",
      "Average test loss: 0.0011307782153081563\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023161039753920504\n",
      "Average test loss: 0.0012446285588666797\n",
      "Epoch 158/300\n",
      "Average training loss: 0.022509220047129526\n",
      "Average test loss: 0.02178264317082034\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02211072654525439\n",
      "Average test loss: 0.34134560523182156\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021922335049344432\n",
      "Average test loss: 0.001153707045233912\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021816807428995767\n",
      "Average test loss: 0.0011849804668583803\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021854636281728745\n",
      "Average test loss: 0.0012692610311528874\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021857447486784722\n",
      "Average test loss: 0.0011006469140346679\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02186825839512878\n",
      "Average test loss: 0.00110834337098317\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0219038826988803\n",
      "Average test loss: 0.0011607677516423994\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021735994597276053\n",
      "Average test loss: 0.0011399790078608525\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02205590756734212\n",
      "Average test loss: 0.0011232533694451883\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021730121948652797\n",
      "Average test loss: 0.001114771879899005\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02162869637625085\n",
      "Average test loss: 0.02261851495794124\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021847051868836086\n",
      "Average test loss: 0.0013375859401292271\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021720860362880758\n",
      "Average test loss: 0.05082316878437996\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02165878465357754\n",
      "Average test loss: 0.0023441585179728767\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021610782305399576\n",
      "Average test loss: 0.001107100607206424\n",
      "Epoch 174/300\n",
      "Average training loss: 0.022342105478048324\n",
      "Average test loss: 0.0011389342380894556\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021590698786907725\n",
      "Average test loss: 0.001138566980138421\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021511869305537806\n",
      "Average test loss: 0.0011546395952399405\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021494748319188753\n",
      "Average test loss: 0.0011439661788754166\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02149578593009048\n",
      "Average test loss: 0.0016261921037091977\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021446482434868814\n",
      "Average test loss: 0.0011358066161887513\n",
      "Epoch 180/300\n",
      "Average training loss: 0.021453984517190192\n",
      "Average test loss: 0.0011370122220574154\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02178590941760275\n",
      "Average test loss: 0.001292116434727278\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021453933455877833\n",
      "Average test loss: 0.0011823041032378873\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021420075309773287\n",
      "Average test loss: 0.005815626403937737\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02141517202059428\n",
      "Average test loss: 0.001592930468213227\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02198879824909899\n",
      "Average test loss: 0.0018785815990219514\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021240630123350354\n",
      "Average test loss: 0.0011187130399048328\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021220856504307852\n",
      "Average test loss: 0.0011368555796022217\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0212627444366614\n",
      "Average test loss: 0.0011330061703402963\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021283025019698674\n",
      "Average test loss: 0.0011942333134098186\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021173309467732906\n",
      "Average test loss: 0.0011769924296273126\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02114212922917472\n",
      "Average test loss: 0.0011775506974922286\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021081147573060458\n",
      "Average test loss: 0.001167654889312366\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021148025335537064\n",
      "Average test loss: 0.0012524588795171843\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021134652169214355\n",
      "Average test loss: 0.001186134738019771\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02110548599395487\n",
      "Average test loss: 0.0011419932024760379\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021083943815694915\n",
      "Average test loss: 0.0012859225913675295\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02155953221188651\n",
      "Average test loss: 0.0011489326076375113\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02096230756905344\n",
      "Average test loss: 0.0011585145316397151\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020998955637216567\n",
      "Average test loss: 0.0014137828861259753\n",
      "Epoch 200/300\n",
      "Average training loss: 0.022473299264907837\n",
      "Average test loss: 0.001123788350623929\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0213956841710541\n",
      "Average test loss: 0.0013419036214343376\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020890097987320687\n",
      "Average test loss: 0.0011355142037694653\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020808323267433378\n",
      "Average test loss: 0.001215724169794056\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02085661004980405\n",
      "Average test loss: 0.0012774714667143094\n",
      "Epoch 205/300\n",
      "Average training loss: 0.020973633603917227\n",
      "Average test loss: 0.001366082307934347\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020930518887109225\n",
      "Average test loss: 7.8662953340295285\n",
      "Epoch 207/300\n",
      "Average training loss: 0.020831514734360908\n",
      "Average test loss: 0.0011727553997447507\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020789450152880615\n",
      "Average test loss: 0.30191694945014186\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02084480983018875\n",
      "Average test loss: 0.0011940829089532295\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020837399421466722\n",
      "Average test loss: 13.211693979582853\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02074425383243296\n",
      "Average test loss: 0.001181575074998869\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0207942133926683\n",
      "Average test loss: 0.05786341690272093\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020925639202197394\n",
      "Average test loss: 0.0012132813869458105\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02073872666226493\n",
      "Average test loss: 0.001602912033494148\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02069596240421136\n",
      "Average test loss: 0.0012036497158826225\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020775052598781055\n",
      "Average test loss: 0.0011476412018140156\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020675372284319666\n",
      "Average test loss: 1.0995520553059048\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020624692257907657\n",
      "Average test loss: 59.61298528671264\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02067046688993772\n",
      "Average test loss: 0.0012620534922203257\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023043415723575485\n",
      "Average test loss: 0.0011583487026703854\n",
      "Epoch 221/300\n",
      "Average training loss: 0.020770367236600983\n",
      "Average test loss: 0.0011536748928742276\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020624631461169985\n",
      "Average test loss: 0.0011556733517079718\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02056472366054853\n",
      "Average test loss: 0.0021018732882932658\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02060397924979528\n",
      "Average test loss: 0.00117619811795238\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020543022450473573\n",
      "Average test loss: 0.0011675035226055318\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02065310491952631\n",
      "Average test loss: 0.04474350686785247\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020519887261920507\n",
      "Average test loss: 0.001197173953263296\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020591440048482682\n",
      "Average test loss: 0.0012147450330149796\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02053784837325414\n",
      "Average test loss: 0.0013147705586420166\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02049424388508002\n",
      "Average test loss: 0.010641995139420032\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02068823046485583\n",
      "Average test loss: 0.0036781594244142376\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020815768051478598\n",
      "Average test loss: 0.0038197048927346867\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020511058312323357\n",
      "Average test loss: 0.0012918638849320511\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020503626921110685\n",
      "Average test loss: 0.11603515647186173\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020423351276252003\n",
      "Average test loss: 0.0011622011413694256\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0204167157659928\n",
      "Average test loss: 0.0011604997329413892\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020401303536362117\n",
      "Average test loss: 1.5823854932246937\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02095343816611502\n",
      "Average test loss: 0.0014462825862897766\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020347591176629067\n",
      "Average test loss: 0.0012948506830984519\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020318115815520288\n",
      "Average test loss: 0.0011844218520240652\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02036295887662305\n",
      "Average test loss: 91.26004013170136\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020611730068922043\n",
      "Average test loss: 0.0013945302900651264\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020319914837678272\n",
      "Average test loss: 0.0048599451987797184\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020289548893769584\n",
      "Average test loss: 0.0011941075986251235\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02033245921631654\n",
      "Average test loss: 0.0011959254304981894\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020278642998801336\n",
      "Average test loss: 0.0013075175163232617\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0202960600025124\n",
      "Average test loss: 0.0013694994394770927\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02033338451054361\n",
      "Average test loss: 0.0011519314976822998\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02026077413227823\n",
      "Average test loss: 0.047591571493281255\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020229595387975374\n",
      "Average test loss: 0.0012356970593747165\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020230769362714555\n",
      "Average test loss: 0.0012167622388030092\n",
      "Epoch 252/300\n",
      "Average training loss: 0.020255918438235918\n",
      "Average test loss: 0.05500685680657625\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02048704572353098\n",
      "Average test loss: 0.0011956622023135423\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020166841353807183\n",
      "Average test loss: 0.0018381647951900958\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02015444269610776\n",
      "Average test loss: 0.002439738105982542\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02013645572794808\n",
      "Average test loss: 0.001181385524260501\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026988221959935294\n",
      "Average test loss: 0.001174485504730708\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021230691467722256\n",
      "Average test loss: 0.0019714513599044746\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020481563192274836\n",
      "Average test loss: 0.00118982827073584\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02025801036424107\n",
      "Average test loss: 0.041370850779116154\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02019670861131615\n",
      "Average test loss: 0.004849999351323479\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020139432744847405\n",
      "Average test loss: 0.001979707150099178\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020117635423938435\n",
      "Average test loss: 0.0013666465173785885\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020127918471892676\n",
      "Average test loss: 0.002432274863848256\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020070565874377888\n",
      "Average test loss: 0.001240880187963032\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020359397527244357\n",
      "Average test loss: 0.3289097727490589\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020079364087846545\n",
      "Average test loss: 0.0012019820250570774\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020068290510111386\n",
      "Average test loss: 0.0011644840608868335\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020109642765588232\n",
      "Average test loss: 0.001159681932043491\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02008846453991201\n",
      "Average test loss: 0.0012266161881594195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020157095930642552\n",
      "Average test loss: 0.001309172462309814\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020031537693407802\n",
      "Average test loss: 0.0012668802216649056\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020081264357599948\n",
      "Average test loss: 0.0059288352732029224\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020055412990351518\n",
      "Average test loss: 0.005220878270040784\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02003956502179305\n",
      "Average test loss: 0.004916668427073293\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020340233751469188\n",
      "Average test loss: 0.0014247657916922536\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01999121218919754\n",
      "Average test loss: 0.0012656732349553043\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020105470137463676\n",
      "Average test loss: 0.004366595669339101\n",
      "Epoch 279/300\n",
      "Average training loss: 0.019924659781985814\n",
      "Average test loss: 0.0012004001366181505\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019938879541224905\n",
      "Average test loss: 0.001171774003189057\n",
      "Epoch 281/300\n",
      "Average training loss: 0.019890278046329816\n",
      "Average test loss: 0.0012249898249283432\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019931924358838133\n",
      "Average test loss: 0.001386996967614525\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02000680587026808\n",
      "Average test loss: 0.0011722621855636437\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019937177942858802\n",
      "Average test loss: 0.0012927107284259466\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019924090231458347\n",
      "Average test loss: 0.010116333537010682\n",
      "Epoch 286/300\n",
      "Average training loss: 0.019869488446248904\n",
      "Average test loss: 0.001254463611688051\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019860838355289564\n",
      "Average test loss: 0.0011913894009259011\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01989080716503991\n",
      "Average test loss: 0.0011996369658865862\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019835672370261615\n",
      "Average test loss: 0.0011669576921914187\n",
      "Epoch 290/300\n",
      "Average training loss: 0.019905006003048686\n",
      "Average test loss: 0.0012842091301249134\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01981538020571073\n",
      "Average test loss: 0.0011776905379051136\n",
      "Epoch 292/300\n",
      "Average training loss: 0.019903623479935857\n",
      "Average test loss: 0.020472735231949223\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019851781398057936\n",
      "Average test loss: 0.0011988160331837005\n",
      "Epoch 294/300\n",
      "Average training loss: 0.020593037371006276\n",
      "Average test loss: 0.07026115023406844\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020025121183858977\n",
      "Average test loss: 0.0014651469460998973\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019777273835407363\n",
      "Average test loss: 0.0011947302085657913\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01977522423532274\n",
      "Average test loss: 0.01959471521112654\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01984021995961666\n",
      "Average test loss: 0.0012246152109776933\n",
      "Epoch 299/300\n",
      "Average training loss: 0.019767069154315524\n",
      "Average test loss: 0.010632769728079439\n",
      "Epoch 300/300\n",
      "Average training loss: 0.019757371720340516\n",
      "Average test loss: 2.0891872211121436\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_64_Depth10/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.77\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.57\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.84\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.63\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.48\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.81\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.76\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.56\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.14\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.76\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.70\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.88\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.11\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.79\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.50\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 33.02\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.75\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.91\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 34.27\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.52\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.79\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.89\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 39.67578960503472\n",
      "Average test loss: 2.168905628030499\n",
      "Epoch 2/300\n",
      "Average training loss: 16.656426905314127\n",
      "Average test loss: 8.47004756618705\n",
      "Epoch 3/300\n",
      "Average training loss: 11.419305319044325\n",
      "Average test loss: 4.993009575009346\n",
      "Epoch 4/300\n",
      "Average training loss: 9.89632901679145\n",
      "Average test loss: 547.1018825575643\n",
      "Epoch 5/300\n",
      "Average training loss: 8.029263626946344\n",
      "Average test loss: 23.103450840608943\n",
      "Epoch 6/300\n",
      "Average training loss: 5.963072159237332\n",
      "Average test loss: 30.217504882744617\n",
      "Epoch 7/300\n",
      "Average training loss: 5.773285608503554\n",
      "Average test loss: 894.8532976709695\n",
      "Epoch 8/300\n",
      "Average training loss: 4.908407551235623\n",
      "Average test loss: 2.092129064642721\n",
      "Epoch 9/300\n",
      "Average training loss: 4.4046848831176755\n",
      "Average test loss: 3836.7620120442707\n",
      "Epoch 10/300\n",
      "Average training loss: 3.630043007320828\n",
      "Average test loss: 5.218635973519749\n",
      "Epoch 11/300\n",
      "Average training loss: 3.0829789282480875\n",
      "Average test loss: 0.010941177027920882\n",
      "Epoch 12/300\n",
      "Average training loss: 3.3935625788370767\n",
      "Average test loss: 248.87222103343407\n",
      "Epoch 13/300\n",
      "Average training loss: 2.7151017756991918\n",
      "Average test loss: 0.1041600783020258\n",
      "Epoch 14/300\n",
      "Average training loss: 2.3057990544637046\n",
      "Average test loss: 1.2082041367656655\n",
      "Epoch 15/300\n",
      "Average training loss: 1.9916289882659912\n",
      "Average test loss: 0.007681037316719691\n",
      "Epoch 16/300\n",
      "Average training loss: 1.7837524694866604\n",
      "Average test loss: 0.08719600273130669\n",
      "Epoch 17/300\n",
      "Average training loss: 1.625224181599087\n",
      "Average test loss: 0.006098848277495967\n",
      "Epoch 18/300\n",
      "Average training loss: 1.3656685615115696\n",
      "Average test loss: 0.36130479781826336\n",
      "Epoch 19/300\n",
      "Average training loss: 1.1490940109888712\n",
      "Average test loss: 0.007012408298750718\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9763869709968567\n",
      "Average test loss: 0.015826300288240114\n",
      "Epoch 21/300\n",
      "Average training loss: 0.857207098113166\n",
      "Average test loss: 0.006254557215919097\n",
      "Epoch 22/300\n",
      "Average training loss: 0.7569391499625312\n",
      "Average test loss: 0.005464898404975732\n",
      "Epoch 23/300\n",
      "Average training loss: 0.6785800773832533\n",
      "Average test loss: 0.059329201261202495\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6105100243356493\n",
      "Average test loss: 0.005881925175587336\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5543858759668139\n",
      "Average test loss: 0.01881224617941512\n",
      "Epoch 26/300\n",
      "Average training loss: 0.5043507589764065\n",
      "Average test loss: 0.04295002892282274\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4654428036212921\n",
      "Average test loss: 0.005205879881150193\n",
      "Epoch 28/300\n",
      "Average training loss: 0.42905934974882337\n",
      "Average test loss: 0.005140493488974041\n",
      "Epoch 29/300\n",
      "Average training loss: 0.39717988787757025\n",
      "Average test loss: 0.005659865340424909\n",
      "Epoch 30/300\n",
      "Average training loss: 0.3720962134467231\n",
      "Average test loss: 0.00638012240992652\n",
      "Epoch 31/300\n",
      "Average training loss: 0.3468634803560045\n",
      "Average test loss: 0.005051479586710533\n",
      "Epoch 32/300\n",
      "Average training loss: 0.326482336362203\n",
      "Average test loss: 0.005566223582873742\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3075021127594842\n",
      "Average test loss: 0.005469417021920284\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2935197247531679\n",
      "Average test loss: 0.008329543717619445\n",
      "Epoch 35/300\n",
      "Average training loss: 0.28056534473101297\n",
      "Average test loss: 1.1237119062079324\n",
      "Epoch 36/300\n",
      "Average training loss: 0.26810527420043945\n",
      "Average test loss: 33.984718551635744\n",
      "Epoch 37/300\n",
      "Average training loss: 0.25701779715220135\n",
      "Average test loss: 0.018376902546940577\n",
      "Epoch 38/300\n",
      "Average training loss: 0.24838954378498926\n",
      "Average test loss: 0.018131968567768733\n",
      "Epoch 39/300\n",
      "Average training loss: 0.23832448869281345\n",
      "Average test loss: 0.03867224117554724\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2304013859960768\n",
      "Average test loss: 0.004764741208404303\n",
      "Epoch 41/300\n",
      "Average training loss: 0.2290458293888304\n",
      "Average test loss: 0.004655836157086823\n",
      "Epoch 42/300\n",
      "Average training loss: 0.21758235592312283\n",
      "Average test loss: 0.00482729379253255\n",
      "Epoch 43/300\n",
      "Average training loss: 0.21160984465810986\n",
      "Average test loss: 0.005747274115267727\n",
      "Epoch 44/300\n",
      "Average training loss: 0.20639436434374916\n",
      "Average test loss: 3.281465629239877\n",
      "Epoch 45/300\n",
      "Average training loss: 0.20224811496999529\n",
      "Average test loss: 0.005289874751120806\n",
      "Epoch 46/300\n",
      "Average training loss: 0.19897229557567173\n",
      "Average test loss: 0.004675284128636122\n",
      "Epoch 47/300\n",
      "Average training loss: 0.19333045597871146\n",
      "Average test loss: 0.005283868773529927\n",
      "Epoch 48/300\n",
      "Average training loss: 0.18929136800765992\n",
      "Average test loss: 0.005285955153405666\n",
      "Epoch 49/300\n",
      "Average training loss: 0.18578093649281396\n",
      "Average test loss: 0.004485667495677868\n",
      "Epoch 50/300\n",
      "Average training loss: 0.18494695230325062\n",
      "Average test loss: 1179034.29342749\n",
      "Epoch 51/300\n",
      "Average training loss: 0.17853088952435386\n",
      "Average test loss: 0.012449904668662283\n",
      "Epoch 52/300\n",
      "Average training loss: 0.17637993098629845\n",
      "Average test loss: 0.004532636722342835\n",
      "Epoch 53/300\n",
      "Average training loss: 0.17546681861082714\n",
      "Average test loss: 0.005012725178566244\n",
      "Epoch 54/300\n",
      "Average training loss: 0.17034540482362112\n",
      "Average test loss: 0.2616471694509188\n",
      "Epoch 55/300\n",
      "Average training loss: 0.16915055127938589\n",
      "Average test loss: 0.0076475394831763376\n",
      "Epoch 56/300\n",
      "Average training loss: 0.3610171218845579\n",
      "Average test loss: 0.005160997399232454\n",
      "Epoch 57/300\n",
      "Average training loss: 0.21219197403060067\n",
      "Average test loss: 0.0049918098710477355\n",
      "Epoch 58/300\n",
      "Average training loss: 0.19585807473129696\n",
      "Average test loss: 0.0050010702059500745\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1875557951397366\n",
      "Average test loss: 0.004679772207306491\n",
      "Epoch 60/300\n",
      "Average training loss: 0.18084133285946316\n",
      "Average test loss: 0.004866273801152905\n",
      "Epoch 61/300\n",
      "Average training loss: 0.17691867395242056\n",
      "Average test loss: 0.004682108886539936\n",
      "Epoch 62/300\n",
      "Average training loss: 0.17258528202109866\n",
      "Average test loss: 0.004490750208083126\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1703702648083369\n",
      "Average test loss: 0.004568881357295646\n",
      "Epoch 64/300\n",
      "Average training loss: 0.168311553325918\n",
      "Average test loss: 0.004455569598823786\n",
      "Epoch 65/300\n",
      "Average training loss: 0.16647047736909654\n",
      "Average test loss: 0.0052823381937212414\n",
      "Epoch 66/300\n",
      "Average training loss: 0.16494510886404248\n",
      "Average test loss: 0.007323707771177093\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1636484657128652\n",
      "Average test loss: 0.018027438817752733\n",
      "Epoch 68/300\n",
      "Average training loss: 0.16185958551035987\n",
      "Average test loss: 0.005073547323544821\n",
      "Epoch 69/300\n",
      "Average training loss: 0.16168526633580527\n",
      "Average test loss: 0.0043993626837101245\n",
      "Epoch 70/300\n",
      "Average training loss: 0.15971522692839304\n",
      "Average test loss: 0.031372598305344584\n",
      "Epoch 71/300\n",
      "Average training loss: 0.15744991138246325\n",
      "Average test loss: 0.004967398560709424\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1571526294019487\n",
      "Average test loss: 162769.8256111111\n",
      "Epoch 73/300\n",
      "Average training loss: 0.15470312435097164\n",
      "Average test loss: 0.004800764294548167\n",
      "Epoch 74/300\n",
      "Average training loss: 0.1550834318002065\n",
      "Average test loss: 0.004827058881935146\n",
      "Epoch 75/300\n",
      "Average training loss: 0.8860736935204929\n",
      "Average test loss: 10.500521102648642\n",
      "Epoch 76/300\n",
      "Average training loss: 0.6055890576044718\n",
      "Average test loss: 48215.935676938774\n",
      "Epoch 77/300\n",
      "Average training loss: 0.43766989482773677\n",
      "Average test loss: 196375479896566.56\n",
      "Epoch 78/300\n",
      "Average training loss: 0.2777220898469289\n",
      "Average test loss: 6252780735413.694\n",
      "Epoch 79/300\n",
      "Average training loss: 0.24373697430557675\n",
      "Average test loss: 151203041395.261\n",
      "Epoch 80/300\n",
      "Average training loss: 0.22661476235919528\n",
      "Average test loss: 3469223740.304669\n",
      "Epoch 81/300\n",
      "Average training loss: 0.22753528304894766\n",
      "Average test loss: 2505998.144009129\n",
      "Epoch 82/300\n",
      "Average training loss: 0.20906374786959753\n",
      "Average test loss: 4095.584875462928\n",
      "Epoch 83/300\n",
      "Average training loss: 0.20028262970844904\n",
      "Average test loss: 23918886425.75733\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1922719026406606\n",
      "Average test loss: 10444.372566029884\n",
      "Epoch 85/300\n",
      "Average training loss: 0.18692768499586318\n",
      "Average test loss: 587272.3207097417\n",
      "Epoch 86/300\n",
      "Average training loss: 0.18035567194885677\n",
      "Average test loss: 0.004426057806859414\n",
      "Epoch 87/300\n",
      "Average training loss: 0.1743417910999722\n",
      "Average test loss: 86.69002412097818\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1711987958153089\n",
      "Average test loss: 0.004527493761231502\n",
      "Epoch 89/300\n",
      "Average training loss: 0.16759892170959048\n",
      "Average test loss: 0.0044891881011426445\n",
      "Epoch 90/300\n",
      "Average training loss: 0.16618629255559708\n",
      "Average test loss: 0.004556245137833887\n",
      "Epoch 91/300\n",
      "Average training loss: 0.16843658712175158\n",
      "Average test loss: 0.011404697781014774\n",
      "Epoch 92/300\n",
      "Average training loss: 0.20973355617788103\n",
      "Average test loss: 0.005856076294349299\n",
      "Epoch 93/300\n",
      "Average training loss: 0.8952365162372589\n",
      "Average test loss: 2611108797.076513\n",
      "Epoch 94/300\n",
      "Average training loss: 0.26894204946358996\n",
      "Average test loss: 506194594.8013596\n",
      "Epoch 95/300\n",
      "Average training loss: 0.23007695893446603\n",
      "Average test loss: 0.00908926136750314\n",
      "Epoch 96/300\n",
      "Average training loss: 0.20368702858024174\n",
      "Average test loss: 115.1535112702745\n",
      "Epoch 97/300\n",
      "Average training loss: 0.19528842238585153\n",
      "Average test loss: 0.007198139765196376\n",
      "Epoch 98/300\n",
      "Average training loss: 0.18621584173043568\n",
      "Average test loss: 0.007358784590330389\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1822023522456487\n",
      "Average test loss: 0.005102872857203086\n",
      "Epoch 100/300\n",
      "Average training loss: 0.17645861903826396\n",
      "Average test loss: 0.004473481248650286\n",
      "Epoch 101/300\n",
      "Average training loss: 0.18158286510573493\n",
      "Average test loss: 11.130004789620639\n",
      "Epoch 102/300\n",
      "Average training loss: 0.4061557291348775\n",
      "Average test loss: 22.59282449468308\n",
      "Epoch 103/300\n",
      "Average training loss: 0.212339382370313\n",
      "Average test loss: 0.03572239990201261\n",
      "Epoch 104/300\n",
      "Average training loss: 0.19272148576047685\n",
      "Average test loss: 0.004557923275563452\n",
      "Epoch 105/300\n",
      "Average training loss: 0.18459493335088095\n",
      "Average test loss: 0.004472016205390295\n",
      "Epoch 106/300\n",
      "Average training loss: 0.1792668613990148\n",
      "Average test loss: 0.07350329558882449\n",
      "Epoch 107/300\n",
      "Average training loss: 0.17450714377562204\n",
      "Average test loss: 0.3941370489266184\n",
      "Epoch 108/300\n",
      "Average training loss: 0.1720923014746772\n",
      "Average test loss: 0.010511064499616623\n",
      "Epoch 109/300\n",
      "Average training loss: 0.16834885052177642\n",
      "Average test loss: 0.15060000427481202\n",
      "Epoch 110/300\n",
      "Average training loss: 0.16406577936808267\n",
      "Average test loss: 0.008260187850644191\n",
      "Epoch 111/300\n",
      "Average training loss: 0.16374335787693658\n",
      "Average test loss: 0.004542537745088339\n",
      "Epoch 112/300\n",
      "Average training loss: 0.161170747478803\n",
      "Average test loss: 0.005663951710694366\n",
      "Epoch 113/300\n",
      "Average training loss: 5.527019363681475\n",
      "Average test loss: 141880.01233315803\n",
      "Epoch 114/300\n",
      "Average training loss: 2.2674782469007706\n",
      "Average test loss: 80718282.89035933\n",
      "Epoch 115/300\n",
      "Average training loss: 1.2128563460244073\n",
      "Average test loss: 1170604.97076395\n",
      "Epoch 116/300\n",
      "Average training loss: 0.824704252243042\n",
      "Average test loss: 7726270.843561462\n",
      "Epoch 117/300\n",
      "Average training loss: 0.618268554104699\n",
      "Average test loss: 704563304.4200921\n",
      "Epoch 118/300\n",
      "Average training loss: 0.4736625757482317\n",
      "Average test loss: 7617129613159.184\n",
      "Epoch 119/300\n",
      "Average training loss: 0.3826752030054728\n",
      "Average test loss: 5006189.657042602\n",
      "Epoch 120/300\n",
      "Average training loss: 0.33547407425774467\n",
      "Average test loss: 385623.05731357815\n",
      "Epoch 121/300\n",
      "Average training loss: 0.3011613235208723\n",
      "Average test loss: 3088882760.100173\n",
      "Epoch 122/300\n",
      "Average training loss: 0.27234642582469515\n",
      "Average test loss: 1485065302.1155555\n",
      "Epoch 123/300\n",
      "Average training loss: 0.25583923655086094\n",
      "Average test loss: 5669.831759691099\n",
      "Epoch 124/300\n",
      "Average training loss: 0.24749194312095643\n",
      "Average test loss: 3016254.456216433\n",
      "Epoch 125/300\n",
      "Average training loss: 0.23229672372341156\n",
      "Average test loss: 157.45991719629987\n",
      "Epoch 126/300\n",
      "Average training loss: 0.23015826212035284\n",
      "Average test loss: 52.03236380493496\n",
      "Epoch 127/300\n",
      "Average training loss: 0.20958164868089887\n",
      "Average test loss: 0.008397649412353833\n",
      "Epoch 128/300\n",
      "Average training loss: 0.20426689740022025\n",
      "Average test loss: 15241.512776069145\n",
      "Epoch 129/300\n",
      "Average training loss: 0.1938668277064959\n",
      "Average test loss: 11450.62127048973\n",
      "Epoch 130/300\n",
      "Average training loss: 0.31616246994336444\n",
      "Average test loss: 34275.20391040846\n",
      "Epoch 131/300\n",
      "Average training loss: 0.23487710837523143\n",
      "Average test loss: 243299.73090867032\n",
      "Epoch 132/300\n",
      "Average training loss: 0.20421481515301598\n",
      "Average test loss: 839.9446155914102\n",
      "Epoch 133/300\n",
      "Average training loss: 0.18964517790741392\n",
      "Average test loss: 34652825.92\n",
      "Epoch 134/300\n",
      "Average training loss: 0.17810560123125713\n",
      "Average test loss: 25800.942908863573\n",
      "Epoch 135/300\n",
      "Average training loss: 0.17634190875954098\n",
      "Average test loss: 305.91872600526614\n",
      "Epoch 136/300\n",
      "Average training loss: 0.1717890577978558\n",
      "Average test loss: 0.19593041786799828\n",
      "Epoch 137/300\n",
      "Average training loss: 0.16981290006637573\n",
      "Average test loss: 0.006922649164166715\n",
      "Epoch 138/300\n",
      "Average training loss: 0.370379585120413\n",
      "Average test loss: 144685.99732291666\n",
      "Epoch 139/300\n",
      "Average training loss: 0.19714164752430385\n",
      "Average test loss: 0.00588355407015317\n",
      "Epoch 140/300\n",
      "Average training loss: 0.18266467473242018\n",
      "Average test loss: 0.0052868936682740845\n",
      "Epoch 141/300\n",
      "Average training loss: 0.22908410573005678\n",
      "Average test loss: 0.006839559255374802\n",
      "Epoch 142/300\n",
      "Average training loss: 0.18881205364068349\n",
      "Average test loss: 0.004595333594001002\n",
      "Epoch 143/300\n",
      "Average training loss: 0.20731515253914728\n",
      "Average test loss: 2464792.3867502217\n",
      "Epoch 144/300\n",
      "Average training loss: 0.2112860823472341\n",
      "Average test loss: 2.466137194469571\n",
      "Epoch 145/300\n",
      "Average training loss: 0.18217318477895525\n",
      "Average test loss: 0.01982130509076847\n",
      "Epoch 146/300\n",
      "Average training loss: 0.18272437662548488\n",
      "Average test loss: 8202888.7135753855\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1698652084403568\n",
      "Average test loss: 0.004609995061324703\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1666049227449629\n",
      "Average test loss: 0.018845267451885674\n",
      "Epoch 149/300\n",
      "Average training loss: 0.3124608108202616\n",
      "Average test loss: 1.3839716493123106\n",
      "Epoch 150/300\n",
      "Average training loss: 0.19418915474414825\n",
      "Average test loss: 5377.153988585558\n",
      "Epoch 151/300\n",
      "Average training loss: 0.18031665996710458\n",
      "Average test loss: 0.004438582525485092\n",
      "Epoch 152/300\n",
      "Average training loss: 0.1715492696563403\n",
      "Average test loss: 0.006850200588918395\n",
      "Epoch 153/300\n",
      "Average training loss: 0.17311235234472486\n",
      "Average test loss: 0.004448347994022899\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1647844686243269\n",
      "Average test loss: 0.004353380979349216\n",
      "Epoch 155/300\n",
      "Average training loss: 0.16423772149615817\n",
      "Average test loss: 0.004508005490940478\n",
      "Epoch 156/300\n",
      "Average training loss: 0.18104794863859813\n",
      "Average test loss: 0.008265920516517427\n",
      "Epoch 157/300\n",
      "Average training loss: 0.19259292842282189\n",
      "Average test loss: 203512314.42003432\n",
      "Epoch 158/300\n",
      "Average training loss: 0.16804523607095082\n",
      "Average test loss: 0.004445860300420059\n",
      "Epoch 159/300\n",
      "Average training loss: 0.16154163954986467\n",
      "Average test loss: 0.004399633891466591\n",
      "Epoch 160/300\n",
      "Average training loss: 0.20834964958826702\n",
      "Average test loss: 23990.00390277778\n",
      "Epoch 161/300\n",
      "Average training loss: 0.21785592130819956\n",
      "Average test loss: 0.004414012112965187\n",
      "Epoch 162/300\n",
      "Average training loss: 0.16841865011056265\n",
      "Average test loss: 0.004359204933875137\n",
      "Epoch 163/300\n",
      "Average training loss: 0.16307853223217858\n",
      "Average test loss: 0.004346336527831025\n",
      "Epoch 164/300\n",
      "Average training loss: 0.1655310454633501\n",
      "Average test loss: 0.004803554328365459\n",
      "Epoch 165/300\n",
      "Average training loss: 0.1599871203104655\n",
      "Average test loss: 0.004527806154969666\n",
      "Epoch 166/300\n",
      "Average training loss: 0.17263025954034594\n",
      "Average test loss: 0.004589883560728696\n",
      "Epoch 167/300\n",
      "Average training loss: 0.17078750137488047\n",
      "Average test loss: 0.004556825941221581\n",
      "Epoch 168/300\n",
      "Average training loss: 0.15652842545509338\n",
      "Average test loss: 0.0043882186623911065\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1570434461434682\n",
      "Average test loss: 0.5101011781377924\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1571345257163048\n",
      "Average test loss: 0.009698973199973504\n",
      "Epoch 171/300\n",
      "Average training loss: 0.20256663885381487\n",
      "Average test loss: 10.766130875344079\n",
      "Epoch 172/300\n",
      "Average training loss: 0.17812111047903698\n",
      "Average test loss: 0.004456537341492044\n",
      "Epoch 173/300\n",
      "Average training loss: 0.1607281004720264\n",
      "Average test loss: 385011724.75555557\n",
      "Epoch 174/300\n",
      "Average training loss: 0.1780237081779374\n",
      "Average test loss: 0.0048052164361708696\n",
      "Epoch 175/300\n",
      "Average test loss: 0.004381463933322165\n",
      "Epoch 176/300\n",
      "Average training loss: 0.15547212502691482\n",
      "Average test loss: 34.64172173718322\n",
      "Epoch 177/300\n",
      "Average training loss: 0.1536689910226398\n",
      "Average test loss: 0.004556846238672733\n",
      "Epoch 178/300\n",
      "Average training loss: 0.15961683087878756\n",
      "Average test loss: 0.03696311698605617\n",
      "Epoch 179/300\n",
      "Average training loss: 0.15643569493293763\n",
      "Average test loss: 0.0044943610411137345\n",
      "Epoch 180/300\n",
      "Average training loss: 0.15373471171988382\n",
      "Average test loss: 0.7243567136890359\n",
      "Epoch 181/300\n",
      "Average training loss: 0.153445723745558\n",
      "Average test loss: 0.09177275871899393\n",
      "Epoch 182/300\n",
      "Average training loss: 0.15326185592015584\n",
      "Average test loss: 0.005991322323886885\n",
      "Epoch 183/300\n",
      "Average training loss: 0.14929148807790543\n",
      "Average test loss: 0.004387603210906188\n",
      "Epoch 184/300\n",
      "Average training loss: 0.1536493540737364\n",
      "Average test loss: 0.004405464289709926\n",
      "Epoch 185/300\n",
      "Average training loss: 0.15094467134608164\n",
      "Average test loss: 0.010412672010560831\n",
      "Epoch 186/300\n",
      "Average training loss: 0.14918159067630768\n",
      "Average test loss: 0.005291890178703599\n",
      "Epoch 187/300\n",
      "Average training loss: 0.14666290499104395\n",
      "Average test loss: 0.004436440496395032\n",
      "Epoch 188/300\n",
      "Average training loss: 0.14717585011323292\n",
      "Average test loss: 0.0045505683765643175\n",
      "Epoch 189/300\n",
      "Average training loss: 0.20595846588081784\n",
      "Average test loss: 913.9549725287267\n",
      "Epoch 190/300\n",
      "Average training loss: 0.1729234439863099\n",
      "Average test loss: 0.004408799562603235\n",
      "Epoch 191/300\n",
      "Average training loss: 0.15425061637825435\n",
      "Average test loss: 0.004542775483181079\n",
      "Epoch 192/300\n",
      "Average training loss: 0.15005930852890015\n",
      "Average test loss: 0.45900015369968283\n",
      "Epoch 193/300\n",
      "Average training loss: 0.14924026707808177\n",
      "Average test loss: 0.0046782351243827075\n",
      "Epoch 194/300\n",
      "Average training loss: 0.1553048700094223\n",
      "Average test loss: 749628596949789.4\n",
      "Epoch 195/300\n",
      "Average training loss: 7.069280912995338\n",
      "Average test loss: 3.396385886589686\n",
      "Epoch 196/300\n",
      "Average training loss: 4.36966256099277\n",
      "Average test loss: 16685.68972342585\n",
      "Epoch 197/300\n",
      "Average training loss: 3.4655162332322864\n",
      "Average test loss: 53.84772776395455\n",
      "Epoch 198/300\n",
      "Average training loss: 2.9040896956125897\n",
      "Average test loss: 0.09538941894844175\n",
      "Epoch 199/300\n",
      "Average training loss: 2.492333110173543\n",
      "Average test loss: 34.96903087308134\n",
      "Epoch 200/300\n",
      "Average training loss: 2.189598227394952\n",
      "Average test loss: 0.6703242505109972\n",
      "Epoch 201/300\n",
      "Average training loss: 1.9313029740651448\n",
      "Average test loss: 0.01711846061423421\n",
      "Epoch 202/300\n",
      "Average training loss: 1.6914030431111653\n",
      "Average test loss: 0.007484125460601516\n",
      "Epoch 203/300\n",
      "Average training loss: 1.4686058318879869\n",
      "Average test loss: 0.0067517036890818016\n",
      "Epoch 204/300\n",
      "Average training loss: 1.247045755174425\n",
      "Average test loss: 0.02139609747959508\n",
      "Epoch 205/300\n",
      "Average training loss: 1.0070834351115756\n",
      "Average test loss: 0.1295571609172556\n",
      "Epoch 206/300\n",
      "Average training loss: 0.7795708447562324\n",
      "Average test loss: 0.5986946515209145\n",
      "Epoch 207/300\n",
      "Average training loss: 0.6112946529653337\n",
      "Average test loss: 1.3480193172246218\n",
      "Epoch 208/300\n",
      "Average training loss: 0.4912993897861905\n",
      "Average test loss: 0.01392084841719932\n",
      "Epoch 209/300\n",
      "Average training loss: 0.40583132516013254\n",
      "Average test loss: 0.7086107304704686\n",
      "Epoch 210/300\n",
      "Average training loss: 0.3453741958406236\n",
      "Average test loss: 0.0065509121922983065\n",
      "Epoch 211/300\n",
      "Average training loss: 0.30376094007492066\n",
      "Average test loss: 0.004773965946709116\n",
      "Epoch 212/300\n",
      "Average training loss: 0.26953264488114254\n",
      "Average test loss: 0.07864510461688042\n",
      "Epoch 213/300\n",
      "Average training loss: 0.27351691653993393\n",
      "Average test loss: 55.53764024186797\n",
      "Epoch 214/300\n",
      "Average training loss: 0.23317275873819987\n",
      "Average test loss: 23.976082585701924\n",
      "Epoch 215/300\n",
      "Average training loss: 0.21697627997398378\n",
      "Average test loss: 0.004418214701530006\n",
      "Epoch 216/300\n",
      "Average training loss: 0.2092564681371053\n",
      "Average test loss: 0.008839556807445156\n",
      "Epoch 217/300\n",
      "Average training loss: 0.19958765188852945\n",
      "Average test loss: 0.8500173387825489\n",
      "Epoch 218/300\n",
      "Average training loss: 0.1944665509727266\n",
      "Average test loss: 0.004675904522753424\n",
      "Epoch 219/300\n",
      "Average training loss: 0.18827484338813358\n",
      "Average test loss: 6610.151504787313\n",
      "Epoch 220/300\n",
      "Average training loss: 0.17941759110821617\n",
      "Average test loss: 0.0927969681918621\n",
      "Epoch 221/300\n",
      "Average training loss: 0.1724651677476035\n",
      "Average test loss: 0.0418055262433158\n",
      "Epoch 222/300\n",
      "Average training loss: 0.17344442937109206\n",
      "Average test loss: 0.010637928670893113\n",
      "Epoch 223/300\n",
      "Average training loss: 6.310961069689856\n",
      "Average test loss: 0.0076061288296348515\n",
      "Epoch 224/300\n",
      "Average training loss: 3.230363754060533\n",
      "Average test loss: 0.017050382328944073\n",
      "Epoch 225/300\n",
      "Average training loss: 2.5044520022074384\n",
      "Average test loss: 11.190546558292375\n",
      "Epoch 226/300\n",
      "Average training loss: 1.955729849603441\n",
      "Average test loss: 0.004796142331221037\n",
      "Epoch 227/300\n",
      "Average training loss: 1.5590951833724975\n",
      "Average test loss: 0.013264850661986405\n",
      "Epoch 228/300\n",
      "Average training loss: 1.1813133860694038\n",
      "Average test loss: 0.007729327795406183\n",
      "Epoch 229/300\n",
      "Average training loss: 0.8822388038635254\n",
      "Average test loss: 1.4011649793916279\n",
      "Epoch 230/300\n",
      "Average training loss: 0.9106530990600586\n",
      "Average test loss: 15989233.03039169\n",
      "Epoch 231/300\n",
      "Average training loss: 0.5944397911495632\n",
      "Average test loss: 192421.72274481578\n",
      "Epoch 232/300\n",
      "Average training loss: 0.46219748089048596\n",
      "Average test loss: 53907.74272040679\n",
      "Epoch 233/300\n",
      "Average training loss: 0.36978245043754576\n",
      "Average test loss: 0.22122542001017265\n",
      "Epoch 234/300\n",
      "Average training loss: 0.28711834147241383\n",
      "Average test loss: 2027050.937761965\n",
      "Epoch 235/300\n",
      "Average training loss: 0.2504809882905748\n",
      "Average test loss: 0.36635985393863585\n",
      "Epoch 236/300\n",
      "Average training loss: 0.2282998112572564\n",
      "Average test loss: 4599043.921599108\n",
      "Epoch 237/300\n",
      "Average training loss: 0.22893829641077254\n",
      "Average test loss: 3901870345.5579114\n",
      "Epoch 238/300\n",
      "Average training loss: 0.2178588315380944\n",
      "Average test loss: 613.70144695856\n",
      "Epoch 239/300\n",
      "Average training loss: 0.2054548484219445\n",
      "Average test loss: 12.332982383683325\n",
      "Epoch 240/300\n",
      "Average training loss: 0.192763720591863\n",
      "Average test loss: 39.52014562168055\n",
      "Epoch 241/300\n",
      "Average training loss: 0.19519193781746758\n",
      "Average test loss: 1.7677624668584753e+24\n",
      "Epoch 242/300\n",
      "Average training loss: 0.1957995603084564\n",
      "Average test loss: 100782.51143558079\n",
      "Epoch 243/300\n",
      "Average training loss: 0.18570710202058158\n",
      "Average test loss: 102.4578250263499\n",
      "Epoch 244/300\n",
      "Average training loss: 0.1808258581161499\n",
      "Average test loss: 0.00441550559985141\n",
      "Epoch 245/300\n",
      "Average training loss: 0.18606311459011501\n",
      "Average test loss: 753840.3517494925\n",
      "Epoch 246/300\n",
      "Average training loss: 0.169943754752477\n",
      "Average test loss: 63.15265064164003\n",
      "Epoch 247/300\n",
      "Average training loss: 0.18248600782288446\n",
      "Average test loss: 2.59742587893539\n",
      "Epoch 248/300\n",
      "Average training loss: 0.19259851813316345\n",
      "Average test loss: 16388516391.139555\n",
      "Epoch 249/300\n",
      "Average training loss: 0.18564080231719546\n",
      "Average test loss: 0.004523265350816978\n",
      "Epoch 250/300\n",
      "Average training loss: 0.1653347320093049\n",
      "Average test loss: 0.0044660984851006\n",
      "Epoch 251/300\n",
      "Average training loss: 0.16386430667506324\n",
      "Average test loss: 0.00631696244308518\n",
      "Epoch 252/300\n",
      "Average training loss: 0.15928645875056585\n",
      "Average test loss: 0.07536422248681386\n",
      "Epoch 253/300\n",
      "Average training loss: 0.25399129445023005\n",
      "Average test loss: 1.034626586024546\n",
      "Epoch 254/300\n",
      "Average training loss: 0.16536128147443135\n",
      "Average test loss: 0.1406572705540392\n",
      "Epoch 255/300\n",
      "Average training loss: 0.17031787924634087\n",
      "Average test loss: 0.006974814395937654\n",
      "Epoch 256/300\n",
      "Average training loss: 0.15633261320988337\n",
      "Average test loss: 0.00446071378638347\n",
      "Epoch 257/300\n",
      "Average training loss: 0.17213550933202107\n",
      "Average test loss: 2.3774721187055112\n",
      "Epoch 258/300\n",
      "Average training loss: 0.27401524127854243\n",
      "Average test loss: 0.004406072890179025\n",
      "Epoch 259/300\n",
      "Average training loss: 0.16685045989354452\n",
      "Average test loss: 0.004377687526659833\n",
      "Epoch 260/300\n",
      "Average training loss: 0.15654758936166763\n",
      "Average test loss: 0.004580432013918956\n",
      "Epoch 261/300\n",
      "Average training loss: 0.15183254798253376\n",
      "Average test loss: 0.004816779450823864\n",
      "Epoch 262/300\n",
      "Average training loss: 0.20156616637441846\n",
      "Average test loss: 0.00767331039864156\n",
      "Epoch 263/300\n",
      "Average training loss: 0.1625799591143926\n",
      "Average test loss: 0.009055850129988458\n",
      "Epoch 264/300\n",
      "Average training loss: 0.15587809529569413\n",
      "Average test loss: 0.02224613720840878\n",
      "Epoch 265/300\n",
      "Average training loss: 0.14910568528705173\n",
      "Average test loss: 0.004461355864794718\n",
      "Epoch 266/300\n",
      "Average training loss: 0.22146341767576005\n",
      "Average test loss: 0.005059098928752873\n",
      "Epoch 267/300\n",
      "Average training loss: 0.1559249751302931\n",
      "Average test loss: 0.004644524882444077\n",
      "Epoch 268/300\n",
      "Average training loss: 0.158770089076625\n",
      "Average test loss: 0.004462595860784253\n",
      "Epoch 269/300\n",
      "Average training loss: 0.19546950754854414\n",
      "Average test loss: 38708351.52016242\n",
      "Epoch 270/300\n",
      "Average training loss: 0.1760311426056756\n",
      "Average test loss: 13.914685928829842\n",
      "Epoch 271/300\n",
      "Average training loss: 0.16361035917202632\n",
      "Average test loss: 0.004543619520548317\n",
      "Epoch 272/300\n",
      "Average training loss: 0.15998800402217442\n",
      "Average test loss: 0.01204738536559873\n",
      "Epoch 273/300\n",
      "Average training loss: 444.5428232142793\n",
      "Average test loss: 0.042344822506109873\n",
      "Epoch 274/300\n",
      "Average training loss: 17.720561529371473\n",
      "Average test loss: 592.9884404644404\n",
      "Epoch 275/300\n",
      "Average training loss: 13.253244455973308\n",
      "Average test loss: 17666.85442034559\n",
      "Epoch 276/300\n",
      "Average training loss: 10.707270302666558\n",
      "Average test loss: 11745.139305710356\n",
      "Epoch 277/300\n",
      "Average training loss: 9.012818001641168\n",
      "Average test loss: 1.4978321462439166\n",
      "Epoch 278/300\n",
      "Average training loss: 7.649760331471761\n",
      "Average test loss: 1101.8334256654846\n",
      "Epoch 279/300\n",
      "Average training loss: 6.411590677897135\n",
      "Average test loss: 0.012881916076772743\n",
      "Epoch 280/300\n",
      "Average training loss: 5.3760546035766605\n",
      "Average test loss: 0.09017862297097842\n",
      "Epoch 281/300\n",
      "Average training loss: 4.596165089501275\n",
      "Average test loss: 1386.9327334087707\n",
      "Epoch 282/300\n",
      "Average training loss: 4.005460063722398\n",
      "Average test loss: 91.30568065528902\n",
      "Epoch 283/300\n",
      "Average training loss: 3.5676817247602677\n",
      "Average test loss: 6.143556771239266\n",
      "Epoch 284/300\n",
      "Average training loss: 3.216893666373359\n",
      "Average test loss: 2769.668117762814\n",
      "Epoch 285/300\n",
      "Average training loss: 2.9187751655578613\n",
      "Average test loss: 11267.049018602\n",
      "Epoch 286/300\n",
      "Average training loss: 2.6657371463775634\n",
      "Average test loss: 638.7197246093319\n",
      "Epoch 287/300\n",
      "Average training loss: 2.4264370358784992\n",
      "Average test loss: 2.247635064252549\n",
      "Epoch 288/300\n",
      "Average training loss: 2.2205168912675646\n",
      "Average test loss: 236234.23823830078\n",
      "Epoch 289/300\n",
      "Average training loss: 2.044742728127374\n",
      "Average test loss: 357.60526293090317\n",
      "Epoch 290/300\n",
      "Average training loss: 1.8836442652808296\n",
      "Average test loss: 349334.9128620447\n",
      "Epoch 291/300\n",
      "Average training loss: 1.7253471505906848\n",
      "Average test loss: 10409.625520285235\n",
      "Epoch 292/300\n",
      "Average training loss: 1.5496570508744982\n",
      "Average test loss: 5987.737917953236\n",
      "Epoch 293/300\n",
      "Average training loss: 1.3755084028244018\n",
      "Average test loss: 211332.7671702189\n",
      "Epoch 294/300\n",
      "Average training loss: 1.2028244987063939\n",
      "Average test loss: 2930980161.9419255\n",
      "Epoch 295/300\n",
      "Average training loss: 1.018727953804864\n",
      "Average test loss: 173.5033892742681\n",
      "Epoch 296/300\n",
      "Average training loss: 0.8663912056816949\n",
      "Average test loss: 91562944276000.55\n",
      "Epoch 297/300\n",
      "Average training loss: 0.7072181052101983\n",
      "Average test loss: 60.09298176378012\n",
      "Epoch 298/300\n",
      "Average training loss: 0.5669591761695014\n",
      "Average test loss: 423.6476839347761\n",
      "Epoch 299/300\n",
      "Average training loss: 0.47300275442335343\n",
      "Average test loss: 75.9666549970609\n",
      "Epoch 300/300\n",
      "Average training loss: 0.3993540952735477\n",
      "Average test loss: 0.018587700790415208\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 37.607381625705294\n",
      "Average test loss: 0.014270321076114973\n",
      "Epoch 2/300\n",
      "Average training loss: 19.238623611450194\n",
      "Average test loss: 0.010323586036761602\n",
      "Epoch 3/300\n",
      "Average training loss: 13.183432692633735\n",
      "Average test loss: 2659.640190535691\n",
      "Epoch 4/300\n",
      "Average training loss: 11.205274223327637\n",
      "Average test loss: 0.014367286981393893\n",
      "Epoch 5/300\n",
      "Average training loss: 9.231427680121527\n",
      "Average test loss: 2.3899732484093144\n",
      "Epoch 6/300\n",
      "Average training loss: 7.867462204827203\n",
      "Average test loss: 0.006178670254846414\n",
      "Epoch 7/300\n",
      "Average training loss: 7.615367292616102\n",
      "Average test loss: 0.9184155821858181\n",
      "Epoch 8/300\n",
      "Average training loss: 6.217700091044108\n",
      "Average test loss: 19.869509754174285\n",
      "Epoch 9/300\n",
      "Average training loss: 5.538177854749891\n",
      "Average test loss: 32.47889194182182\n",
      "Epoch 10/300\n",
      "Average training loss: 4.598719008551703\n",
      "Average test loss: 89.68662513907915\n",
      "Epoch 11/300\n",
      "Average training loss: 3.721229835510254\n",
      "Average test loss: 0.0048068816777732636\n",
      "Epoch 12/300\n",
      "Average training loss: 3.0638764870961506\n",
      "Average test loss: 0.0421021120150884\n",
      "Epoch 13/300\n",
      "Average training loss: 2.5618138879140218\n",
      "Average test loss: 0.004048042597870032\n",
      "Epoch 14/300\n",
      "Average training loss: 2.197672311676873\n",
      "Average test loss: 0.0044240500819351936\n",
      "Epoch 15/300\n",
      "Average training loss: 1.9500826340781319\n",
      "Average test loss: 4.704751986530092\n",
      "Epoch 16/300\n",
      "Average training loss: 1.6526715137693617\n",
      "Average test loss: 0.004208378072828055\n",
      "Epoch 17/300\n",
      "Average training loss: 1.4154012044270834\n",
      "Average test loss: 0.0035323474098824794\n",
      "Epoch 18/300\n",
      "Average training loss: 1.2269865402645534\n",
      "Average test loss: 0.12676157517400052\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0703588706122504\n",
      "Average test loss: 0.0188206016106738\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9279955761697557\n",
      "Average test loss: 3.843307367960612\n",
      "Epoch 21/300\n",
      "Average training loss: 0.8117144566112094\n",
      "Average test loss: 0.006092271270851294\n",
      "Epoch 22/300\n",
      "Average training loss: 0.7186077486673991\n",
      "Average test loss: 0.0032342303602231875\n",
      "Epoch 23/300\n",
      "Average training loss: 0.6413132780922783\n",
      "Average test loss: 0.003089833131680886\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5764133307668898\n",
      "Average test loss: 0.009374620878034168\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5189068006939358\n",
      "Average test loss: 0.003053720245344771\n",
      "Epoch 26/300\n",
      "Average training loss: 0.4704554221100277\n",
      "Average test loss: 0.006327684602596694\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4235919572777218\n",
      "Average test loss: 0.016593328083761863\n",
      "Epoch 28/300\n",
      "Average training loss: 0.38556568500730726\n",
      "Average test loss: 0.00669474757483436\n",
      "Epoch 29/300\n",
      "Average training loss: 0.34926600262853835\n",
      "Average test loss: 0.0030931734922859406\n",
      "Epoch 30/300\n",
      "Average training loss: 0.32043559606870015\n",
      "Average test loss: 0.00339313004952338\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2956708323160807\n",
      "Average test loss: 0.002932338739020957\n",
      "Epoch 32/300\n",
      "Average training loss: 0.27217301988601683\n",
      "Average test loss: 0.006877340245888465\n",
      "Epoch 33/300\n",
      "Average training loss: 0.25394017816914455\n",
      "Average test loss: 10.209048895809385\n",
      "Epoch 34/300\n",
      "Average training loss: 0.23763569598727757\n",
      "Average test loss: 0.0035775689728971983\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2243646947807736\n",
      "Average test loss: 0.0028487225394282075\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2111689650747511\n",
      "Average test loss: 0.009929870285300745\n",
      "Epoch 37/300\n",
      "Average training loss: 0.20128662217987908\n",
      "Average test loss: 0.0027206520013925103\n",
      "Epoch 38/300\n",
      "Average training loss: 0.18971457317140367\n",
      "Average test loss: 0.3301646687007613\n",
      "Epoch 39/300\n",
      "Average training loss: 0.18187257187896305\n",
      "Average test loss: 0.0032399841911262936\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1713425341712104\n",
      "Average test loss: 0.03391479390818212\n",
      "Epoch 41/300\n",
      "Average training loss: 0.2336397815015581\n",
      "Average test loss: 0.0029414422783172792\n",
      "Epoch 42/300\n",
      "Average training loss: 0.16573821998967064\n",
      "Average test loss: 0.0031008120864215822\n",
      "Epoch 43/300\n",
      "Average training loss: 0.15515594793690576\n",
      "Average test loss: 0.0028503704364928936\n",
      "Epoch 44/300\n",
      "Average training loss: 0.14726931714349323\n",
      "Average test loss: 0.0027283691414114503\n",
      "Epoch 45/300\n",
      "Average training loss: 0.14236364897092182\n",
      "Average test loss: 0.0026635652422491046\n",
      "Epoch 46/300\n",
      "Average training loss: 0.13700433729754555\n",
      "Average test loss: 0.002653262709163957\n",
      "Epoch 47/300\n",
      "Average training loss: 0.13268559012148115\n",
      "Average test loss: 0.002788453025329444\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12846571021609837\n",
      "Average test loss: 0.0742581551373005\n",
      "Epoch 49/300\n",
      "Average training loss: 0.125270183801651\n",
      "Average test loss: 0.006756845203538735\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12136196011967129\n",
      "Average test loss: 0.003172204852104187\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11855152881807751\n",
      "Average test loss: 0.003196346244464318\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11460728363196056\n",
      "Average test loss: 0.0028857042545245752\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11235840176873738\n",
      "Average test loss: 0.0027440885544444123\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1104778531856007\n",
      "Average test loss: 0.0025660687167611386\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10711551529169083\n",
      "Average test loss: 0.0061756402486935254\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10483033980263604\n",
      "Average test loss: 0.08931082969490024\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10331756644116508\n",
      "Average test loss: 0.0629957876747681\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1028059620724784\n",
      "Average test loss: 0.0027376862994084755\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09944056785768933\n",
      "Average test loss: 0.002590785668335027\n",
      "Epoch 60/300\n",
      "Average training loss: 0.2253747173282835\n",
      "Average test loss: 0.006253807544294331\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1450979481538137\n",
      "Average test loss: 0.0027958540374206173\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12200968972841898\n",
      "Average test loss: 0.0035940330100970135\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11342091753085454\n",
      "Average test loss: 0.0025992632483442623\n",
      "Epoch 64/300\n",
      "Average training loss: 0.10877379591597451\n",
      "Average test loss: 0.0025639617205080057\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1055685358080599\n",
      "Average test loss: 0.012651343536045816\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10327749857637618\n",
      "Average test loss: 0.0027771788485762144\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10180898662408193\n",
      "Average test loss: 0.0026263209767639637\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09919072500533528\n",
      "Average test loss: 0.003029080411626233\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09785786253213882\n",
      "Average test loss: 0.0027010444746249254\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09686125118864908\n",
      "Average test loss: 0.010916606387433907\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09602568601237403\n",
      "Average test loss: 0.0025659813541505073\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09527347994512982\n",
      "Average test loss: 0.0025931327529251577\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0943274012207985\n",
      "Average test loss: 0.002704384207104643\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09283390371667015\n",
      "Average test loss: 0.0024445528992348246\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09224491297536426\n",
      "Average test loss: 13067.231994713993\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0921422661476665\n",
      "Average test loss: 0.0024542520696090326\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09075820146666633\n",
      "Average test loss: 1.0888199462903043\n",
      "Epoch 78/300\n",
      "Average training loss: 0.090181258837382\n",
      "Average test loss: 0.0025146476208335825\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08962560377518336\n",
      "Average test loss: 0.0054949722629454405\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0888520905441708\n",
      "Average test loss: 0.002495084526224269\n",
      "Epoch 81/300\n",
      "Average training loss: 14.741180299990708\n",
      "Average test loss: 888405963520868.4\n",
      "Epoch 82/300\n",
      "Average training loss: 6.801657092836168\n",
      "Average test loss: 5743882466.587111\n",
      "Epoch 83/300\n",
      "Average training loss: 2.4155060065587364\n",
      "Average test loss: 61331.447028354276\n",
      "Epoch 84/300\n",
      "Average training loss: 1.448318787680732\n",
      "Average test loss: 36616.50652903452\n",
      "Epoch 85/300\n",
      "Average training loss: 0.9485220271216499\n",
      "Average test loss: 10282.492729744976\n",
      "Epoch 86/300\n",
      "Average training loss: 0.6747567524380154\n",
      "Average test loss: 130.70358688311404\n",
      "Epoch 87/300\n",
      "Average training loss: 0.521129480626848\n",
      "Average test loss: 1047.9561626569728\n",
      "Epoch 88/300\n",
      "Average training loss: 0.4218587610456679\n",
      "Average test loss: 1.2669083669972088\n",
      "Epoch 89/300\n",
      "Average training loss: 0.3547222940127055\n",
      "Average test loss: 0.003078798831750949\n",
      "Epoch 90/300\n",
      "Average training loss: 0.31043749480777316\n",
      "Average test loss: 0.0031450696049465073\n",
      "Epoch 91/300\n",
      "Average training loss: 0.27668859753343794\n",
      "Average test loss: 0.0062058522036919994\n",
      "Epoch 92/300\n",
      "Average training loss: 0.2518192449145847\n",
      "Average test loss: 273366.2268801628\n",
      "Epoch 93/300\n",
      "Average training loss: 0.23109213083320193\n",
      "Average test loss: 5.89135748402112\n",
      "Epoch 94/300\n",
      "Average training loss: 0.21185062095854018\n",
      "Average test loss: 2141459.2296354165\n",
      "Epoch 95/300\n",
      "Average training loss: 0.19515784210628934\n",
      "Average test loss: 7.59107904500597\n",
      "Epoch 96/300\n",
      "Average training loss: 0.18001756416426765\n",
      "Average test loss: 1819.6409112838255\n",
      "Epoch 97/300\n",
      "Average training loss: 0.16850584208965302\n",
      "Average test loss: 179.01468540984723\n",
      "Epoch 98/300\n",
      "Average training loss: 0.16016695280869803\n",
      "Average test loss: 456.3209603138413\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1488193191687266\n",
      "Average test loss: 27.747756216329833\n",
      "Epoch 100/300\n",
      "Average training loss: 0.14045250176058874\n",
      "Average test loss: 159.97646683468753\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1315008176366488\n",
      "Average test loss: 0.00898804872069094\n",
      "Epoch 102/300\n",
      "Average training loss: 0.12471158660782708\n",
      "Average test loss: 0.0024930943430711825\n",
      "Epoch 103/300\n",
      "Average training loss: 0.12032012422879537\n",
      "Average test loss: 0.0026945252356429896\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11638378465837902\n",
      "Average test loss: 0.002583987570471234\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11446615362829632\n",
      "Average test loss: 0.004555505767257677\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11098538284169303\n",
      "Average test loss: 0.0024768567447447116\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10728178890546163\n",
      "Average test loss: 0.002638249239263435\n",
      "Epoch 108/300\n",
      "Average training loss: 0.1045703211095598\n",
      "Average test loss: 0.0038600581880244944\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10238593541251288\n",
      "Average test loss: 0.003300046760485404\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10026345507303874\n",
      "Average test loss: 0.0024835420364720954\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0981752526693874\n",
      "Average test loss: 0.002468982765658034\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09670575012101068\n",
      "Average test loss: 0.002842552515781588\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09506941619846555\n",
      "Average test loss: 0.0024022067677643565\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09384877749946383\n",
      "Average test loss: 0.0025750602586194875\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09300530556175444\n",
      "Average test loss: 0.002622732581777705\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09232213204105695\n",
      "Average test loss: 0.22998619443178178\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09231123822927476\n",
      "Average test loss: 0.0025906823573427066\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09092976962857777\n",
      "Average test loss: 0.016793973101509943\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08996912289328045\n",
      "Average test loss: 24352617.91422222\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09142991731564204\n",
      "Average test loss: 0.004868327293958929\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08927993734015359\n",
      "Average test loss: 0.0024433292063573996\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09118443042039871\n",
      "Average test loss: 0.002919759306228823\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08800234934356478\n",
      "Average test loss: 0.002498410303042167\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08723035079240798\n",
      "Average test loss: 0.002598506700868408\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08671523779299524\n",
      "Average test loss: 0.00243801468403803\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08630564855204688\n",
      "Average test loss: 0.0024657247244483896\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08526485973596573\n",
      "Average test loss: 0.002415403935851322\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08514469794432322\n",
      "Average test loss: 0.010878325763675901\n",
      "Epoch 129/300\n",
      "Average training loss: 25.97711296899451\n",
      "Average test loss: 137086321860542.47\n",
      "Epoch 130/300\n",
      "Average training loss: 3.1788335592481824\n",
      "Average test loss: 58.465348769199935\n",
      "Epoch 131/300\n",
      "Average training loss: 1.5179244668748644\n",
      "Average test loss: 1128.8990482727836\n",
      "Epoch 132/300\n",
      "Average training loss: 1.0552860255241394\n",
      "Average test loss: 0.006068746647901005\n",
      "Epoch 133/300\n",
      "Average training loss: 0.8639521453115675\n",
      "Average test loss: 0.12208532633052932\n",
      "Epoch 134/300\n",
      "Average training loss: 0.7353890665372212\n",
      "Average test loss: 12401.012935468005\n",
      "Epoch 135/300\n",
      "Average training loss: 0.6154096858766344\n",
      "Average test loss: 3403.032368241406\n",
      "Epoch 136/300\n",
      "Average training loss: 0.5037838599681854\n",
      "Average test loss: 0.025678711359906528\n",
      "Epoch 137/300\n",
      "Average training loss: 0.3767631021870507\n",
      "Average test loss: 0.034722560578336316\n",
      "Epoch 138/300\n",
      "Average training loss: 0.27993469031651813\n",
      "Average test loss: 0.0051031638785368864\n",
      "Epoch 139/300\n",
      "Average training loss: 0.22828151586320666\n",
      "Average test loss: 0.0027164319372839398\n",
      "Epoch 140/300\n",
      "Average training loss: 0.18220834012826284\n",
      "Average test loss: 0.04325011567357514\n",
      "Epoch 141/300\n",
      "Average training loss: 0.15148174544175466\n",
      "Average test loss: 0.0035591441959970526\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1389468030863338\n",
      "Average test loss: 0.002487497586549984\n",
      "Epoch 143/300\n",
      "Average training loss: 0.13106763883431752\n",
      "Average test loss: 0.003059495515293545\n",
      "Epoch 144/300\n",
      "Average training loss: 0.21963394951820372\n",
      "Average test loss: 743.1421195369512\n",
      "Epoch 145/300\n",
      "Average training loss: 0.14414403649833468\n",
      "Average test loss: 0.0026608569226745102\n",
      "Epoch 146/300\n",
      "Average training loss: 0.1245305732952224\n",
      "Average test loss: 0.002785762013453576\n",
      "Epoch 147/300\n",
      "Average training loss: 0.11960800022549099\n",
      "Average test loss: 0.0028215700021634498\n",
      "Epoch 148/300\n",
      "Average training loss: 0.11338418176439073\n",
      "Average test loss: 0.0024729381412681604\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10979150206512875\n",
      "Average test loss: 0.0025097422336952556\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10673781180381775\n",
      "Average test loss: 0.002587093049660325\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10475368514325883\n",
      "Average test loss: 0.009083399973395799\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10158016612132391\n",
      "Average test loss: 0.0027209023971938424\n",
      "Epoch 153/300\n",
      "Average training loss: 0.20933521666791705\n",
      "Average test loss: 1537510861.5239666\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1275033194952541\n",
      "Average test loss: 0.0025426675041930543\n",
      "Epoch 155/300\n",
      "Average training loss: 0.11119903065098656\n",
      "Average test loss: 0.00250310368442701\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10596930134296417\n",
      "Average test loss: 0.003072252333785097\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10225520987643136\n",
      "Average test loss: 0.010419647884865602\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09936622911691666\n",
      "Average test loss: 0.0026429069285384484\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09669709794388877\n",
      "Average test loss: 0.00240911501687434\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09608617620997958\n",
      "Average test loss: 0.007593246743083\n",
      "Epoch 161/300\n",
      "Average training loss: 0.093187809990512\n",
      "Average test loss: 0.013732517402205203\n",
      "Epoch 162/300\n",
      "Average training loss: 0.15823848853508632\n",
      "Average test loss: 82040.04923521371\n",
      "Epoch 163/300\n",
      "Average training loss: 0.12344749693075815\n",
      "Average test loss: 0.002673941294145253\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10630507221486833\n",
      "Average test loss: 0.002658917190869235\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10049186704556147\n",
      "Average test loss: 0.002510062790165345\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09664514343606101\n",
      "Average test loss: 0.0025870621951503887\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09644065545664893\n",
      "Average test loss: 0.0024620964208410846\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09254350678126018\n",
      "Average test loss: 0.002408442467658056\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09252120068338182\n",
      "Average test loss: 922.1701388821072\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08910329824023777\n",
      "Average test loss: 0.002561397130704588\n",
      "Epoch 171/300\n",
      "Average training loss: 0.15212507695621913\n",
      "Average test loss: 0.0025623008631583716\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09881701830360624\n",
      "Average test loss: 0.002566735367808077\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09600893694824643\n",
      "Average test loss: 0.0024528259123779007\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09221548252635532\n",
      "Average test loss: 0.002637935022926993\n",
      "Epoch 175/300\n",
      "Average training loss: 8.778798325108157\n",
      "Average test loss: 1465572.5183229651\n",
      "Epoch 176/300\n",
      "Average training loss: 4.496433779822455\n",
      "Average test loss: 0.0038873601315750016\n",
      "Epoch 177/300\n",
      "Average training loss: 1.827616290304396\n",
      "Average test loss: 347.8493382708786\n",
      "Epoch 178/300\n",
      "Average training loss: 1.2657912236319648\n",
      "Average test loss: 69.26761704685126\n",
      "Epoch 179/300\n",
      "Average training loss: 0.9714552669525146\n",
      "Average test loss: 1.0856852770480845\n",
      "Epoch 180/300\n",
      "Average training loss: 0.7665969924396939\n",
      "Average test loss: 17.27712317405641\n",
      "Epoch 181/300\n",
      "Average training loss: 0.6026719363530477\n",
      "Average test loss: 23.601630403052393\n",
      "Epoch 182/300\n",
      "Average training loss: 0.45283432973755733\n",
      "Average test loss: 0.06799538008454774\n",
      "Epoch 183/300\n",
      "Average training loss: 0.3771334114604526\n",
      "Average test loss: 6.508295462961826\n",
      "Epoch 184/300\n",
      "Average training loss: 0.31384450573391387\n",
      "Average test loss: 2.457224101204839\n",
      "Epoch 185/300\n",
      "Average training loss: 0.25357354180018105\n",
      "Average test loss: 12.78403879000495\n",
      "Epoch 186/300\n",
      "Average training loss: 0.2166126041677263\n",
      "Average test loss: 2733097.049972222\n",
      "Epoch 187/300\n",
      "Average training loss: 0.18697209073437585\n",
      "Average test loss: 2845.836299417284\n",
      "Epoch 188/300\n",
      "Average training loss: 0.1634307214419047\n",
      "Average test loss: 0.002570192483978139\n",
      "Epoch 189/300\n",
      "Average training loss: 0.14570370924472809\n",
      "Average test loss: 0.0025858891339351734\n",
      "Epoch 190/300\n",
      "Average training loss: 0.13284912661711376\n",
      "Average test loss: 0.008549149399623274\n",
      "Epoch 191/300\n",
      "Average training loss: 0.12416785673962699\n",
      "Average test loss: 0.002566353378817439\n",
      "Epoch 192/300\n",
      "Average training loss: 0.18122097419367897\n",
      "Average test loss: 0.003352874210725228\n",
      "Epoch 193/300\n",
      "Average training loss: 0.12491984420352512\n",
      "Average test loss: 0.31148527347524135\n",
      "Epoch 194/300\n",
      "Average training loss: 0.11326640031735102\n",
      "Average test loss: 0.026783355890048873\n",
      "Epoch 195/300\n",
      "Average training loss: 0.107794613023599\n",
      "Average test loss: 0.007422215232832564\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10423472795883815\n",
      "Average test loss: 0.004788174036476347\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10118618268436856\n",
      "Average test loss: 228493075536.68512\n",
      "Epoch 198/300\n",
      "Average training loss: 0.16680784577793545\n",
      "Average test loss: 0.002457761705956525\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10504645302560595\n",
      "Average test loss: 0.0028312352968172896\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1003926396701071\n",
      "Average test loss: 0.0024133765728523333\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10276429929998186\n",
      "Average test loss: 0.0174050705993755\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0961825219326549\n",
      "Average test loss: 0.0052869343873527316\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09539034406344096\n",
      "Average test loss: 0.00613409241495861\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09218631917238236\n",
      "Average test loss: 0.0037755321162856285\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09293099772267871\n",
      "Average test loss: 0.0025599417217696705\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0917434699734052\n",
      "Average test loss: 0.003040484772167272\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08894403582149082\n",
      "Average test loss: 0.004416299596221911\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08967196162541707\n",
      "Average test loss: 0.002390487846401003\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08978564870357514\n",
      "Average test loss: 0.002466060143171085\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0929929678440094\n",
      "Average test loss: 0.002567757830644647\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08764799337916904\n",
      "Average test loss: 2255.4387934027777\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08870772996875975\n",
      "Average test loss: 0.002563291652335061\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08614230882790354\n",
      "Average test loss: 0.0024389519209249153\n",
      "Epoch 214/300\n",
      "Average training loss: 0.11290552001529269\n",
      "Average test loss: 0.012763124292716383\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09217453886402978\n",
      "Average test loss: 0.00506061810006698\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08769474257363213\n",
      "Average test loss: 0.002413470528812872\n",
      "Epoch 217/300\n",
      "Average training loss: 0.087509704583221\n",
      "Average test loss: 0.09989780468576484\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0852243616051144\n",
      "Average test loss: 0.3825868578337961\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08486952914794287\n",
      "Average test loss: 0.0025484886697183053\n",
      "Epoch 220/300\n",
      "Average training loss: 0.2357284218735165\n",
      "Average test loss: 0.003213441907738646\n",
      "Epoch 221/300\n",
      "Average training loss: 0.10748952986796698\n",
      "Average test loss: 0.0024758204869512056\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09767437558041679\n",
      "Average test loss: 0.002612637704445256\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09216803277201123\n",
      "Average test loss: 0.00275093851900763\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08882192236185074\n",
      "Average test loss: 0.0024674938437011505\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08722668406036165\n",
      "Average test loss: 0.015320419065447318\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08648704045348697\n",
      "Average test loss: 3.7777765205105145\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08484287889136208\n",
      "Average test loss: 0.005960329621305896\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10122005415625042\n",
      "Average test loss: 0.002500309434408943\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08557516700691647\n",
      "Average test loss: 0.0024321605641808774\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08483373390965991\n",
      "Average test loss: 0.002495940257070793\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08427557748225001\n",
      "Average test loss: 0.20491545340749953\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08391723303000133\n",
      "Average test loss: 0.00560118686945902\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08227426822980245\n",
      "Average test loss: 0.009496034065882364\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08227669915888045\n",
      "Average test loss: 0.002426347597812613\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08272122410270903\n",
      "Average test loss: 0.0025866481065750123\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0817142069803344\n",
      "Average test loss: 0.0024581556651327345\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08110232910845015\n",
      "Average test loss: 0.0027214272424785626\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0809470475382275\n",
      "Average test loss: 0.0025549142389661734\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08250898532734977\n",
      "Average test loss: 0.0025783596500340436\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08680240889059172\n",
      "Average test loss: 0.0024537517046555875\n",
      "Epoch 241/300\n",
      "Average training loss: 0.1532026255991724\n",
      "Average test loss: 0.0025557228688978485\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09231609667672051\n",
      "Average test loss: 0.0025667931778977316\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0862200999127494\n",
      "Average test loss: 0.010675166839526759\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08342535769939423\n",
      "Average test loss: 0.00245203170987467\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08353556452857124\n",
      "Average test loss: 0.002906219651301702\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08105415733655294\n",
      "Average test loss: 0.0024322345324067607\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08514351245429781\n",
      "Average test loss: 0.0024580227405660683\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08034128901693556\n",
      "Average test loss: 0.0041484108372694915\n",
      "Epoch 249/300\n",
      "Average training loss: 0.12376385017236073\n",
      "Average test loss: 147.2103517180979\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08623660618729062\n",
      "Average test loss: 0.011985298777723478\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08164613717794418\n",
      "Average test loss: 0.0024914573752838703\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08032385736703873\n",
      "Average test loss: 0.002664097508208619\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08001809177796046\n",
      "Average test loss: 0.0024708735063258143\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07920264140102598\n",
      "Average test loss: 0.0024790314957499502\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07913313757710987\n",
      "Average test loss: 0.002727853117096755\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07813005266586939\n",
      "Average test loss: 0.0027017761611690125\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07852466306421492\n",
      "Average test loss: 0.002714027010732227\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0780716664592425\n",
      "Average test loss: 0.0027624752254535754\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09246706943379508\n",
      "Average test loss: 0.002733589578833845\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07869794482654996\n",
      "Average test loss: 0.0024878559032869008\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07723104273610645\n",
      "Average test loss: 0.0026890758561591305\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08132168296972911\n",
      "Average test loss: 6.057162241299947\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07982984603444736\n",
      "Average test loss: 0.0026114636054262517\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08059425089094374\n",
      "Average test loss: 0.0026814924501296546\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07644533081849417\n",
      "Average test loss: 0.02860484328866005\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07955355122685433\n",
      "Average test loss: 0.0211683389828023\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07672253233856625\n",
      "Average test loss: 0.031851677866445646\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07980525088310242\n",
      "Average test loss: 0.0029161386414327554\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07588486856553289\n",
      "Average test loss: 0.0075954961923675405\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08649705160988702\n",
      "Average test loss: 0.029682394019431537\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0752125135064125\n",
      "Average test loss: 0.0025094844821012684\n",
      "Epoch 272/300\n",
      "Average training loss: 0.1446373574667507\n",
      "Average test loss: 173.21008071802723\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09914306072394054\n",
      "Average test loss: 0.00248644361541503\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08615676131513383\n",
      "Average test loss: 0.00434925748863154\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08427281931373808\n",
      "Average test loss: 0.21625945314930545\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08461258017354541\n",
      "Average test loss: 0.002490449368022382\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08501857526434792\n",
      "Average test loss: 0.002539417571284705\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08287586236000061\n",
      "Average test loss: 0.0025442136304659976\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07641846942901612\n",
      "Average test loss: 0.002638401154635681\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0757988903787401\n",
      "Average test loss: 0.0025214156409104666\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08215789222386148\n",
      "Average test loss: 0.002746876566360394\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07943844992584652\n",
      "Average test loss: 0.0042213528700586825\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07496300508578618\n",
      "Average test loss: 0.00330308257167538\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07856501270002789\n",
      "Average test loss: 0.002895478961161441\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07773918302853902\n",
      "Average test loss: 0.0031048134494986797\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07544691985514429\n",
      "Average test loss: 0.0025853347670700814\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07653613030579355\n",
      "Average test loss: 0.0026468599778082637\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07537447655863232\n",
      "Average test loss: 0.02351217142906454\n",
      "Epoch 289/300\n",
      "Average training loss: 0.2999878998001417\n",
      "Average test loss: 12.062352942915426\n",
      "Epoch 290/300\n",
      "Average training loss: 0.12655588930182987\n",
      "Average test loss: 0.004396242392145925\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10506717222266727\n",
      "Average test loss: 0.0024754266879624793\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09709856904877556\n",
      "Average test loss: 0.0038549042058487735\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09112731921672822\n",
      "Average test loss: 0.002606960316085153\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08738143318229251\n",
      "Average test loss: 0.006670887173774342\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08397181034750409\n",
      "Average test loss: 0.0024594034522564877\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08148587540123198\n",
      "Average test loss: 0.0026789908930659294\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08080530279874802\n",
      "Average test loss: 0.005493686837868558\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07922571390867233\n",
      "Average test loss: 0.002510129526671436\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07877556171019873\n",
      "Average test loss: 0.003261332479822967\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07675885560777453\n",
      "Average test loss: 0.0035360511591037112\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 41.817043965657554\n",
      "Average test loss: 35.30657576275369\n",
      "Epoch 2/300\n",
      "Average training loss: 20.249976303100585\n",
      "Average test loss: 5345.3182239999405\n",
      "Epoch 3/300\n",
      "Average training loss: 15.208147149827745\n",
      "Average test loss: 4.550668266296387\n",
      "Epoch 4/300\n",
      "Average training loss: 11.672827491760254\n",
      "Average test loss: 3855.8326107326347\n",
      "Epoch 5/300\n",
      "Average training loss: 10.217396674262153\n",
      "Average test loss: 12.778743554981219\n",
      "Epoch 6/300\n",
      "Average training loss: 9.077579476250543\n",
      "Average test loss: 13.557905463990238\n",
      "Epoch 7/300\n",
      "Average training loss: 8.975174473232693\n",
      "Average test loss: 42.00157431758816\n",
      "Epoch 8/300\n",
      "Average training loss: 8.108926685333252\n",
      "Average test loss: 0.004041573721915484\n",
      "Epoch 9/300\n",
      "Average training loss: 7.014173930274116\n",
      "Average test loss: 91.8959498770535\n",
      "Epoch 10/300\n",
      "Average training loss: 6.097536198086209\n",
      "Average test loss: 60.994471859018006\n",
      "Epoch 11/300\n",
      "Average training loss: 5.2728631299336755\n",
      "Average test loss: 389.2528711324036\n",
      "Epoch 12/300\n",
      "Average training loss: 4.400751413133409\n",
      "Average test loss: 0.0032898452969061004\n",
      "Epoch 13/300\n",
      "Average training loss: 3.8518419244554307\n",
      "Average test loss: 0.003135287291266852\n",
      "Epoch 14/300\n",
      "Average training loss: 3.5550315901438396\n",
      "Average test loss: 0.004035817024815413\n",
      "Epoch 15/300\n",
      "Average training loss: 3.2984456248813205\n",
      "Average test loss: 0.0033798694876540037\n",
      "Epoch 16/300\n",
      "Average training loss: 3.0512100989023843\n",
      "Average test loss: 0.003221088613486952\n",
      "Epoch 17/300\n",
      "Average training loss: 2.7923355804019505\n",
      "Average test loss: 0.002809745519318514\n",
      "Epoch 18/300\n",
      "Average training loss: 2.486492071787516\n",
      "Average test loss: 0.0029650449965977006\n",
      "Epoch 19/300\n",
      "Average training loss: 2.2339491443634034\n",
      "Average test loss: 0.024822652948399384\n",
      "Epoch 20/300\n",
      "Average training loss: 2.0639030164082843\n",
      "Average test loss: 0.0028370766126447253\n",
      "Epoch 21/300\n",
      "Average training loss: 1.853228063689338\n",
      "Average test loss: 0.0035413805202891427\n",
      "Epoch 22/300\n",
      "Average training loss: 1.671372898525662\n",
      "Average test loss: 0.0024355761303255954\n",
      "Epoch 23/300\n",
      "Average training loss: 1.5121569100485908\n",
      "Average test loss: 38.99354641826285\n",
      "Epoch 24/300\n",
      "Average training loss: 1.3595157089233398\n",
      "Average test loss: 0.028199382386273807\n",
      "Epoch 25/300\n",
      "Average training loss: 1.232572987874349\n",
      "Average test loss: 0.0023765921687914264\n",
      "Epoch 26/300\n",
      "Average training loss: 1.1103378500408596\n",
      "Average test loss: 0.0022518918191393217\n",
      "Epoch 27/300\n",
      "Average training loss: 1.0051572809749179\n",
      "Average test loss: 0.002282811675220728\n",
      "Epoch 28/300\n",
      "Average training loss: 0.9051647599538167\n",
      "Average test loss: 0.005261018773954775\n",
      "Epoch 29/300\n",
      "Average training loss: 0.8155312767028808\n",
      "Average test loss: 0.003138925948180258\n",
      "Epoch 30/300\n",
      "Average training loss: 0.739289952966902\n",
      "Average test loss: 0.002863998949734701\n",
      "Epoch 31/300\n",
      "Average training loss: 0.6672150944603814\n",
      "Average test loss: 0.0022499065294654835\n",
      "Epoch 32/300\n",
      "Average training loss: 0.6053801204893324\n",
      "Average test loss: 0.0025728370267897845\n",
      "Epoch 33/300\n",
      "Average training loss: 0.5475421769883898\n",
      "Average test loss: 0.004165104738436639\n",
      "Epoch 34/300\n",
      "Average training loss: 0.4963479414516025\n",
      "Average test loss: 0.0019991757381293508\n",
      "Epoch 35/300\n",
      "Average training loss: 0.44983139742745293\n",
      "Average test loss: 0.013292845064981117\n",
      "Epoch 36/300\n",
      "Average training loss: 0.40672485494613647\n",
      "Average test loss: 0.007418418602604005\n",
      "Epoch 37/300\n",
      "Average training loss: 0.36979154515266416\n",
      "Average test loss: 0.006060098585155275\n",
      "Epoch 38/300\n",
      "Average training loss: 0.334834106736713\n",
      "Average test loss: 0.016545752223581074\n",
      "Epoch 39/300\n",
      "Average training loss: 0.3019896249771118\n",
      "Average test loss: 0.0021106990148416823\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2751599517530865\n",
      "Average test loss: 0.0028095886651426554\n",
      "Epoch 41/300\n",
      "Average training loss: 0.2501125643518236\n",
      "Average test loss: 0.002306637239538961\n",
      "Epoch 42/300\n",
      "Average training loss: 0.23337689990467494\n",
      "Average test loss: 0.003778611528376738\n",
      "Epoch 43/300\n",
      "Average training loss: 0.2103972250620524\n",
      "Average test loss: 0.004259176021752258\n",
      "Epoch 44/300\n",
      "Average training loss: 0.19314671240912543\n",
      "Average test loss: 2.5368776964942614\n",
      "Epoch 45/300\n",
      "Average training loss: 0.17841469983259836\n",
      "Average test loss: 0.002056470787463089\n",
      "Epoch 46/300\n",
      "Average training loss: 0.16586443730195363\n",
      "Average test loss: 0.08157259168393081\n",
      "Epoch 47/300\n",
      "Average training loss: 0.15577515794171226\n",
      "Average test loss: 0.0016724750893190503\n",
      "Epoch 48/300\n",
      "Average training loss: 0.14562380120489332\n",
      "Average test loss: 0.0017742450802276531\n",
      "Epoch 49/300\n",
      "Average training loss: 0.13643835690286424\n",
      "Average test loss: 0.0017090096130139298\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12808127907911937\n",
      "Average test loss: 0.007154565892285771\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12135686010784573\n",
      "Average test loss: 0.0023845084414093032\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11493591062890159\n",
      "Average test loss: 0.0019631554186344147\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10861766285366482\n",
      "Average test loss: 0.001667818038413922\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1036397453877661\n",
      "Average test loss: 0.009573192276681464\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1151401392420133\n",
      "Average test loss: 0.0019693819929121267\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10456585088041094\n",
      "Average test loss: 0.012070656562844912\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09856038745906617\n",
      "Average test loss: 0.0041174888432853754\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09262765294975704\n",
      "Average test loss: 0.0018381667239591479\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08833188545041615\n",
      "Average test loss: 0.006406257276837197\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08523006274965074\n",
      "Average test loss: 0.0016433221131770146\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0829661633769671\n",
      "Average test loss: 0.002013032447132799\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08039681830671098\n",
      "Average test loss: 0.012466456870237987\n",
      "Epoch 63/300\n",
      "Average training loss: 0.078208430243863\n",
      "Average test loss: 0.0021183987375762727\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09107294721073575\n",
      "Average test loss: 0.0025387086264996063\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08304305300447676\n",
      "Average test loss: 896.2903194308132\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0753037164443069\n",
      "Average test loss: 0.0018231251436389155\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0732241813937823\n",
      "Average test loss: 66.3884618938292\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07122995873292287\n",
      "Average test loss: 0.0015498316957511837\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07131945501102342\n",
      "Average test loss: 0.0020238442371288937\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06839532207780415\n",
      "Average test loss: 0.006807001683550576\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06738824995027648\n",
      "Average test loss: 0.0017117225151095125\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06637870896855991\n",
      "Average test loss: 0.04749792328311337\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06570289795597395\n",
      "Average test loss: 0.0015903095717852314\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06450565068920454\n",
      "Average test loss: 0.0018874661198092831\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06391953674289916\n",
      "Average test loss: 0.0064428853831357425\n",
      "Epoch 76/300\n",
      "Average training loss: 0.49858096113469863\n",
      "Average test loss: 182.2431089379266\n",
      "Epoch 77/300\n",
      "Average training loss: 0.15050383720795313\n",
      "Average test loss: 0.008459648452078302\n",
      "Epoch 78/300\n",
      "Average training loss: 0.12280725563897027\n",
      "Average test loss: 0.3174605713238319\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10784636257092158\n",
      "Average test loss: 0.0027774391051174864\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09648412948184543\n",
      "Average test loss: 0.0028704433937867483\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09058622688055039\n",
      "Average test loss: 0.0017727428753342893\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08593594888183806\n",
      "Average test loss: 0.002003132021158106\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08224675816959805\n",
      "Average test loss: 0.0016323389928891428\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07915353410111534\n",
      "Average test loss: 0.0016637502079829573\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07654625179370245\n",
      "Average test loss: 11552628059.932444\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07473118158181509\n",
      "Average test loss: 0.001662491911297871\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07287747900684674\n",
      "Average test loss: 0.0016535275284614828\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07171232002973557\n",
      "Average test loss: 0.0017765198702820472\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0700730097591877\n",
      "Average test loss: 0.0019891564055449433\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06980596073137389\n",
      "Average test loss: 0.0016280423127528693\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0678675162990888\n",
      "Average test loss: 0.001655789551532103\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06693191952175565\n",
      "Average test loss: 0.006426533127617505\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06791967966159185\n",
      "Average test loss: 0.0040489260024494595\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06599642156230079\n",
      "Average test loss: 0.0017990262195364468\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06556587043735716\n",
      "Average test loss: 0.02208198187655459\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06425261498822106\n",
      "Average test loss: 0.0016141476921944155\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06421459873517354\n",
      "Average test loss: 0.001543106846899415\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06402088972263865\n",
      "Average test loss: 1.9980377362180086\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06268823891878128\n",
      "Average test loss: 0.004391881797462702\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1510301805138588\n",
      "Average test loss: 0.0017708268399453826\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08081224211057027\n",
      "Average test loss: 0.0016500449128862885\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07345362892746925\n",
      "Average test loss: 0.0016339291365196308\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0697734271619055\n",
      "Average test loss: 0.17009085392268997\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06763166720006202\n",
      "Average test loss: 33.44046733889977\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06610816693637106\n",
      "Average test loss: 0.04626508267223835\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06476963647537762\n",
      "Average test loss: 0.002633825142816123\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06386963754230075\n",
      "Average test loss: 0.0015789710144615836\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0628345260222753\n",
      "Average test loss: 0.0016955594517704513\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06231779004136721\n",
      "Average test loss: 0.041422334271586606\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0620190001891719\n",
      "Average test loss: 0.0025591074911256633\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06120344442129135\n",
      "Average test loss: 0.00875051064685815\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06273469121257465\n",
      "Average test loss: 0.002194262518340515\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06009746223356989\n",
      "Average test loss: 0.021408969360093275\n",
      "Epoch 114/300\n",
      "Average training loss: 0.059795785225099984\n",
      "Average test loss: 0.0026932917520817784\n",
      "Epoch 115/300\n",
      "Average training loss: 0.059304111623101764\n",
      "Average test loss: 0.10809753476041886\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06379504757457309\n",
      "Average test loss: 0.0021421089073022205\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09935603317949507\n",
      "Average test loss: 0.001757231702355461\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06880049783653683\n",
      "Average test loss: 0.014040758670825097\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06296988768378893\n",
      "Average test loss: 0.0015462818603134817\n",
      "Epoch 120/300\n",
      "Average training loss: 0.062139601717392606\n",
      "Average test loss: 0.0015431899279356003\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06050159101022615\n",
      "Average test loss: 0.002969481332641509\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05962539563576381\n",
      "Average test loss: 0.001992185775707993\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05900595017274221\n",
      "Average test loss: 0.0015972993136900994\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05823993221918742\n",
      "Average test loss: 0.0015564342133390406\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05848137671748797\n",
      "Average test loss: 0.0015515826554555032\n",
      "Epoch 126/300\n",
      "Average training loss: 0.058156840003199047\n",
      "Average test loss: 0.0016680174306448962\n",
      "Epoch 127/300\n",
      "Average training loss: 0.057216362837288114\n",
      "Average test loss: 0.006526097935831381\n",
      "Epoch 128/300\n",
      "Average training loss: 0.058182823446061877\n",
      "Average test loss: 0.00332182685409983\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07814859796232647\n",
      "Average test loss: 1696.5783298373751\n",
      "Epoch 130/300\n",
      "Average training loss: 0.059714446438683406\n",
      "Average test loss: 3.311990506615076\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05764835766951243\n",
      "Average test loss: 0.002008326447982755\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05687792811459965\n",
      "Average test loss: 0.0016369005193312962\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05647753620478842\n",
      "Average test loss: 0.001585623008923398\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05585243778096305\n",
      "Average test loss: 0.0016095332748567064\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05546217465731833\n",
      "Average test loss: 0.0016641701123573714\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05593435802393489\n",
      "Average test loss: 0.002882815674878657\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05528614903820885\n",
      "Average test loss: 12.080682393408484\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05468162864777777\n",
      "Average test loss: 0.001616642978352805\n",
      "Epoch 139/300\n",
      "Average training loss: 0.054342328369617464\n",
      "Average test loss: 3.9150876979091103\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05705920701225599\n",
      "Average test loss: 676.749442308161\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0545697157714102\n",
      "Average test loss: 0.0016645772581816546\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05357288304302427\n",
      "Average test loss: 0.002865088194401728\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05354626719156901\n",
      "Average test loss: 0.01576310891430411\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05311360523766941\n",
      "Average test loss: 0.0015674213597344027\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05274074607756403\n",
      "Average test loss: 466.99425677490234\n",
      "Epoch 146/300\n",
      "Average training loss: 0.052586715930038025\n",
      "Average test loss: 0.0021964389266860155\n",
      "Epoch 147/300\n",
      "Average training loss: 0.052381160477797194\n",
      "Average test loss: 0.003376689513110452\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05242122624317805\n",
      "Average test loss: 0.0016669462336641218\n",
      "Epoch 149/300\n",
      "Average training loss: 0.051727800614304015\n",
      "Average test loss: 0.001648038983448512\n",
      "Epoch 150/300\n",
      "Average training loss: 0.051814870556195575\n",
      "Average test loss: 0.0016000321490897073\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05147706288099289\n",
      "Average test loss: 0.0018492255379549331\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05142383452587657\n",
      "Average test loss: 0.001780187307856977\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05089860341946284\n",
      "Average test loss: 0.0015953939504300555\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09334723402063051\n",
      "Average test loss: 0.025180325751503307\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06269629371497366\n",
      "Average test loss: 0.0015708139460119937\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05624982355038325\n",
      "Average test loss: 43.04780836317274\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05354044503470262\n",
      "Average test loss: 0.004160051889717579\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05213380081454913\n",
      "Average test loss: 0.0022110869338115057\n",
      "Epoch 159/300\n",
      "Average training loss: 0.051194333102968004\n",
      "Average test loss: 0.009255203291152914\n",
      "Epoch 160/300\n",
      "Average training loss: 0.050672416819466486\n",
      "Average test loss: 0.0016027300773809353\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05027831484211816\n",
      "Average test loss: 0.00423190272723635\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0508798722492324\n",
      "Average test loss: 0.002547223323956132\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05113294242156877\n",
      "Average test loss: 33.231365518266955\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04983054379456573\n",
      "Average test loss: 0.0017813613688987162\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050037390838066735\n",
      "Average test loss: 0.001592118691239092\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05098247836861346\n",
      "Average test loss: 0.04289482637122274\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05388280709253417\n",
      "Average test loss: 0.0025100489573346243\n",
      "Epoch 168/300\n",
      "Average training loss: 0.050186036402980486\n",
      "Average test loss: 74.71985039944119\n",
      "Epoch 169/300\n",
      "Average training loss: 0.050706908805502784\n",
      "Average test loss: 0.0017368784607905482\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0494751539627711\n",
      "Average test loss: 0.0020476699877116414\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04945955721868409\n",
      "Average test loss: 0.0016191643894546562\n",
      "Epoch 172/300\n",
      "Average training loss: 0.049531296481688816\n",
      "Average test loss: 0.0016252194499183031\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04902419518099891\n",
      "Average test loss: 0.017140814042287983\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07428556728363037\n",
      "Average test loss: 0.0018401456862274144\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07026341585980521\n",
      "Average test loss: 0.017554697916946478\n",
      "Epoch 176/300\n",
      "Average training loss: 0.059416928516493904\n",
      "Average test loss: 0.0016000038111685878\n",
      "Epoch 177/300\n",
      "Average training loss: 0.055875225732723875\n",
      "Average test loss: 0.0037696973722842007\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05348974844151073\n",
      "Average test loss: 0.0017731044623586867\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05221950627366702\n",
      "Average test loss: 0.002724518738893999\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05179430899686283\n",
      "Average test loss: 0.0018892797428286738\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05040294049845801\n",
      "Average test loss: 0.0016276857887084285\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05038551959064272\n",
      "Average test loss: 0.001976983280541996\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05095265301399761\n",
      "Average test loss: 0.001863171767650379\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0502124040507608\n",
      "Average test loss: 0.0077350478052265114\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04911096687780486\n",
      "Average test loss: 0.3976452877951993\n",
      "Epoch 186/300\n",
      "Average training loss: 0.5047665282554097\n",
      "Average test loss: 0.0052362870600902375\n",
      "Epoch 187/300\n",
      "Average training loss: 0.11601190263032914\n",
      "Average test loss: 0.004137911000289023\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09524694820907381\n",
      "Average test loss: 0.0018393269654156433\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08512184045712153\n",
      "Average test loss: 0.001897508010475172\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0787612249188953\n",
      "Average test loss: 0.003417931864400291\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0737390828066402\n",
      "Average test loss: 0.0018284101016405555\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06987589441405402\n",
      "Average test loss: 0.3082997689346472\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06649505949351522\n",
      "Average test loss: 0.23346845719963313\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06383621434370677\n",
      "Average test loss: 0.10063993892818689\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06127726601560911\n",
      "Average test loss: 0.030673940325259334\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05919537974397342\n",
      "Average test loss: 0.0019699226551585726\n",
      "Epoch 197/300\n",
      "Average training loss: 0.057441458327902685\n",
      "Average test loss: 0.0026049327432281443\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05608833839827114\n",
      "Average test loss: 8.578421424775902\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05571207003792127\n",
      "Average test loss: 0.0016692701128001013\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05457706825931867\n",
      "Average test loss: 0.0015808312910505467\n",
      "Epoch 201/300\n",
      "Average training loss: 0.052014559752411316\n",
      "Average test loss: 0.0018260831642482017\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0514544551703665\n",
      "Average test loss: 0.005302097852859232\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05276271871394581\n",
      "Average test loss: 0.0026105845146295096\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05027207815647125\n",
      "Average test loss: 0.006704374990529484\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04942591420147154\n",
      "Average test loss: 0.0017269341178859274\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05441567469636599\n",
      "Average test loss: 0.00259671612844492\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0574274756345484\n",
      "Average test loss: 0.003561269584939712\n",
      "Epoch 208/300\n",
      "Average training loss: 0.050326994521750344\n",
      "Average test loss: 0.006753599832248357\n",
      "Epoch 209/300\n",
      "Average training loss: 0.049698702570464874\n",
      "Average test loss: 0.02598549383878708\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05030512931611803\n",
      "Average test loss: 0.0021617571748793127\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05093103132976426\n",
      "Average test loss: 0.005452155364056428\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04889893869890107\n",
      "Average test loss: 0.0016040585240763095\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04829432308673859\n",
      "Average test loss: 0.003266182944385542\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04847654605242941\n",
      "Average test loss: 0.0016501683807517919\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04841096176372634\n",
      "Average test loss: 2.7469616763500704\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04853642238179843\n",
      "Average test loss: 0.007149350576309694\n",
      "Epoch 217/300\n",
      "Average training loss: 0.051099066972732546\n",
      "Average test loss: 0.0024097076574754383\n",
      "Epoch 218/300\n",
      "Average training loss: 0.049992627100812065\n",
      "Average test loss: 0.0017267492657216887\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05879697960615158\n",
      "Average test loss: 0.026201467813716996\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05303513180216154\n",
      "Average test loss: 0.0027351352319949203\n",
      "Epoch 221/300\n",
      "Average training loss: 0.049089349733458625\n",
      "Average test loss: 0.0016733164515139328\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05007359375225173\n",
      "Average test loss: 0.24023542952806584\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04757034228907691\n",
      "Average test loss: 3.098911828512947\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04792378623286883\n",
      "Average test loss: 0.006145927860918972\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04734442656570011\n",
      "Average test loss: 344.2114654108617\n",
      "Epoch 226/300\n",
      "Average training loss: 0.1695516639550527\n",
      "Average test loss: 0.0016525603708190222\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07660181326998605\n",
      "Average test loss: 0.0016126394375848274\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06739267162150807\n",
      "Average test loss: 0.0015882961607227722\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06225245209203826\n",
      "Average test loss: 0.0016543333845006095\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05857048548592461\n",
      "Average test loss: 0.0050212414999388985\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0559676800734467\n",
      "Average test loss: 0.0016255625184211466\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05383295542663998\n",
      "Average test loss: 0.020295181209118003\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05221608955661456\n",
      "Average test loss: 0.001782957314927545\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05110134819481108\n",
      "Average test loss: 0.10611297218667136\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05070499749647246\n",
      "Average test loss: 0.0016499148326822454\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06378973391983245\n",
      "Average test loss: 0.6174776570633468\n",
      "Epoch 237/300\n",
      "Average training loss: 0.056310453040732275\n",
      "Average test loss: 0.10858182588276556\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05033473687039481\n",
      "Average test loss: 7.765773721994625\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04880167819394006\n",
      "Average test loss: 0.0018500002997203005\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04787906953692436\n",
      "Average test loss: 0.0016364480637841754\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04748287742998865\n",
      "Average test loss: 0.0016598448829932345\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04736212180389298\n",
      "Average test loss: 0.0017248035416834884\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05923714334103796\n",
      "Average test loss: 0.0016867299255811507\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0490107617146439\n",
      "Average test loss: 0.0016572940291112496\n",
      "Epoch 245/300\n",
      "Average training loss: 0.047382145868407353\n",
      "Average test loss: 0.002715901031572786\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04695521163278156\n",
      "Average test loss: 0.0016351237534027961\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05433302324348026\n",
      "Average test loss: 0.010721182692795992\n",
      "Epoch 248/300\n",
      "Average training loss: 0.048253504736555944\n",
      "Average test loss: 0.002924603148880932\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04854965836471981\n",
      "Average test loss: 0.006715709498359097\n",
      "Epoch 250/300\n",
      "Average training loss: 0.046525739590326944\n",
      "Average test loss: 0.0120263087606161\n",
      "Epoch 251/300\n",
      "Average training loss: 0.4065384668376711\n",
      "Average test loss: 0.0019140133142678275\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0982799397110939\n",
      "Average test loss: 0.012103353341627453\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0805910722149743\n",
      "Average test loss: 3.6150481643104513\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07379399536715614\n",
      "Average test loss: 0.004225609770355125\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08588626951641506\n",
      "Average test loss: 0.03434270513885551\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06572897374629974\n",
      "Average test loss: 0.001638168045009176\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06195281950301594\n",
      "Average test loss: 0.0016718957733569875\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0590293612546391\n",
      "Average test loss: 0.001700308496494674\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05682401078277164\n",
      "Average test loss: 0.0015938114648063977\n",
      "Epoch 260/300\n",
      "Average training loss: 0.13181784844729635\n",
      "Average test loss: 1.3987873383186967e+22\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0784111752708753\n",
      "Average test loss: 1.0064893962913328e+20\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06497150879436069\n",
      "Average test loss: 0.0016713905627321866\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05934680913554297\n",
      "Average test loss: 0.017171305535568133\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05625371970070733\n",
      "Average test loss: 0.005306089366372261\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05417450504667229\n",
      "Average test loss: 0.0016426764547617899\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05952596438593335\n",
      "Average test loss: 0.0019491504459745355\n",
      "Epoch 267/300\n",
      "Average training loss: 0.051663252143396274\n",
      "Average test loss: 0.002077360012051132\n",
      "Epoch 268/300\n",
      "Average training loss: 0.050455901574757364\n",
      "Average test loss: 0.01703889237406353\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07292459473345014\n",
      "Average test loss: 0.0019249746002670792\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05292065796090497\n",
      "Average test loss: 0.0067873763061232035\n",
      "Epoch 271/300\n",
      "Average training loss: 0.050169929603735605\n",
      "Average test loss: 0.0042569394707679745\n",
      "Epoch 272/300\n",
      "Average training loss: 0.048864429384469985\n",
      "Average test loss: 0.003160804598592222\n",
      "Epoch 273/300\n",
      "Average training loss: 0.053642578303813936\n",
      "Average test loss: 0.001688925304553575\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04855898710754183\n",
      "Average test loss: 0.0016323660250960124\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06663269351919492\n",
      "Average test loss: 0.0016886325415430797\n",
      "Epoch 276/300\n",
      "Average training loss: 0.049608804182873834\n",
      "Average test loss: 0.001650214842106733\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04750904718372557\n",
      "Average test loss: 0.0015919578463460008\n",
      "Epoch 278/300\n",
      "Average training loss: 0.046854678567912846\n",
      "Average test loss: 0.001641243709354765\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06008965638611052\n",
      "Average test loss: 0.0025964895616699423\n",
      "Epoch 280/300\n",
      "Average training loss: 0.056064444141255484\n",
      "Average test loss: 0.0016896001528948546\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0537771740688218\n",
      "Average test loss: 0.027083466996542283\n",
      "Epoch 282/300\n",
      "Average training loss: 0.047275292956166795\n",
      "Average test loss: 0.0017874599680718448\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04669123767150773\n",
      "Average test loss: 0.0018526375730418497\n",
      "Epoch 284/300\n",
      "Average training loss: 0.047241909705930285\n",
      "Average test loss: 0.005645557126858168\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04642646674646272\n",
      "Average test loss: 0.0023044106397363874\n",
      "Epoch 286/300\n",
      "Average training loss: 0.046153147250413896\n",
      "Average test loss: 0.0035504942214012974\n",
      "Epoch 287/300\n",
      "Average training loss: 0.052845888866318594\n",
      "Average test loss: 0.0068908617132239875\n",
      "Epoch 288/300\n",
      "Average training loss: 0.048574380768669975\n",
      "Average test loss: 0.0019413414993840787\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04723082593414519\n",
      "Average test loss: 0.0016533959151452614\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04576509630845653\n",
      "Average test loss: 0.008000202784935634\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04593369913101196\n",
      "Average test loss: 41.761417351033955\n",
      "Epoch 292/300\n",
      "Average training loss: 0.047641536108321614\n",
      "Average test loss: 0.0016764166717314058\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04933067617151472\n",
      "Average test loss: 0.003661690721909205\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04568238785862923\n",
      "Average test loss: 0.22636188262266418\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04619633347127173\n",
      "Average test loss: 0.027409550631625784\n",
      "Epoch 296/300\n",
      "Average training loss: 0.045472215486897366\n",
      "Average test loss: 47.039866633839075\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05003197903765572\n",
      "Average test loss: 0.004271042567574316\n",
      "Epoch 298/300\n",
      "Average training loss: 0.046316587375269995\n",
      "Average test loss: 0.033355502180755135\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04495054544674026\n",
      "Average test loss: 0.8819457554817199\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04550087394979265\n",
      "Average test loss: 0.004073704201624625\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 36361.54362333849\n",
      "Average test loss: 223003237020.12256\n",
      "Epoch 2/300\n",
      "Average training loss: 42.04121930270725\n",
      "Average test loss: 47786179.00749715\n",
      "Epoch 3/300\n",
      "Average training loss: 35.22710275777181\n",
      "Average test loss: 12825090720.020779\n",
      "Epoch 4/300\n",
      "Average training loss: 29.51673379007975\n",
      "Average test loss: 3776969.3022816936\n",
      "Epoch 5/300\n",
      "Average training loss: 26.969138749864367\n",
      "Average test loss: 741878.1116282654\n",
      "Epoch 6/300\n",
      "Average training loss: 24.39205334303114\n",
      "Average test loss: 1761079.8788635486\n",
      "Epoch 7/300\n",
      "Average training loss: 20.52162761433919\n",
      "Average test loss: 2201163.6704754946\n",
      "Epoch 8/300\n",
      "Average training loss: 20.76750715128581\n",
      "Average test loss: 7550806.035172169\n",
      "Epoch 9/300\n",
      "Average training loss: 20.769172743055556\n",
      "Average test loss: 19538169.366677035\n",
      "Epoch 10/300\n",
      "Average training loss: 21.181141035291883\n",
      "Average test loss: 2319.4978926903077\n",
      "Epoch 11/300\n",
      "Average training loss: 19.9116361456977\n",
      "Average test loss: 1198519.8130035754\n",
      "Epoch 12/300\n",
      "Average training loss: 18.770031365288627\n",
      "Average test loss: 634294.6354960544\n",
      "Epoch 13/300\n",
      "Average training loss: 17.761887144300673\n",
      "Average test loss: 37751.960878336125\n",
      "Epoch 14/300\n",
      "Average training loss: 16.61048589155409\n",
      "Average test loss: 239324.6549692757\n",
      "Epoch 15/300\n",
      "Average training loss: 15.176269728766547\n",
      "Average test loss: 193711.8863049941\n",
      "Epoch 16/300\n",
      "Average training loss: 13.582424034966364\n",
      "Average test loss: 0.004236989757666986\n",
      "Epoch 17/300\n",
      "Average training loss: 12.621927730984158\n",
      "Average test loss: 1547.3269200068978\n",
      "Epoch 18/300\n",
      "Average training loss: 12.39298735470242\n",
      "Average test loss: 0.030524712642240857\n",
      "Epoch 19/300\n",
      "Average training loss: 11.789797106424967\n",
      "Average test loss: 0.980449750860739\n",
      "Epoch 20/300\n",
      "Average training loss: 11.213670853508843\n",
      "Average test loss: 41.7712245018387\n",
      "Epoch 21/300\n",
      "Average training loss: 10.369716850280762\n",
      "Average test loss: 512610.6276737383\n",
      "Epoch 22/300\n",
      "Average training loss: 9.658611404418945\n",
      "Average test loss: 4.128689891926944\n",
      "Epoch 23/300\n",
      "Average training loss: 9.177099420335558\n",
      "Average test loss: 0.5519986132066697\n",
      "Epoch 24/300\n",
      "Average training loss: 8.625611050923665\n",
      "Average test loss: 0.4624067393292983\n",
      "Epoch 25/300\n",
      "Average training loss: 8.070508693695068\n",
      "Average test loss: 0.48293788444250824\n",
      "Epoch 26/300\n",
      "Average training loss: 7.532309638553196\n",
      "Average test loss: 0.011258203121523062\n",
      "Epoch 27/300\n",
      "Average training loss: 7.082428696526422\n",
      "Average test loss: 0.05084069252924787\n",
      "Epoch 28/300\n",
      "Average training loss: 6.667486454857721\n",
      "Average test loss: 0.011500230270334416\n",
      "Epoch 29/300\n",
      "Average training loss: 6.202023678673638\n",
      "Average test loss: 0.0028649971406492923\n",
      "Epoch 30/300\n",
      "Average training loss: 5.8855356936984595\n",
      "Average test loss: 68.54568285888764\n",
      "Epoch 31/300\n",
      "Average training loss: 5.488713312360975\n",
      "Average test loss: 48.45139176111089\n",
      "Epoch 32/300\n",
      "Average training loss: 4.957176408131917\n",
      "Average test loss: 0.005460347779302133\n",
      "Epoch 33/300\n",
      "Average training loss: 4.582143128712972\n",
      "Average test loss: 20.36244575854722\n",
      "Epoch 34/300\n",
      "Average training loss: 4.241114687601725\n",
      "Average test loss: 0.0018322176397260693\n",
      "Epoch 35/300\n",
      "Average training loss: 3.9337543862660724\n",
      "Average test loss: 0.0021247294752134217\n",
      "Epoch 36/300\n",
      "Average training loss: 3.583869705200195\n",
      "Average test loss: 0.020776224625607333\n",
      "Epoch 37/300\n",
      "Average training loss: 3.2858315862019856\n",
      "Average test loss: 0.0017366129161996973\n",
      "Epoch 38/300\n",
      "Average training loss: 3.061170166227553\n",
      "Average test loss: 0.0017675222037360071\n",
      "Epoch 39/300\n",
      "Average training loss: 2.6965107561747232\n",
      "Average test loss: 0.018381268730594053\n",
      "Epoch 40/300\n",
      "Average training loss: 2.467803602642483\n",
      "Average test loss: 0.0017375203970716232\n",
      "Epoch 41/300\n",
      "Average training loss: 2.2323240367041692\n",
      "Average test loss: 0.0018204352994345956\n",
      "Epoch 42/300\n",
      "Average training loss: 1.9931847120920818\n",
      "Average test loss: 0.25422110868328146\n",
      "Epoch 43/300\n",
      "Average training loss: 1.836568894598219\n",
      "Average test loss: 0.0026254690426091355\n",
      "Epoch 44/300\n",
      "Average training loss: 1.684240375518799\n",
      "Average test loss: 0.0018590288616509901\n",
      "Epoch 45/300\n",
      "Average training loss: 1.527810674349467\n",
      "Average test loss: 0.007470425382256508\n",
      "Epoch 46/300\n",
      "Average training loss: 1.4018120893902248\n",
      "Average test loss: 0.12576076983350018\n",
      "Epoch 47/300\n",
      "Average training loss: 1.298524240175883\n",
      "Average test loss: 0.052763143496380914\n",
      "Epoch 48/300\n",
      "Average training loss: 1.1992592251035903\n",
      "Average test loss: 0.004085293902705113\n",
      "Epoch 49/300\n",
      "Average training loss: 1.0992310718960232\n",
      "Average test loss: 0.00241372390732997\n",
      "Epoch 50/300\n",
      "Average training loss: 1.0167416147126092\n",
      "Average test loss: 0.013795523553258842\n",
      "Epoch 51/300\n",
      "Average training loss: 0.9211547123591105\n",
      "Average test loss: 0.0015440503953852588\n",
      "Epoch 52/300\n",
      "Average training loss: 0.8350937159326342\n",
      "Average test loss: 0.01355214460297591\n",
      "Epoch 53/300\n",
      "Average training loss: 0.7591815594037374\n",
      "Average test loss: 0.0015909486275373233\n",
      "Epoch 54/300\n",
      "Average training loss: 0.6945766460630629\n",
      "Average test loss: 0.008107866924876968\n",
      "Epoch 55/300\n",
      "Average training loss: 0.6335269065962897\n",
      "Average test loss: 0.0014501845370978117\n",
      "Epoch 56/300\n",
      "Average training loss: 0.5681564121776157\n",
      "Average test loss: 0.001385178854378561\n",
      "Epoch 57/300\n",
      "Average training loss: 0.5473943140771654\n",
      "Average test loss: 0.0027549626192905838\n",
      "Epoch 58/300\n",
      "Average training loss: 0.4741632124582926\n",
      "Average test loss: 0.0014607375224845278\n",
      "Epoch 59/300\n",
      "Average training loss: 0.42912178031603493\n",
      "Average test loss: 0.002248086600771381\n",
      "Epoch 60/300\n",
      "Average training loss: 0.3908312218454149\n",
      "Average test loss: 0.0014653410443829166\n",
      "Epoch 61/300\n",
      "Average training loss: 0.35395490786764355\n",
      "Average test loss: 0.0015721123959455225\n",
      "Epoch 62/300\n",
      "Average training loss: 0.31297726771566603\n",
      "Average test loss: 1.6118110059102377\n",
      "Epoch 63/300\n",
      "Average training loss: 0.28154945426517064\n",
      "Average test loss: 0.0014002250416411293\n",
      "Epoch 64/300\n",
      "Average training loss: 0.2544586872789595\n",
      "Average test loss: 0.00137964506012698\n",
      "Epoch 65/300\n",
      "Average training loss: 0.23118824821048312\n",
      "Average test loss: 0.001397476427670982\n",
      "Epoch 66/300\n",
      "Average training loss: 0.20967114851209853\n",
      "Average test loss: 26885.063482747395\n",
      "Epoch 67/300\n",
      "Average training loss: 0.19208438596460553\n",
      "Average test loss: 0.0012611747671746546\n",
      "Epoch 68/300\n",
      "Average training loss: 0.1762157349983851\n",
      "Average test loss: 0.0012938717611961895\n",
      "Epoch 69/300\n",
      "Average training loss: 0.16209773446453943\n",
      "Average test loss: 0.0012054298369006979\n",
      "Epoch 70/300\n",
      "Average training loss: 0.14905883920854993\n",
      "Average test loss: 0.0025823782455796996\n",
      "Epoch 71/300\n",
      "Average training loss: 0.138070080253813\n",
      "Average test loss: 0.001402977356997629\n",
      "Epoch 72/300\n",
      "Average training loss: 0.12816280685530768\n",
      "Average test loss: 0.0011857462718875872\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1184365750087632\n",
      "Average test loss: 0.0011534912737810776\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11050303194920222\n",
      "Average test loss: 0.05575118500532376\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1028624568382899\n",
      "Average test loss: 0.001361056433059275\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09716551801231173\n",
      "Average test loss: 0.0011493201174048915\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09210751186476814\n",
      "Average test loss: 0.4565278178089195\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08559961660040749\n",
      "Average test loss: 834.1677425294262\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08097811129358079\n",
      "Average test loss: 0.6552679080690982\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07693510290318065\n",
      "Average test loss: 2.6420385117795733\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07281772434049183\n",
      "Average test loss: 0.0011361968488328986\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07010497047503789\n",
      "Average test loss: 0.0011100889898629652\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06796373116970063\n",
      "Average test loss: 0.0011119393397950464\n",
      "Epoch 84/300\n",
      "Average training loss: 0.063596072309547\n",
      "Average test loss: 0.0012712375403692326\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06209469867414898\n",
      "Average test loss: 0.0010824445507799586\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05938673404521412\n",
      "Average test loss: 14.164545340417988\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05760606803496679\n",
      "Average test loss: 0.0017041707927775052\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05599320572283533\n",
      "Average test loss: 9537279.653916666\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05412120117081536\n",
      "Average test loss: 34472.171360112545\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08028189603487651\n",
      "Average test loss: 0.0012009595329355862\n",
      "Epoch 91/300\n",
      "Average training loss: 0.058113064861959884\n",
      "Average test loss: 0.001708427765717109\n",
      "Epoch 92/300\n",
      "Average training loss: 0.054158794356717006\n",
      "Average test loss: 0.0011437822789367703\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05379969784948561\n",
      "Average test loss: 938.9770119699472\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05132572815484471\n",
      "Average test loss: 0.0011289310997558964\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04993648974431886\n",
      "Average test loss: 0.16694793822864692\n",
      "Epoch 96/300\n",
      "Average training loss: 0.048786045084396996\n",
      "Average test loss: 0.005524534384926988\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04809699719813135\n",
      "Average test loss: 0.0012617125048612556\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04753026274840037\n",
      "Average test loss: 0.0011563161886814569\n",
      "Epoch 99/300\n",
      "Average training loss: 0.046839303301440344\n",
      "Average test loss: 0.1954473224737578\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04675835360089938\n",
      "Average test loss: 8.568832175061935\n",
      "Epoch 101/300\n",
      "Average training loss: 0.055292354537381065\n",
      "Average test loss: 0.019348846830427646\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04626654962367482\n",
      "Average test loss: 0.019024758405155606\n",
      "Epoch 103/300\n",
      "Average training loss: 0.047124865776962704\n",
      "Average test loss: 0.0020728865965372986\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04893362608220842\n",
      "Average test loss: 4.270489705874688\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04643830622567071\n",
      "Average test loss: 0.0021501677989338836\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04494047701689932\n",
      "Average test loss: 0.001097616169342978\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04431418562266562\n",
      "Average test loss: 6.10832039248695\n",
      "Epoch 108/300\n",
      "Average training loss: 0.044248003582159676\n",
      "Average test loss: 0.15805489826492136\n",
      "Epoch 109/300\n",
      "Average training loss: 0.043314448724190394\n",
      "Average test loss: 6.398306883085933\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04295349728067716\n",
      "Average test loss: 0.0027626637457869945\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04274354703227679\n",
      "Average test loss: 0.001565306572864453\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04441959456602732\n",
      "Average test loss: 0.0010705801063838104\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04266660770939456\n",
      "Average test loss: 0.07652307803101009\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04177798601322704\n",
      "Average test loss: 0.0023329359280566376\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04133613805969556\n",
      "Average test loss: 6110.4136154773405\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04163731823861599\n",
      "Average test loss: 0.001067784105013642\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04092252503832181\n",
      "Average test loss: 53.26055149917931\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0427933912674586\n",
      "Average test loss: 102777.15667306857\n",
      "Epoch 119/300\n",
      "Average training loss: 0.041788238909509445\n",
      "Average test loss: 0.9689820683673024\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04042622825834486\n",
      "Average test loss: 0.0033630550124992926\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03989822907745838\n",
      "Average test loss: 0.011589182409250902\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03983222562074661\n",
      "Average test loss: 0.030658920228481292\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0408338145116965\n",
      "Average test loss: 0.0017610838486709528\n",
      "Epoch 124/300\n",
      "Average training loss: 0.050826423284080297\n",
      "Average test loss: 0.0010990824326614125\n",
      "Epoch 125/300\n",
      "Average training loss: 0.040138384491205215\n",
      "Average test loss: 0.0010319866171727577\n",
      "Epoch 126/300\n",
      "Average training loss: 0.039194138725598654\n",
      "Average test loss: 3.1272080198311145\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03879703708324168\n",
      "Average test loss: 0.2550491912271827\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03852168633871608\n",
      "Average test loss: 14.58405761243941\n",
      "Epoch 129/300\n",
      "Average training loss: 0.061788321796390745\n",
      "Average test loss: 738.58303773329\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04226811274223857\n",
      "Average test loss: 0.001082762641366571\n",
      "Epoch 131/300\n",
      "Average training loss: 0.040482741243309445\n",
      "Average test loss: 0.001477753967564139\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03957424920135074\n",
      "Average test loss: 0.628846664617252\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03896031045582559\n",
      "Average test loss: 0.0010820779650368624\n",
      "Epoch 134/300\n",
      "Average training loss: 0.038606763647662266\n",
      "Average test loss: 0.001062346093925751\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03828282459908062\n",
      "Average test loss: 0.0016123803750508362\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03979762405984932\n",
      "Average test loss: 0.0014778521791514424\n",
      "Epoch 137/300\n",
      "Average training loss: 0.038631855077213714\n",
      "Average test loss: 4.986010580614209\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03789053422709306\n",
      "Average test loss: 714.6022372537417\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04974390323625671\n",
      "Average test loss: 0.0012234675642102957\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03865944226582845\n",
      "Average test loss: 0.4971556238943659\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03821420558955934\n",
      "Average test loss: 0.0011250257730070089\n",
      "Epoch 142/300\n",
      "Average training loss: 0.037262480391396416\n",
      "Average test loss: 0.0011258327020332217\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03704642455776532\n",
      "Average test loss: 3.7419198351128853\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04249653094013532\n",
      "Average test loss: 0.0010661847597608963\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03734165708720684\n",
      "Average test loss: 0.001959838850216733\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03670418526728948\n",
      "Average test loss: 0.5446238523477481\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03678853692611059\n",
      "Average test loss: 0.0027977960364272197\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03689393256439103\n",
      "Average test loss: 0.010279946185855402\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03646534665756755\n",
      "Average test loss: 0.0010621236089823974\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03721833998627133\n",
      "Average test loss: 0.00723286537732929\n",
      "Epoch 151/300\n",
      "Average training loss: 0.036836660961310067\n",
      "Average test loss: 0.0011884560944098565\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03697062127788862\n",
      "Average test loss: 0.0031459652728711564\n",
      "Epoch 153/300\n",
      "Average training loss: 0.035833153723014724\n",
      "Average test loss: 1059059.2639911978\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03586392031775581\n",
      "Average test loss: 1.0027732387632131\n",
      "Epoch 155/300\n",
      "Average training loss: 0.037152014116446176\n",
      "Average test loss: 0.010233942752497064\n",
      "Epoch 156/300\n",
      "Average training loss: 0.035839326358503766\n",
      "Average test loss: 24.50670627425611\n",
      "Epoch 157/300\n",
      "Average training loss: 0.036094201477037534\n",
      "Average test loss: 0.008117322421736187\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06708215897613101\n",
      "Average test loss: 0.0011997537129662104\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04886499029066828\n",
      "Average test loss: 0.001827997573547893\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04242848069965839\n",
      "Average test loss: 0.0016781469967423214\n",
      "Epoch 161/300\n",
      "Average training loss: 0.039826303975449666\n",
      "Average test loss: 0.00461770514316029\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03894021198153496\n",
      "Average test loss: 941812833604.9493\n",
      "Epoch 163/300\n",
      "Average training loss: 0.037072756121555965\n",
      "Average test loss: 521.5469580262626\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0363588707447052\n",
      "Average test loss: 2.981116745367168e+27\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06254305073287753\n",
      "Average test loss: 0.08750176375317904\n",
      "Epoch 166/300\n",
      "Average training loss: 0.040442279027567966\n",
      "Average test loss: 0.001375711139705446\n",
      "Epoch 167/300\n",
      "Average training loss: 0.038166263292233146\n",
      "Average test loss: 0.015612408986936013\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03691964221662945\n",
      "Average test loss: 0.006922895959888895\n",
      "Epoch 169/300\n",
      "Average training loss: 0.036487159892916676\n",
      "Average test loss: 118.36954912399169\n",
      "Epoch 170/300\n",
      "Average training loss: 0.038354643639591006\n",
      "Average test loss: 0.0012949294238868687\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03614817600780063\n",
      "Average test loss: 2.645585634884735\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03557646748092439\n",
      "Average test loss: 0.1359558514034272\n",
      "Epoch 173/300\n",
      "Average training loss: 0.036946168300178314\n",
      "Average test loss: 0.024495643543182975\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04635693194468816\n",
      "Average test loss: 0.0011008864536157085\n",
      "Epoch 175/300\n",
      "Average training loss: 0.035982146380676164\n",
      "Average test loss: 0.003796205635699961\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03549576628539297\n",
      "Average test loss: 0.0010769482012838124\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03519422549009323\n",
      "Average test loss: 0.00164342586365011\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0417093097600672\n",
      "Average test loss: 0.007224425954640739\n",
      "Epoch 179/300\n",
      "Average training loss: 0.035727143017782105\n",
      "Average test loss: 0.022984690765436324\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03489921318160163\n",
      "Average test loss: 0.001319810800668266\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03465013910333316\n",
      "Average test loss: 0.0017984111162109508\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05085665581292576\n",
      "Average test loss: 0.0019263936946582463\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08000386819905705\n",
      "Average test loss: 0.08741091650889979\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04983255590829584\n",
      "Average test loss: 1205113.4320833334\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04453888689478239\n",
      "Average test loss: 0.09543722309213545\n",
      "Epoch 186/300\n",
      "Average training loss: 0.041866905013720195\n",
      "Average test loss: 0.002040064772384034\n",
      "Epoch 187/300\n",
      "Average training loss: 0.039588248676723906\n",
      "Average test loss: 0.0010678071512116327\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05790520966053009\n",
      "Average test loss: 0.1680990509664019\n",
      "Epoch 189/300\n",
      "Average training loss: 0.047171344798472195\n",
      "Average test loss: 0.0099351143582931\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04108508834242821\n",
      "Average test loss: 1.4529550348100149\n",
      "Epoch 191/300\n",
      "Average training loss: 0.038779171624117426\n",
      "Average test loss: 0.0011156465972049368\n",
      "Epoch 192/300\n",
      "Average training loss: 0.28716984400484297\n",
      "Average test loss: 0.051950786619136734\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07488565638992521\n",
      "Average test loss: 2.0875100410016623e+22\n",
      "Epoch 194/300\n",
      "Average training loss: 0.061928497208489315\n",
      "Average test loss: 0.0011014759298413992\n",
      "Epoch 195/300\n",
      "Average training loss: 0.056120575153165396\n",
      "Average test loss: 0.0010644801954428356\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05486104371481471\n",
      "Average test loss: 0.009257402229226297\n",
      "Epoch 197/300\n",
      "Average training loss: 0.049368301570415496\n",
      "Average test loss: 37945864.26580004\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04673100800315539\n",
      "Average test loss: 851.3964796835375\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04491202190187242\n",
      "Average test loss: 25882313.318447113\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04504136598772473\n",
      "Average test loss: 0.0010362868612218234\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0424029424322976\n",
      "Average test loss: 5437966.908888889\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04372225254442957\n",
      "Average test loss: 0.0018803772021912866\n",
      "Epoch 203/300\n",
      "Average training loss: 0.040283022064301705\n",
      "Average test loss: 1.6269860519518455\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03932200285875135\n",
      "Average test loss: 0.014209538069864113\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03934023435248269\n",
      "Average test loss: 0.020516055253644784\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04020161031352149\n",
      "Average test loss: 0.0010377981658611032\n",
      "Epoch 207/300\n",
      "Average training loss: 0.037696951788332725\n",
      "Average test loss: 1047.4421917619886\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03809550736678971\n",
      "Average test loss: 0.00403328904364672\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04169602018594742\n",
      "Average test loss: 0.0010953825669777063\n",
      "Epoch 210/300\n",
      "Average training loss: 0.036833478152751925\n",
      "Average test loss: 0.0010373761717540522\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03688585736685329\n",
      "Average test loss: 0.005743451381723086\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03999803821576966\n",
      "Average test loss: 0.0039258262856553\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03645303269227346\n",
      "Average test loss: 0.0023990295531435145\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03546899593869845\n",
      "Average test loss: 0.6802520477763481\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07044365866316689\n",
      "Average test loss: 631.9605568876722\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06158621277742916\n",
      "Average test loss: 0.0010325332090465559\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04608255354894532\n",
      "Average test loss: 0.0010319544965815212\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04619535592198372\n",
      "Average test loss: 0.0010862315560484098\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03955495702226957\n",
      "Average test loss: 0.003512540265917778\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03837461122539308\n",
      "Average test loss: 6.069604126397934\n",
      "Epoch 221/300\n",
      "Average training loss: 0.037896705624130035\n",
      "Average test loss: 1.0781120372230395\n",
      "Epoch 222/300\n",
      "Average training loss: 0.11414813056257037\n",
      "Average test loss: 7.337523075378397\n",
      "Epoch 223/300\n",
      "Average training loss: 0.15231588134500715\n",
      "Average test loss: 363.21162357557563\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06523508450720045\n",
      "Average test loss: 0.004715808688766427\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05574404449595345\n",
      "Average test loss: 0.0011786808763734169\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05212724154194196\n",
      "Average test loss: 0.0011762878266680572\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04877509050402377\n",
      "Average test loss: 0.7233537267359594\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04630299444993337\n",
      "Average test loss: 15.604806112510463\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04419999165336291\n",
      "Average test loss: 0.001076585131800837\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04256719653142823\n",
      "Average test loss: 0.0010631688008498814\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0407765873670578\n",
      "Average test loss: 0.008498443705754147\n",
      "Epoch 232/300\n",
      "Average training loss: 0.040209452140662405\n",
      "Average test loss: 0.002514824608133899\n",
      "Epoch 233/300\n",
      "Average training loss: 0.11074162814683385\n",
      "Average test loss: 0.5058982322853472\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05368118055330382\n",
      "Average test loss: 0.0010598351714822152\n",
      "Epoch 235/300\n",
      "Average training loss: 0.047463557243347165\n",
      "Average test loss: 0.0010984607754896084\n",
      "Epoch 236/300\n",
      "Average training loss: 0.044617845167716344\n",
      "Average test loss: 0.3863099344908777\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0424890106121699\n",
      "Average test loss: 3826.1690127247107\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04099491448865997\n",
      "Average test loss: 0.0012507228665053845\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06951016560196877\n",
      "Average test loss: 191.27569382893378\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04253604754474428\n",
      "Average test loss: 0.12517914724556936\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04043326451712184\n",
      "Average test loss: 9050005591.754902\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03958553633424971\n",
      "Average test loss: 10.842384363010423\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03883502524097761\n",
      "Average test loss: 864.8345347154105\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0388973235156801\n",
      "Average test loss: 0.40416177231063033\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03896846954690086\n",
      "Average test loss: 1588621.3010088624\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04028216992484199\n",
      "Average test loss: 451690904.34103495\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08605837497446273\n",
      "Average test loss: 0.0010610283310007718\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04912423094775942\n",
      "Average test loss: 0.0015720370687130425\n",
      "Epoch 249/300\n",
      "Average training loss: 0.043352165073156354\n",
      "Average test loss: 0.0010425821508591375\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04093518231312434\n",
      "Average test loss: 0.0012539115083507366\n",
      "Epoch 251/300\n",
      "Average training loss: 0.039865936987929874\n",
      "Average test loss: 1.1316257871036521\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03976202982001834\n",
      "Average test loss: 6.061907925488955e+25\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0829108357363277\n",
      "Average test loss: 298282.48280050914\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04565133919649654\n",
      "Average test loss: 295693779519.26044\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04140604581766658\n",
      "Average test loss: 0.0010318173413268394\n",
      "Epoch 256/300\n",
      "Average training loss: 0.14328123880426089\n",
      "Average test loss: 4839.948217815862\n",
      "Epoch 257/300\n",
      "Average training loss: 0.1114093432393339\n",
      "Average test loss: 39916.71184397004\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04888024966915448\n",
      "Average test loss: 1095341.5929291178\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04608918023440573\n",
      "Average test loss: 0.0013722570584052138\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04341246429416869\n",
      "Average test loss: 0.0010352363145599763\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04176796231004927\n",
      "Average test loss: 0.2672236182197101\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04064121346506808\n",
      "Average test loss: 0.0024044961931390897\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03975327341920799\n",
      "Average test loss: 0.01613444535773144\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05011414423584938\n",
      "Average test loss: 34910988.936725445\n",
      "Epoch 265/300\n",
      "Average training loss: 0.043882812271515526\n",
      "Average test loss: 0.0032215930765701665\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03930026011665662\n",
      "Average test loss: 0.004150427910757975\n",
      "Epoch 267/300\n",
      "Average training loss: 0.10032920664548874\n",
      "Average test loss: 0.0010474306896535886\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0458626884188917\n",
      "Average test loss: 0.002609849264534811\n",
      "Epoch 269/300\n",
      "Average training loss: 0.042044890582561494\n",
      "Average test loss: 0.0010523975579481985\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04023984188669258\n",
      "Average test loss: 0.5622677829364936\n",
      "Epoch 271/300\n",
      "Average training loss: 0.038817281113730534\n",
      "Average test loss: 60.36054022032188\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03827730654345619\n",
      "Average test loss: 0.00885047377532141\n",
      "Epoch 273/300\n",
      "Average training loss: 0.038404496063788734\n",
      "Average test loss: 36955.32810244607\n",
      "Epoch 274/300\n",
      "Average training loss: 0.039046561035845015\n",
      "Average test loss: 0.31289193519825736\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03691665841473474\n",
      "Average test loss: 0.013783261153640019\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039228485895527736\n",
      "Average test loss: 0.0012352328795370543\n",
      "Epoch 277/300\n",
      "Average training loss: 0.036091330409049986\n",
      "Average test loss: 34.984297394665994\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03631464465293619\n",
      "Average test loss: 8039.727703922566\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03731742708550559\n",
      "Average test loss: 0.024203167881557925\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03554234291613102\n",
      "Average test loss: 0.00373251549858186\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03526313433713383\n",
      "Average test loss: 72048060016.13313\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03669120908776919\n",
      "Average test loss: 123879.41114612698\n",
      "Epoch 283/300\n",
      "Average training loss: 0.049906170944372816\n",
      "Average test loss: 0.005245766791515052\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03889022686746385\n",
      "Average test loss: 0.001045914522682627\n",
      "Epoch 285/300\n",
      "Average training loss: 0.035768028668231436\n",
      "Average test loss: 0.0035659910871957737\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04042297066582574\n",
      "Average test loss: 0.006507641145907756\n",
      "Epoch 287/300\n",
      "Average training loss: 0.035056520271632405\n",
      "Average test loss: 6451754.871228408\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05515754900376002\n",
      "Average test loss: 3.7047624001908632\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03638668909006649\n",
      "Average test loss: 150460714.02578744\n",
      "Epoch 290/300\n",
      "Average training loss: 0.035735777616500854\n",
      "Average test loss: 3642436.8728777277\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03457976542744372\n",
      "Average test loss: 0.0026377077259692465\n",
      "Epoch 292/300\n",
      "Average training loss: 0.034580977257755066\n",
      "Average test loss: 0.0013088953993800614\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04243526760240396\n",
      "Average test loss: 0.00285074764645348\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04024345310197936\n",
      "Average test loss: 0.0010553051828820672\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0355528106060293\n",
      "Average test loss: 0.006248588957513372\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09420867166254256\n",
      "Average test loss: 5.375400898356715\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05423804331156942\n",
      "Average test loss: 0.0010436608849300278\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04542729249265459\n",
      "Average test loss: 6.592144009939498\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04167187058925629\n",
      "Average test loss: 811.5336465043806\n",
      "Epoch 300/300\n",
      "Average training loss: 0.038925124065743555\n",
      "Average test loss: 0.0012144939507254297\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_64_Depth10/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.06\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.57\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.68\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.26\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.57\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.74\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.12\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.22\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.37\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.60\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.98\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.23\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.81\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.83\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.05\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.71\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.08\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.34\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.40\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.56\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.04\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.40\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.03\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.40\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.54\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.09\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.51\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.82\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.23\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.34\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.44\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.71\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.76\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.99\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 34.00\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 34.01\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 33.78\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 33.76\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 33.77\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.97\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 34.35\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 34.56\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 34.81\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 35.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 35.12\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 35.25\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 35.39\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 35.27\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 35.40\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
