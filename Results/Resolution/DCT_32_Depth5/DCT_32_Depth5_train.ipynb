{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_32x32.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06313871338632372\n",
      "Average test loss: 0.005042340987051527\n",
      "Epoch 2/300\n",
      "Average training loss: 0.024620196673605176\n",
      "Average test loss: 0.0047560481576042046\n",
      "Epoch 3/300\n",
      "Average training loss: 0.023512551594939497\n",
      "Average test loss: 0.004668490430961052\n",
      "Epoch 4/300\n",
      "Average training loss: 0.023034626689222123\n",
      "Average test loss: 0.004602760399795241\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022751358636551432\n",
      "Average test loss: 0.004562286726716492\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022562033312188253\n",
      "Average test loss: 0.004530653908745282\n",
      "Epoch 7/300\n",
      "Average training loss: 0.022385004955033462\n",
      "Average test loss: 0.004501940273576313\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022244713672333292\n",
      "Average test loss: 0.00452137959169017\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022140113171603946\n",
      "Average test loss: 0.004557130109518767\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022031057058109177\n",
      "Average test loss: 0.0044421704734365144\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02192554355992211\n",
      "Average test loss: 0.00443272372500764\n",
      "Epoch 12/300\n",
      "Average training loss: 0.021846182114548154\n",
      "Average test loss: 0.004410118049217595\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02175482569138209\n",
      "Average test loss: 0.004402499585102001\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021685305545727413\n",
      "Average test loss: 0.004384823769330979\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021612743005156518\n",
      "Average test loss: 0.004365841913554404\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021526788512865703\n",
      "Average test loss: 0.004359326181726323\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0214671468436718\n",
      "Average test loss: 0.004345516517344448\n",
      "Epoch 18/300\n",
      "Average training loss: 0.021408007838659815\n",
      "Average test loss: 0.0043608980919751855\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021344431643684707\n",
      "Average test loss: 0.0043527952763769365\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021282895150283972\n",
      "Average test loss: 0.004326295183350643\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02124338919752174\n",
      "Average test loss: 0.0043177787121385335\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021188180193305015\n",
      "Average test loss: 0.004290524655539129\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021130946354733574\n",
      "Average test loss: 0.004290720637473796\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021082013741135597\n",
      "Average test loss: 0.0043042337811655465\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021038754180073737\n",
      "Average test loss: 0.004284762250466479\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02099964819682969\n",
      "Average test loss: 0.0042837879901958835\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0209591446734137\n",
      "Average test loss: 0.004282685891000761\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02090532660484314\n",
      "Average test loss: 0.004271383816169368\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020868688472443157\n",
      "Average test loss: 0.004270383699072732\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020824286366502444\n",
      "Average test loss: 0.004295047394517395\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020808239994777572\n",
      "Average test loss: 0.004256824434010519\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0207636373076174\n",
      "Average test loss: 0.004246061337283916\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020723110246989463\n",
      "Average test loss: 0.004256923595650329\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02068941101928552\n",
      "Average test loss: 0.004269081397602956\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0206671232862605\n",
      "Average test loss: 0.004242373944570621\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02063755442864365\n",
      "Average test loss: 0.00425036360302733\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02059450701872508\n",
      "Average test loss: 0.004236603441337744\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020561919246282843\n",
      "Average test loss: 0.004241902886786395\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020528769104844993\n",
      "Average test loss: 0.004252601662029822\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020504834464854665\n",
      "Average test loss: 0.004257196012677418\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020470834626091852\n",
      "Average test loss: 0.004243015085657438\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0204390893023875\n",
      "Average test loss: 0.0042419102858338095\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020420109003782272\n",
      "Average test loss: 0.004224980132033427\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020398150511085986\n",
      "Average test loss: 0.004240330109579696\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02036512235386504\n",
      "Average test loss: 0.004233335567845238\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02033705407463842\n",
      "Average test loss: 0.004235011850380235\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020296552592681514\n",
      "Average test loss: 0.004239246631662051\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02027418344385094\n",
      "Average test loss: 0.004230027165263891\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020240300158659616\n",
      "Average test loss: 0.0042342371882663835\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02022204248607159\n",
      "Average test loss: 0.004228392768030365\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02019466501639949\n",
      "Average test loss: 0.004245919701746768\n",
      "Epoch 52/300\n",
      "Average training loss: 0.020171082415514522\n",
      "Average test loss: 0.004274670532387164\n",
      "Epoch 53/300\n",
      "Average training loss: 0.020131041495336427\n",
      "Average test loss: 0.0042727003569404285\n",
      "Epoch 54/300\n",
      "Average training loss: 0.020111273169517517\n",
      "Average test loss: 0.0042671514625350635\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0200758229908016\n",
      "Average test loss: 0.004285692424823841\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020050838152567547\n",
      "Average test loss: 0.004235522697162297\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02003986537290944\n",
      "Average test loss: 0.0042616734732356335\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020011910235716236\n",
      "Average test loss: 0.004283892975085311\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019975046298570102\n",
      "Average test loss: 0.004237765401808752\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01993543729434411\n",
      "Average test loss: 0.0042639160748157235\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019924611939324273\n",
      "Average test loss: 0.004262488000715772\n",
      "Epoch 62/300\n",
      "Average training loss: 0.019889679942693975\n",
      "Average test loss: 0.004273562744259834\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019859323942826853\n",
      "Average test loss: 0.004301327838251988\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01983580849236912\n",
      "Average test loss: 0.004237066814262006\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019816953567994965\n",
      "Average test loss: 0.004573716151217619\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019793243144949277\n",
      "Average test loss: 0.004302427873429325\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019760869110623996\n",
      "Average test loss: 0.004341817991187175\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0197262825253937\n",
      "Average test loss: 0.004248553986764617\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01969535000456704\n",
      "Average test loss: 0.004256304228885306\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019670150409142176\n",
      "Average test loss: 0.004338838948557774\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019633528762393528\n",
      "Average test loss: 0.004322694704764419\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01961574051115248\n",
      "Average test loss: 0.004322246635953585\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01958772853182422\n",
      "Average test loss: 0.004307084385305643\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019559700558582943\n",
      "Average test loss: 0.0043033809479739935\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01953196548918883\n",
      "Average test loss: 0.004340840750891302\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019505373464690313\n",
      "Average test loss: 0.004265613256643216\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01948101918399334\n",
      "Average test loss: 0.004371500223461125\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01946221647163232\n",
      "Average test loss: 0.00427978048256288\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019427278806765876\n",
      "Average test loss: 0.004330713328801923\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019398701753881243\n",
      "Average test loss: 0.004331726226955652\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0193881855905056\n",
      "Average test loss: 0.004334062644797894\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019352331707874933\n",
      "Average test loss: 0.004587755992594693\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01934206887251801\n",
      "Average test loss: 0.004282437924709585\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019288565665483474\n",
      "Average test loss: 0.00428252092955841\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019288113873038026\n",
      "Average test loss: 0.004346799696485201\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019257808547880916\n",
      "Average test loss: 0.004342133750725123\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01921371250682407\n",
      "Average test loss: 0.004312100661711561\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019207086467080647\n",
      "Average test loss: 0.004298784207759632\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01917247723456886\n",
      "Average test loss: 0.004471642471022076\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019145803183317184\n",
      "Average test loss: 0.00444037073602279\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01912549197839366\n",
      "Average test loss: 0.00438247292364637\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01910485644141833\n",
      "Average test loss: 0.004318559561752611\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019073391225602893\n",
      "Average test loss: 0.004445550200011995\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01905981956091192\n",
      "Average test loss: 0.004301091762466563\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019020059210558733\n",
      "Average test loss: 0.0044355121817853714\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01901218899173869\n",
      "Average test loss: 0.004415302567183971\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01898507543736034\n",
      "Average test loss: 0.004345849940967228\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018943110439512464\n",
      "Average test loss: 0.004369878837217887\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018935855949090585\n",
      "Average test loss: 0.00450069748217033\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018909749632080395\n",
      "Average test loss: 0.0044416071546988355\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018888184571431745\n",
      "Average test loss: 0.00438428161583013\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018866734779543346\n",
      "Average test loss: 0.004416857075360086\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018851897687547737\n",
      "Average test loss: 0.004370138539622227\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01883128426141209\n",
      "Average test loss: 0.004418912745184369\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01880938201314873\n",
      "Average test loss: 0.004321251584630873\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018774749530686273\n",
      "Average test loss: 0.004544708466778199\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018761066822542083\n",
      "Average test loss: 0.004482764792111185\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01873658794330226\n",
      "Average test loss: 0.00450027089793649\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018730341780516836\n",
      "Average test loss: 0.004518254044569201\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018700012865993713\n",
      "Average test loss: 0.004470927234325144\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018669058516621588\n",
      "Average test loss: 0.004431199975932638\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01866529701153437\n",
      "Average test loss: 0.004390532068080372\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01863610397775968\n",
      "Average test loss: 0.00457878342601988\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018616541706853444\n",
      "Average test loss: 0.004704087094714244\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018597823646333483\n",
      "Average test loss: 0.004434309283892313\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018583522468805312\n",
      "Average test loss: 0.0044679295615189605\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01854659688224395\n",
      "Average test loss: 0.004417471967223618\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018542328558034366\n",
      "Average test loss: 0.004434393080158366\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01853972001373768\n",
      "Average test loss: 0.0046043347066475285\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01853380495475398\n",
      "Average test loss: 0.004477686236715979\n",
      "Epoch 121/300\n",
      "Average training loss: 0.018479248866438865\n",
      "Average test loss: 0.0044370937268767095\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01845842504170206\n",
      "Average test loss: 0.004511428826798995\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018444839460982215\n",
      "Average test loss: 0.0044018524655451375\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018431388633118734\n",
      "Average test loss: 0.0045930009020699394\n",
      "Epoch 125/300\n",
      "Average training loss: 0.018425312416421043\n",
      "Average test loss: 0.004654219992872742\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01839275134768751\n",
      "Average test loss: 0.004512790454344617\n",
      "Epoch 127/300\n",
      "Average training loss: 0.018361342769530085\n",
      "Average test loss: 0.004441375686269668\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0183588061672118\n",
      "Average test loss: 0.0045442970883515146\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018342901562650997\n",
      "Average test loss: 0.004477899017847247\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0183494233439366\n",
      "Average test loss: 0.004385037304833531\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01830913714485036\n",
      "Average test loss: 0.004490738545027044\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018269727455245122\n",
      "Average test loss: 0.00461943098405997\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018272519280513128\n",
      "Average test loss: 0.004419224447674222\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018254642138878504\n",
      "Average test loss: 0.004459120806513561\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018254568556944527\n",
      "Average test loss: 0.004426907945424319\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01822649950120184\n",
      "Average test loss: 0.004470330150384042\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018205671486755212\n",
      "Average test loss: 0.004692129869220986\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018188563427163493\n",
      "Average test loss: 0.004404103288840917\n",
      "Epoch 139/300\n",
      "Average training loss: 0.018181656239761246\n",
      "Average test loss: 0.00444497326347563\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0181758768633008\n",
      "Average test loss: 0.004705942054589589\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0181502344649699\n",
      "Average test loss: 0.004483315111034446\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018140615062581168\n",
      "Average test loss: 0.004492622995956076\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01810983283321063\n",
      "Average test loss: 0.004593779837505685\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018118614339166218\n",
      "Average test loss: 0.00460243426139156\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018094460941023297\n",
      "Average test loss: 0.004665057494408554\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018074248275823063\n",
      "Average test loss: 0.004457352514068286\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018054997114671602\n",
      "Average test loss: 0.004483897388188375\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018035550124943258\n",
      "Average test loss: 0.004663820001400179\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018026708042456045\n",
      "Average test loss: 0.004566727294276158\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018005483670367136\n",
      "Average test loss: 0.004471113194608026\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018009623812304604\n",
      "Average test loss: 0.0045579885567228\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01798580473661423\n",
      "Average test loss: 0.00466689718556073\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017965879062811534\n",
      "Average test loss: 0.004521992792271905\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01795073982245392\n",
      "Average test loss: 0.00448163650992016\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01793247732354535\n",
      "Average test loss: 0.004593079018923972\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01792981868982315\n",
      "Average test loss: 0.004563283166744643\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017941811670859655\n",
      "Average test loss: 0.004545385241715444\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01792292075438632\n",
      "Average test loss: 0.00450569683396154\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01789430960516135\n",
      "Average test loss: 0.00454356873449352\n",
      "Epoch 160/300\n",
      "Average training loss: 0.017897250084413424\n",
      "Average test loss: 0.004628417160568966\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01786982327202956\n",
      "Average test loss: 0.004588433349712028\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01788548009097576\n",
      "Average test loss: 0.004544779245638185\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017848948700560464\n",
      "Average test loss: 0.004535075377258989\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017831309410432976\n",
      "Average test loss: 0.004828082009736035\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017806103052364457\n",
      "Average test loss: 0.0045193788603776035\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01779150795771016\n",
      "Average test loss: 0.004624217911106017\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01780086833652523\n",
      "Average test loss: 0.004809496630397108\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017779172273145783\n",
      "Average test loss: 0.004819920956260628\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01775832151124875\n",
      "Average test loss: 0.004509889918689927\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017765880501104725\n",
      "Average test loss: 0.004731957294874721\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017751983872718282\n",
      "Average test loss: 0.0045556756077955165\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01775855100320445\n",
      "Average test loss: 0.00453830354495181\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017726190442012416\n",
      "Average test loss: 0.004786429381618897\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017736951403319837\n",
      "Average test loss: 0.0050125371455732315\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017710705070859856\n",
      "Average test loss: 0.004502054859780603\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01769113688998752\n",
      "Average test loss: 0.004633245663096508\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017670964132580493\n",
      "Average test loss: 0.004658701063030296\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01768070205880536\n",
      "Average test loss: 0.004715018861616651\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01764972381790479\n",
      "Average test loss: 0.004586724069383409\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017667915259798366\n",
      "Average test loss: 0.004523845944553614\n",
      "Epoch 181/300\n",
      "Average training loss: 0.017649561994605593\n",
      "Average test loss: 0.004716011229281624\n",
      "Epoch 182/300\n",
      "Average training loss: 0.017619392479459445\n",
      "Average test loss: 0.004709824336899651\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01761391308406989\n",
      "Average test loss: 0.004512850504368544\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017595893662008973\n",
      "Average test loss: 0.004598150510340929\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017609436915980443\n",
      "Average test loss: 0.0045279597097800835\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01757653897172875\n",
      "Average test loss: 0.004625783160535826\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01755263578395049\n",
      "Average test loss: 0.004713026060619288\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017557964616351656\n",
      "Average test loss: 0.004569584362001882\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01754736231515805\n",
      "Average test loss: 0.004717417093200816\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01755688604629702\n",
      "Average test loss: 0.00458359876068102\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017538765375812847\n",
      "Average test loss: 0.004653763054145707\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017528889740506808\n",
      "Average test loss: 0.004540154976977242\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017520605580674278\n",
      "Average test loss: 0.004659521618651019\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01750085375044081\n",
      "Average test loss: 0.004634701249914037\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017495673274000485\n",
      "Average test loss: 0.004704393480180038\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017461244760288132\n",
      "Average test loss: 0.004531023163348437\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01746735958920585\n",
      "Average test loss: 0.004495453366595838\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017461399904555745\n",
      "Average test loss: 0.004621170776171817\n",
      "Epoch 199/300\n",
      "Average training loss: 0.017456461193660896\n",
      "Average test loss: 0.004748083218932152\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01746198545065191\n",
      "Average test loss: 0.0046400612981783015\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01744821067319976\n",
      "Average test loss: 0.004493854207711087\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017408263557487064\n",
      "Average test loss: 0.004672494544544154\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017427538628379504\n",
      "Average test loss: 0.004640292382902569\n",
      "Epoch 204/300\n",
      "Average training loss: 0.017416251416007677\n",
      "Average test loss: 0.0045858138305031595\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017399171030355823\n",
      "Average test loss: 0.00466635109235843\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01738432340323925\n",
      "Average test loss: 0.004657667729589674\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01737950189246072\n",
      "Average test loss: 0.00468672602582309\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017367661173144977\n",
      "Average test loss: 0.004639261535679301\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017374275705880588\n",
      "Average test loss: 0.004704289524712496\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017367280842529403\n",
      "Average test loss: 0.0047356414136787255\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017336900242500834\n",
      "Average test loss: 0.004806768369757467\n",
      "Epoch 212/300\n",
      "Average training loss: 0.017350292581650946\n",
      "Average test loss: 0.004942841280251741\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017333624886969726\n",
      "Average test loss: 0.004705192026785678\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01732536637203561\n",
      "Average test loss: 0.004564668321775066\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01730336061782307\n",
      "Average test loss: 0.004728124209990104\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017310437235567303\n",
      "Average test loss: 0.00491385279616548\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017305030860006808\n",
      "Average test loss: 0.004873724283857478\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01728394955231084\n",
      "Average test loss: 0.0048279899164206455\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01730256849527359\n",
      "Average test loss: 0.004731257014390495\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017276077140950495\n",
      "Average test loss: 0.004815202024247911\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01727756659520997\n",
      "Average test loss: 0.00452191360688044\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017260409274862873\n",
      "Average test loss: 0.004810476056817505\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017266446090406843\n",
      "Average test loss: 0.004629344969780909\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017249165295312802\n",
      "Average test loss: 0.004757573925372627\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017255499717262056\n",
      "Average test loss: 0.004624105959302849\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01722582127319442\n",
      "Average test loss: 0.0047372871757381495\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01721350988331768\n",
      "Average test loss: 0.004581357394862506\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017200205182863606\n",
      "Average test loss: 0.004669737172209554\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017220141576396095\n",
      "Average test loss: 0.004550433033042484\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017220170819924937\n",
      "Average test loss: 0.004609340024284191\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01718644157714314\n",
      "Average test loss: 0.004773521639820602\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01720260390639305\n",
      "Average test loss: 0.004593235878066884\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017193271559973557\n",
      "Average test loss: 0.004777345975033111\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017180939356486003\n",
      "Average test loss: 0.0047380445202191675\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01717837134665913\n",
      "Average test loss: 0.004590178192489677\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017134066197607253\n",
      "Average test loss: 0.004624011425094472\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01716175616118643\n",
      "Average test loss: 0.004690048180106613\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017167409723003707\n",
      "Average test loss: 0.004707097271250354\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017126596306761107\n",
      "Average test loss: 0.004628074969268508\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01712700551913844\n",
      "Average test loss: 0.0047457141540944575\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01710516987658209\n",
      "Average test loss: 0.0046057618823316365\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01710113065275881\n",
      "Average test loss: 0.0045992017640835705\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01710452554292149\n",
      "Average test loss: 0.004682751502841711\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017125299511684313\n",
      "Average test loss: 0.00450701720164054\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01709148760057158\n",
      "Average test loss: 0.004702070037523905\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017096685517165398\n",
      "Average test loss: 0.004797968708806568\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017084401374061902\n",
      "Average test loss: 0.004602800516618623\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01706920260687669\n",
      "Average test loss: 0.0046419178764853216\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017074247874319555\n",
      "Average test loss: 0.004682644246766964\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017051437796817885\n",
      "Average test loss: 0.004746500259472264\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017052738110224407\n",
      "Average test loss: 0.0046337065464920465\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017046683667434587\n",
      "Average test loss: 0.004841329292704662\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017037838059994908\n",
      "Average test loss: 0.004666028879996803\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01705613844924503\n",
      "Average test loss: 0.004642334939705001\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017025730891360176\n",
      "Average test loss: 0.004712160069081518\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01700441034966045\n",
      "Average test loss: 0.004844581988122728\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01700331599679258\n",
      "Average test loss: 0.0047779614633570115\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017021087200277382\n",
      "Average test loss: 0.004637580011246933\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017008662262724506\n",
      "Average test loss: 0.004961447270380126\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01700691041515933\n",
      "Average test loss: 0.004699989689721002\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016987509344187048\n",
      "Average test loss: 0.004749143202685648\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016981209450297886\n",
      "Average test loss: 0.004627061896440056\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016988975569605828\n",
      "Average test loss: 0.004924848920355241\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016969981142216258\n",
      "Average test loss: 0.004880530832128392\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016975506871110862\n",
      "Average test loss: 0.004949172713276413\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01695934664044115\n",
      "Average test loss: 0.004738765944209364\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016968242848085034\n",
      "Average test loss: 0.004657803835554255\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016961844745609495\n",
      "Average test loss: 0.00475087570440438\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016937515305976075\n",
      "Average test loss: 0.004771058107415835\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016944202135006588\n",
      "Average test loss: 0.004764023226168421\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01695236548781395\n",
      "Average test loss: 0.005420087486091588\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016949926920235156\n",
      "Average test loss: 0.004723868562322524\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01692076650261879\n",
      "Average test loss: 0.00489136914908886\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01691186191638311\n",
      "Average test loss: 0.004601543458799521\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01692224954565366\n",
      "Average test loss: 0.004803389459848404\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016897903756962884\n",
      "Average test loss: 0.0047264262123240365\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01688901705791553\n",
      "Average test loss: 0.004750683180159992\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016888920666442975\n",
      "Average test loss: 0.004608763163702356\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0168782261783878\n",
      "Average test loss: 0.005026719626453188\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016901190254423354\n",
      "Average test loss: 0.004619302291216122\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016877224618362056\n",
      "Average test loss: 0.0048136943818794355\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016874123685889773\n",
      "Average test loss: 0.004719984016484684\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01685288573635949\n",
      "Average test loss: 0.004709596640947792\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016840245202183725\n",
      "Average test loss: 0.004805008987999624\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01683931240439415\n",
      "Average test loss: 0.004581775129669242\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016843332447111607\n",
      "Average test loss: 0.004734904045859973\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01685154072691997\n",
      "Average test loss: 0.004845510375168589\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016838862424923315\n",
      "Average test loss: 0.004586147746071219\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016834603952864804\n",
      "Average test loss: 0.004713183994922373\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016822134994798236\n",
      "Average test loss: 0.004904780517849657\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016816160876717834\n",
      "Average test loss: 0.004754685553825564\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016806452237897448\n",
      "Average test loss: 0.004935567530079021\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016819264764587085\n",
      "Average test loss: 0.004882403876632452\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01680254102498293\n",
      "Average test loss: 0.004759358473949963\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016820009344153935\n",
      "Average test loss: 0.004701931879131331\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016784745190706517\n",
      "Average test loss: 0.00489287244528532\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016803042088945706\n",
      "Average test loss: 0.004619776109440459\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01677421122458246\n",
      "Average test loss: 0.004637984462496307\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01677466817200184\n",
      "Average test loss: 0.004793998370567957\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016772199392318724\n",
      "Average test loss: 0.004611716299214297\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05706427062220044\n",
      "Average test loss: 0.004343284166107575\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0212080936729908\n",
      "Average test loss: 0.004222611172745625\n",
      "Epoch 3/300\n",
      "Average training loss: 0.020041022294097478\n",
      "Average test loss: 0.004067126877605915\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019498272423942885\n",
      "Average test loss: 0.0038766639832821157\n",
      "Epoch 5/300\n",
      "Average training loss: 0.019141235904561148\n",
      "Average test loss: 0.003806774669016401\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018849561118417315\n",
      "Average test loss: 0.0037733289553887315\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01858734412242969\n",
      "Average test loss: 0.0037215448030167154\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018356295216414662\n",
      "Average test loss: 0.0036963833185533683\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018161916588743526\n",
      "Average test loss: 0.003660562532850438\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017976813900801872\n",
      "Average test loss: 0.0035812463756236764\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017782947736481824\n",
      "Average test loss: 0.0035518388936503064\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01760600767698553\n",
      "Average test loss: 0.003528467558324337\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01746174356341362\n",
      "Average test loss: 0.003479481618644463\n",
      "Epoch 14/300\n",
      "Average training loss: 0.017307847663760187\n",
      "Average test loss: 0.003436215625041061\n",
      "Epoch 15/300\n",
      "Average training loss: 0.017175726858278114\n",
      "Average test loss: 0.0034143043224596314\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017054061498079035\n",
      "Average test loss: 0.003406075562350452\n",
      "Epoch 17/300\n",
      "Average training loss: 0.016933249241775937\n",
      "Average test loss: 0.0033600333985976047\n",
      "Epoch 18/300\n",
      "Average training loss: 0.016845148677627247\n",
      "Average test loss: 0.0033813050352036952\n",
      "Epoch 19/300\n",
      "Average training loss: 0.016727738624645605\n",
      "Average test loss: 0.003371526131613387\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01663079783982701\n",
      "Average test loss: 0.0033265177398506137\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016539816852245065\n",
      "Average test loss: 0.003286624126550224\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016460378179119693\n",
      "Average test loss: 0.0032978424885206753\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016349810796479385\n",
      "Average test loss: 0.0032764841235346266\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016289505183696748\n",
      "Average test loss: 0.003266120134231945\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016204206560220983\n",
      "Average test loss: 0.0033164520185026856\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01614010049816635\n",
      "Average test loss: 0.0032411096839027272\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016081405431860023\n",
      "Average test loss: 0.0032649995887445078\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01600850000232458\n",
      "Average test loss: 0.0032754581350212296\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015927298670841587\n",
      "Average test loss: 0.0032240615791330734\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015883197132911947\n",
      "Average test loss: 0.0032300252879245415\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015819705665111542\n",
      "Average test loss: 0.0032159611457544895\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015760173327393\n",
      "Average test loss: 0.003230603723683291\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015703412800199455\n",
      "Average test loss: 0.003219847951291336\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015659779493179586\n",
      "Average test loss: 0.0032234626691788437\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015587487598260244\n",
      "Average test loss: 0.0032305322190125784\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015540078370107544\n",
      "Average test loss: 0.003233923690807488\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015481038000848558\n",
      "Average test loss: 0.003213435949343774\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015435407468014294\n",
      "Average test loss: 0.0032501735747274426\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01539283630086316\n",
      "Average test loss: 0.0032075362495250174\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01532086512280835\n",
      "Average test loss: 0.0031908650965326362\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015283399241666\n",
      "Average test loss: 0.0032033774707880287\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015246550210648113\n",
      "Average test loss: 0.0031829139126671687\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015189594961702824\n",
      "Average test loss: 0.003200630222550697\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015132889720300833\n",
      "Average test loss: 0.0031856048131982486\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015099570616251893\n",
      "Average test loss: 0.0032419601796815794\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015024590858154826\n",
      "Average test loss: 0.0031857113854752645\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015010968645413717\n",
      "Average test loss: 0.0032225089059728716\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014962910026311874\n",
      "Average test loss: 0.0033216142300516367\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014910209932261043\n",
      "Average test loss: 0.0032205621343519954\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014875579728020562\n",
      "Average test loss: 0.003236740480694506\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014819487710793814\n",
      "Average test loss: 0.0032006264585587715\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014778160165581438\n",
      "Average test loss: 0.003207705571833584\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014727064996129937\n",
      "Average test loss: 0.0032969318187485138\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014698260012600156\n",
      "Average test loss: 0.003280157935598658\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014646168407880598\n",
      "Average test loss: 0.0032828974738303156\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014646558399001757\n",
      "Average test loss: 0.003317818820476532\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014574796889391211\n",
      "Average test loss: 0.0032916228611850075\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01453402510782083\n",
      "Average test loss: 0.003227475930005312\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014472366055680646\n",
      "Average test loss: 0.0032820988916274574\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014459955249395636\n",
      "Average test loss: 0.003239966306835413\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014409884785612424\n",
      "Average test loss: 0.0032450773666302364\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014387112599280145\n",
      "Average test loss: 0.0032483138371672896\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014353093083533976\n",
      "Average test loss: 0.003318263160271777\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014314132208625475\n",
      "Average test loss: 0.003342216287015213\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014252843971881602\n",
      "Average test loss: 0.0032452574790351922\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014221593058771556\n",
      "Average test loss: 0.0033768965850273767\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014202332169645362\n",
      "Average test loss: 0.0033689523939457204\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01417615215231975\n",
      "Average test loss: 0.003278553023106522\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014124839657710659\n",
      "Average test loss: 0.0033435527444299726\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014111389762825437\n",
      "Average test loss: 0.00348420191100902\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01406169783572356\n",
      "Average test loss: 0.0032867765821930437\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0140416005825003\n",
      "Average test loss: 0.003325223262111346\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013998987192908922\n",
      "Average test loss: 0.0032738135933048193\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013967149613632096\n",
      "Average test loss: 0.0034457628093659876\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013938302734659777\n",
      "Average test loss: 0.0033539768794758453\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013902415682872137\n",
      "Average test loss: 0.0033184197524355518\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013872668913669057\n",
      "Average test loss: 0.003334034411650565\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013852808845539888\n",
      "Average test loss: 0.003253088793820805\n",
      "Epoch 79/300\n",
      "Average training loss: 0.013823225441492266\n",
      "Average test loss: 0.003354500159621239\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013792022209200594\n",
      "Average test loss: 0.0033629047920306523\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01375273254679309\n",
      "Average test loss: 0.0033674298121283454\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013723949063155386\n",
      "Average test loss: 0.0036466007816294828\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013709309740198983\n",
      "Average test loss: 0.0033069009406285154\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013669728622668319\n",
      "Average test loss: 0.0034007353333549368\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013642791868084007\n",
      "Average test loss: 0.0033793296735319827\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01362395883931054\n",
      "Average test loss: 0.0034051690981205967\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013602915473282337\n",
      "Average test loss: 0.0033346685046950976\n",
      "Epoch 88/300\n",
      "Average training loss: 0.013580221067700122\n",
      "Average test loss: 0.0036157158642179436\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01354760155826807\n",
      "Average test loss: 0.003476795898957385\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013521849152942498\n",
      "Average test loss: 0.003425281299485101\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013514253648618857\n",
      "Average test loss: 0.003312568489048216\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01348126168217924\n",
      "Average test loss: 0.003418442239777909\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013457887948387199\n",
      "Average test loss: 0.003504174500703812\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013428448408014245\n",
      "Average test loss: 0.003378549817742573\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013421303496592575\n",
      "Average test loss: 0.0033928406342036196\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013385669357246823\n",
      "Average test loss: 0.0033523278031498192\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013384378485381604\n",
      "Average test loss: 0.003332916292672356\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01334365597118934\n",
      "Average test loss: 0.003437841222724981\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013318316547407043\n",
      "Average test loss: 0.0034447738041894303\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013298759494390753\n",
      "Average test loss: 0.0034039813122815556\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013305066127743986\n",
      "Average test loss: 0.0035034388869793877\n",
      "Epoch 102/300\n",
      "Average training loss: 0.013254470657143328\n",
      "Average test loss: 0.003618428065441549\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01323492825196849\n",
      "Average test loss: 0.00333706830172903\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013229646917846468\n",
      "Average test loss: 0.0033955158723725214\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013205617592566544\n",
      "Average test loss: 0.0034513249279310306\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013183670521610314\n",
      "Average test loss: 0.0033496017600927086\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013171229230033027\n",
      "Average test loss: 0.003407302591949701\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013150272550682227\n",
      "Average test loss: 0.003413485595335563\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013142204129861461\n",
      "Average test loss: 0.0034618509397324588\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013111380723615488\n",
      "Average test loss: 0.003427645581463973\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013089490100741386\n",
      "Average test loss: 0.003605321060348716\n",
      "Epoch 112/300\n",
      "Average training loss: 0.013076594312985738\n",
      "Average test loss: 0.003405996455707484\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01306873249842061\n",
      "Average test loss: 0.0035715606411298117\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013037454014850987\n",
      "Average test loss: 0.0034095315107454858\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013028547537823518\n",
      "Average test loss: 0.0034670476147698032\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01302212166868978\n",
      "Average test loss: 0.003383344148182207\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012976153468920126\n",
      "Average test loss: 0.003434710004263454\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012964681209789383\n",
      "Average test loss: 0.0033636861855371133\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012951939707828892\n",
      "Average test loss: 0.0036255940780457524\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01296969067884816\n",
      "Average test loss: 0.0035083294701245098\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012925391008456548\n",
      "Average test loss: 0.0037224172136435907\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012923018691440424\n",
      "Average test loss: 0.003507719327799148\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012888392729891672\n",
      "Average test loss: 0.003746817994448874\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012873994039164649\n",
      "Average test loss: 0.003442698354108466\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012861592756377327\n",
      "Average test loss: 0.0034689834057870838\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01287303569416205\n",
      "Average test loss: 0.0033756421454664734\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01285980649292469\n",
      "Average test loss: 0.0035752007284512123\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01284241998112864\n",
      "Average test loss: 0.0035299355900949903\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012822587633298504\n",
      "Average test loss: 0.0036360146711683933\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0128214064521922\n",
      "Average test loss: 0.003496009966565503\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012799294751551416\n",
      "Average test loss: 0.0034744302179250453\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012779569961958462\n",
      "Average test loss: 0.003592798405016462\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012773832037217088\n",
      "Average test loss: 0.0037032529016335805\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01276410803405775\n",
      "Average test loss: 0.003519886689674523\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01273541472107172\n",
      "Average test loss: 0.0034686956169704595\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012728506701687972\n",
      "Average test loss: 0.0034187322710123326\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012711205095052719\n",
      "Average test loss: 0.003468527184178432\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012688229588998689\n",
      "Average test loss: 0.003736017118725512\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01268512418700589\n",
      "Average test loss: 0.0036568517796695234\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012672974754538801\n",
      "Average test loss: 0.0036801893882867365\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012663614926238855\n",
      "Average test loss: 0.0036321086200575034\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012669315554201603\n",
      "Average test loss: 0.003814694878541761\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012635564581387574\n",
      "Average test loss: 0.0035540385217302374\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012607709747221734\n",
      "Average test loss: 0.0034570635995931093\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012628753331800302\n",
      "Average test loss: 0.0036146042326258288\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012611515744692749\n",
      "Average test loss: 0.003525457858004504\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012597782824602392\n",
      "Average test loss: 0.003547587594224347\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012586707681417465\n",
      "Average test loss: 0.003562877220205135\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012585119214322833\n",
      "Average test loss: 0.0036623206461469334\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012579701358245479\n",
      "Average test loss: 0.003529883539511098\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012553693600826794\n",
      "Average test loss: 0.0035292635135766534\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012524762126306693\n",
      "Average test loss: 0.0035044488344962398\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012545135211613443\n",
      "Average test loss: 0.003563932282850146\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012529683974054125\n",
      "Average test loss: 0.003636043833775653\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012518345057136483\n",
      "Average test loss: 0.003504964710937606\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012526225114034282\n",
      "Average test loss: 0.0035695165418502357\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012509941399925286\n",
      "Average test loss: 0.0034506801219864026\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012476049282484584\n",
      "Average test loss: 0.0035378194902506137\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012471565674576494\n",
      "Average test loss: 0.003534530387053059\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0124679797258642\n",
      "Average test loss: 0.0036944349348131153\n",
      "Epoch 161/300\n",
      "Average training loss: 0.012451916108528773\n",
      "Average test loss: 0.0035528992445518575\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01243238181869189\n",
      "Average test loss: 0.0034748979268802538\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012436839231186443\n",
      "Average test loss: 0.0035888456327633723\n",
      "Epoch 164/300\n",
      "Average training loss: 0.012437514453298515\n",
      "Average test loss: 0.003604955542418692\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012414196324845155\n",
      "Average test loss: 0.003648405740658442\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012391383671098285\n",
      "Average test loss: 0.0036290510615540877\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01240961980405781\n",
      "Average test loss: 0.0035053624218950668\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012397464729845525\n",
      "Average test loss: 0.003524247978089584\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012383147339026132\n",
      "Average test loss: 0.003624114392325282\n",
      "Epoch 170/300\n",
      "Average training loss: 0.012362980518076156\n",
      "Average test loss: 0.003564679062614838\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012363441340625286\n",
      "Average test loss: 0.003568180365177492\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012359448233412371\n",
      "Average test loss: 0.003627733695217305\n",
      "Epoch 173/300\n",
      "Average training loss: 0.012350796952843666\n",
      "Average test loss: 0.003637556297083696\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012345467269420624\n",
      "Average test loss: 0.003560504781289233\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01234414295355479\n",
      "Average test loss: 0.0036247217546527583\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012317120572759046\n",
      "Average test loss: 0.0036507817829648653\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012329840468035804\n",
      "Average test loss: 0.0034960088671909437\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012304662033915519\n",
      "Average test loss: 0.003655107066449192\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012290320946110619\n",
      "Average test loss: 0.0035895235248737866\n",
      "Epoch 180/300\n",
      "Average training loss: 0.012295534015529686\n",
      "Average test loss: 0.003559519880554742\n",
      "Epoch 181/300\n",
      "Average training loss: 0.012278918080031872\n",
      "Average test loss: 0.003653379344691833\n",
      "Epoch 182/300\n",
      "Average training loss: 0.012276021521952417\n",
      "Average test loss: 0.003609978960826993\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01227781873030795\n",
      "Average test loss: 0.0038669074144628314\n",
      "Epoch 184/300\n",
      "Average training loss: 0.012282613020804193\n",
      "Average test loss: 0.0036647861026641395\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01224504260553254\n",
      "Average test loss: 0.003581934761049019\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012235931458572546\n",
      "Average test loss: 0.0036446843908892737\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012238178203503291\n",
      "Average test loss: 0.003486123203817341\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012225261548740997\n",
      "Average test loss: 0.003594276288110349\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012220035763250457\n",
      "Average test loss: 0.003619559690563215\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012201975209017595\n",
      "Average test loss: 0.0035925755972663564\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012205995767894719\n",
      "Average test loss: 0.003549566184480985\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012198826275765896\n",
      "Average test loss: 0.003614281249543031\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01219671095493767\n",
      "Average test loss: 0.0036825999745892153\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012181744085417853\n",
      "Average test loss: 0.003577469408719076\n",
      "Epoch 195/300\n",
      "Average training loss: 0.012180063775844045\n",
      "Average test loss: 0.0035540733778228364\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012168515810536013\n",
      "Average test loss: 0.0036238321492241487\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012161203078097767\n",
      "Average test loss: 0.003891735386931234\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012159906504054864\n",
      "Average test loss: 0.003731929976079199\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012168702628877428\n",
      "Average test loss: 0.0035890689726091095\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01212885929726892\n",
      "Average test loss: 0.003711597419447369\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012138604951401552\n",
      "Average test loss: 0.003636231403797865\n",
      "Epoch 202/300\n",
      "Average training loss: 0.012119931855135494\n",
      "Average test loss: 0.003600965174742871\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012128754721747504\n",
      "Average test loss: 0.003604329533254107\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012114540021452638\n",
      "Average test loss: 0.00364910992483298\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012112104503644838\n",
      "Average test loss: 0.0036937881956497828\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012124847106635571\n",
      "Average test loss: 0.003616761113620467\n",
      "Epoch 207/300\n",
      "Average training loss: 0.012109330629309018\n",
      "Average test loss: 0.003568190748699837\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012088228452536796\n",
      "Average test loss: 0.0037466809056285353\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01208351385427846\n",
      "Average test loss: 0.0035815327314452994\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012082428619265556\n",
      "Average test loss: 0.0037342159936411515\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012071974087092612\n",
      "Average test loss: 0.0034705409624924263\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012066118569009835\n",
      "Average test loss: 0.003636516343181332\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012071919173830086\n",
      "Average test loss: 0.0035865892368472283\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012063260079258018\n",
      "Average test loss: 0.0036581946603125998\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012048509349425633\n",
      "Average test loss: 0.003794001228072577\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012059272571570344\n",
      "Average test loss: 0.003616084670441018\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012044799323711131\n",
      "Average test loss: 0.0036337749937342275\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012040233500301837\n",
      "Average test loss: 0.003747177840107017\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012028976722723909\n",
      "Average test loss: 0.003617565103289154\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012019167789982426\n",
      "Average test loss: 0.003523518577011095\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01202574791179763\n",
      "Average test loss: 0.0035151056452757783\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01200226192590263\n",
      "Average test loss: 0.003676160880054037\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01197957693040371\n",
      "Average test loss: 0.003637669910987218\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011992825891408656\n",
      "Average test loss: 0.0036307777894867793\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011994931194517347\n",
      "Average test loss: 0.0036361282374709845\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011994097211294704\n",
      "Average test loss: 0.003876745713667737\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011977269570860597\n",
      "Average test loss: 0.0035393033435361255\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011989574793312285\n",
      "Average test loss: 0.0035611982426295677\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011994393681486448\n",
      "Average test loss: 0.0036192971598356964\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011960210004614459\n",
      "Average test loss: 0.0035680965344525046\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01195490907629331\n",
      "Average test loss: 0.003707211828480164\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011953388487299283\n",
      "Average test loss: 0.003604483306614889\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0119580059821407\n",
      "Average test loss: 0.0037373347203764652\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011942707815931902\n",
      "Average test loss: 0.003567777678370476\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011938736702005069\n",
      "Average test loss: 0.0036022777871953116\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011952342256903649\n",
      "Average test loss: 0.0037905178734411794\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011925754591822624\n",
      "Average test loss: 0.00373364547909134\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01192804005742073\n",
      "Average test loss: 0.0038655181982451014\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011922288070950244\n",
      "Average test loss: 0.0035361843191915087\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011919168517821365\n",
      "Average test loss: 0.0036190501960615316\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011922304920024341\n",
      "Average test loss: 0.0037460016902122234\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011917824938893318\n",
      "Average test loss: 0.0036600668968425855\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011906355244417985\n",
      "Average test loss: 0.0036797146242525842\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011902928267916044\n",
      "Average test loss: 0.003930124395216505\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011908184324701627\n",
      "Average test loss: 0.003617885143392616\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01190068523089091\n",
      "Average test loss: 0.003545109534222219\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011859793584379885\n",
      "Average test loss: 0.0037395385263694655\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011866360868016878\n",
      "Average test loss: 0.0037604079999857478\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011857442456814977\n",
      "Average test loss: 0.0036974757090210914\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011879623355136978\n",
      "Average test loss: 0.003831193427865704\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011879882787664732\n",
      "Average test loss: 0.0036419828728669217\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011867010722557704\n",
      "Average test loss: 0.003688757268918885\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011852707504398293\n",
      "Average test loss: 0.003741619712155726\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011845269637803236\n",
      "Average test loss: 0.0036993620590203336\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011840307045314047\n",
      "Average test loss: 0.0036756676592760618\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011857881272004711\n",
      "Average test loss: 0.003607977747089333\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011824394909044108\n",
      "Average test loss: 0.003733994484361675\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011829723986486594\n",
      "Average test loss: 0.003651873654375474\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011827398642069763\n",
      "Average test loss: 0.0037360730485783682\n",
      "Epoch 260/300\n",
      "Average training loss: 0.011816353242430423\n",
      "Average test loss: 0.003780933633239733\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011815596359057559\n",
      "Average test loss: 0.003822378479772144\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011803517449233266\n",
      "Average test loss: 0.0037018702005346617\n",
      "Epoch 263/300\n",
      "Average training loss: 0.011809970099892882\n",
      "Average test loss: 0.003579737280805906\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011792122081749968\n",
      "Average test loss: 0.003737409448458089\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0118152718635069\n",
      "Average test loss: 0.0036954769370042616\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011803097720775339\n",
      "Average test loss: 0.003544534350021018\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011811883827878369\n",
      "Average test loss: 0.003690209535881877\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011800147102110916\n",
      "Average test loss: 0.003658166787483626\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011780039311283165\n",
      "Average test loss: 0.0037174006351787183\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01177418490830395\n",
      "Average test loss: 0.0036428831617037454\n",
      "Epoch 271/300\n",
      "Average training loss: 0.011776631842884753\n",
      "Average test loss: 0.0037444166232728296\n",
      "Epoch 272/300\n",
      "Average training loss: 0.011787384701271851\n",
      "Average test loss: 0.0037782505895528526\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0117667946194609\n",
      "Average test loss: 0.003768058240413666\n",
      "Epoch 274/300\n",
      "Average training loss: 0.011747718968325192\n",
      "Average test loss: 0.003611550028125445\n",
      "Epoch 275/300\n",
      "Average training loss: 0.011771501893798511\n",
      "Average test loss: 0.0036882659902589187\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01174762504051129\n",
      "Average test loss: 0.003895296047752102\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0117366379491157\n",
      "Average test loss: 0.0037319310429609486\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011750228990283277\n",
      "Average test loss: 0.003744915411497156\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011753179225656721\n",
      "Average test loss: 0.0037002535331994296\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01173685476432244\n",
      "Average test loss: 0.0036211031863672867\n",
      "Epoch 281/300\n",
      "Average training loss: 0.011744292897482713\n",
      "Average test loss: 0.0037212899648067024\n",
      "Epoch 282/300\n",
      "Average training loss: 0.011736487401442395\n",
      "Average test loss: 0.0037260242385996714\n",
      "Epoch 283/300\n",
      "Average training loss: 0.011716731174952455\n",
      "Average test loss: 0.003758808025883304\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01171559134622415\n",
      "Average test loss: 0.0037167774418161974\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011726455354028277\n",
      "Average test loss: 0.0036765344058059986\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01170638684845633\n",
      "Average test loss: 0.003654485911130905\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011710510573453374\n",
      "Average test loss: 0.003803780711359448\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01169232150581148\n",
      "Average test loss: 0.0035950556461595826\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011685047679477268\n",
      "Average test loss: 0.003974857598957088\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011702256676223544\n",
      "Average test loss: 0.004026271858149104\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01170555837535196\n",
      "Average test loss: 0.0036612917383511863\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011677893572383457\n",
      "Average test loss: 0.003841348434281018\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011696260202262136\n",
      "Average test loss: 0.0035905727054923774\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011699057382014063\n",
      "Average test loss: 0.0037166887428611516\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011685722697112296\n",
      "Average test loss: 0.003949076390928692\n",
      "Epoch 296/300\n",
      "Average training loss: 0.011691532555553647\n",
      "Average test loss: 0.003706334459905823\n",
      "Epoch 297/300\n",
      "Average training loss: 0.011662646840016047\n",
      "Average test loss: 0.0037233021089600193\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011678566503855917\n",
      "Average test loss: 0.003789009953331616\n",
      "Epoch 299/300\n",
      "Average training loss: 0.011675767031808695\n",
      "Average test loss: 0.0036603811596416765\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011657441010077794\n",
      "Average test loss: 0.0036205265749659804\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.055356066036555504\n",
      "Average test loss: 0.00391717686297165\n",
      "Epoch 2/300\n",
      "Average training loss: 0.019072258891330823\n",
      "Average test loss: 0.003974253558243314\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017841454169816442\n",
      "Average test loss: 0.0034739473342067667\n",
      "Epoch 4/300\n",
      "Average training loss: 0.017238570213317872\n",
      "Average test loss: 0.003365879961806867\n",
      "Epoch 5/300\n",
      "Average training loss: 0.016819479778409006\n",
      "Average test loss: 0.003331022020843294\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016455234439008766\n",
      "Average test loss: 0.003219097782133354\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01612493646144867\n",
      "Average test loss: 0.0031561516024586228\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015820980398191346\n",
      "Average test loss: 0.003092861701423923\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01555050755209393\n",
      "Average test loss: 0.003120666202571657\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015295693564746115\n",
      "Average test loss: 0.003028890810906887\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015090037108709415\n",
      "Average test loss: 0.003022132350752751\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014861079084376495\n",
      "Average test loss: 0.002924423051170177\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014696231978634993\n",
      "Average test loss: 0.00285992342647579\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014518036280241278\n",
      "Average test loss: 0.002835939705785778\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014362466829518477\n",
      "Average test loss: 0.0029987854241496988\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014234652488595909\n",
      "Average test loss: 0.0027811104394495486\n",
      "Epoch 17/300\n",
      "Average training loss: 0.014086546183460289\n",
      "Average test loss: 0.0027470147791836\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013965073079698615\n",
      "Average test loss: 0.002712547866006692\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013853377552496063\n",
      "Average test loss: 0.002715985446754429\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013732015029423767\n",
      "Average test loss: 0.0026675328988995817\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013640814934339789\n",
      "Average test loss: 0.002745654748338792\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013533780104584165\n",
      "Average test loss: 0.0026642153881904153\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013461373154487874\n",
      "Average test loss: 0.002637067564452688\n",
      "Epoch 24/300\n",
      "Average training loss: 0.013359642911288473\n",
      "Average test loss: 0.002681491325298945\n",
      "Epoch 25/300\n",
      "Average training loss: 0.013298634149134158\n",
      "Average test loss: 0.0026528603132400246\n",
      "Epoch 26/300\n",
      "Average training loss: 0.013195273667573929\n",
      "Average test loss: 0.002662956497942408\n",
      "Epoch 27/300\n",
      "Average training loss: 0.013143596179783345\n",
      "Average test loss: 0.0026077641681250597\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013084904293219249\n",
      "Average test loss: 0.0025804786838384136\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013010336197084851\n",
      "Average test loss: 0.002672752955928445\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01294777207242118\n",
      "Average test loss: 0.0025862734669612514\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0128700749228398\n",
      "Average test loss: 0.0025700083517779908\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012820498677591483\n",
      "Average test loss: 0.002683743030660682\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012754203706979751\n",
      "Average test loss: 0.0025658019671423566\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012716248191065259\n",
      "Average test loss: 0.0025783958341926336\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012654012725998958\n",
      "Average test loss: 0.0026170594367302125\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012611879707624516\n",
      "Average test loss: 0.0026145227872249152\n",
      "Epoch 37/300\n",
      "Average training loss: 0.012541431712607543\n",
      "Average test loss: 0.0025680131525215176\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012505709392329057\n",
      "Average test loss: 0.0025614282565398347\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012457539036870002\n",
      "Average test loss: 0.002581587515047027\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012404428809881211\n",
      "Average test loss: 0.0025424407933735183\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01234362002958854\n",
      "Average test loss: 0.0025800611734804178\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012296048003766272\n",
      "Average test loss: 0.0025709341754102046\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012264515861868858\n",
      "Average test loss: 0.002550495931878686\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01221200694806046\n",
      "Average test loss: 0.002620456973090768\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012159414054205021\n",
      "Average test loss: 0.0026626684262106817\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01212406513757176\n",
      "Average test loss: 0.0025588002788523834\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012068904625044929\n",
      "Average test loss: 0.0025735974562250906\n",
      "Epoch 48/300\n",
      "Average training loss: 0.012039988020228015\n",
      "Average test loss: 0.002536320804514819\n",
      "Epoch 49/300\n",
      "Average training loss: 0.012009457398619916\n",
      "Average test loss: 0.002601373508469098\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011965251507858436\n",
      "Average test loss: 0.002666657359339297\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011890998281538487\n",
      "Average test loss: 0.002561082923474411\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011874083754916985\n",
      "Average test loss: 0.002691568840502037\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011832791162033876\n",
      "Average test loss: 0.002563862849854761\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011789761050707764\n",
      "Average test loss: 0.002568760216443075\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011770725918312868\n",
      "Average test loss: 0.0026068668506211705\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011717299964692857\n",
      "Average test loss: 0.0025787843945953583\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011690776689598958\n",
      "Average test loss: 0.0026839912872140604\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011644049719803862\n",
      "Average test loss: 0.00259751916769892\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011624191289146742\n",
      "Average test loss: 0.002571413835303651\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011593905579712656\n",
      "Average test loss: 0.0026638683333165118\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01155043600872159\n",
      "Average test loss: 0.002618055773898959\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011504497770633962\n",
      "Average test loss: 0.0026802137340936395\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011494137975076835\n",
      "Average test loss: 0.0026023046352590122\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01143331971930133\n",
      "Average test loss: 0.0025985098804036776\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011411235887557268\n",
      "Average test loss: 0.0026795264223797455\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011388303072916137\n",
      "Average test loss: 0.002614358350220654\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01136207150419553\n",
      "Average test loss: 0.0025903346909003123\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011304764855239126\n",
      "Average test loss: 0.002589133616743816\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011309039948715105\n",
      "Average test loss: 0.002593473098758194\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01126318976117505\n",
      "Average test loss: 0.002624185430506865\n",
      "Epoch 71/300\n",
      "Average training loss: 0.011229435838758945\n",
      "Average test loss: 0.0026552741010155943\n",
      "Epoch 72/300\n",
      "Average training loss: 0.011220725115802552\n",
      "Average test loss: 0.0026705015076117383\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011205896522435877\n",
      "Average test loss: 0.0027597582232620982\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011153356476790376\n",
      "Average test loss: 0.0026325487150914138\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011123246190448603\n",
      "Average test loss: 0.002627359219516317\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011096537804438008\n",
      "Average test loss: 0.0026495200025124684\n",
      "Epoch 77/300\n",
      "Average training loss: 0.011085301166607274\n",
      "Average test loss: 0.002632150850776169\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011053601423899332\n",
      "Average test loss: 0.0026099766890207926\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011027773906787237\n",
      "Average test loss: 0.002692463160182039\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01100665501008431\n",
      "Average test loss: 0.002667691955135928\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010973342509733306\n",
      "Average test loss: 0.0026472929856843417\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010963255737390784\n",
      "Average test loss: 0.0026819113238404193\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01093086014356878\n",
      "Average test loss: 0.0027602673148115478\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010926740584274133\n",
      "Average test loss: 0.0027075515963758032\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010882752418518066\n",
      "Average test loss: 0.0027067199384586678\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010869608198602995\n",
      "Average test loss: 0.002838809826514787\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010838019880983565\n",
      "Average test loss: 0.0027330950390961435\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010813177495366998\n",
      "Average test loss: 0.002644245299614138\n",
      "Epoch 89/300\n",
      "Average training loss: 0.010801547674669159\n",
      "Average test loss: 0.002814690289604995\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010797562451826202\n",
      "Average test loss: 0.002651314516241352\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010759123071200317\n",
      "Average test loss: 0.002769556171985136\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010744264740910794\n",
      "Average test loss: 0.0027285983053346474\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010747870090521043\n",
      "Average test loss: 0.0028410588232800366\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010700401825209458\n",
      "Average test loss: 0.0026953431246802212\n",
      "Epoch 95/300\n",
      "Average training loss: 0.010689850526551406\n",
      "Average test loss: 0.0027216924843895766\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010668240879972776\n",
      "Average test loss: 0.0026952091186410853\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010652481365534995\n",
      "Average test loss: 0.002737801821074552\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010626066378421254\n",
      "Average test loss: 0.0027571786135021184\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010618760394553343\n",
      "Average test loss: 0.002738547511605753\n",
      "Epoch 100/300\n",
      "Average training loss: 0.010601815632647938\n",
      "Average test loss: 0.002682308650886019\n",
      "Epoch 101/300\n",
      "Average training loss: 0.010581292814678615\n",
      "Average test loss: 0.0028021195906524857\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010553886575003465\n",
      "Average test loss: 0.0028806828413572576\n",
      "Epoch 103/300\n",
      "Average training loss: 0.010559604222575824\n",
      "Average test loss: 0.0027025490445602273\n",
      "Epoch 104/300\n",
      "Average training loss: 0.010527830384671689\n",
      "Average test loss: 0.0026851697050862843\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0105266621808211\n",
      "Average test loss: 0.0027495722007006406\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01049735579556889\n",
      "Average test loss: 0.002750679273572233\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010479590978887347\n",
      "Average test loss: 0.002772207228673829\n",
      "Epoch 108/300\n",
      "Average training loss: 0.010466786850657729\n",
      "Average test loss: 0.0027960714250802993\n",
      "Epoch 109/300\n",
      "Average training loss: 0.010468356928063763\n",
      "Average test loss: 0.0027289486103173758\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01043814568552706\n",
      "Average test loss: 0.002721642212735282\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0104190039055215\n",
      "Average test loss: 0.002689962403848767\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01041646318965488\n",
      "Average test loss: 0.002731381464542614\n",
      "Epoch 113/300\n",
      "Average training loss: 0.010393924326532416\n",
      "Average test loss: 0.002801912265519301\n",
      "Epoch 114/300\n",
      "Average training loss: 0.010366415288713243\n",
      "Average test loss: 0.0028258209412710534\n",
      "Epoch 115/300\n",
      "Average training loss: 0.010360260752340159\n",
      "Average test loss: 0.0028896967212979994\n",
      "Epoch 116/300\n",
      "Average training loss: 0.010344788071182038\n",
      "Average test loss: 0.0029209351367834543\n",
      "Epoch 117/300\n",
      "Average training loss: 0.010329508581095272\n",
      "Average test loss: 0.002759332367322511\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010311316665675904\n",
      "Average test loss: 0.002807138024001486\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010324118427932263\n",
      "Average test loss: 0.0027554655451741484\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01029342494573858\n",
      "Average test loss: 0.002922218973024024\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01029220313247707\n",
      "Average test loss: 0.002811517471033666\n",
      "Epoch 122/300\n",
      "Average training loss: 0.010274778975794712\n",
      "Average test loss: 0.002846102198999789\n",
      "Epoch 123/300\n",
      "Average training loss: 0.010255300876994928\n",
      "Average test loss: 0.002858074666000903\n",
      "Epoch 124/300\n",
      "Average training loss: 0.010258640336493652\n",
      "Average test loss: 0.0027101368804772694\n",
      "Epoch 125/300\n",
      "Average training loss: 0.010236231529050403\n",
      "Average test loss: 0.0027018971243459315\n",
      "Epoch 126/300\n",
      "Average training loss: 0.010213380677832498\n",
      "Average test loss: 0.0026681237305617995\n",
      "Epoch 127/300\n",
      "Average training loss: 0.010213807643287711\n",
      "Average test loss: 0.00282518244927956\n",
      "Epoch 128/300\n",
      "Average training loss: 0.010203776920835177\n",
      "Average test loss: 0.0028824732206347917\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01017803278979328\n",
      "Average test loss: 0.002828748964394132\n",
      "Epoch 130/300\n",
      "Average training loss: 0.010183655796365605\n",
      "Average test loss: 0.002904594412073493\n",
      "Epoch 131/300\n",
      "Average training loss: 0.010164883462091287\n",
      "Average test loss: 0.002925722364750173\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01015205324606763\n",
      "Average test loss: 0.0027815073320849072\n",
      "Epoch 133/300\n",
      "Average training loss: 0.010121434160404735\n",
      "Average test loss: 0.0027897174399760033\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01012656726853715\n",
      "Average test loss: 0.002821987997740507\n",
      "Epoch 135/300\n",
      "Average training loss: 0.010130497564044263\n",
      "Average test loss: 0.0028531888412932555\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01013598352836238\n",
      "Average test loss: 0.002883468043886953\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010091090168390009\n",
      "Average test loss: 0.0028077987989203797\n",
      "Epoch 138/300\n",
      "Average training loss: 0.010091282949679427\n",
      "Average test loss: 0.0028322693212992616\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010085542098515563\n",
      "Average test loss: 0.0028171624472985665\n",
      "Epoch 140/300\n",
      "Average training loss: 0.010072603544427289\n",
      "Average test loss: 0.002864168362898959\n",
      "Epoch 141/300\n",
      "Average training loss: 0.010047068857484394\n",
      "Average test loss: 0.0027523920283549362\n",
      "Epoch 142/300\n",
      "Average training loss: 0.010050117751790417\n",
      "Average test loss: 0.0028284170979426967\n",
      "Epoch 143/300\n",
      "Average training loss: 0.010048482382463085\n",
      "Average test loss: 0.002835154640591807\n",
      "Epoch 144/300\n",
      "Average training loss: 0.010047088594900238\n",
      "Average test loss: 0.002806834580583705\n",
      "Epoch 145/300\n",
      "Average training loss: 0.010036524744497404\n",
      "Average test loss: 0.002799048342845506\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01000731512821383\n",
      "Average test loss: 0.002926472858008411\n",
      "Epoch 147/300\n",
      "Average training loss: 0.010013734041816658\n",
      "Average test loss: 0.0027447292192114723\n",
      "Epoch 148/300\n",
      "Average training loss: 0.010004036213788722\n",
      "Average test loss: 0.0028062007373405827\n",
      "Epoch 149/300\n",
      "Average training loss: 0.009992824147972796\n",
      "Average test loss: 0.0028429502027316226\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009975594451030096\n",
      "Average test loss: 0.0027887647332002718\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009981203039487203\n",
      "Average test loss: 0.0027747597804086074\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00996409255431758\n",
      "Average test loss: 0.002806991180611981\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009958555344906118\n",
      "Average test loss: 0.0029767178781330587\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009961820736527443\n",
      "Average test loss: 0.0029999525795380276\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009937672047979303\n",
      "Average test loss: 0.002805686084346639\n",
      "Epoch 156/300\n",
      "Average training loss: 0.009926950876911481\n",
      "Average test loss: 0.0028235591983215677\n",
      "Epoch 157/300\n",
      "Average training loss: 0.00990591890282101\n",
      "Average test loss: 0.0027915352812657754\n",
      "Epoch 158/300\n",
      "Average training loss: 0.009914232677883572\n",
      "Average test loss: 0.0028370618371085987\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009905871966646777\n",
      "Average test loss: 0.0028022801712569265\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009894566557059685\n",
      "Average test loss: 0.003113600086627735\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009910118316610653\n",
      "Average test loss: 0.0028840588492651783\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009888510045905907\n",
      "Average test loss: 0.0027880111647148927\n",
      "Epoch 163/300\n",
      "Average training loss: 0.009880717992368672\n",
      "Average test loss: 0.0029025546322680182\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00987050855325328\n",
      "Average test loss: 0.002846182552476724\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009852981453140577\n",
      "Average test loss: 0.002802714149778088\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009866354504393207\n",
      "Average test loss: 0.0028662912224729858\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009845340932408969\n",
      "Average test loss: 0.002904885551271339\n",
      "Epoch 168/300\n",
      "Average training loss: 0.009829997843338384\n",
      "Average test loss: 0.002838251141624318\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009849108788702224\n",
      "Average test loss: 0.002847017929578821\n",
      "Epoch 170/300\n",
      "Average training loss: 0.009831376096440686\n",
      "Average test loss: 0.003050569098028872\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009808071691956785\n",
      "Average test loss: 0.0027796634231797525\n",
      "Epoch 172/300\n",
      "Average training loss: 0.009803746236281262\n",
      "Average test loss: 0.002873601065741645\n",
      "Epoch 173/300\n",
      "Average training loss: 0.009804855165382226\n",
      "Average test loss: 0.002837071542110708\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009799313893748654\n",
      "Average test loss: 0.0028179905462182232\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009777498690618409\n",
      "Average test loss: 0.002937660628929734\n",
      "Epoch 176/300\n",
      "Average training loss: 0.009781805412636863\n",
      "Average test loss: 0.0028482536654919387\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009765873801791006\n",
      "Average test loss: 0.0028394315385570127\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00976377019037803\n",
      "Average test loss: 0.0028832346190594965\n",
      "Epoch 179/300\n",
      "Average training loss: 0.009778672603269417\n",
      "Average test loss: 0.0028745964057743548\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009753784192105134\n",
      "Average test loss: 0.0028474454581737516\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00974282064040502\n",
      "Average test loss: 0.003037491095976697\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009743940050403277\n",
      "Average test loss: 0.00288922770611114\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009731343078944419\n",
      "Average test loss: 0.0027950642769121463\n",
      "Epoch 184/300\n",
      "Average training loss: 0.009723121679491468\n",
      "Average test loss: 0.002931604197766218\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009725429670678245\n",
      "Average test loss: 0.0028669211311886707\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009699934296309947\n",
      "Average test loss: 0.0027482709526601766\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009705596466031339\n",
      "Average test loss: 0.0028643587432387804\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009698286212152905\n",
      "Average test loss: 0.002926290003789796\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009696089213920964\n",
      "Average test loss: 0.0028930388188196554\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009705508097178406\n",
      "Average test loss: 0.0028644664887752798\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009683834968341722\n",
      "Average test loss: 0.002944916734057996\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009677519230378998\n",
      "Average test loss: 0.0029803904477092953\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009673367949823538\n",
      "Average test loss: 0.002826744838721222\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00966135354919566\n",
      "Average test loss: 0.0029395918117629156\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009662985857990053\n",
      "Average test loss: 0.002807434326244725\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009653424008025064\n",
      "Average test loss: 0.002939821630095442\n",
      "Epoch 197/300\n",
      "Average training loss: 0.009624949237538708\n",
      "Average test loss: 0.0029304958308736485\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00964468319631285\n",
      "Average test loss: 0.002887104087612695\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009648964398437076\n",
      "Average test loss: 0.00300001686397526\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009622386804057493\n",
      "Average test loss: 0.0028703497977306445\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009617495175451041\n",
      "Average test loss: 0.0029770628000713056\n",
      "Epoch 202/300\n",
      "Average training loss: 0.009625242425335778\n",
      "Average test loss: 0.002954194080291523\n",
      "Epoch 203/300\n",
      "Average training loss: 0.009623871877789498\n",
      "Average test loss: 0.0028467266098078756\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009623816572957569\n",
      "Average test loss: 0.00284900983567867\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009620276935398579\n",
      "Average test loss: 0.002852312673504154\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009601025816467074\n",
      "Average test loss: 0.0028386966522990003\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009604155414634281\n",
      "Average test loss: 0.0028802998753057587\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009601051009363598\n",
      "Average test loss: 0.002869016931909654\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009579614534146255\n",
      "Average test loss: 0.0029485102211021714\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009596866188777817\n",
      "Average test loss: 0.0028045720199329984\n",
      "Epoch 211/300\n",
      "Average training loss: 0.009563197691407468\n",
      "Average test loss: 0.0029278307747509745\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009573074703415235\n",
      "Average test loss: 0.0028737674287209907\n",
      "Epoch 213/300\n",
      "Average training loss: 0.009566010443700684\n",
      "Average test loss: 0.00286923772489859\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00956093397281236\n",
      "Average test loss: 0.0029901070983873474\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009543149581385984\n",
      "Average test loss: 0.002820842649166783\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009562156765825218\n",
      "Average test loss: 0.002835125714747442\n",
      "Epoch 217/300\n",
      "Average training loss: 0.00957555519623889\n",
      "Average test loss: 0.0029065475184470416\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00954884160641167\n",
      "Average test loss: 0.002945430451176233\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009529541704389783\n",
      "Average test loss: 0.0029844074046446217\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009530287188788254\n",
      "Average test loss: 0.0028283685563753047\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009537889551785257\n",
      "Average test loss: 0.007564378860923979\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009566680957873662\n",
      "Average test loss: 0.0030275755722282663\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009523764255146186\n",
      "Average test loss: 0.0028582205660641195\n",
      "Epoch 224/300\n",
      "Average training loss: 0.009525813102722169\n",
      "Average test loss: 0.002892350076387326\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009502275231811736\n",
      "Average test loss: 0.002917858706166347\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009507455387877093\n",
      "Average test loss: 0.002939210960848464\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009497045487165451\n",
      "Average test loss: 0.002863810742791328\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009506844052424034\n",
      "Average test loss: 0.0028885572722388638\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009504123556117216\n",
      "Average test loss: 0.00294588388564686\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00950257971800036\n",
      "Average test loss: 0.0028572480633027022\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009488598253991868\n",
      "Average test loss: 0.002950327955186367\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009471330907609727\n",
      "Average test loss: 0.002844266417539782\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009485643915004201\n",
      "Average test loss: 0.0029106834252468415\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009461927107638783\n",
      "Average test loss: 0.0029555725378708707\n",
      "Epoch 235/300\n",
      "Average training loss: 0.009473513320916229\n",
      "Average test loss: 0.0028450353987928894\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009461144532594416\n",
      "Average test loss: 0.0029520674691432052\n",
      "Epoch 237/300\n",
      "Average training loss: 0.009466559141874314\n",
      "Average test loss: 0.0029872739097724357\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009444932048519452\n",
      "Average test loss: 0.0029613485683997473\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009458833337657981\n",
      "Average test loss: 0.00298193141611086\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009446404258824056\n",
      "Average test loss: 0.002960374048911035\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009436532851722506\n",
      "Average test loss: 0.0029311034282048544\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009445924195564455\n",
      "Average test loss: 0.0029771120297826\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009428181839485962\n",
      "Average test loss: 0.002910854989041885\n",
      "Epoch 244/300\n",
      "Average training loss: 0.009422834167877833\n",
      "Average test loss: 0.002980094493470258\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009430863695840041\n",
      "Average test loss: 0.0029730176688689326\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009447114226718744\n",
      "Average test loss: 0.002885528534857763\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009414266636802091\n",
      "Average test loss: 0.0029681754648271533\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009399157736036513\n",
      "Average test loss: 0.00292085133989652\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009404828291800288\n",
      "Average test loss: 0.002870312562212348\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009419031388229793\n",
      "Average test loss: 0.002918924417967598\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009399347318543328\n",
      "Average test loss: 0.0028221594020724295\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009400816601597601\n",
      "Average test loss: 0.0029688615736862024\n",
      "Epoch 253/300\n",
      "Average training loss: 0.009382465740872753\n",
      "Average test loss: 0.002921755984115104\n",
      "Epoch 254/300\n",
      "Average training loss: 0.009402926584912671\n",
      "Average test loss: 0.0029604839492175313\n",
      "Epoch 255/300\n",
      "Average training loss: 0.00939690251979563\n",
      "Average test loss: 0.0028954462892272406\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00939018588513136\n",
      "Average test loss: 0.0028958352831088834\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009387810881353088\n",
      "Average test loss: 0.0028854388849188883\n",
      "Epoch 258/300\n",
      "Average training loss: 0.009390911671022574\n",
      "Average test loss: 0.0029104169375366636\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009362577069964673\n",
      "Average test loss: 0.002925474965530965\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009372403405606747\n",
      "Average test loss: 0.0029334606542769405\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009374000877141953\n",
      "Average test loss: 0.0029993234098785455\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009368004950384298\n",
      "Average test loss: 0.0028843283208294045\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009358573561741246\n",
      "Average test loss: 0.0029425768572837116\n",
      "Epoch 264/300\n",
      "Average training loss: 0.009366298578679561\n",
      "Average test loss: 0.0029767137692413397\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00935144571463267\n",
      "Average test loss: 0.002948105004719562\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00936318666405148\n",
      "Average test loss: 0.0028580484164671765\n",
      "Epoch 267/300\n",
      "Average training loss: 0.009343635631932152\n",
      "Average test loss: 0.0029926529724357857\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00934610413842731\n",
      "Average test loss: 0.0028964204601943495\n",
      "Epoch 269/300\n",
      "Average training loss: 0.009351883668866422\n",
      "Average test loss: 0.0029934694934636353\n",
      "Epoch 270/300\n",
      "Average training loss: 0.009345879128409756\n",
      "Average test loss: 0.0028987926272675394\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009330344679454962\n",
      "Average test loss: 0.0029344973299238416\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009338234492888054\n",
      "Average test loss: 0.0029605269096791742\n",
      "Epoch 273/300\n",
      "Average training loss: 0.009327064473595884\n",
      "Average test loss: 0.0028517097971505587\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009321432209677166\n",
      "Average test loss: 0.002957463886588812\n",
      "Epoch 275/300\n",
      "Average training loss: 0.009342021689232852\n",
      "Average test loss: 0.0029056474543693992\n",
      "Epoch 276/300\n",
      "Average training loss: 0.009315600897702905\n",
      "Average test loss: 0.0030060826340276335\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009315504028565354\n",
      "Average test loss: 0.002948895152244303\n",
      "Epoch 278/300\n",
      "Average training loss: 0.00933308846089575\n",
      "Average test loss: 0.003078444149138199\n",
      "Epoch 279/300\n",
      "Average training loss: 0.009320864835960998\n",
      "Average test loss: 0.002955611938610673\n",
      "Epoch 280/300\n",
      "Average training loss: 0.00931011825054884\n",
      "Average test loss: 0.002922996791700522\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009308862852553527\n",
      "Average test loss: 0.0029877682166794934\n",
      "Epoch 282/300\n",
      "Average training loss: 0.009310009671582116\n",
      "Average test loss: 0.0031234785409437285\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009305373101598687\n",
      "Average test loss: 0.003061563226704796\n",
      "Epoch 284/300\n",
      "Average training loss: 0.00930327190210422\n",
      "Average test loss: 0.002864001028136247\n",
      "Epoch 285/300\n",
      "Average training loss: 0.009286803795231714\n",
      "Average test loss: 0.0029744223114103077\n",
      "Epoch 286/300\n",
      "Average training loss: 0.009310826606220669\n",
      "Average test loss: 0.0028897663940572076\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009285034221907457\n",
      "Average test loss: 0.002899580398677952\n",
      "Epoch 288/300\n",
      "Average training loss: 0.009278993199268977\n",
      "Average test loss: 0.003089391712927156\n",
      "Epoch 289/300\n",
      "Average training loss: 0.009293985236022208\n",
      "Average test loss: 0.003063038228286637\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00928334004845884\n",
      "Average test loss: 0.0029537195476392903\n",
      "Epoch 291/300\n",
      "Average training loss: 0.009286210517916414\n",
      "Average test loss: 0.0030088137932535674\n",
      "Epoch 292/300\n",
      "Average training loss: 0.009251334971851773\n",
      "Average test loss: 0.0029185059000220565\n",
      "Epoch 293/300\n",
      "Average training loss: 0.009270012812895908\n",
      "Average test loss: 0.0029528765436261893\n",
      "Epoch 294/300\n",
      "Average training loss: 0.009263618213435014\n",
      "Average test loss: 0.0029819626762635177\n",
      "Epoch 295/300\n",
      "Average training loss: 0.009262253483550417\n",
      "Average test loss: 0.0030803335237627226\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0092720453499092\n",
      "Average test loss: 0.0030058166105300186\n",
      "Epoch 297/300\n",
      "Average training loss: 0.009271681285566755\n",
      "Average test loss: 0.002945219509717491\n",
      "Epoch 298/300\n",
      "Average training loss: 0.009252244121498531\n",
      "Average test loss: 0.002931512442210482\n",
      "Epoch 299/300\n",
      "Average training loss: 0.009249476898875501\n",
      "Average test loss: 0.0028771275484727487\n",
      "Epoch 300/300\n",
      "Average training loss: 0.009240840794311629\n",
      "Average test loss: 0.002897951170595156\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04958456864953041\n",
      "Average test loss: 0.0035097634030712976\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015692924202316338\n",
      "Average test loss: 0.003037506941912903\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014572847563359472\n",
      "Average test loss: 0.0029761807858530017\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013961495734751224\n",
      "Average test loss: 0.002815321089803345\n",
      "Epoch 5/300\n",
      "Average training loss: 0.013484813540346093\n",
      "Average test loss: 0.002592311172849602\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013073453705757857\n",
      "Average test loss: 0.0025134761466955144\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012711635447210736\n",
      "Average test loss: 0.0024157148585137395\n",
      "Epoch 8/300\n",
      "Average training loss: 0.012381916678614087\n",
      "Average test loss: 0.002355989562968413\n",
      "Epoch 9/300\n",
      "Average training loss: 0.012094717366000016\n",
      "Average test loss: 0.0023008350736151138\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011841161449750264\n",
      "Average test loss: 0.002235615137964487\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011593838164789809\n",
      "Average test loss: 0.002207736417858137\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011375812747412258\n",
      "Average test loss: 0.002224514132158624\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011192735268837876\n",
      "Average test loss: 0.0021052640937268734\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01101787719130516\n",
      "Average test loss: 0.002098303828595413\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010869419379366769\n",
      "Average test loss: 0.002062120521441102\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010720640045073298\n",
      "Average test loss: 0.002006471804343164\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01060118666291237\n",
      "Average test loss: 0.0019952822511291337\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010485295905007257\n",
      "Average test loss: 0.0019718978016947707\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010381385814812448\n",
      "Average test loss: 0.001971922877762053\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010286964753435717\n",
      "Average test loss: 0.0019102166761747665\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010199345567160182\n",
      "Average test loss: 0.0019302813618754346\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010110950846225023\n",
      "Average test loss: 0.0019102840806461043\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01002813375948204\n",
      "Average test loss: 0.0018794647966408067\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009969738784763548\n",
      "Average test loss: 0.0019482259129484495\n",
      "Epoch 25/300\n",
      "Average training loss: 0.009895169379810493\n",
      "Average test loss: 0.0018987049490420355\n",
      "Epoch 26/300\n",
      "Average training loss: 0.00981221778028541\n",
      "Average test loss: 0.0018856743747989336\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009763737126357025\n",
      "Average test loss: 0.0018394829889552462\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0097097393543356\n",
      "Average test loss: 0.001915539050888684\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00964288179576397\n",
      "Average test loss: 0.0018558042536800106\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009608508901463614\n",
      "Average test loss: 0.0018490568055874771\n",
      "Epoch 31/300\n",
      "Average training loss: 0.009545244405666987\n",
      "Average test loss: 0.0018183935443974203\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009495048634707928\n",
      "Average test loss: 0.0018499737617870171\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009448419375965992\n",
      "Average test loss: 0.0018643228895962238\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009409179633690251\n",
      "Average test loss: 0.001826942645220293\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009360624201595783\n",
      "Average test loss: 0.0018391964323818683\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009314938954181142\n",
      "Average test loss: 0.00180163447248439\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009277427026795017\n",
      "Average test loss: 0.0018450294573687845\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009242916169265905\n",
      "Average test loss: 0.0018224113062024116\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009193008068535064\n",
      "Average test loss: 0.0018211266255627075\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009156837658749686\n",
      "Average test loss: 0.0018104319334444072\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009117540682355563\n",
      "Average test loss: 0.001783332453109324\n",
      "Epoch 42/300\n",
      "Average training loss: 0.00907653429524766\n",
      "Average test loss: 0.001856591574744218\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009041849671138658\n",
      "Average test loss: 0.0017935456308639712\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009011821630100409\n",
      "Average test loss: 0.0017910960295961962\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00898811281638013\n",
      "Average test loss: 0.0017966742412083678\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00894708660369118\n",
      "Average test loss: 0.0018301562163978815\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008914131678640842\n",
      "Average test loss: 0.001808709170617577\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008877671086539824\n",
      "Average test loss: 0.0017856792606827285\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008840298141042391\n",
      "Average test loss: 0.0018089282454715834\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008821921734346283\n",
      "Average test loss: 0.0017839872883632778\n",
      "Epoch 51/300\n",
      "Average training loss: 0.00877816103067663\n",
      "Average test loss: 0.0017872884208336472\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00875717819068167\n",
      "Average test loss: 0.0017909257307441699\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00872032419178221\n",
      "Average test loss: 0.001782465990115371\n",
      "Epoch 54/300\n",
      "Average training loss: 0.008695126756197876\n",
      "Average test loss: 0.001793749063793156\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008671655821717448\n",
      "Average test loss: 0.0018711057581628362\n",
      "Epoch 56/300\n",
      "Average training loss: 0.008654079447189968\n",
      "Average test loss: 0.0018420299939397309\n",
      "Epoch 57/300\n",
      "Average training loss: 0.008631747554573748\n",
      "Average test loss: 0.0018193518672552373\n",
      "Epoch 58/300\n",
      "Average training loss: 0.008583460409194232\n",
      "Average test loss: 0.0018261359117718206\n",
      "Epoch 59/300\n",
      "Average training loss: 0.008559497515360514\n",
      "Average test loss: 0.0018956819447792238\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008540774529592858\n",
      "Average test loss: 0.0018885062715659538\n",
      "Epoch 61/300\n",
      "Average training loss: 0.008501255745689074\n",
      "Average test loss: 0.0018071047145252427\n",
      "Epoch 62/300\n",
      "Average training loss: 0.008498027711278862\n",
      "Average test loss: 0.001946213249117136\n",
      "Epoch 63/300\n",
      "Average training loss: 0.008453838081823454\n",
      "Average test loss: 0.0018203947201578152\n",
      "Epoch 64/300\n",
      "Average training loss: 0.00842819527991944\n",
      "Average test loss: 0.0018287047336084975\n",
      "Epoch 65/300\n",
      "Average training loss: 0.008405727976726162\n",
      "Average test loss: 0.0018578605725326472\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00838861239494549\n",
      "Average test loss: 0.0018104375121701095\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008371326547943883\n",
      "Average test loss: 0.0018701879820889897\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008353664447863897\n",
      "Average test loss: 0.001881172866250078\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008327480636950995\n",
      "Average test loss: 0.0018261572188801236\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008300668237937821\n",
      "Average test loss: 0.001861901000762979\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008283890037073029\n",
      "Average test loss: 0.0017968496424663398\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008250792258315617\n",
      "Average test loss: 0.0018617324664567908\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008241249302195178\n",
      "Average test loss: 0.0018284476812308033\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00822790056384272\n",
      "Average test loss: 0.0019435448546169533\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008199713695380422\n",
      "Average test loss: 0.0018768385745998885\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008180131276862488\n",
      "Average test loss: 0.001851505394714574\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008164908644639783\n",
      "Average test loss: 0.0018833365176493923\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008148397316535314\n",
      "Average test loss: 0.0018911169945365852\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008119474462750886\n",
      "Average test loss: 0.001832812446066075\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008104076633436812\n",
      "Average test loss: 0.001891901040656699\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008088114691691265\n",
      "Average test loss: 0.0018859017082593506\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008058607227686379\n",
      "Average test loss: 0.0018367280053595701\n",
      "Epoch 83/300\n",
      "Average training loss: 0.00806023702522119\n",
      "Average test loss: 0.0018716500209023555\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008028543760379156\n",
      "Average test loss: 0.001893812666543656\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008017049844894144\n",
      "Average test loss: 0.0018849302985601956\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00800408336189058\n",
      "Average test loss: 0.001887744068271584\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007977976883037223\n",
      "Average test loss: 0.0019089844292029738\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007972201398263375\n",
      "Average test loss: 0.0019191296895345051\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007975526659439007\n",
      "Average test loss: 0.002066187338075704\n",
      "Epoch 90/300\n",
      "Average training loss: 0.00793075592070818\n",
      "Average test loss: 0.0018691869042813778\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007915420584380627\n",
      "Average test loss: 0.001908343943249848\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007910246994760301\n",
      "Average test loss: 0.0018873301416428552\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007891832717590861\n",
      "Average test loss: 0.0019108923075513706\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007865076544384161\n",
      "Average test loss: 0.0018868008038649957\n",
      "Epoch 95/300\n",
      "Average training loss: 0.00786474384388162\n",
      "Average test loss: 0.0019408466248876518\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00783795789298084\n",
      "Average test loss: 0.0019569808887317775\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007840942583150334\n",
      "Average test loss: 0.0020867305795351666\n",
      "Epoch 98/300\n",
      "Average training loss: 0.007829004433833891\n",
      "Average test loss: 0.0019392616705348094\n",
      "Epoch 99/300\n",
      "Average training loss: 0.00780098059028387\n",
      "Average test loss: 0.002030457701844474\n",
      "Epoch 100/300\n",
      "Average training loss: 0.007778794857362906\n",
      "Average test loss: 0.0018808483030233118\n",
      "Epoch 101/300\n",
      "Average training loss: 0.007788674931973219\n",
      "Average test loss: 0.0018881384941438833\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007765755602882968\n",
      "Average test loss: 0.001945322990624441\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00775331973657012\n",
      "Average test loss: 0.0019193910322048598\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007749903830389182\n",
      "Average test loss: 0.0019443662028966679\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0077311712673140895\n",
      "Average test loss: 0.0019010188738918968\n",
      "Epoch 106/300\n",
      "Average training loss: 0.007720483792738782\n",
      "Average test loss: 0.0019507443526138861\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007710168732123242\n",
      "Average test loss: 0.0019031065578262011\n",
      "Epoch 108/300\n",
      "Average training loss: 0.007698319596962796\n",
      "Average test loss: 0.0019341704903377427\n",
      "Epoch 109/300\n",
      "Average training loss: 0.007688192424674829\n",
      "Average test loss: 0.0019216109758449925\n",
      "Epoch 110/300\n",
      "Average training loss: 0.007663939331968625\n",
      "Average test loss: 0.0019120497434503502\n",
      "Epoch 111/300\n",
      "Average training loss: 0.007666557097600566\n",
      "Average test loss: 0.0021474752662082514\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007652746849175956\n",
      "Average test loss: 0.002027723419893947\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0076413635363181435\n",
      "Average test loss: 0.001891871697993742\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007624463334265683\n",
      "Average test loss: 0.0019049163810494873\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00761953409264485\n",
      "Average test loss: 0.0019234901623179515\n",
      "Epoch 116/300\n",
      "Average training loss: 0.007612269828716914\n",
      "Average test loss: 0.0020398481324728993\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0075960610132250524\n",
      "Average test loss: 0.0020088403780634204\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007600685065405236\n",
      "Average test loss: 0.00198793353450795\n",
      "Epoch 119/300\n",
      "Average training loss: 0.007587798493189944\n",
      "Average test loss: 0.0020888981111347677\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0075623625372019075\n",
      "Average test loss: 0.0018914879076182842\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007564438735445341\n",
      "Average test loss: 0.0019481951518812114\n",
      "Epoch 122/300\n",
      "Average training loss: 0.007549010808683104\n",
      "Average test loss: 0.0019317859170130557\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007538853839039803\n",
      "Average test loss: 0.001954691853477723\n",
      "Epoch 124/300\n",
      "Average training loss: 0.00753204432171252\n",
      "Average test loss: 0.0019023573884947432\n",
      "Epoch 125/300\n",
      "Average training loss: 0.007525646047045787\n",
      "Average test loss: 0.0019590688724484707\n",
      "Epoch 126/300\n",
      "Average training loss: 0.007504403207865026\n",
      "Average test loss: 0.0018905423431553774\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007510027825832367\n",
      "Average test loss: 0.001957660305417246\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007488893768025769\n",
      "Average test loss: 0.0019509781489355697\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007487888682219717\n",
      "Average test loss: 0.0019767903602785535\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007463312146564325\n",
      "Average test loss: 0.0020254000406712295\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007458705051905579\n",
      "Average test loss: 0.0020056459894403815\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007480332004941172\n",
      "Average test loss: 0.0019442036434387168\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007449169682131873\n",
      "Average test loss: 0.0019097177820901075\n",
      "Epoch 134/300\n",
      "Average training loss: 0.007439108199543423\n",
      "Average test loss: 0.002033109563713272\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0074386337962415484\n",
      "Average test loss: 0.0019981340006407764\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007423568456123272\n",
      "Average test loss: 0.0020220482082416616\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007415192893395821\n",
      "Average test loss: 0.0019446279582464032\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007408800467020935\n",
      "Average test loss: 0.0020482181693530746\n",
      "Epoch 139/300\n",
      "Average training loss: 0.007403817677249511\n",
      "Average test loss: 0.002014617880495886\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007393614862528112\n",
      "Average test loss: 0.002001344851942526\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0073845904171466826\n",
      "Average test loss: 0.0020492238617605633\n",
      "Epoch 142/300\n",
      "Average training loss: 0.007387057571361462\n",
      "Average test loss: 0.001976644467562437\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007367159562806288\n",
      "Average test loss: 0.0019232801964713467\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007362157773640421\n",
      "Average test loss: 0.0019931303068167635\n",
      "Epoch 145/300\n",
      "Average training loss: 0.007349969364702702\n",
      "Average test loss: 0.0019219458582293656\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007351557441469696\n",
      "Average test loss: 0.0020734476277397738\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007343707400063674\n",
      "Average test loss: 0.001930525085578362\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007344425464255943\n",
      "Average test loss: 0.0019495390049285358\n",
      "Epoch 149/300\n",
      "Average training loss: 0.007324865132156346\n",
      "Average test loss: 0.0019673960066090026\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0073133294193281064\n",
      "Average test loss: 0.001967616034257743\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007311079625454214\n",
      "Average test loss: 0.0019582666712295678\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0073094944047431155\n",
      "Average test loss: 0.0019270711399407851\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007302190235919423\n",
      "Average test loss: 0.0019304035677471095\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007302339416825109\n",
      "Average test loss: 0.0019595865320621267\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007288433814214335\n",
      "Average test loss: 0.002024317294255727\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0072919423712624444\n",
      "Average test loss: 0.002030724035472506\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007280822381791141\n",
      "Average test loss: 0.001977960109917654\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007267048889564144\n",
      "Average test loss: 0.0020789241082966327\n",
      "Epoch 159/300\n",
      "Average training loss: 0.007270651767237319\n",
      "Average test loss: 0.0020111710618560515\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007256394593252076\n",
      "Average test loss: 0.0020001357096350856\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0072489361407028306\n",
      "Average test loss: 0.001983602259722021\n",
      "Epoch 162/300\n",
      "Average training loss: 0.007251659988115231\n",
      "Average test loss: 0.001961103421428965\n",
      "Epoch 163/300\n",
      "Average training loss: 0.007232797296096881\n",
      "Average test loss: 0.0019287482984364033\n",
      "Epoch 164/300\n",
      "Average training loss: 0.007243835503856341\n",
      "Average test loss: 0.0020111363288015127\n",
      "Epoch 165/300\n",
      "Average training loss: 0.00722043776139617\n",
      "Average test loss: 0.0020365434187567897\n",
      "Epoch 166/300\n",
      "Average training loss: 0.007236055127448506\n",
      "Average test loss: 0.0020343518938041394\n",
      "Epoch 167/300\n",
      "Average training loss: 0.007226446332203017\n",
      "Average test loss: 0.0019202775423311524\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0072195226583215925\n",
      "Average test loss: 0.0021333844051178957\n",
      "Epoch 169/300\n",
      "Average training loss: 0.007225246162464221\n",
      "Average test loss: 0.0019323162361979485\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007222141062633859\n",
      "Average test loss: 0.0019862021766602995\n",
      "Epoch 171/300\n",
      "Average training loss: 0.007183360193338659\n",
      "Average test loss: 0.0019507701761192747\n",
      "Epoch 172/300\n",
      "Average training loss: 0.007190856775889794\n",
      "Average test loss: 0.002095265702654918\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007187068240924014\n",
      "Average test loss: 0.002037433603985442\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0071809942204919125\n",
      "Average test loss: 0.002010478307596511\n",
      "Epoch 175/300\n",
      "Average training loss: 0.007165160874939627\n",
      "Average test loss: 0.0019948367095655867\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0071813804196814695\n",
      "Average test loss: 0.00195578898034162\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007172079547825787\n",
      "Average test loss: 0.0019831650188813607\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007155676236583127\n",
      "Average test loss: 0.0020169069402747684\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007160081658512354\n",
      "Average test loss: 0.0020426818397310045\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007155817715244161\n",
      "Average test loss: 0.00202432425159754\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007161480844020843\n",
      "Average test loss: 0.0020596579160127373\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007135808064291874\n",
      "Average test loss: 0.0020007123299356964\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0071400366127490995\n",
      "Average test loss: 0.001965981699112389\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0071396824912064605\n",
      "Average test loss: 0.00198308994103637\n",
      "Epoch 185/300\n",
      "Average training loss: 0.007116715728822682\n",
      "Average test loss: 0.0020280151460319758\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007132663157665068\n",
      "Average test loss: 0.0020332477630840406\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007124259064180983\n",
      "Average test loss: 0.0019874111065227127\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007115843475692802\n",
      "Average test loss: 0.0019104267718373902\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007106183785117335\n",
      "Average test loss: 0.001987112623328964\n",
      "Epoch 190/300\n",
      "Average training loss: 0.007107856742623779\n",
      "Average test loss: 0.0021453276984393596\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007098976074821419\n",
      "Average test loss: 0.0020563196767535473\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007094047015739811\n",
      "Average test loss: 0.002041869105357263\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007097436676422755\n",
      "Average test loss: 0.002006627273849315\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00707891888750924\n",
      "Average test loss: 0.0020768743482314877\n",
      "Epoch 195/300\n",
      "Average training loss: 0.007079751999013954\n",
      "Average test loss: 0.0020870353166634837\n",
      "Epoch 196/300\n",
      "Average training loss: 0.007077982905838225\n",
      "Average test loss: 0.002014831041192843\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0070759085975587365\n",
      "Average test loss: 0.002040908355576297\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007059918030682537\n",
      "Average test loss: 0.002036555601283908\n",
      "Epoch 199/300\n",
      "Average training loss: 0.007055163520077864\n",
      "Average test loss: 0.0021446317873067325\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007058534983545542\n",
      "Average test loss: 0.0021703459210693837\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007052810721099377\n",
      "Average test loss: 0.0020178049666186174\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007045862775709894\n",
      "Average test loss: 0.001978195424191654\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007051440722826454\n",
      "Average test loss: 0.0020166400990759333\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007041649454169803\n",
      "Average test loss: 0.0021109603281236357\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0070284250208901035\n",
      "Average test loss: 0.0020738061639583773\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007042941682454612\n",
      "Average test loss: 0.0020475105264534553\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0070427383047839\n",
      "Average test loss: 0.002077734655390183\n",
      "Epoch 208/300\n",
      "Average training loss: 0.007026542297254006\n",
      "Average test loss: 0.0021070444881916045\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007035169996735122\n",
      "Average test loss: 0.0020262410141941574\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007020843494269583\n",
      "Average test loss: 0.0020312712914827796\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007024071094476514\n",
      "Average test loss: 0.0022075249167780083\n",
      "Epoch 212/300\n",
      "Average training loss: 0.007010756559669972\n",
      "Average test loss: 0.0020504737821304135\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0070154713346726365\n",
      "Average test loss: 0.0020286248623289996\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007017880774620506\n",
      "Average test loss: 0.0020410964735266234\n",
      "Epoch 215/300\n",
      "Average training loss: 0.006999929532822635\n",
      "Average test loss: 0.0020559736771716013\n",
      "Epoch 216/300\n",
      "Average training loss: 0.006998160351067781\n",
      "Average test loss: 0.0020477093137386774\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0070070283408794135\n",
      "Average test loss: 0.002041554393764171\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006997750877092282\n",
      "Average test loss: 0.0019937524444734057\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0069855905696749685\n",
      "Average test loss: 0.0020393760290203823\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0069880696071518795\n",
      "Average test loss: 0.002085129276745849\n",
      "Epoch 221/300\n",
      "Average training loss: 0.006988556650363737\n",
      "Average test loss: 0.002033518964010808\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006977586955659919\n",
      "Average test loss: 0.00203503033084174\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006975611455738545\n",
      "Average test loss: 0.001993413798097107\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006984593687372075\n",
      "Average test loss: 0.0020906425593420863\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0069677086319360464\n",
      "Average test loss: 0.0020432682389186487\n",
      "Epoch 226/300\n",
      "Average training loss: 0.00696694131133457\n",
      "Average test loss: 0.0021043816208839417\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006966210979968309\n",
      "Average test loss: 0.002066328942361805\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006956202175468207\n",
      "Average test loss: 0.0019918207521032955\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006970192031727896\n",
      "Average test loss: 0.0019216377870697114\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006950993890977568\n",
      "Average test loss: 0.0020320995355852774\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006953281371543805\n",
      "Average test loss: 0.002043814331293106\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006942185318718354\n",
      "Average test loss: 0.0021196259862432875\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00695370807664262\n",
      "Average test loss: 0.0020434740270591444\n",
      "Epoch 234/300\n",
      "Average training loss: 0.006942514930748277\n",
      "Average test loss: 0.002092139059884681\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006947031094796128\n",
      "Average test loss: 0.0020516998999648623\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0069371576135357224\n",
      "Average test loss: 0.002118746239898933\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006932559499310122\n",
      "Average test loss: 0.001987897879237102\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00692032157174415\n",
      "Average test loss: 0.0020347801147856644\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0069220096725556585\n",
      "Average test loss: 0.002041342342375881\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0069234592517217\n",
      "Average test loss: 0.002028751082718372\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006917841265185012\n",
      "Average test loss: 0.002116105961199436\n",
      "Epoch 242/300\n",
      "Average training loss: 0.006914770179738601\n",
      "Average test loss: 0.0021282955228040614\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006906567648881011\n",
      "Average test loss: 0.00206079099399762\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0069093246873882085\n",
      "Average test loss: 0.002058413453400135\n",
      "Epoch 245/300\n",
      "Average training loss: 0.006909335517221027\n",
      "Average test loss: 0.0020894798905485208\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006917179876317581\n",
      "Average test loss: 0.002062980169016454\n",
      "Epoch 247/300\n",
      "Average training loss: 0.006893766389538845\n",
      "Average test loss: 0.0020828914280152985\n",
      "Epoch 248/300\n",
      "Average training loss: 0.006903584262563122\n",
      "Average test loss: 0.0020342641012329192\n",
      "Epoch 249/300\n",
      "Average training loss: 0.006896962014337381\n",
      "Average test loss: 0.001996298327110708\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0068984195176098086\n",
      "Average test loss: 0.0020593656334612106\n",
      "Epoch 251/300\n",
      "Average training loss: 0.006893951044728359\n",
      "Average test loss: 0.0020190533368537825\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0068885839204821326\n",
      "Average test loss: 0.00214811807198243\n",
      "Epoch 253/300\n",
      "Average training loss: 0.006873146416412459\n",
      "Average test loss: 0.002028283946423067\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00689055423438549\n",
      "Average test loss: 0.002076333642523322\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0068910979098743864\n",
      "Average test loss: 0.0020204671726872523\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006869259561929438\n",
      "Average test loss: 0.0020528547006348767\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0068739329282608294\n",
      "Average test loss: 0.0020893537611183194\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006870502072903845\n",
      "Average test loss: 0.0020996775798913505\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006869069321701924\n",
      "Average test loss: 0.0021010936200618743\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00685850544191069\n",
      "Average test loss: 0.0021034582236574757\n",
      "Epoch 261/300\n",
      "Average training loss: 0.006860367846157816\n",
      "Average test loss: 0.0020855729488862885\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006866669903198878\n",
      "Average test loss: 0.0020647598461558422\n",
      "Epoch 263/300\n",
      "Average training loss: 0.006867769052584966\n",
      "Average test loss: 0.002015521439827151\n",
      "Epoch 264/300\n",
      "Average training loss: 0.006854827912317382\n",
      "Average test loss: 0.002101874258576168\n",
      "Epoch 265/300\n",
      "Average training loss: 0.006849576707929373\n",
      "Average test loss: 0.002127423066749341\n",
      "Epoch 266/300\n",
      "Average training loss: 0.006842418212857511\n",
      "Average test loss: 0.0020574151349978314\n",
      "Epoch 267/300\n",
      "Average training loss: 0.006850952269302474\n",
      "Average test loss: 0.0021317017841049365\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00684397775390082\n",
      "Average test loss: 0.0020077655389904975\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006835551649745968\n",
      "Average test loss: 0.002077553033414814\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006830713811433978\n",
      "Average test loss: 0.0020710008325469163\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006833725908978118\n",
      "Average test loss: 0.002057282303666903\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006850065880351596\n",
      "Average test loss: 0.0021096131712612176\n",
      "Epoch 273/300\n",
      "Average training loss: 0.006835836820718315\n",
      "Average test loss: 0.0020037825148449175\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006825985920098093\n",
      "Average test loss: 0.0021349924330910045\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006824771888554096\n",
      "Average test loss: 0.0020716871844811574\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006825987997154395\n",
      "Average test loss: 0.0022773657960610256\n",
      "Epoch 277/300\n",
      "Average training loss: 0.006823674313724041\n",
      "Average test loss: 0.002117946058925655\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006820409798373779\n",
      "Average test loss: 0.0020432184872010515\n",
      "Epoch 279/300\n",
      "Average training loss: 0.006818912148061726\n",
      "Average test loss: 0.0020252261608839036\n",
      "Epoch 280/300\n",
      "Average training loss: 0.006820809831221898\n",
      "Average test loss: 0.0022329401827106874\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006811341250108348\n",
      "Average test loss: 0.002300820969666044\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006828652924961514\n",
      "Average test loss: 0.0021514531459866297\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006807118910882208\n",
      "Average test loss: 0.0021305967786659796\n",
      "Epoch 284/300\n",
      "Average training loss: 0.006805541415595346\n",
      "Average test loss: 0.0020909655051719813\n",
      "Epoch 285/300\n",
      "Average training loss: 0.006800268287046088\n",
      "Average test loss: 0.0020235097511774962\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0068031488979856175\n",
      "Average test loss: 0.0020991447957025636\n",
      "Epoch 287/300\n",
      "Average training loss: 0.006805929771727986\n",
      "Average test loss: 0.0021541562674360143\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006798182813657655\n",
      "Average test loss: 0.0020948042544639773\n",
      "Epoch 289/300\n",
      "Average training loss: 0.006786143246624205\n",
      "Average test loss: 0.0020454034712165596\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00679337706665198\n",
      "Average test loss: 0.0020749538058622017\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0067958290932907\n",
      "Average test loss: 0.0019990327406881584\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006782351955771446\n",
      "Average test loss: 0.0020974048131869897\n",
      "Epoch 293/300\n",
      "Average training loss: 0.006804302825075057\n",
      "Average test loss: 0.002066598457801673\n",
      "Epoch 294/300\n",
      "Average training loss: 0.006783940017223358\n",
      "Average test loss: 0.0020752457811807593\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006785058533979787\n",
      "Average test loss: 0.0020796663131978778\n",
      "Epoch 296/300\n",
      "Average training loss: 0.006776023896618022\n",
      "Average test loss: 0.002097621058838235\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006783461540523503\n",
      "Average test loss: 0.0021255562564151155\n",
      "Epoch 298/300\n",
      "Average training loss: 0.006784150923705763\n",
      "Average test loss: 0.002063534445543256\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006769415537930197\n",
      "Average test loss: 0.0022345748467163906\n",
      "Epoch 300/300\n",
      "Average training loss: 0.006769055006404717\n",
      "Average test loss: 0.0021465821993640727\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_32_Depth5/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.36\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.83\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.1330272437466515\n",
      "Average test loss: 0.006804341605140103\n",
      "Epoch 2/300\n",
      "Average training loss: 0.282161224020852\n",
      "Average test loss: 0.005253058234436644\n",
      "Epoch 3/300\n",
      "Average training loss: 0.17864352610376147\n",
      "Average test loss: 0.00479353008295099\n",
      "Epoch 4/300\n",
      "Average training loss: 0.136078941391574\n",
      "Average test loss: 0.0047023117937561535\n",
      "Epoch 5/300\n",
      "Average training loss: 0.11347179792324702\n",
      "Average test loss: 0.004673058655526903\n",
      "Epoch 6/300\n",
      "Average training loss: 0.10054848439163631\n",
      "Average test loss: 0.004607108553250631\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0915685053202841\n",
      "Average test loss: 0.004573599487129185\n",
      "Epoch 8/300\n",
      "Average training loss: 0.08530502916706934\n",
      "Average test loss: 0.004589980493817064\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08076136020157072\n",
      "Average test loss: 0.004503343797599276\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07731829083628125\n",
      "Average test loss: 0.004472379236585564\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07450551848279105\n",
      "Average test loss: 0.004517703520548012\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0723587768541442\n",
      "Average test loss: 0.004478709028619859\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07080118945903248\n",
      "Average test loss: 0.004448655359033081\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06963847411672275\n",
      "Average test loss: 0.004416575155324406\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06867472794983122\n",
      "Average test loss: 0.00443004102839364\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0678913764986727\n",
      "Average test loss: 0.0043808789967248836\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06724424865510728\n",
      "Average test loss: 0.004437615832520856\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06676003071996901\n",
      "Average test loss: 0.004424042372239961\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06627590215537284\n",
      "Average test loss: 0.004336951723115312\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06588013917869992\n",
      "Average test loss: 0.004363847260673841\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06553390757408407\n",
      "Average test loss: 0.0043583698587285145\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06514812102582719\n",
      "Average test loss: 0.004307199261875616\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0649109015001191\n",
      "Average test loss: 0.004303400874137878\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06465635664264362\n",
      "Average test loss: 0.004303786089436876\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06440385181705156\n",
      "Average test loss: 0.00428697108104825\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06420869109365675\n",
      "Average test loss: 0.004271339778891868\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0639895260334015\n",
      "Average test loss: 0.004286844969623619\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06380124155349201\n",
      "Average test loss: 0.004264702387568023\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06361039636863602\n",
      "Average test loss: 0.004252912610769272\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0634450066321426\n",
      "Average test loss: 0.004262365378646387\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0632306575510237\n",
      "Average test loss: 0.004238956314408117\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06309865512781673\n",
      "Average test loss: 0.004243068262520764\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06288431727555063\n",
      "Average test loss: 0.0042237729622672\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06273552248213027\n",
      "Average test loss: 0.004222397844410605\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06253948918647237\n",
      "Average test loss: 0.004271488072971503\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06237552664677302\n",
      "Average test loss: 0.004217285689173473\n",
      "Epoch 37/300\n",
      "Average training loss: 0.062219700518581604\n",
      "Average test loss: 0.004209028175721566\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06208895208438237\n",
      "Average test loss: 0.004215212497653232\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06188917079236772\n",
      "Average test loss: 0.00421844324717919\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06179293019241757\n",
      "Average test loss: 0.004206476469420724\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06159273511502478\n",
      "Average test loss: 0.0042118359785526995\n",
      "Epoch 42/300\n",
      "Average training loss: 0.061530628942781024\n",
      "Average test loss: 0.004195803108728594\n",
      "Epoch 43/300\n",
      "Average training loss: 0.061331653909550775\n",
      "Average test loss: 0.004219412061489291\n",
      "Epoch 44/300\n",
      "Average training loss: 0.061244218945503234\n",
      "Average test loss: 0.004216348686565955\n",
      "Epoch 45/300\n",
      "Average training loss: 0.061066952029863994\n",
      "Average test loss: 0.004190762754943636\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0609490678343508\n",
      "Average test loss: 0.004179147789047824\n",
      "Epoch 47/300\n",
      "Average training loss: 0.060811605390575195\n",
      "Average test loss: 0.004190918066849311\n",
      "Epoch 48/300\n",
      "Average training loss: 0.060706844088104034\n",
      "Average test loss: 0.004196458408402072\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06054858183529642\n",
      "Average test loss: 0.004197880625103911\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06047252669930458\n",
      "Average test loss: 0.004206036281875439\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06031963044404984\n",
      "Average test loss: 0.004178240632017454\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06019819240106477\n",
      "Average test loss: 0.004202315767606099\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06005449442068736\n",
      "Average test loss: 0.004234198011043999\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06002510508232647\n",
      "Average test loss: 0.004179750466926231\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05989692332678371\n",
      "Average test loss: 0.004186557029063503\n",
      "Epoch 56/300\n",
      "Average training loss: 0.059745728462934494\n",
      "Average test loss: 0.004278643625064029\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05964698735210631\n",
      "Average test loss: 0.0042136765494942665\n",
      "Epoch 58/300\n",
      "Average training loss: 0.059475618100828596\n",
      "Average test loss: 0.004219872921498285\n",
      "Epoch 59/300\n",
      "Average training loss: 0.059337783025370704\n",
      "Average test loss: 0.004218926251555483\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05922757802406947\n",
      "Average test loss: 0.00419319278622667\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05911697542005115\n",
      "Average test loss: 0.0042185720412267575\n",
      "Epoch 62/300\n",
      "Average training loss: 0.058944879839817686\n",
      "Average test loss: 0.004234187831067377\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05886906914247407\n",
      "Average test loss: 0.004228699697802464\n",
      "Epoch 64/300\n",
      "Average training loss: 0.058663463115692135\n",
      "Average test loss: 0.004186807837337256\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0585849462515778\n",
      "Average test loss: 0.004205112198781636\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05851274362537596\n",
      "Average test loss: 0.0043316715011994045\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05840054436855846\n",
      "Average test loss: 0.004256742880576187\n",
      "Epoch 68/300\n",
      "Average training loss: 0.058250784420304826\n",
      "Average test loss: 0.0042469450928684736\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05810219518012471\n",
      "Average test loss: 0.004294295454604758\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0579513684047593\n",
      "Average test loss: 0.004252335423189733\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05786405868993865\n",
      "Average test loss: 0.004264757133606407\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0576651705371009\n",
      "Average test loss: 0.004256811406877305\n",
      "Epoch 73/300\n",
      "Average training loss: 0.057539417737060124\n",
      "Average test loss: 0.004295380860567093\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05745413504375352\n",
      "Average test loss: 0.004303596798537506\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05729174543089337\n",
      "Average test loss: 0.0042526811340616805\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05723098648587863\n",
      "Average test loss: 0.004248114123940468\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05703806940383381\n",
      "Average test loss: 0.004430719600369533\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0568905499180158\n",
      "Average test loss: 0.004256427615053124\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05682663835088412\n",
      "Average test loss: 0.004281917905228006\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05663506916165352\n",
      "Average test loss: 0.00444293247618609\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05650075105494923\n",
      "Average test loss: 0.004274254242372181\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05649425850643052\n",
      "Average test loss: 0.004429608611596955\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0562510066529115\n",
      "Average test loss: 0.004377439928965436\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05614753080076641\n",
      "Average test loss: 0.004282190339846744\n",
      "Epoch 85/300\n",
      "Average training loss: 0.056071030129988986\n",
      "Average test loss: 0.004261500070699387\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05586811337206099\n",
      "Average test loss: 0.00432075956525902\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05570117716325654\n",
      "Average test loss: 0.004319392169929213\n",
      "Epoch 88/300\n",
      "Average training loss: 0.055516802661948736\n",
      "Average test loss: 0.00431065273616049\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05544865555067857\n",
      "Average test loss: 0.004348078707440032\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05541070477498902\n",
      "Average test loss: 0.0044222054858174585\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0552379025320212\n",
      "Average test loss: 0.004303160835057497\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05525157487723562\n",
      "Average test loss: 0.004353426441136334\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05500523135728306\n",
      "Average test loss: 0.004391651355557972\n",
      "Epoch 94/300\n",
      "Average training loss: 0.054847177141242556\n",
      "Average test loss: 0.0043598690475854605\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05476434494389428\n",
      "Average test loss: 0.004295675731781456\n",
      "Epoch 96/300\n",
      "Average training loss: 0.054680346058474646\n",
      "Average test loss: 0.004377372450298733\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05454643830988142\n",
      "Average test loss: 0.004476823302606741\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05445155343082216\n",
      "Average test loss: 0.004388615709212091\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05427845834692319\n",
      "Average test loss: 0.004427061717129416\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05424838947256406\n",
      "Average test loss: 0.004343280767401059\n",
      "Epoch 101/300\n",
      "Average training loss: 0.054137650930219224\n",
      "Average test loss: 0.004602842848334047\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0540725725458728\n",
      "Average test loss: 0.004479815525313218\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05386141532328394\n",
      "Average test loss: 0.0043410546320180095\n",
      "Epoch 104/300\n",
      "Average training loss: 0.053809858520825704\n",
      "Average test loss: 0.00449761326942179\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05365612181358867\n",
      "Average test loss: 0.004464331687738498\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05357202251421081\n",
      "Average test loss: 0.004386136723475324\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05351394221186638\n",
      "Average test loss: 0.004448394797742367\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05333221968015035\n",
      "Average test loss: 0.0045053841357843745\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05326446018781927\n",
      "Average test loss: 0.004447956730922063\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05313217167390717\n",
      "Average test loss: 0.004446817709960871\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05311826493177149\n",
      "Average test loss: 0.0044888797118845915\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05296810104780727\n",
      "Average test loss: 0.004501586560573843\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0529035134712855\n",
      "Average test loss: 0.004742966529809767\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05290286806556914\n",
      "Average test loss: 0.004626112979733281\n",
      "Epoch 115/300\n",
      "Average training loss: 0.052604265193144484\n",
      "Average test loss: 0.0045781361158523295\n",
      "Epoch 116/300\n",
      "Average training loss: 0.052546439783440695\n",
      "Average test loss: 0.004610547220541371\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05243890079524782\n",
      "Average test loss: 0.004402801126655605\n",
      "Epoch 118/300\n",
      "Average training loss: 0.052385066287385096\n",
      "Average test loss: 0.004483455661063393\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05227691196070777\n",
      "Average test loss: 0.004547018670166532\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05223378461930487\n",
      "Average test loss: 0.004506591770797968\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05219166037771437\n",
      "Average test loss: 0.0047192138539006315\n",
      "Epoch 122/300\n",
      "Average training loss: 0.052131781230370204\n",
      "Average test loss: 0.004497789185908106\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05201881815658675\n",
      "Average test loss: 0.004485655426151223\n",
      "Epoch 124/300\n",
      "Average training loss: 0.051810041735569634\n",
      "Average test loss: 0.0046969893421563835\n",
      "Epoch 125/300\n",
      "Average training loss: 0.051733394824796254\n",
      "Average test loss: 0.004528326554844777\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05166345018148422\n",
      "Average test loss: 0.00450924355412523\n",
      "Epoch 127/300\n",
      "Average training loss: 0.051614338510566286\n",
      "Average test loss: 0.004550021804041333\n",
      "Epoch 128/300\n",
      "Average training loss: 0.051588409096002576\n",
      "Average test loss: 0.0045474321132318845\n",
      "Epoch 129/300\n",
      "Average training loss: 0.051504570878214306\n",
      "Average test loss: 0.004510377659979793\n",
      "Epoch 130/300\n",
      "Average training loss: 0.051362172795666586\n",
      "Average test loss: 0.004542943595598141\n",
      "Epoch 131/300\n",
      "Average training loss: 0.051327937596374086\n",
      "Average test loss: 0.004512677520099613\n",
      "Epoch 132/300\n",
      "Average training loss: 0.051273033297724195\n",
      "Average test loss: 0.004702143317088485\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05119627423087756\n",
      "Average test loss: 0.004632711220532656\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05113212398687998\n",
      "Average test loss: 0.004494732768792245\n",
      "Epoch 135/300\n",
      "Average training loss: 0.051037334356043074\n",
      "Average test loss: 0.004614488739520311\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05095779712498188\n",
      "Average test loss: 0.004529371341897382\n",
      "Epoch 137/300\n",
      "Average training loss: 0.050985691567262015\n",
      "Average test loss: 0.004531353916972875\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05087748487790426\n",
      "Average test loss: 0.004546788748146759\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05069921734266811\n",
      "Average test loss: 0.0045127516744865315\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05060081801480717\n",
      "Average test loss: 0.00445453729107976\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05057964235875342\n",
      "Average test loss: 0.00444128555763099\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05048196350865894\n",
      "Average test loss: 0.0045488798879086976\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05050096208519406\n",
      "Average test loss: 0.004525954695211516\n",
      "Epoch 144/300\n",
      "Average training loss: 0.050418150318993465\n",
      "Average test loss: 0.004640544005773134\n",
      "Epoch 145/300\n",
      "Average training loss: 0.050282897565099925\n",
      "Average test loss: 0.004702204124795066\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0503290493355857\n",
      "Average test loss: 0.004573984293681052\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05015888845920563\n",
      "Average test loss: 0.004474118929356337\n",
      "Epoch 148/300\n",
      "Average training loss: 0.050088170518477755\n",
      "Average test loss: 0.00448249827283952\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05001350125670433\n",
      "Average test loss: 0.004515202061583599\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05000271830956141\n",
      "Average test loss: 0.004539504641873969\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04995138513379627\n",
      "Average test loss: 0.004491726962021656\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05003567345605956\n",
      "Average test loss: 0.004562260488669078\n",
      "Epoch 153/300\n",
      "Average training loss: 0.049878205921914845\n",
      "Average test loss: 0.004667711748017205\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04962413258685006\n",
      "Average test loss: 0.004556035825361808\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04962417565782865\n",
      "Average test loss: 0.004553490686540802\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04965281508366267\n",
      "Average test loss: 0.004680994396201439\n",
      "Epoch 157/300\n",
      "Average training loss: 0.049566822253995474\n",
      "Average test loss: 0.0045992354978289865\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04944043653872278\n",
      "Average test loss: 0.0045687870644032956\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04945866400500139\n",
      "Average test loss: 0.004562639650785261\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04937553061710464\n",
      "Average test loss: 0.004669300457669629\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04923667346437772\n",
      "Average test loss: 0.004540958018352588\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0492329837679863\n",
      "Average test loss: 0.004684732625881831\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04934360461433729\n",
      "Average test loss: 0.004599776731183131\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04917805593212445\n",
      "Average test loss: 0.004649245984231432\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04907936513092783\n",
      "Average test loss: 0.00458406227413151\n",
      "Epoch 166/300\n",
      "Average training loss: 0.049027149554755955\n",
      "Average test loss: 0.004643285815914472\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04910975239011976\n",
      "Average test loss: 0.0047131363045838145\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04899903485841221\n",
      "Average test loss: 0.004624814947860109\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04883541843295097\n",
      "Average test loss: 0.004573893203503556\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04887121059166061\n",
      "Average test loss: 0.004661025688052178\n",
      "Epoch 171/300\n",
      "Average training loss: 0.048746750705771974\n",
      "Average test loss: 0.004675505283806059\n",
      "Epoch 172/300\n",
      "Average training loss: 0.048778041485283107\n",
      "Average test loss: 0.004515153158870008\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0486262423131201\n",
      "Average test loss: 0.004774980298967825\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0486724967526065\n",
      "Average test loss: 0.0046480613197717405\n",
      "Epoch 175/300\n",
      "Average training loss: 0.048625119315253366\n",
      "Average test loss: 0.004653959017453922\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04848031884431839\n",
      "Average test loss: 0.004676520140634643\n",
      "Epoch 177/300\n",
      "Average training loss: 0.048425824721654256\n",
      "Average test loss: 0.004611505305601491\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04848078061474694\n",
      "Average test loss: 0.004662080320219199\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04834308566980892\n",
      "Average test loss: 0.004676454626644651\n",
      "Epoch 180/300\n",
      "Average training loss: 0.048386152959532205\n",
      "Average test loss: 0.004793894219315714\n",
      "Epoch 181/300\n",
      "Average training loss: 0.048337041093243494\n",
      "Average test loss: 0.004535851428699163\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04831743430429035\n",
      "Average test loss: 0.004739630852308538\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04820230411158668\n",
      "Average test loss: 0.004608894354353348\n",
      "Epoch 184/300\n",
      "Average training loss: 0.048164401100741494\n",
      "Average test loss: 0.004676993624617656\n",
      "Epoch 185/300\n",
      "Average training loss: 0.048133071233828865\n",
      "Average test loss: 0.004520954828088482\n",
      "Epoch 186/300\n",
      "Average training loss: 0.048063217076990336\n",
      "Average test loss: 0.004583165927893586\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0479595378835996\n",
      "Average test loss: 0.004571870728085439\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04792302433649699\n",
      "Average test loss: 0.00471321727335453\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0479365166094568\n",
      "Average test loss: 0.004568404106630219\n",
      "Epoch 190/300\n",
      "Average training loss: 0.047907715813981164\n",
      "Average test loss: 0.004710212285733885\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04782403149207433\n",
      "Average test loss: 0.004603271920647886\n",
      "Epoch 192/300\n",
      "Average training loss: 0.047745565825038486\n",
      "Average test loss: 0.004657412266979615\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04782962504360411\n",
      "Average test loss: 0.004618717604213291\n",
      "Epoch 194/300\n",
      "Average training loss: 0.047752350134982004\n",
      "Average test loss: 0.00475988805356125\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04764033136102888\n",
      "Average test loss: 0.004713828298366732\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04762793791790803\n",
      "Average test loss: 0.004793006480981906\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04759034066730075\n",
      "Average test loss: 0.004780330133520894\n",
      "Epoch 198/300\n",
      "Average training loss: 0.047496628714932335\n",
      "Average test loss: 0.004612946285141839\n",
      "Epoch 199/300\n",
      "Average training loss: 0.047453577342960566\n",
      "Average test loss: 0.004614933625691467\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04743169790506363\n",
      "Average test loss: 0.004489465110417869\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04741675568454795\n",
      "Average test loss: 0.004718557277487384\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04737872717115614\n",
      "Average test loss: 0.004754782730920447\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04735315468741788\n",
      "Average test loss: 0.004721873328710596\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04719603647457229\n",
      "Average test loss: 0.0047123491271502446\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04720119862424003\n",
      "Average test loss: 0.004565486861599816\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04715900546974606\n",
      "Average test loss: 0.004655858928544654\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04720036783152157\n",
      "Average test loss: 0.0047279907270438145\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04724883566465643\n",
      "Average test loss: 0.004861847564578056\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04709597269362874\n",
      "Average test loss: 0.004711201835630669\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047074188656277126\n",
      "Average test loss: 0.004653953762104114\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04707273544536696\n",
      "Average test loss: 0.004594741241799461\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04700579798883862\n",
      "Average test loss: 0.004669417704972956\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04684396998418702\n",
      "Average test loss: 0.004594757959246636\n",
      "Epoch 214/300\n",
      "Average training loss: 0.046907965941561595\n",
      "Average test loss: 0.004620713593852189\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04678920395175616\n",
      "Average test loss: 0.0046168396783371765\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04709694719314575\n",
      "Average test loss: 0.0045354464803304935\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04680897486872143\n",
      "Average test loss: 0.004462574921548367\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04666873918970426\n",
      "Average test loss: 0.004591783338329858\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0466811333960957\n",
      "Average test loss: 0.004682043565230237\n",
      "Epoch 220/300\n",
      "Average training loss: 0.046593717094924714\n",
      "Average test loss: 0.004666385188284847\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04671484732627869\n",
      "Average test loss: 0.004710544260839621\n",
      "Epoch 222/300\n",
      "Average training loss: 0.046583158469862405\n",
      "Average test loss: 0.00468111802596185\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04658975960479842\n",
      "Average test loss: 0.004809882925202449\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04651431597603692\n",
      "Average test loss: 0.004716389401505391\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04644454496436649\n",
      "Average test loss: 0.004643931173202064\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04649164119031694\n",
      "Average test loss: 0.004703403754573729\n",
      "Epoch 227/300\n",
      "Average training loss: 0.046473883055978354\n",
      "Average test loss: 0.004634943881382545\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04643018566568693\n",
      "Average test loss: 0.004765866477870279\n",
      "Epoch 229/300\n",
      "Average training loss: 0.046418518175681434\n",
      "Average test loss: 0.0046473642289638515\n",
      "Epoch 230/300\n",
      "Average training loss: 0.046332565436760585\n",
      "Average test loss: 0.004576251087917222\n",
      "Epoch 231/300\n",
      "Average training loss: 0.046325377964311173\n",
      "Average test loss: 0.004843071347723405\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04631235836280717\n",
      "Average test loss: 0.004574209071074923\n",
      "Epoch 233/300\n",
      "Average training loss: 0.046200512031714124\n",
      "Average test loss: 0.004775293461564514\n",
      "Epoch 234/300\n",
      "Average training loss: 0.046208477874596914\n",
      "Average test loss: 0.004622852064669132\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04618480145600107\n",
      "Average test loss: 0.004599639011340009\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04615090717540847\n",
      "Average test loss: 0.004622207672645648\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04602004745602608\n",
      "Average test loss: 0.004661619362524814\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04612724524736404\n",
      "Average test loss: 0.004666718792170286\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04604498419000043\n",
      "Average test loss: 0.004663564745336771\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04591320871975687\n",
      "Average test loss: 0.004829361319541931\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04600359454751015\n",
      "Average test loss: 0.004653006424506505\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04593460491630766\n",
      "Average test loss: 0.004768370255620943\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04592241907450888\n",
      "Average test loss: 0.004750895075293051\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04586161323388418\n",
      "Average test loss: 0.004634307160766588\n",
      "Epoch 245/300\n",
      "Average training loss: 0.045874726914697225\n",
      "Average test loss: 0.004625764191978508\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0458605764226781\n",
      "Average test loss: 0.004591995289756192\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04587957818309466\n",
      "Average test loss: 0.004886215685142411\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0458818308810393\n",
      "Average test loss: 0.004737585736231672\n",
      "Epoch 249/300\n",
      "Average training loss: 0.045739905532863404\n",
      "Average test loss: 0.004532700068006913\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04579965791106224\n",
      "Average test loss: 0.004669370970792241\n",
      "Epoch 251/300\n",
      "Average training loss: 0.045672138293584186\n",
      "Average test loss: 0.004697365045547485\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04573076790571213\n",
      "Average test loss: 0.004777145534339878\n",
      "Epoch 253/300\n",
      "Average training loss: 0.045583159403668506\n",
      "Average test loss: 0.004712263063010242\n",
      "Epoch 254/300\n",
      "Average training loss: 0.045721366594235105\n",
      "Average test loss: 0.004658175165868468\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04553062365452448\n",
      "Average test loss: 0.004755997362650103\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04556299601991971\n",
      "Average test loss: 0.004842986320041948\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04552521173159282\n",
      "Average test loss: 0.004629900274591313\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04542043991221322\n",
      "Average test loss: 0.0047057561605340905\n",
      "Epoch 259/300\n",
      "Average training loss: 0.045419074694315595\n",
      "Average test loss: 0.004537671960476372\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04545067775580618\n",
      "Average test loss: 0.004674723374760813\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04540536698698998\n",
      "Average test loss: 0.004586222948299514\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04539805909494559\n",
      "Average test loss: 0.00475251845187611\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04536015366514524\n",
      "Average test loss: 0.004826544615957472\n",
      "Epoch 264/300\n",
      "Average training loss: 0.045320845696661206\n",
      "Average test loss: 0.00477489205615388\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04533227979309029\n",
      "Average test loss: 0.004838832883371247\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04525403643978967\n",
      "Average test loss: 0.004587353437104159\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04530981648630566\n",
      "Average test loss: 0.004755090205412772\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04524063037832578\n",
      "Average test loss: 0.00485796949474348\n",
      "Epoch 269/300\n",
      "Average training loss: 0.045171500507328245\n",
      "Average test loss: 0.004901329378700919\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04520142856902546\n",
      "Average test loss: 0.00472008615359664\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04513427346613672\n",
      "Average test loss: 0.004633264233254724\n",
      "Epoch 272/300\n",
      "Average training loss: 0.045137716054916385\n",
      "Average test loss: 0.004696812153690391\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04512762832972739\n",
      "Average test loss: 0.004888429211038682\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04509260692530208\n",
      "Average test loss: 0.004642507555998034\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04501238983869553\n",
      "Average test loss: 0.00466169371166163\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04500152287218306\n",
      "Average test loss: 0.004828140115986267\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04499489136868053\n",
      "Average test loss: 0.0047383464583092266\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04509628626704216\n",
      "Average test loss: 0.004778214562684298\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04498379725548956\n",
      "Average test loss: 0.004896833647870355\n",
      "Epoch 280/300\n",
      "Average training loss: 0.044945175502035355\n",
      "Average test loss: 0.0047101893768542345\n",
      "Epoch 281/300\n",
      "Average training loss: 0.044838190303908454\n",
      "Average test loss: 0.004778051283831398\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04501452603936195\n",
      "Average test loss: 0.004863773369954692\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04483658836285273\n",
      "Average test loss: 0.004670593664877944\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04484999855193827\n",
      "Average test loss: 0.004889212083899312\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04477794099185202\n",
      "Average test loss: 0.004651734979616271\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04470723606480492\n",
      "Average test loss: 0.0047658584486279226\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04465617962347137\n",
      "Average test loss: 0.00479874174690081\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04472497208250893\n",
      "Average test loss: 0.004797161610176166\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04488938257429335\n",
      "Average test loss: 0.004821222641815742\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04461064206560453\n",
      "Average test loss: 0.004653363453431262\n",
      "Epoch 291/300\n",
      "Average training loss: 0.044551597469382816\n",
      "Average test loss: 0.0056203508132861724\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04465203346643183\n",
      "Average test loss: 0.004579465131378836\n",
      "Epoch 293/300\n",
      "Average training loss: 0.044543819944063826\n",
      "Average test loss: 0.004646808461596569\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04451979995767275\n",
      "Average test loss: 0.004733259113298522\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0445054746998681\n",
      "Average test loss: 0.004807413342926238\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04456720062593619\n",
      "Average test loss: 0.004893289688146777\n",
      "Epoch 297/300\n",
      "Average training loss: 0.044609066973129906\n",
      "Average test loss: 0.0048803380450440775\n",
      "Epoch 298/300\n",
      "Average training loss: 0.044518404165903726\n",
      "Average test loss: 0.004743226564178864\n",
      "Epoch 299/300\n",
      "Average training loss: 0.044505022015836504\n",
      "Average test loss: 0.004644105030016767\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04435572472380267\n",
      "Average test loss: 0.004819673815949096\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.1079801807403564\n",
      "Average test loss: 0.005067715879943636\n",
      "Epoch 2/300\n",
      "Average training loss: 0.28664555139011805\n",
      "Average test loss: 0.004463917614271244\n",
      "Epoch 3/300\n",
      "Average training loss: 0.16663353381554286\n",
      "Average test loss: 0.004187622855727872\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11770998034212324\n",
      "Average test loss: 0.004146280137615072\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09402195430464215\n",
      "Average test loss: 0.004084282113860051\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08162645019094149\n",
      "Average test loss: 0.0038659574079016844\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0742922634879748\n",
      "Average test loss: 0.003891681148774094\n",
      "Epoch 8/300\n",
      "Average training loss: 0.069589413460758\n",
      "Average test loss: 0.0037381717674434185\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06619070114692052\n",
      "Average test loss: 0.0037933675286670526\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06357085856464174\n",
      "Average test loss: 0.00372998824239605\n",
      "Epoch 11/300\n",
      "Average training loss: 0.061513538509607314\n",
      "Average test loss: 0.0035816153321001263\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05983932674262259\n",
      "Average test loss: 0.003858135670423508\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05851243862178591\n",
      "Average test loss: 0.0035140000393407214\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05731089699268341\n",
      "Average test loss: 0.003533440477318234\n",
      "Epoch 15/300\n",
      "Average training loss: 0.056356914934184814\n",
      "Average test loss: 0.003633276021728913\n",
      "Epoch 16/300\n",
      "Average training loss: 0.055474820223119524\n",
      "Average test loss: 0.0034923774676604404\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05474855652782652\n",
      "Average test loss: 0.0033941759173240927\n",
      "Epoch 18/300\n",
      "Average training loss: 0.054105711857477826\n",
      "Average test loss: 0.003347330305104454\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0535139361437824\n",
      "Average test loss: 0.0033243838016771606\n",
      "Epoch 20/300\n",
      "Average training loss: 0.052959801726871064\n",
      "Average test loss: 0.0033070923222435844\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05242134326365259\n",
      "Average test loss: 0.003319184331016408\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05196574139926169\n",
      "Average test loss: 0.003353412937786844\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05147575056884024\n",
      "Average test loss: 0.00325562053008212\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05109948852658272\n",
      "Average test loss: 0.003229013764195972\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05055940726896127\n",
      "Average test loss: 0.0032210957403812144\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05024949391351806\n",
      "Average test loss: 0.0032356852036383417\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04978697607252333\n",
      "Average test loss: 0.0031784952582998406\n",
      "Epoch 28/300\n",
      "Average training loss: 0.049402279363738165\n",
      "Average test loss: 0.0031771126468148497\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04903308201167319\n",
      "Average test loss: 0.003177651250734925\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04876112017366621\n",
      "Average test loss: 0.0031616183053702115\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04836529177096155\n",
      "Average test loss: 0.003120543904809488\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04810528261462847\n",
      "Average test loss: 0.0031762227976901663\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04778439302245776\n",
      "Average test loss: 0.003132624784277545\n",
      "Epoch 34/300\n",
      "Average training loss: 0.047485680470863975\n",
      "Average test loss: 0.0031501873903390432\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04719608853260676\n",
      "Average test loss: 0.0031454663858231573\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04685131864746411\n",
      "Average test loss: 0.0031717449327309925\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04656096160411835\n",
      "Average test loss: 0.003113280967498819\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04631108366118537\n",
      "Average test loss: 0.003157417223064436\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04607135378321012\n",
      "Average test loss: 0.0031536198320488134\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04579331424004502\n",
      "Average test loss: 0.003167658690570129\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04554154544737604\n",
      "Average test loss: 0.003078416503758894\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04528275753723251\n",
      "Average test loss: 0.003085484286977185\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04501506264342202\n",
      "Average test loss: 0.003083726539793942\n",
      "Epoch 44/300\n",
      "Average training loss: 0.044771125535170236\n",
      "Average test loss: 0.003088145709493094\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04453207849793964\n",
      "Average test loss: 0.003168560031801462\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04421328859527906\n",
      "Average test loss: 0.0031011829763236973\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04406509891152382\n",
      "Average test loss: 0.00313414644346469\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04377843840254678\n",
      "Average test loss: 0.003102674350970321\n",
      "Epoch 49/300\n",
      "Average training loss: 0.043544248044490814\n",
      "Average test loss: 0.003103784193802211\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04326106607086129\n",
      "Average test loss: 0.003170956492630972\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04307323956158426\n",
      "Average test loss: 0.003121252998502718\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04281246302194065\n",
      "Average test loss: 0.0030997374600006474\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04267333825429281\n",
      "Average test loss: 0.0030969219423002668\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0424293806122409\n",
      "Average test loss: 0.003098772833537724\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04217058898011843\n",
      "Average test loss: 0.0030759059023112057\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04202771536178059\n",
      "Average test loss: 0.003124325034312076\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04176037383907371\n",
      "Average test loss: 0.0033303117007017135\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04151574703388744\n",
      "Average test loss: 0.0034270167754342157\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04128909354077445\n",
      "Average test loss: 0.0031855408971508343\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04110525949464904\n",
      "Average test loss: 0.0031436162201894653\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04082971222201983\n",
      "Average test loss: 0.003121162437523405\n",
      "Epoch 62/300\n",
      "Average training loss: 0.040700316107935376\n",
      "Average test loss: 0.003107733730847637\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0404476292166445\n",
      "Average test loss: 0.0031798479372842445\n",
      "Epoch 64/300\n",
      "Average training loss: 0.040250634137127136\n",
      "Average test loss: 0.0031672470352301996\n",
      "Epoch 65/300\n",
      "Average training loss: 0.040036524799135\n",
      "Average test loss: 0.0031615433514946036\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03985814679993523\n",
      "Average test loss: 0.0031946641854527923\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03975131710701518\n",
      "Average test loss: 0.0031377989675642716\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03955427223609553\n",
      "Average test loss: 0.0032106443672544425\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0393485818306605\n",
      "Average test loss: 0.003244228169735935\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03911524080236753\n",
      "Average test loss: 0.0032074770546621748\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03898672257032659\n",
      "Average test loss: 0.0032559013499153986\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03885456471972995\n",
      "Average test loss: 0.0031884195177505413\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03867814824233452\n",
      "Average test loss: 0.0033742409919699035\n",
      "Epoch 74/300\n",
      "Average training loss: 0.038490358074506126\n",
      "Average test loss: 0.0032601203080266715\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03844825458195474\n",
      "Average test loss: 0.0032663880739774967\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0382002431270149\n",
      "Average test loss: 0.0032479273202932544\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03809037508401606\n",
      "Average test loss: 0.0032219498078856204\n",
      "Epoch 78/300\n",
      "Average training loss: 0.037895027044746614\n",
      "Average test loss: 0.003264317950854699\n",
      "Epoch 79/300\n",
      "Average training loss: 0.037808297266562776\n",
      "Average test loss: 0.003324456879662143\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03757310615314378\n",
      "Average test loss: 0.0033328541573137043\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03756151318715678\n",
      "Average test loss: 0.0033727209396246407\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03735064606202973\n",
      "Average test loss: 0.003266004303056333\n",
      "Epoch 83/300\n",
      "Average training loss: 0.037252819965283075\n",
      "Average test loss: 0.003450330571995841\n",
      "Epoch 84/300\n",
      "Average training loss: 0.037094618875119424\n",
      "Average test loss: 0.003365979230652253\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03705666312906477\n",
      "Average test loss: 0.003250793679099944\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03690548470947477\n",
      "Average test loss: 0.00330989963395728\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0367984857890341\n",
      "Average test loss: 0.003390008930116892\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03657176214456558\n",
      "Average test loss: 0.003337133169174194\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03657853348718749\n",
      "Average test loss: 0.003406350245492326\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03650305511719651\n",
      "Average test loss: 0.003367245715525415\n",
      "Epoch 91/300\n",
      "Average training loss: 0.036393139214979275\n",
      "Average test loss: 0.003225450140941474\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03620458491974407\n",
      "Average test loss: 0.0033364189606573847\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03606067934963438\n",
      "Average test loss: 0.003287149967625737\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03596153116557333\n",
      "Average test loss: 0.0033846465388519898\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03595469306409359\n",
      "Average test loss: 0.003340024744781355\n",
      "Epoch 96/300\n",
      "Average training loss: 0.035781062771876655\n",
      "Average test loss: 0.003320611354170574\n",
      "Epoch 97/300\n",
      "Average training loss: 0.035768892645835874\n",
      "Average test loss: 0.00340892503120833\n",
      "Epoch 98/300\n",
      "Average training loss: 0.035576258902748424\n",
      "Average test loss: 0.0034829839364522034\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03548856689201461\n",
      "Average test loss: 0.0034218711105899677\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03545315635866589\n",
      "Average test loss: 0.0033799340882235103\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03546615473926067\n",
      "Average test loss: 0.003402597317679061\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03519487764603562\n",
      "Average test loss: 0.0034534302649812565\n",
      "Epoch 103/300\n",
      "Average training loss: 0.035238216049141353\n",
      "Average test loss: 0.003319721551405059\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03515814175870684\n",
      "Average test loss: 0.003358408557665017\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0349733048296637\n",
      "Average test loss: 0.003291444102095233\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0349594021903144\n",
      "Average test loss: 0.003423844170032276\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034861521500680184\n",
      "Average test loss: 0.003304012130532\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03471053619848357\n",
      "Average test loss: 0.003426654023014837\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03467035551700327\n",
      "Average test loss: 0.0035490611319740615\n",
      "Epoch 110/300\n",
      "Average training loss: 0.034536276853746835\n",
      "Average test loss: 0.0033891794433196384\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03452393380800883\n",
      "Average test loss: 0.003329977740223209\n",
      "Epoch 112/300\n",
      "Average training loss: 0.034484665522972746\n",
      "Average test loss: 0.003372892545122239\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03439412656260861\n",
      "Average test loss: 0.0033283822192913958\n",
      "Epoch 114/300\n",
      "Average training loss: 0.034350465885467\n",
      "Average test loss: 0.003480702148957385\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03422882535391384\n",
      "Average test loss: 0.0033520849189824526\n",
      "Epoch 116/300\n",
      "Average training loss: 0.034149673567877875\n",
      "Average test loss: 0.00337481593237155\n",
      "Epoch 117/300\n",
      "Average training loss: 0.034140776043136915\n",
      "Average test loss: 0.003343287799300419\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03399982720282343\n",
      "Average test loss: 0.003485839946816365\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03401567734943496\n",
      "Average test loss: 0.0033335086910261046\n",
      "Epoch 120/300\n",
      "Average training loss: 0.033890616941783164\n",
      "Average test loss: 0.003576304844684071\n",
      "Epoch 121/300\n",
      "Average training loss: 0.033790268381436665\n",
      "Average test loss: 0.0035476443622675206\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03371591487195757\n",
      "Average test loss: 0.0035086155190236037\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03379738139775064\n",
      "Average test loss: 0.0033535538918028273\n",
      "Epoch 124/300\n",
      "Average training loss: 0.033670891619390915\n",
      "Average test loss: 0.0035124319965640703\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03354621566666497\n",
      "Average test loss: 0.003466572468065553\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03352065605752998\n",
      "Average test loss: 0.0034631093036797313\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03344929055869579\n",
      "Average test loss: 0.003397407967183325\n",
      "Epoch 128/300\n",
      "Average training loss: 0.033410916911231145\n",
      "Average test loss: 0.0034363355684197613\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03341969239049487\n",
      "Average test loss: 0.003590155520786842\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03330344460076756\n",
      "Average test loss: 0.0033757200402518115\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03321120538314184\n",
      "Average test loss: 0.003370152570307255\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03311780411667294\n",
      "Average test loss: 0.0033893227837979794\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03315004156695472\n",
      "Average test loss: 0.003344130933077799\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03308583485583464\n",
      "Average test loss: 0.003410637642774317\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03300644720925225\n",
      "Average test loss: 0.0033976759684996474\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03289374041557312\n",
      "Average test loss: 0.003476187381479475\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03295078185697397\n",
      "Average test loss: 0.00338284095749259\n",
      "Epoch 138/300\n",
      "Average training loss: 0.032880525074071354\n",
      "Average test loss: 0.0034120223578065633\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03277108514474498\n",
      "Average test loss: 0.0035066998242917987\n",
      "Epoch 140/300\n",
      "Average training loss: 0.032774957501226\n",
      "Average test loss: 0.003566198780718777\n",
      "Epoch 141/300\n",
      "Average training loss: 0.032705484910143745\n",
      "Average test loss: 0.0034695363632506796\n",
      "Epoch 142/300\n",
      "Average training loss: 0.032665449721945655\n",
      "Average test loss: 0.0035363608472463158\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03261922530664338\n",
      "Average test loss: 0.0034803700297036107\n",
      "Epoch 144/300\n",
      "Average training loss: 0.032554818443126146\n",
      "Average test loss: 0.003504522323401438\n",
      "Epoch 145/300\n",
      "Average training loss: 0.032538851148552365\n",
      "Average test loss: 0.0034241820726957587\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03249330360194047\n",
      "Average test loss: 0.0034102107025682925\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03240320906539758\n",
      "Average test loss: 0.0034224322494119407\n",
      "Epoch 148/300\n",
      "Average training loss: 0.032361415705747076\n",
      "Average test loss: 0.0035023249253216715\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03237837815947003\n",
      "Average test loss: 0.003509219779322545\n",
      "Epoch 150/300\n",
      "Average training loss: 0.032373674814899764\n",
      "Average test loss: 0.003547204016397397\n",
      "Epoch 151/300\n",
      "Average training loss: 0.032347392393483054\n",
      "Average test loss: 0.0035663709263834688\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03223815019759867\n",
      "Average test loss: 0.0034698261014289326\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03217561894820796\n",
      "Average test loss: 0.0033912023692909215\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03204207078119119\n",
      "Average test loss: 0.0034570128185053665\n",
      "Epoch 155/300\n",
      "Average training loss: 0.032108478430244655\n",
      "Average test loss: 0.0034910541025714743\n",
      "Epoch 156/300\n",
      "Average training loss: 0.032092149558994504\n",
      "Average test loss: 0.0035030294491185084\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03195878485673004\n",
      "Average test loss: 0.00349238454302152\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03202190363075998\n",
      "Average test loss: 0.0035795113266342215\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03195611874593629\n",
      "Average test loss: 0.0034496401249327594\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031881184341179\n",
      "Average test loss: 0.003396106309360928\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0317740343577332\n",
      "Average test loss: 0.0034451978630903692\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031834085249238545\n",
      "Average test loss: 0.0034520895899169974\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03181501853466034\n",
      "Average test loss: 0.003563359245244\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031735848163565\n",
      "Average test loss: 0.003442642811064919\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031708230495452884\n",
      "Average test loss: 0.0035461050557593506\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03167350601984395\n",
      "Average test loss: 0.0035113826856637996\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03158093611896038\n",
      "Average test loss: 0.0033954179903699293\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03156811807221836\n",
      "Average test loss: 0.0036120681824783484\n",
      "Epoch 169/300\n",
      "Average training loss: 0.031540826936562856\n",
      "Average test loss: 0.003418026116780109\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0315234188636144\n",
      "Average test loss: 0.003494989382310046\n",
      "Epoch 171/300\n",
      "Average training loss: 0.031458328465620675\n",
      "Average test loss: 0.0034841833698252837\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0315072012792031\n",
      "Average test loss: 0.003683474043591155\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03141066187289026\n",
      "Average test loss: 0.0035056434617274338\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03136430798636542\n",
      "Average test loss: 0.0035502568446099756\n",
      "Epoch 175/300\n",
      "Average training loss: 0.031311189136571355\n",
      "Average test loss: 0.0034570747214472957\n",
      "Epoch 176/300\n",
      "Average training loss: 0.031300229012966155\n",
      "Average test loss: 0.003401256663931741\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031216432430677944\n",
      "Average test loss: 0.003530487779320942\n",
      "Epoch 178/300\n",
      "Average training loss: 0.031239219968517622\n",
      "Average test loss: 0.003497683857671089\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03120290124913057\n",
      "Average test loss: 0.003494819743765725\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03118340246213807\n",
      "Average test loss: 0.0035317234916405547\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03109831085635556\n",
      "Average test loss: 0.003481113007085191\n",
      "Epoch 182/300\n",
      "Average training loss: 0.031113511915008225\n",
      "Average test loss: 0.0034550798286994296\n",
      "Epoch 183/300\n",
      "Average training loss: 0.031067427701420253\n",
      "Average test loss: 0.0035313632887684636\n",
      "Epoch 184/300\n",
      "Average training loss: 0.031063468219505418\n",
      "Average test loss: 0.0036548543547590575\n",
      "Epoch 185/300\n",
      "Average training loss: 0.031032632950279446\n",
      "Average test loss: 0.0034314546142187384\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030959271237254143\n",
      "Average test loss: 0.003394218595491515\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03095941783653365\n",
      "Average test loss: 0.003564002591185272\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030963956779903837\n",
      "Average test loss: 0.0036244197893473837\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030885925230052735\n",
      "Average test loss: 0.003621710458977355\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03084553749859333\n",
      "Average test loss: 0.003449262631436189\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030758887042601903\n",
      "Average test loss: 0.0036185428818894757\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030850676720341048\n",
      "Average test loss: 0.003684204222013553\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030792730818192164\n",
      "Average test loss: 0.003486689144021107\n",
      "Epoch 194/300\n",
      "Average training loss: 0.030738520961668757\n",
      "Average test loss: 0.003584878034475777\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030684390773375828\n",
      "Average test loss: 0.0034855290489892166\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030673855586184396\n",
      "Average test loss: 0.0035874765896134904\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030655149973101085\n",
      "Average test loss: 0.0035169028451459273\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030602743772996797\n",
      "Average test loss: 0.0035597718571209244\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030603032771084043\n",
      "Average test loss: 0.0035754880456046927\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030555628624227314\n",
      "Average test loss: 0.0035260725954754484\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030475373581051825\n",
      "Average test loss: 0.003657780979656511\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030529748817284902\n",
      "Average test loss: 0.0034252968318760394\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030591647502448825\n",
      "Average test loss: 0.0034825908833493787\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030453898914986188\n",
      "Average test loss: 0.0036173970030827654\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030427436846825813\n",
      "Average test loss: 0.0034667087385637893\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030355017670326764\n",
      "Average test loss: 0.00355646207018031\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03033818999098407\n",
      "Average test loss: 0.003562423400580883\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03037962688340081\n",
      "Average test loss: 0.0035453236723939578\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030301337361335754\n",
      "Average test loss: 0.0037022858450396195\n",
      "Epoch 210/300\n",
      "Average training loss: 0.030260146141052244\n",
      "Average test loss: 0.003586065945112043\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03021381931503614\n",
      "Average test loss: 0.0035129541150397724\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030175025641918183\n",
      "Average test loss: 0.0035705842528906133\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0302293654150433\n",
      "Average test loss: 0.003484788275220328\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030201220926311282\n",
      "Average test loss: 0.003554951272490952\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030215989594658217\n",
      "Average test loss: 0.0034719937609301674\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030163445250855552\n",
      "Average test loss: 0.0035977684119716286\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030172339065207377\n",
      "Average test loss: 0.003554047134394447\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0301051687217421\n",
      "Average test loss: 0.0035476746939950518\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030055317223072052\n",
      "Average test loss: 0.0036779053778284126\n",
      "Epoch 220/300\n",
      "Average training loss: 0.030002992916438313\n",
      "Average test loss: 0.00363197163782186\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030058889736731847\n",
      "Average test loss: 0.003634112009778619\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02999046277999878\n",
      "Average test loss: 0.0036509410250518057\n",
      "Epoch 223/300\n",
      "Average training loss: 0.030012280328406228\n",
      "Average test loss: 0.003528831165076958\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029901807490322323\n",
      "Average test loss: 0.0035800360871685874\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030017367505364947\n",
      "Average test loss: 0.003520823647785518\n",
      "Epoch 226/300\n",
      "Average training loss: 0.029889538480175865\n",
      "Average test loss: 0.0035303262803289626\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029889618002706102\n",
      "Average test loss: 0.0035973010326011312\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029842395142548613\n",
      "Average test loss: 0.003620200284446279\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02981190547347069\n",
      "Average test loss: 0.003562377957627177\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029806770286626285\n",
      "Average test loss: 0.0034950706623494625\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029720846898025937\n",
      "Average test loss: 0.003491603355233868\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02987278805507554\n",
      "Average test loss: 0.00367144249793556\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029736660460631052\n",
      "Average test loss: 0.003570824342676335\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029751504719257353\n",
      "Average test loss: 0.0037791025352974733\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0297639289945364\n",
      "Average test loss: 0.0035549358228842416\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02974192883239852\n",
      "Average test loss: 0.003548963599734836\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02960794377161397\n",
      "Average test loss: 0.0035976850961645444\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02967166689866119\n",
      "Average test loss: 0.003463536274929841\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029645453532536826\n",
      "Average test loss: 0.0035684205904189083\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029669819990793864\n",
      "Average test loss: 0.003755247633283337\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029572148733668858\n",
      "Average test loss: 0.0036095463029212423\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029582965301142797\n",
      "Average test loss: 0.0034229436736139987\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02952858655154705\n",
      "Average test loss: 0.003588288256277641\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029542818945315148\n",
      "Average test loss: 0.003790223226779037\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029462865913907687\n",
      "Average test loss: 0.0035854710288759735\n",
      "Epoch 246/300\n",
      "Average training loss: 0.029477822028928333\n",
      "Average test loss: 0.003525224483054545\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02942642325328456\n",
      "Average test loss: 0.0035494700241833926\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02946512068145805\n",
      "Average test loss: 0.0035841235369443894\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029436458695265982\n",
      "Average test loss: 0.0034576749951682156\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02936852335764302\n",
      "Average test loss: 0.0035688223439372248\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02933611328734292\n",
      "Average test loss: 0.0036740337908267974\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029372209936380386\n",
      "Average test loss: 0.003583308489786254\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02933179066909684\n",
      "Average test loss: 0.0035677195207940208\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029315646992789376\n",
      "Average test loss: 0.0035204517980002696\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029340935635897847\n",
      "Average test loss: 0.0035680149123072623\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02934594040777948\n",
      "Average test loss: 0.0036097618642573557\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029286594791544808\n",
      "Average test loss: 0.0036743491157475443\n",
      "Epoch 258/300\n",
      "Average training loss: 0.029274273329310947\n",
      "Average test loss: 0.0038520170826878814\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02920678677989377\n",
      "Average test loss: 0.003708975492666165\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029237588892380397\n",
      "Average test loss: 0.003623250437279542\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029237705669469302\n",
      "Average test loss: 0.0035617448252936203\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02919989076919026\n",
      "Average test loss: 0.003662389016813702\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0291143915736013\n",
      "Average test loss: 0.0036417619594269327\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029140391969018513\n",
      "Average test loss: 0.003591720370782746\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029166164285606807\n",
      "Average test loss: 0.003600001471117139\n",
      "Epoch 266/300\n",
      "Average training loss: 0.029134020747409926\n",
      "Average test loss: 0.003669822581940227\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029170349131027857\n",
      "Average test loss: 0.0036054330372975933\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0290584329896503\n",
      "Average test loss: 0.003584327546879649\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029035120588209895\n",
      "Average test loss: 0.003568565742009216\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028998501822352408\n",
      "Average test loss: 0.0035586042298624914\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029019381086031595\n",
      "Average test loss: 0.0035370354776581127\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02899427428841591\n",
      "Average test loss: 0.0035168530787858697\n",
      "Epoch 273/300\n",
      "Average training loss: 0.028990933342112436\n",
      "Average test loss: 0.003634546893959244\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02899535656472047\n",
      "Average test loss: 0.003630515296012163\n",
      "Epoch 275/300\n",
      "Average training loss: 0.028905882101919914\n",
      "Average test loss: 0.003594957109954622\n",
      "Epoch 276/300\n",
      "Average training loss: 0.028881303555435604\n",
      "Average test loss: 0.003495686597708199\n",
      "Epoch 277/300\n",
      "Average training loss: 0.028904216700130037\n",
      "Average test loss: 0.003568441976275709\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028914682863487137\n",
      "Average test loss: 0.003594294982030988\n",
      "Epoch 279/300\n",
      "Average training loss: 0.028867678253187073\n",
      "Average test loss: 0.003640535499072737\n",
      "Epoch 280/300\n",
      "Average training loss: 0.028950551663835842\n",
      "Average test loss: 0.0036347818240109416\n",
      "Epoch 281/300\n",
      "Average training loss: 0.028878612190485\n",
      "Average test loss: 0.003558568813941545\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02879866967101892\n",
      "Average test loss: 0.003522817623698049\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02879690490083562\n",
      "Average test loss: 0.003653262493511041\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02883400237229135\n",
      "Average test loss: 0.003567419180025657\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02883959781130155\n",
      "Average test loss: 0.0035478422445141605\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0288128911058108\n",
      "Average test loss: 0.0035202409510190287\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02876228043437004\n",
      "Average test loss: 0.003788584100910359\n",
      "Epoch 288/300\n",
      "Average training loss: 0.028761055989397898\n",
      "Average test loss: 0.0035833435331781706\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0286787239379353\n",
      "Average test loss: 0.00365915982466605\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02868243514498075\n",
      "Average test loss: 0.0034886557043840487\n",
      "Epoch 291/300\n",
      "Average training loss: 0.028681292919649017\n",
      "Average test loss: 0.0035575143065717484\n",
      "Epoch 292/300\n",
      "Average training loss: 0.028736827126807637\n",
      "Average test loss: 0.003542734539343251\n",
      "Epoch 293/300\n",
      "Average training loss: 0.028694728520181442\n",
      "Average test loss: 0.00364590992902716\n",
      "Epoch 294/300\n",
      "Average training loss: 0.028592543196347023\n",
      "Average test loss: 0.0035159902278747825\n",
      "Epoch 295/300\n",
      "Average training loss: 0.028608426919413937\n",
      "Average test loss: 0.003644792895350191\n",
      "Epoch 296/300\n",
      "Average training loss: 0.028640522965126567\n",
      "Average test loss: 0.0036113587150143254\n",
      "Epoch 297/300\n",
      "Average training loss: 0.028649139929148887\n",
      "Average test loss: 0.0036142019824021393\n",
      "Epoch 298/300\n",
      "Average training loss: 0.028607637150420084\n",
      "Average test loss: 0.0035712157872815928\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028571554561456044\n",
      "Average test loss: 0.003625540288372172\n",
      "Epoch 300/300\n",
      "Average training loss: 0.028491509608096547\n",
      "Average test loss: 0.0035905322865065602\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.018772535112169\n",
      "Average test loss: 0.004534849689652522\n",
      "Epoch 2/300\n",
      "Average training loss: 0.24985908566580878\n",
      "Average test loss: 0.004234575647860766\n",
      "Epoch 3/300\n",
      "Average training loss: 0.14849542654222914\n",
      "Average test loss: 0.0037452927740911643\n",
      "Epoch 4/300\n",
      "Average training loss: 0.10692488622665405\n",
      "Average test loss: 0.003603159083881312\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0861752134097947\n",
      "Average test loss: 0.0034686874536176524\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07448158315486378\n",
      "Average test loss: 0.00343211286701262\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0672189526160558\n",
      "Average test loss: 0.00330909515482684\n",
      "Epoch 8/300\n",
      "Average training loss: 0.062362637006574205\n",
      "Average test loss: 0.003263691892847419\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05879396053817537\n",
      "Average test loss: 0.003197770534704129\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05603447412451108\n",
      "Average test loss: 0.003165975160482857\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0538638005455335\n",
      "Average test loss: 0.003041267726984289\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05212824906574355\n",
      "Average test loss: 0.0030716682066106132\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05066674863298734\n",
      "Average test loss: 0.0029324393769105278\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04951342483692699\n",
      "Average test loss: 0.0029207014687773255\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0483987506462468\n",
      "Average test loss: 0.0028535792200515666\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04754718109634187\n",
      "Average test loss: 0.0027791689582582976\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04674246161182721\n",
      "Average test loss: 0.0028154792137857943\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04603165464599927\n",
      "Average test loss: 0.0027378864617397386\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04529602917697695\n",
      "Average test loss: 0.002708200464439061\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04462856209774812\n",
      "Average test loss: 0.0026741284924662774\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04404717839426465\n",
      "Average test loss: 0.002690828249272373\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04345729334817992\n",
      "Average test loss: 0.002616470804851916\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04289946715533734\n",
      "Average test loss: 0.0025910357284463113\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04232952413956324\n",
      "Average test loss: 0.0026291495313247046\n",
      "Epoch 25/300\n",
      "Average training loss: 0.041819806830750574\n",
      "Average test loss: 0.0026206800574436785\n",
      "Epoch 26/300\n",
      "Average training loss: 0.041382709758149255\n",
      "Average test loss: 0.0025790392083840236\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04095693234105905\n",
      "Average test loss: 0.0026299679986097747\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04048695389760865\n",
      "Average test loss: 0.002534246082521147\n",
      "Epoch 29/300\n",
      "Average training loss: 0.040037916772895386\n",
      "Average test loss: 0.0025023186184051963\n",
      "Epoch 30/300\n",
      "Average training loss: 0.039604769902096854\n",
      "Average test loss: 0.0025096041324237984\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0392420853757196\n",
      "Average test loss: 0.0024825002559357217\n",
      "Epoch 32/300\n",
      "Average training loss: 0.038840140860941674\n",
      "Average test loss: 0.0025064807317944035\n",
      "Epoch 33/300\n",
      "Average training loss: 0.038544840388827856\n",
      "Average test loss: 0.002482686118947135\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03813896166781584\n",
      "Average test loss: 0.0024918884966108534\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03795500403642654\n",
      "Average test loss: 0.0024653256067799197\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0374986491534445\n",
      "Average test loss: 0.0024547945449335706\n",
      "Epoch 37/300\n",
      "Average training loss: 0.037120824529065025\n",
      "Average test loss: 0.0024444938076453077\n",
      "Epoch 38/300\n",
      "Average training loss: 0.036898239102628495\n",
      "Average test loss: 0.0024830211953570445\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03656556654307577\n",
      "Average test loss: 0.0024374605992601977\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03628600798547268\n",
      "Average test loss: 0.0024730107040247984\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03608987146284845\n",
      "Average test loss: 0.002452162641204066\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03577565682265493\n",
      "Average test loss: 0.002420545196781556\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03547281502518389\n",
      "Average test loss: 0.002477533060229487\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03521041035486592\n",
      "Average test loss: 0.002425384035954873\n",
      "Epoch 45/300\n",
      "Average training loss: 0.034946760925981736\n",
      "Average test loss: 0.0024635438180218142\n",
      "Epoch 46/300\n",
      "Average training loss: 0.034713581931259896\n",
      "Average test loss: 0.0024356024755785863\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03442543172505167\n",
      "Average test loss: 0.0024475884389960102\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03421190776262018\n",
      "Average test loss: 0.002418767713631193\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03399956427597337\n",
      "Average test loss: 0.002572236240737968\n",
      "Epoch 50/300\n",
      "Average training loss: 0.033815909816159145\n",
      "Average test loss: 0.0024938627237247094\n",
      "Epoch 51/300\n",
      "Average training loss: 0.033525478627946644\n",
      "Average test loss: 0.0024531488600704407\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03327743310232957\n",
      "Average test loss: 0.0024576306038846573\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03304885047674179\n",
      "Average test loss: 0.0024400286187107366\n",
      "Epoch 54/300\n",
      "Average training loss: 0.032887005690071316\n",
      "Average test loss: 0.0024492226431353224\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03262675901585155\n",
      "Average test loss: 0.002512875696644187\n",
      "Epoch 56/300\n",
      "Average training loss: 0.032419485544164975\n",
      "Average test loss: 0.0024751051121080915\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03223415447605981\n",
      "Average test loss: 0.0032842378697047632\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03200186316172282\n",
      "Average test loss: 0.0024906741196496617\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03183402257495456\n",
      "Average test loss: 0.0024695783564820887\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03166160156826178\n",
      "Average test loss: 0.002461671931358675\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0314476176119513\n",
      "Average test loss: 0.0025221391966980364\n",
      "Epoch 62/300\n",
      "Average training loss: 0.031244288820359442\n",
      "Average test loss: 0.002613384537398815\n",
      "Epoch 63/300\n",
      "Average training loss: 0.031063827652070258\n",
      "Average test loss: 0.002445637035080128\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03087644443743759\n",
      "Average test loss: 0.0024592181593179703\n",
      "Epoch 65/300\n",
      "Average training loss: 0.030705325027306874\n",
      "Average test loss: 0.0027324882416675488\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0305479932245281\n",
      "Average test loss: 0.0025428263768553735\n",
      "Epoch 67/300\n",
      "Average training loss: 0.030548473808500502\n",
      "Average test loss: 0.0025773625595288145\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03023606300354004\n",
      "Average test loss: 0.0025034525781455967\n",
      "Epoch 69/300\n",
      "Average training loss: 0.030032651397917005\n",
      "Average test loss: 0.002567440580162737\n",
      "Epoch 70/300\n",
      "Average training loss: 0.029953123377429114\n",
      "Average test loss: 0.002497648995576633\n",
      "Epoch 71/300\n",
      "Average training loss: 0.029764430354038873\n",
      "Average test loss: 0.002531467538326979\n",
      "Epoch 72/300\n",
      "Average training loss: 0.029612267684605387\n",
      "Average test loss: 0.002557667371092571\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0294792323840989\n",
      "Average test loss: 0.002655581444915798\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02931820553706752\n",
      "Average test loss: 0.002538249482297235\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02921232586933507\n",
      "Average test loss: 0.0025550748908685313\n",
      "Epoch 76/300\n",
      "Average training loss: 0.029145111307501793\n",
      "Average test loss: 0.002832330198130674\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02905526273449262\n",
      "Average test loss: 0.002551553730956382\n",
      "Epoch 78/300\n",
      "Average training loss: 0.028838182848360802\n",
      "Average test loss: 0.0027407927576245535\n",
      "Epoch 79/300\n",
      "Average training loss: 0.028695715518461333\n",
      "Average test loss: 0.0025204719396101105\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028593055774768193\n",
      "Average test loss: 0.002530520383682516\n",
      "Epoch 81/300\n",
      "Average training loss: 0.028443307946125665\n",
      "Average test loss: 0.0026266007342686257\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028373363201816878\n",
      "Average test loss: 0.0025566018426583874\n",
      "Epoch 83/300\n",
      "Average training loss: 0.028315397062235407\n",
      "Average test loss: 0.002541002843528986\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028156610146992736\n",
      "Average test loss: 0.0026408503217001758\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02818515930407577\n",
      "Average test loss: 0.002551766256801784\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02789416059686078\n",
      "Average test loss: 0.0025583242368366985\n",
      "Epoch 87/300\n",
      "Average training loss: 0.027843588466445603\n",
      "Average test loss: 0.0026323390460262696\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02778105436348253\n",
      "Average test loss: 0.002635961450325946\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02763072095811367\n",
      "Average test loss: 0.0026770161839409006\n",
      "Epoch 90/300\n",
      "Average training loss: 0.027520679818259345\n",
      "Average test loss: 0.0027354148568378553\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02747402005063163\n",
      "Average test loss: 0.0025925899820609223\n",
      "Epoch 92/300\n",
      "Average training loss: 0.027314323647154703\n",
      "Average test loss: 0.0026384161690043077\n",
      "Epoch 93/300\n",
      "Average training loss: 0.027327474234832658\n",
      "Average test loss: 0.0026665628414808047\n",
      "Epoch 94/300\n",
      "Average training loss: 0.027169265555010903\n",
      "Average test loss: 0.002553182047895259\n",
      "Epoch 95/300\n",
      "Average training loss: 0.027068997285432286\n",
      "Average test loss: 0.002608179434099131\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02706250367065271\n",
      "Average test loss: 0.002587734348451098\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026934339331256017\n",
      "Average test loss: 0.002685352603594462\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026856834019223848\n",
      "Average test loss: 0.0027301917988806963\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0268128019604418\n",
      "Average test loss: 0.002540954103279445\n",
      "Epoch 100/300\n",
      "Average training loss: 0.026682446357276703\n",
      "Average test loss: 0.002593352535118659\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02660668179558383\n",
      "Average test loss: 0.002662328725266788\n",
      "Epoch 102/300\n",
      "Average training loss: 0.026603188982440365\n",
      "Average test loss: 0.0027033801244364846\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026534022624293965\n",
      "Average test loss: 0.0026961412569507957\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026387282732460233\n",
      "Average test loss: 0.0026098090038220913\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026304365502463446\n",
      "Average test loss: 0.0026079464233997795\n",
      "Epoch 106/300\n",
      "Average training loss: 0.026284343757563167\n",
      "Average test loss: 0.0026602111638834077\n",
      "Epoch 107/300\n",
      "Average training loss: 0.026189931866195466\n",
      "Average test loss: 0.0026736641776644522\n",
      "Epoch 108/300\n",
      "Average training loss: 0.026165188054243723\n",
      "Average test loss: 0.0026841921291003625\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02606825219757027\n",
      "Average test loss: 0.0026338673223637874\n",
      "Epoch 110/300\n",
      "Average training loss: 0.025995629873540666\n",
      "Average test loss: 0.002708691201276249\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02594084683391783\n",
      "Average test loss: 0.0026297394115891723\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02592178546388944\n",
      "Average test loss: 0.002734769559154908\n",
      "Epoch 113/300\n",
      "Average training loss: 0.025847748685214254\n",
      "Average test loss: 0.002668341131673919\n",
      "Epoch 114/300\n",
      "Average training loss: 0.025753100001149706\n",
      "Average test loss: 0.0026786945060723356\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02575230528579818\n",
      "Average test loss: 0.0027164356083505684\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025637227987249694\n",
      "Average test loss: 0.0026094539858814744\n",
      "Epoch 117/300\n",
      "Average training loss: 0.025548643108871248\n",
      "Average test loss: 0.0026460291591679885\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025464938178658487\n",
      "Average test loss: 0.002683200797273053\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025507022536463208\n",
      "Average test loss: 0.0026559790341804425\n",
      "Epoch 120/300\n",
      "Average training loss: 0.025434672626356285\n",
      "Average test loss: 0.002716640472205149\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025415295453535186\n",
      "Average test loss: 0.0027194652379386956\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025414919265442426\n",
      "Average test loss: 0.0027344660678257546\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02527084058192041\n",
      "Average test loss: 0.002636289439474543\n",
      "Epoch 124/300\n",
      "Average training loss: 0.025202728503280217\n",
      "Average test loss: 0.0027259934842586517\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025171946714321772\n",
      "Average test loss: 0.002696184058363239\n",
      "Epoch 126/300\n",
      "Average training loss: 0.025123217832711008\n",
      "Average test loss: 0.0026791881047603158\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025106867495510315\n",
      "Average test loss: 0.00271488727008303\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025032525059249668\n",
      "Average test loss: 0.0027081277275251016\n",
      "Epoch 129/300\n",
      "Average training loss: 0.025017562647660575\n",
      "Average test loss: 0.0026457563870482975\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024912409103578993\n",
      "Average test loss: 0.0026643430567863916\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024903746616509227\n",
      "Average test loss: 0.002691107271032201\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024875354791680973\n",
      "Average test loss: 0.0028395099923428564\n",
      "Epoch 133/300\n",
      "Average training loss: 0.024810720319549243\n",
      "Average test loss: 0.002733200643418564\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0247373337480757\n",
      "Average test loss: 0.0027166535885383685\n",
      "Epoch 135/300\n",
      "Average training loss: 0.024759587311082416\n",
      "Average test loss: 0.0028071122393012048\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02467039052479797\n",
      "Average test loss: 0.002732761881003777\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02462066800892353\n",
      "Average test loss: 0.0027191786758808626\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02458833888835377\n",
      "Average test loss: 0.0028144687972962855\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024513837180203862\n",
      "Average test loss: 0.002732274419110682\n",
      "Epoch 140/300\n",
      "Average training loss: 0.024469405420952373\n",
      "Average test loss: 0.0028107388460387787\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02448474301563369\n",
      "Average test loss: 0.002668811730109155\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02442435693409708\n",
      "Average test loss: 0.0027197583955195213\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024407331008050176\n",
      "Average test loss: 0.002697040735433499\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0243566907197237\n",
      "Average test loss: 0.0027754942638178667\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024400552600622176\n",
      "Average test loss: 0.0027367558183355465\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024297892259226905\n",
      "Average test loss: 0.002738878924606575\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024247642467419308\n",
      "Average test loss: 0.0027437880684932072\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024247776130835216\n",
      "Average test loss: 0.00275480371237629\n",
      "Epoch 149/300\n",
      "Average training loss: 0.024127548292279245\n",
      "Average test loss: 0.0026953117170681554\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02414449591603544\n",
      "Average test loss: 0.0027288951178391775\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024012964377800622\n",
      "Average test loss: 0.002732526863200797\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0240581778883934\n",
      "Average test loss: 0.002774859621603456\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02405998890929752\n",
      "Average test loss: 0.0027210338707599376\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024065946348839337\n",
      "Average test loss: 0.002669708537765675\n",
      "Epoch 155/300\n",
      "Average training loss: 0.023984282980362575\n",
      "Average test loss: 0.002759430125562681\n",
      "Epoch 156/300\n",
      "Average training loss: 0.023924820088677936\n",
      "Average test loss: 0.00279280777172082\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023859678546587625\n",
      "Average test loss: 0.0028188673559990194\n",
      "Epoch 158/300\n",
      "Average training loss: 0.023821006463633644\n",
      "Average test loss: 0.0026792847712834677\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02381869285553694\n",
      "Average test loss: 0.0027931889823327462\n",
      "Epoch 160/300\n",
      "Average training loss: 0.023841873346103562\n",
      "Average test loss: 0.00267489061339034\n",
      "Epoch 161/300\n",
      "Average training loss: 0.023729791859785715\n",
      "Average test loss: 0.002804229986336496\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023696220335033207\n",
      "Average test loss: 0.002724411278238727\n",
      "Epoch 163/300\n",
      "Average training loss: 0.023706606805324555\n",
      "Average test loss: 0.002704991929957436\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02372493760039409\n",
      "Average test loss: 0.0027604574716339507\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023720381600989236\n",
      "Average test loss: 0.0028207193177400363\n",
      "Epoch 166/300\n",
      "Average training loss: 0.023636491192711724\n",
      "Average test loss: 0.0026626539393845533\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023547096972664196\n",
      "Average test loss: 0.0028290645031051505\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023560058595405685\n",
      "Average test loss: 0.0027326666793475547\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02355448534256882\n",
      "Average test loss: 0.0027601962046076853\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023466600702868566\n",
      "Average test loss: 0.002779066075467401\n",
      "Epoch 171/300\n",
      "Average training loss: 0.023500949058267804\n",
      "Average test loss: 0.002681405727027191\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02347732000715203\n",
      "Average test loss: 0.002758171078024639\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02334621413052082\n",
      "Average test loss: 0.0027454225532710552\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02336362172994349\n",
      "Average test loss: 0.002732872318269478\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023366692748334673\n",
      "Average test loss: 0.002850392970359988\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02333974706298775\n",
      "Average test loss: 0.002776818854527341\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023316486120224\n",
      "Average test loss: 0.0027595651511930757\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023221200136674777\n",
      "Average test loss: 0.002767787656229403\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0232074184268713\n",
      "Average test loss: 0.0028100975015097193\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023250071924593715\n",
      "Average test loss: 0.0030045601934608488\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023256882084740534\n",
      "Average test loss: 0.0027944955062121153\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023163907201753722\n",
      "Average test loss: 0.002704608193702168\n",
      "Epoch 183/300\n",
      "Average training loss: 0.023147765268882115\n",
      "Average test loss: 0.002721346001037293\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0231356384207805\n",
      "Average test loss: 0.002706538916255037\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023097782073749438\n",
      "Average test loss: 0.0028038683810995683\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023050942849781778\n",
      "Average test loss: 0.0028515225851701367\n",
      "Epoch 187/300\n",
      "Average training loss: 0.023043577858143382\n",
      "Average test loss: 0.00274351946843995\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02300471880700853\n",
      "Average test loss: 0.0029736785050481556\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023013083626826605\n",
      "Average test loss: 0.003670404741540551\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02296065569255087\n",
      "Average test loss: 0.0027673717946228053\n",
      "Epoch 191/300\n",
      "Average training loss: 0.022946730226278304\n",
      "Average test loss: 0.0028541602643413675\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022982423231005668\n",
      "Average test loss: 0.0028018850015683306\n",
      "Epoch 193/300\n",
      "Average training loss: 0.022849836871027946\n",
      "Average test loss: 0.0027407191917300225\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022892532237701945\n",
      "Average test loss: 0.002777729451449381\n",
      "Epoch 195/300\n",
      "Average training loss: 0.022889916752775512\n",
      "Average test loss: 0.003099525927255551\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022841246702604824\n",
      "Average test loss: 0.0027031967683384815\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022841593821843465\n",
      "Average test loss: 0.002740056880439321\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02276764387637377\n",
      "Average test loss: 0.002862497589447432\n",
      "Epoch 199/300\n",
      "Average training loss: 0.022725392702553006\n",
      "Average test loss: 0.0028717753514647484\n",
      "Epoch 200/300\n",
      "Average training loss: 0.022700432780716154\n",
      "Average test loss: 0.0027261411322073805\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022768205843038028\n",
      "Average test loss: 0.002843164722331696\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02271642433769173\n",
      "Average test loss: 0.002761049176255862\n",
      "Epoch 203/300\n",
      "Average training loss: 0.022651464151011574\n",
      "Average test loss: 0.002942217571971317\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02264522236585617\n",
      "Average test loss: 0.002750268541690376\n",
      "Epoch 205/300\n",
      "Average training loss: 0.022637345198127958\n",
      "Average test loss: 0.002794544676525725\n",
      "Epoch 206/300\n",
      "Average training loss: 0.022656897394193545\n",
      "Average test loss: 0.002879026542417705\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022631939578387474\n",
      "Average test loss: 0.002820295288124018\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02256523294084602\n",
      "Average test loss: 0.0027258264700778657\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022563770312401983\n",
      "Average test loss: 0.0027957920556267103\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02254253460963567\n",
      "Average test loss: 0.0027106673028320072\n",
      "Epoch 211/300\n",
      "Average training loss: 0.022547051856915155\n",
      "Average test loss: 0.0027740673199295996\n",
      "Epoch 212/300\n",
      "Average training loss: 0.022528241998619504\n",
      "Average test loss: 0.002803549468931225\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022441780921485688\n",
      "Average test loss: 0.0028072324430363045\n",
      "Epoch 214/300\n",
      "Average training loss: 0.022415382001135083\n",
      "Average test loss: 0.0027706545293331147\n",
      "Epoch 215/300\n",
      "Average training loss: 0.022394959749446976\n",
      "Average test loss: 0.0028550294282742674\n",
      "Epoch 216/300\n",
      "Average training loss: 0.022413967185550266\n",
      "Average test loss: 0.0027600036058574914\n",
      "Epoch 217/300\n",
      "Average training loss: 0.022363311373525197\n",
      "Average test loss: 0.002810080309708913\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0223970231761535\n",
      "Average test loss: 0.0028471857241044443\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02234949244227674\n",
      "Average test loss: 0.0027728161633842523\n",
      "Epoch 220/300\n",
      "Average training loss: 0.022324261900451448\n",
      "Average test loss: 0.0027969578926761943\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02239630318681399\n",
      "Average test loss: 0.0028640576880425215\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022312343935171765\n",
      "Average test loss: 0.002827403971925378\n",
      "Epoch 223/300\n",
      "Average training loss: 0.022327634728617138\n",
      "Average test loss: 0.002805385086685419\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022200361947218576\n",
      "Average test loss: 0.002717569593133198\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022226547812422115\n",
      "Average test loss: 0.002875157228567534\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02224786513712671\n",
      "Average test loss: 0.0028101188726723193\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022225717296202978\n",
      "Average test loss: 0.002808705248559515\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022209851652383806\n",
      "Average test loss: 0.0028486453684874705\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02219690792262554\n",
      "Average test loss: 0.0029124347598602374\n",
      "Epoch 230/300\n",
      "Average training loss: 0.022102537176675267\n",
      "Average test loss: 0.002827568077792724\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022204891385303602\n",
      "Average test loss: 0.0028561890708903473\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022150900514589417\n",
      "Average test loss: 0.002859546408471134\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022117098240388763\n",
      "Average test loss: 0.007785592607325978\n",
      "Epoch 234/300\n",
      "Average training loss: 0.022231884472899966\n",
      "Average test loss: 0.002746317139826715\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02199579830467701\n",
      "Average test loss: 0.002790075364626116\n",
      "Epoch 236/300\n",
      "Average training loss: 0.022060343965888022\n",
      "Average test loss: 0.0028223302963500223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02203681111915244\n",
      "Average test loss: 0.0028387282031277815\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022017309486865998\n",
      "Average test loss: 0.002852079655975103\n",
      "Epoch 239/300\n",
      "Average training loss: 0.022062149935298495\n",
      "Average test loss: 0.002797742541465494\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02202310174372461\n",
      "Average test loss: 0.0027290370553317996\n",
      "Epoch 241/300\n",
      "Average training loss: 0.021984517542852296\n",
      "Average test loss: 0.0028829881248788702\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02195969835917155\n",
      "Average test loss: 0.0028494539355031316\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021974440193838544\n",
      "Average test loss: 0.0027767919657441477\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021909930823577776\n",
      "Average test loss: 0.0028409986576686305\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021921789361370934\n",
      "Average test loss: 0.0028255304623809128\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021900277988778222\n",
      "Average test loss: 0.0028242640453908178\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02189082472357485\n",
      "Average test loss: 0.002801760765723884\n",
      "Epoch 248/300\n",
      "Average training loss: 0.021907523792650964\n",
      "Average test loss: 0.0030786983233152166\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021854396729005708\n",
      "Average test loss: 0.0029022800632649\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02184427445464664\n",
      "Average test loss: 0.002819072014755673\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021800178206629224\n",
      "Average test loss: 0.002684520364842481\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02181676461464829\n",
      "Average test loss: 0.00286985397628612\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021779158209760983\n",
      "Average test loss: 0.0028122362657450137\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02178212033212185\n",
      "Average test loss: 0.0030629648814598717\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02180664049420092\n",
      "Average test loss: 0.002842957991589275\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0217150693188111\n",
      "Average test loss: 0.002896330667866601\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021778509202930663\n",
      "Average test loss: 0.003178052803294526\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021710746897591483\n",
      "Average test loss: 0.002827096211930944\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021651383923159705\n",
      "Average test loss: 0.0029415712296548815\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02173469045923816\n",
      "Average test loss: 0.002914883501827717\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02170209741095702\n",
      "Average test loss: 0.002785499410910739\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02165352398653825\n",
      "Average test loss: 0.002846481748546163\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02165415737198459\n",
      "Average test loss: 0.0028207388990041282\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02162638901670774\n",
      "Average test loss: 0.0028702376656648187\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021654870465397835\n",
      "Average test loss: 0.0027924666938682397\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021601589732699922\n",
      "Average test loss: 0.0028141138268013796\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021579350943366687\n",
      "Average test loss: 0.0028087897286232975\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021584697172045707\n",
      "Average test loss: 0.0029012588916553393\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02164208137161202\n",
      "Average test loss: 0.0028712158180359336\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021554627625478637\n",
      "Average test loss: 0.0027643962224117584\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021520116117265488\n",
      "Average test loss: 0.002712088400911954\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021523066490888594\n",
      "Average test loss: 0.0028184168975179395\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021499571745594342\n",
      "Average test loss: 0.002944086954618494\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02157027632991473\n",
      "Average test loss: 0.0027805990171101357\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02147343338529269\n",
      "Average test loss: 0.002969830790327655\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021644401285383436\n",
      "Average test loss: 0.0029058691867523724\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02143887551294433\n",
      "Average test loss: 0.002815121969001161\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021442337150375048\n",
      "Average test loss: 0.0027939906674954627\n",
      "Epoch 279/300\n",
      "Average training loss: 0.021478456276986333\n",
      "Average test loss: 0.0028075876144899262\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021494272210531764\n",
      "Average test loss: 0.0029351998896648486\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021430397914515602\n",
      "Average test loss: 0.002859300516442292\n",
      "Epoch 282/300\n",
      "Average training loss: 0.021445638959606488\n",
      "Average test loss: 0.002795853839152389\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02140445111029678\n",
      "Average test loss: 0.002830770908544461\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021346906392110718\n",
      "Average test loss: 0.0028345372008366718\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021384782004687523\n",
      "Average test loss: 0.002959954298618767\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02135342915356159\n",
      "Average test loss: 0.0027704165855215656\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02133172251118554\n",
      "Average test loss: 0.002849271763012641\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021338377543621594\n",
      "Average test loss: 0.0028286342708807853\n",
      "Epoch 289/300\n",
      "Average training loss: 0.021320698630478647\n",
      "Average test loss: 0.0028966195565751857\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02131886009540823\n",
      "Average test loss: 0.0029755909581565196\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02129196531905068\n",
      "Average test loss: 0.002803053112493621\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021280087490876516\n",
      "Average test loss: 0.002824023480837544\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021238764415184658\n",
      "Average test loss: 0.0028066415468023884\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021272536761230892\n",
      "Average test loss: 0.0027883749817394546\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021283945290578737\n",
      "Average test loss: 0.0028021873926950824\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021287483730249936\n",
      "Average test loss: 0.0028308200167698992\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021218793193499248\n",
      "Average test loss: 0.002886447425517771\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021238811747895346\n",
      "Average test loss: 0.0028321412490266893\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02117908816370699\n",
      "Average test loss: 0.0028349211474673615\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021196265288525157\n",
      "Average test loss: 0.0028443771277864775\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.8888322383827634\n",
      "Average test loss: 0.004119862142536375\n",
      "Epoch 2/300\n",
      "Average training loss: 0.22225634585486517\n",
      "Average test loss: 0.0033177108930216893\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12424088849623997\n",
      "Average test loss: 0.0031307142737011114\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08819985330767102\n",
      "Average test loss: 0.002934275205143624\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0713735152218077\n",
      "Average test loss: 0.0027861536532226535\n",
      "Epoch 6/300\n",
      "Average training loss: 0.061684637665748596\n",
      "Average test loss: 0.002733997310615248\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05533120979203118\n",
      "Average test loss: 0.0028282040970193014\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05086260204845005\n",
      "Average test loss: 0.0026188102724651495\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04747274497151375\n",
      "Average test loss: 0.002406652795926978\n",
      "Epoch 10/300\n",
      "Average training loss: 0.044921227365732196\n",
      "Average test loss: 0.002514607502768437\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04288516295949618\n",
      "Average test loss: 0.0023207062915381457\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04124183321992556\n",
      "Average test loss: 0.002214220725827747\n",
      "Epoch 13/300\n",
      "Average training loss: 0.039893452833096185\n",
      "Average test loss: 0.0021668189539470607\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03871835028131803\n",
      "Average test loss: 0.002266396008638872\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03762858511507511\n",
      "Average test loss: 0.002059907511807978\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03678019451101621\n",
      "Average test loss: 0.002010778956529167\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03594846758246422\n",
      "Average test loss: 0.0019924315206913483\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0351710831903749\n",
      "Average test loss: 0.0021433203373518255\n",
      "Epoch 19/300\n",
      "Average training loss: 0.034513759596480266\n",
      "Average test loss: 0.001957083235287832\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03387523150775168\n",
      "Average test loss: 0.0018879420222300623\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0332494019832876\n",
      "Average test loss: 0.0019785921305624974\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03267016316453616\n",
      "Average test loss: 0.0018751640524715186\n",
      "Epoch 23/300\n",
      "Average training loss: 0.032095620420244005\n",
      "Average test loss: 0.0018686473859060141\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03155357972615295\n",
      "Average test loss: 0.0018323474609189562\n",
      "Epoch 25/300\n",
      "Average training loss: 0.031001816343930032\n",
      "Average test loss: 0.0018031190446474487\n",
      "Epoch 26/300\n",
      "Average training loss: 0.030514523703191014\n",
      "Average test loss: 0.0018481013625860213\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03013059530655543\n",
      "Average test loss: 0.0017861041504268844\n",
      "Epoch 28/300\n",
      "Average training loss: 0.029611762003766164\n",
      "Average test loss: 0.001761579563634263\n",
      "Epoch 29/300\n",
      "Average training loss: 0.029318760269218022\n",
      "Average test loss: 0.001749428347684443\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028804427633682888\n",
      "Average test loss: 0.0018161689131003288\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02851989573902554\n",
      "Average test loss: 0.0017392263787074222\n",
      "Epoch 32/300\n",
      "Average training loss: 0.028127931838234265\n",
      "Average test loss: 0.0017191943645270334\n",
      "Epoch 33/300\n",
      "Average training loss: 0.027852042372028034\n",
      "Average test loss: 0.001711850481107831\n",
      "Epoch 34/300\n",
      "Average training loss: 0.027580390375521447\n",
      "Average test loss: 0.0017157510608020756\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027292272226678002\n",
      "Average test loss: 0.0017311829085358316\n",
      "Epoch 36/300\n",
      "Average training loss: 0.026980009310775332\n",
      "Average test loss: 0.001716417518340879\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026760769353972542\n",
      "Average test loss: 0.0016875086840656068\n",
      "Epoch 38/300\n",
      "Average training loss: 0.026485963217086263\n",
      "Average test loss: 0.001704736449652248\n",
      "Epoch 39/300\n",
      "Average training loss: 0.026140679745210543\n",
      "Average test loss: 0.0017043854612857103\n",
      "Epoch 40/300\n",
      "Average training loss: 0.025998452328973347\n",
      "Average test loss: 0.0016951278396364715\n",
      "Epoch 41/300\n",
      "Average training loss: 0.025873712359203233\n",
      "Average test loss: 0.0016919361994498306\n",
      "Epoch 42/300\n",
      "Average training loss: 0.025537071112129422\n",
      "Average test loss: 0.0016728347175651126\n",
      "Epoch 43/300\n",
      "Average training loss: 0.025378224621216455\n",
      "Average test loss: 0.0016709693916555908\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02510217927561866\n",
      "Average test loss: 0.0016769137343184816\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024878481790423394\n",
      "Average test loss: 0.0016684641972598102\n",
      "Epoch 46/300\n",
      "Average training loss: 0.024650077126092382\n",
      "Average test loss: 0.0016926484987553623\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02444808219704363\n",
      "Average test loss: 0.0016913948136692246\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02432735878560278\n",
      "Average test loss: 0.001681279959105369\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024128355344136557\n",
      "Average test loss: 0.0017349646656204844\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02390227327081892\n",
      "Average test loss: 0.0017107730220175452\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023740001092354458\n",
      "Average test loss: 0.001717066260158188\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023525480194224253\n",
      "Average test loss: 0.0016787647194642988\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023346890926361084\n",
      "Average test loss: 0.0016890089785059294\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02321149603029092\n",
      "Average test loss: 0.0017235085426105393\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02301427643580569\n",
      "Average test loss: 0.0016790872782261835\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02286939723044634\n",
      "Average test loss: 0.0016873792312625381\n",
      "Epoch 57/300\n",
      "Average training loss: 0.022671004204286468\n",
      "Average test loss: 0.001780575350133909\n",
      "Epoch 58/300\n",
      "Average training loss: 0.022566228325168292\n",
      "Average test loss: 0.0016853754419005578\n",
      "Epoch 59/300\n",
      "Average training loss: 0.022417809384564558\n",
      "Average test loss: 0.0017529908023360703\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02223133546113968\n",
      "Average test loss: 0.0016910895249909825\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022092396350370514\n",
      "Average test loss: 0.001746573143121269\n",
      "Epoch 62/300\n",
      "Average training loss: 0.021954502416981592\n",
      "Average test loss: 0.0017147405552160409\n",
      "Epoch 63/300\n",
      "Average training loss: 0.021773336806231076\n",
      "Average test loss: 0.001699461978756719\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02166194506486257\n",
      "Average test loss: 0.0017783289572430982\n",
      "Epoch 65/300\n",
      "Average training loss: 0.021525498767693836\n",
      "Average test loss: 0.0017107980427228742\n",
      "Epoch 66/300\n",
      "Average training loss: 0.021398566527499094\n",
      "Average test loss: 0.0017103892493372161\n",
      "Epoch 67/300\n",
      "Average training loss: 0.021310707163479593\n",
      "Average test loss: 0.0017321791297031774\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02110434308151404\n",
      "Average test loss: 0.0017934095389727088\n",
      "Epoch 69/300\n",
      "Average training loss: 0.021081546381115913\n",
      "Average test loss: 0.0018282416613979472\n",
      "Epoch 70/300\n",
      "Average training loss: 0.021038073903984492\n",
      "Average test loss: 0.0018566724413798916\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020778082374069425\n",
      "Average test loss: 0.0017400404936634005\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020671823244955805\n",
      "Average test loss: 0.001763492724754744\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020559525185161166\n",
      "Average test loss: 0.0017339347458134094\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020514756047063404\n",
      "Average test loss: 0.0017461770301063855\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02040192462172773\n",
      "Average test loss: 0.002152632575896051\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02026291706164678\n",
      "Average test loss: 0.0017496191119361255\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020170513334373634\n",
      "Average test loss: 0.0018108762620120413\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020067447915673255\n",
      "Average test loss: 0.0017365519447873035\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02004732638100783\n",
      "Average test loss: 0.0017639392212861114\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019909432881408267\n",
      "Average test loss: 0.0017477668158503042\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019839007356100613\n",
      "Average test loss: 0.0017371857195264764\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019730643999245432\n",
      "Average test loss: 0.0017907579135563638\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019660971261560918\n",
      "Average test loss: 0.0017639966611233023\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019566661722130247\n",
      "Average test loss: 0.0017868571510124538\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019453583023614354\n",
      "Average test loss: 0.0018022319372329447\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019403220358822082\n",
      "Average test loss: 0.0018258359469473361\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0193206877824333\n",
      "Average test loss: 0.001807670920000722\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019230961052079994\n",
      "Average test loss: 0.00186049343769749\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01927935278829601\n",
      "Average test loss: 0.0017980925009275475\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01903538845645057\n",
      "Average test loss: 0.0017838209908869532\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019032331853277153\n",
      "Average test loss: 0.0018522193210406435\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01894855524930689\n",
      "Average test loss: 0.0017889262263973554\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018909788691335253\n",
      "Average test loss: 0.0017910168135745659\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018797739169663853\n",
      "Average test loss: 0.0019277574351678293\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018727671515610483\n",
      "Average test loss: 0.001823030670846088\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01870134049985144\n",
      "Average test loss: 0.0018756944501979484\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01860975904431608\n",
      "Average test loss: 0.0018707172818895844\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018549041108952627\n",
      "Average test loss: 0.0018076474542419116\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018550250737203493\n",
      "Average test loss: 0.0018455857827017704\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018426808098951974\n",
      "Average test loss: 0.0018176688367707863\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01838381460474597\n",
      "Average test loss: 0.0017773160737835699\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01833371406959163\n",
      "Average test loss: 0.0018358903715593948\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018274160996079446\n",
      "Average test loss: 0.001813545551771919\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01824884719070461\n",
      "Average test loss: 0.001956949311412043\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01817150326901012\n",
      "Average test loss: 0.0018458841728667419\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01813472479581833\n",
      "Average test loss: 0.0018606763616618182\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018082949308885467\n",
      "Average test loss: 0.001847220006916258\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01802741089132097\n",
      "Average test loss: 0.0018321791622373793\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01797912671830919\n",
      "Average test loss: 0.0019382565641361807\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01794481453879012\n",
      "Average test loss: 0.001873298459375898\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01791570957832866\n",
      "Average test loss: 0.0018559122309088707\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017807173037694562\n",
      "Average test loss: 0.0019166380525049236\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0178255830961797\n",
      "Average test loss: 0.0018613960571173165\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017768998164269658\n",
      "Average test loss: 0.0018292577095950643\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017747370696730085\n",
      "Average test loss: 0.0018743437804902593\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017641793098714616\n",
      "Average test loss: 0.00185213836758501\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017680252994100254\n",
      "Average test loss: 0.0018423572232325871\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017631379255817998\n",
      "Average test loss: 0.001902058582752943\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01757326560219129\n",
      "Average test loss: 0.0018337030989221401\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017448287751939563\n",
      "Average test loss: 0.0018432052419003514\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017439851191308763\n",
      "Average test loss: 0.0017926499602488345\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01742826988796393\n",
      "Average test loss: 0.00202319333764414\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01743538385298517\n",
      "Average test loss: 0.001854097910432352\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017326876166794035\n",
      "Average test loss: 0.0019059557293852171\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017339354873531394\n",
      "Average test loss: 0.0019012805515279372\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017304688452018633\n",
      "Average test loss: 0.0018565522140512863\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01723201741774877\n",
      "Average test loss: 0.0019522394710737797\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01720307640400198\n",
      "Average test loss: 0.0018683400913659068\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017183636241488987\n",
      "Average test loss: 0.001870891670592957\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01711070304363966\n",
      "Average test loss: 0.0019061249155137275\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017096323016617034\n",
      "Average test loss: 0.001886377802118659\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0170556753344006\n",
      "Average test loss: 0.0019780306811961862\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01702300457573599\n",
      "Average test loss: 0.0019147753020127614\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017003594332271152\n",
      "Average test loss: 0.0018718454050314095\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017002267652087743\n",
      "Average test loss: 0.0018573767401071058\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01690139571328958\n",
      "Average test loss: 0.0018430018649022613\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016957584860424202\n",
      "Average test loss: 0.0019228875274873442\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016927242756717736\n",
      "Average test loss: 0.0019343854084403979\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016857019575105773\n",
      "Average test loss: 0.0018684169476230938\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016873422992726168\n",
      "Average test loss: 0.0018833918426599768\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016810506365365453\n",
      "Average test loss: 0.001861180768451757\n",
      "Epoch 142/300\n",
      "Average training loss: 0.016810727165804968\n",
      "Average test loss: 0.0018816521255713371\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01677662518620491\n",
      "Average test loss: 0.0018733928710636166\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01668090792828136\n",
      "Average test loss: 0.001822764667061468\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016722364933126503\n",
      "Average test loss: 0.0018868023259565234\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016670777168538837\n",
      "Average test loss: 0.0018648814768013027\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016616048980090354\n",
      "Average test loss: 0.001852861459677418\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016591253253320854\n",
      "Average test loss: 0.0019358285376802086\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01656220437751876\n",
      "Average test loss: 0.001955692885029647\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01657933246427112\n",
      "Average test loss: 0.002001557311974466\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01654820614390903\n",
      "Average test loss: 0.001920247232955363\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016485333013865684\n",
      "Average test loss: 0.0019235832693262232\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01647820105155309\n",
      "Average test loss: 0.0019056514278054236\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016488073498838476\n",
      "Average test loss: 0.0019197988131393988\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016419556549853748\n",
      "Average test loss: 0.002017312205189632\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016426314898663096\n",
      "Average test loss: 0.0018312270732906957\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01641914642188284\n",
      "Average test loss: 0.0019477091193613079\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016392933810750642\n",
      "Average test loss: 0.0018810483776032926\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016301879482136832\n",
      "Average test loss: 0.001887453104576303\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016399548414680692\n",
      "Average test loss: 0.0019097688936938841\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01623787749144766\n",
      "Average test loss: 0.0019419952655831973\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016242812674078677\n",
      "Average test loss: 0.00192964730784297\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01623990291853746\n",
      "Average test loss: 0.0018675252412342362\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016237538450294072\n",
      "Average test loss: 0.0018625612954298655\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016167282298207283\n",
      "Average test loss: 0.0019049671658625206\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016197361823585298\n",
      "Average test loss: 0.001943643887527287\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016158170950081614\n",
      "Average test loss: 0.0019474310094697608\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016092678431007597\n",
      "Average test loss: 0.001978017139972912\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016170864718655746\n",
      "Average test loss: 0.001936800186211864\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016096136319968434\n",
      "Average test loss: 0.0018914226767503552\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016051070382197698\n",
      "Average test loss: 0.0020462169301592643\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016086237331231434\n",
      "Average test loss: 0.0020096688439241715\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016074594875176748\n",
      "Average test loss: 0.001932222884769241\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016032456330127186\n",
      "Average test loss: 0.002034280673497253\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01605056543648243\n",
      "Average test loss: 0.0019191389297031694\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015928602122598223\n",
      "Average test loss: 0.0019529835428628657\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015992770098977618\n",
      "Average test loss: 0.001929816571995616\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015932272625466187\n",
      "Average test loss: 0.0019799672113731503\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01592249129547013\n",
      "Average test loss: 0.0019175095949321985\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015878426790237425\n",
      "Average test loss: 0.001968942455947399\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015870527158180874\n",
      "Average test loss: 0.0020284403569789397\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015861578166484834\n",
      "Average test loss: 0.001919262760836217\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01581785370906194\n",
      "Average test loss: 0.0020016939360648395\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015835417788061832\n",
      "Average test loss: 0.0019077543727018767\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01583040272858408\n",
      "Average test loss: 0.0020226653726357555\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015804214749899174\n",
      "Average test loss: 0.001970864166919556\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015807281446953616\n",
      "Average test loss: 0.00192169697686202\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01573447166548835\n",
      "Average test loss: 0.0019575058904786904\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0158167432397604\n",
      "Average test loss: 0.001907746272575524\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015745424864192805\n",
      "Average test loss: 0.0019796851502938404\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015701046557890043\n",
      "Average test loss: 0.0019326076043976678\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015673224659429656\n",
      "Average test loss: 0.0019484573481604458\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01570311614208751\n",
      "Average test loss: 0.0019275540348349346\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01567424842549695\n",
      "Average test loss: 0.002002975460038417\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015644763890239926\n",
      "Average test loss: 0.0019521333238937788\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015623832823501693\n",
      "Average test loss: 0.0019847710964580375\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01566022822757562\n",
      "Average test loss: 0.001967544992764791\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015565979819330904\n",
      "Average test loss: 0.001973882084660646\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015573176571064525\n",
      "Average test loss: 0.0019028400337944428\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015547748727930917\n",
      "Average test loss: 0.001865770390878121\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01554003432724211\n",
      "Average test loss: 0.001979427265624205\n",
      "Epoch 202/300\n",
      "Average training loss: 0.015538703046739101\n",
      "Average test loss: 0.0019374079486976067\n",
      "Epoch 203/300\n",
      "Average training loss: 0.015530255121489365\n",
      "Average test loss: 0.0019333832280503378\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01553896205044455\n",
      "Average test loss: 0.0019609209310470356\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015504963033729129\n",
      "Average test loss: 0.001958559311305483\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01548970944682757\n",
      "Average test loss: 0.001965832750623425\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015509586976634132\n",
      "Average test loss: 0.0019700269451261394\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015442472150756253\n",
      "Average test loss: 0.0020465723886154592\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015478149814738168\n",
      "Average test loss: 0.00199804752961629\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01544230858816041\n",
      "Average test loss: 0.001984981950579418\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015427092432147927\n",
      "Average test loss: 0.0020120518513851698\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01539828852398528\n",
      "Average test loss: 0.001968953496673041\n",
      "Epoch 213/300\n",
      "Average training loss: 0.015386250221067006\n",
      "Average test loss: 0.002021045883807043\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01533593413896031\n",
      "Average test loss: 0.0019522062276179592\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015350340528620615\n",
      "Average test loss: 0.0019873914958702193\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015334181722667482\n",
      "Average test loss: 0.0019296976125074757\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015347270687421163\n",
      "Average test loss: 0.0019618839861618147\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015320244820581542\n",
      "Average test loss: 0.0020360912940361434\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015303276110026572\n",
      "Average test loss: 0.001989903226080868\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01533577260375023\n",
      "Average test loss: 0.0019605057150539427\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015300505927039517\n",
      "Average test loss: 0.00197059236239228\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01524359046916167\n",
      "Average test loss: 0.0019156494619738725\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01522603398313125\n",
      "Average test loss: 0.0020174699794087143\n",
      "Epoch 224/300\n",
      "Average training loss: 0.015257156155175634\n",
      "Average test loss: 0.0020541586791061692\n",
      "Epoch 225/300\n",
      "Average training loss: 0.015205227720240752\n",
      "Average test loss: 0.0019236227470553584\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015249021461440457\n",
      "Average test loss: 0.0019924230016767977\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015182762655119102\n",
      "Average test loss: 0.001967456569067306\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015150768638485008\n",
      "Average test loss: 0.0020219995035893387\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015183145316938559\n",
      "Average test loss: 0.001951962997412516\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01516088965949085\n",
      "Average test loss: 0.002046048143464658\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015158894843525356\n",
      "Average test loss: 0.0019180807441266047\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01511449198342032\n",
      "Average test loss: 0.0019481098933352365\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01515088048328956\n",
      "Average test loss: 0.0019913080553006796\n",
      "Epoch 234/300\n",
      "Average training loss: 0.015121131383710438\n",
      "Average test loss: 0.002013086638102929\n",
      "Epoch 235/300\n",
      "Average training loss: 0.015100962557726437\n",
      "Average test loss: 0.0019100397738317648\n",
      "Epoch 236/300\n",
      "Average training loss: 0.015092500098877483\n",
      "Average test loss: 0.0020787821122341685\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015081555479102664\n",
      "Average test loss: 0.0019519226075046592\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015042662529481781\n",
      "Average test loss: 0.0019604684782938823\n",
      "Epoch 239/300\n",
      "Average training loss: 0.015064161635107464\n",
      "Average test loss: 0.0019624264744213886\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015041873212489816\n",
      "Average test loss: 0.0020187458606022927\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015030005027022626\n",
      "Average test loss: 0.002010129456511802\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015033149427423875\n",
      "Average test loss: 0.0020096286752571664\n",
      "Epoch 243/300\n",
      "Average training loss: 0.015015577619274457\n",
      "Average test loss: 0.0019732811564786568\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014975731704798009\n",
      "Average test loss: 0.0019774468561841382\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014994957658151786\n",
      "Average test loss: 0.0019834502798815566\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014965122611986266\n",
      "Average test loss: 0.0019290027351429066\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014974213396509489\n",
      "Average test loss: 0.0019856687804891003\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014967442434695033\n",
      "Average test loss: 0.0019210406481805775\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014962555728024907\n",
      "Average test loss: 0.001963284428541859\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014933665262328253\n",
      "Average test loss: 0.0019179152212002212\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014924687219990624\n",
      "Average test loss: 0.0019979262159516412\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01492895413024558\n",
      "Average test loss: 0.002024613765999675\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014898306188484032\n",
      "Average test loss: 0.0018793659758650594\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014917426041430898\n",
      "Average test loss: 0.0020277905106130573\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01488350988427798\n",
      "Average test loss: 0.0019660834945324393\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014880352952414088\n",
      "Average test loss: 0.0020371689261454674\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014882318998376528\n",
      "Average test loss: 0.00199961443638636\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014866003225247065\n",
      "Average test loss: 0.0020080292485654353\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014857300450404485\n",
      "Average test loss: 0.0019754152429393595\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014859980212317573\n",
      "Average test loss: 0.0019262386390732395\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014846481605536407\n",
      "Average test loss: 0.002131597460558017\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01481189220564233\n",
      "Average test loss: 0.002018622053683632\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014800646016995113\n",
      "Average test loss: 0.0019421033899610241\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01478833467927244\n",
      "Average test loss: 0.001970436902509795\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014797810435295105\n",
      "Average test loss: 0.0020607111876209576\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014789022710588243\n",
      "Average test loss: 0.00201987290289253\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014774627902441554\n",
      "Average test loss: 0.001982892097594837\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014768520552251073\n",
      "Average test loss: 0.0020153428901814752\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01476453902075688\n",
      "Average test loss: 0.0021248384749309886\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014710442906452549\n",
      "Average test loss: 0.0019580567271138233\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014755607787933615\n",
      "Average test loss: 0.0020122954001029333\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014734310287568305\n",
      "Average test loss: 0.0019802779737446044\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014721972281734148\n",
      "Average test loss: 0.001969539137557149\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014705040762821833\n",
      "Average test loss: 0.0020043885245298347\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014663415243228277\n",
      "Average test loss: 0.001992594459404548\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01466446884555949\n",
      "Average test loss: 0.0019795642223002182\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014690782016350164\n",
      "Average test loss: 0.0020345568714870347\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014652936927974224\n",
      "Average test loss: 0.0019951860312786367\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014658781655960613\n",
      "Average test loss: 0.0019981150287720893\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014686435244977473\n",
      "Average test loss: 0.001998785508796573\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014632332079940372\n",
      "Average test loss: 0.001995767413224611\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014630102834767766\n",
      "Average test loss: 0.0020254620656164155\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014632794871098465\n",
      "Average test loss: 0.0020931084365066557\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014607597346935007\n",
      "Average test loss: 0.001980222318114506\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014607447377509542\n",
      "Average test loss: 0.0019909716736421818\n",
      "Epoch 286/300\n",
      "Average training loss: 0.014603959508240222\n",
      "Average test loss: 0.0020339958866437274\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014586337810589208\n",
      "Average test loss: 0.002042049620404012\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01460881391995483\n",
      "Average test loss: 0.0019455553784759507\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014575980390939448\n",
      "Average test loss: 0.002027060105568833\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014524522033830483\n",
      "Average test loss: 0.002000514967367053\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014519514252742132\n",
      "Average test loss: 0.0020108402261717452\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014552345390121142\n",
      "Average test loss: 0.0019878345891419383\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014570661008358002\n",
      "Average test loss: 0.0019949200773197746\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014601874192555745\n",
      "Average test loss: 0.0019998594319654834\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014483887038297123\n",
      "Average test loss: 0.0020694332879243626\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014527108868790998\n",
      "Average test loss: 0.001957725014951494\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014501542118688425\n",
      "Average test loss: 0.002025710776034329\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014542863190174104\n",
      "Average test loss: 0.0019860854622804455\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014520377098686166\n",
      "Average test loss: 0.002088000417376558\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014491423483524057\n",
      "Average test loss: 0.00207360519706789\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_32_Depth5/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.79\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.46\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.74\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.37\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.91\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.25\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.36\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.611238171047635\n",
      "Average test loss: 0.0058268382638278935\n",
      "Epoch 2/300\n",
      "Average training loss: 4.36401690059238\n",
      "Average test loss: 0.005199668066783084\n",
      "Epoch 3/300\n",
      "Average training loss: 3.242692593044705\n",
      "Average test loss: 0.0049144512642588884\n",
      "Epoch 4/300\n",
      "Average training loss: 2.455190651787652\n",
      "Average test loss: 0.005047742110987504\n",
      "Epoch 5/300\n",
      "Average training loss: 1.8665049104690552\n",
      "Average test loss: 0.007264497928735283\n",
      "Epoch 6/300\n",
      "Average training loss: 1.4589252036412557\n",
      "Average test loss: 0.004670723306636016\n",
      "Epoch 7/300\n",
      "Average training loss: 1.2261941148969862\n",
      "Average test loss: 0.004662390823372537\n",
      "Epoch 8/300\n",
      "Average training loss: 1.021850704352061\n",
      "Average test loss: 0.00458984771081143\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8324774016274347\n",
      "Average test loss: 0.004619019539405902\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6846325942145454\n",
      "Average test loss: 0.004617574618508418\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5623610128561656\n",
      "Average test loss: 0.004492982859205868\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4615436606142256\n",
      "Average test loss: 0.004474325583626827\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3862159725030263\n",
      "Average test loss: 0.004451263763838345\n",
      "Epoch 14/300\n",
      "Average training loss: 0.33248941002951726\n",
      "Average test loss: 0.004462283585634497\n",
      "Epoch 15/300\n",
      "Average training loss: 0.29103720196088156\n",
      "Average test loss: 0.004496194388924374\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2585508342583974\n",
      "Average test loss: 0.0044016527512835135\n",
      "Epoch 17/300\n",
      "Average training loss: 0.233485163503223\n",
      "Average test loss: 0.004420416505800353\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2131638585329056\n",
      "Average test loss: 0.004386585062369704\n",
      "Epoch 19/300\n",
      "Average training loss: 0.19688953646024068\n",
      "Average test loss: 0.004353343324114879\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1840117648575041\n",
      "Average test loss: 0.00436764613373412\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1734325510263443\n",
      "Average test loss: 0.004347765970147318\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1645759186413553\n",
      "Average test loss: 0.004364484123057789\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1573241542842653\n",
      "Average test loss: 0.004317981693893671\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15183919594022963\n",
      "Average test loss: 0.004317478118464351\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14739476251602174\n",
      "Average test loss: 0.004306056499895123\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1437708419760068\n",
      "Average test loss: 0.004318431333949169\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14072637412283157\n",
      "Average test loss: 0.004284775032972296\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1381985404756334\n",
      "Average test loss: 0.004299040041449998\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13606311631202697\n",
      "Average test loss: 0.004293085731152032\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13419485341178047\n",
      "Average test loss: 0.004262750046948592\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13282586487796572\n",
      "Average test loss: 0.004314490160801344\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13180065184169346\n",
      "Average test loss: 0.0042642365044189825\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1309322221941418\n",
      "Average test loss: 0.004271339447134071\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1300719388657146\n",
      "Average test loss: 0.004259726428737243\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1294350130359332\n",
      "Average test loss: 0.004216233155379692\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12877633498774635\n",
      "Average test loss: 0.004253145532889499\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1282647286189927\n",
      "Average test loss: 0.0042200347058888935\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1278709260755115\n",
      "Average test loss: 0.004211312878876924\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12725621914201313\n",
      "Average test loss: 0.004239760677226715\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1268859096897973\n",
      "Average test loss: 0.004219054588427146\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12647725747028987\n",
      "Average test loss: 0.004221165685604016\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12627805761496225\n",
      "Average test loss: 0.004195243907264537\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12575081997447543\n",
      "Average test loss: 0.00419525575513641\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12552099047104517\n",
      "Average test loss: 0.004313172795292404\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1250665893289778\n",
      "Average test loss: 0.00420887800761395\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12477228209045198\n",
      "Average test loss: 0.004193298335497578\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12447094193432066\n",
      "Average test loss: 0.004201912089768383\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12410064760512776\n",
      "Average test loss: 0.004198017213079664\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12372770655155182\n",
      "Average test loss: 0.004245029110668434\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12353679268227684\n",
      "Average test loss: 0.004213449800180064\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12325383486019241\n",
      "Average test loss: 0.004197841347091728\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1228942848775122\n",
      "Average test loss: 0.004198523598205712\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12248755141099293\n",
      "Average test loss: 0.004181633389244477\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12219145011901855\n",
      "Average test loss: 0.004190729101498922\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12201796833674113\n",
      "Average test loss: 0.004209351911726925\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12170533694823583\n",
      "Average test loss: 0.004257311210450199\n",
      "Epoch 57/300\n",
      "Average training loss: 0.12140518239471647\n",
      "Average test loss: 0.00418918809791406\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12115513008832932\n",
      "Average test loss: 0.004207343093430002\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1208012807700369\n",
      "Average test loss: 0.004186276574929555\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12071275669336319\n",
      "Average test loss: 0.004181191430323653\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12018972137901518\n",
      "Average test loss: 0.004211033049349984\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12001305585437351\n",
      "Average test loss: 0.004206279693171382\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11973207766479917\n",
      "Average test loss: 0.004203811595009433\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11937271744012833\n",
      "Average test loss: 0.004207506027900511\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11924102407031589\n",
      "Average test loss: 0.004203673395638665\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11892253240611818\n",
      "Average test loss: 0.004216197736561299\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11861317571666506\n",
      "Average test loss: 0.0042434179000556465\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11847426449590259\n",
      "Average test loss: 0.00419079179979033\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1179629089170032\n",
      "Average test loss: 0.004207199748191569\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11778039362364345\n",
      "Average test loss: 0.004290954068303108\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11774852834145227\n",
      "Average test loss: 0.004252983312432965\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11725067836046218\n",
      "Average test loss: 0.0042061734360953175\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11689630962080426\n",
      "Average test loss: 0.0042349857058789995\n",
      "Epoch 74/300\n",
      "Average training loss: 0.1166002631187439\n",
      "Average test loss: 0.004257192194254862\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11633028687371148\n",
      "Average test loss: 0.0042131941049463216\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11612744653224945\n",
      "Average test loss: 0.004198179145861003\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1157996539539761\n",
      "Average test loss: 0.004252216718677018\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11545234321223365\n",
      "Average test loss: 0.004199540067464113\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1149859407875273\n",
      "Average test loss: 0.004325826381436653\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11476162413756053\n",
      "Average test loss: 0.004226828675406674\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11447766605350707\n",
      "Average test loss: 0.004282467858865857\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11416448947456148\n",
      "Average test loss: 0.004254259214012159\n",
      "Epoch 83/300\n",
      "Average training loss: 0.113821848154068\n",
      "Average test loss: 0.0042491462751188215\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11360524994797176\n",
      "Average test loss: 0.004249020884020461\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11341071861982345\n",
      "Average test loss: 0.004335503235045407\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11294188435210122\n",
      "Average test loss: 0.004352639265358448\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11252574822968907\n",
      "Average test loss: 0.004278071294021276\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11226568564441469\n",
      "Average test loss: 0.004314899329923921\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11194262194633484\n",
      "Average test loss: 0.004311368369807799\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11171565178367826\n",
      "Average test loss: 0.004351457181076209\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11138933726151784\n",
      "Average test loss: 0.004311098598771625\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11113994766606225\n",
      "Average test loss: 0.0043649180939214095\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11052764501836565\n",
      "Average test loss: 0.004359115612589651\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11065362468030718\n",
      "Average test loss: 0.004367129882176717\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11001768457889557\n",
      "Average test loss: 0.004436185030059682\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10968295563591851\n",
      "Average test loss: 0.004410004226697815\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10939880171749326\n",
      "Average test loss: 0.004296187666555246\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1090141382879681\n",
      "Average test loss: 0.004375192740518186\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10868813851144579\n",
      "Average test loss: 0.004523867518123653\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10864049575726191\n",
      "Average test loss: 0.004357751619484689\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10812603894869487\n",
      "Average test loss: 0.00431927104625437\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10778709639443292\n",
      "Average test loss: 0.004474132193252444\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10744438599877887\n",
      "Average test loss: 0.004409376944311791\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10730919967095057\n",
      "Average test loss: 0.004447242324137025\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10708009688721763\n",
      "Average test loss: 0.004476372607052326\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10689351742135154\n",
      "Average test loss: 0.004460319050484233\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10659323388006953\n",
      "Average test loss: 0.004551139093521568\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10605817977587383\n",
      "Average test loss: 0.0043973108571436674\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10572275828652912\n",
      "Average test loss: 0.004366138687978189\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10549822823206584\n",
      "Average test loss: 0.004385696531997787\n",
      "Epoch 111/300\n",
      "Average training loss: 0.1054456792473793\n",
      "Average test loss: 0.004392077463161614\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10508331158426072\n",
      "Average test loss: 0.0044371421668264605\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10467114469740126\n",
      "Average test loss: 0.004588604400141372\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10435040477911631\n",
      "Average test loss: 0.00462805337831378\n",
      "Epoch 115/300\n",
      "Average training loss: 0.1043187088171641\n",
      "Average test loss: 0.004436883834294147\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10395087069272994\n",
      "Average test loss: 0.00437672912557092\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10389426490333345\n",
      "Average test loss: 0.004606874932017591\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10352122786972258\n",
      "Average test loss: 0.004390250136868821\n",
      "Epoch 119/300\n",
      "Average training loss: 0.1031375420888265\n",
      "Average test loss: 0.004411120918993321\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10282154414388868\n",
      "Average test loss: 0.00453156362960322\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10277878122197256\n",
      "Average test loss: 0.0044044481209582756\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10266943352089988\n",
      "Average test loss: 0.004519299070868227\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10248209963904487\n",
      "Average test loss: 0.004393597741507821\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10202747007873324\n",
      "Average test loss: 0.004423004413644473\n",
      "Epoch 125/300\n",
      "Average training loss: 0.1017003324230512\n",
      "Average test loss: 0.004554577436712053\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10153507610824374\n",
      "Average test loss: 0.0044197134520444605\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10131024490462409\n",
      "Average test loss: 0.004436031982923547\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10111948205365075\n",
      "Average test loss: 0.0044883687939080924\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10089067193203502\n",
      "Average test loss: 0.004540170893487003\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10054729506373405\n",
      "Average test loss: 0.004473327226315936\n",
      "Epoch 131/300\n",
      "Average training loss: 0.1004650904999839\n",
      "Average test loss: 0.004547774881745378\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10019191730684704\n",
      "Average test loss: 0.0044657183337128824\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09997605076763365\n",
      "Average test loss: 0.004496033947293957\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09985635417699813\n",
      "Average test loss: 0.0045604884454773535\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09971351856655544\n",
      "Average test loss: 0.004667308696028259\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09941128687726127\n",
      "Average test loss: 0.00449986840515501\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09950955576366849\n",
      "Average test loss: 0.004530787149651183\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10008815704451667\n",
      "Average test loss: 0.004557321147993207\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09869915415843328\n",
      "Average test loss: 0.004557850461453199\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09842848626772563\n",
      "Average test loss: 0.004540693570342329\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09841736273633109\n",
      "Average test loss: 0.004590796466088958\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09815850969817903\n",
      "Average test loss: 0.004582613718592458\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0983691990772883\n",
      "Average test loss: 0.004493254531588819\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09795452810658349\n",
      "Average test loss: 0.0045712982366482415\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09755936545133591\n",
      "Average test loss: 0.0045011004408200585\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09785273903608323\n",
      "Average test loss: 0.004736384188549386\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0972701087726487\n",
      "Average test loss: 0.0045457749298463265\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09710358573330773\n",
      "Average test loss: 0.0046275676629609535\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09687398253546821\n",
      "Average test loss: 0.004444591205774082\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09750749907890956\n",
      "Average test loss: 0.004517581655126479\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0966973773042361\n",
      "Average test loss: 0.00467534930134813\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09630432817671034\n",
      "Average test loss: 0.004530523946508765\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09637465790245268\n",
      "Average test loss: 0.004508488347753883\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09610882518688837\n",
      "Average test loss: 0.004558109001153045\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09632036771376928\n",
      "Average test loss: 0.004674519744805164\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09572197246551514\n",
      "Average test loss: 0.004591779414564371\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09600466717282931\n",
      "Average test loss: 0.004530699289093415\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09553220852878358\n",
      "Average test loss: 0.004579991336911917\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0952140909433365\n",
      "Average test loss: 0.004590903023464812\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0952831028368738\n",
      "Average test loss: 0.004531369604377283\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09497804811265734\n",
      "Average test loss: 0.004911433729446597\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09503824011153646\n",
      "Average test loss: 0.004612609722961982\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0973699575662613\n",
      "Average test loss: 0.004602717068460252\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09544355932871501\n",
      "Average test loss: 0.004605268739163876\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0940863877468639\n",
      "Average test loss: 0.004633475880655978\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0938752837644683\n",
      "Average test loss: 0.004800505214681228\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09402995485067367\n",
      "Average test loss: 0.004543113872822788\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09409056219127443\n",
      "Average test loss: 0.004770743954512808\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0938654783434338\n",
      "Average test loss: 0.004656277963270744\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09394127290116416\n",
      "Average test loss: 0.0045101032964885235\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0936419490178426\n",
      "Average test loss: 0.004581539489328861\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09339397084050709\n",
      "Average test loss: 0.00468558071512315\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09355604134003322\n",
      "Average test loss: 0.004686310137311618\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09312129694223403\n",
      "Average test loss: 0.004703203989813725\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09300257596042422\n",
      "Average test loss: 0.004539071491815977\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09329237424665027\n",
      "Average test loss: 0.0045179069174660575\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09280156510406071\n",
      "Average test loss: 0.004725822056747145\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09302279278304842\n",
      "Average test loss: 0.004843109738909536\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09241464650630951\n",
      "Average test loss: 0.004649264433731635\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09254210359520382\n",
      "Average test loss: 0.004850600609762801\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09230193061961069\n",
      "Average test loss: 0.004652243729680776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09238618890444437\n",
      "Average test loss: 0.004605354464612901\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0920509413878123\n",
      "Average test loss: 0.004663895095802016\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09193197337786356\n",
      "Average test loss: 0.004697220251585046\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09207067390282948\n",
      "Average test loss: 0.004723005752182669\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09191395910580953\n",
      "Average test loss: 0.004566129640986522\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09163127127620908\n",
      "Average test loss: 0.004679914234826962\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09157642130719292\n",
      "Average test loss: 0.0046765715746829905\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09131416722800996\n",
      "Average test loss: 0.004575588213900725\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0917855720586247\n",
      "Average test loss: 0.004643151233179702\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0913509102066358\n",
      "Average test loss: 0.004587751579781373\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0909091348250707\n",
      "Average test loss: 0.00475776629936364\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09066566178533766\n",
      "Average test loss: 0.004615435450441308\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09082792902655072\n",
      "Average test loss: 0.004526930254987544\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09059146922826768\n",
      "Average test loss: 0.004619134484479825\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09070647350284788\n",
      "Average test loss: 0.004710372129041287\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09064990327093336\n",
      "Average test loss: 0.004597522549745109\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09099044226937823\n",
      "Average test loss: 0.004643230144141449\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09017086899280548\n",
      "Average test loss: 0.004604076939738459\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0900404704478052\n",
      "Average test loss: 0.004614805127597517\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09009609576728608\n",
      "Average test loss: 0.004791318220396836\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08984938493039873\n",
      "Average test loss: 0.004685113612562418\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08993036185370551\n",
      "Average test loss: 0.004602673062433799\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08959258945120706\n",
      "Average test loss: 0.00472667923114366\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08952808162901137\n",
      "Average test loss: 0.0047293474831514885\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0897979041867786\n",
      "Average test loss: 0.0046531767410536605\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0894477939373917\n",
      "Average test loss: 0.0048278276556067996\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08934377823935614\n",
      "Average test loss: 0.004597383553783099\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08971280587381787\n",
      "Average test loss: 0.004565988306783967\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08912334865993923\n",
      "Average test loss: 0.0046753200300865706\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0887526598042912\n",
      "Average test loss: 0.004649749688597189\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08871003017822901\n",
      "Average test loss: 0.0047989435758855606\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08952254339390331\n",
      "Average test loss: 0.0047036542259156705\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08863602545526293\n",
      "Average test loss: 0.004674666337048014\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08853433407677544\n",
      "Average test loss: 0.0046783176913029615\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08839540815353393\n",
      "Average test loss: 0.004602845316545831\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08833791815572314\n",
      "Average test loss: 0.004635886364099053\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08845933083030913\n",
      "Average test loss: 0.0046224349014874965\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08831150540378359\n",
      "Average test loss: 0.004514335110369656\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08811042662130462\n",
      "Average test loss: 0.004579908553510904\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08796098567379845\n",
      "Average test loss: 0.004600590167567134\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08842932383881674\n",
      "Average test loss: 0.0046004117193321386\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09043696444233258\n",
      "Average test loss: 0.004702195504887236\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08743454241752624\n",
      "Average test loss: 0.004558583627972338\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08724093596802818\n",
      "Average test loss: 0.004646400790661574\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08723454980055491\n",
      "Average test loss: 0.00464652462862432\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08759673283497492\n",
      "Average test loss: 0.004746660725110107\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0876032316883405\n",
      "Average test loss: 0.0046588171751548845\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08723792241679297\n",
      "Average test loss: 0.004658280396627055\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08734767245584064\n",
      "Average test loss: 0.004638649245517121\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08745346503787571\n",
      "Average test loss: 0.004683930264578925\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08739390596416262\n",
      "Average test loss: 0.004641042692379819\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08679076986842685\n",
      "Average test loss: 0.004670408625362648\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08679179056485493\n",
      "Average test loss: 0.0045875174017209145\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08702469003862805\n",
      "Average test loss: 0.00465937600698736\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08684258329868316\n",
      "Average test loss: 0.004627250284163488\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08653646802571084\n",
      "Average test loss: 0.0046296017099585795\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08654753722084893\n",
      "Average test loss: 0.004711171764466498\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08668661944733726\n",
      "Average test loss: 0.004669917684048414\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08641926739613215\n",
      "Average test loss: 0.004530520571809676\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0862642833325598\n",
      "Average test loss: 0.004729342890282472\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08653340686692132\n",
      "Average test loss: 0.004866357974915041\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08647091305918164\n",
      "Average test loss: 0.0048077176101505754\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08604550092087852\n",
      "Average test loss: 0.004636289039833678\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08590442309776943\n",
      "Average test loss: 0.004852380927238199\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08591370395157072\n",
      "Average test loss: 0.004687932161407339\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08591213929653167\n",
      "Average test loss: 0.004747581116441224\n",
      "Epoch 248/300\n",
      "Average training loss: 0.085993685901165\n",
      "Average test loss: 0.004589009995882709\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08573074752754635\n",
      "Average test loss: 0.0046414794408612785\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0853971620798111\n",
      "Average test loss: 0.004619121093509926\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08571333751413557\n",
      "Average test loss: 0.004675315080417527\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08612882322735256\n",
      "Average test loss: 0.004559754446148872\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08537467069096036\n",
      "Average test loss: 0.0047575587953130405\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08544163948297501\n",
      "Average test loss: 0.0045603584222909475\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08494349064429602\n",
      "Average test loss: 0.004650927919273575\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08565005724959904\n",
      "Average test loss: 0.00461628285360833\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08520284008979798\n",
      "Average test loss: 0.00472991154798203\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08498325091600419\n",
      "Average test loss: 0.004714711479635703\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08495195000039206\n",
      "Average test loss: 0.004706929308672746\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08498490417003632\n",
      "Average test loss: 0.004720055452237527\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08482351951466666\n",
      "Average test loss: 0.004686851843363709\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08469605159759522\n",
      "Average test loss: 0.004687020272016526\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08471190238661236\n",
      "Average test loss: 0.00466618898179796\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08468866243627336\n",
      "Average test loss: 0.004684322123726209\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08442491671774123\n",
      "Average test loss: 0.004753970306366682\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08465093550417159\n",
      "Average test loss: 0.004717406761315134\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08430538272857666\n",
      "Average test loss: 0.004966350987967517\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08467748251888486\n",
      "Average test loss: 0.004642286686847607\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0841977508465449\n",
      "Average test loss: 0.00455831705696053\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08412769553396438\n",
      "Average test loss: 0.004779503314238456\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08419043731027179\n",
      "Average test loss: 0.004682774344666137\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08451159360011419\n",
      "Average test loss: 0.004568783364155227\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08393237639798058\n",
      "Average test loss: 0.00470929226734572\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08399601578050189\n",
      "Average test loss: 0.004703444574856096\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08390419052706824\n",
      "Average test loss: 0.00460535046706597\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08392378114991718\n",
      "Average test loss: 0.004707831141642398\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08377731549739838\n",
      "Average test loss: 0.0046846493122478326\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0841485140853458\n",
      "Average test loss: 0.004554268535226584\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08387631791498926\n",
      "Average test loss: 0.004702718327028884\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08340479032860862\n",
      "Average test loss: 0.0048512874382237595\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08339708116650581\n",
      "Average test loss: 0.004645555804173152\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08336489376094607\n",
      "Average test loss: 0.004661157018194596\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08317746709121598\n",
      "Average test loss: 0.004725473155577978\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08356280369228787\n",
      "Average test loss: 0.004705327169348796\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08334484784801802\n",
      "Average test loss: 0.004785250313993957\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08333832685814964\n",
      "Average test loss: 0.0048174615626533825\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08312112182709906\n",
      "Average test loss: 0.004711577941974004\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08292941939168506\n",
      "Average test loss: 0.0045330529871086276\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08275861612293456\n",
      "Average test loss: 0.004877791130294402\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08293903446528647\n",
      "Average test loss: 0.00496224355780416\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08283177882432938\n",
      "Average test loss: 0.004573659751978185\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08298780991633734\n",
      "Average test loss: 0.004705779466157158\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08281916158066856\n",
      "Average test loss: 0.004617603627757894\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08281180973185433\n",
      "Average test loss: 0.0046522487275716335\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08270312208599515\n",
      "Average test loss: 0.00465482552929057\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0823715067240927\n",
      "Average test loss: 0.0046788278251058526\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08255003698335754\n",
      "Average test loss: 0.004639447491616011\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08230151791705026\n",
      "Average test loss: 0.004639423320276869\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08221005546384387\n",
      "Average test loss: 0.004714133134732644\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08321304320626789\n",
      "Average test loss: 0.004807068187950386\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.80144174363878\n",
      "Average test loss: 1.2597156808641221\n",
      "Epoch 2/300\n",
      "Average training loss: 4.672303236219618\n",
      "Average test loss: 0.004735987325509389\n",
      "Epoch 3/300\n",
      "Average training loss: 3.328421936670939\n",
      "Average test loss: 0.004313770521846083\n",
      "Epoch 4/300\n",
      "Average training loss: 2.8672466214497883\n",
      "Average test loss: 0.004160518707086643\n",
      "Epoch 5/300\n",
      "Average training loss: 2.236081927511427\n",
      "Average test loss: 0.006232686575916078\n",
      "Epoch 6/300\n",
      "Average training loss: 1.8240469484329223\n",
      "Average test loss: 0.004045975002667142\n",
      "Epoch 7/300\n",
      "Average training loss: 1.5620623322592841\n",
      "Average test loss: 0.004063462340583404\n",
      "Epoch 8/300\n",
      "Average training loss: 1.2204894811842177\n",
      "Average test loss: 0.0038338535279035567\n",
      "Epoch 9/300\n",
      "Average training loss: 1.0178762361208598\n",
      "Average test loss: 0.0037783231642097236\n",
      "Epoch 10/300\n",
      "Average training loss: 0.8640726869901021\n",
      "Average test loss: 0.0037288531632059152\n",
      "Epoch 11/300\n",
      "Average training loss: 0.71635042280621\n",
      "Average test loss: 0.003758307876893216\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5980215930408902\n",
      "Average test loss: 0.0036302177494184837\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5031262012322744\n",
      "Average test loss: 0.0035846998256941634\n",
      "Epoch 14/300\n",
      "Average training loss: 0.42203912083307904\n",
      "Average test loss: 0.003576378643512726\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3570770304997762\n",
      "Average test loss: 0.0034949835158056683\n",
      "Epoch 16/300\n",
      "Average training loss: 0.3075743672317929\n",
      "Average test loss: 0.0035147680793371464\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2667682232591841\n",
      "Average test loss: 0.003452193218179875\n",
      "Epoch 18/300\n",
      "Average training loss: 0.23420141426722207\n",
      "Average test loss: 0.0033938744893918433\n",
      "Epoch 19/300\n",
      "Average training loss: 0.20784930466281043\n",
      "Average test loss: 0.0034125997436543305\n",
      "Epoch 20/300\n",
      "Average training loss: 0.18636446854803299\n",
      "Average test loss: 0.0033700959752831195\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16992551526758407\n",
      "Average test loss: 0.0033006064945624936\n",
      "Epoch 22/300\n",
      "Average training loss: 0.15678524838553534\n",
      "Average test loss: 0.0033479279963713553\n",
      "Epoch 23/300\n",
      "Average training loss: 0.146377329521709\n",
      "Average test loss: 0.0032858941310809717\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13787799362341563\n",
      "Average test loss: 0.0032507403598477443\n",
      "Epoch 25/300\n",
      "Average training loss: 0.13090986053148906\n",
      "Average test loss: 0.0032555306444151536\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12490349802705977\n",
      "Average test loss: 0.003210332166403532\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12027100735240512\n",
      "Average test loss: 0.0032471707454986044\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1162392068306605\n",
      "Average test loss: 0.0031978153019315667\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11328433267275492\n",
      "Average test loss: 0.0031552539207041264\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11063544657495286\n",
      "Average test loss: 0.0031903825571967495\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10861078268289566\n",
      "Average test loss: 0.0032088782079517843\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10681913690434562\n",
      "Average test loss: 0.0031433186636616785\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1053322261373202\n",
      "Average test loss: 0.0031639732834365633\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10394765192270279\n",
      "Average test loss: 0.0031334586462212935\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10285045028395123\n",
      "Average test loss: 0.0031125241298642424\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10177019761006037\n",
      "Average test loss: 0.003103991740900609\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10073316242959764\n",
      "Average test loss: 0.0031464869402762917\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09985692828893661\n",
      "Average test loss: 0.0031036576848063203\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0988300606807073\n",
      "Average test loss: 0.0031000034850504664\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09815230745739408\n",
      "Average test loss: 0.003120578023708529\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09732700677050485\n",
      "Average test loss: 0.0031000195135258966\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09662481529182858\n",
      "Average test loss: 0.0030988611307822995\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09579395775662528\n",
      "Average test loss: 0.0030892921489559942\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0951187878317303\n",
      "Average test loss: 0.0030908798139749304\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09452778851323658\n",
      "Average test loss: 0.003105639464739296\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0939572186867396\n",
      "Average test loss: 0.0030881781579098766\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09302906867530611\n",
      "Average test loss: 0.0031434508908746973\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0926977378792233\n",
      "Average test loss: 0.0030895865265693928\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09190708682934443\n",
      "Average test loss: 0.0031048147937075958\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09144582164287567\n",
      "Average test loss: 0.0030860687765396303\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0908158186044958\n",
      "Average test loss: 0.003105061218763391\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09031673969162834\n",
      "Average test loss: 0.003094654634077516\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08962059373988046\n",
      "Average test loss: 0.0031073271959192223\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08918207118908564\n",
      "Average test loss: 0.0031900280865116253\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08848360082175996\n",
      "Average test loss: 0.003120263588304321\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08815602843628989\n",
      "Average test loss: 0.003171222918563419\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08736139651801851\n",
      "Average test loss: 0.003071844548607866\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08692316914929284\n",
      "Average test loss: 0.003092381130490038\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08636969385544459\n",
      "Average test loss: 0.003161712363362312\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08582740037971073\n",
      "Average test loss: 0.0031416149358782504\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08514147689607408\n",
      "Average test loss: 0.0030974203147408033\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08468210972017712\n",
      "Average test loss: 0.003217744369680683\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08447368274132411\n",
      "Average test loss: 0.0030852258104003137\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08361653842528662\n",
      "Average test loss: 0.003127503508908881\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08317178539435069\n",
      "Average test loss: 0.003135195275768638\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0825796044005288\n",
      "Average test loss: 0.0033318435048891436\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08236327916052606\n",
      "Average test loss: 0.0031987757223347823\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08189499294757843\n",
      "Average test loss: 0.0031568447883344357\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08104908748467764\n",
      "Average test loss: 0.003129372165020969\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08066123254431619\n",
      "Average test loss: 0.0030961176397071946\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0803992002606392\n",
      "Average test loss: 0.003169157774808506\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0798296007182863\n",
      "Average test loss: 0.0031944561226086485\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07923866475290722\n",
      "Average test loss: 0.003163157002379497\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07867816274695927\n",
      "Average test loss: 0.0031084798155352473\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07812178022331662\n",
      "Average test loss: 0.00314393137788607\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07797900924417708\n",
      "Average test loss: 0.003263941802912288\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07747878633936246\n",
      "Average test loss: 0.003200722033364905\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07686234816246562\n",
      "Average test loss: 0.0031609381248967517\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07647813173135122\n",
      "Average test loss: 0.0032140693865302535\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0760386745714479\n",
      "Average test loss: 0.0032266549799177383\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07560192060470582\n",
      "Average test loss: 0.0032729302818576495\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07516014377938376\n",
      "Average test loss: 0.003198087191830079\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07483478250437313\n",
      "Average test loss: 0.0033310973942279817\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07450308416949378\n",
      "Average test loss: 0.003213155069284969\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07416797541578611\n",
      "Average test loss: 0.0032862799370454417\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07367568462424808\n",
      "Average test loss: 0.0032265134397894144\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07329134566585223\n",
      "Average test loss: 0.0032509137880471018\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07295137523942523\n",
      "Average test loss: 0.00319210934970114\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07277479141288333\n",
      "Average test loss: 0.0033374902887476814\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0723505780365732\n",
      "Average test loss: 0.0032343789318369496\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07191867085629039\n",
      "Average test loss: 0.003206029519852665\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07169210771057341\n",
      "Average test loss: 0.0033435066404441994\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07129939456780751\n",
      "Average test loss: 0.003282495540670223\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07097287744283676\n",
      "Average test loss: 0.003241678853622741\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07068216210603714\n",
      "Average test loss: 0.0032927027421279084\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07035175020496051\n",
      "Average test loss: 0.0033088734332058166\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07006985510057873\n",
      "Average test loss: 0.0032186795508282054\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06989275746213065\n",
      "Average test loss: 0.003249254438198275\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06956607926885287\n",
      "Average test loss: 0.003348044304177165\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06925359261698193\n",
      "Average test loss: 0.003219896826065249\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06893931387530433\n",
      "Average test loss: 0.0034544394225296047\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06873726707365778\n",
      "Average test loss: 0.003826160178002384\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06848868585295147\n",
      "Average test loss: 0.00336343327185346\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06824220094747013\n",
      "Average test loss: 0.0033330328636285334\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06786354813641972\n",
      "Average test loss: 0.003338485799108942\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06787796298000548\n",
      "Average test loss: 0.0032961983173671696\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06748758610420758\n",
      "Average test loss: 0.0035228083017799588\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06718740847375658\n",
      "Average test loss: 0.003303394235463606\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06704343420929379\n",
      "Average test loss: 0.003302506494646271\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06691873156693247\n",
      "Average test loss: 0.0033953223623749283\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06662230798602105\n",
      "Average test loss: 0.0033502420838922263\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0663313172062238\n",
      "Average test loss: 0.0033026490388438105\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06605284879604975\n",
      "Average test loss: 0.0033640723787248133\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06595169624355104\n",
      "Average test loss: 0.003324885773575968\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06576840264598528\n",
      "Average test loss: 0.0033188214896039832\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06551072704129748\n",
      "Average test loss: 0.0034095561841709747\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06548858643571535\n",
      "Average test loss: 0.0032991525425265233\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06505126672155327\n",
      "Average test loss: 0.003566362318065431\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06497604416476356\n",
      "Average test loss: 0.0032876098338100643\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06475916929708586\n",
      "Average test loss: 0.0033650700942509703\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06443524058659872\n",
      "Average test loss: 0.0033075874323646227\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06429039788908429\n",
      "Average test loss: 0.003378012353554368\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06428820549117194\n",
      "Average test loss: 0.0033741749388476214\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06405005427532726\n",
      "Average test loss: 0.003280934969584147\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06401689099603229\n",
      "Average test loss: 0.0033561372152633136\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06359400709138976\n",
      "Average test loss: 0.003362241182062361\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06348636478185654\n",
      "Average test loss: 0.0034203077567120394\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06338415497872564\n",
      "Average test loss: 0.0033369877077639104\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06301921476754878\n",
      "Average test loss: 0.0033821254587835738\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0629481186038918\n",
      "Average test loss: 0.00338719801687532\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06276957047316763\n",
      "Average test loss: 0.0034220214991105926\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0627450998822848\n",
      "Average test loss: 0.0033501220227529606\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06240556385450893\n",
      "Average test loss: 0.003378174893351065\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06222006442480617\n",
      "Average test loss: 0.0034562254057576257\n",
      "Epoch 135/300\n",
      "Average training loss: 0.062305739220645695\n",
      "Average test loss: 0.00357794810914331\n",
      "Epoch 136/300\n",
      "Average training loss: 0.062142627136574854\n",
      "Average test loss: 0.003425799358636141\n",
      "Epoch 137/300\n",
      "Average training loss: 0.061887443294127785\n",
      "Average test loss: 0.003389708585002356\n",
      "Epoch 138/300\n",
      "Average training loss: 0.061766402237945134\n",
      "Average test loss: 0.003357413262542751\n",
      "Epoch 139/300\n",
      "Average training loss: 0.061536037216583885\n",
      "Average test loss: 0.0033815741340319314\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06135723625289069\n",
      "Average test loss: 0.0034496392380032274\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06132809383339352\n",
      "Average test loss: 0.0034629361884047586\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06120722039209472\n",
      "Average test loss: 0.003485207418186797\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06104297899868753\n",
      "Average test loss: 0.0034973569326102734\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06091827972067727\n",
      "Average test loss: 0.0034868053315828245\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06083794585863749\n",
      "Average test loss: 0.00334042956514491\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06071656949983703\n",
      "Average test loss: 0.0034337434673474897\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06056651987300979\n",
      "Average test loss: 0.003380129356144203\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06054192194011476\n",
      "Average test loss: 0.0034376619414736827\n",
      "Epoch 149/300\n",
      "Average training loss: 0.060170008351405464\n",
      "Average test loss: 0.0034218001978264913\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06025348709689246\n",
      "Average test loss: 0.003441124534855286\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06005234286520216\n",
      "Average test loss: 0.0034056885399752195\n",
      "Epoch 152/300\n",
      "Average training loss: 0.060107210569911536\n",
      "Average test loss: 0.004883331378300985\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0599896442592144\n",
      "Average test loss: 0.005440457008365128\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05972114529543453\n",
      "Average test loss: 0.0034333503525704146\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05950408985879686\n",
      "Average test loss: 0.0033687844148112666\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05954283217920198\n",
      "Average test loss: 0.003437606959500247\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05943812123603291\n",
      "Average test loss: 0.003435110422058238\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05921107215682665\n",
      "Average test loss: 0.003514450091868639\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05915158815847503\n",
      "Average test loss: 0.0034232281810707517\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05907638804117839\n",
      "Average test loss: 0.003460581784033113\n",
      "Epoch 161/300\n",
      "Average training loss: 0.058937765151262284\n",
      "Average test loss: 0.0035037739214797813\n",
      "Epoch 162/300\n",
      "Average training loss: 0.058791506038771736\n",
      "Average test loss: 0.0033835962801757784\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05872562348511484\n",
      "Average test loss: 0.0034772158867369097\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05864510846469138\n",
      "Average test loss: 0.003555719510962566\n",
      "Epoch 165/300\n",
      "Average training loss: 0.058569594966040715\n",
      "Average test loss: 0.0034800640911691718\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05857345938682556\n",
      "Average test loss: 0.003417852307152417\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05833669270409478\n",
      "Average test loss: 0.0034305636684099834\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05828980756468243\n",
      "Average test loss: 0.0034359154211771156\n",
      "Epoch 169/300\n",
      "Average training loss: 0.057977176593409646\n",
      "Average test loss: 0.0034165188583234946\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05796511427561442\n",
      "Average test loss: 0.0033534782061146367\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05796971075402366\n",
      "Average test loss: 0.003526352989176909\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05777352091338899\n",
      "Average test loss: 0.0034485626609789\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05770753009948466\n",
      "Average test loss: 0.003441866828335656\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05758405918545193\n",
      "Average test loss: 0.0033871037069087228\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05735958403017786\n",
      "Average test loss: 0.00352921254063646\n",
      "Epoch 176/300\n",
      "Average training loss: 0.057280425045225354\n",
      "Average test loss: 0.0033191572880993288\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05733235070109367\n",
      "Average test loss: 0.0034682590402662756\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05724591740634706\n",
      "Average test loss: 0.0035256371916168264\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05707217753595776\n",
      "Average test loss: 0.0034270131163713006\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05718569761845801\n",
      "Average test loss: 0.003404790090397\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05706809778345956\n",
      "Average test loss: 0.003454474320324759\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05698456401295132\n",
      "Average test loss: 0.0034777936113791334\n",
      "Epoch 183/300\n",
      "Average training loss: 0.056776234510872096\n",
      "Average test loss: 0.0034758019120328955\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0565834235449632\n",
      "Average test loss: 0.00350628212839365\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0566348923974567\n",
      "Average test loss: 0.003369296168287595\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05659263041615486\n",
      "Average test loss: 0.003474308428665002\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0563027686741617\n",
      "Average test loss: 0.0034396897614416147\n",
      "Epoch 188/300\n",
      "Average training loss: 0.056341933048433726\n",
      "Average test loss: 0.003472168497120341\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05644220136271583\n",
      "Average test loss: 0.003387177658991681\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0561553022828367\n",
      "Average test loss: 0.0034879459322740636\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05620220238632626\n",
      "Average test loss: 0.0034475028692848154\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05612720833553208\n",
      "Average test loss: 0.0035043583284649583\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05592795333928532\n",
      "Average test loss: 0.0034410646866179177\n",
      "Epoch 194/300\n",
      "Average training loss: 0.055880005770259436\n",
      "Average test loss: 0.0034278279095888136\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0557794386976295\n",
      "Average test loss: 0.003459678403619263\n",
      "Epoch 196/300\n",
      "Average training loss: 0.055703990423017075\n",
      "Average test loss: 0.003429291987377736\n",
      "Epoch 197/300\n",
      "Average training loss: 0.055592475215593976\n",
      "Average test loss: 0.003523248103343778\n",
      "Epoch 198/300\n",
      "Average training loss: 0.055514693448940916\n",
      "Average test loss: 0.003529771498714884\n",
      "Epoch 199/300\n",
      "Average training loss: 0.055505304167668024\n",
      "Average test loss: 0.003420384178352025\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05530858679943614\n",
      "Average test loss: 0.0036157146696415214\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05530602438913451\n",
      "Average test loss: 0.0034446971950431666\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05547295284933514\n",
      "Average test loss: 0.0034483049420846834\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05508710669477781\n",
      "Average test loss: 0.00356775996213158\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05501861062977049\n",
      "Average test loss: 0.003472896978052126\n",
      "Epoch 205/300\n",
      "Average training loss: 0.055032475080755025\n",
      "Average test loss: 0.003446271612205439\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05492643313275443\n",
      "Average test loss: 0.0034911774057481025\n",
      "Epoch 207/300\n",
      "Average training loss: 0.054990825871626535\n",
      "Average test loss: 0.003478748926685916\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05487801687253846\n",
      "Average test loss: 0.003516698494967487\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05474513851934009\n",
      "Average test loss: 0.003477158340315024\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05467709940009647\n",
      "Average test loss: 0.0035091212803704873\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05466208818554878\n",
      "Average test loss: 0.003608586551828517\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05451001142462095\n",
      "Average test loss: 0.003498558204414116\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0545953268342548\n",
      "Average test loss: 0.003495951781877213\n",
      "Epoch 214/300\n",
      "Average training loss: 0.054400911764966114\n",
      "Average test loss: 0.0034723418620932435\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05450694796774123\n",
      "Average test loss: 0.0036104193524354035\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05449306372139189\n",
      "Average test loss: 0.003442060244994031\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05430375090241432\n",
      "Average test loss: 0.0034700035026503932\n",
      "Epoch 218/300\n",
      "Average training loss: 0.054148928821086885\n",
      "Average test loss: 0.003424225421829356\n",
      "Epoch 219/300\n",
      "Average training loss: 0.054435879008637535\n",
      "Average test loss: 0.0035256744089225927\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05388967511388991\n",
      "Average test loss: 0.0035642319238848156\n",
      "Epoch 221/300\n",
      "Average training loss: 0.053831471211380426\n",
      "Average test loss: 0.0034507837779819963\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05381032421191533\n",
      "Average test loss: 0.0034289429465101827\n",
      "Epoch 223/300\n",
      "Average training loss: 0.053958179050021704\n",
      "Average test loss: 0.003528466762560937\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05376715978980064\n",
      "Average test loss: 0.0035535080737123886\n",
      "Epoch 225/300\n",
      "Average training loss: 0.053777804358137976\n",
      "Average test loss: 0.003512590320056511\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0536803544263045\n",
      "Average test loss: 0.0035086884792480204\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05357829244931539\n",
      "Average test loss: 0.003490450522138013\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05366322885619269\n",
      "Average test loss: 0.0035358349080714916\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05352076488071018\n",
      "Average test loss: 0.0034374533279074564\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05367864484919442\n",
      "Average test loss: 0.0035570365455415516\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05327215817901823\n",
      "Average test loss: 0.003564859288227227\n",
      "Epoch 232/300\n",
      "Average training loss: 0.053296117948161234\n",
      "Average test loss: 0.0035584746518482766\n",
      "Epoch 233/300\n",
      "Average training loss: 0.053494925243986975\n",
      "Average test loss: 0.003487150001236134\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05328516006800863\n",
      "Average test loss: 0.003460828293528822\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05303447336620755\n",
      "Average test loss: 0.00385565296953751\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05308245904246966\n",
      "Average test loss: 0.0034856801455219587\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05309046071767807\n",
      "Average test loss: 0.003469580604798264\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05300636305742794\n",
      "Average test loss: 0.0034362600218090745\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05294790636168586\n",
      "Average test loss: 0.003516547666448686\n",
      "Epoch 240/300\n",
      "Average training loss: 0.052928672826952405\n",
      "Average test loss: 0.0034475974324676725\n",
      "Epoch 241/300\n",
      "Average training loss: 0.052904995328850214\n",
      "Average test loss: 0.003553766391757462\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05283668619063166\n",
      "Average test loss: 0.003548438694741991\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05267974512444602\n",
      "Average test loss: 0.0035009063215305407\n",
      "Epoch 244/300\n",
      "Average training loss: 0.052466180430518254\n",
      "Average test loss: 0.0035221452292882732\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05263349205917782\n",
      "Average test loss: 0.003499440110805962\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05268739795022541\n",
      "Average test loss: 0.003742298694741395\n",
      "Epoch 247/300\n",
      "Average training loss: 0.052524426841073564\n",
      "Average test loss: 0.003479353250935674\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05239884392751588\n",
      "Average test loss: 0.00342135725915432\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05241467141608397\n",
      "Average test loss: 0.0035484767746594217\n",
      "Epoch 250/300\n",
      "Average training loss: 0.052379577265845405\n",
      "Average test loss: 0.003604336068034172\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05229895268877347\n",
      "Average test loss: 0.00356642201791207\n",
      "Epoch 252/300\n",
      "Average training loss: 0.052204798251390454\n",
      "Average test loss: 0.0034451110096027455\n",
      "Epoch 253/300\n",
      "Average training loss: 0.052041675789488685\n",
      "Average test loss: 0.0035285993820677202\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05219979105393092\n",
      "Average test loss: 0.0034918800964951514\n",
      "Epoch 255/300\n",
      "Average training loss: 0.052191862242089375\n",
      "Average test loss: 0.003509171568478147\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05200202261077033\n",
      "Average test loss: 0.0035869011912080975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05211599609255791\n",
      "Average test loss: 0.003551124570270379\n",
      "Epoch 258/300\n",
      "Average training loss: 0.051901699271467\n",
      "Average test loss: 0.0035194825418293474\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05192656468020545\n",
      "Average test loss: 0.0034969680273077555\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05191026358140839\n",
      "Average test loss: 0.0035933583409835895\n",
      "Epoch 261/300\n",
      "Average training loss: 0.051894505672984655\n",
      "Average test loss: 0.0035834586748646367\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05180464141898685\n",
      "Average test loss: 0.0035291410417606435\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05176922898822361\n",
      "Average test loss: 0.003611705505806539\n",
      "Epoch 264/300\n",
      "Average training loss: 0.051661654167705115\n",
      "Average test loss: 0.0035400746090130672\n",
      "Epoch 265/300\n",
      "Average training loss: 0.051607911520534094\n",
      "Average test loss: 0.003529886779271894\n",
      "Epoch 266/300\n",
      "Average training loss: 0.051520090321699775\n",
      "Average test loss: 0.0035585149741835065\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05147739114363988\n",
      "Average test loss: 0.0035101066328999068\n",
      "Epoch 268/300\n",
      "Average training loss: 0.051496128218041524\n",
      "Average test loss: 0.003445976110796134\n",
      "Epoch 269/300\n",
      "Average training loss: 0.051385383556286496\n",
      "Average test loss: 0.003590780249072446\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05140286339653863\n",
      "Average test loss: 0.003534828445563714\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05141078960067696\n",
      "Average test loss: 0.003411923974338505\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05126827169458072\n",
      "Average test loss: 0.0034701479994174505\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05133900815248489\n",
      "Average test loss: 0.003572630575961537\n",
      "Epoch 274/300\n",
      "Average training loss: 0.051113674683703314\n",
      "Average test loss: 0.003557712733538614\n",
      "Epoch 275/300\n",
      "Average training loss: 0.051336412327157126\n",
      "Average test loss: 0.0035565364489124882\n",
      "Epoch 276/300\n",
      "Average training loss: 0.051384973986281286\n",
      "Average test loss: 0.003504260459293922\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05106141100658311\n",
      "Average test loss: 0.0035156226770745383\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05109815531306797\n",
      "Average test loss: 0.003576494846699966\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05116988640692499\n",
      "Average test loss: 0.0035443563180872135\n",
      "Epoch 280/300\n",
      "Average training loss: 0.050952070415019986\n",
      "Average test loss: 0.003576997026594149\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05097708585858345\n",
      "Average test loss: 0.0035209058084421686\n",
      "Epoch 282/300\n",
      "Average training loss: 0.051010196563270355\n",
      "Average test loss: 0.0035558984354138374\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05079160185986095\n",
      "Average test loss: 0.003526987451232142\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05077934317125214\n",
      "Average test loss: 0.0034800323018183313\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05077118544777234\n",
      "Average test loss: 0.0035698500921328862\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05057841830452283\n",
      "Average test loss: 0.00349650061606533\n",
      "Epoch 287/300\n",
      "Average training loss: 0.050727984484699035\n",
      "Average test loss: 0.0035008828453719615\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05064566470848189\n",
      "Average test loss: 0.0034917869712743493\n",
      "Epoch 289/300\n",
      "Average training loss: 0.050665280905034804\n",
      "Average test loss: 0.003605298100142843\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05038638840450181\n",
      "Average test loss: 0.00352100234458016\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05058509013056755\n",
      "Average test loss: 0.0035396303973264163\n",
      "Epoch 292/300\n",
      "Average training loss: 0.050559324655267926\n",
      "Average test loss: 0.003529449076288276\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05042087835735745\n",
      "Average test loss: 0.0035054359826155834\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0504352238840527\n",
      "Average test loss: 0.003553738421243098\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05018049553367827\n",
      "Average test loss: 0.0035919514424684975\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05027866590685315\n",
      "Average test loss: 0.003539800614532497\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05022830004824532\n",
      "Average test loss: 0.003481010125743018\n",
      "Epoch 298/300\n",
      "Average training loss: 0.050333439936240516\n",
      "Average test loss: 0.0037182252276688815\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05018116124139892\n",
      "Average test loss: 0.003446665745228529\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05013268300228649\n",
      "Average test loss: 0.003624858663106958\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.911529075198704\n",
      "Average test loss: 0.005298731756707032\n",
      "Epoch 2/300\n",
      "Average training loss: 3.589761365678575\n",
      "Average test loss: 0.05695314096245501\n",
      "Epoch 3/300\n",
      "Average training loss: 2.404548774507311\n",
      "Average test loss: 0.0040560029581603076\n",
      "Epoch 4/300\n",
      "Average training loss: 1.7977704785664876\n",
      "Average test loss: 0.004767289434973564\n",
      "Epoch 5/300\n",
      "Average training loss: 1.444937676111857\n",
      "Average test loss: 0.003709175226175123\n",
      "Epoch 6/300\n",
      "Average training loss: 1.0565142588085599\n",
      "Average test loss: 0.003585421803303891\n",
      "Epoch 7/300\n",
      "Average training loss: 0.8478694521056281\n",
      "Average test loss: 0.0035160725329899125\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6816769409179687\n",
      "Average test loss: 0.003421221770346165\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5474452574517992\n",
      "Average test loss: 0.004381577740112941\n",
      "Epoch 10/300\n",
      "Average training loss: 0.4416924984720018\n",
      "Average test loss: 0.003273835951048467\n",
      "Epoch 11/300\n",
      "Average training loss: 0.3644721334775289\n",
      "Average test loss: 0.003288453585157792\n",
      "Epoch 12/300\n",
      "Average training loss: 0.30593525173929004\n",
      "Average test loss: 0.0031615745193428464\n",
      "Epoch 13/300\n",
      "Average training loss: 0.26230516192648146\n",
      "Average test loss: 0.0031171136237680913\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2282760571108924\n",
      "Average test loss: 0.002970556098346909\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20207044389512804\n",
      "Average test loss: 0.002983204936194751\n",
      "Epoch 16/300\n",
      "Average training loss: 0.18078649068540997\n",
      "Average test loss: 0.002840491536590788\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1633817770746019\n",
      "Average test loss: 0.0029336164357761543\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1490410226980845\n",
      "Average test loss: 0.0028777686713470354\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13694598370128208\n",
      "Average test loss: 0.0027489345181319447\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12733057073752085\n",
      "Average test loss: 0.002684503460095988\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11904180560509364\n",
      "Average test loss: 0.0026986157111823558\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11234993471039666\n",
      "Average test loss: 0.0026412984960609014\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1070458836654822\n",
      "Average test loss: 0.0026238295663562087\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1029311160047849\n",
      "Average test loss: 0.002667928969073627\n",
      "Epoch 25/300\n",
      "Average training loss: 0.099751578980022\n",
      "Average test loss: 0.0025628696371697715\n",
      "Epoch 26/300\n",
      "Average training loss: 0.09698446721500821\n",
      "Average test loss: 0.002545126629786359\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09450100977553262\n",
      "Average test loss: 0.00257442436243097\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09264434660143323\n",
      "Average test loss: 0.0025254163034260273\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09077758820851643\n",
      "Average test loss: 0.002542663625958893\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08915245633986262\n",
      "Average test loss: 0.0025455912631005048\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08760953999227948\n",
      "Average test loss: 0.002562926544704371\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0861907198296653\n",
      "Average test loss: 0.002486546227398018\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08491096180677414\n",
      "Average test loss: 0.002474696934533616\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0838798440893491\n",
      "Average test loss: 0.002475800725320975\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08285218727588653\n",
      "Average test loss: 0.00244552218210366\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08150167411565781\n",
      "Average test loss: 0.0024589209254417153\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08062150895264414\n",
      "Average test loss: 0.002473692050203681\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07957968830400043\n",
      "Average test loss: 0.002456509268635677\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07881167856521076\n",
      "Average test loss: 0.0024520272018594873\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07780450834830602\n",
      "Average test loss: 0.0024565945549143687\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07706977404157321\n",
      "Average test loss: 0.002428951230107082\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07619670082132021\n",
      "Average test loss: 0.002528755714909898\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07544599022467931\n",
      "Average test loss: 0.0024084562191532717\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07490654252800677\n",
      "Average test loss: 0.002527063853624794\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07406329485442903\n",
      "Average test loss: 0.002424816216652592\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07338993421528074\n",
      "Average test loss: 0.0024071750762975877\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07293032899167802\n",
      "Average test loss: 0.0024007150561859212\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07198130760921373\n",
      "Average test loss: 0.0024278171302543746\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07151170932915475\n",
      "Average test loss: 0.0023861565014554396\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07075444820854399\n",
      "Average test loss: 0.0024286139017591873\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07030331337451935\n",
      "Average test loss: 0.0024178868099633192\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0694897684223122\n",
      "Average test loss: 0.0023804451516932913\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06906101584103372\n",
      "Average test loss: 0.00247907645907253\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06838192422522439\n",
      "Average test loss: 0.0024439726961362693\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06783378738164901\n",
      "Average test loss: 0.002451009403500292\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06725310345490773\n",
      "Average test loss: 0.002461596062199937\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06670154959294532\n",
      "Average test loss: 0.002542575989746385\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0660778024395307\n",
      "Average test loss: 0.002415197635690371\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06560392203595904\n",
      "Average test loss: 0.002380772788491514\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0648165940841039\n",
      "Average test loss: 0.0024153859660857252\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06448132981856664\n",
      "Average test loss: 0.0023988352711829873\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06401219186517927\n",
      "Average test loss: 0.002426486018838154\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06354733153846528\n",
      "Average test loss: 0.0024490707595315243\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06276977592209974\n",
      "Average test loss: 0.002567974430301951\n",
      "Epoch 65/300\n",
      "Average training loss: 0.062323993927902645\n",
      "Average test loss: 0.002458836990926001\n",
      "Epoch 66/300\n",
      "Average training loss: 0.061731365594598984\n",
      "Average test loss: 0.0024353491893659034\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0613618523478508\n",
      "Average test loss: 0.0024386875421429674\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0609032113817003\n",
      "Average test loss: 0.002467528315157526\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06044698985583252\n",
      "Average test loss: 0.0025348428645067746\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05992187830143505\n",
      "Average test loss: 0.002566732809568445\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05938862958550453\n",
      "Average test loss: 0.0025543272654629415\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05908242205447621\n",
      "Average test loss: 0.00255955292760498\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0587218860619598\n",
      "Average test loss: 0.002429037132817838\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05809764811065462\n",
      "Average test loss: 0.0025574021736780805\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05775700505243407\n",
      "Average test loss: 0.0024339852194405266\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05740010092324681\n",
      "Average test loss: 0.00254286054852936\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05706793772180875\n",
      "Average test loss: 0.002465403558065494\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0565390426615874\n",
      "Average test loss: 0.0024520802830035487\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0563873949514495\n",
      "Average test loss: 0.002479749109182093\n",
      "Epoch 80/300\n",
      "Average training loss: 0.055887775182723996\n",
      "Average test loss: 0.0024654454946931867\n",
      "Epoch 81/300\n",
      "Average training loss: 0.055468206445376075\n",
      "Average test loss: 0.0025783130532751482\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0551542560491297\n",
      "Average test loss: 0.00248407089492927\n",
      "Epoch 83/300\n",
      "Average training loss: 0.054798202832539875\n",
      "Average test loss: 0.0024537692680541012\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05444282704260614\n",
      "Average test loss: 0.0025646572709083558\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05411289534303877\n",
      "Average test loss: 0.00253095069879459\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0539941656490167\n",
      "Average test loss: 0.002620199341016511\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05353320763177342\n",
      "Average test loss: 0.0024915105363147124\n",
      "Epoch 88/300\n",
      "Average training loss: 0.053277556462420356\n",
      "Average test loss: 0.0025286949223114383\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05292633340093825\n",
      "Average test loss: 0.0025277103709263936\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05255298706226879\n",
      "Average test loss: 0.002541534046125081\n",
      "Epoch 91/300\n",
      "Average training loss: 0.052345411873526045\n",
      "Average test loss: 0.002580784477914373\n",
      "Epoch 92/300\n",
      "Average training loss: 0.052136833621395956\n",
      "Average test loss: 0.0025632234843861726\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05192121833893988\n",
      "Average test loss: 0.002528551591974166\n",
      "Epoch 94/300\n",
      "Average training loss: 0.051488878183894686\n",
      "Average test loss: 0.002561934574196736\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05141170357333289\n",
      "Average test loss: 0.002602435898863607\n",
      "Epoch 96/300\n",
      "Average training loss: 0.051155709915690954\n",
      "Average test loss: 0.002575358810110225\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05094280232654678\n",
      "Average test loss: 0.0025979495503836206\n",
      "Epoch 98/300\n",
      "Average training loss: 0.050547604680061343\n",
      "Average test loss: 0.002598648700759643\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0505074622631073\n",
      "Average test loss: 0.0025729599106642936\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04994631389776866\n",
      "Average test loss: 0.0026462215669453143\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05000136329068078\n",
      "Average test loss: 0.002594668476117982\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04983226256900364\n",
      "Average test loss: 0.002619164411806398\n",
      "Epoch 103/300\n",
      "Average training loss: 0.049619965212212666\n",
      "Average test loss: 0.003114706908042232\n",
      "Epoch 104/300\n",
      "Average training loss: 0.049362304280201594\n",
      "Average test loss: 0.002571508824514846\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04926862246129248\n",
      "Average test loss: 0.002614256026016341\n",
      "Epoch 106/300\n",
      "Average training loss: 0.048928243868880804\n",
      "Average test loss: 0.002611408674675557\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04866053858399391\n",
      "Average test loss: 0.002578295047291451\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04845791889561547\n",
      "Average test loss: 0.002597902965835399\n",
      "Epoch 109/300\n",
      "Average training loss: 0.048380098674032423\n",
      "Average test loss: 0.002604075905970401\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04806131349338426\n",
      "Average test loss: 0.002560951091763046\n",
      "Epoch 111/300\n",
      "Average training loss: 0.048125326113568415\n",
      "Average test loss: 0.0026581016168412235\n",
      "Epoch 112/300\n",
      "Average training loss: 0.047826043138901396\n",
      "Average test loss: 0.0028406811873945923\n",
      "Epoch 113/300\n",
      "Average training loss: 0.047662516815794836\n",
      "Average test loss: 0.0025830803863290284\n",
      "Epoch 114/300\n",
      "Average training loss: 0.047562226626608105\n",
      "Average test loss: 0.0025769946698306337\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04727992925047875\n",
      "Average test loss: 0.0025625828069945175\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04716097414162424\n",
      "Average test loss: 0.002604190327847997\n",
      "Epoch 117/300\n",
      "Average training loss: 0.046943590230411954\n",
      "Average test loss: 0.0025822560456064013\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0467914505203565\n",
      "Average test loss: 0.002623977301435338\n",
      "Epoch 119/300\n",
      "Average training loss: 0.046763465573390325\n",
      "Average test loss: 0.0026696410696539614\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04638707819581032\n",
      "Average test loss: 0.0027069480599214634\n",
      "Epoch 121/300\n",
      "Average training loss: 0.046399684339761736\n",
      "Average test loss: 0.002691446816134784\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04662027488483323\n",
      "Average test loss: 0.002682564194003741\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04610345573226611\n",
      "Average test loss: 0.002644600093157755\n",
      "Epoch 124/300\n",
      "Average training loss: 0.045826865954531566\n",
      "Average test loss: 0.0026422925221009385\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04569658523797989\n",
      "Average test loss: 0.0026386091231058043\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04575477182534006\n",
      "Average test loss: 0.002626769969653752\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04555101927783754\n",
      "Average test loss: 0.0026630113267650207\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04546165161662632\n",
      "Average test loss: 0.002682660496690207\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04530074698064062\n",
      "Average test loss: 0.0026940431632101534\n",
      "Epoch 130/300\n",
      "Average training loss: 0.045134778516160115\n",
      "Average test loss: 0.002715544822729296\n",
      "Epoch 131/300\n",
      "Average training loss: 0.045142121987210376\n",
      "Average test loss: 0.0026837472220261893\n",
      "Epoch 132/300\n",
      "Average training loss: 0.044862434597478976\n",
      "Average test loss: 0.002619531630848845\n",
      "Epoch 133/300\n",
      "Average training loss: 0.044871451621254284\n",
      "Average test loss: 0.0027205920287718378\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04485798128114806\n",
      "Average test loss: 0.0026155714665850004\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04468682032823563\n",
      "Average test loss: 0.002653643142638935\n",
      "Epoch 136/300\n",
      "Average training loss: 0.044421839005417296\n",
      "Average test loss: 0.002634358370676637\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04428895827796724\n",
      "Average test loss: 0.002691088806734317\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04429107882248031\n",
      "Average test loss: 0.0026744744589345323\n",
      "Epoch 139/300\n",
      "Average training loss: 0.044197734885745577\n",
      "Average test loss: 0.0026607976224687366\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0440651108937131\n",
      "Average test loss: 0.002692122122272849\n",
      "Epoch 141/300\n",
      "Average training loss: 0.043945701413684424\n",
      "Average test loss: 0.002616459485143423\n",
      "Epoch 142/300\n",
      "Average training loss: 0.043677320993608895\n",
      "Average test loss: 0.002718729321120514\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04369731073909336\n",
      "Average test loss: 0.002654559980560508\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04354260415169928\n",
      "Average test loss: 0.0026502884461854894\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04347745561268594\n",
      "Average test loss: 0.0026066760948548714\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04335195975005627\n",
      "Average test loss: 0.0027615076462841697\n",
      "Epoch 147/300\n",
      "Average training loss: 0.043111418556835916\n",
      "Average test loss: 0.0026668429803103208\n",
      "Epoch 148/300\n",
      "Average training loss: 0.043165322095155716\n",
      "Average test loss: 0.002694326019121541\n",
      "Epoch 149/300\n",
      "Average training loss: 0.043171867774592504\n",
      "Average test loss: 0.0026412537469425136\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0430495390229755\n",
      "Average test loss: 0.0026514935325831174\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04296231119169129\n",
      "Average test loss: 0.0026684974819007846\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04288136751453082\n",
      "Average test loss: 0.002627984290218188\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0427365128993988\n",
      "Average test loss: 0.0027541222785496047\n",
      "Epoch 154/300\n",
      "Average training loss: 0.042651566886239584\n",
      "Average test loss: 0.0026948974111841786\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04253700914316707\n",
      "Average test loss: 0.002709868687722418\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04245613972677125\n",
      "Average test loss: 0.002703245194421874\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04249147860209147\n",
      "Average test loss: 0.0026834192217017215\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04220630124542448\n",
      "Average test loss: 0.0027497053576840294\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04225545025865237\n",
      "Average test loss: 0.0027094430388468834\n",
      "Epoch 160/300\n",
      "Average training loss: 0.042051190578275256\n",
      "Average test loss: 0.0026392742842435836\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04196588196025954\n",
      "Average test loss: 0.0027347977641555997\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04196856671902868\n",
      "Average test loss: 0.0027606496638101006\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04187020666400591\n",
      "Average test loss: 0.00267092526724769\n",
      "Epoch 164/300\n",
      "Average training loss: 0.041839112939106096\n",
      "Average test loss: 0.0026685933453134366\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04173175964587265\n",
      "Average test loss: 0.0027342896825737423\n",
      "Epoch 166/300\n",
      "Average training loss: 0.041885106401311024\n",
      "Average test loss: 0.0028730245360897647\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04158555317256186\n",
      "Average test loss: 0.0029865784210463364\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04146633226010534\n",
      "Average test loss: 0.002763847950638996\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04133328683839904\n",
      "Average test loss: 0.0028319639824330805\n",
      "Epoch 170/300\n",
      "Average training loss: 0.041179330974817276\n",
      "Average test loss: 0.0027188025786437924\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04111512915955649\n",
      "Average test loss: 0.0026954836021694873\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04122005565298928\n",
      "Average test loss: 0.0027283411626186638\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04099934668342273\n",
      "Average test loss: 0.002740311892082294\n",
      "Epoch 174/300\n",
      "Average training loss: 0.041158402999242144\n",
      "Average test loss: 0.002789780523421036\n",
      "Epoch 175/300\n",
      "Average training loss: 0.041081398626168567\n",
      "Average test loss: 0.0027338949257714883\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04097761979036861\n",
      "Average test loss: 0.002719517788746291\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04078680038452148\n",
      "Average test loss: 0.002613553263247013\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040783524468541145\n",
      "Average test loss: 0.0027782792711837424\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04073184312714471\n",
      "Average test loss: 0.002703976456903749\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04066952704058753\n",
      "Average test loss: 0.0027790814139362838\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0404592923687564\n",
      "Average test loss: 0.0027163738353798788\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04047869147194756\n",
      "Average test loss: 0.002706588558335271\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04053666492965486\n",
      "Average test loss: 0.002706041294253535\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04033698978026708\n",
      "Average test loss: 0.0026892705931224757\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04039448783795039\n",
      "Average test loss: 0.002669061631688641\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040350975170731544\n",
      "Average test loss: 0.00274397920899921\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04025021788477898\n",
      "Average test loss: 0.0029723532357149654\n",
      "Epoch 188/300\n",
      "Average training loss: 0.040148850798606875\n",
      "Average test loss: 0.0027730089503650865\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04000684067938063\n",
      "Average test loss: 0.0026806462406077317\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04011496338248253\n",
      "Average test loss: 0.0026526530854817892\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03998959399594201\n",
      "Average test loss: 0.0027408964716725878\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0397866852051682\n",
      "Average test loss: 0.0027194905853312875\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03989127223028077\n",
      "Average test loss: 0.002753256583586335\n",
      "Epoch 194/300\n",
      "Average training loss: 0.039810038520230184\n",
      "Average test loss: 0.0028146801892047126\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03970510927504963\n",
      "Average test loss: 0.0027368902773078946\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03962007555696699\n",
      "Average test loss: 0.0027454557485050626\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03954781800839636\n",
      "Average test loss: 0.002758694684960776\n",
      "Epoch 198/300\n",
      "Average training loss: 0.039458230353064006\n",
      "Average test loss: 0.002782786049983568\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03946522417995665\n",
      "Average test loss: 0.002675026434784134\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0394428729083803\n",
      "Average test loss: 0.0028181862499978808\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03933364493979348\n",
      "Average test loss: 0.0027370199407968257\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03921884964903196\n",
      "Average test loss: 0.0026852745088852114\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03921396875547038\n",
      "Average test loss: 0.0027503546393579906\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03911511454648442\n",
      "Average test loss: 0.002795879564765427\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03910492994222376\n",
      "Average test loss: 0.002789353886205289\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03918345364265972\n",
      "Average test loss: 0.002758841669600871\n",
      "Epoch 207/300\n",
      "Average training loss: 0.039043686320384345\n",
      "Average test loss: 0.002817037558596995\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03889483089579476\n",
      "Average test loss: 0.0028150218843171993\n",
      "Epoch 209/300\n",
      "Average training loss: 0.039123885677920446\n",
      "Average test loss: 0.002692529172119167\n",
      "Epoch 210/300\n",
      "Average training loss: 0.038853074845340514\n",
      "Average test loss: 0.002767951940910684\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03876321424709426\n",
      "Average test loss: 0.0027127362719426553\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03878790927595562\n",
      "Average test loss: 0.002824155283677909\n",
      "Epoch 213/300\n",
      "Average training loss: 0.038879452978571254\n",
      "Average test loss: 0.0030308780312124227\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0390092209511333\n",
      "Average test loss: 0.0026724603745258515\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03857977129684554\n",
      "Average test loss: 0.002953492699811856\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03861802072491911\n",
      "Average test loss: 0.002776165627977914\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03846495027012295\n",
      "Average test loss: 0.002715784461444451\n",
      "Epoch 218/300\n",
      "Average training loss: 0.038441075957483715\n",
      "Average test loss: 0.002713054316946202\n",
      "Epoch 219/300\n",
      "Average training loss: 0.038388073846697804\n",
      "Average test loss: 0.002708849977081021\n",
      "Epoch 220/300\n",
      "Average training loss: 0.038388912826776506\n",
      "Average test loss: 0.0027264093595246474\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0382458508014679\n",
      "Average test loss: 0.00276043105042643\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03834682810306549\n",
      "Average test loss: 0.0027809630137764743\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03821865332126617\n",
      "Average test loss: 0.0026765882002396718\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03825540362464057\n",
      "Average test loss: 0.002784235614351928\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03808474481768078\n",
      "Average test loss: 0.002790261767597662\n",
      "Epoch 226/300\n",
      "Average training loss: 0.038172678848107655\n",
      "Average test loss: 0.0028909926141301792\n",
      "Epoch 227/300\n",
      "Average training loss: 0.038144780821270416\n",
      "Average test loss: 0.0028322228317459426\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0380624147620466\n",
      "Average test loss: 0.002775281560917695\n",
      "Epoch 229/300\n",
      "Average training loss: 0.038006998035642836\n",
      "Average test loss: 0.0027403294725550547\n",
      "Epoch 230/300\n",
      "Average training loss: 0.038020820336209404\n",
      "Average test loss: 0.0027540932511910796\n",
      "Epoch 231/300\n",
      "Average training loss: 0.037892496089140575\n",
      "Average test loss: 0.002713343171816733\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03783508970008956\n",
      "Average test loss: 0.0028779041127612193\n",
      "Epoch 233/300\n",
      "Average training loss: 0.037861589743031394\n",
      "Average test loss: 0.0027616819478571415\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03775753249724706\n",
      "Average test loss: 0.002726373690490921\n",
      "Epoch 235/300\n",
      "Average training loss: 0.037727354321214886\n",
      "Average test loss: 0.0027324470122241313\n",
      "Epoch 236/300\n",
      "Average training loss: 0.037743721885813604\n",
      "Average test loss: 0.0028197562031240927\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0377052906288041\n",
      "Average test loss: 0.002715463180301918\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03757249872883161\n",
      "Average test loss: 0.0028069179594102833\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03762266586224238\n",
      "Average test loss: 0.0027473277747631073\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03758273361788856\n",
      "Average test loss: 0.0027310823370806044\n",
      "Epoch 241/300\n",
      "Average training loss: 0.037362081587314605\n",
      "Average test loss: 0.0027349278064858584\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0375012353029516\n",
      "Average test loss: 0.0027885271060383984\n",
      "Epoch 243/300\n",
      "Average training loss: 0.037456402975651955\n",
      "Average test loss: 0.0027533046890878014\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03740186016758283\n",
      "Average test loss: 0.0027445202042452165\n",
      "Epoch 245/300\n",
      "Average training loss: 0.037380363874965246\n",
      "Average test loss: 0.0028765280112210247\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03719249420033561\n",
      "Average test loss: 0.0027302407214625014\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03724402639932103\n",
      "Average test loss: 0.002760901136116849\n",
      "Epoch 248/300\n",
      "Average training loss: 0.037395089338223136\n",
      "Average test loss: 0.0027284843751953706\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03717367507848475\n",
      "Average test loss: 0.0027962127251343594\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03716399103237523\n",
      "Average test loss: 0.0028080179236001437\n",
      "Epoch 251/300\n",
      "Average training loss: 0.037095073103904724\n",
      "Average test loss: 0.0027151172948587272\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03704017765323321\n",
      "Average test loss: 0.0027739521202941736\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03705181871520148\n",
      "Average test loss: 0.0027768519113047256\n",
      "Epoch 254/300\n",
      "Average training loss: 0.036925785226954354\n",
      "Average test loss: 0.0027622084656937253\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03689455365141233\n",
      "Average test loss: 0.002716235070385867\n",
      "Epoch 256/300\n",
      "Average training loss: 0.036792811382148\n",
      "Average test loss: 0.0028004771750420334\n",
      "Epoch 257/300\n",
      "Average training loss: 0.036935546846853365\n",
      "Average test loss: 0.0027608429439779786\n",
      "Epoch 258/300\n",
      "Average training loss: 0.036895616779724756\n",
      "Average test loss: 0.0028130642345382106\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03671535487969716\n",
      "Average test loss: 0.0027751558888703585\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03670994467205471\n",
      "Average test loss: 0.002885764666315582\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03675656028588613\n",
      "Average test loss: 0.0030358426380488607\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0370458172576295\n",
      "Average test loss: 0.0028385686110705137\n",
      "Epoch 263/300\n",
      "Average training loss: 0.036570165273216035\n",
      "Average test loss: 0.0027354128331773813\n",
      "Epoch 264/300\n",
      "Average training loss: 0.036658752623531556\n",
      "Average test loss: 0.002745191501453519\n",
      "Epoch 265/300\n",
      "Average training loss: 0.036651231000820794\n",
      "Average test loss: 0.0029085652126620213\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03652892732289102\n",
      "Average test loss: 0.002799896541982889\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0365017149746418\n",
      "Average test loss: 0.0027380475184569757\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03655278858873579\n",
      "Average test loss: 0.0027176737191362513\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03652385741306676\n",
      "Average test loss: 0.002844486748903162\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03650585028529167\n",
      "Average test loss: 0.002797781285519401\n",
      "Epoch 271/300\n",
      "Average training loss: 0.036405583755837544\n",
      "Average test loss: 0.002714682185401519\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03621305267016093\n",
      "Average test loss: 0.0028181480227245223\n",
      "Epoch 273/300\n",
      "Average training loss: 0.036345108214351864\n",
      "Average test loss: 0.002781213155430224\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03644863645235697\n",
      "Average test loss: 0.002782294299039576\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03631947862936391\n",
      "Average test loss: 0.002826659428162707\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03620444166329172\n",
      "Average test loss: 0.0028407798127995597\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03631520512368944\n",
      "Average test loss: 0.0028467318542922536\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03616150968273481\n",
      "Average test loss: 0.0028630729499790402\n",
      "Epoch 279/300\n",
      "Average training loss: 0.036134819343686106\n",
      "Average test loss: 0.0027595321101446945\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0361774791876475\n",
      "Average test loss: 0.0027468469771039154\n",
      "Epoch 281/300\n",
      "Average training loss: 0.036089789519707365\n",
      "Average test loss: 0.002808055175261365\n",
      "Epoch 282/300\n",
      "Average training loss: 0.036100432952245076\n",
      "Average test loss: 0.0027839278125514587\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03601038845214579\n",
      "Average test loss: 0.002800717576303416\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03601422301265929\n",
      "Average test loss: 0.002772703216307693\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03618750577502781\n",
      "Average test loss: 0.002775173253276282\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03597194352083736\n",
      "Average test loss: 0.0028542267096539338\n",
      "Epoch 287/300\n",
      "Average training loss: 0.035901740163564684\n",
      "Average test loss: 0.0028201770573440524\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0358622858689891\n",
      "Average test loss: 0.0028281244507266413\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03584608262280623\n",
      "Average test loss: 0.002810382475869523\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03581618564327558\n",
      "Average test loss: 0.002821074501093891\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0358802470697297\n",
      "Average test loss: 0.002813170664012432\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03578349390294817\n",
      "Average test loss: 0.002776545903645456\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0357856904996766\n",
      "Average test loss: 0.002850168087416225\n",
      "Epoch 294/300\n",
      "Average training loss: 0.035675468580590355\n",
      "Average test loss: 0.0027640910246926878\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03560887774825096\n",
      "Average test loss: 0.0027726129132012527\n",
      "Epoch 296/300\n",
      "Average training loss: 0.035575920754008825\n",
      "Average test loss: 0.002782205043981473\n",
      "Epoch 297/300\n",
      "Average training loss: 0.035485563890801534\n",
      "Average test loss: 0.0027639043358051113\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03560025330053435\n",
      "Average test loss: 0.002809841441611449\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03554018481903606\n",
      "Average test loss: 0.002764590720956524\n",
      "Epoch 300/300\n",
      "Average training loss: 0.035657361406419015\n",
      "Average test loss: 0.002825272756525212\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.033005324045817\n",
      "Average test loss: 0.0803284790876011\n",
      "Epoch 2/300\n",
      "Average training loss: 3.642741487926907\n",
      "Average test loss: 0.056623401467998824\n",
      "Epoch 3/300\n",
      "Average training loss: 2.5432686231401234\n",
      "Average test loss: 0.004658801636555129\n",
      "Epoch 4/300\n",
      "Average training loss: 1.9463126747343276\n",
      "Average test loss: 0.003102011437113914\n",
      "Epoch 5/300\n",
      "Average training loss: 1.6073214781019423\n",
      "Average test loss: 0.0030268395060880316\n",
      "Epoch 6/300\n",
      "Average training loss: 1.3677124490737915\n",
      "Average test loss: 0.07227731570787728\n",
      "Epoch 7/300\n",
      "Average training loss: 1.1474740830527412\n",
      "Average test loss: 0.002746158636278576\n",
      "Epoch 8/300\n",
      "Average training loss: 0.9797203366491529\n",
      "Average test loss: 0.0026889709753708706\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7898225363625421\n",
      "Average test loss: 0.0047475425799687705\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6492920832633973\n",
      "Average test loss: 0.011999436273549994\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5263902124828762\n",
      "Average test loss: 0.002393718173727393\n",
      "Epoch 12/300\n",
      "Average training loss: 0.42659752429856196\n",
      "Average test loss: 0.0023068924204756817\n",
      "Epoch 13/300\n",
      "Average training loss: 0.35049445192019146\n",
      "Average test loss: 0.0022551478886355955\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2926230117347505\n",
      "Average test loss: 0.0022340369404604036\n",
      "Epoch 15/300\n",
      "Average training loss: 0.24766949469513364\n",
      "Average test loss: 0.002135216532482041\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2135522108607822\n",
      "Average test loss: 0.02456052502161927\n",
      "Epoch 17/300\n",
      "Average training loss: 0.18597014604674444\n",
      "Average test loss: 0.0020329197199187347\n",
      "Epoch 18/300\n",
      "Average training loss: 0.16350013257397547\n",
      "Average test loss: 0.0020186700842653713\n",
      "Epoch 19/300\n",
      "Average training loss: 0.14549663327799903\n",
      "Average test loss: 0.0020264888720379934\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13105460682180192\n",
      "Average test loss: 0.0019456566822094222\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11900498659743203\n",
      "Average test loss: 0.0019005370469142994\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10936873346567154\n",
      "Average test loss: 0.0019103174451738595\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10110318350791932\n",
      "Average test loss: 0.0018949449420389201\n",
      "Epoch 24/300\n",
      "Average training loss: 0.09441132341490852\n",
      "Average test loss: 0.0018311712595944602\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08852305059962802\n",
      "Average test loss: 0.0018352716214333972\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08350738170742988\n",
      "Average test loss: 0.0018287257719267573\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07927938666608599\n",
      "Average test loss: 0.001811549086175445\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07585162490937444\n",
      "Average test loss: 0.0017717088086841007\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07302216937475735\n",
      "Average test loss: 0.0018275501309997506\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07075083441204495\n",
      "Average test loss: 0.0017672517535587153\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06894995767540402\n",
      "Average test loss: 0.0017803640190718903\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06733724335829416\n",
      "Average test loss: 0.0017586920340028074\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0658396434850163\n",
      "Average test loss: 0.0017332278796368175\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06455861068103048\n",
      "Average test loss: 0.0017501793255408605\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06326009001996782\n",
      "Average test loss: 0.001775292951096263\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06218705630633566\n",
      "Average test loss: 0.001721252942043874\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06109109874566396\n",
      "Average test loss: 0.001704605130892661\n",
      "Epoch 38/300\n",
      "Average training loss: 0.059975360694858766\n",
      "Average test loss: 0.0017226915728921692\n",
      "Epoch 39/300\n",
      "Average training loss: 0.059161847882800635\n",
      "Average test loss: 0.0017070837181268467\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05807746597793367\n",
      "Average test loss: 0.0016740777988193763\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05740133868323432\n",
      "Average test loss: 0.0016845217434068522\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05652530264192157\n",
      "Average test loss: 0.0018137430753558874\n",
      "Epoch 43/300\n",
      "Average training loss: 0.055731096125311325\n",
      "Average test loss: 0.0016985265744022197\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05487647147476673\n",
      "Average test loss: 0.001691368736533655\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05426345571213299\n",
      "Average test loss: 0.0016965294435827268\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05346569189594852\n",
      "Average test loss: 0.0017481635922773017\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05315033283498552\n",
      "Average test loss: 0.0016881703988959392\n",
      "Epoch 48/300\n",
      "Average training loss: 0.052191890193356405\n",
      "Average test loss: 0.0016591630799488888\n",
      "Epoch 49/300\n",
      "Average training loss: 0.051669055008225974\n",
      "Average test loss: 0.0016925318744033576\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05126187353995111\n",
      "Average test loss: 0.0016714231744408609\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05046289686361949\n",
      "Average test loss: 0.001665636405762699\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05002682994802793\n",
      "Average test loss: 0.0017150695594027637\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04988778634203805\n",
      "Average test loss: 0.0016676156603627735\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04885312044951651\n",
      "Average test loss: 0.001687685662259658\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04839087629980511\n",
      "Average test loss: 0.0016637130034570064\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04792721865905656\n",
      "Average test loss: 0.0016604721409579119\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04740105088882976\n",
      "Average test loss: 0.0017642911272123456\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0468276475403044\n",
      "Average test loss: 0.001696657745478054\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04656718710064888\n",
      "Average test loss: 0.0016755199528092311\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04597636504968007\n",
      "Average test loss: 0.001660150911141601\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04553405889868736\n",
      "Average test loss: 0.0017153759118583467\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04509529435634613\n",
      "Average test loss: 0.0017071398330024547\n",
      "Epoch 63/300\n",
      "Average training loss: 0.044634960860013964\n",
      "Average test loss: 0.0016668658350697822\n",
      "Epoch 64/300\n",
      "Average training loss: 0.044230653951565425\n",
      "Average test loss: 0.0016561416539674005\n",
      "Epoch 65/300\n",
      "Average training loss: 0.043773751674426924\n",
      "Average test loss: 0.0017301630775133769\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04339040242963367\n",
      "Average test loss: 0.0016949611506942245\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04291557152403726\n",
      "Average test loss: 0.0016508979104045364\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04242880939775043\n",
      "Average test loss: 0.0017169503095663256\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04207491034931607\n",
      "Average test loss: 0.0016853216506747737\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04166356021496985\n",
      "Average test loss: 0.0016641705199662183\n",
      "Epoch 71/300\n",
      "Average training loss: 0.041729211944672794\n",
      "Average test loss: 0.0017060710338668691\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04092747806509336\n",
      "Average test loss: 0.0016790776999874248\n",
      "Epoch 73/300\n",
      "Average training loss: 0.040459877557224695\n",
      "Average test loss: 0.0017011728678933448\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04029520124528143\n",
      "Average test loss: 0.0017372048433042235\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03987218217055003\n",
      "Average test loss: 0.0017044127981902825\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03963169762161043\n",
      "Average test loss: 0.0017026475514802667\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03944007713596026\n",
      "Average test loss: 0.0017289404855627152\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03874062484171655\n",
      "Average test loss: 0.0017454729055364927\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03858406103319592\n",
      "Average test loss: 0.0017475710846483708\n",
      "Epoch 80/300\n",
      "Average training loss: 0.038269038832849926\n",
      "Average test loss: 0.0017796411806096633\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03800728665788968\n",
      "Average test loss: 0.0017364253739102018\n",
      "Epoch 82/300\n",
      "Average training loss: 0.037782419928246075\n",
      "Average test loss: 0.0018254532465297314\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03750352959003713\n",
      "Average test loss: 0.0017468064009315439\n",
      "Epoch 84/300\n",
      "Average training loss: 0.037146137634913125\n",
      "Average test loss: 0.001771767219942477\n",
      "Epoch 85/300\n",
      "Average training loss: 0.036936781131558946\n",
      "Average test loss: 0.001742255383792023\n",
      "Epoch 86/300\n",
      "Average training loss: 0.036735820606350895\n",
      "Average test loss: 0.0017777061318564745\n",
      "Epoch 87/300\n",
      "Average training loss: 0.036299245787991416\n",
      "Average test loss: 0.0017464539692219761\n",
      "Epoch 88/300\n",
      "Average training loss: 0.036232014290160604\n",
      "Average test loss: 0.0020150758556814656\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03603693513241079\n",
      "Average test loss: 0.0018707207178490029\n",
      "Epoch 90/300\n",
      "Average training loss: 0.035763524456156624\n",
      "Average test loss: 0.0017488915976136923\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03539850803713004\n",
      "Average test loss: 0.001804925807699975\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03519592191444503\n",
      "Average test loss: 0.001811702260437111\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03522539201047686\n",
      "Average test loss: 0.0017588929467731052\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0348687857290109\n",
      "Average test loss: 0.0018370948462850519\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03456515988873111\n",
      "Average test loss: 0.0017299581497079797\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03443730109598901\n",
      "Average test loss: 0.001827811840093798\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03421055569582515\n",
      "Average test loss: 0.0017754756077710124\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03404356481631597\n",
      "Average test loss: 0.001751836902461946\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03393533035450511\n",
      "Average test loss: 0.0017910215095099475\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03365391798483001\n",
      "Average test loss: 0.0017504456212951078\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03346820035245683\n",
      "Average test loss: 0.0019498690507478184\n",
      "Epoch 102/300\n",
      "Average training loss: 0.033328185384472214\n",
      "Average test loss: 0.0018245931089752251\n",
      "Epoch 103/300\n",
      "Average training loss: 0.033168941984574\n",
      "Average test loss: 0.001759017512616184\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03299183291859097\n",
      "Average test loss: 0.0017824439999336997\n",
      "Epoch 105/300\n",
      "Average training loss: 0.032812787019544175\n",
      "Average test loss: 0.0018621728522703052\n",
      "Epoch 106/300\n",
      "Average training loss: 0.032633396317561465\n",
      "Average test loss: 0.001780498802041014\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03258926460146904\n",
      "Average test loss: 0.0017748788950136965\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0324535719934437\n",
      "Average test loss: 0.0018624489368456933\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03222608945270379\n",
      "Average test loss: 0.0018796865360604393\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03202283377117581\n",
      "Average test loss: 0.0018012215163972643\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03193447020981047\n",
      "Average test loss: 0.0018532000604189105\n",
      "Epoch 112/300\n",
      "Average training loss: 0.031820888095431855\n",
      "Average test loss: 0.0018588665593415499\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03170135418242878\n",
      "Average test loss: 0.0018592875088668532\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03158992598454158\n",
      "Average test loss: 0.0017938312478363513\n",
      "Epoch 115/300\n",
      "Average training loss: 0.031434297275212075\n",
      "Average test loss: 0.0017715658457535836\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03144308338231511\n",
      "Average test loss: 0.0018085040267970826\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03114289752311177\n",
      "Average test loss: 0.001770729997712705\n",
      "Epoch 118/300\n",
      "Average training loss: 0.031060238344801795\n",
      "Average test loss: 0.0018421586175552673\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03105630817843808\n",
      "Average test loss: 0.0018786882465291355\n",
      "Epoch 120/300\n",
      "Average training loss: 0.030826880541112687\n",
      "Average test loss: 0.0018288299140209953\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03069328321185377\n",
      "Average test loss: 0.0019449015142810014\n",
      "Epoch 122/300\n",
      "Average training loss: 0.030559160803755123\n",
      "Average test loss: 0.001933222162226836\n",
      "Epoch 123/300\n",
      "Average training loss: 0.030494305168588955\n",
      "Average test loss: 0.0018682352933618758\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03040603626933363\n",
      "Average test loss: 0.0018754878833683\n",
      "Epoch 125/300\n",
      "Average training loss: 0.030298908943931262\n",
      "Average test loss: 0.0017970660375948582\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03012210132347213\n",
      "Average test loss: 0.001855651288707223\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0300450622704294\n",
      "Average test loss: 0.0018067490899314482\n",
      "Epoch 128/300\n",
      "Average training loss: 0.029984972122642728\n",
      "Average test loss: 0.0018379889295126002\n",
      "Epoch 129/300\n",
      "Average training loss: 0.029913359959920248\n",
      "Average test loss: 0.0018441172737835181\n",
      "Epoch 130/300\n",
      "Average training loss: 0.029722638986176914\n",
      "Average test loss: 0.00195366871315572\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02976789184908072\n",
      "Average test loss: 0.001931735084599091\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0295736908879545\n",
      "Average test loss: 0.0018350051673543123\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02939629392657015\n",
      "Average test loss: 0.0018060799191395441\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02936426494187779\n",
      "Average test loss: 0.0018844945272430778\n",
      "Epoch 135/300\n",
      "Average training loss: 0.029501369709769885\n",
      "Average test loss: 0.0018329929396924045\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02931716204351849\n",
      "Average test loss: 0.0019113749098032713\n",
      "Epoch 137/300\n",
      "Average training loss: 0.029252486369676062\n",
      "Average test loss: 0.0018630362351735434\n",
      "Epoch 138/300\n",
      "Average training loss: 0.029028906901677448\n",
      "Average test loss: 0.0019179101114471754\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02893869717584716\n",
      "Average test loss: 0.0019127170198286574\n",
      "Epoch 140/300\n",
      "Average training loss: 0.028926546106735864\n",
      "Average test loss: 0.0019063485368258422\n",
      "Epoch 141/300\n",
      "Average training loss: 0.028927134354909262\n",
      "Average test loss: 0.00183902872643537\n",
      "Epoch 142/300\n",
      "Average training loss: 0.028641154636939368\n",
      "Average test loss: 0.0019392152646970418\n",
      "Epoch 143/300\n",
      "Average training loss: 0.028712030834621854\n",
      "Average test loss: 0.0018462152977784475\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028592412365807427\n",
      "Average test loss: 0.0018551662183470196\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028568907363547218\n",
      "Average test loss: 0.0018615769488323065\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02858926885823409\n",
      "Average test loss: 0.0018707555705267521\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02831008533967866\n",
      "Average test loss: 0.0018655109830821553\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02834516661696964\n",
      "Average test loss: 0.0018423974843074877\n",
      "Epoch 149/300\n",
      "Average training loss: 0.028171337900890246\n",
      "Average test loss: 0.0018574300713630186\n",
      "Epoch 150/300\n",
      "Average training loss: 0.028202034706870716\n",
      "Average test loss: 0.0018426854105459318\n",
      "Epoch 151/300\n",
      "Average training loss: 0.028034476606382263\n",
      "Average test loss: 0.0018653112201847963\n",
      "Epoch 152/300\n",
      "Average training loss: 0.028034979669584167\n",
      "Average test loss: 0.001901981745329168\n",
      "Epoch 153/300\n",
      "Average training loss: 0.027928643835915458\n",
      "Average test loss: 0.0018898415512715776\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027932132151391773\n",
      "Average test loss: 0.0018950374587956402\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02785989309847355\n",
      "Average test loss: 0.001904490660979516\n",
      "Epoch 156/300\n",
      "Average training loss: 0.027771797249714535\n",
      "Average test loss: 0.001865482923678226\n",
      "Epoch 157/300\n",
      "Average training loss: 0.027656027987599374\n",
      "Average test loss: 0.0018933571560515298\n",
      "Epoch 158/300\n",
      "Average training loss: 0.027649272147152158\n",
      "Average test loss: 0.001923102048329181\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027531277615163063\n",
      "Average test loss: 0.0019322385085850126\n",
      "Epoch 160/300\n",
      "Average training loss: 0.027467085800237125\n",
      "Average test loss: 0.0019219366599702173\n",
      "Epoch 161/300\n",
      "Average training loss: 0.027435903653502465\n",
      "Average test loss: 0.0018916398364429673\n",
      "Epoch 162/300\n",
      "Average training loss: 0.027419133692979814\n",
      "Average test loss: 0.001890261833762957\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027357901440726386\n",
      "Average test loss: 0.0018632806004542444\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027180603752533596\n",
      "Average test loss: 0.0019407195702402127\n",
      "Epoch 165/300\n",
      "Average training loss: 0.027295500804980596\n",
      "Average test loss: 0.001895316964843207\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02722083842754364\n",
      "Average test loss: 0.001911579721245087\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02715697596470515\n",
      "Average test loss: 0.0019585696460886134\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02702996015052001\n",
      "Average test loss: 0.0019101134148529835\n",
      "Epoch 169/300\n",
      "Average training loss: 0.027036981316076385\n",
      "Average test loss: 0.00192165467225843\n",
      "Epoch 170/300\n",
      "Average training loss: 0.026971211064192983\n",
      "Average test loss: 0.0018810966794068616\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02693910302221775\n",
      "Average test loss: 0.001974668840877712\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026809078403645093\n",
      "Average test loss: 0.0018726955054120884\n",
      "Epoch 173/300\n",
      "Average training loss: 0.026812460747030045\n",
      "Average test loss: 0.001882063271684779\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02681039415796598\n",
      "Average test loss: 0.0018901418008738095\n",
      "Epoch 175/300\n",
      "Average training loss: 0.026688025552365515\n",
      "Average test loss: 0.0018726319693442848\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026687364530232217\n",
      "Average test loss: 0.0018535902191781336\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02660459243423409\n",
      "Average test loss: 0.001965350307420724\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02685670356121328\n",
      "Average test loss: 0.0018715737054331436\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02650924860106574\n",
      "Average test loss: 0.0019049910033742587\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02635793097813924\n",
      "Average test loss: 0.0018767090785420603\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026329788191450966\n",
      "Average test loss: 0.0019541169841670327\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026351509826050865\n",
      "Average test loss: 0.0019303284347471264\n",
      "Epoch 183/300\n",
      "Average training loss: 0.026272172485788663\n",
      "Average test loss: 0.0019440934838106235\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026220662150118087\n",
      "Average test loss: 0.0018905187412682507\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02620080828666687\n",
      "Average test loss: 0.001970715129127105\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026158303745918803\n",
      "Average test loss: 0.0019736596225864355\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02610302778085073\n",
      "Average test loss: 0.005213968131691217\n",
      "Epoch 188/300\n",
      "Average training loss: 0.026157768062419363\n",
      "Average test loss: 0.001920552127684156\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02594025466342767\n",
      "Average test loss: 0.0023548803360511857\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026044688830773037\n",
      "Average test loss: 0.0018949977524785533\n",
      "Epoch 191/300\n",
      "Average training loss: 0.025909747891955905\n",
      "Average test loss: 0.0018776050443864531\n",
      "Epoch 192/300\n",
      "Average training loss: 0.025911681612332663\n",
      "Average test loss: 0.002123046367532677\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02591254630519284\n",
      "Average test loss: 0.0019179593521273799\n",
      "Epoch 194/300\n",
      "Average training loss: 0.025782681581046848\n",
      "Average test loss: 0.002002139655045337\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02578985447353787\n",
      "Average test loss: 0.001913151069337295\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02573027975194984\n",
      "Average test loss: 0.0019896494332287047\n",
      "Epoch 197/300\n",
      "Average training loss: 0.025663019872374005\n",
      "Average test loss: 0.0019111866106589635\n",
      "Epoch 198/300\n",
      "Average training loss: 0.025639421630236837\n",
      "Average test loss: 0.0019322030544281007\n",
      "Epoch 199/300\n",
      "Average training loss: 0.025632825260361036\n",
      "Average test loss: 0.0019214315244721041\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02567327273885409\n",
      "Average test loss: 0.0019136266736313702\n",
      "Epoch 201/300\n",
      "Average training loss: 0.025507787669698398\n",
      "Average test loss: 0.0019404121512133215\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02542315517531501\n",
      "Average test loss: 0.001964928560062415\n",
      "Epoch 203/300\n",
      "Average training loss: 0.025496248338785436\n",
      "Average test loss: 0.001913158588214881\n",
      "Epoch 204/300\n",
      "Average training loss: 0.025379368465807704\n",
      "Average test loss: 0.0019503263370651339\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0254599353124698\n",
      "Average test loss: 0.0019527154689033825\n",
      "Epoch 206/300\n",
      "Average training loss: 0.025448033490114742\n",
      "Average test loss: 0.0022316027904550235\n",
      "Epoch 207/300\n",
      "Average training loss: 0.025362059106429417\n",
      "Average test loss: 0.0019392301102375818\n",
      "Epoch 208/300\n",
      "Average training loss: 0.025250076330370373\n",
      "Average test loss: 0.001900647210681604\n",
      "Epoch 209/300\n",
      "Average training loss: 0.025373658034536575\n",
      "Average test loss: 0.0019203003881913092\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025180553500023153\n",
      "Average test loss: 0.0020924495552769964\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02511250468591849\n",
      "Average test loss: 0.0019136427969982227\n",
      "Epoch 212/300\n",
      "Average training loss: 0.025091154207785925\n",
      "Average test loss: 0.001936045807061924\n",
      "Epoch 213/300\n",
      "Average training loss: 0.025149563022785718\n",
      "Average test loss: 0.0019009779321236743\n",
      "Epoch 214/300\n",
      "Average training loss: 0.025105787883202235\n",
      "Average test loss: 0.0019389158381770055\n",
      "Epoch 215/300\n",
      "Average training loss: 0.024930965160330137\n",
      "Average test loss: 0.0019515699034349785\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02505538416405519\n",
      "Average test loss: 0.001899159991182387\n",
      "Epoch 217/300\n",
      "Average training loss: 0.024941032752394676\n",
      "Average test loss: 0.0018918706459096735\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02491575175854895\n",
      "Average test loss: 0.0019080936919069953\n",
      "Epoch 219/300\n",
      "Average training loss: 0.024884198852711252\n",
      "Average test loss: 0.0019359553696380722\n",
      "Epoch 220/300\n",
      "Average training loss: 0.024946396365761757\n",
      "Average test loss: 0.0020818330927027595\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0248185147460964\n",
      "Average test loss: 0.001918982506988363\n",
      "Epoch 222/300\n",
      "Average training loss: 0.024774338839782608\n",
      "Average test loss: 0.001895565900641183\n",
      "Epoch 223/300\n",
      "Average training loss: 0.024762649739782015\n",
      "Average test loss: 0.0019918645980457465\n",
      "Epoch 224/300\n",
      "Average training loss: 0.024915936125649345\n",
      "Average test loss: 0.0019401566699768105\n",
      "Epoch 225/300\n",
      "Average training loss: 0.024684924062755374\n",
      "Average test loss: 0.0019717021810097828\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02467632862097687\n",
      "Average test loss: 0.0019653533601926435\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02466743407646815\n",
      "Average test loss: 0.0019058576863672998\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0245468669914537\n",
      "Average test loss: 0.002008246471174061\n",
      "Epoch 229/300\n",
      "Average training loss: 0.024599670057495436\n",
      "Average test loss: 0.001917437897477713\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02457077212797271\n",
      "Average test loss: 0.0019410032004428407\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02452416484720177\n",
      "Average test loss: 0.0019463344009386168\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024481136581963964\n",
      "Average test loss: 0.0019365045678698355\n",
      "Epoch 233/300\n",
      "Average training loss: 0.024391779641310375\n",
      "Average test loss: 0.001904288320698672\n",
      "Epoch 234/300\n",
      "Average training loss: 0.024489032361242506\n",
      "Average test loss: 0.0020570822939690618\n",
      "Epoch 235/300\n",
      "Average training loss: 0.024516470172339017\n",
      "Average test loss: 0.0019158725926859512\n",
      "Epoch 236/300\n",
      "Average training loss: 0.024393437291185063\n",
      "Average test loss: 0.0019308980927906102\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02421698561641905\n",
      "Average test loss: 0.0018999950649837654\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02434171614713139\n",
      "Average test loss: 0.002097141661060353\n",
      "Epoch 239/300\n",
      "Average training loss: 0.024311715650889607\n",
      "Average test loss: 0.00212587280323108\n",
      "Epoch 240/300\n",
      "Average training loss: 0.024228949053419963\n",
      "Average test loss: 0.001959120750427246\n",
      "Epoch 241/300\n",
      "Average training loss: 0.024213987575636968\n",
      "Average test loss: 0.001973932034853432\n",
      "Epoch 242/300\n",
      "Average training loss: 0.024261614799499513\n",
      "Average test loss: 0.00191845720488992\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02419934528072675\n",
      "Average test loss: 0.0019429472025690806\n",
      "Epoch 244/300\n",
      "Average training loss: 0.024099933824605412\n",
      "Average test loss: 0.001937950981160005\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02404724679556158\n",
      "Average test loss: 0.0018976751203234824\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02407915560901165\n",
      "Average test loss: 0.0019524530416561497\n",
      "Epoch 247/300\n",
      "Average training loss: 0.023999738307462798\n",
      "Average test loss: 0.0019567795597637693\n",
      "Epoch 248/300\n",
      "Average training loss: 0.024089793430434332\n",
      "Average test loss: 0.00218255959244238\n",
      "Epoch 249/300\n",
      "Average training loss: 0.024029987613360088\n",
      "Average test loss: 0.0019712379367815124\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02397848878469732\n",
      "Average test loss: 0.001945023597114616\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02396029979818397\n",
      "Average test loss: 0.001900727843451831\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023930808174941275\n",
      "Average test loss: 0.0019212906712459193\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02387792262269391\n",
      "Average test loss: 0.0019155465676966642\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02392078259587288\n",
      "Average test loss: 0.0019360942951300079\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023852013341254658\n",
      "Average test loss: 0.0019408236959328254\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023897823775808017\n",
      "Average test loss: 0.0019891903166555694\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023824089548654028\n",
      "Average test loss: 0.0019670848728468022\n",
      "Epoch 258/300\n",
      "Average training loss: 0.023826678183343674\n",
      "Average test loss: 0.001972269809080495\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02379421115666628\n",
      "Average test loss: 0.001958496539129151\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02396209697259797\n",
      "Average test loss: 0.0019851481682724424\n",
      "Epoch 261/300\n",
      "Average training loss: 0.023667274571127363\n",
      "Average test loss: 0.0019256325316511923\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02371346040070057\n",
      "Average test loss: 0.0019458189603562155\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023676066843171913\n",
      "Average test loss: 0.001966046997449464\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023642165884375572\n",
      "Average test loss: 0.0019528127873523367\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023582427726851568\n",
      "Average test loss: 0.001951349892343084\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023598833734790485\n",
      "Average test loss: 0.0019984150238645574\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02367284418642521\n",
      "Average test loss: 0.0019475307991314264\n",
      "Epoch 268/300\n",
      "Average training loss: 0.023702575451797908\n",
      "Average test loss: 0.0019570635684455434\n",
      "Epoch 269/300\n",
      "Average training loss: 0.023480185319979984\n",
      "Average test loss: 0.0019832821754324765\n",
      "Epoch 270/300\n",
      "Average training loss: 0.023472189234362707\n",
      "Average test loss: 0.0019469452540700634\n",
      "Epoch 271/300\n",
      "Average training loss: 0.023533461332321166\n",
      "Average test loss: 0.0019258700566780237\n",
      "Epoch 272/300\n",
      "Average training loss: 0.023433906773726147\n",
      "Average test loss: 0.0019605690346409877\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0234021616164181\n",
      "Average test loss: 0.0020508582265012796\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023447828849156697\n",
      "Average test loss: 0.0019126703296270635\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023383598865734205\n",
      "Average test loss: 0.001991977985637883\n",
      "Epoch 276/300\n",
      "Average training loss: 0.023397307435671488\n",
      "Average test loss: 0.001961840869134499\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023430728516644902\n",
      "Average test loss: 0.0019305118146455951\n",
      "Epoch 278/300\n",
      "Average training loss: 0.023465809368424944\n",
      "Average test loss: 0.0019524984266608954\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023313056276904214\n",
      "Average test loss: 0.0019085130048915745\n",
      "Epoch 280/300\n",
      "Average training loss: 0.023245253648195\n",
      "Average test loss: 0.0019904175562163195\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02323471892045604\n",
      "Average test loss: 0.002000539100004567\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023229874791370498\n",
      "Average test loss: 0.0020482458603671857\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02327072150342994\n",
      "Average test loss: 0.0019521096290813552\n",
      "Epoch 284/300\n",
      "Average training loss: 0.023214173153042795\n",
      "Average test loss: 0.0019239649211780893\n",
      "Epoch 285/300\n",
      "Average training loss: 0.023235240565405952\n",
      "Average test loss: 0.0019835003424021934\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02318068550858233\n",
      "Average test loss: 0.002026619918230507\n",
      "Epoch 287/300\n",
      "Average training loss: 0.023169097955028216\n",
      "Average test loss: 0.0019218013907472292\n",
      "Epoch 288/300\n",
      "Average training loss: 0.023133491612142988\n",
      "Average test loss: 0.001959860461194896\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02309464149011506\n",
      "Average test loss: 0.001994855044202672\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02314174744155672\n",
      "Average test loss: 0.00200815359937648\n",
      "Epoch 291/300\n",
      "Average training loss: 0.023074202411704593\n",
      "Average test loss: 0.0019467563048625985\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0232833344489336\n",
      "Average test loss: 0.0019423408829089667\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022980754165185822\n",
      "Average test loss: 0.002003821657763587\n",
      "Epoch 294/300\n",
      "Average training loss: 0.022989890396595002\n",
      "Average test loss: 0.0020289414587120213\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022995306904117267\n",
      "Average test loss: 0.0019355542566627264\n",
      "Epoch 296/300\n",
      "Average training loss: 0.022937199383974077\n",
      "Average test loss: 0.001926078826499482\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02293643995953931\n",
      "Average test loss: 0.0019491744565053118\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022906722704569497\n",
      "Average test loss: 0.0019784795991662474\n",
      "Epoch 299/300\n",
      "Average training loss: 0.023024010358585253\n",
      "Average test loss: 0.0019439921128667063\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022929736180437935\n",
      "Average test loss: 0.0019783810154638355\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_32_Depth5/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.41\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.74\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.46\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.49\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.65\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.81\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.32\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.62\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.53\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.69\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.48\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.69\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.71\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.55\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.64\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.86\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.97\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.92\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.10\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.21\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.26\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.22\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.50\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.46\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
