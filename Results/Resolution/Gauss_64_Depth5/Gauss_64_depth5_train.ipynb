{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/64 x 64 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_64x64_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15598707983228896\n",
      "Average test loss: 0.011658013568984138\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0644736950662401\n",
      "Average test loss: 0.009328207202669647\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05750394963224729\n",
      "Average test loss: 0.009705708081523578\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05423388868570328\n",
      "Average test loss: 0.008976573336041635\n",
      "Epoch 5/300\n",
      "Average training loss: 0.052451778199937606\n",
      "Average test loss: 0.00797944614249799\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04938272934820917\n",
      "Average test loss: 0.008738897619148095\n",
      "Epoch 7/300\n",
      "Average training loss: 0.047450244002872045\n",
      "Average test loss: 0.007640497989952564\n",
      "Epoch 8/300\n",
      "Average training loss: 0.046034170309702555\n",
      "Average test loss: 0.008080495287560755\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04472726974884669\n",
      "Average test loss: 0.007413036768221193\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04393243000242445\n",
      "Average test loss: 0.0071025846277674035\n",
      "Epoch 11/300\n",
      "Average training loss: 0.043013613253831864\n",
      "Average test loss: 0.00700354756663243\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04240395419134034\n",
      "Average test loss: 0.006775665150748358\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04160074160165257\n",
      "Average test loss: 0.006930407786534892\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04101010963983006\n",
      "Average test loss: 0.006589917111314006\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04067559271719721\n",
      "Average test loss: 0.006718995628257593\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04005212825536728\n",
      "Average test loss: 0.0065239681282805075\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03958110907342699\n",
      "Average test loss: 0.006392895560711622\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03923805043101311\n",
      "Average test loss: 0.0064567769836220475\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03872123198376762\n",
      "Average test loss: 0.006272501153664457\n",
      "Epoch 20/300\n",
      "Average training loss: 0.038442586100763745\n",
      "Average test loss: 0.006208943056977457\n",
      "Epoch 21/300\n",
      "Average training loss: 0.038180866258011925\n",
      "Average test loss: 0.006258821058190531\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03782484117150307\n",
      "Average test loss: 0.006328896488166517\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03753990132610003\n",
      "Average test loss: 0.006425266304777728\n",
      "Epoch 24/300\n",
      "Average training loss: 0.037305793169471954\n",
      "Average test loss: 0.006045134494702021\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03709377640154626\n",
      "Average test loss: 0.006042783124993244\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03685101015037961\n",
      "Average test loss: 0.006463338619718949\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03661792566710048\n",
      "Average test loss: 0.006382138354910745\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03640025834904777\n",
      "Average test loss: 0.005762927268528276\n",
      "Epoch 29/300\n",
      "Average training loss: 0.036300460050503414\n",
      "Average test loss: 0.0060042585395276545\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03613741770055559\n",
      "Average test loss: 0.007485513434641891\n",
      "Epoch 31/300\n",
      "Average training loss: 0.035914704723490606\n",
      "Average test loss: 0.0061502526139633525\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03572031599614355\n",
      "Average test loss: 0.005849230600314008\n",
      "Epoch 33/300\n",
      "Average training loss: 0.035570056565933754\n",
      "Average test loss: 0.006014018226000998\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03554034556282891\n",
      "Average test loss: 0.005751523462848531\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03528795409202576\n",
      "Average test loss: 0.005710815886656443\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03512099580963453\n",
      "Average test loss: 0.007781846524112754\n",
      "Epoch 37/300\n",
      "Average training loss: 0.034984209577242534\n",
      "Average test loss: 0.005730344126207961\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03485658318135473\n",
      "Average test loss: 0.005647489800635311\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03474837053318818\n",
      "Average test loss: 0.0060674232405920825\n",
      "Epoch 40/300\n",
      "Average training loss: 0.034679510924551224\n",
      "Average test loss: 0.00590408144146204\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03452319528162479\n",
      "Average test loss: 0.005626208675818311\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03445699872076512\n",
      "Average test loss: 0.005718362297034926\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03448836213350296\n",
      "Average test loss: 0.008046786898540126\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03429816154638926\n",
      "Average test loss: 0.005593900016198555\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03415802502301004\n",
      "Average test loss: 0.006507716134190559\n",
      "Epoch 46/300\n",
      "Average training loss: 0.034058358702394695\n",
      "Average test loss: 0.005464002049217621\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03400642089711295\n",
      "Average test loss: 0.005506439381175571\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03389569081862768\n",
      "Average test loss: 0.00554744177353051\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03381371222767565\n",
      "Average test loss: 0.005450578110913436\n",
      "Epoch 50/300\n",
      "Average training loss: 0.033791073338852985\n",
      "Average test loss: 0.007530102838244703\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03368416223261091\n",
      "Average test loss: 0.005517760172486306\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03355726734134886\n",
      "Average test loss: 0.005812296339621146\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0334419030447801\n",
      "Average test loss: 0.0055152792545656365\n",
      "Epoch 54/300\n",
      "Average training loss: 0.033444832594858274\n",
      "Average test loss: 0.005574380848556757\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03341774171590805\n",
      "Average test loss: 0.005390779514693552\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03339474547571606\n",
      "Average test loss: 0.005997307812174162\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03326511064502928\n",
      "Average test loss: 0.005519217561516497\n",
      "Epoch 58/300\n",
      "Average training loss: 0.033222588620252076\n",
      "Average test loss: 0.005518613435741928\n",
      "Epoch 59/300\n",
      "Average training loss: 0.033149696720971004\n",
      "Average test loss: 0.005400820991852217\n",
      "Epoch 60/300\n",
      "Average training loss: 0.033091537821624016\n",
      "Average test loss: 0.005444460057136086\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03303326159881221\n",
      "Average test loss: 0.005344433420027296\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03295555044876205\n",
      "Average test loss: 0.005555738778991831\n",
      "Epoch 63/300\n",
      "Average training loss: 0.032945033722453644\n",
      "Average test loss: 0.0053934778658052285\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03284187962942653\n",
      "Average test loss: 0.0053309238250884745\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03289719252785047\n",
      "Average test loss: 0.012677453842427995\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03287448108196259\n",
      "Average test loss: 0.0054931762446131975\n",
      "Epoch 67/300\n",
      "Average training loss: 0.032766064032912254\n",
      "Average test loss: 0.009836743793139854\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03263953730795119\n",
      "Average test loss: 0.005356944237732225\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03266923916339874\n",
      "Average test loss: 0.0053920027468767435\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03269636400375101\n",
      "Average test loss: 0.005285189278837707\n",
      "Epoch 71/300\n",
      "Average training loss: 0.032506667943464386\n",
      "Average test loss: 0.005292010869416925\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03244759585460027\n",
      "Average test loss: 0.005588653880274958\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03251631432771683\n",
      "Average test loss: 0.005735257308516237\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03241985659466849\n",
      "Average test loss: 0.005298820638615224\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03238439875178867\n",
      "Average test loss: 0.005570976669589678\n",
      "Epoch 76/300\n",
      "Average training loss: 0.032430035094420114\n",
      "Average test loss: 0.005245488420956665\n",
      "Epoch 77/300\n",
      "Average training loss: 0.032354201558563445\n",
      "Average test loss: 0.005340232651680708\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03233861646056175\n",
      "Average test loss: 0.005317227490039335\n",
      "Epoch 79/300\n",
      "Average training loss: 0.032297341687811745\n",
      "Average test loss: 0.005332704772137933\n",
      "Epoch 80/300\n",
      "Average training loss: 0.032242394530110886\n",
      "Average test loss: 0.005469740620917744\n",
      "Epoch 81/300\n",
      "Average training loss: 0.032272241983148785\n",
      "Average test loss: 0.005418791543692351\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03213649112648434\n",
      "Average test loss: 0.005380272060632706\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03220243372519811\n",
      "Average test loss: 0.005473798063687152\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03219539364510112\n",
      "Average test loss: 0.005351588148209784\n",
      "Epoch 85/300\n",
      "Average training loss: 0.032074054489533106\n",
      "Average test loss: 0.005264176306625208\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03207611735330688\n",
      "Average test loss: 0.005464104003169471\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03204007647766007\n",
      "Average test loss: 0.0053183424257569846\n",
      "Epoch 88/300\n",
      "Average training loss: 0.032023668123616116\n",
      "Average test loss: 0.005280160579830408\n",
      "Epoch 89/300\n",
      "Average training loss: 0.032006812479760914\n",
      "Average test loss: 0.0055385735295712946\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03193292696608437\n",
      "Average test loss: 0.005252631011522479\n",
      "Epoch 91/300\n",
      "Average training loss: 0.031914241737789575\n",
      "Average test loss: 0.005224439652429687\n",
      "Epoch 92/300\n",
      "Average training loss: 0.031867851148049034\n",
      "Average test loss: 0.00528425646159384\n",
      "Epoch 93/300\n",
      "Average training loss: 0.031917943815390266\n",
      "Average test loss: 0.005247069321986702\n",
      "Epoch 94/300\n",
      "Average training loss: 0.031889751546912724\n",
      "Average test loss: 0.005219244727657901\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03181793489721086\n",
      "Average test loss: 0.005484235059469938\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03183778270913495\n",
      "Average test loss: 0.005357941639920076\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0317893760005633\n",
      "Average test loss: 0.005226117385758294\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03175082043310006\n",
      "Average test loss: 0.005271948748992549\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03178783001502355\n",
      "Average test loss: 0.005300497163914972\n",
      "Epoch 100/300\n",
      "Average training loss: 0.031724294030004076\n",
      "Average test loss: 0.005293214026010699\n",
      "Epoch 101/300\n",
      "Average training loss: 0.031665755689144134\n",
      "Average test loss: 0.005280995977421601\n",
      "Epoch 102/300\n",
      "Average training loss: 0.031637834002574285\n",
      "Average test loss: 0.0053505689650774\n",
      "Epoch 103/300\n",
      "Average training loss: 0.031655962912572756\n",
      "Average test loss: 0.0052511487267911435\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03163468759920862\n",
      "Average test loss: 0.0060274476417236855\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03160347999797927\n",
      "Average test loss: 0.005333608304460844\n",
      "Epoch 106/300\n",
      "Average training loss: 0.031583504034413235\n",
      "Average test loss: 0.005300114456150267\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0315068055888017\n",
      "Average test loss: 0.00528401059905688\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03151556383570035\n",
      "Average test loss: 0.005510225764993164\n",
      "Epoch 109/300\n",
      "Average training loss: 0.031532721781068376\n",
      "Average test loss: 0.053483452588319776\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03161828561955028\n",
      "Average test loss: 0.00528045131224725\n",
      "Epoch 111/300\n",
      "Average training loss: 0.031479811214738425\n",
      "Average test loss: 0.005344726727861497\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03144115699662103\n",
      "Average test loss: 0.005414854610959688\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03144548123578231\n",
      "Average test loss: 0.00521237849858072\n",
      "Epoch 114/300\n",
      "Average training loss: 0.031427655708458686\n",
      "Average test loss: 0.005441649709724718\n",
      "Epoch 115/300\n",
      "Average training loss: 0.031394591265254554\n",
      "Average test loss: 0.005284523372434908\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031384810182783336\n",
      "Average test loss: 0.0053524256650772355\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03137319607039293\n",
      "Average test loss: 0.005405056120620834\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03138264892829789\n",
      "Average test loss: 0.005269143163743946\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0313270397004154\n",
      "Average test loss: 0.005386987551632855\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03131163327561484\n",
      "Average test loss: 0.0051646749298605655\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03130811302529441\n",
      "Average test loss: 0.00525472927507427\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03125053721997473\n",
      "Average test loss: 0.005227173270450698\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03130967183907827\n",
      "Average test loss: 0.005302723218583398\n",
      "Epoch 124/300\n",
      "Average training loss: 0.031243307063976922\n",
      "Average test loss: 0.005198260062270694\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03129429245988528\n",
      "Average test loss: 0.005333771699418624\n",
      "Epoch 126/300\n",
      "Average training loss: 0.031259065248899986\n",
      "Average test loss: 0.005474805746020542\n",
      "Epoch 127/300\n",
      "Average training loss: 0.031212248729334938\n",
      "Average test loss: 0.005280733072095447\n",
      "Epoch 128/300\n",
      "Average training loss: 0.031218152387274635\n",
      "Average test loss: 0.005405570156458352\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03143665122654703\n",
      "Average test loss: 0.005276537545025349\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03109355029794905\n",
      "Average test loss: 0.005150059154464139\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03113474044203758\n",
      "Average test loss: 0.0052188093802995154\n",
      "Epoch 132/300\n",
      "Average training loss: 0.031097016586197748\n",
      "Average test loss: 0.005181684758928087\n",
      "Epoch 133/300\n",
      "Average training loss: 0.031095714744594362\n",
      "Average test loss: 0.00569637874306904\n",
      "Epoch 134/300\n",
      "Average training loss: 0.031144414802392325\n",
      "Average test loss: 0.005247244308392207\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031100974811447992\n",
      "Average test loss: 0.005213758079542054\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031029685210850505\n",
      "Average test loss: 0.006435373737166325\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031058547011680072\n",
      "Average test loss: 0.005323307555375828\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03101317160328229\n",
      "Average test loss: 0.00530795671335525\n",
      "Epoch 139/300\n",
      "Average training loss: 0.031017191035879983\n",
      "Average test loss: 0.006284006911019484\n",
      "Epoch 140/300\n",
      "Average training loss: 0.031013635400268767\n",
      "Average test loss: 0.005353020884717504\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0310438435855839\n",
      "Average test loss: 1.9808425242371028\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03121206208070119\n",
      "Average test loss: 0.005280580024545392\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030904164211617575\n",
      "Average test loss: 0.005209969883370731\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030922538611623977\n",
      "Average test loss: 0.005215060389704175\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030987523112032147\n",
      "Average test loss: 0.00519289636073841\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03090246190627416\n",
      "Average test loss: 0.005353593970959385\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030948507131801712\n",
      "Average test loss: 0.005230997681410776\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03126866089966562\n",
      "Average test loss: 0.00518534349815713\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030806694997681513\n",
      "Average test loss: 0.00521034460204343\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03086920068661372\n",
      "Average test loss: 0.005197994381603267\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030822803573475943\n",
      "Average test loss: 0.005399386960185237\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030908565123875936\n",
      "Average test loss: 0.005218616899102926\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03077930276427004\n",
      "Average test loss: 0.005274434707644913\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030839463498857286\n",
      "Average test loss: 0.005835248712657226\n",
      "Epoch 155/300\n",
      "Average training loss: 0.030837706509563658\n",
      "Average test loss: 0.005223377072148853\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030780757106012767\n",
      "Average test loss: 0.0053691087300992675\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03075891065597534\n",
      "Average test loss: 0.005311386961075994\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03078229409125116\n",
      "Average test loss: 0.005281376035263141\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030756427788072162\n",
      "Average test loss: 0.005286275982442829\n",
      "Epoch 160/300\n",
      "Average training loss: 0.030736305915647084\n",
      "Average test loss: 0.005620131687571605\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030736989223294788\n",
      "Average test loss: 0.005289199179659287\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030750535395410325\n",
      "Average test loss: 0.0052613282476862274\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030685731924242444\n",
      "Average test loss: 0.0051859565408279495\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030699339866638184\n",
      "Average test loss: 0.005202486657020118\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030691011195381483\n",
      "Average test loss: 0.005159491571701235\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030663124966952535\n",
      "Average test loss: 0.005215328032771746\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030675945406158766\n",
      "Average test loss: 0.005428645578523477\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03066251654922962\n",
      "Average test loss: 0.0052316525346703\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030709978583786223\n",
      "Average test loss: 0.005386930272810989\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030592924060093032\n",
      "Average test loss: 0.005336127692419622\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030683677948183483\n",
      "Average test loss: 0.005187608773923582\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030619255426857207\n",
      "Average test loss: 0.005226567092869017\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030586617406871585\n",
      "Average test loss: 0.005250615731502573\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030616625645094448\n",
      "Average test loss: 0.005304123029733697\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03056969368788931\n",
      "Average test loss: 0.005372582160764271\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030514687430527473\n",
      "Average test loss: 0.0054224117861853705\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030524649860130416\n",
      "Average test loss: 0.00534489607769582\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030576641864246794\n",
      "Average test loss: 0.005221782914259367\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030639388629131847\n",
      "Average test loss: 0.005358318514294094\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030537221683396232\n",
      "Average test loss: 0.005424159590154886\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030577621711624994\n",
      "Average test loss: 0.005449759457467331\n",
      "Epoch 182/300\n",
      "Average training loss: 0.030512817036774425\n",
      "Average test loss: 0.005212096083288391\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030526050900419554\n",
      "Average test loss: 0.0052127398438751695\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030485232937667105\n",
      "Average test loss: 0.0059744337478445635\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03051357524262534\n",
      "Average test loss: 0.0052830788689768975\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030462181177404193\n",
      "Average test loss: 0.0052540512916942434\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0304373568776581\n",
      "Average test loss: 0.0052073496782945265\n",
      "Epoch 188/300\n",
      "Average training loss: 0.030456179731422\n",
      "Average test loss: 0.005190866831690073\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03043657871087392\n",
      "Average test loss: 0.0055187088168329665\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03044350260661708\n",
      "Average test loss: 0.005424112788918946\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03044256289800008\n",
      "Average test loss: 0.005319496841894256\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03042065578699112\n",
      "Average test loss: 0.005219986634328961\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030387199482984014\n",
      "Average test loss: 0.00519794910815027\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03040128799610668\n",
      "Average test loss: 0.005299767474747367\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030375014040205213\n",
      "Average test loss: 0.005278848244084252\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030342463157243198\n",
      "Average test loss: 0.005304590751313501\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030364317450258468\n",
      "Average test loss: 0.0052943186346027585\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0303283022062646\n",
      "Average test loss: 0.005363313132690059\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03030982272161378\n",
      "Average test loss: 0.005295479489283429\n",
      "Epoch 200/300\n",
      "Average training loss: 0.030374206746617952\n",
      "Average test loss: 0.005466472365376022\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030357364330026837\n",
      "Average test loss: 0.0052294681883520554\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030273167625069618\n",
      "Average test loss: 0.005331674814017282\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030341665368941097\n",
      "Average test loss: 0.005249516109832459\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030256026999817952\n",
      "Average test loss: 0.005251328159123659\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03030847805738449\n",
      "Average test loss: 0.005222306905521287\n",
      "Epoch 206/300\n",
      "Average training loss: 0.030246713136633237\n",
      "Average test loss: 0.005199461474600766\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03027565093835195\n",
      "Average test loss: 0.005310536168929603\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030271672636270522\n",
      "Average test loss: 0.005238618273288012\n",
      "Epoch 209/300\n",
      "Average training loss: 0.030219908316930135\n",
      "Average test loss: 0.0056101906059516805\n",
      "Epoch 210/300\n",
      "Average training loss: 0.030247928846213554\n",
      "Average test loss: 0.005417254824191332\n",
      "Epoch 211/300\n",
      "Average training loss: 0.030271425684293113\n",
      "Average test loss: 0.00537078203053938\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030165958682696024\n",
      "Average test loss: 0.0052306210502154294\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03021841029326121\n",
      "Average test loss: 0.005650279798855384\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03020454815692372\n",
      "Average test loss: 0.0053798965335720116\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030168508797883988\n",
      "Average test loss: 0.005372161172330379\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030223878554171985\n",
      "Average test loss: 0.006014728413687812\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030236154437065125\n",
      "Average test loss: 0.005194164143875241\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030146161569489374\n",
      "Average test loss: 0.0053453484794331926\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03015012420217196\n",
      "Average test loss: 0.005255146938065688\n",
      "Epoch 220/300\n",
      "Average training loss: 0.030146034464240076\n",
      "Average test loss: 0.005299648327545987\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030132848603857888\n",
      "Average test loss: 0.005288922723796633\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030114904610647097\n",
      "Average test loss: 0.005517337377286619\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0301556412908766\n",
      "Average test loss: 0.00530267608910799\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03015439493788613\n",
      "Average test loss: 0.005375927442477809\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03012966762151983\n",
      "Average test loss: 0.005236814601967732\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030062756611241234\n",
      "Average test loss: 0.0052574551047550305\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03010324853161971\n",
      "Average test loss: 0.005296029573099481\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030060225913921993\n",
      "Average test loss: 0.0064413443336056335\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030091655318935713\n",
      "Average test loss: 0.005231553449812862\n",
      "Epoch 230/300\n",
      "Average training loss: 0.030045710570282408\n",
      "Average test loss: 0.005483230140474108\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030075601008203296\n",
      "Average test loss: 0.005844508421089914\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030085424898399247\n",
      "Average test loss: 0.005296755086630583\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030110243909888798\n",
      "Average test loss: 0.005279165419439475\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029992586818006303\n",
      "Average test loss: 0.005274792075157166\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03010533799065484\n",
      "Average test loss: 0.005295138786236445\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030027397947178947\n",
      "Average test loss: 0.005222644902765751\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030045862442917293\n",
      "Average test loss: 0.005236416206177738\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03010987441076173\n",
      "Average test loss: 0.005511494295464622\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030056680219040975\n",
      "Average test loss: 0.005472674098279741\n",
      "Epoch 240/300\n",
      "Average training loss: 0.029949078511860634\n",
      "Average test loss: 0.005406216529922353\n",
      "Epoch 241/300\n",
      "Average training loss: 0.029926541273792583\n",
      "Average test loss: 0.005598503168672323\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029994717655910386\n",
      "Average test loss: 0.005194164049915142\n",
      "Epoch 243/300\n",
      "Average training loss: 0.029966426488425995\n",
      "Average test loss: 0.0052728333804342485\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02995497289631102\n",
      "Average test loss: 0.005289577387687233\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029997802116804653\n",
      "Average test loss: 0.005346376958820555\n",
      "Epoch 246/300\n",
      "Average training loss: 0.029949030626151295\n",
      "Average test loss: 0.0052785290905998815\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02996071860525343\n",
      "Average test loss: 0.005335111627148257\n",
      "Epoch 248/300\n",
      "Average training loss: 0.029946824009219806\n",
      "Average test loss: 0.005437296660410033\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029937650127543344\n",
      "Average test loss: 0.005564556689725982\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02994338690241178\n",
      "Average test loss: 0.005248222864750358\n",
      "Epoch 251/300\n",
      "Average training loss: 0.029945865008566115\n",
      "Average test loss: 0.00534174540038738\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02997518468234274\n",
      "Average test loss: 0.0053209171589050025\n",
      "Epoch 253/300\n",
      "Average training loss: 0.029882501375344064\n",
      "Average test loss: 0.005311540431860421\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02992684218287468\n",
      "Average test loss: 0.0052051919901536575\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02989660048484802\n",
      "Average test loss: 0.005348907904078563\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02991876948210928\n",
      "Average test loss: 0.005286436753140555\n",
      "Epoch 257/300\n",
      "Average training loss: 0.029891554173496034\n",
      "Average test loss: 0.005714265248013867\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0298665595319536\n",
      "Average test loss: 0.005414046364939875\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02983745444317659\n",
      "Average test loss: 0.005381084556380908\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029856272354722022\n",
      "Average test loss: 0.005348907484362523\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029900787280665503\n",
      "Average test loss: 0.005385942102306419\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029887957155704498\n",
      "Average test loss: 0.005775704698430167\n",
      "Epoch 263/300\n",
      "Average training loss: 0.029847293087177807\n",
      "Average test loss: 0.005320097628153032\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029867302149534227\n",
      "Average test loss: 0.005310099377814266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029820648362239202\n",
      "Average test loss: 0.00560738859574\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02978729623556137\n",
      "Average test loss: 0.005327357271065315\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029802482252319653\n",
      "Average test loss: 0.006490019248591529\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029838714251915612\n",
      "Average test loss: 0.005323461044993666\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029830419078469277\n",
      "Average test loss: 0.005233617226282756\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029814797977606455\n",
      "Average test loss: 0.0056669064226249856\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0297681549721294\n",
      "Average test loss: 0.006067695486048857\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029857962754037647\n",
      "Average test loss: 0.005497639924908678\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029770133887728055\n",
      "Average test loss: 0.005386156458821562\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029745889655417867\n",
      "Average test loss: 0.005229109709047609\n",
      "Epoch 275/300\n",
      "Average training loss: 0.029772762260503238\n",
      "Average test loss: 0.010491404806574185\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029721359638704193\n",
      "Average test loss: 0.005459335330873728\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029859063951505557\n",
      "Average test loss: 0.005354814086523321\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029708014282915328\n",
      "Average test loss: 0.005690456861837043\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029764860750900374\n",
      "Average test loss: 0.005400334311028322\n",
      "Epoch 280/300\n",
      "Average training loss: 0.029677709258264965\n",
      "Average test loss: 0.005255595336564713\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029743936051925022\n",
      "Average test loss: 0.005382433664260639\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02971707060105271\n",
      "Average test loss: 0.005280425549381309\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029718670659595065\n",
      "Average test loss: 0.00525355604580707\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02971746697359615\n",
      "Average test loss: 0.005245664110200273\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02968047108915117\n",
      "Average test loss: 0.006364743505087164\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029706374138593675\n",
      "Average test loss: 0.005273055354340209\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029744870296782917\n",
      "Average test loss: 0.005394845652497477\n",
      "Epoch 288/300\n",
      "Average training loss: 0.029675184248222245\n",
      "Average test loss: 0.00535924236352245\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029682179931137295\n",
      "Average test loss: 0.005508657458962666\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029643556863069534\n",
      "Average test loss: 0.005242474949194325\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02957903168598811\n",
      "Average test loss: 0.0053054886729353\n",
      "Epoch 292/300\n",
      "Average training loss: 0.029654575117760235\n",
      "Average test loss: 0.005525233099651005\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02971714757548438\n",
      "Average test loss: 0.005505754383073913\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029640611131985982\n",
      "Average test loss: 0.005296255424825681\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02972064754532443\n",
      "Average test loss: 0.005234757020655605\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02961195313433806\n",
      "Average test loss: 0.005294226163791285\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029654897001054552\n",
      "Average test loss: 0.005572885947508944\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0296352755592929\n",
      "Average test loss: 0.005279529041714138\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029574329319927428\n",
      "Average test loss: 0.005469028254350026\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029648249576489132\n",
      "Average test loss: 0.0053532777685258125\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13643981365362803\n",
      "Average test loss: 0.007782794587314129\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05015433255500264\n",
      "Average test loss: 0.010073446438544326\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04506699663069513\n",
      "Average test loss: 0.006238849333590931\n",
      "Epoch 4/300\n",
      "Average training loss: 0.041259127921528284\n",
      "Average test loss: 0.005644259562095006\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03877035537362099\n",
      "Average test loss: 0.005518178744034635\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03720094609260559\n",
      "Average test loss: 0.006629072500185834\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03542301952176624\n",
      "Average test loss: 0.006420160789456632\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03385312932729721\n",
      "Average test loss: 0.005046347132987446\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03305591798159811\n",
      "Average test loss: 0.004916168206267887\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03204616590175364\n",
      "Average test loss: 0.004549103944251935\n",
      "Epoch 11/300\n",
      "Average training loss: 0.031085933771398334\n",
      "Average test loss: 0.004426653479743335\n",
      "Epoch 12/300\n",
      "Average training loss: 0.030433755058381293\n",
      "Average test loss: 0.004390740145411756\n",
      "Epoch 13/300\n",
      "Average training loss: 0.029733849500616392\n",
      "Average test loss: 0.004062239399800698\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02925982038842307\n",
      "Average test loss: 0.0041696554207139545\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02864453607963191\n",
      "Average test loss: 0.004015797143802046\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02829834247297711\n",
      "Average test loss: 0.003959949034990536\n",
      "Epoch 17/300\n",
      "Average training loss: 0.027759314679437215\n",
      "Average test loss: 0.004250628310773108\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02739448225994905\n",
      "Average test loss: 0.003782119362304608\n",
      "Epoch 19/300\n",
      "Average training loss: 0.027134282113777268\n",
      "Average test loss: 0.004056335295033124\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02675683940284782\n",
      "Average test loss: 0.003722056493990951\n",
      "Epoch 21/300\n",
      "Average training loss: 0.026569338376323384\n",
      "Average test loss: 0.003614905628272229\n",
      "Epoch 22/300\n",
      "Average training loss: 0.026266146327058475\n",
      "Average test loss: 0.0035610622608413297\n",
      "Epoch 23/300\n",
      "Average training loss: 0.026052211024694974\n",
      "Average test loss: 0.003527114477422502\n",
      "Epoch 24/300\n",
      "Average training loss: 0.025852179560396406\n",
      "Average test loss: 0.003536893476007713\n",
      "Epoch 25/300\n",
      "Average training loss: 0.025605887255734868\n",
      "Average test loss: 0.0035467947044720252\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025439612713125018\n",
      "Average test loss: 0.0034327444821182222\n",
      "Epoch 27/300\n",
      "Average training loss: 0.025241167982419332\n",
      "Average test loss: 0.0034728793801946773\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025088889471358723\n",
      "Average test loss: 0.00352552322174112\n",
      "Epoch 29/300\n",
      "Average training loss: 0.024975855085584853\n",
      "Average test loss: 0.0033761348355975415\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02495019020802445\n",
      "Average test loss: 0.003475056768084566\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024665763455960485\n",
      "Average test loss: 0.0033272468135174776\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024645917369259727\n",
      "Average test loss: 0.0033368930456538996\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02444630272189776\n",
      "Average test loss: 0.0034907526216573187\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02433473552432325\n",
      "Average test loss: 0.003338064980589681\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024313870678345364\n",
      "Average test loss: 0.003484208155216442\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024216869741678238\n",
      "Average test loss: 0.0037055474559052125\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024044526770710947\n",
      "Average test loss: 0.0032290955423894857\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023966855585575104\n",
      "Average test loss: 0.003825295730804404\n",
      "Epoch 39/300\n",
      "Average training loss: 0.023920892655849457\n",
      "Average test loss: 0.0035008984967652293\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02382518922785918\n",
      "Average test loss: 0.0032944647965745793\n",
      "Epoch 41/300\n",
      "Average training loss: 0.023738991470800507\n",
      "Average test loss: 0.0032254511473907366\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023761913566125763\n",
      "Average test loss: 0.0032957885201192563\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02365087996588813\n",
      "Average test loss: 0.0032627577307737534\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0236013759755426\n",
      "Average test loss: 0.003272849102814992\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02368797905329201\n",
      "Average test loss: 0.003340361728022496\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023405363015002675\n",
      "Average test loss: 0.003220751261131631\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02336425266497665\n",
      "Average test loss: 0.0032219532966199847\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023300971181856263\n",
      "Average test loss: 0.003217021986221274\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02328394943310155\n",
      "Average test loss: 0.003189464094531205\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02326102078954379\n",
      "Average test loss: 0.0032232716019368834\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023170490733451312\n",
      "Average test loss: 0.0031584766109784444\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023163244176242086\n",
      "Average test loss: 0.0033762025357120568\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02311824718117714\n",
      "Average test loss: 0.0031507231847693524\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023073585541711912\n",
      "Average test loss: 0.0031996803008433844\n",
      "Epoch 55/300\n",
      "Average training loss: 0.023048653890689216\n",
      "Average test loss: 0.003278602032818728\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02300666103263696\n",
      "Average test loss: 0.003243581482105785\n",
      "Epoch 57/300\n",
      "Average training loss: 0.022977069700757663\n",
      "Average test loss: 0.003206079760359393\n",
      "Epoch 58/300\n",
      "Average training loss: 0.022908472546272807\n",
      "Average test loss: 0.0031418987752662763\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02288780186408096\n",
      "Average test loss: 0.0031178422218395606\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02283708589937952\n",
      "Average test loss: 0.0030751069051524003\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022810152783989908\n",
      "Average test loss: 0.003086981145458089\n",
      "Epoch 62/300\n",
      "Average training loss: 0.022769832273324332\n",
      "Average test loss: 0.003086262089717719\n",
      "Epoch 63/300\n",
      "Average training loss: 0.022730724980433784\n",
      "Average test loss: 0.003188441346296006\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02272911854253875\n",
      "Average test loss: 0.003079937390776144\n",
      "Epoch 65/300\n",
      "Average training loss: 0.022640481811430718\n",
      "Average test loss: 0.003139688081211514\n",
      "Epoch 66/300\n",
      "Average training loss: 0.022718346804380417\n",
      "Average test loss: 0.0030539671522047786\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02261877903342247\n",
      "Average test loss: 0.0030884213973250653\n",
      "Epoch 68/300\n",
      "Average training loss: 0.022605021715164184\n",
      "Average test loss: 0.0030830077696591615\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022586461615231302\n",
      "Average test loss: 0.0030942784282896253\n",
      "Epoch 70/300\n",
      "Average training loss: 0.022529701133569082\n",
      "Average test loss: 0.003085285412354602\n",
      "Epoch 71/300\n",
      "Average training loss: 0.022533007500900162\n",
      "Average test loss: 0.003055985514902406\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022505427141984304\n",
      "Average test loss: 0.0030181956593361165\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022464393632279502\n",
      "Average test loss: 0.0034689456396218804\n",
      "Epoch 74/300\n",
      "Average training loss: 0.022442598248521486\n",
      "Average test loss: 0.0031315761419634025\n",
      "Epoch 75/300\n",
      "Average training loss: 0.022389484034644233\n",
      "Average test loss: 0.0030528978024505905\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08992583287258943\n",
      "Average test loss: 0.004352062623947859\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03290722291337119\n",
      "Average test loss: 0.003967575485300687\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029351597471369638\n",
      "Average test loss: 0.00339895761364864\n",
      "Epoch 79/300\n",
      "Average training loss: 0.027529062372114922\n",
      "Average test loss: 0.003414309493369526\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02634429229961501\n",
      "Average test loss: 0.0032249687794182035\n",
      "Epoch 81/300\n",
      "Average training loss: 0.025472200623816915\n",
      "Average test loss: 0.0032159226131108073\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02478681038154496\n",
      "Average test loss: 0.0031610002020994823\n",
      "Epoch 83/300\n",
      "Average training loss: 0.024327864383657773\n",
      "Average test loss: 0.0031842439698262347\n",
      "Epoch 84/300\n",
      "Average training loss: 0.023850224915477964\n",
      "Average test loss: 0.003166230178541607\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02353595983982086\n",
      "Average test loss: 0.0031022505023413235\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02332013967964384\n",
      "Average test loss: 0.0031098491296999986\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02311455379757616\n",
      "Average test loss: 0.0031128747316284313\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022968145176768304\n",
      "Average test loss: 0.003107689859966437\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02283883947796292\n",
      "Average test loss: 0.003088480907181899\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022784980638159646\n",
      "Average test loss: 0.003099390442793568\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022688649483852918\n",
      "Average test loss: 0.003046562877173225\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022641380574968128\n",
      "Average test loss: 0.00306415145802829\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02262311432758967\n",
      "Average test loss: 0.0030439727188398442\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02254562388526069\n",
      "Average test loss: 0.0030737502903987962\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02252388051317798\n",
      "Average test loss: 0.003045840454598268\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02250307534635067\n",
      "Average test loss: 0.0030588922766554686\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022451568272378708\n",
      "Average test loss: 0.003099705115788513\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022418351007832422\n",
      "Average test loss: 0.0030180351972166034\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022387665193941857\n",
      "Average test loss: 0.0030834494698792694\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022375426616933612\n",
      "Average test loss: 0.003040847974932856\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0223388891302877\n",
      "Average test loss: 0.0031495340236773095\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022320648877157106\n",
      "Average test loss: 0.0030105928876954647\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02227197723918491\n",
      "Average test loss: 0.003103711735871103\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02225773288971848\n",
      "Average test loss: 0.003077170178708103\n",
      "Epoch 105/300\n",
      "Average training loss: 0.022200265599621665\n",
      "Average test loss: 0.003075271166861057\n",
      "Epoch 106/300\n",
      "Average training loss: 0.022183027466138205\n",
      "Average test loss: 0.0031212417535069915\n",
      "Epoch 107/300\n",
      "Average training loss: 0.022158169522881507\n",
      "Average test loss: 0.003029701121772329\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022155009917087024\n",
      "Average test loss: 0.003047426419539584\n",
      "Epoch 109/300\n",
      "Average training loss: 0.022147484572397336\n",
      "Average test loss: 0.003012580862889687\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02209687489271164\n",
      "Average test loss: 0.0030156675378481546\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022054488696985775\n",
      "Average test loss: 0.003016958749646114\n",
      "Epoch 112/300\n",
      "Average training loss: 0.022053815957572723\n",
      "Average test loss: 0.0030786284462859235\n",
      "Epoch 113/300\n",
      "Average training loss: 0.022025606110692025\n",
      "Average test loss: 0.0030428107049730087\n",
      "Epoch 114/300\n",
      "Average training loss: 0.022007180571556092\n",
      "Average test loss: 0.003157594221747584\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022016805335879325\n",
      "Average test loss: 0.003034652472784122\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021962652443183794\n",
      "Average test loss: 0.003012101426720619\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02194619420170784\n",
      "Average test loss: 0.003041354911815789\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021927707634038396\n",
      "Average test loss: 0.0030565422119365796\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021927849969930118\n",
      "Average test loss: 0.003080010630811254\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021926126716865432\n",
      "Average test loss: 0.0030159299197710225\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021892642026146252\n",
      "Average test loss: 0.003015207546349201\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021871036274565592\n",
      "Average test loss: 0.003117602244847351\n",
      "Epoch 123/300\n",
      "Average training loss: 0.021853603132896953\n",
      "Average test loss: 0.0031744218319654464\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02185407887564765\n",
      "Average test loss: 0.0029883501103354824\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02180534802046087\n",
      "Average test loss: 0.003002184315274159\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02180643334819211\n",
      "Average test loss: 0.003073007879571782\n",
      "Epoch 127/300\n",
      "Average training loss: 0.021786439173751407\n",
      "Average test loss: 0.0030550919661505357\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02176852397289541\n",
      "Average test loss: 0.002992017723412977\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02172089038623704\n",
      "Average test loss: 0.003005242516183191\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021786073353555466\n",
      "Average test loss: 0.0032336941531134976\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021727204410566223\n",
      "Average test loss: 0.0030566064425640634\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021703624291552436\n",
      "Average test loss: 0.0030035143091032904\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021720770430233745\n",
      "Average test loss: 0.0031136648311383196\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02167002550760905\n",
      "Average test loss: 0.003030387993488047\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0216719924973117\n",
      "Average test loss: 0.002978745074321826\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021665334069066577\n",
      "Average test loss: 0.0031585842288202708\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021632136043575074\n",
      "Average test loss: 0.0029889673812107907\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02165093235174815\n",
      "Average test loss: 0.0029938931167125702\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02161040988398923\n",
      "Average test loss: 0.0032739654330329764\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02159510909848743\n",
      "Average test loss: 0.014022989712655544\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021599828691946137\n",
      "Average test loss: 0.0029857089285635287\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02156407295167446\n",
      "Average test loss: 0.003186827280662126\n",
      "Epoch 143/300\n",
      "Average training loss: 0.021581649137867823\n",
      "Average test loss: 0.0029997427101350494\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02154513904452324\n",
      "Average test loss: 0.0029765336480405595\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021556719210412766\n",
      "Average test loss: 0.002967127292727431\n",
      "Epoch 146/300\n",
      "Average training loss: 0.021540892407298087\n",
      "Average test loss: 0.0029711271033932766\n",
      "Epoch 147/300\n",
      "Average training loss: 0.021509300008416176\n",
      "Average test loss: 0.0029704331755638124\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02154729324248102\n",
      "Average test loss: 0.0030468166321516036\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02149545678330792\n",
      "Average test loss: 0.0029943808015022015\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02148559305071831\n",
      "Average test loss: 0.0030187293396641813\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02146043013698525\n",
      "Average test loss: 0.0030093586666302546\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021478172461191814\n",
      "Average test loss: 0.0032880960441090995\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021470359900759326\n",
      "Average test loss: 0.00301325707986123\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021466142892009682\n",
      "Average test loss: 0.0030350424316194324\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0214541647715701\n",
      "Average test loss: 0.002981750894544853\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021400217758284674\n",
      "Average test loss: 0.0031026704094062247\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02141923698782921\n",
      "Average test loss: 0.0031137458361271356\n",
      "Epoch 158/300\n",
      "Average training loss: 0.021420011268721686\n",
      "Average test loss: 0.003001580297739969\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021431075347794426\n",
      "Average test loss: 0.002990212679012782\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02137436306476593\n",
      "Average test loss: 0.0029856515765811008\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02137529540144735\n",
      "Average test loss: 0.0030353291287190383\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021359327238467004\n",
      "Average test loss: 0.0030622151485747763\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021361633492840662\n",
      "Average test loss: 0.0029931728208644525\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021352409437298776\n",
      "Average test loss: 0.0030003313900281987\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02134465519587199\n",
      "Average test loss: 0.0037269717703262966\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021339326007498636\n",
      "Average test loss: 0.0030688195323778523\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02130721427500248\n",
      "Average test loss: 0.0030312828396757445\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021310490378075177\n",
      "Average test loss: 0.0030059706005785202\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02131677023569743\n",
      "Average test loss: 0.003003620400817858\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02130266692241033\n",
      "Average test loss: 0.0030374528867089085\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021262079058421982\n",
      "Average test loss: 0.002966243161302474\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0212870771371656\n",
      "Average test loss: 0.0030214312026898065\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02126312340795994\n",
      "Average test loss: 0.0031029631874213617\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021277704707450336\n",
      "Average test loss: 0.0031129046604037283\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02121897466149595\n",
      "Average test loss: 0.0032124473600544864\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021252284622854656\n",
      "Average test loss: 0.0029977860084424417\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02126384888920519\n",
      "Average test loss: 0.002981552153411839\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02120922872092989\n",
      "Average test loss: 0.0030393809595455727\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021184019497699207\n",
      "Average test loss: 0.003042760468191571\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02122873981297016\n",
      "Average test loss: 0.003214440995827317\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02121052105062538\n",
      "Average test loss: 0.003191199796895186\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021179928711718983\n",
      "Average test loss: 0.003008274125970072\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021208351246184774\n",
      "Average test loss: 0.0030469711925834417\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02115257447461287\n",
      "Average test loss: 0.003025996756636434\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02115910283724467\n",
      "Average test loss: 0.003021333780967527\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021180344993869463\n",
      "Average test loss: 0.0030403508837852215\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02117556385695934\n",
      "Average test loss: 0.0030397072511000765\n",
      "Epoch 188/300\n",
      "Average training loss: 0.021140300856696236\n",
      "Average test loss: 0.0034604515760309168\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021132489687866636\n",
      "Average test loss: 0.0030159109376577866\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021126806929707528\n",
      "Average test loss: 0.0029603442448294824\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02111831671661801\n",
      "Average test loss: 0.002972781874653366\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021091145977377893\n",
      "Average test loss: 0.0029833824783563616\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021138584017753603\n",
      "Average test loss: 0.00301401536539197\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02110875311990579\n",
      "Average test loss: 0.0031509837088071636\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021073787750469315\n",
      "Average test loss: 0.003059659365771545\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021076850548386573\n",
      "Average test loss: 0.003089360292380055\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021079295641846126\n",
      "Average test loss: 0.0030397032772501308\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021090978130698206\n",
      "Average test loss: 0.0030012186985048984\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021089280328816837\n",
      "Average test loss: 0.002986974499084883\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021041117274098926\n",
      "Average test loss: 0.0029854878930168018\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02102818426158693\n",
      "Average test loss: 0.002975510986935761\n",
      "Epoch 202/300\n",
      "Average training loss: 0.021032217633393076\n",
      "Average test loss: 0.0029715241928481395\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02103538176582919\n",
      "Average test loss: 0.00304341447705196\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02102905775109927\n",
      "Average test loss: 0.002982029696719514\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021027457748850186\n",
      "Average test loss: 0.0030592746084762944\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021024465006258753\n",
      "Average test loss: 0.003150343816106518\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02101005572742886\n",
      "Average test loss: 0.0029932755494697224\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02102833785282241\n",
      "Average test loss: 0.003089121749210689\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02098226567606131\n",
      "Average test loss: 0.0030370548574460876\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020974402194221815\n",
      "Average test loss: 0.0030034845914277764\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021001974915464718\n",
      "Average test loss: 0.002984665168449283\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02099377050995827\n",
      "Average test loss: 0.003060852144534389\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020974007674389415\n",
      "Average test loss: 0.002997167277460297\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020942515414622095\n",
      "Average test loss: 0.0029912276924070383\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02094016790141662\n",
      "Average test loss: 0.0030997243834038576\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020935034037464193\n",
      "Average test loss: 0.0030614851334442696\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02095723215904501\n",
      "Average test loss: 0.003129276172775361\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020955848529934883\n",
      "Average test loss: 0.003017850566862358\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02092251757449574\n",
      "Average test loss: 0.0029881984612180126\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020921390742063523\n",
      "Average test loss: 0.0030338233270578913\n",
      "Epoch 221/300\n",
      "Average training loss: 0.020932645296057064\n",
      "Average test loss: 0.002972429357883003\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02089019670089086\n",
      "Average test loss: 0.003033361088277565\n",
      "Epoch 223/300\n",
      "Average training loss: 0.020904137901133962\n",
      "Average test loss: 0.003017843123525381\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02090694353481134\n",
      "Average test loss: 0.0030511264691336286\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020890192225575446\n",
      "Average test loss: 0.002972273366111848\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020898003477189276\n",
      "Average test loss: 0.003016499675810337\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020886339333322314\n",
      "Average test loss: 0.0030757533522943656\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02088527591029803\n",
      "Average test loss: 0.0029718181194944514\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020883777047197023\n",
      "Average test loss: 0.0030253939893510606\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020884211127956707\n",
      "Average test loss: 0.003031023415322933\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020845436152484683\n",
      "Average test loss: 0.0030650641787797212\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020871535998251704\n",
      "Average test loss: 0.002987960395299726\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020850773163967662\n",
      "Average test loss: 0.0030608710468643242\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02088163730667697\n",
      "Average test loss: 0.003010868174334367\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020842512445317374\n",
      "Average test loss: 0.0029763270244002344\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020823291540145873\n",
      "Average test loss: 0.003135289311202036\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02085241406990422\n",
      "Average test loss: 0.0029926404104464585\n",
      "Epoch 238/300\n",
      "Average training loss: 0.020817856134639847\n",
      "Average test loss: 0.0031793070185101697\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020823579827116596\n",
      "Average test loss: 0.003043931828190883\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020803056753344006\n",
      "Average test loss: 0.0030767347233162986\n",
      "Epoch 241/300\n",
      "Average training loss: 0.020799103167321947\n",
      "Average test loss: 0.003089761957526207\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020818665403458808\n",
      "Average test loss: 0.003102070060869058\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02080096880098184\n",
      "Average test loss: 0.0029893949557509686\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020809448411067327\n",
      "Average test loss: 0.0029971361812204124\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02077353300816483\n",
      "Average test loss: 0.0030497609417264662\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020811465644174153\n",
      "Average test loss: 0.002979441766316692\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020755226608779695\n",
      "Average test loss: 0.0030185879793845944\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02077540648397472\n",
      "Average test loss: 0.003118303821939561\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020753549680941636\n",
      "Average test loss: 0.003046056026799811\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020755600588189232\n",
      "Average test loss: 0.0030291052036401298\n",
      "Epoch 251/300\n",
      "Average training loss: 0.020743746780686908\n",
      "Average test loss: 0.0030699913437581723\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02077124808397558\n",
      "Average test loss: 0.0029926672929690944\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020726956056223975\n",
      "Average test loss: 0.003037756041727132\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02074768841266632\n",
      "Average test loss: 0.0029799822063909637\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020749337509274483\n",
      "Average test loss: 0.0030260847633083662\n",
      "Epoch 256/300\n",
      "Average training loss: 0.020722644097275204\n",
      "Average test loss: 0.002979921827920609\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020734196020497215\n",
      "Average test loss: 0.0030029790151036447\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020713069788283772\n",
      "Average test loss: 0.003034492100485497\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020748095620837476\n",
      "Average test loss: 0.003059084781135122\n",
      "Epoch 260/300\n",
      "Average training loss: 0.020694741831885445\n",
      "Average test loss: 0.003002954353267948\n",
      "Epoch 261/300\n",
      "Average training loss: 0.020727538354694843\n",
      "Average test loss: 0.003333168979527222\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02068179317149851\n",
      "Average test loss: 0.003149606650902165\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020704557579424646\n",
      "Average test loss: 0.0030866189152002335\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020691748647226227\n",
      "Average test loss: 0.003021985719187392\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020683994981977673\n",
      "Average test loss: 0.0032588132824748752\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020663178260127705\n",
      "Average test loss: 0.003075495140420066\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020652731120586396\n",
      "Average test loss: 0.0030022878241207866\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02071356958647569\n",
      "Average test loss: 0.0030167121289090978\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020653002219895522\n",
      "Average test loss: 0.002990855377477904\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020677763811416096\n",
      "Average test loss: 0.003033634505338139\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020662999314566454\n",
      "Average test loss: 0.0032026741821318864\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020633682390054066\n",
      "Average test loss: 0.0030162158579462106\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020653289877706104\n",
      "Average test loss: 0.0030946023592518437\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0206631179501613\n",
      "Average test loss: 0.0029620932256802916\n",
      "Epoch 275/300\n",
      "Average training loss: 0.020638578029142485\n",
      "Average test loss: 0.0029729078315819304\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020653906325499218\n",
      "Average test loss: 0.003036924858060148\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020629761897855335\n",
      "Average test loss: 0.0030736773723943366\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020610552557640605\n",
      "Average test loss: 0.0030367062631994485\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020619591338766945\n",
      "Average test loss: 0.0030483877238714034\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020627991638249823\n",
      "Average test loss: 0.0032416431496126784\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020617652479145262\n",
      "Average test loss: 0.0031666026537617046\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02062017250392172\n",
      "Average test loss: 0.0031219363833467164\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02063198814789454\n",
      "Average test loss: 0.0030855594573335515\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020595165804028513\n",
      "Average test loss: 0.003100176241248846\n",
      "Epoch 285/300\n",
      "Average training loss: 0.020607598511709107\n",
      "Average test loss: 0.0030186302761236825\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020614643088645405\n",
      "Average test loss: 0.002992951778901948\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02056551997529136\n",
      "Average test loss: 0.0030496771722618075\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020579623435934384\n",
      "Average test loss: 0.003411459550054537\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020585115750630698\n",
      "Average test loss: 0.0030739320224771895\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020579722228977415\n",
      "Average test loss: 0.003124650835990906\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020566812245382204\n",
      "Average test loss: 0.0030100826339589225\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020591554787423874\n",
      "Average test loss: 0.003037131584352917\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02057411948674255\n",
      "Average test loss: 0.003201474732408921\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02056188810037242\n",
      "Average test loss: 0.003074388757451541\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020536867391732005\n",
      "Average test loss: 0.003062990294355485\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02055906299004952\n",
      "Average test loss: 0.0030802238755342033\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020535563722252844\n",
      "Average test loss: 0.0030720852661050026\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020549682199954986\n",
      "Average test loss: 0.0030531977280560466\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020547909973396197\n",
      "Average test loss: 0.0029739190493192937\n",
      "Epoch 300/300\n",
      "Average training loss: 0.020549775204724736\n",
      "Average test loss: 0.0030737792872306373\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12144728123148282\n",
      "Average test loss: 0.005468226471708881\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04237611906064881\n",
      "Average test loss: 0.004894782807884945\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03696406817436218\n",
      "Average test loss: 0.004216157293568055\n",
      "Epoch 4/300\n",
      "Average training loss: 0.033818250053458745\n",
      "Average test loss: 0.00411500279729565\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03163972861236996\n",
      "Average test loss: 0.004859669802503453\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02991174734963311\n",
      "Average test loss: 0.0035028932152522934\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02834496783051226\n",
      "Average test loss: 0.003530041284031338\n",
      "Epoch 8/300\n",
      "Average training loss: 0.027291628292865223\n",
      "Average test loss: 0.003540804631801115\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02627870058351093\n",
      "Average test loss: 0.003339299152501755\n",
      "Epoch 10/300\n",
      "Average training loss: 0.025533990701039632\n",
      "Average test loss: 0.00349356502149668\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02472832192811701\n",
      "Average test loss: 0.003796018034219742\n",
      "Epoch 12/300\n",
      "Average training loss: 0.024117509024010764\n",
      "Average test loss: 0.003411792984025346\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02351298492981328\n",
      "Average test loss: 0.003223325731025802\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02313096813691987\n",
      "Average test loss: 0.002743418383101622\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022590006627970272\n",
      "Average test loss: 0.0030410276715540224\n",
      "Epoch 16/300\n",
      "Average training loss: 0.022252255785796378\n",
      "Average test loss: 0.0032751756740940943\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02180991959075133\n",
      "Average test loss: 0.0028116389345377683\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0215178677721156\n",
      "Average test loss: 0.002528346771374345\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02119981027642886\n",
      "Average test loss: 0.0025073031971438063\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020974776693516307\n",
      "Average test loss: 0.002490656321040458\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020754553018344772\n",
      "Average test loss: 0.002472959699937039\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020578035922514067\n",
      "Average test loss: 0.0024859839338395332\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020366699708832633\n",
      "Average test loss: 0.002370697792412506\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020190581411951118\n",
      "Average test loss: 0.0023400024125973386\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020042615819308492\n",
      "Average test loss: 0.0024137708911051354\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01993702969782882\n",
      "Average test loss: 0.0023227838499264583\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019822797872953945\n",
      "Average test loss: 0.002284787345263693\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01964099619951513\n",
      "Average test loss: 0.0022523685422622497\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019589499664803348\n",
      "Average test loss: 0.0024107804726809265\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0194688856502374\n",
      "Average test loss: 0.0023116013945804702\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01933688518570529\n",
      "Average test loss: 0.002281759373222788\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019296736490395334\n",
      "Average test loss: 0.0022049601334147156\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019171928523315323\n",
      "Average test loss: 0.002243482075838579\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019103056139416166\n",
      "Average test loss: 0.0022383225665738187\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01907960972189903\n",
      "Average test loss: 0.0021677490832904974\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01895314916637209\n",
      "Average test loss: 0.0022081239582556816\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018879415356450612\n",
      "Average test loss: 0.002174053817573521\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018826500251889228\n",
      "Average test loss: 0.0022023841377554667\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018746382157835695\n",
      "Average test loss: 0.002176304735553761\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01868747161825498\n",
      "Average test loss: 0.0021546579121301574\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018697831938664117\n",
      "Average test loss: 0.0021464193204624787\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018608600580030017\n",
      "Average test loss: 0.002353503896544377\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018586904701259403\n",
      "Average test loss: 0.002151626284648147\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018503144821359053\n",
      "Average test loss: 0.0021553231908215415\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01845086041175657\n",
      "Average test loss: 0.0022813218839259613\n",
      "Epoch 46/300\n",
      "Average training loss: 0.018434714789191883\n",
      "Average test loss: 0.0021438114020145603\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01838019506136576\n",
      "Average test loss: 0.0021297526380254163\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01831150347987811\n",
      "Average test loss: 0.0021218434385955336\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01836385434286462\n",
      "Average test loss: 0.002159725433318979\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018343104999926356\n",
      "Average test loss: 0.002303557199219035\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018215236354205345\n",
      "Average test loss: 0.0021022142411934006\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018243955381214618\n",
      "Average test loss: 0.002132228463681208\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018149810650282434\n",
      "Average test loss: 0.0020842001585082877\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018120727070503766\n",
      "Average test loss: 0.0021756358824463356\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01811993756890297\n",
      "Average test loss: 0.0021168866817735965\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018042232417398028\n",
      "Average test loss: 0.0022360891009577445\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018099215598569977\n",
      "Average test loss: 0.0023027159804478287\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01798500319487519\n",
      "Average test loss: 0.0020852009321873388\n",
      "Epoch 59/300\n",
      "Average training loss: 0.017958464870850246\n",
      "Average test loss: 0.002254633158031437\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017949334984024366\n",
      "Average test loss: 0.0020863764008714094\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01793353190190262\n",
      "Average test loss: 0.0022698241888235013\n",
      "Epoch 62/300\n",
      "Average training loss: 0.017881284780800342\n",
      "Average test loss: 0.0021043967381119727\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017854827617605526\n",
      "Average test loss: 0.002269404004120992\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017863815855648783\n",
      "Average test loss: 0.002155714056144158\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017795380097296504\n",
      "Average test loss: 0.00205398014249901\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017786135912769372\n",
      "Average test loss: 0.002048509131703112\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017769953434666\n",
      "Average test loss: 0.002061917255529099\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017760908423198593\n",
      "Average test loss: 0.0020688601260383925\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017717411579357252\n",
      "Average test loss: 0.002360201742189626\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017689815590778986\n",
      "Average test loss: 0.002088458749983046\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01770517162150807\n",
      "Average test loss: 0.0020767853926453327\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01765579311715232\n",
      "Average test loss: 0.002158101414433784\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017621429584092563\n",
      "Average test loss: 0.0020797429910550517\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01761236443453365\n",
      "Average test loss: 0.0020632147731052507\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017624439833892717\n",
      "Average test loss: 0.002175501652682821\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017550633110933835\n",
      "Average test loss: 0.00206054655669464\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017552490981088745\n",
      "Average test loss: 0.0020966988783329725\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017564043725530307\n",
      "Average test loss: 0.0021082857412596545\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017513236201471754\n",
      "Average test loss: 0.002203355818366011\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017523938748571607\n",
      "Average test loss: 0.002261095809336338\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01747977458106147\n",
      "Average test loss: 0.002154366435897019\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017471015808482964\n",
      "Average test loss: 0.0021133936886779136\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01746640526254972\n",
      "Average test loss: 0.0021562511170696882\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017493146751489903\n",
      "Average test loss: 0.0020755273010581734\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017425094138416978\n",
      "Average test loss: 0.002047059353854921\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017392833626932568\n",
      "Average test loss: 0.002087198129337695\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017391910230947865\n",
      "Average test loss: 0.00203877564954261\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017392834732929865\n",
      "Average test loss: 0.0020468101375218893\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01736775303714805\n",
      "Average test loss: 0.0020505260698911215\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017383536575569045\n",
      "Average test loss: 0.0020707233847222393\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017326510517133607\n",
      "Average test loss: 0.002194205973090397\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017329604556163152\n",
      "Average test loss: 0.0020328177424768605\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017272995590335793\n",
      "Average test loss: 0.002034775745278845\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017301120221614837\n",
      "Average test loss: 0.0020262255066384872\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017290144367350473\n",
      "Average test loss: 0.002440885689109564\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017268421123425165\n",
      "Average test loss: 0.00202904524281621\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017261991515755654\n",
      "Average test loss: 0.0020819401995589336\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01723947022193008\n",
      "Average test loss: 0.002024025047197938\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01720833336810271\n",
      "Average test loss: 0.0019991169369055167\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017222717018591033\n",
      "Average test loss: 0.002029398755480846\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019116635614799128\n",
      "Average test loss: 0.002108668461545474\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01725098908940951\n",
      "Average test loss: 0.0020337211855997643\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017125926628708838\n",
      "Average test loss: 0.0020459332523039644\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017122690106431644\n",
      "Average test loss: 0.0020204283041465616\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017152320307162072\n",
      "Average test loss: 0.0021170544299400516\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01715179501556688\n",
      "Average test loss: 0.0021161919399682017\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017143817401594586\n",
      "Average test loss: 0.0020236876248899433\n",
      "Epoch 108/300\n",
      "Average training loss: 0.017137618059913318\n",
      "Average test loss: 0.0020212617135710186\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017151041752762265\n",
      "Average test loss: 0.0020256517047269476\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017155360930495793\n",
      "Average test loss: 0.002074548287524117\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017099647399452\n",
      "Average test loss: 0.002206092681321833\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01710298117250204\n",
      "Average test loss: 0.002050046301757296\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017098888882332377\n",
      "Average test loss: 0.0020835364467153947\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017073070221477086\n",
      "Average test loss: 0.002177544784835643\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017063814906610384\n",
      "Average test loss: 0.0028655986314018566\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017060268490678734\n",
      "Average test loss: 0.002020886152982712\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017046365881959596\n",
      "Average test loss: 0.0020145559972152114\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017036915393339264\n",
      "Average test loss: 0.0021282940213051106\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017060365373889604\n",
      "Average test loss: 0.002122077040995161\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017016716570489938\n",
      "Average test loss: 0.0021874922313210037\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01700725775129265\n",
      "Average test loss: 0.002028356317948136\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01699547817889187\n",
      "Average test loss: 0.002413150717711283\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017014252422584426\n",
      "Average test loss: 0.0020250076243860855\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0169535721167922\n",
      "Average test loss: 0.0020186714929425055\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016962828962339294\n",
      "Average test loss: 0.0023036701116814383\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01698026873668035\n",
      "Average test loss: 0.0020220109640310207\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016940771285030576\n",
      "Average test loss: 0.0020318492549575035\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01696290595249997\n",
      "Average test loss: 0.0020768246926988164\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016974667444825172\n",
      "Average test loss: 0.002245654406026006\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016912000444200305\n",
      "Average test loss: 0.0021015310366120604\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016896894070837234\n",
      "Average test loss: 0.002047145525097019\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01691660691301028\n",
      "Average test loss: 0.0022763806082722214\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016905010024706523\n",
      "Average test loss: 0.002045233641233709\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016900057142807377\n",
      "Average test loss: 0.0019902470449192655\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016877218400438626\n",
      "Average test loss: 0.002053440587077704\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016878693439894253\n",
      "Average test loss: 0.002145611867929498\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016886176630854607\n",
      "Average test loss: 0.0019955980349332094\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016875869650807646\n",
      "Average test loss: 0.0020088886544108392\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01684592115630706\n",
      "Average test loss: 0.002017182082662152\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016864512331783772\n",
      "Average test loss: 0.002061092532343335\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016905090439650747\n",
      "Average test loss: 0.0020148585860927898\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01681332735882865\n",
      "Average test loss: 0.002026934989934994\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016803706184029578\n",
      "Average test loss: 0.002082508392425047\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01683105057477951\n",
      "Average test loss: 0.00199779756522427\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016819677917493715\n",
      "Average test loss: 0.002155259957537055\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016776424155467085\n",
      "Average test loss: 0.002041672516407238\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01681682547595766\n",
      "Average test loss: 0.002026064238200585\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016796111872626675\n",
      "Average test loss: 0.0020315977475709388\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016790229584607814\n",
      "Average test loss: 0.0020672982834900417\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016761034241980978\n",
      "Average test loss: 0.0020452452724178632\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01675615695781178\n",
      "Average test loss: 0.002059777810445262\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016755430814292697\n",
      "Average test loss: 0.0020042933725441495\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01676102033836974\n",
      "Average test loss: 0.0021123496385084257\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016755323713024458\n",
      "Average test loss: 0.0020517342990885178\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016730104214615293\n",
      "Average test loss: 0.0020541967209428548\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016727185999353727\n",
      "Average test loss: 0.002030247110356059\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016714292157027456\n",
      "Average test loss: 0.0020061693127370545\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01669850222269694\n",
      "Average test loss: 0.0021061157658696173\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016717305895355014\n",
      "Average test loss: 0.002053738343529403\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01671245215833187\n",
      "Average test loss: 0.002002423089825445\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016682834674914677\n",
      "Average test loss: 0.00200668141618371\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016681026328768996\n",
      "Average test loss: 0.0020435871510869927\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016669848691258166\n",
      "Average test loss: 0.0020950718122637936\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016671561631891464\n",
      "Average test loss: 0.0020039299755460686\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016716083467834524\n",
      "Average test loss: 0.002052621320510904\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016657171307338608\n",
      "Average test loss: 0.0020054076680292685\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016638543326821593\n",
      "Average test loss: 0.0020292564043775203\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01666333641939693\n",
      "Average test loss: 0.0020732308303316434\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01662543181164397\n",
      "Average test loss: 0.00219376667154332\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01662675142288208\n",
      "Average test loss: 0.002178805036677255\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016639969494607713\n",
      "Average test loss: 0.002015419038530025\n",
      "Epoch 172/300\n",
      "Average training loss: 0.016605374207099278\n",
      "Average test loss: 0.002107729578804639\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01665307597319285\n",
      "Average test loss: 0.0020288670789450408\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016626813242004976\n",
      "Average test loss: 0.0020683467268115944\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01985019785993629\n",
      "Average test loss: 0.0020883644717848964\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01713970643447505\n",
      "Average test loss: 0.0019947614648068946\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01666352288093832\n",
      "Average test loss: 0.0019983861988617313\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01654202394021882\n",
      "Average test loss: 0.0020269777725140255\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01652097343736225\n",
      "Average test loss: 0.0020293899652444655\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016521574288606643\n",
      "Average test loss: 0.002049688142310414\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01652016800807582\n",
      "Average test loss: 0.0020157591408739486\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016526192439926996\n",
      "Average test loss: 0.002034881007133259\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01652355348567168\n",
      "Average test loss: 0.0020116532072424888\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01655716839267148\n",
      "Average test loss: 0.002051422289055255\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016562942312823403\n",
      "Average test loss: 0.002063249897522231\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01656410535838869\n",
      "Average test loss: 0.0020347667512380413\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016542449206113814\n",
      "Average test loss: 0.002018031117092404\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01654152252773444\n",
      "Average test loss: 0.002058483509450323\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016541856918897892\n",
      "Average test loss: 0.0020293934096892673\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01652516621682379\n",
      "Average test loss: 0.0020475566911821565\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01653421635263496\n",
      "Average test loss: 0.002116450982582238\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016528361027439434\n",
      "Average test loss: 0.0020903505632239913\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016513889286253188\n",
      "Average test loss: 0.002005117598403659\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01653418373564879\n",
      "Average test loss: 0.0019977498474634354\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01652963390946388\n",
      "Average test loss: 0.0021134675808457863\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016508334186342028\n",
      "Average test loss: 0.0021824135766882035\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01648095387717088\n",
      "Average test loss: 0.0020378214221240744\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016477939548591773\n",
      "Average test loss: 0.002062821353889174\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016478804246419006\n",
      "Average test loss: 0.002690329080240594\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01649134435504675\n",
      "Average test loss: 0.0020705025535490778\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016471095993287034\n",
      "Average test loss: 0.0020897372559540803\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01647761807839076\n",
      "Average test loss: 0.0020595404086634515\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01645008725590176\n",
      "Average test loss: 0.002037262393368615\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016463320621185832\n",
      "Average test loss: 0.0020373540927345556\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016483887270092965\n",
      "Average test loss: 0.0020662585044693618\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016450629147390525\n",
      "Average test loss: 0.002066425595846441\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016431617952883244\n",
      "Average test loss: 0.002076306949990491\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016434953815407223\n",
      "Average test loss: 0.0020179987167939545\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016415186350544293\n",
      "Average test loss: 0.0019915933648331297\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016449913124243418\n",
      "Average test loss: 0.0020367543674591516\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016428151450223392\n",
      "Average test loss: 0.002088365166551537\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01640475183725357\n",
      "Average test loss: 0.0019896632058338986\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016421751213570437\n",
      "Average test loss: 0.00202570052134494\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016427153675920434\n",
      "Average test loss: 0.0021229585483670235\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016380794752803112\n",
      "Average test loss: 0.002028288620834549\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016432452694409422\n",
      "Average test loss: 0.002023143591048817\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016403848614129756\n",
      "Average test loss: 0.0020472444807075794\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016387972267137632\n",
      "Average test loss: 0.0020259864683159524\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016391677033984\n",
      "Average test loss: 0.002098932994322644\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01640006269680129\n",
      "Average test loss: 0.0020317290915797155\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016377177341944642\n",
      "Average test loss: 0.002065443128761318\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01637730917500125\n",
      "Average test loss: 0.002084739891915686\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01637136990825335\n",
      "Average test loss: 0.0020460166306131415\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01637489367193646\n",
      "Average test loss: 0.00211190023045573\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01635629769331879\n",
      "Average test loss: 0.00203291604667902\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016366344870792495\n",
      "Average test loss: 0.0020552244912832975\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016349271827273897\n",
      "Average test loss: 0.0020171556022639077\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01634448484248585\n",
      "Average test loss: 0.0020710550585968626\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016358288247552182\n",
      "Average test loss: 0.002101561424839828\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016342345252633096\n",
      "Average test loss: 0.002043456113172902\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016317292782995436\n",
      "Average test loss: 0.001984495371683604\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0163432187454568\n",
      "Average test loss: 0.002021029446894924\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016316209616760412\n",
      "Average test loss: 0.002113337553416689\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016312172341677877\n",
      "Average test loss: 0.0020733625013381243\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01631250992996825\n",
      "Average test loss: 0.0020607936173263524\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016304676287704045\n",
      "Average test loss: 0.0021064207773241733\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016302531441880597\n",
      "Average test loss: 0.0020848683070184455\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016298714604642658\n",
      "Average test loss: 0.002106442359690037\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016304392086135017\n",
      "Average test loss: 0.002043562510775195\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016270352725353505\n",
      "Average test loss: 0.0023053049924266007\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016291645954052608\n",
      "Average test loss: 0.0019949731806086167\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01627520059214698\n",
      "Average test loss: 0.0020647627296340133\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01629161376092169\n",
      "Average test loss: 0.002002409358198444\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016268741566273903\n",
      "Average test loss: 0.0020344951564653054\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01628530560599433\n",
      "Average test loss: 0.002045613401879867\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016286345299747255\n",
      "Average test loss: 0.002016387489520841\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016253644368714758\n",
      "Average test loss: 0.00219683538015104\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01626141870684094\n",
      "Average test loss: 0.002174136963672936\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016239046453601785\n",
      "Average test loss: 0.002069584557890064\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016280526460044915\n",
      "Average test loss: 0.0021474508999122515\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016243600212865406\n",
      "Average test loss: 0.0020769499459614357\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016235776931875282\n",
      "Average test loss: 0.0020228752048893107\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01622265533440643\n",
      "Average test loss: 0.002016463029405309\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016237581045263343\n",
      "Average test loss: 0.002050189393055108\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01621125213801861\n",
      "Average test loss: 0.002022176842722628\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01620252455936538\n",
      "Average test loss: 0.002096626508567068\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016231009205182393\n",
      "Average test loss: 0.002366970216855407\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016222582567897107\n",
      "Average test loss: 0.0020888123055920003\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016208601132035254\n",
      "Average test loss: 0.002010107182690667\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01620400641527441\n",
      "Average test loss: 0.0019938250771827167\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016231995417012108\n",
      "Average test loss: 0.002003851883113384\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01619256457272503\n",
      "Average test loss: 0.002035040676076379\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016207177567813133\n",
      "Average test loss: 0.002022637604528831\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016179686958591143\n",
      "Average test loss: 0.0020260761041815084\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01619105234824949\n",
      "Average test loss: 0.002099960343187882\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016183995980355474\n",
      "Average test loss: 0.0020049820308470066\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016187984424332776\n",
      "Average test loss: 0.0021301889644107885\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01617282452268733\n",
      "Average test loss: 0.002072401748970151\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016191008486681514\n",
      "Average test loss: 0.002011038403544161\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01615914095689853\n",
      "Average test loss: 0.00210370435907195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01616432651049561\n",
      "Average test loss: 0.0020142896467198928\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016143996260232395\n",
      "Average test loss: 0.0020544584781552354\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016158411064081724\n",
      "Average test loss: 0.0021171436943113802\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01615076349096166\n",
      "Average test loss: 0.0020358586284435457\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016146702431970174\n",
      "Average test loss: 0.00205406939652231\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01614958414600955\n",
      "Average test loss: 0.0021332585092427004\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016167728191448582\n",
      "Average test loss: 0.0020958229723489945\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01613653649141391\n",
      "Average test loss: 0.0020411961279395555\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016139069957037767\n",
      "Average test loss: 0.002037390527418918\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016115887088908088\n",
      "Average test loss: 0.0020275725699547263\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016121500510308476\n",
      "Average test loss: 0.0020514355374293197\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01613009908878141\n",
      "Average test loss: 0.0020408922750502823\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016121025342080327\n",
      "Average test loss: 0.0021555793589601913\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016122797033853\n",
      "Average test loss: 0.0020217676845689612\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016103641575409305\n",
      "Average test loss: 0.0020422352191267743\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016119130401147735\n",
      "Average test loss: 0.002031803800207045\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016101958956983353\n",
      "Average test loss: 0.002032704172862901\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016097760308119987\n",
      "Average test loss: 0.002016410611776842\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016098888231648338\n",
      "Average test loss: 0.0020517318847899637\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016122541405260562\n",
      "Average test loss: 0.002032380465004179\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016071396821075015\n",
      "Average test loss: 0.002018467631397976\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016073771120773423\n",
      "Average test loss: 0.002030183108420008\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016078478366964393\n",
      "Average test loss: 0.002020730997332268\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01608074822690752\n",
      "Average test loss: 0.0020108673411111037\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016086179073486064\n",
      "Average test loss: 0.0020646231055466667\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016068680269850626\n",
      "Average test loss: 0.0020745960240148836\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01607209146519502\n",
      "Average test loss: 0.0033562158317201668\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016073471047812038\n",
      "Average test loss: 0.0020242931549954747\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016065131064918307\n",
      "Average test loss: 0.0050152410500579414\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01607273947364754\n",
      "Average test loss: 0.0020288464394915434\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11183004390531116\n",
      "Average test loss: 0.0046196198003987474\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03683724709517426\n",
      "Average test loss: 0.004020539528379837\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03156703066825867\n",
      "Average test loss: 0.0038273397837248115\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02875682981312275\n",
      "Average test loss: 0.003202159107766218\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02687115380830235\n",
      "Average test loss: 0.002978415898150868\n",
      "Epoch 6/300\n",
      "Average training loss: 0.025188396727045377\n",
      "Average test loss: 0.0030854725256148313\n",
      "Epoch 7/300\n",
      "Average training loss: 0.024292734901110332\n",
      "Average test loss: 0.0027283172162456647\n",
      "Epoch 8/300\n",
      "Average training loss: 0.023027143821120262\n",
      "Average test loss: 0.002661208058603936\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022283524642388027\n",
      "Average test loss: 0.0028892158199515607\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021476302802562713\n",
      "Average test loss: 0.0023670940868970422\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020875598975353772\n",
      "Average test loss: 0.0024273149776789875\n",
      "Epoch 12/300\n",
      "Average training loss: 0.020308915432956483\n",
      "Average test loss: 0.002360446520563629\n",
      "Epoch 13/300\n",
      "Average training loss: 0.019792990957697234\n",
      "Average test loss: 0.0020963984071794483\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019350933346483443\n",
      "Average test loss: 0.0022961481517801683\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01898259108927515\n",
      "Average test loss: 0.0020596151320884624\n",
      "Epoch 16/300\n",
      "Average training loss: 0.018649034577939247\n",
      "Average test loss: 0.002233701003094514\n",
      "Epoch 17/300\n",
      "Average training loss: 0.018269386053085326\n",
      "Average test loss: 0.00202762823469109\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018032067394918867\n",
      "Average test loss: 0.0022912239308158556\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017801107247670492\n",
      "Average test loss: 0.0018482514196592901\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01757443487809764\n",
      "Average test loss: 0.002085336251391305\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01741508444978131\n",
      "Average test loss: 0.001915267189964652\n",
      "Epoch 22/300\n",
      "Average training loss: 0.017210344031453134\n",
      "Average test loss: 0.0017771779671311378\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017028328628175788\n",
      "Average test loss: 0.0018498094394389126\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01695381878150834\n",
      "Average test loss: 0.0017763579634742603\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016807130297025043\n",
      "Average test loss: 0.0017351597180176113\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016684886435667675\n",
      "Average test loss: 0.0017222564367370473\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016569266221589513\n",
      "Average test loss: 0.001707988182393213\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016548284098505974\n",
      "Average test loss: 0.0017083355153186455\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016424210376209683\n",
      "Average test loss: 0.001655022494908836\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016301442189349067\n",
      "Average test loss: 0.0017219374653779798\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01623894401722484\n",
      "Average test loss: 0.001604824291335212\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016135600169499716\n",
      "Average test loss: 0.0016009977515786887\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01609716543306907\n",
      "Average test loss: 0.0016394432852458623\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016054285972482627\n",
      "Average test loss: 0.0016600524803313116\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015938965515130098\n",
      "Average test loss: 0.00165997514128685\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01588632249335448\n",
      "Average test loss: 0.0015712532203437553\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015831870811680953\n",
      "Average test loss: 0.0016473653604172998\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015762807695402038\n",
      "Average test loss: 0.001604566752910614\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015725400247507626\n",
      "Average test loss: 0.001584945662568013\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015689579389161535\n",
      "Average test loss: 0.0016086867006702556\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015593243687517112\n",
      "Average test loss: 0.0015564513738370604\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015598273151450686\n",
      "Average test loss: 0.0015436070419641004\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01554051573243406\n",
      "Average test loss: 0.0016441250693880849\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015506546306941245\n",
      "Average test loss: 0.0015288193132728337\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015512195059822665\n",
      "Average test loss: 0.0015951141545342074\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015479645775424109\n",
      "Average test loss: 0.001593439961783588\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015348870779077211\n",
      "Average test loss: 0.0015062862631554405\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015360651599864165\n",
      "Average test loss: 0.0015068598423774043\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015299112377067407\n",
      "Average test loss: 0.0015235725975491935\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015274398646420903\n",
      "Average test loss: 0.001550453406551646\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015272674337029456\n",
      "Average test loss: 0.0015296814788339868\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015192371870908472\n",
      "Average test loss: 0.0015065275101612012\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015208571708864636\n",
      "Average test loss: 0.0015095523892798357\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015162337091233996\n",
      "Average test loss: 0.0015742006085606086\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015138067943354448\n",
      "Average test loss: 0.0016273820537866818\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015096134821573893\n",
      "Average test loss: 0.0014948261568529738\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015073998174733585\n",
      "Average test loss: 0.0015248062514389555\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015046938598155975\n",
      "Average test loss: 0.0015275250050342745\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015044203288853168\n",
      "Average test loss: 0.0015270623758228288\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014992532110048666\n",
      "Average test loss: 0.001564865072361297\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014981710754334927\n",
      "Average test loss: 0.0017937475462547607\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01494523546844721\n",
      "Average test loss: 0.0015256331524708205\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014915003774894608\n",
      "Average test loss: 0.0015574091162739528\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014923738890224032\n",
      "Average test loss: 0.0018702452787094646\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014880332317617205\n",
      "Average test loss: 0.0015017029690659709\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014860673114657401\n",
      "Average test loss: 0.0014791122781526711\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014849148032565911\n",
      "Average test loss: 0.001627768374979496\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014827134090993139\n",
      "Average test loss: 0.001528699371860259\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0147960414952702\n",
      "Average test loss: 0.0015117926146421168\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014784050400058429\n",
      "Average test loss: 0.0015182531841306222\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014767635017633438\n",
      "Average test loss: 0.0014841886846762564\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014729520608153608\n",
      "Average test loss: 0.0014895078054525785\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014710198609365358\n",
      "Average test loss: 0.0015272527276538313\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014713994504676925\n",
      "Average test loss: 0.0015130205679063995\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014670740968651242\n",
      "Average test loss: 0.0015517922262143758\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014691788858837552\n",
      "Average test loss: 0.0014661391894850466\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01464308026764128\n",
      "Average test loss: 0.0015225364320601027\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014649999832113583\n",
      "Average test loss: 0.0015214114014266266\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014610262417131\n",
      "Average test loss: 0.0014743097412089506\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014598169391353924\n",
      "Average test loss: 0.001522764307860699\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01460637825810247\n",
      "Average test loss: 0.001523731246487134\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014604132590194543\n",
      "Average test loss: 0.0014510484573741753\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014547329026791785\n",
      "Average test loss: 0.0014740621447563171\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014542168142067062\n",
      "Average test loss: 0.0014778053251405558\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014549263204137484\n",
      "Average test loss: 0.001502776329095165\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014511671988500488\n",
      "Average test loss: 0.001463812340464857\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014474314033571217\n",
      "Average test loss: 0.0014946644890846477\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014491887840131919\n",
      "Average test loss: 0.0014732614352057378\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014466999751826127\n",
      "Average test loss: 0.001538243241608143\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014453738892243969\n",
      "Average test loss: 0.0015420000005720391\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014446334768500593\n",
      "Average test loss: 0.0014982548569225603\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014430252643095122\n",
      "Average test loss: 0.0015022000410180125\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014393445409006543\n",
      "Average test loss: 0.0014590721865081124\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014398042352663147\n",
      "Average test loss: 0.0014925752182801564\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014383909760249985\n",
      "Average test loss: 0.0015148308705538512\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014399317333267795\n",
      "Average test loss: 0.0014713202975690364\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014362349423269432\n",
      "Average test loss: 0.001486598721705377\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014362862359318468\n",
      "Average test loss: 0.0015105675504439406\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014342853462530508\n",
      "Average test loss: 0.0015109970942139626\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014300556233359707\n",
      "Average test loss: 0.0014774681949574087\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014307969583405388\n",
      "Average test loss: 0.0014508749987516137\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014329562171465821\n",
      "Average test loss: 0.0014873830849925678\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014304921947419643\n",
      "Average test loss: 0.0014334850964239903\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014278952554696136\n",
      "Average test loss: 0.0015073340753507282\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014266141358349058\n",
      "Average test loss: 0.0014744077808120184\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014272010934021738\n",
      "Average test loss: 0.0015180985456229085\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01425427750746409\n",
      "Average test loss: 0.0014501372206335266\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014227957277662224\n",
      "Average test loss: 0.001446975805900163\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01422778157144785\n",
      "Average test loss: 0.0014763253807193702\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014204433609214095\n",
      "Average test loss: 0.0025490522492263054\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014220862661798795\n",
      "Average test loss: 0.0017076789600153765\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014192898365358511\n",
      "Average test loss: 0.0015190858014652298\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014204755569497744\n",
      "Average test loss: 0.0014591978475865391\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014174271662202146\n",
      "Average test loss: 0.0014876008862629534\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014172577733794848\n",
      "Average test loss: 0.0015465659981386529\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01415721603234609\n",
      "Average test loss: 0.0015816659536212682\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014153978028231196\n",
      "Average test loss: 0.0016151034810269872\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01414067075153192\n",
      "Average test loss: 0.0015097771730894843\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014161122956209712\n",
      "Average test loss: 0.0017516587684965796\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01414510464337137\n",
      "Average test loss: 0.0014536855253908369\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014101039403014713\n",
      "Average test loss: 0.0014457994119471147\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014102689291867945\n",
      "Average test loss: 0.00702371205886205\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014097389876842498\n",
      "Average test loss: 0.0015133915460771984\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014094621239436998\n",
      "Average test loss: 0.0014533677029733856\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014088253154522843\n",
      "Average test loss: 0.0014515883716651136\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014076988384955458\n",
      "Average test loss: 0.0014755841484293342\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014055879033274122\n",
      "Average test loss: 0.0014491815341429579\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014035690336591668\n",
      "Average test loss: 0.0014904562943201098\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014043518853684267\n",
      "Average test loss: 0.0014877510016991031\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014048897583451537\n",
      "Average test loss: 0.0015496416362002492\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014011389202541776\n",
      "Average test loss: 0.0014723842640717824\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013990320110486613\n",
      "Average test loss: 0.0014990999939747984\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013992735512554645\n",
      "Average test loss: 0.001468894664508601\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013988566757076316\n",
      "Average test loss: 0.0015024789198198251\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013985650644534164\n",
      "Average test loss: 0.001640396031861504\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013968309361073706\n",
      "Average test loss: 0.0014900681695176495\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013974011386434237\n",
      "Average test loss: 0.001474667170498934\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013957347959280013\n",
      "Average test loss: 0.001448044219157762\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013961824540462759\n",
      "Average test loss: 0.00155002088430855\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013937378779881531\n",
      "Average test loss: 0.0014967469727723962\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013942287065916591\n",
      "Average test loss: 0.0014808108016020722\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013937702365219594\n",
      "Average test loss: 0.0014737960557556814\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013917328724430668\n",
      "Average test loss: 0.0014634725797093577\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013916639923221535\n",
      "Average test loss: 0.0014807845367532638\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013901102301975091\n",
      "Average test loss: 0.0014590973566389746\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013914179830087556\n",
      "Average test loss: 0.0014542282168856925\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013873514272272586\n",
      "Average test loss: 0.0014468853841018345\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013897976136042012\n",
      "Average test loss: 0.0014420393269716039\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013883922461834218\n",
      "Average test loss: 0.0014773854482918979\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013861856036716037\n",
      "Average test loss: 0.0014851596317150527\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013844793893396854\n",
      "Average test loss: 0.0014529427091280619\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013875535217424234\n",
      "Average test loss: 0.0014493640659170018\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013848865892324183\n",
      "Average test loss: 0.0014265220890649492\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013837965000006888\n",
      "Average test loss: 0.0014440248357132078\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01383453477339612\n",
      "Average test loss: 0.009026874607428908\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013856595133741696\n",
      "Average test loss: 0.0014513260408097672\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013817922324770027\n",
      "Average test loss: 0.0014579960305450691\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013797986442844074\n",
      "Average test loss: 0.001471699641707043\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013801026680403285\n",
      "Average test loss: 0.0014855442372047238\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013796094582312637\n",
      "Average test loss: 0.0014501487450260255\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013795968869494067\n",
      "Average test loss: 0.0014674874870106579\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013788207893570265\n",
      "Average test loss: 0.0014308744855225086\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013772192613118225\n",
      "Average test loss: 0.0014918102630310588\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013785891844994493\n",
      "Average test loss: 0.0014486270981012947\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013751994921929307\n",
      "Average test loss: 0.0016189817163265413\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013761465382244853\n",
      "Average test loss: 0.0014991395987777247\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013739805017908413\n",
      "Average test loss: 0.0014486961902843581\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013764837521645758\n",
      "Average test loss: 0.0014615336765224736\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01376060888502333\n",
      "Average test loss: 0.0014422052727701762\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013732197364171346\n",
      "Average test loss: 0.0015324818902752466\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013723582652707894\n",
      "Average test loss: 0.0014735911436792877\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013725299945308102\n",
      "Average test loss: 0.0014524594909614986\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013706843542555969\n",
      "Average test loss: 0.001501584154346751\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013721678981350528\n",
      "Average test loss: 0.0014807383968598313\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01368593115111192\n",
      "Average test loss: 0.0014780064599795473\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013711845125589105\n",
      "Average test loss: 0.0014286987762898206\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013724667263527711\n",
      "Average test loss: 0.0014526718016713857\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013693952602644761\n",
      "Average test loss: 0.001688150521264308\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01369042021367285\n",
      "Average test loss: 0.0014864906186444892\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013676308684051036\n",
      "Average test loss: 0.0014609562718412943\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013676209473775493\n",
      "Average test loss: 0.0014851997046627932\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013655016508367326\n",
      "Average test loss: 0.001465197455137968\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013646940641105176\n",
      "Average test loss: 0.0014341575559228658\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01366658115469747\n",
      "Average test loss: 0.001471723572557999\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013642600705226262\n",
      "Average test loss: 0.0015493457269751363\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013658577980266677\n",
      "Average test loss: 0.0014793308747725355\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013652838408119148\n",
      "Average test loss: 0.0014639101265412238\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01363157475243012\n",
      "Average test loss: 0.0017266819882206619\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013648119935558901\n",
      "Average test loss: 0.0014551123761468463\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013627614406247934\n",
      "Average test loss: 0.0015018967533898023\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013605823990371493\n",
      "Average test loss: 0.001456294576637447\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013611661602225569\n",
      "Average test loss: 0.0015152173472775354\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013613301235768531\n",
      "Average test loss: 0.0014557016619170706\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013597397095627255\n",
      "Average test loss: 0.0014664111700322894\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0135854413981239\n",
      "Average test loss: 0.0014756612252030108\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013574255858030584\n",
      "Average test loss: 0.0015139741910518043\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013593983072373602\n",
      "Average test loss: 0.0014644691169572372\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013580390579998493\n",
      "Average test loss: 0.0014300603023212816\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013593156716889805\n",
      "Average test loss: 0.0015568700443125433\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013585806820955542\n",
      "Average test loss: 0.0016090545502698255\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01354675614171558\n",
      "Average test loss: 0.0014340162141145104\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013572568943103155\n",
      "Average test loss: 0.0014285480831232335\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013552801503903335\n",
      "Average test loss: 0.001511596226754288\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013552953094244003\n",
      "Average test loss: 0.0014674293915223743\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013554198945562045\n",
      "Average test loss: 0.0014799439798419674\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013543211099174288\n",
      "Average test loss: 0.0015025157651139631\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013547427873644564\n",
      "Average test loss: 0.0014757074420857761\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013523743332260184\n",
      "Average test loss: 0.0014912593273652924\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013561738967067666\n",
      "Average test loss: 0.0014512823735260301\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013524439409375191\n",
      "Average test loss: 0.0014927509918601976\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013526863457428085\n",
      "Average test loss: 0.0015027635436919\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013511638160381052\n",
      "Average test loss: 0.0015963639650079938\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013523779516418775\n",
      "Average test loss: 0.0014948817648304006\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013495900586247445\n",
      "Average test loss: 0.0014190936814476218\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013518474052349726\n",
      "Average test loss: 0.0014835041103263696\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013498457888762156\n",
      "Average test loss: 0.0014872780710251795\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013484096311032772\n",
      "Average test loss: 0.0014384197492359413\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013504246056079864\n",
      "Average test loss: 0.0014296223561589916\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013488737371232774\n",
      "Average test loss: 0.001519495672856768\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013492074441578654\n",
      "Average test loss: 0.0015163490236219433\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013482863740788565\n",
      "Average test loss: 0.0014855729227678644\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013477913420233462\n",
      "Average test loss: 0.0014741906835180189\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013472314279940393\n",
      "Average test loss: 0.007376420716444651\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013533042035996914\n",
      "Average test loss: 0.001491982225742605\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013463147641056114\n",
      "Average test loss: 0.001436685750881831\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013456589159038333\n",
      "Average test loss: 0.0014993103104126123\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013457142176727454\n",
      "Average test loss: 0.0014934767154562805\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013490587447252539\n",
      "Average test loss: 0.0014365658890455961\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013442862540483476\n",
      "Average test loss: 0.0014425484348709384\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013447981393999523\n",
      "Average test loss: 0.001554666846524924\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0134564207105173\n",
      "Average test loss: 0.001438823352360891\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013421959804164038\n",
      "Average test loss: 0.0014531568478172024\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013433720930582947\n",
      "Average test loss: 0.001571261394251552\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013441511350373428\n",
      "Average test loss: 0.001520994747367998\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01343298740271065\n",
      "Average test loss: 0.0015141993424751692\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013413863581915696\n",
      "Average test loss: 0.0014865613222743075\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013417161180741257\n",
      "Average test loss: 0.0017045526014020046\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013417278625898891\n",
      "Average test loss: 0.0014552659125377735\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01341382098611858\n",
      "Average test loss: 0.0014786158163721363\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013419183276593685\n",
      "Average test loss: 0.0014235528149745531\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01339833937254217\n",
      "Average test loss: 0.0014720632956984143\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013403604127466679\n",
      "Average test loss: 0.0015528123347709577\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013394640298353301\n",
      "Average test loss: 0.0014799436347352134\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013403124113049772\n",
      "Average test loss: 0.0015600043150285879\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013381995938718319\n",
      "Average test loss: 0.0014612056377033393\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013408053981761137\n",
      "Average test loss: 0.0014787255096663204\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0133722937181592\n",
      "Average test loss: 0.001440501079791122\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01338972080581718\n",
      "Average test loss: 0.0014665808982940185\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013377937586771117\n",
      "Average test loss: 0.0014410107675422398\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013376553163760237\n",
      "Average test loss: 0.0016820575545748902\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013364866894152429\n",
      "Average test loss: 0.0014684436589272485\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013370954645176729\n",
      "Average test loss: 0.0014462557319137785\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013377189517021179\n",
      "Average test loss: 0.002188584757244421\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01335154527094629\n",
      "Average test loss: 0.0014852135544642807\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01334135433865918\n",
      "Average test loss: 0.0014379347826664647\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013346488787896103\n",
      "Average test loss: 0.001466434520462321\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013355309874647194\n",
      "Average test loss: 0.0014735713413813049\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013351756206817098\n",
      "Average test loss: 0.0014476371763481033\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013350287328163783\n",
      "Average test loss: 0.0014450606080806917\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013341905479629835\n",
      "Average test loss: 0.0014812336597177717\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013343629597789711\n",
      "Average test loss: 0.0014979208622955614\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013331233101586501\n",
      "Average test loss: 0.0014614669143015312\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013334318530228402\n",
      "Average test loss: 0.0014343586751363344\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013329595114621852\n",
      "Average test loss: 0.0014539764914661647\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013319341407881843\n",
      "Average test loss: 0.0015026303956078158\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013333831544551585\n",
      "Average test loss: 0.0014700169760940803\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013328096110373736\n",
      "Average test loss: 0.0015741039623713327\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013315059941675927\n",
      "Average test loss: 0.0014834403534316355\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013301615633898312\n",
      "Average test loss: 0.0014702820353106492\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013315411816040675\n",
      "Average test loss: 0.001457657973902921\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013306239006419977\n",
      "Average test loss: 0.0014516334081482556\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01331173537671566\n",
      "Average test loss: 0.0014394712856867248\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01331595544434256\n",
      "Average test loss: 0.0014751995034101937\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013295315076907475\n",
      "Average test loss: 0.0015024163450838791\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01330823761307531\n",
      "Average test loss: 0.0014882379650241798\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013289921577605937\n",
      "Average test loss: 0.0014780171735005246\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01328861336575614\n",
      "Average test loss: 0.0014466953054070473\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013283196622298824\n",
      "Average test loss: 0.0014597787982784211\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013280773707562022\n",
      "Average test loss: 0.0014480186132714153\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013277029294934538\n",
      "Average test loss: 0.0014738354292801684\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013275078850487869\n",
      "Average test loss: 0.0014731830472333564\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013272749154104127\n",
      "Average test loss: 0.0014938170445255108\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013291029840707779\n",
      "Average test loss: 0.0015026637434752451\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01327711919364002\n",
      "Average test loss: 0.001434886270823578\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0132718660177456\n",
      "Average test loss: 0.0014423199143881599\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013272766792111926\n",
      "Average test loss: 0.0014655088826807008\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013245722262395752\n",
      "Average test loss: 0.0014734188094735145\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0132722174409363\n",
      "Average test loss: 0.001461694331943161\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013253459127412902\n",
      "Average test loss: 0.0014678491636489828\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01325134355492062\n",
      "Average test loss: 0.0015021426806019413\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013240135874185297\n",
      "Average test loss: 0.001420420491343571\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013261390914519628\n",
      "Average test loss: 0.0016607855110325747\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013238977509240309\n",
      "Average test loss: 0.0015173368747863506\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013239890883366266\n",
      "Average test loss: 0.0014780615507107642\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013259058520197868\n",
      "Average test loss: 0.001469265621672902\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013233241400784917\n",
      "Average test loss: 0.0015114231740848887\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013226410880684852\n",
      "Average test loss: 0.0016926471151204573\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013245681292480892\n",
      "Average test loss: 0.0015358554334897133\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013233925452248918\n",
      "Average test loss: 0.0015136162587958905\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013239676856332355\n",
      "Average test loss: 0.00148143664178335\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_64_Depth5/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.01\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.96\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.985914975060357\n",
      "Average test loss: 0.011495959897008207\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7068398025830587\n",
      "Average test loss: 0.009443187534809112\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4725997794204288\n",
      "Average test loss: 0.008195544961425993\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3704102176825205\n",
      "Average test loss: 0.00795029717311263\n",
      "Epoch 5/300\n",
      "Average training loss: 0.31015565186076693\n",
      "Average test loss: 0.007453564032912255\n",
      "Epoch 6/300\n",
      "Average training loss: 0.27099211049079897\n",
      "Average test loss: 0.007117503167440494\n",
      "Epoch 7/300\n",
      "Average training loss: 0.24187704363134171\n",
      "Average test loss: 0.00850981379341748\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2181187744140625\n",
      "Average test loss: 0.006951010062048833\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2011922672457165\n",
      "Average test loss: 0.007823510912143523\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18734425718254513\n",
      "Average test loss: 0.006395382797138558\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17453567787011465\n",
      "Average test loss: 0.0068028475273814466\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1647735334502326\n",
      "Average test loss: 0.007025688569992781\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15623556991418203\n",
      "Average test loss: 0.006264415811747312\n",
      "Epoch 14/300\n",
      "Average training loss: 0.15037210473749374\n",
      "Average test loss: 0.006017861846006579\n",
      "Epoch 15/300\n",
      "Average training loss: 0.14323019875420465\n",
      "Average test loss: 0.009446891232497163\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1370449367231793\n",
      "Average test loss: 0.007173272466907898\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13310319243537055\n",
      "Average test loss: 0.006001964097221693\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12753697253598106\n",
      "Average test loss: 0.006001184181620677\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12338661852147845\n",
      "Average test loss: 0.0059481348374651535\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12027967270215352\n",
      "Average test loss: 0.005528499599546194\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11630921482377582\n",
      "Average test loss: 0.00671256468941768\n",
      "Epoch 22/300\n",
      "Average training loss: 0.113921293258667\n",
      "Average test loss: 0.0056528463409178786\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11175746150149239\n",
      "Average test loss: 0.005291183342950211\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10908990832832124\n",
      "Average test loss: 0.0052437095025347335\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10725847683350245\n",
      "Average test loss: 0.005232007324281666\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10484480496909883\n",
      "Average test loss: 0.005142635808636745\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10336661142110824\n",
      "Average test loss: 0.005181155340125163\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10148561554484897\n",
      "Average test loss: 0.005115641124960449\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10018796852562163\n",
      "Average test loss: 0.005030347150233057\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09908172072304619\n",
      "Average test loss: 0.005158629512828257\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09749818887313207\n",
      "Average test loss: 0.0049772255296508475\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09627660007609261\n",
      "Average test loss: 0.0051007735443611936\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09549139798349804\n",
      "Average test loss: 0.005045771489126815\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0943213387992647\n",
      "Average test loss: 0.0049875423887537585\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09341328693760766\n",
      "Average test loss: 0.005295809163401524\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09272661119699478\n",
      "Average test loss: 0.0049480223562568424\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0916662661102083\n",
      "Average test loss: 0.00490566999423835\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09159797236654493\n",
      "Average test loss: 0.005073806795395083\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09070643869373533\n",
      "Average test loss: 0.004861783519801166\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08979569885465834\n",
      "Average test loss: 0.004890070202863878\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08983607669671377\n",
      "Average test loss: 0.004916787421123849\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08847854370541043\n",
      "Average test loss: 0.005996442582458258\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08803218649493323\n",
      "Average test loss: 0.005022708563754956\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08764827525284555\n",
      "Average test loss: 0.004928332376397318\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08718638558520211\n",
      "Average test loss: 0.004832428404026561\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08641740904914008\n",
      "Average test loss: 0.004779483394076427\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08616695320606231\n",
      "Average test loss: 0.004764484610408544\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08568673510683908\n",
      "Average test loss: 0.004871447296606169\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0852922197646565\n",
      "Average test loss: 0.0047650029448171455\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08503331576453314\n",
      "Average test loss: 0.0049359970082425405\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08499103632238177\n",
      "Average test loss: 0.00472045165921251\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08423320762316386\n",
      "Average test loss: 0.004823366042847435\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08387827669911914\n",
      "Average test loss: 0.005071541121022568\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08345058498779932\n",
      "Average test loss: 0.004699949173877636\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08340961089399127\n",
      "Average test loss: 0.005543566439507736\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08307604934109582\n",
      "Average test loss: 0.0047366721923980445\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08317042647467719\n",
      "Average test loss: 0.004728038774182399\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08239447379774517\n",
      "Average test loss: 0.004725017962770329\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08229288762145573\n",
      "Average test loss: 0.004778773703301946\n",
      "Epoch 60/300\n",
      "Average training loss: 725.0207258111503\n",
      "Average test loss: 0.04406237117118306\n",
      "Epoch 61/300\n",
      "Average training loss: 9.55847799173991\n",
      "Average test loss: 0.0470817543665568\n",
      "Epoch 62/300\n",
      "Average training loss: 8.638402353074815\n",
      "Average test loss: 0.0171673035091824\n",
      "Epoch 63/300\n",
      "Average training loss: 7.941511762830946\n",
      "Average test loss: 0.019363131879104507\n",
      "Epoch 64/300\n",
      "Average training loss: 7.267412353939481\n",
      "Average test loss: 0.024151495859026907\n",
      "Epoch 65/300\n",
      "Average training loss: 6.538756344265408\n",
      "Average test loss: 0.013149496755666203\n",
      "Epoch 66/300\n",
      "Average training loss: 5.750620849609375\n",
      "Average test loss: 0.01381823717057705\n",
      "Epoch 67/300\n",
      "Average training loss: 5.0073574697706436\n",
      "Average test loss: 0.1656500200289819\n",
      "Epoch 68/300\n",
      "Average training loss: 4.403209911346435\n",
      "Average test loss: 0.00922375560593274\n",
      "Epoch 69/300\n",
      "Average training loss: 3.8662706796858046\n",
      "Average test loss: 0.008434169732034206\n",
      "Epoch 70/300\n",
      "Average training loss: 3.365404611799452\n",
      "Average test loss: 0.007813584693190124\n",
      "Epoch 71/300\n",
      "Average training loss: 2.905757624520196\n",
      "Average test loss: 0.007467360587169727\n",
      "Epoch 72/300\n",
      "Average training loss: 684.2276886482239\n",
      "Average test loss: 0.04405880663792292\n",
      "Epoch 73/300\n",
      "Average training loss: 14.429294544643826\n",
      "Average test loss: 0.0275370179431306\n",
      "Epoch 74/300\n",
      "Average training loss: 13.092675773620606\n",
      "Average test loss: 0.019689406815502377\n",
      "Epoch 75/300\n",
      "Average training loss: 12.313629199557834\n",
      "Average test loss: 0.020738377066122162\n",
      "Epoch 76/300\n",
      "Average training loss: 11.751562698364257\n",
      "Average test loss: 0.01766474600467417\n",
      "Epoch 77/300\n",
      "Average training loss: 11.23479263136122\n",
      "Average test loss: 0.01462273584637377\n",
      "Epoch 78/300\n",
      "Average training loss: 10.725402549743652\n",
      "Average test loss: 0.021837759943472014\n",
      "Epoch 79/300\n",
      "Average training loss: 10.187069803025988\n",
      "Average test loss: 0.013282710248397457\n",
      "Epoch 80/300\n",
      "Average training loss: 9.646926512824164\n",
      "Average test loss: 0.012925878908899095\n",
      "Epoch 81/300\n",
      "Average training loss: 9.107581769307455\n",
      "Average test loss: 0.012188927027086417\n",
      "Epoch 82/300\n",
      "Average training loss: 8.554540438334147\n",
      "Average test loss: 0.011997661773529318\n",
      "Epoch 83/300\n",
      "Average training loss: 7.999724589877658\n",
      "Average test loss: 0.010210789275666078\n",
      "Epoch 84/300\n",
      "Average training loss: 7.447804821438259\n",
      "Average test loss: 0.010069579189022381\n",
      "Epoch 85/300\n",
      "Average training loss: 6.886821620093452\n",
      "Average test loss: 0.009229631625529793\n",
      "Epoch 86/300\n",
      "Average training loss: 6.31958752907647\n",
      "Average test loss: 0.00956542005472713\n",
      "Epoch 87/300\n",
      "Average training loss: 5.726915567186143\n",
      "Average test loss: 0.008433786888089444\n",
      "Epoch 88/300\n",
      "Average training loss: 5.097057647281223\n",
      "Average test loss: 0.008279668120874299\n",
      "Epoch 89/300\n",
      "Average training loss: 4.424394160376655\n",
      "Average test loss: 0.007775543596181604\n",
      "Epoch 90/300\n",
      "Average training loss: 3.7230182348887126\n",
      "Average test loss: 0.007804789896640513\n",
      "Epoch 91/300\n",
      "Average training loss: 3.0053954073588054\n",
      "Average test loss: 0.006895718597289589\n",
      "Epoch 92/300\n",
      "Average training loss: 2.2570099675920274\n",
      "Average test loss: 0.006906125284731388\n",
      "Epoch 93/300\n",
      "Average training loss: 1.6041337180667454\n",
      "Average test loss: 0.006582699719402525\n",
      "Epoch 94/300\n",
      "Average training loss: 1.151546302901374\n",
      "Average test loss: 0.006463263787329197\n",
      "Epoch 95/300\n",
      "Average training loss: 0.8479492636786566\n",
      "Average test loss: 0.0059634449014233214\n",
      "Epoch 96/300\n",
      "Average training loss: 0.6310060093667772\n",
      "Average test loss: 0.006166066565861304\n",
      "Epoch 97/300\n",
      "Average training loss: 0.466498276286655\n",
      "Average test loss: 0.006026085787349277\n",
      "Epoch 98/300\n",
      "Average training loss: 0.34562336860762705\n",
      "Average test loss: 0.005483237860517369\n",
      "Epoch 99/300\n",
      "Average training loss: 0.26341182584232753\n",
      "Average test loss: 0.005409950592865547\n",
      "Epoch 100/300\n",
      "Average training loss: 0.21574299697081248\n",
      "Average test loss: 0.005292792083902492\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1880743893120024\n",
      "Average test loss: 0.00520590864866972\n",
      "Epoch 102/300\n",
      "Average training loss: 0.16967353622780906\n",
      "Average test loss: 0.005180906762679418\n",
      "Epoch 103/300\n",
      "Average training loss: 0.15421459135744306\n",
      "Average test loss: 0.005244699423925745\n",
      "Epoch 104/300\n",
      "Average training loss: 0.14247841087977092\n",
      "Average test loss: 0.005472996719595459\n",
      "Epoch 105/300\n",
      "Average training loss: 0.1328622524605857\n",
      "Average test loss: 0.005083490285194583\n",
      "Epoch 106/300\n",
      "Average training loss: 0.12453949905766382\n",
      "Average test loss: 0.005172055411256022\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11786715114778942\n",
      "Average test loss: 0.005104162576297919\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11293983995583322\n",
      "Average test loss: 0.005112297299421495\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10849387525187598\n",
      "Average test loss: 0.006867694917238421\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10536201820770899\n",
      "Average test loss: 0.004966168239919676\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10270790174603463\n",
      "Average test loss: 0.004870440810918808\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1008001434803009\n",
      "Average test loss: 0.005119338719381227\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09860803899500105\n",
      "Average test loss: 0.0048568258591824105\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09675060651368565\n",
      "Average test loss: 0.00476829765116175\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09497695259253185\n",
      "Average test loss: 0.004926152453240421\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09341116019752291\n",
      "Average test loss: 0.004764794133189651\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09174570167064666\n",
      "Average test loss: 0.004860249917126364\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09045303293069204\n",
      "Average test loss: 0.004818669696855876\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08901233169767592\n",
      "Average test loss: 0.004780367448925972\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08810374469227261\n",
      "Average test loss: 0.004897401725666391\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08695629881487953\n",
      "Average test loss: 0.004902048038111792\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08610924155182309\n",
      "Average test loss: 0.004815103997372919\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08541990819242265\n",
      "Average test loss: 0.004818866878334019\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08472188826402029\n",
      "Average test loss: 0.004716403469857243\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08419188785221841\n",
      "Average test loss: 0.004806473404996925\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08386814061138365\n",
      "Average test loss: 0.004792060280425681\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08365118788348304\n",
      "Average test loss: 0.004679089686522881\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08304845411909952\n",
      "Average test loss: 0.005151369285045399\n",
      "Epoch 129/300\n",
      "Average training loss: 0.08269229071670109\n",
      "Average test loss: 0.006364063887132539\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08233497812350592\n",
      "Average test loss: 0.0050994525415201985\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08213401071230571\n",
      "Average test loss: 0.004746290806266997\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08179362812307146\n",
      "Average test loss: 0.005905936907149023\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08154803200893931\n",
      "Average test loss: 0.0047811719224684765\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08189910948938793\n",
      "Average test loss: 0.005020205838398801\n",
      "Epoch 135/300\n",
      "Average training loss: 1888.6596561767724\n",
      "Average test loss: 0.6041684434149001\n",
      "Epoch 136/300\n",
      "Average training loss: 13.657887557135687\n",
      "Average test loss: 0.07117716021007962\n",
      "Epoch 137/300\n",
      "Average training loss: 10.981538312276204\n",
      "Average test loss: 0.05267778169777658\n",
      "Epoch 138/300\n",
      "Average training loss: 9.987453692966037\n",
      "Average test loss: 0.038083264665471184\n",
      "Epoch 139/300\n",
      "Average training loss: 9.026492879231771\n",
      "Average test loss: 0.03597234170635541\n",
      "Epoch 140/300\n",
      "Average training loss: 8.192617331610785\n",
      "Average test loss: 0.026842299055722024\n",
      "Epoch 141/300\n",
      "Average training loss: 7.415675894843208\n",
      "Average test loss: 0.025825240371955764\n",
      "Epoch 142/300\n",
      "Average training loss: 6.164327860938178\n",
      "Average test loss: 0.020898031670186255\n",
      "Epoch 143/300\n",
      "Average training loss: 5.438667805989583\n",
      "Average test loss: 0.015472763133545717\n",
      "Epoch 144/300\n",
      "Average training loss: 4.936957656436496\n",
      "Average test loss: 0.019198577273223136\n",
      "Epoch 145/300\n",
      "Average training loss: 4.445548661973741\n",
      "Average test loss: 0.013559991972313987\n",
      "Epoch 146/300\n",
      "Average training loss: 4.004898443433974\n",
      "Average test loss: 0.012186617088814576\n",
      "Epoch 147/300\n",
      "Average training loss: 3.553273921966553\n",
      "Average test loss: 0.010824580393731595\n",
      "Epoch 148/300\n",
      "Average training loss: 3.1351468130747477\n",
      "Average test loss: 0.010505131142834823\n",
      "Epoch 149/300\n",
      "Average training loss: 2.740060278786553\n",
      "Average test loss: 0.00897462777958976\n",
      "Epoch 150/300\n",
      "Average training loss: 2.3540231488545738\n",
      "Average test loss: 0.008372168485075236\n",
      "Epoch 151/300\n",
      "Average training loss: 2.011810780949063\n",
      "Average test loss: 0.00793123870756891\n",
      "Epoch 152/300\n",
      "Average training loss: 1.693490576426188\n",
      "Average test loss: 0.00873212894383404\n",
      "Epoch 153/300\n",
      "Average training loss: 1.3983978238635593\n",
      "Average test loss: 0.007982831541862752\n",
      "Epoch 154/300\n",
      "Average training loss: 1.1222658553653293\n",
      "Average test loss: 0.007204684287309647\n",
      "Epoch 155/300\n",
      "Average training loss: 0.8996864532364739\n",
      "Average test loss: 0.006780416343361139\n",
      "Epoch 156/300\n",
      "Average training loss: 0.7432257155842251\n",
      "Average test loss: 0.0063991719422241055\n",
      "Epoch 157/300\n",
      "Average training loss: 0.6331412165429857\n",
      "Average test loss: 0.0062032574493851925\n",
      "Epoch 158/300\n",
      "Average training loss: 0.5467705142233107\n",
      "Average test loss: 0.010977995860907766\n",
      "Epoch 159/300\n",
      "Average training loss: 0.48152309754159717\n",
      "Average test loss: 0.0058876757178869515\n",
      "Epoch 160/300\n",
      "Average training loss: 0.4249195573859745\n",
      "Average test loss: 0.005750590385662185\n",
      "Epoch 161/300\n",
      "Average training loss: 0.37415839107831317\n",
      "Average test loss: 0.005496685694903135\n",
      "Epoch 162/300\n",
      "Average training loss: 0.32115292096138\n",
      "Average test loss: 0.005419937291906939\n",
      "Epoch 163/300\n",
      "Average training loss: 0.28341599045859445\n",
      "Average test loss: 0.005299264332900445\n",
      "Epoch 164/300\n",
      "Average training loss: 0.2531053426000807\n",
      "Average test loss: 0.005354791328724888\n",
      "Epoch 165/300\n",
      "Average training loss: 0.2248377514415317\n",
      "Average test loss: 0.0052218082257443005\n",
      "Epoch 166/300\n",
      "Average training loss: 0.19847116763061948\n",
      "Average test loss: 0.005165814896838533\n",
      "Epoch 167/300\n",
      "Average training loss: 0.17752795969115362\n",
      "Average test loss: 0.005250068128108978\n",
      "Epoch 168/300\n",
      "Average training loss: 0.16129557167159186\n",
      "Average test loss: 0.0050556935448613435\n",
      "Epoch 169/300\n",
      "Average training loss: 0.14859753092130026\n",
      "Average test loss: 0.004930489300025834\n",
      "Epoch 170/300\n",
      "Average training loss: 0.13851499086618424\n",
      "Average test loss: 0.004940112354233861\n",
      "Epoch 171/300\n",
      "Average training loss: 0.13010188122590383\n",
      "Average test loss: 0.004904472064226866\n",
      "Epoch 172/300\n",
      "Average training loss: 0.12258615998427073\n",
      "Average test loss: 0.004948576533546051\n",
      "Epoch 173/300\n",
      "Average training loss: 0.11497595491674212\n",
      "Average test loss: 0.004870483540619413\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10954644033643934\n",
      "Average test loss: 0.0048136505612896546\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10473839792278078\n",
      "Average test loss: 0.004827556237785352\n",
      "Epoch 176/300\n",
      "Average training loss: 0.1007668732934528\n",
      "Average test loss: 0.004776376790884468\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0980658474697007\n",
      "Average test loss: 0.004713869183013837\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0957886966003312\n",
      "Average test loss: 0.004823511556204823\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0933753203286065\n",
      "Average test loss: 0.004785135364780823\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09161492785480288\n",
      "Average test loss: 0.004947820201102231\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08977188214328553\n",
      "Average test loss: 0.004644967140836848\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0882197119527393\n",
      "Average test loss: 0.00468219942599535\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08666674199369219\n",
      "Average test loss: 0.004719442070772251\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08566182819339964\n",
      "Average test loss: 0.004659109871006675\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08442840251657698\n",
      "Average test loss: 0.004750224978559547\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08372994682523939\n",
      "Average test loss: 0.004679277708960904\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08305361162291633\n",
      "Average test loss: 0.005024864073842764\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08250751685433917\n",
      "Average test loss: 0.004738325872975919\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08199875983926985\n",
      "Average test loss: 0.004983905116717021\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08190530985593795\n",
      "Average test loss: 0.004727298916007081\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0813160300652186\n",
      "Average test loss: 0.004699010327872303\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08119208947155211\n",
      "Average test loss: 0.004764728210866451\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0808584188024203\n",
      "Average test loss: 0.004730381524811188\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08066008267137739\n",
      "Average test loss: 0.004666523470025923\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08050851215256585\n",
      "Average test loss: 0.004688486309515106\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08017762979865074\n",
      "Average test loss: 0.005056206270638439\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08002263375454478\n",
      "Average test loss: 0.004697723611361451\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07974687193499672\n",
      "Average test loss: 0.004708611374927891\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07969604876306322\n",
      "Average test loss: 0.006411345503396458\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07946912806563908\n",
      "Average test loss: 0.004625824734154675\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07932805530230204\n",
      "Average test loss: 0.005811633069482115\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07895274321900474\n",
      "Average test loss: 0.004621321775019169\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07887018371952904\n",
      "Average test loss: 0.004680559595425924\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07875506970948642\n",
      "Average test loss: 0.004660312585946586\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07841681317488353\n",
      "Average test loss: 0.004641703334119585\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07827698569165335\n",
      "Average test loss: 0.004743854233581159\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07849144117699729\n",
      "Average test loss: 0.004884734028743373\n",
      "Epoch 208/300\n",
      "Average training loss: 109.1141490427918\n",
      "Average test loss: 0.02689010234673818\n",
      "Epoch 209/300\n",
      "Average training loss: 5.221407162560357\n",
      "Average test loss: 0.017834751352667808\n",
      "Epoch 210/300\n",
      "Average training loss: 4.40033145904541\n",
      "Average test loss: 0.01955021144118574\n",
      "Epoch 211/300\n",
      "Average training loss: 3.827467401928372\n",
      "Average test loss: 0.013276771139767435\n",
      "Epoch 212/300\n",
      "Average training loss: 3.393653693517049\n",
      "Average test loss: 0.013725000303652552\n",
      "Epoch 213/300\n",
      "Average training loss: 3.046070818371243\n",
      "Average test loss: 0.013592081358035406\n",
      "Epoch 214/300\n",
      "Average training loss: 2.7719168071746827\n",
      "Average test loss: 0.011140358604490757\n",
      "Epoch 215/300\n",
      "Average training loss: 2.518111368391249\n",
      "Average test loss: 0.011297847177419398\n",
      "Epoch 216/300\n",
      "Average training loss: 2.2796047880384656\n",
      "Average test loss: 0.009431171483877633\n",
      "Epoch 217/300\n",
      "Average training loss: 2.0539080621931287\n",
      "Average test loss: 0.024410612596405878\n",
      "Epoch 218/300\n",
      "Average training loss: 1.8308411534627278\n",
      "Average test loss: 0.00812846607218186\n",
      "Epoch 219/300\n",
      "Average training loss: 1.6257848785188462\n",
      "Average test loss: 0.008091628514230252\n",
      "Epoch 220/300\n",
      "Average training loss: 1.4222169816758898\n",
      "Average test loss: 0.007448463543007771\n",
      "Epoch 221/300\n",
      "Average training loss: 1.2288141232596503\n",
      "Average test loss: 0.006887640359501044\n",
      "Epoch 222/300\n",
      "Average training loss: 1.0411165732277765\n",
      "Average test loss: 0.006673918243911532\n",
      "Epoch 223/300\n",
      "Average training loss: 0.8669819114473131\n",
      "Average test loss: 0.006677588377975756\n",
      "Epoch 224/300\n",
      "Average training loss: 0.7177692156367832\n",
      "Average test loss: 0.006213424787753158\n",
      "Epoch 225/300\n",
      "Average training loss: 0.5934162386788262\n",
      "Average test loss: 0.006030707247141335\n",
      "Epoch 226/300\n",
      "Average training loss: 0.48728821227285596\n",
      "Average test loss: 0.006119927742415004\n",
      "Epoch 227/300\n",
      "Average training loss: 0.39287690795792474\n",
      "Average test loss: 0.005799839490817653\n",
      "Epoch 228/300\n",
      "Average training loss: 0.3100832998222775\n",
      "Average test loss: 0.0059765578466984964\n",
      "Epoch 229/300\n",
      "Average training loss: 0.25366616978910234\n",
      "Average test loss: 0.005659631546586752\n",
      "Epoch 230/300\n",
      "Average training loss: 0.21480603851212396\n",
      "Average test loss: 0.0053053859045936\n",
      "Epoch 231/300\n",
      "Average training loss: 0.18589151810275184\n",
      "Average test loss: 0.00532406048476696\n",
      "Epoch 232/300\n",
      "Average training loss: 0.1639089997212092\n",
      "Average test loss: 0.005259250561189321\n",
      "Epoch 233/300\n",
      "Average training loss: 0.14660176189078225\n",
      "Average test loss: 0.005287081627796094\n",
      "Epoch 234/300\n",
      "Average training loss: 0.1333053758541743\n",
      "Average test loss: 0.005042551209943162\n",
      "Epoch 235/300\n",
      "Average training loss: 0.12332264827357398\n",
      "Average test loss: 0.004953385101010402\n",
      "Epoch 236/300\n",
      "Average training loss: 0.11546256189213859\n",
      "Average test loss: 0.005093288143061929\n",
      "Epoch 237/300\n",
      "Average training loss: 0.10923994013998244\n",
      "Average test loss: 0.004914979272211592\n",
      "Epoch 238/300\n",
      "Average training loss: 0.10499247643682692\n",
      "Average test loss: 0.004775902101149161\n",
      "Epoch 239/300\n",
      "Average training loss: 0.10156827088859346\n",
      "Average test loss: 0.0047686645305818984\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09808790867196189\n",
      "Average test loss: 0.00480797548364434\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09688341860638724\n",
      "Average test loss: 0.004756645969632599\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09332832003302044\n",
      "Average test loss: 0.004798161656906208\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09134120013978746\n",
      "Average test loss: 0.005317493046737379\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08953035867214203\n",
      "Average test loss: 0.004821580169101556\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0878133514854643\n",
      "Average test loss: 0.0047315248602794275\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08705365286270778\n",
      "Average test loss: 0.004656059602482452\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08496446596251593\n",
      "Average test loss: 0.004955100048747328\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08376465958356857\n",
      "Average test loss: 0.004664361655298207\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08284665860070123\n",
      "Average test loss: 0.004703643033487929\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0818369728591707\n",
      "Average test loss: 0.004803173498974906\n",
      "Epoch 251/300\n",
      "Average training loss: 0.081102070040173\n",
      "Average test loss: 0.004659400260696808\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08064524732695685\n",
      "Average test loss: 0.004717836667680078\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08003664327992334\n",
      "Average test loss: 0.0046731866180068915\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07974134100808038\n",
      "Average test loss: 0.004954672868053118\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0792920103404257\n",
      "Average test loss: 0.004704365286148256\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07918869845734702\n",
      "Average test loss: 0.004679529446694586\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07886101579666138\n",
      "Average test loss: 0.004814480800181627\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07859116808573405\n",
      "Average test loss: 0.005865229629808002\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07848493420415455\n",
      "Average test loss: 0.00649458507531219\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07815551649861865\n",
      "Average test loss: 0.0046964569042126335\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07795581032832463\n",
      "Average test loss: 0.004688792592949338\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07797832333379322\n",
      "Average test loss: 0.00465011636291941\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07769346426592932\n",
      "Average test loss: 0.004648229300768839\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07737532212999132\n",
      "Average test loss: 0.004671689070347282\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07745895654625364\n",
      "Average test loss: 0.00490824831980798\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07721437972121768\n",
      "Average test loss: 0.00909714439863132\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09030195237530603\n",
      "Average test loss: 0.004698038489247362\n",
      "Epoch 268/300\n",
      "Average training loss: 6325.1871738076015\n",
      "Average test loss: 6.347923352413707\n",
      "Epoch 269/300\n",
      "Average training loss: 5.153818731943766\n",
      "Average test loss: 2.7117543872528604\n",
      "Epoch 270/300\n",
      "Average training loss: 4.29056449423896\n",
      "Average test loss: 0.12159397970967822\n",
      "Epoch 271/300\n",
      "Average training loss: 3.7174255792829727\n",
      "Average test loss: 0.6177309957544009\n",
      "Epoch 272/300\n",
      "Average training loss: 3.2587156386905245\n",
      "Average test loss: 0.1525263147867388\n",
      "Epoch 273/300\n",
      "Average training loss: 2.8724581926133896\n",
      "Average test loss: 0.0121753939282563\n",
      "Epoch 274/300\n",
      "Average training loss: 2.537696527481079\n",
      "Average test loss: 1.4552287747164567\n",
      "Epoch 275/300\n",
      "Average training loss: 2.2746343404981824\n",
      "Average test loss: 0.015839674804773596\n",
      "Epoch 276/300\n",
      "Average training loss: 2.0377954652574326\n",
      "Average test loss: 0.011425103942553203\n",
      "Epoch 277/300\n",
      "Average training loss: 1.8289121039708456\n",
      "Average test loss: 0.007744362525641918\n",
      "Epoch 278/300\n",
      "Average training loss: 1.6497982940673828\n",
      "Average test loss: 0.007553155614684026\n",
      "Epoch 279/300\n",
      "Average training loss: 1.5007988805770873\n",
      "Average test loss: 0.007862140360805724\n",
      "Epoch 280/300\n",
      "Average training loss: 1.3657048304875692\n",
      "Average test loss: 0.007016501208974255\n",
      "Epoch 281/300\n",
      "Average training loss: 1.2365311885409884\n",
      "Average test loss: 0.0067121300850477485\n",
      "Epoch 282/300\n",
      "Average training loss: 1.1238456814024183\n",
      "Average test loss: 0.00644404280020131\n",
      "Epoch 283/300\n",
      "Average training loss: 1.0189460578494602\n",
      "Average test loss: 0.006233428193049298\n",
      "Epoch 284/300\n",
      "Average training loss: 0.910011415110694\n",
      "Average test loss: 0.006965495556592941\n",
      "Epoch 285/300\n",
      "Average training loss: 0.8048375844425625\n",
      "Average test loss: 0.00567961738920874\n",
      "Epoch 286/300\n",
      "Average training loss: 0.6987014843093025\n",
      "Average test loss: 0.005743511949976285\n",
      "Epoch 287/300\n",
      "Average training loss: 0.6057334126896329\n",
      "Average test loss: 0.005594495859411028\n",
      "Epoch 288/300\n",
      "Average training loss: 0.5259826077620189\n",
      "Average test loss: 0.005370109766307804\n",
      "Epoch 289/300\n",
      "Average training loss: 0.4607558192147149\n",
      "Average test loss: 0.005785136435594824\n",
      "Epoch 290/300\n",
      "Average training loss: 0.4076105921268463\n",
      "Average test loss: 0.005191555545561844\n",
      "Epoch 291/300\n",
      "Average training loss: 0.3627642896705204\n",
      "Average test loss: 0.005280685287796788\n",
      "Epoch 292/300\n",
      "Average training loss: 0.32237127730581494\n",
      "Average test loss: 0.00517388228368428\n",
      "Epoch 293/300\n",
      "Average training loss: 0.2853445058133867\n",
      "Average test loss: 0.005552514906972647\n",
      "Epoch 294/300\n",
      "Average training loss: 0.24988558247354295\n",
      "Average test loss: 0.005010972649272945\n",
      "Epoch 295/300\n",
      "Average training loss: 0.21421804310215845\n",
      "Average test loss: 0.004906740584307246\n",
      "Epoch 296/300\n",
      "Average training loss: 0.18350783885849847\n",
      "Average test loss: 0.0049264874735640155\n",
      "Epoch 297/300\n",
      "Average training loss: 0.16099584657616087\n",
      "Average test loss: 0.004873638743741645\n",
      "Epoch 298/300\n",
      "Average training loss: 0.14435331467787424\n",
      "Average test loss: 0.005255182921472523\n",
      "Epoch 299/300\n",
      "Average training loss: 0.13295740979909898\n",
      "Average test loss: 0.0049921788983047005\n",
      "Epoch 300/300\n",
      "Average training loss: 0.12515718298488193\n",
      "Average test loss: 0.00480191779219442\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6621686000823974\n",
      "Average test loss: 0.0068283780838052435\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5702951589160495\n",
      "Average test loss: 0.0061195526027845015\n",
      "Epoch 3/300\n",
      "Average training loss: 0.36419841837882994\n",
      "Average test loss: 0.005576081698139509\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2726608975728353\n",
      "Average test loss: 0.004982942887892325\n",
      "Epoch 5/300\n",
      "Average training loss: 0.21985944883028666\n",
      "Average test loss: 0.005314599509040514\n",
      "Epoch 6/300\n",
      "Average training loss: 0.18697449470890892\n",
      "Average test loss: 0.00449985790045725\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1644527040719986\n",
      "Average test loss: 0.0043850681299550665\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14834980450736152\n",
      "Average test loss: 0.004441858157929447\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13586864921781752\n",
      "Average test loss: 0.004022872921079397\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12641530112425486\n",
      "Average test loss: 0.004720868200477626\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11798635886112849\n",
      "Average test loss: 0.0040192949324846265\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11109591988060209\n",
      "Average test loss: 0.0062948674476808976\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10541366503636042\n",
      "Average test loss: 0.003616798768440882\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10080465669764413\n",
      "Average test loss: 0.0038009156487468217\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09625394816531076\n",
      "Average test loss: 0.0034891513633645245\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09265042603678174\n",
      "Average test loss: 0.004150249674088425\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08897811586327023\n",
      "Average test loss: 0.0033979979658292398\n",
      "Epoch 18/300\n",
      "Average training loss: 0.08668230293856727\n",
      "Average test loss: 0.003366078328548206\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08282837014065848\n",
      "Average test loss: 0.0032596814564118783\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08041418877575132\n",
      "Average test loss: 0.0036493926131063036\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07707554402616289\n",
      "Average test loss: 0.003144963104277849\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07454786963264147\n",
      "Average test loss: 0.006488537143915891\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07234967207246357\n",
      "Average test loss: 0.0029724339379204644\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07015546238422393\n",
      "Average test loss: 0.0031286659749845663\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06850334781739446\n",
      "Average test loss: 0.003001025089373191\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06658878295951419\n",
      "Average test loss: 0.003044999748468399\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06543573238452276\n",
      "Average test loss: 0.00284965865748624\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06396672083271875\n",
      "Average test loss: 0.003180036619719532\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06265412899520661\n",
      "Average test loss: 0.003069479214855366\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06141459315021833\n",
      "Average test loss: 0.002789823512029317\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06039740115404129\n",
      "Average test loss: 0.0032064414492083923\n",
      "Epoch 32/300\n",
      "Average training loss: 0.059676270693540576\n",
      "Average test loss: 0.0027745761875477104\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05891375638710128\n",
      "Average test loss: 0.002861397736395399\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05811508957876099\n",
      "Average test loss: 0.002879738356297215\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05767451760503981\n",
      "Average test loss: 0.0027819535579118463\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05715509412354893\n",
      "Average test loss: 0.0028557886843466095\n",
      "Epoch 37/300\n",
      "Average training loss: 0.056536315626568266\n",
      "Average test loss: 0.0027211093734949825\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05927083397573895\n",
      "Average test loss: 0.002985072851387991\n",
      "Epoch 39/300\n",
      "Average training loss: 0.13030217385292053\n",
      "Average test loss: 0.004791232621504201\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07103962160812484\n",
      "Average test loss: 0.0029330555562757784\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0648351998726527\n",
      "Average test loss: 0.0028209639872527783\n",
      "Epoch 42/300\n",
      "Average training loss: 0.061867720782756805\n",
      "Average test loss: 0.0028491467229194113\n",
      "Epoch 43/300\n",
      "Average training loss: 0.060092326660950976\n",
      "Average test loss: 0.0028584176804870368\n",
      "Epoch 44/300\n",
      "Average training loss: 0.058913088851504855\n",
      "Average test loss: 0.0028322673133677908\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05815944197442797\n",
      "Average test loss: 0.0038644300618519384\n",
      "Epoch 46/300\n",
      "Average training loss: 0.057340622362163335\n",
      "Average test loss: 0.0030174953976853025\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05672108028332393\n",
      "Average test loss: 0.002747650027068125\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05631908562117153\n",
      "Average test loss: 0.002812686228296823\n",
      "Epoch 49/300\n",
      "Average training loss: 0.056215635071198145\n",
      "Average test loss: 0.002841807781615191\n",
      "Epoch 50/300\n",
      "Average training loss: 0.055642646428611546\n",
      "Average test loss: 0.00274184473666052\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05527089405722088\n",
      "Average test loss: 0.007483215085334248\n",
      "Epoch 52/300\n",
      "Average training loss: 0.054996103210581675\n",
      "Average test loss: 0.0027517344924724765\n",
      "Epoch 53/300\n",
      "Average training loss: 0.054568204704258175\n",
      "Average test loss: 0.0026823018381579053\n",
      "Epoch 54/300\n",
      "Average training loss: 0.054350519037908974\n",
      "Average test loss: 0.0027249632734391424\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05418962697850333\n",
      "Average test loss: 0.0028332654924856293\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05382004187504451\n",
      "Average test loss: 0.0026313239108357164\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05352306439148055\n",
      "Average test loss: 0.0027620726273291643\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05330726625853115\n",
      "Average test loss: 0.0028611214481708076\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05324075385928154\n",
      "Average test loss: 0.0026865970130181974\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05286349131332503\n",
      "Average test loss: 0.002815492786674036\n",
      "Epoch 61/300\n",
      "Average training loss: 0.052932619992229674\n",
      "Average test loss: 0.00409563582100802\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06043597497873836\n",
      "Average test loss: 0.0028671207492136294\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09850198816590838\n",
      "Average test loss: 0.004099562221103244\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0778666902449396\n",
      "Average test loss: 0.002866979005229142\n",
      "Epoch 65/300\n",
      "Average training loss: 0.061427789734469516\n",
      "Average test loss: 0.002752001330256462\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05813873527447383\n",
      "Average test loss: 0.0027347763979600538\n",
      "Epoch 67/300\n",
      "Average training loss: 0.056416414356893964\n",
      "Average test loss: 0.0027036080329368513\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05536286695467101\n",
      "Average test loss: 0.002695201431090633\n",
      "Epoch 69/300\n",
      "Average training loss: 0.054514836453729204\n",
      "Average test loss: 0.0026965976268466976\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05394504231214523\n",
      "Average test loss: 0.002676268081284232\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05345495461424192\n",
      "Average test loss: 0.02304251992371347\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05306321842471758\n",
      "Average test loss: 0.0026435719254530136\n",
      "Epoch 73/300\n",
      "Average training loss: 0.052859510406851766\n",
      "Average test loss: 0.0027248815945867034\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05250102983911832\n",
      "Average test loss: 0.0027415993503398365\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05230818142493566\n",
      "Average test loss: 0.0026177220321777794\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05214289251797729\n",
      "Average test loss: 0.003316010835684008\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05193648875421948\n",
      "Average test loss: 0.0026849621329456566\n",
      "Epoch 78/300\n",
      "Average training loss: 0.051745205875900056\n",
      "Average test loss: 0.0026346609700057242\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05158434549967448\n",
      "Average test loss: 0.0026580189681715436\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05147883927490976\n",
      "Average test loss: 0.0027255332103619974\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05167650384373135\n",
      "Average test loss: 0.0026522221902592316\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05310366347432136\n",
      "Average test loss: 0.00394346882444289\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06070293611950345\n",
      "Average test loss: 0.002815785902655787\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05427935709887081\n",
      "Average test loss: 0.0026277363033344347\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05203415488534503\n",
      "Average test loss: 0.0026312255906975933\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05132017454836103\n",
      "Average test loss: 0.00267233180699663\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05104552401271131\n",
      "Average test loss: 0.0026727750353101225\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05081378696031041\n",
      "Average test loss: 0.0026002080680595503\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05075165268447664\n",
      "Average test loss: 0.002692718038128482\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05053233616219627\n",
      "Average test loss: 0.002653402491576142\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05041516039106581\n",
      "Average test loss: 0.002744430496253901\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05031521257095867\n",
      "Average test loss: 0.002601476839847035\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05020987954404619\n",
      "Average test loss: 0.0026213625048597654\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04999089812570148\n",
      "Average test loss: 0.0032890921466880376\n",
      "Epoch 95/300\n",
      "Average training loss: 0.049919821941190295\n",
      "Average test loss: 0.00262324758236193\n",
      "Epoch 96/300\n",
      "Average training loss: 0.049812255418962906\n",
      "Average test loss: 0.0026965934834960433\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04966590984993511\n",
      "Average test loss: 0.0026438636552128525\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04961949497461319\n",
      "Average test loss: 0.002600921491988831\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05026763011680709\n",
      "Average test loss: 0.0026381521799291176\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0495464027888245\n",
      "Average test loss: 0.0025842411106245383\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04915699273182286\n",
      "Average test loss: 0.002698316871085101\n",
      "Epoch 102/300\n",
      "Average training loss: 0.049057284279002085\n",
      "Average test loss: 0.0047017697493235274\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04894672451416651\n",
      "Average test loss: 0.0028687852100572654\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04891286054253578\n",
      "Average test loss: 0.0026231323532346223\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04880162290069792\n",
      "Average test loss: 0.00257813444485267\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04864784737096892\n",
      "Average test loss: 0.0032600612722130285\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04868610672156016\n",
      "Average test loss: 0.0025733184748225743\n",
      "Epoch 108/300\n",
      "Average training loss: 0.048957115160094364\n",
      "Average test loss: 0.003194435816672113\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04851098046037886\n",
      "Average test loss: 0.0025961573277082707\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04819093426730898\n",
      "Average test loss: 0.002578244745006992\n",
      "Epoch 111/300\n",
      "Average training loss: 0.048120315253734586\n",
      "Average test loss: 0.002895784242492583\n",
      "Epoch 112/300\n",
      "Average training loss: 0.048092049916585285\n",
      "Average test loss: 0.002592442092175285\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04804360864559809\n",
      "Average test loss: 0.004190962918516662\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04795965977509817\n",
      "Average test loss: 0.0030621569371885723\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0488141339454386\n",
      "Average test loss: 0.002810860996031099\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05183011249370045\n",
      "Average test loss: 0.0025776127750674884\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04858317630489667\n",
      "Average test loss: 0.0025775362863722775\n",
      "Epoch 118/300\n",
      "Average training loss: 0.047910416159364914\n",
      "Average test loss: 0.0039742631218913525\n",
      "Epoch 119/300\n",
      "Average training loss: 0.047881987700859704\n",
      "Average test loss: 0.0025760260625845854\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04765013030833668\n",
      "Average test loss: 0.0026162300142976973\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04750355411569277\n",
      "Average test loss: 0.002674101182156139\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04747484749224451\n",
      "Average test loss: 0.002825803152595957\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04736149553457896\n",
      "Average test loss: 0.002639422226076325\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04737943364845382\n",
      "Average test loss: 0.002794466233708792\n",
      "Epoch 125/300\n",
      "Average training loss: 0.047206860231028665\n",
      "Average test loss: 0.0026081020159439906\n",
      "Epoch 126/300\n",
      "Average training loss: 0.047153806424803206\n",
      "Average test loss: 0.0026462754867970944\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04717803685532676\n",
      "Average test loss: 0.0026919837420185406\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04724912589126163\n",
      "Average test loss: 0.002650950494946705\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04741840649313397\n",
      "Average test loss: 0.0025929958371238575\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04691163814730114\n",
      "Average test loss: 0.0026289394955254265\n",
      "Epoch 131/300\n",
      "Average training loss: 0.046873773051632775\n",
      "Average test loss: 0.0025989253661698767\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04715778758459621\n",
      "Average test loss: 0.002672881379102667\n",
      "Epoch 133/300\n",
      "Average training loss: 9.835116435408592\n",
      "Average test loss: 0.004968647846331199\n",
      "Epoch 134/300\n",
      "Average training loss: 0.974712560918596\n",
      "Average test loss: 0.004353359509259462\n",
      "Epoch 135/300\n",
      "Average training loss: 0.6922316428290473\n",
      "Average test loss: 0.004876325885868735\n",
      "Epoch 136/300\n",
      "Average training loss: 0.5441978267033895\n",
      "Average test loss: 0.003909929841756821\n",
      "Epoch 137/300\n",
      "Average training loss: 0.4377252252101898\n",
      "Average test loss: 0.0034099374461091228\n",
      "Epoch 138/300\n",
      "Average training loss: 0.3539650211334229\n",
      "Average test loss: 0.003146969601718916\n",
      "Epoch 139/300\n",
      "Average training loss: 0.2845491828388638\n",
      "Average test loss: 0.0030777920886046355\n",
      "Epoch 140/300\n",
      "Average training loss: 0.22861120459768508\n",
      "Average test loss: 0.0030050889677885506\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1852469894886017\n",
      "Average test loss: 0.0030219350593785445\n",
      "Epoch 142/300\n",
      "Average training loss: 0.15313574318091075\n",
      "Average test loss: 0.0028532282463792297\n",
      "Epoch 143/300\n",
      "Average training loss: 0.13077762636211182\n",
      "Average test loss: 0.003296553443910347\n",
      "Epoch 144/300\n",
      "Average training loss: 0.11592084256145689\n",
      "Average test loss: 0.0028039335707823435\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10461627589331733\n",
      "Average test loss: 0.0027995002631925874\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0962048134803772\n",
      "Average test loss: 0.0027630045886875856\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08866944147480858\n",
      "Average test loss: 0.002702173415881892\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08219839744766554\n",
      "Average test loss: 0.002748432426402966\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07634656126631631\n",
      "Average test loss: 0.0026927498618347775\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07174705436494615\n",
      "Average test loss: 0.002808360755029652\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06726931708057722\n",
      "Average test loss: 0.0031366586809357007\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06351994650893741\n",
      "Average test loss: 0.002641408661587371\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06090401475959354\n",
      "Average test loss: 0.0031024740669462415\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05882520922356182\n",
      "Average test loss: 0.002590153709053993\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05715581188599268\n",
      "Average test loss: 0.002782436340012484\n",
      "Epoch 156/300\n",
      "Average training loss: 0.055835595746835075\n",
      "Average test loss: 0.002647066328674555\n",
      "Epoch 157/300\n",
      "Average training loss: 0.054622185554769304\n",
      "Average test loss: 0.0026606274309257665\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0536260183652242\n",
      "Average test loss: 670.597990342882\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05277839063273536\n",
      "Average test loss: 0.002633803201839328\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05181041329105695\n",
      "Average test loss: 0.002562867989970578\n",
      "Epoch 161/300\n",
      "Average training loss: 0.051029495924711225\n",
      "Average test loss: 0.0025835276195365522\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05059523027141889\n",
      "Average test loss: 0.002781559376459983\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05001577486594518\n",
      "Average test loss: 0.002613003247314029\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04949629536933369\n",
      "Average test loss: 0.002578306317122446\n",
      "Epoch 165/300\n",
      "Average training loss: 0.049702800628211766\n",
      "Average test loss: 0.002628196423045463\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0487736493382189\n",
      "Average test loss: 0.0026208122595627274\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04841744139459398\n",
      "Average test loss: 0.0028435860108584165\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04820334891809357\n",
      "Average test loss: 0.002875704892600576\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04791413962841034\n",
      "Average test loss: 0.0026178169360177384\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04781449768278334\n",
      "Average test loss: 0.002826114485040307\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04760750260617998\n",
      "Average test loss: 0.0028796521075483824\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04734729601939519\n",
      "Average test loss: 0.0029716667280428937\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04729594322045644\n",
      "Average test loss: 0.0026891160358985264\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04715223913722568\n",
      "Average test loss: 0.002566931341878242\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04698791613512569\n",
      "Average test loss: 0.002600863780412409\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04695727738075786\n",
      "Average test loss: 0.00258020143976642\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0468056550986237\n",
      "Average test loss: 0.00266917094681412\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0467565536333455\n",
      "Average test loss: 0.0026028848946508436\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04663879829976294\n",
      "Average test loss: 0.0025754991293781334\n",
      "Epoch 180/300\n",
      "Average training loss: 0.046552347981267506\n",
      "Average test loss: 0.0025709546371880504\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0463566745205058\n",
      "Average test loss: 0.0026056758287466234\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04640392575992478\n",
      "Average test loss: 0.0025778376466284196\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04627281750738621\n",
      "Average test loss: 0.0028407325796369048\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04697819602820608\n",
      "Average test loss: 0.0026390360347512696\n",
      "Epoch 185/300\n",
      "Average training loss: 0.046398687528239356\n",
      "Average test loss: 0.0032698691005094184\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04681720330152247\n",
      "Average test loss: 0.0027470782642356226\n",
      "Epoch 187/300\n",
      "Average training loss: 1.908935942762428\n",
      "Average test loss: 0.003933806367632416\n",
      "Epoch 188/300\n",
      "Average training loss: 0.7127074098057217\n",
      "Average test loss: 0.00468213505918781\n",
      "Epoch 189/300\n",
      "Average training loss: 0.4292010498576694\n",
      "Average test loss: 0.003360398119936387\n",
      "Epoch 190/300\n",
      "Average training loss: 0.28662474224302503\n",
      "Average test loss: 0.003072791498568323\n",
      "Epoch 191/300\n",
      "Average training loss: 0.20474005393187206\n",
      "Average test loss: 0.0030460110876916182\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1588723224931293\n",
      "Average test loss: 0.002942649661252896\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1327670823070738\n",
      "Average test loss: 0.0028273080420783826\n",
      "Epoch 194/300\n",
      "Average training loss: 0.11637837663624022\n",
      "Average test loss: 0.0027906130105257035\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1056926753256056\n",
      "Average test loss: 0.0028292053072816797\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09832077601883146\n",
      "Average test loss: 0.0027901080008596183\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09176012525955836\n",
      "Average test loss: 0.0026932138277010785\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08532740689648523\n",
      "Average test loss: 0.002697578401511742\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07908241285880406\n",
      "Average test loss: 0.0026513267666515377\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07336294742425283\n",
      "Average test loss: 0.0029099846614731683\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06852609891030524\n",
      "Average test loss: 0.013670583457582527\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06489251448710759\n",
      "Average test loss: 0.002639565530543526\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06295727138386832\n",
      "Average test loss: 0.002726152343985935\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06081694853968091\n",
      "Average test loss: 0.0029207533984962437\n",
      "Epoch 205/300\n",
      "Average training loss: 0.059867086443636156\n",
      "Average test loss: 0.0026547681318802967\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05747839348514875\n",
      "Average test loss: 0.0025887192071725926\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05624060868554645\n",
      "Average test loss: 0.0025575437237405113\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0550965858300527\n",
      "Average test loss: 0.002589551418191857\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05410704493191507\n",
      "Average test loss: 0.0026089778594258757\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05288897364669376\n",
      "Average test loss: 0.0025773397696514924\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052195188264052075\n",
      "Average test loss: 0.002566308200152384\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05124369113975101\n",
      "Average test loss: 0.0025800749354271423\n",
      "Epoch 213/300\n",
      "Average training loss: 0.050705305258433024\n",
      "Average test loss: 0.002636857940401468\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05004270644982656\n",
      "Average test loss: 0.0029216378981040585\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04948597689138518\n",
      "Average test loss: 0.0025744512706167167\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04892679790655772\n",
      "Average test loss: 0.0028110233382839297\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04861547185315026\n",
      "Average test loss: 0.002961881010172268\n",
      "Epoch 218/300\n",
      "Average training loss: 0.048163545191287994\n",
      "Average test loss: 0.0027025149547391467\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04783347609308031\n",
      "Average test loss: 0.0026068824583457577\n",
      "Epoch 220/300\n",
      "Average training loss: 0.047449771775139704\n",
      "Average test loss: 0.00276179300290015\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04734511457880338\n",
      "Average test loss: 0.0026258953890452782\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04712696706917551\n",
      "Average test loss: 0.002564721322721905\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04687788046730889\n",
      "Average test loss: 0.0026383574505647023\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04673467028141022\n",
      "Average test loss: 0.002587760718125436\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04664169802268346\n",
      "Average test loss: 0.0029895440625647704\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04659657568732897\n",
      "Average test loss: 0.004420466656900115\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0463855394853486\n",
      "Average test loss: 0.0046818482509503765\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04631569620470206\n",
      "Average test loss: 0.0026502620199074347\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04619463375541899\n",
      "Average test loss: 0.0033325673728767367\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04617645934224129\n",
      "Average test loss: 0.0026325723114940854\n",
      "Epoch 231/300\n",
      "Average training loss: 0.045958724213971035\n",
      "Average test loss: 0.002594623332325783\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04650663288599915\n",
      "Average test loss: 0.002718996149591274\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05452457904484537\n",
      "Average test loss: 0.0026077292315247985\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04759311533636517\n",
      "Average test loss: 0.002586901552768217\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04598806967172358\n",
      "Average test loss: 0.0026061082925233577\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04562565408481492\n",
      "Average test loss: 0.0025661977844105826\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04549579341544045\n",
      "Average test loss: 0.0030102293509989978\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04555568409297201\n",
      "Average test loss: 0.0026309014020694625\n",
      "Epoch 239/300\n",
      "Average training loss: 0.045517566535207964\n",
      "Average test loss: 0.0026173948310315607\n",
      "Epoch 240/300\n",
      "Average training loss: 0.045501441122757065\n",
      "Average test loss: 0.003516134212828345\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04595711452100012\n",
      "Average test loss: 0.002776837931635479\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04587554829981592\n",
      "Average test loss: 0.002687034503867229\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04541240471601486\n",
      "Average test loss: 0.002714895893302229\n",
      "Epoch 244/300\n",
      "Average training loss: 0.045207028885682427\n",
      "Average test loss: 0.005500523016270664\n",
      "Epoch 245/300\n",
      "Average training loss: 0.045248088134659664\n",
      "Average test loss: 0.0026255882280982203\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04521585746275054\n",
      "Average test loss: 0.0027314230111531086\n",
      "Epoch 247/300\n",
      "Average training loss: 0.045160265690750545\n",
      "Average test loss: 0.002617122258577082\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04529141435689396\n",
      "Average test loss: 0.0026311404965817927\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04525730217165417\n",
      "Average test loss: 0.003141237111762166\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04535854623052809\n",
      "Average test loss: 0.002956446313092278\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04499389882882436\n",
      "Average test loss: 0.0029201800065735977\n",
      "Epoch 252/300\n",
      "Average training loss: 0.045200245400269824\n",
      "Average test loss: 0.0026865656611820064\n",
      "Epoch 253/300\n",
      "Average training loss: 0.045240992976559535\n",
      "Average test loss: 0.0026717042173776363\n",
      "Epoch 254/300\n",
      "Average training loss: 0.044745395316018\n",
      "Average test loss: 0.002606291818122069\n",
      "Epoch 255/300\n",
      "Average training loss: 0.044828811632262336\n",
      "Average test loss: 0.0026262025690327088\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04476320875353283\n",
      "Average test loss: 0.004279555781847901\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04474667255414857\n",
      "Average test loss: 0.002666027490877443\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04468190374970436\n",
      "Average test loss: 0.0027193098620822034\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04466168722510338\n",
      "Average test loss: 0.0026771859232750206\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04492227401667171\n",
      "Average test loss: 0.0026226098195960123\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0450043093793922\n",
      "Average test loss: 0.0026219789176765416\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04452332145306799\n",
      "Average test loss: 0.0026480890947083633\n",
      "Epoch 263/300\n",
      "Average training loss: 0.044470098525285724\n",
      "Average test loss: 0.0026910328689134784\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04440143238008022\n",
      "Average test loss: 0.0027176615711715485\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04442202381955253\n",
      "Average test loss: 0.0031637737492306367\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04436991336610582\n",
      "Average test loss: 0.00319879154012435\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04436726242966122\n",
      "Average test loss: 0.002753564490005374\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04461247405409813\n",
      "Average test loss: 0.0028420591654462948\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04447068159116639\n",
      "Average test loss: 0.00343907504921986\n",
      "Epoch 270/300\n",
      "Average training loss: 0.044445531964302065\n",
      "Average test loss: 0.0030830288684616488\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04465661470095317\n",
      "Average test loss: 0.0026475646727614935\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04434993899861971\n",
      "Average test loss: 0.0027058552242815496\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04425918624798457\n",
      "Average test loss: 0.003586028864193294\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04400463381078508\n",
      "Average test loss: 0.0026888472396466466\n",
      "Epoch 275/300\n",
      "Average training loss: 0.044062092850605646\n",
      "Average test loss: 11.805357753011915\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04643302487333616\n",
      "Average test loss: 0.0026280409650256235\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0438884242027998\n",
      "Average test loss: 0.0028514716223710115\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04385599069794019\n",
      "Average test loss: 0.0026705582164641885\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04380857716335191\n",
      "Average test loss: 0.0026504473519615\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04396786960297161\n",
      "Average test loss: 0.002711183505753676\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04389560094806883\n",
      "Average test loss: 0.002678350548570355\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04407707018653552\n",
      "Average test loss: 0.0027463075638645227\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04404627857274479\n",
      "Average test loss: 0.002783691617970665\n",
      "Epoch 284/300\n",
      "Average training loss: 0.043763780577315226\n",
      "Average test loss: 0.0026871008661886056\n",
      "Epoch 285/300\n",
      "Average training loss: 0.044077719463242425\n",
      "Average test loss: 0.002958218875237637\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04384146311216884\n",
      "Average test loss: 0.0026616713788567316\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04375710522135099\n",
      "Average test loss: 0.0034107302681853375\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04375220482879215\n",
      "Average test loss: 0.0027088785947610933\n",
      "Epoch 289/300\n",
      "Average training loss: 0.043771527376439835\n",
      "Average test loss: 0.002742182720762988\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04389973632825746\n",
      "Average test loss: 0.002627981675374839\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04372514871093962\n",
      "Average test loss: 0.002755812430340383\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04365552555521329\n",
      "Average test loss: 0.0026959531450023253\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04355284643338786\n",
      "Average test loss: 0.002912436366081238\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04366035389900207\n",
      "Average test loss: 0.0026918333139684465\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04393817887372441\n",
      "Average test loss: 0.0026725688880930346\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04376468563742108\n",
      "Average test loss: 0.002706875831923551\n",
      "Epoch 297/300\n",
      "Average training loss: 0.043611185292402904\n",
      "Average test loss: 0.0037014186059435208\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04343605667352676\n",
      "Average test loss: 0.002667526438418362\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04341328045394686\n",
      "Average test loss: 0.002922293355067571\n",
      "Epoch 300/300\n",
      "Average training loss: 0.043613742487298116\n",
      "Average test loss: 0.002681663113232288\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.405584695286221\n",
      "Average test loss: 0.00536833561418785\n",
      "Epoch 2/300\n",
      "Average training loss: 0.46829261536068384\n",
      "Average test loss: 0.004429974187165499\n",
      "Epoch 3/300\n",
      "Average training loss: 0.31396030072371167\n",
      "Average test loss: 0.003956488781919082\n",
      "Epoch 4/300\n",
      "Average training loss: 0.23943089707692464\n",
      "Average test loss: 0.0036561825358205373\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1933983003033532\n",
      "Average test loss: 0.004240358781483438\n",
      "Epoch 6/300\n",
      "Average training loss: 0.16313685989379884\n",
      "Average test loss: 0.003318154156518479\n",
      "Epoch 7/300\n",
      "Average training loss: 0.14035499767462412\n",
      "Average test loss: 0.0034967402252886032\n",
      "Epoch 8/300\n",
      "Average training loss: 0.12489360386133194\n",
      "Average test loss: 0.0032452601184033685\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11334457314014434\n",
      "Average test loss: 0.003111906656788455\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10425812771585252\n",
      "Average test loss: 0.0028186844618370136\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09695781250794729\n",
      "Average test loss: 0.0028552161508964167\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09093263111511866\n",
      "Average test loss: 0.004581160999834537\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08613348859548568\n",
      "Average test loss: 0.0033592025118155612\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08124866204129325\n",
      "Average test loss: 0.003417209611170822\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07715325592623816\n",
      "Average test loss: 0.004375039466967185\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07358332782321506\n",
      "Average test loss: 0.0023608614347047276\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07005380987789896\n",
      "Average test loss: 0.0023725761293123164\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06761149397492408\n",
      "Average test loss: 0.0023797650922917656\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06445169568061829\n",
      "Average test loss: 0.0021718455913166206\n",
      "Epoch 20/300\n",
      "Average training loss: 0.061566559453805285\n",
      "Average test loss: 0.004741899244694247\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06039815142419603\n",
      "Average test loss: 0.002800912905173997\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05754519049657716\n",
      "Average test loss: 0.002272233234718442\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05535866780413522\n",
      "Average test loss: 0.0019945549106018413\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05379871796568235\n",
      "Average test loss: 0.002185911043857535\n",
      "Epoch 25/300\n",
      "Average training loss: 0.051621469679805965\n",
      "Average test loss: 0.012484181731939315\n",
      "Epoch 26/300\n",
      "Average training loss: 0.050277154644330344\n",
      "Average test loss: 0.002006522140569157\n",
      "Epoch 27/300\n",
      "Average training loss: 0.048808725769321125\n",
      "Average test loss: 0.0021152578172170455\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04734912916024526\n",
      "Average test loss: 0.001982254757028487\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04629884644018279\n",
      "Average test loss: 0.002178749085093538\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04534916800260544\n",
      "Average test loss: 0.0018634780483941238\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04454595062798924\n",
      "Average test loss: 0.001813946889920367\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0487724459403091\n",
      "Average test loss: 0.0018982865595155293\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04822840002510283\n",
      "Average test loss: 0.001900263588461611\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04398051185740365\n",
      "Average test loss: 0.0018915105232348046\n",
      "Epoch 35/300\n",
      "Average training loss: 0.043282792740397986\n",
      "Average test loss: 0.0018413165702174108\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04270024052593443\n",
      "Average test loss: 0.001784306558676892\n",
      "Epoch 37/300\n",
      "Average training loss: 0.042133055259784065\n",
      "Average test loss: 0.0018306439564459854\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04196078166365624\n",
      "Average test loss: 0.0020670885007000634\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04132586460643344\n",
      "Average test loss: 0.28188172282112967\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04103504511051708\n",
      "Average test loss: 0.00190560714310656\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04067845248017046\n",
      "Average test loss: 0.0017759819813072681\n",
      "Epoch 42/300\n",
      "Average training loss: 0.042349162502421274\n",
      "Average test loss: 0.0017954841084364388\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04013364305429989\n",
      "Average test loss: 0.0017479454661822982\n",
      "Epoch 44/300\n",
      "Average training loss: 0.039714619666337964\n",
      "Average test loss: 0.0017469064829250176\n",
      "Epoch 45/300\n",
      "Average training loss: 0.039388168235619865\n",
      "Average test loss: 0.0017526447837137513\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0390169159869353\n",
      "Average test loss: 0.0017556470128604108\n",
      "Epoch 47/300\n",
      "Average training loss: 0.039437541961669924\n",
      "Average test loss: 0.001786094914449172\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04246460908651352\n",
      "Average test loss: 0.0023146832291450767\n",
      "Epoch 49/300\n",
      "Average training loss: 0.043101255613896584\n",
      "Average test loss: 0.001803635210212734\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03983663101659881\n",
      "Average test loss: 0.0017425333325647645\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03915380992823177\n",
      "Average test loss: 0.0017338221807860666\n",
      "Epoch 52/300\n",
      "Average training loss: 0.038652587238285276\n",
      "Average test loss: 0.0017418618587156137\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03846348323424657\n",
      "Average test loss: 0.0017578496832607521\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03817885578009817\n",
      "Average test loss: 0.0018966327388253478\n",
      "Epoch 55/300\n",
      "Average training loss: 0.037926527791553075\n",
      "Average test loss: 0.0019575949737595188\n",
      "Epoch 56/300\n",
      "Average training loss: 0.037860711481836104\n",
      "Average test loss: 0.001948556653637853\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03768989411658711\n",
      "Average test loss: 0.004783635656866762\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03754243755671713\n",
      "Average test loss: 0.0017697431094323595\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03737025707297855\n",
      "Average test loss: 0.0019865806043768922\n",
      "Epoch 60/300\n",
      "Average training loss: 0.037242954138252474\n",
      "Average test loss: 0.0016800253338490924\n",
      "Epoch 61/300\n",
      "Average training loss: 0.037059433350960416\n",
      "Average test loss: 0.0017097398525931769\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03691123541858461\n",
      "Average test loss: 0.0022530288733541965\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03677958935499191\n",
      "Average test loss: 0.0018640114282154376\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0365690879854891\n",
      "Average test loss: 0.0016945105019128986\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03655860805842612\n",
      "Average test loss: 0.001844315242022276\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03620236066646046\n",
      "Average test loss: 0.0033157519650542076\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03622539532515738\n",
      "Average test loss: 0.001755151771215929\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03630032074451447\n",
      "Average test loss: 0.004554262470453978\n",
      "Epoch 69/300\n",
      "Average training loss: 0.036561998542812134\n",
      "Average test loss: 0.0021571940230205654\n",
      "Epoch 70/300\n",
      "Average training loss: 0.035954747729831274\n",
      "Average test loss: 0.0016654797099116775\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03577283619509803\n",
      "Average test loss: 0.0016772684779846007\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03573417905966441\n",
      "Average test loss: 0.001745583726403614\n",
      "Epoch 73/300\n",
      "Average training loss: 0.035772288827432525\n",
      "Average test loss: 2.61438990577062\n",
      "Epoch 74/300\n",
      "Average training loss: 0.035589610013696885\n",
      "Average test loss: 0.0017371077287114329\n",
      "Epoch 75/300\n",
      "Average training loss: 0.035346877629558245\n",
      "Average test loss: 0.0018142667396201027\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03525431673725446\n",
      "Average test loss: 0.001909033075078494\n",
      "Epoch 77/300\n",
      "Average training loss: 0.035300614304012726\n",
      "Average test loss: 0.0018059532432299521\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03527633702423837\n",
      "Average test loss: 0.0027183255249013504\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03535244279106458\n",
      "Average test loss: 0.001675601825531986\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03498419550392363\n",
      "Average test loss: 0.001636032348860883\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03486115385757552\n",
      "Average test loss: 0.001994104104737441\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034839993332823116\n",
      "Average test loss: 0.0017699220394715666\n",
      "Epoch 83/300\n",
      "Average training loss: 0.034685875520110133\n",
      "Average test loss: 0.0016692067328840494\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03512157185044554\n",
      "Average test loss: 0.0019076562364482217\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03459330857462353\n",
      "Average test loss: 0.001651202281833523\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03453461202647951\n",
      "Average test loss: 0.0016889179959479305\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0344287183980147\n",
      "Average test loss: 0.002109476733228399\n",
      "Epoch 88/300\n",
      "Average training loss: 0.034367638015084793\n",
      "Average test loss: 0.001678995171872278\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03434674457377858\n",
      "Average test loss: 0.0016515675821445054\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03444487350516849\n",
      "Average test loss: 0.0016672021202329133\n",
      "Epoch 91/300\n",
      "Average training loss: 0.034249705612659456\n",
      "Average test loss: 0.0019575245429037345\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03417954433792167\n",
      "Average test loss: 0.0016368028649853335\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0341597654985057\n",
      "Average test loss: 0.0017119139218702913\n",
      "Epoch 94/300\n",
      "Average training loss: 0.034062269353204305\n",
      "Average test loss: 0.0019412204264145758\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03407747291194068\n",
      "Average test loss: 0.0016338289524945948\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03417131889197562\n",
      "Average test loss: 0.0016537761612691812\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03378242553936111\n",
      "Average test loss: 0.0016987585826880403\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033866983905434606\n",
      "Average test loss: 0.002200982817345195\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0337534235831764\n",
      "Average test loss: 0.0016617616857919429\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03372278120617072\n",
      "Average test loss: 0.0016315061109554436\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03361961430807908\n",
      "Average test loss: 0.0016627669709010256\n",
      "Epoch 102/300\n",
      "Average training loss: 0.033662617082397144\n",
      "Average test loss: 0.02024442979527844\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0335115650428666\n",
      "Average test loss: 0.0016434929275160862\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03381668964359495\n",
      "Average test loss: 0.0017479478879831732\n",
      "Epoch 105/300\n",
      "Average training loss: 0.033734871711995865\n",
      "Average test loss: 0.0016490615189799832\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03336970562405057\n",
      "Average test loss: 0.001656001152160267\n",
      "Epoch 107/300\n",
      "Average training loss: 0.033380930188629365\n",
      "Average test loss: 0.0016573521512457067\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03325172878636254\n",
      "Average test loss: 0.001698473318376475\n",
      "Epoch 109/300\n",
      "Average training loss: 0.034383404682079954\n",
      "Average test loss: 0.0016957561641724574\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03332605624198914\n",
      "Average test loss: 0.0017499366024922994\n",
      "Epoch 111/300\n",
      "Average training loss: 0.033570856160587734\n",
      "Average test loss: 0.001634797789156437\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03326736650367578\n",
      "Average test loss: 0.0016430403085218536\n",
      "Epoch 113/300\n",
      "Average training loss: 0.033080824004279245\n",
      "Average test loss: 0.0016811446446097558\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03307870297796196\n",
      "Average test loss: 0.0016593359066173433\n",
      "Epoch 115/300\n",
      "Average training loss: 0.033060039785173204\n",
      "Average test loss: 0.001786120000605782\n",
      "Epoch 116/300\n",
      "Average training loss: 0.033294178707732096\n",
      "Average test loss: 0.001626509603112936\n",
      "Epoch 117/300\n",
      "Average training loss: 0.033221390783786775\n",
      "Average test loss: 0.0029443876159687835\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03301713827252388\n",
      "Average test loss: 0.0016394050957428085\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0329107904765341\n",
      "Average test loss: 0.0021713026995874114\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03309444587098228\n",
      "Average test loss: 0.0016577198545758922\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03296626678771443\n",
      "Average test loss: 0.0016631024754088786\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03291917854381932\n",
      "Average test loss: 0.009752045463356706\n",
      "Epoch 123/300\n",
      "Average training loss: 0.032912028731571306\n",
      "Average test loss: 0.0018232174304624398\n",
      "Epoch 124/300\n",
      "Average training loss: 0.032824015840888024\n",
      "Average test loss: 0.0017013578014448286\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03273628380066819\n",
      "Average test loss: 0.001784638216925992\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032584461235337786\n",
      "Average test loss: 0.0016798311753405465\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03279640785025226\n",
      "Average test loss: 0.0018627744685444567\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03281573920448621\n",
      "Average test loss: 0.0017230370851854483\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03282562744948599\n",
      "Average test loss: 0.0016984724980882472\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03269767247305976\n",
      "Average test loss: 0.0018250857497461967\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03262237105932501\n",
      "Average test loss: 0.0016414465616043242\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03247136632932557\n",
      "Average test loss: 0.0022678669611405997\n",
      "Epoch 133/300\n",
      "Average training loss: 0.032547159638669755\n",
      "Average test loss: 0.001633719688695338\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0323633210675584\n",
      "Average test loss: 0.0017201098951821526\n",
      "Epoch 135/300\n",
      "Average training loss: 0.032429059661097\n",
      "Average test loss: 0.0027225203692085214\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03248074156873756\n",
      "Average test loss: 0.0018174749444135362\n",
      "Epoch 137/300\n",
      "Average training loss: 0.032292454201314186\n",
      "Average test loss: 0.00174477651849803\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03261696427563826\n",
      "Average test loss: 0.0016968228530345692\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03229798235495885\n",
      "Average test loss: 0.0021243071578856973\n",
      "Epoch 140/300\n",
      "Average training loss: 0.032308877161807484\n",
      "Average test loss: 0.001904799602408376\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03224826227459643\n",
      "Average test loss: 0.002205341734819942\n",
      "Epoch 142/300\n",
      "Average training loss: 0.032274311085542046\n",
      "Average test loss: 0.0016593081374756164\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0324309901562002\n",
      "Average test loss: 0.0016690354945345057\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03223797729942534\n",
      "Average test loss: 0.0016999062767459286\n",
      "Epoch 145/300\n",
      "Average training loss: 0.032206309033764736\n",
      "Average test loss: 0.0017059764692352877\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03214015761680073\n",
      "Average test loss: 0.001763860039723416\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03217258079515563\n",
      "Average test loss: 0.005138475279013316\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03219467047850291\n",
      "Average test loss: 0.0016519748667875925\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03204698875546455\n",
      "Average test loss: 0.0016715037290834717\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03210734096831746\n",
      "Average test loss: 0.0017366302702058521\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03198104158540567\n",
      "Average test loss: 0.0016648539954589473\n",
      "Epoch 152/300\n",
      "Average training loss: 0.032152419689628814\n",
      "Average test loss: 0.0016831191631240976\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03210056485566828\n",
      "Average test loss: 0.002166529599784149\n",
      "Epoch 154/300\n",
      "Average training loss: 0.031907005649473935\n",
      "Average test loss: 0.0017333290253041518\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03203975195520454\n",
      "Average test loss: 0.002037202155217528\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03183607953455713\n",
      "Average test loss: 0.001703332041700681\n",
      "Epoch 157/300\n",
      "Average training loss: 0.032061598574121795\n",
      "Average test loss: 0.0017459206935018302\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03179345450633102\n",
      "Average test loss: 0.0016805813473959764\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03176733167469502\n",
      "Average test loss: 0.004807004192844033\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031807312150796255\n",
      "Average test loss: 0.0022804821787608997\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03175959078967571\n",
      "Average test loss: 0.0017928485642704699\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03171417264640331\n",
      "Average test loss: 0.0016754028113144968\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03174768236279488\n",
      "Average test loss: 0.0016693369585813746\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031806582891278794\n",
      "Average test loss: 0.001823800279862351\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031823838187588584\n",
      "Average test loss: 0.0017678596649525894\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03181907882127497\n",
      "Average test loss: 0.0016818714759623012\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03193691655662325\n",
      "Average test loss: 0.0017155953449093633\n",
      "Epoch 168/300\n",
      "Average training loss: 0.031677279526988664\n",
      "Average test loss: 0.002069175249379542\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03167860351502896\n",
      "Average test loss: 0.0019188183946534992\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03168193299240536\n",
      "Average test loss: 0.0017690797436775432\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03161645752357112\n",
      "Average test loss: 0.0017183924511902863\n",
      "Epoch 172/300\n",
      "Average training loss: 0.032027706099881065\n",
      "Average test loss: 0.0017885640801654922\n",
      "Epoch 173/300\n",
      "Average training loss: 0.031521061922113104\n",
      "Average test loss: 0.0017013835277822282\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03155326266421212\n",
      "Average test loss: 0.002077451391456028\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0314374445743031\n",
      "Average test loss: 0.0019268653243780136\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03145567103723685\n",
      "Average test loss: 0.001714566946340104\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03156104194952382\n",
      "Average test loss: 0.001679711896719204\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03163166794843144\n",
      "Average test loss: 0.0017678072361482515\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03156148247917493\n",
      "Average test loss: 0.002252910165944033\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03149525730146302\n",
      "Average test loss: 0.00335713719493813\n",
      "Epoch 181/300\n",
      "Average training loss: 0.031510579097602104\n",
      "Average test loss: 0.002020016754563484\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03133297937115034\n",
      "Average test loss: 0.0020129027095519836\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03137376890248723\n",
      "Average test loss: 0.0020178009368375773\n",
      "Epoch 184/300\n",
      "Average training loss: 0.031513506768478285\n",
      "Average test loss: 0.001874649087070591\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0312965919441647\n",
      "Average test loss: 0.0017431968932764398\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03135863701502482\n",
      "Average test loss: 0.005810626783304744\n",
      "Epoch 187/300\n",
      "Average training loss: 0.031324897575709554\n",
      "Average test loss: 0.001710085286344919\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03139534237980843\n",
      "Average test loss: 0.0016659849219852023\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03146455800036589\n",
      "Average test loss: 165356.23582291667\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03143801483843062\n",
      "Average test loss: 0.0017333723150918052\n",
      "Epoch 191/300\n",
      "Average training loss: 0.031153097566631104\n",
      "Average test loss: 0.0017548699607141316\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0312874725047085\n",
      "Average test loss: 0.0018026334515048397\n",
      "Epoch 193/300\n",
      "Average training loss: 0.033570814164148435\n",
      "Average test loss: 0.0017566462006005975\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03186822094519933\n",
      "Average test loss: 0.0019427205073750682\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031128697602285278\n",
      "Average test loss: 0.0017048190848694908\n",
      "Epoch 196/300\n",
      "Average training loss: 0.031110964169104895\n",
      "Average test loss: 0.3661841591000557\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03107220563458072\n",
      "Average test loss: 0.0017181841190904378\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031216160111957127\n",
      "Average test loss: 0.0017399840412868394\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03126566105749872\n",
      "Average test loss: 0.0024116782198349636\n",
      "Epoch 200/300\n",
      "Average training loss: 0.031263387916816605\n",
      "Average test loss: 0.0018108514778109061\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03114733816848861\n",
      "Average test loss: 0.001868580493558612\n",
      "Epoch 202/300\n",
      "Average training loss: 0.031087748338778812\n",
      "Average test loss: 0.001840532080580791\n",
      "Epoch 203/300\n",
      "Average training loss: 0.031081578135490416\n",
      "Average test loss: 0.001949528791424301\n",
      "Epoch 204/300\n",
      "Average training loss: 0.031014472979638313\n",
      "Average test loss: 0.0017119709768642982\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03116764239801301\n",
      "Average test loss: 0.001848060861436857\n",
      "Epoch 206/300\n",
      "Average training loss: 0.031101217567920683\n",
      "Average test loss: 0.0018750658747222688\n",
      "Epoch 207/300\n",
      "Average training loss: 0.031183391941918267\n",
      "Average test loss: 0.0017549578683554298\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03121870459450616\n",
      "Average test loss: 0.0017045761104673147\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03103890531592899\n",
      "Average test loss: 0.0018610033678511779\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03098872669537862\n",
      "Average test loss: 0.0017577902287658719\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03102042940755685\n",
      "Average test loss: 0.0017660544826131727\n",
      "Epoch 212/300\n",
      "Average training loss: 0.031067138188415104\n",
      "Average test loss: 0.0021968831875258023\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03125250361031956\n",
      "Average test loss: 0.001926288070364131\n",
      "Epoch 214/300\n",
      "Average training loss: 0.030938962143328454\n",
      "Average test loss: 0.0017547915275726053\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03096289253897137\n",
      "Average test loss: 0.0017437024692901306\n",
      "Epoch 216/300\n",
      "Average training loss: 0.031046286799841456\n",
      "Average test loss: 0.0017135055026867325\n",
      "Epoch 217/300\n",
      "Average training loss: 0.031034494249357118\n",
      "Average test loss: 0.0017834051573235128\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030895935828487077\n",
      "Average test loss: 0.001770892209890816\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030886513138810794\n",
      "Average test loss: 0.0018669914967483945\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03080936383207639\n",
      "Average test loss: 0.0016961870066200693\n",
      "Epoch 221/300\n",
      "Average training loss: 0.030887448456552294\n",
      "Average test loss: 0.001771761154031588\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030802185631460612\n",
      "Average test loss: 0.0018676121760573651\n",
      "Epoch 223/300\n",
      "Average training loss: 0.031031654071476726\n",
      "Average test loss: 0.0017507905246069034\n",
      "Epoch 224/300\n",
      "Average training loss: 0.030775802908672226\n",
      "Average test loss: 0.002355383413947291\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030848240885469648\n",
      "Average test loss: 0.001876801288479732\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03105745823515786\n",
      "Average test loss: 0.00171499133337703\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030834463203946748\n",
      "Average test loss: 0.001709582627647453\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03072133297059271\n",
      "Average test loss: 0.0017644146213101016\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030749935385253693\n",
      "Average test loss: 0.0017293572951522138\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03080179821451505\n",
      "Average test loss: 0.0020813240019811525\n",
      "Epoch 231/300\n",
      "Average training loss: 0.031117497165997824\n",
      "Average test loss: 0.0017067308543870846\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03080512437555525\n",
      "Average test loss: 0.001735237330198288\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030694492454330126\n",
      "Average test loss: 0.0017366587688318557\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03076534067094326\n",
      "Average test loss: 0.0016791721511011323\n",
      "Epoch 235/300\n",
      "Average training loss: 0.030778476529651218\n",
      "Average test loss: 0.0017336954210574428\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030705265704128477\n",
      "Average test loss: 0.0017350341154572864\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030614599471290905\n",
      "Average test loss: 0.004012840640627675\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0306620175672902\n",
      "Average test loss: 0.0018280989041344986\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030695896339085368\n",
      "Average test loss: 0.0036463218902548153\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03075243472225136\n",
      "Average test loss: 0.001864025816735294\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03074312592215008\n",
      "Average test loss: 0.0017110212823996942\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030826936427089904\n",
      "Average test loss: 0.0017116228236506382\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0308852087540759\n",
      "Average test loss: 0.0017880494816021786\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03065899441970719\n",
      "Average test loss: 0.0018459061593231229\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03094095449315177\n",
      "Average test loss: 0.0017804893609136344\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030501303816835086\n",
      "Average test loss: 0.0017601695642289188\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03050821322368251\n",
      "Average test loss: 0.0017262128557388981\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03066474611726072\n",
      "Average test loss: 0.0017282204892900255\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03056291849248939\n",
      "Average test loss: 0.0018373488241599666\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030547045926253\n",
      "Average test loss: 0.0017300103018060326\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030551116756267017\n",
      "Average test loss: 0.0016918689790699217\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030825381929675738\n",
      "Average test loss: 0.0017423769967216585\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030485013014740416\n",
      "Average test loss: 0.0017172501664091315\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03053436369366116\n",
      "Average test loss: 0.0017410219042665429\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030397858048478762\n",
      "Average test loss: 0.001826464050863352\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03043106769356463\n",
      "Average test loss: 0.0017525976890077194\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030504861964119806\n",
      "Average test loss: 0.0018761644127468268\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03058713292247719\n",
      "Average test loss: 0.003120354847775565\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030428445746501286\n",
      "Average test loss: 0.0017283815226207177\n",
      "Epoch 260/300\n",
      "Average training loss: 0.030583848112159306\n",
      "Average test loss: 0.0017905016696701448\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030399789152873887\n",
      "Average test loss: 0.001711710292639004\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030447630860739285\n",
      "Average test loss: 0.0024448343263939023\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030435592631498972\n",
      "Average test loss: 0.002036866835421986\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03042113249666161\n",
      "Average test loss: 0.0017627862049266695\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030474258283774057\n",
      "Average test loss: 0.0017553006167420082\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030389876504739125\n",
      "Average test loss: 0.0016921755931236678\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030391799204879336\n",
      "Average test loss: 0.0017638651423363223\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030434095919132233\n",
      "Average test loss: 0.0017973202113062143\n",
      "Epoch 269/300\n",
      "Average training loss: 0.030412411047352685\n",
      "Average test loss: 0.0018539488180540503\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030532193389203814\n",
      "Average test loss: 0.0022353320382535458\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030408054053783416\n",
      "Average test loss: 0.0016940474354972441\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03061351225939062\n",
      "Average test loss: 0.0019454813000435631\n",
      "Epoch 273/300\n",
      "Average training loss: 0.030266124793224866\n",
      "Average test loss: 0.00879772488731477\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03041043761538135\n",
      "Average test loss: 0.002510826336219907\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03033277568552229\n",
      "Average test loss: 0.0019129920812944571\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03030978383951717\n",
      "Average test loss: 0.001788372865671085\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030330621575315794\n",
      "Average test loss: 0.0018140473142266274\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03031500694817967\n",
      "Average test loss: 0.0018255440319577853\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03026844857798682\n",
      "Average test loss: 0.018826882948891985\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030293567958805296\n",
      "Average test loss: 0.0017489268777685033\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03043061608241664\n",
      "Average test loss: 0.00174311617270319\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03020483172767692\n",
      "Average test loss: 0.0024892959807895952\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030419896811246874\n",
      "Average test loss: 0.0017819938339913885\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03038108952343464\n",
      "Average test loss: 0.0017140462808310985\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030425765845510693\n",
      "Average test loss: 0.0019065735870972276\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030190599703126483\n",
      "Average test loss: 0.001769707668779625\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03014015985528628\n",
      "Average test loss: 0.001739070505886856\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030150128382775518\n",
      "Average test loss: 0.0017858485649857256\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030219747086366017\n",
      "Average test loss: 0.0018312538138901195\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03013642051319281\n",
      "Average test loss: 0.0018045909746239584\n",
      "Epoch 291/300\n",
      "Average training loss: 0.030207142988840738\n",
      "Average test loss: 0.001819459127676156\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03020330213341448\n",
      "Average test loss: 0.0018977828429390987\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03046628768576516\n",
      "Average test loss: 0.0017276561950436897\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030163185056712893\n",
      "Average test loss: 0.0018085010066214535\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03045002965297964\n",
      "Average test loss: 0.0017182837659493089\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030162341293361452\n",
      "Average test loss: 0.0018189110466175608\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03139618282847934\n",
      "Average test loss: 0.00420430568108956\n",
      "Epoch 298/300\n",
      "Average training loss: 0.030089907391203773\n",
      "Average test loss: 0.0017339636920951306\n",
      "Epoch 299/300\n",
      "Average training loss: 0.030045122030708526\n",
      "Average test loss: 0.002168957513136168\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03013292103012403\n",
      "Average test loss: 0.0021897877390599915\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.7550982376734416\n",
      "Average test loss: 0.004931800639877717\n",
      "Epoch 2/300\n",
      "Average training loss: 0.46611174816555445\n",
      "Average test loss: 0.003889275958761573\n",
      "Epoch 3/300\n",
      "Average training loss: 0.30473378478156193\n",
      "Average test loss: 0.0032529945669488775\n",
      "Epoch 4/300\n",
      "Average training loss: 0.22596566332711113\n",
      "Average test loss: 0.003069456953141424\n",
      "Epoch 5/300\n",
      "Average training loss: 0.17883912105030483\n",
      "Average test loss: 0.003072461941589912\n",
      "Epoch 6/300\n",
      "Average training loss: 0.14747609463002948\n",
      "Average test loss: 0.002870584496607383\n",
      "Epoch 7/300\n",
      "Average training loss: 0.12665597152047686\n",
      "Average test loss: 0.003696482690465119\n",
      "Epoch 8/300\n",
      "Average training loss: 0.11179365437560611\n",
      "Average test loss: 0.0024888373404327365\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10095226705736585\n",
      "Average test loss: 0.0031114018288337522\n",
      "Epoch 10/300\n",
      "Average training loss: 0.09271919721364975\n",
      "Average test loss: 0.0033576801715211736\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08576758464177449\n",
      "Average test loss: 0.0022752123777237203\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0799718022412724\n",
      "Average test loss: 0.0022530304986155695\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07510929518275791\n",
      "Average test loss: 0.0025460600714302725\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07099360283546978\n",
      "Average test loss: 0.0029750422733939358\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06719436211718453\n",
      "Average test loss: 0.0020642957831215527\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06418897754947345\n",
      "Average test loss: 0.0026240870009900793\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06088178351190355\n",
      "Average test loss: 0.0024053849863509336\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05894354091087977\n",
      "Average test loss: 0.0030501467208895417\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05609307828214433\n",
      "Average test loss: 0.002329201551982098\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05372990306549602\n",
      "Average test loss: 0.00218248677201983\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05196613408790694\n",
      "Average test loss: 0.0019917653051929342\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05008687976002693\n",
      "Average test loss: 0.0018194761775020096\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04796975165274408\n",
      "Average test loss: 0.045094166244069735\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04619940100113551\n",
      "Average test loss: 0.0016506287339660857\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04457535607947244\n",
      "Average test loss: 0.001563655694636206\n",
      "Epoch 26/300\n",
      "Average training loss: 0.043000744640827177\n",
      "Average test loss: 0.0035764877700971234\n",
      "Epoch 27/300\n",
      "Average training loss: 0.041699294643269644\n",
      "Average test loss: 0.0016122352091802491\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04053292349974314\n",
      "Average test loss: 0.0015108830595595968\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039156171960963146\n",
      "Average test loss: 0.001478208318249219\n",
      "Epoch 30/300\n",
      "Average training loss: 0.038114599267641706\n",
      "Average test loss: 0.0014073050732000007\n",
      "Epoch 31/300\n",
      "Average training loss: 0.037311064491669335\n",
      "Average test loss: 0.001400357646143271\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03642772533496221\n",
      "Average test loss: 0.0014339645078612698\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0356738886071576\n",
      "Average test loss: 0.0014209273110868201\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03479374502764808\n",
      "Average test loss: 0.0013941525125669109\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03430769036710262\n",
      "Average test loss: 0.0020244726438282267\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0337025964508454\n",
      "Average test loss: 0.0012890324517049723\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033060490207539664\n",
      "Average test loss: 0.0013599807077811824\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03275054669711325\n",
      "Average test loss: 0.0012765663078882629\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03217231178780397\n",
      "Average test loss: 0.0013718642050193415\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03195407764779197\n",
      "Average test loss: 0.0016772690769284965\n",
      "Epoch 41/300\n",
      "Average training loss: 0.031224274221393795\n",
      "Average test loss: 0.0012191719983497427\n",
      "Epoch 42/300\n",
      "Average training loss: 0.030848411467340256\n",
      "Average test loss: 0.0012418729015108612\n",
      "Epoch 43/300\n",
      "Average training loss: 0.030727178004052905\n",
      "Average test loss: 0.0012253974195466273\n",
      "Epoch 44/300\n",
      "Average training loss: 0.030299358987145954\n",
      "Average test loss: 0.001226048808855315\n",
      "Epoch 45/300\n",
      "Average training loss: 0.029997761420077748\n",
      "Average test loss: 0.0012595439644323455\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03009515896770689\n",
      "Average test loss: 0.0012343065784209304\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02954311090045505\n",
      "Average test loss: 0.0011901402626600531\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02937413667804665\n",
      "Average test loss: 0.0012424482081809806\n",
      "Epoch 49/300\n",
      "Average training loss: 0.029544399484164183\n",
      "Average test loss: 0.0020094347746732336\n",
      "Epoch 50/300\n",
      "Average training loss: 0.028989331697424254\n",
      "Average test loss: 0.001260073458775878\n",
      "Epoch 51/300\n",
      "Average training loss: 0.028821790332595507\n",
      "Average test loss: 0.0012893253062955207\n",
      "Epoch 52/300\n",
      "Average training loss: 0.028663290898005168\n",
      "Average test loss: 0.001165863439026806\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02845001556310389\n",
      "Average test loss: 0.0013551171818334195\n",
      "Epoch 54/300\n",
      "Average training loss: 0.028328817433781094\n",
      "Average test loss: 0.0011614049424727758\n",
      "Epoch 55/300\n",
      "Average training loss: 0.028822613875071207\n",
      "Average test loss: 0.0011893848638153738\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02806043486131562\n",
      "Average test loss: 0.0012786886791388194\n",
      "Epoch 57/300\n",
      "Average training loss: 0.027933780352274576\n",
      "Average test loss: 0.0015002336852873365\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02795644851028919\n",
      "Average test loss: 0.001132495901029971\n",
      "Epoch 59/300\n",
      "Average training loss: 0.027703939684563212\n",
      "Average test loss: 0.0011388191940883796\n",
      "Epoch 60/300\n",
      "Average training loss: 0.027603114075130886\n",
      "Average test loss: 0.0011367184860217902\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02757688292860985\n",
      "Average test loss: 0.0011383338131838374\n",
      "Epoch 62/300\n",
      "Average training loss: 0.027569873231980537\n",
      "Average test loss: 0.0014639488849271503\n",
      "Epoch 63/300\n",
      "Average training loss: 0.027447449068228404\n",
      "Average test loss: 0.0011069681847261058\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02821145418120755\n",
      "Average test loss: 0.0011864300877269772\n",
      "Epoch 65/300\n",
      "Average training loss: 0.028019591139422522\n",
      "Average test loss: 0.001120569987119072\n",
      "Epoch 66/300\n",
      "Average training loss: 0.027245751781596078\n",
      "Average test loss: 0.001107811098607878\n",
      "Epoch 67/300\n",
      "Average training loss: 0.027073493116431768\n",
      "Average test loss: 0.001628373361296124\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027067184458176297\n",
      "Average test loss: 0.0010999580044299365\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02695651419626342\n",
      "Average test loss: 0.0011631350124047862\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02708082825442155\n",
      "Average test loss: 0.001286385608630048\n",
      "Epoch 71/300\n",
      "Average training loss: 0.026974760106868213\n",
      "Average test loss: 0.00118484684628331\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02680839828153451\n",
      "Average test loss: 0.0011144172978690928\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02708830859926012\n",
      "Average test loss: 0.0011340255378228094\n",
      "Epoch 74/300\n",
      "Average training loss: 0.026631843608286646\n",
      "Average test loss: 0.0011042440605039399\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02659231122997072\n",
      "Average test loss: 0.0038163508942557706\n",
      "Epoch 76/300\n",
      "Average training loss: 0.026681667387485505\n",
      "Average test loss: 0.0013322677315316266\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02780093701183796\n",
      "Average test loss: 0.007293784590231047\n",
      "Epoch 78/300\n",
      "Average training loss: 0.026485937876833808\n",
      "Average test loss: 0.001373644807169007\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02639986740383837\n",
      "Average test loss: 0.00110626375889923\n",
      "Epoch 80/300\n",
      "Average training loss: 0.026335427973005508\n",
      "Average test loss: 0.0022454480222529835\n",
      "Epoch 81/300\n",
      "Average training loss: 0.026475016310811044\n",
      "Average test loss: 0.001133708728580839\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026271412384178905\n",
      "Average test loss: 0.0010887715668520994\n",
      "Epoch 83/300\n",
      "Average training loss: 0.026262619247039157\n",
      "Average test loss: 0.0012663712930969066\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02620002142422729\n",
      "Average test loss: 0.0011016755819113718\n",
      "Epoch 85/300\n",
      "Average training loss: 0.026123913044730822\n",
      "Average test loss: 0.0013121861944285532\n",
      "Epoch 86/300\n",
      "Average training loss: 0.026435307428240774\n",
      "Average test loss: 0.008609758131206036\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02749231125911077\n",
      "Average test loss: 0.0022357421894040374\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0261053402705325\n",
      "Average test loss: 0.003480923163290653\n",
      "Epoch 89/300\n",
      "Average training loss: 0.025967688204513655\n",
      "Average test loss: 0.0011056118339507117\n",
      "Epoch 90/300\n",
      "Average training loss: 0.025930320284432835\n",
      "Average test loss: 0.0011405871940983667\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0260259893718693\n",
      "Average test loss: 0.001106290720610155\n",
      "Epoch 92/300\n",
      "Average training loss: 0.025971596754259533\n",
      "Average test loss: 0.0010882313998105625\n",
      "Epoch 93/300\n",
      "Average training loss: 0.026066595155331825\n",
      "Average test loss: 0.00115696998913255\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025772854765256245\n",
      "Average test loss: 0.0012881589906497135\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02572511389520433\n",
      "Average test loss: 0.0011923166171440647\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025776494551863937\n",
      "Average test loss: 0.002482489587428669\n",
      "Epoch 97/300\n",
      "Average training loss: 0.025760429920421705\n",
      "Average test loss: 0.0011086628606749905\n",
      "Epoch 98/300\n",
      "Average training loss: 0.025652570439709556\n",
      "Average test loss: 0.001084271866724723\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02599337616066138\n",
      "Average test loss: 5423.979965684679\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0256514845771922\n",
      "Average test loss: 0.0010998823130503297\n",
      "Epoch 101/300\n",
      "Average training loss: 0.025545817171533904\n",
      "Average test loss: 0.001123882509664529\n",
      "Epoch 102/300\n",
      "Average training loss: 0.025997634087999662\n",
      "Average test loss: 0.001106740429169602\n",
      "Epoch 103/300\n",
      "Average training loss: 0.025442962826953994\n",
      "Average test loss: 0.0010808704551826748\n",
      "Epoch 104/300\n",
      "Average training loss: 0.025507106620404455\n",
      "Average test loss: 0.0011317144506093528\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02546683582663536\n",
      "Average test loss: 0.0011255308826350504\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0253587173488405\n",
      "Average test loss: 0.0010996836751906408\n",
      "Epoch 107/300\n",
      "Average training loss: 0.025389790720409817\n",
      "Average test loss: 0.00119539276851962\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02549402424361971\n",
      "Average test loss: 0.001630780052083234\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02525899585833152\n",
      "Average test loss: 0.0011851123245432973\n",
      "Epoch 110/300\n",
      "Average training loss: 0.025370341122978264\n",
      "Average test loss: 0.001155707817317711\n",
      "Epoch 111/300\n",
      "Average training loss: 0.025401784933275648\n",
      "Average test loss: 0.0011845764897556769\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02520736970918046\n",
      "Average test loss: 0.0011331094758999017\n",
      "Epoch 113/300\n",
      "Average training loss: 0.025243758526113297\n",
      "Average test loss: 0.0014197665687857403\n",
      "Epoch 114/300\n",
      "Average training loss: 0.025079915311601426\n",
      "Average test loss: 0.001083347987021423\n",
      "Epoch 115/300\n",
      "Average training loss: 0.025236356129248936\n",
      "Average test loss: 0.00112030176523452\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025176838093333775\n",
      "Average test loss: 0.0011532454836285777\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02528916031618913\n",
      "Average test loss: 0.0011249540424388316\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025042721312907006\n",
      "Average test loss: 0.003970032001535098\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02502659230430921\n",
      "Average test loss: 0.0011221928395227425\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02501600864198473\n",
      "Average test loss: 0.0011993037048313353\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02548087879518668\n",
      "Average test loss: 0.0012504788890170555\n",
      "Epoch 122/300\n",
      "Average training loss: 0.024935181494388316\n",
      "Average test loss: 0.0011123416741482086\n",
      "Epoch 123/300\n",
      "Average training loss: 0.024972035807040004\n",
      "Average test loss: 0.0010856511132895119\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024891023175583945\n",
      "Average test loss: 0.0013451767744910386\n",
      "Epoch 125/300\n",
      "Average training loss: 0.024849607537190118\n",
      "Average test loss: 0.002032872467612227\n",
      "Epoch 126/300\n",
      "Average training loss: 0.024969530809256766\n",
      "Average test loss: 0.0011527115458415615\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024962949762741726\n",
      "Average test loss: 0.001236522888712999\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02488066164818075\n",
      "Average test loss: 0.0011338328250373403\n",
      "Epoch 129/300\n",
      "Average training loss: 0.024836020135217244\n",
      "Average test loss: 0.0011910496569342084\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024739570564693874\n",
      "Average test loss: 0.0010989343770262268\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024877175367540784\n",
      "Average test loss: 0.0011258418679030405\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024740313917398452\n",
      "Average test loss: 0.0011305212657898665\n",
      "Epoch 133/300\n",
      "Average training loss: 0.024687766111559338\n",
      "Average test loss: 0.0011668576581610574\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02477706085311042\n",
      "Average test loss: 0.0011221214673067961\n",
      "Epoch 135/300\n",
      "Average training loss: 0.024768585872319008\n",
      "Average test loss: 0.0014519381294440892\n",
      "Epoch 136/300\n",
      "Average training loss: 0.024769337839550442\n",
      "Average test loss: 0.0017686369862510926\n",
      "Epoch 137/300\n",
      "Average training loss: 0.024785270864764848\n",
      "Average test loss: 0.006667378338157303\n",
      "Epoch 138/300\n",
      "Average training loss: 0.024534249901771544\n",
      "Average test loss: 0.001160028875184556\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024529985237452718\n",
      "Average test loss: 0.0011011562960532805\n",
      "Epoch 140/300\n",
      "Average training loss: 0.024618668970134525\n",
      "Average test loss: 0.0010984321840935283\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025079943254590033\n",
      "Average test loss: 0.0015366561522500383\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024524715019596948\n",
      "Average test loss: 0.0011100027098113463\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02446040865116649\n",
      "Average test loss: 0.001157622515430881\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024554106613000233\n",
      "Average test loss: 0.0010947522453756797\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0247164053933488\n",
      "Average test loss: 0.0011079754726443854\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024397041007876395\n",
      "Average test loss: 0.0011088712790256574\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02441731999317805\n",
      "Average test loss: 0.0010990533275633222\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024339854809972975\n",
      "Average test loss: 0.0011767163562277953\n",
      "Epoch 149/300\n",
      "Average training loss: 0.024338924535446695\n",
      "Average test loss: 0.0015717725834498803\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024462282557454373\n",
      "Average test loss: 0.001146014154319548\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024563751897878117\n",
      "Average test loss: 0.002954607831935088\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024372433044844204\n",
      "Average test loss: 0.0012690062960609794\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024493208332194222\n",
      "Average test loss: 0.011142693475095762\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024388621845179134\n",
      "Average test loss: 0.0017843191564703982\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024328739343418015\n",
      "Average test loss: 0.0013963287850427958\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02420657162533866\n",
      "Average test loss: 0.0012710990184504126\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024270324392451182\n",
      "Average test loss: 0.0011887908054050058\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02440443503856659\n",
      "Average test loss: 0.0011259090246425734\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024326673350400396\n",
      "Average test loss: 0.0011124412661107877\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02418785169389513\n",
      "Average test loss: 0.0012344980971473786\n",
      "Epoch 161/300\n",
      "Average training loss: 0.024197218620114855\n",
      "Average test loss: 0.0011167925025026004\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02419506899846925\n",
      "Average test loss: 0.001119316841630886\n",
      "Epoch 163/300\n",
      "Average training loss: 0.024182880179749594\n",
      "Average test loss: 3025.2752679036457\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02471606507235103\n",
      "Average test loss: 0.0011447113363279237\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024103260427713394\n",
      "Average test loss: 0.0011163332480937243\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024010714350475207\n",
      "Average test loss: 0.0011300194321407211\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02406286359992292\n",
      "Average test loss: 0.0024224283585531844\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024018864752517806\n",
      "Average test loss: 0.0011563229544295204\n",
      "Epoch 169/300\n",
      "Average training loss: 0.024164778555432954\n",
      "Average test loss: 0.0011440031212858028\n",
      "Epoch 170/300\n",
      "Average training loss: 0.024144811261031362\n",
      "Average test loss: 0.001629677237321933\n",
      "Epoch 171/300\n",
      "Average training loss: 0.024002386561698383\n",
      "Average test loss: 0.0012929523819022709\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02407763454483615\n",
      "Average test loss: 0.0014372060522437095\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024048140926493537\n",
      "Average test loss: 0.0011386694601840442\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02413783113658428\n",
      "Average test loss: 0.0023702901717689304\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02414045910537243\n",
      "Average test loss: 0.0012278417656198144\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02392501431869136\n",
      "Average test loss: 0.001132671379018575\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023918345905012555\n",
      "Average test loss: 0.0012245291644810803\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023984966600106822\n",
      "Average test loss: 0.001153375468061616\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02407854390144348\n",
      "Average test loss: 0.001127085019265198\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023852282034026253\n",
      "Average test loss: 0.002247866815576951\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024045217545496093\n",
      "Average test loss: 0.014033250158031781\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023881726074549886\n",
      "Average test loss: 0.0013977717147726151\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02487670917974578\n",
      "Average test loss: 0.0012757055059903198\n",
      "Epoch 184/300\n",
      "Average training loss: 0.024124603284729852\n",
      "Average test loss: 0.001141296890926444\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023735968030161328\n",
      "Average test loss: 0.0011725197110532058\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023826340863274204\n",
      "Average test loss: 0.0011784966860690878\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02400735671321551\n",
      "Average test loss: 0.0014541428319902884\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02375113153292073\n",
      "Average test loss: 0.0011321971393707727\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023930904158287577\n",
      "Average test loss: 0.001132327810343769\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023779404236210718\n",
      "Average test loss: 0.0011536156396485036\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023749110581146347\n",
      "Average test loss: 0.001202435302755071\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023811896580788826\n",
      "Average test loss: 0.008045935300489266\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023830108627676965\n",
      "Average test loss: 0.0013457411424153381\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023645158481266762\n",
      "Average test loss: 0.0011478370871498354\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02375859961907069\n",
      "Average test loss: 0.0013088709995047086\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023799035881956417\n",
      "Average test loss: 0.0012447396571644478\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02380709982580609\n",
      "Average test loss: 0.0021078060323165525\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02384213811159134\n",
      "Average test loss: 0.0011610844598876106\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023754477437999515\n",
      "Average test loss: 0.0027015762699560985\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023619039883216224\n",
      "Average test loss: 0.001157796662300825\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023744448214769363\n",
      "Average test loss: 0.0013609474766999483\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02364623427391052\n",
      "Average test loss: 0.0011467720427446894\n",
      "Epoch 203/300\n",
      "Average training loss: 0.023643559162815412\n",
      "Average test loss: 0.007348958255102237\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02377395627564854\n",
      "Average test loss: 0.0011393836413820585\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02370735684865051\n",
      "Average test loss: 0.0011449813553028637\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023862944742043812\n",
      "Average test loss: 0.0018721216399636533\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023820168725318377\n",
      "Average test loss: 0.001177332524727616\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02353013222747379\n",
      "Average test loss: 0.00146165703697544\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02350664789146847\n",
      "Average test loss: 0.004258619487182134\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023523235461778112\n",
      "Average test loss: 0.0011289843663366305\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02360805904534128\n",
      "Average test loss: 0.0017160802313850986\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023747961703273986\n",
      "Average test loss: 0.0012623999310243461\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023665399273236593\n",
      "Average test loss: 0.001160574969318178\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023629916267262564\n",
      "Average test loss: 0.0011746579754269785\n",
      "Epoch 215/300\n",
      "Average training loss: 0.023487196755078103\n",
      "Average test loss: 0.0011390166147094633\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023570739588803714\n",
      "Average test loss: 0.0012649533538044327\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023459823669658767\n",
      "Average test loss: 0.0013168385845298569\n",
      "Epoch 218/300\n",
      "Average training loss: 0.023478995230462815\n",
      "Average test loss: 0.0013098488301038743\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02382825226088365\n",
      "Average test loss: 0.004156557429995802\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02342852211329672\n",
      "Average test loss: 0.001117789584522446\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023453572019934654\n",
      "Average test loss: 0.0012541185177655683\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023459517593185108\n",
      "Average test loss: 0.001399396693540944\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023400510258144804\n",
      "Average test loss: 0.002062518201768398\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023440792945524057\n",
      "Average test loss: 0.0011859791931799716\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023417571374111705\n",
      "Average test loss: 0.0012075834633368584\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023607668606771364\n",
      "Average test loss: 0.0011899257455435064\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02350264939169089\n",
      "Average test loss: 0.0011454030593029327\n",
      "Epoch 228/300\n",
      "Average training loss: 0.023350187885264554\n",
      "Average test loss: 0.0011824199571274221\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023571825868553584\n",
      "Average test loss: 0.0011836894225950043\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023333253102170097\n",
      "Average test loss: 0.001238548772306078\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02332029309372107\n",
      "Average test loss: 0.001270509677990857\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02336672239171134\n",
      "Average test loss: 0.001222118027197818\n",
      "Epoch 233/300\n",
      "Average training loss: 0.028093669361538356\n",
      "Average test loss: 0.0011829683220324417\n",
      "Epoch 234/300\n",
      "Average training loss: 0.024541083719995287\n",
      "Average test loss: 0.0014225737957490815\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023549208129445713\n",
      "Average test loss: 0.002150238906343778\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023476775925192567\n",
      "Average test loss: 0.0012468820847053494\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023215646982192993\n",
      "Average test loss: 0.001150236140884873\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02350028812719716\n",
      "Average test loss: 0.0015778444503537483\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023279829538530774\n",
      "Average test loss: 0.0011500946760384574\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02332450847989983\n",
      "Average test loss: 0.001167486732929117\n",
      "Epoch 241/300\n",
      "Average training loss: 0.023490208158890406\n",
      "Average test loss: 0.00120768506380005\n",
      "Epoch 242/300\n",
      "Average training loss: 0.023293257607354057\n",
      "Average test loss: 0.0013124903398565948\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023278565052482818\n",
      "Average test loss: 0.0011767959014202158\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023253827300336625\n",
      "Average test loss: 0.0011733480311102338\n",
      "Epoch 245/300\n",
      "Average training loss: 0.023407667875289917\n",
      "Average test loss: 0.00641373203901781\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02334454843070772\n",
      "Average test loss: 0.0011685282289981842\n",
      "Epoch 247/300\n",
      "Average training loss: 0.023341487902734014\n",
      "Average test loss: 0.001237320111133158\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023444785159495142\n",
      "Average test loss: 0.0011913175613929828\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02328123838371701\n",
      "Average test loss: 0.0011516917964650526\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02322715400159359\n",
      "Average test loss: 0.0011837449688464404\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02320549413892958\n",
      "Average test loss: 0.001168619696671764\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0232352691160308\n",
      "Average test loss: 0.001132586380518559\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023202859373556244\n",
      "Average test loss: 0.0011418264537221856\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02328177152077357\n",
      "Average test loss: 0.0011505605287642942\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023565652097264925\n",
      "Average test loss: 0.001171038575987849\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023107560791903074\n",
      "Average test loss: 0.0018654586851286391\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02322832688358095\n",
      "Average test loss: 0.0011875418945112163\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02340930996172958\n",
      "Average test loss: 0.006431405085656378\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02364483205642965\n",
      "Average test loss: 0.0011589296902012494\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02311819126043055\n",
      "Average test loss: 0.0011727014713817172\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02307696227232615\n",
      "Average test loss: 0.0012322747527311246\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02370883323583338\n",
      "Average test loss: 0.001177195079314212\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023068879337774382\n",
      "Average test loss: 0.0011615159594350391\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023056002797351944\n",
      "Average test loss: 0.0011668501266588768\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023145944084558223\n",
      "Average test loss: 0.001192052750563663\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023116048994991513\n",
      "Average test loss: 0.0012458984264069133\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023140961514578926\n",
      "Average test loss: 0.0011883700530355176\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0231735436088509\n",
      "Average test loss: 0.0019042835659864876\n",
      "Epoch 269/300\n",
      "Average training loss: 0.023149029402269258\n",
      "Average test loss: 0.0011778387613594532\n",
      "Epoch 270/300\n",
      "Average training loss: 0.023002073710163433\n",
      "Average test loss: 0.001165832874416891\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02328242597480615\n",
      "Average test loss: 0.0012764565353799197\n",
      "Epoch 272/300\n",
      "Average training loss: 0.023018015099896327\n",
      "Average test loss: 0.0012036170478289326\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02310201748377747\n",
      "Average test loss: 0.0012339447079640295\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023534561238355105\n",
      "Average test loss: 0.014931860003620387\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02301788648135132\n",
      "Average test loss: 0.001182219295348558\n",
      "Epoch 276/300\n",
      "Average training loss: 0.023033595947755708\n",
      "Average test loss: 0.0011562443214158218\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023124065862761604\n",
      "Average test loss: 0.0011611016899761226\n",
      "Epoch 278/300\n",
      "Average training loss: 0.023186338250835735\n",
      "Average test loss: 0.0011773281964576907\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022996227863762115\n",
      "Average test loss: 215.8801693793403\n",
      "Epoch 280/300\n",
      "Average training loss: 0.023098065349790784\n",
      "Average test loss: 0.0011584538394688731\n",
      "Epoch 281/300\n",
      "Average training loss: 0.023038521928919687\n",
      "Average test loss: 0.0011410959520273738\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023028931905825934\n",
      "Average test loss: 0.0011559623904629714\n",
      "Epoch 283/300\n",
      "Average training loss: 0.023048795537816153\n",
      "Average test loss: 0.0018202852743367354\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022975072859889933\n",
      "Average test loss: 0.0011800930762870445\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02299089240034421\n",
      "Average test loss: 0.0015483295472028355\n",
      "Epoch 286/300\n",
      "Average training loss: 0.023129757958981725\n",
      "Average test loss: 0.0037228431585762238\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02296665437685119\n",
      "Average test loss: 0.001197843643836677\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02299710945950614\n",
      "Average test loss: 0.0016035842357410325\n",
      "Epoch 289/300\n",
      "Average training loss: 0.023025249787502818\n",
      "Average test loss: 0.0011575926390166083\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022939650221003426\n",
      "Average test loss: 0.001210522391522924\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02299444022277991\n",
      "Average test loss: 0.0012043355593664778\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022985125914216042\n",
      "Average test loss: 0.00149343026522547\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02295845372643736\n",
      "Average test loss: 0.0012347713880137437\n",
      "Epoch 294/300\n",
      "Average training loss: 0.023010483413934708\n",
      "Average test loss: 0.0013495492095955545\n",
      "Epoch 295/300\n",
      "Average training loss: 0.022848135099642806\n",
      "Average test loss: 0.0011662699037955867\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02293757714662287\n",
      "Average test loss: 0.0012027398540327945\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0229368807060851\n",
      "Average test loss: 0.001163968491109295\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022986346569326188\n",
      "Average test loss: 0.001142852205162247\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02310718676613437\n",
      "Average test loss: 0.0011841352903801533\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022837734853227935\n",
      "Average test loss: 0.0012196875079017546\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_64_Depth5/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.62\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.64\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.65\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.41\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.87\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.23\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.78\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.63\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.32\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.03\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.42\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.74\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.63\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.75\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.00\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.31\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.79\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.93\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.01\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.33\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.47\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.97\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 33.54\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.81\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 34.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 34.48\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 34.60\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 34.89\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.894894397735596\n",
      "Average test loss: 0.01205452611297369\n",
      "Epoch 2/300\n",
      "Average training loss: 6.3320272551642525\n",
      "Average test loss: 0.025632556497222847\n",
      "Epoch 3/300\n",
      "Average training loss: 5.607419024573432\n",
      "Average test loss: 0.008324858326878813\n",
      "Epoch 4/300\n",
      "Average training loss: 4.179052116394043\n",
      "Average test loss: 0.010815686288807127\n",
      "Epoch 5/300\n",
      "Average training loss: 3.0709195115831163\n",
      "Average test loss: 0.008234375040564273\n",
      "Epoch 6/300\n",
      "Average training loss: 2.522151819652981\n",
      "Average test loss: 0.008259956325921747\n",
      "Epoch 7/300\n",
      "Average training loss: 2.02619872601827\n",
      "Average test loss: 0.008655744524465667\n",
      "Epoch 8/300\n",
      "Average training loss: 1.6659804095162285\n",
      "Average test loss: 0.006770922361355689\n",
      "Epoch 9/300\n",
      "Average training loss: 1.3956462364196778\n",
      "Average test loss: 0.00844115963164303\n",
      "Epoch 10/300\n",
      "Average training loss: 1.144948079003228\n",
      "Average test loss: 0.0066401605581243835\n",
      "Epoch 11/300\n",
      "Average training loss: 0.9454603015051948\n",
      "Average test loss: 0.0065838109859161905\n",
      "Epoch 12/300\n",
      "Average training loss: 0.8064762061436971\n",
      "Average test loss: 0.0071245322624842325\n",
      "Epoch 13/300\n",
      "Average training loss: 0.7003241124153137\n",
      "Average test loss: 0.006612542594472567\n",
      "Epoch 14/300\n",
      "Average training loss: 0.6192054148250156\n",
      "Average test loss: 0.007164759865237607\n",
      "Epoch 15/300\n",
      "Average training loss: 0.5526190319061279\n",
      "Average test loss: 0.00641998492139909\n",
      "Epoch 16/300\n",
      "Average training loss: 0.500260128180186\n",
      "Average test loss: 0.008977413444883293\n",
      "Epoch 17/300\n",
      "Average training loss: 0.4551264648967319\n",
      "Average test loss: 0.005985675645371278\n",
      "Epoch 18/300\n",
      "Average training loss: 0.4204763744937049\n",
      "Average test loss: 0.00743987643553151\n",
      "Epoch 19/300\n",
      "Average training loss: 0.3909452744854821\n",
      "Average test loss: 0.012803015660908487\n",
      "Epoch 20/300\n",
      "Average training loss: 0.36371872231695385\n",
      "Average test loss: 0.005615441161311335\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3430225766234928\n",
      "Average test loss: 0.007452476880616612\n",
      "Epoch 22/300\n",
      "Average training loss: 0.32494592099719577\n",
      "Average test loss: 0.00569276251229975\n",
      "Epoch 23/300\n",
      "Average training loss: 0.3077393032444848\n",
      "Average test loss: 0.005396968095666832\n",
      "Epoch 24/300\n",
      "Average training loss: 0.29450990210639105\n",
      "Average test loss: 0.005283529434767034\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2825686021645864\n",
      "Average test loss: 0.00540471343571941\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2693013704882728\n",
      "Average test loss: 0.008049513765507273\n",
      "Epoch 27/300\n",
      "Average training loss: 0.26050815704133773\n",
      "Average test loss: 0.005734191074139542\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2507525978618198\n",
      "Average test loss: 0.005338478537276387\n",
      "Epoch 29/300\n",
      "Average training loss: 0.24100594867600336\n",
      "Average test loss: 0.005052987061854866\n",
      "Epoch 30/300\n",
      "Average training loss: 0.23436883914470671\n",
      "Average test loss: 0.005200075241426627\n",
      "Epoch 31/300\n",
      "Average training loss: 0.22870972678396437\n",
      "Average test loss: 0.00508230790330304\n",
      "Epoch 32/300\n",
      "Average training loss: 0.22345191319783528\n",
      "Average test loss: 0.004931070784313811\n",
      "Epoch 33/300\n",
      "Average training loss: 0.21742137738068898\n",
      "Average test loss: 0.01203990737348795\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2098843981160058\n",
      "Average test loss: 0.013888763653321399\n",
      "Epoch 35/300\n",
      "Average training loss: 0.20376568463113573\n",
      "Average test loss: 0.00488419353754984\n",
      "Epoch 36/300\n",
      "Average training loss: 0.20150417770279777\n",
      "Average test loss: 0.004883955087512731\n",
      "Epoch 37/300\n",
      "Average training loss: 0.19587927895122104\n",
      "Average test loss: 0.005103466501252519\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1923426174985038\n",
      "Average test loss: 0.004789082525918881\n",
      "Epoch 39/300\n",
      "Average training loss: 0.18997005973921882\n",
      "Average test loss: 0.007378285982956489\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1877066027853224\n",
      "Average test loss: 0.005050591382301516\n",
      "Epoch 41/300\n",
      "Average training loss: 0.18354914413558113\n",
      "Average test loss: 0.004727958622698983\n",
      "Epoch 42/300\n",
      "Average training loss: 0.18081038134627872\n",
      "Average test loss: 0.004718354399626454\n",
      "Epoch 43/300\n",
      "Average training loss: 0.17808755026923287\n",
      "Average test loss: 0.004680510010984209\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1766941022872925\n",
      "Average test loss: 0.049476325343052546\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1751165772676468\n",
      "Average test loss: 0.007052790548238489\n",
      "Epoch 46/300\n",
      "Average training loss: 0.17340739879343245\n",
      "Average test loss: 0.004654840270264281\n",
      "Epoch 47/300\n",
      "Average training loss: 0.17123455289999645\n",
      "Average test loss: 0.004603436325573259\n",
      "Epoch 48/300\n",
      "Average training loss: 0.16910388049814437\n",
      "Average test loss: 0.004795402087685135\n",
      "Epoch 49/300\n",
      "Average training loss: 0.16812082862854003\n",
      "Average test loss: 135.59760705990263\n",
      "Epoch 50/300\n",
      "Average training loss: 0.304032763838768\n",
      "Average test loss: 0.005171289503574371\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1922849270767636\n",
      "Average test loss: 0.00486008250216643\n",
      "Epoch 52/300\n",
      "Average training loss: 0.17960056320826212\n",
      "Average test loss: 0.004823152220083607\n",
      "Epoch 53/300\n",
      "Average training loss: 0.17423146686289046\n",
      "Average test loss: 0.005441600633578168\n",
      "Epoch 54/300\n",
      "Average training loss: 0.17092803744475046\n",
      "Average test loss: 0.004746319100260734\n",
      "Epoch 55/300\n",
      "Average training loss: 0.16914398392041524\n",
      "Average test loss: 0.0048533355986906425\n",
      "Epoch 56/300\n",
      "Average training loss: 0.16673508942127227\n",
      "Average test loss: 0.004720360107719898\n",
      "Epoch 57/300\n",
      "Average training loss: 0.16632763558626176\n",
      "Average test loss: 0.004578474313434627\n",
      "Epoch 58/300\n",
      "Average training loss: 0.164493142830001\n",
      "Average test loss: 0.004596935441510545\n",
      "Epoch 59/300\n",
      "Average training loss: 0.16513092715210384\n",
      "Average test loss: 0.0046171550655530556\n",
      "Epoch 60/300\n",
      "Average training loss: 0.16273203257719676\n",
      "Average test loss: 0.004943486103581057\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1617024352815416\n",
      "Average test loss: 0.00457068358009888\n",
      "Epoch 62/300\n",
      "Average training loss: 0.16109219584200118\n",
      "Average test loss: 0.004550048094656732\n",
      "Epoch 63/300\n",
      "Average training loss: 0.16052095308568742\n",
      "Average test loss: 0.00560223874863651\n",
      "Epoch 64/300\n",
      "Average training loss: 0.15935550843344795\n",
      "Average test loss: 0.0046370169694225\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1583992999792099\n",
      "Average test loss: 0.005504105097303788\n",
      "Epoch 66/300\n",
      "Average training loss: 0.15855189416805904\n",
      "Average test loss: 0.004603996711058749\n",
      "Epoch 67/300\n",
      "Average training loss: 0.15714700274997287\n",
      "Average test loss: 0.004764157613739372\n",
      "Epoch 68/300\n",
      "Average training loss: 0.15620983889367845\n",
      "Average test loss: 0.14043225390381284\n",
      "Epoch 69/300\n",
      "Average training loss: 0.155523195054796\n",
      "Average test loss: 0.004579187389877107\n",
      "Epoch 70/300\n",
      "Average training loss: 0.15434060433838104\n",
      "Average test loss: 0.004575088057253096\n",
      "Epoch 71/300\n",
      "Average training loss: 0.15435200344191657\n",
      "Average test loss: 0.0045471208143151465\n",
      "Epoch 72/300\n",
      "Average training loss: 0.15348290284474692\n",
      "Average test loss: 0.004585905156615708\n",
      "Epoch 73/300\n",
      "Average training loss: 0.15278821864393022\n",
      "Average test loss: 0.0051633448960880434\n",
      "Epoch 74/300\n",
      "Average training loss: 0.15612363033824497\n",
      "Average test loss: 0.004571664736088779\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1520070422357983\n",
      "Average test loss: 0.004823355932616525\n",
      "Epoch 76/300\n",
      "Average training loss: 0.15132097073396047\n",
      "Average test loss: 0.004493239681132966\n",
      "Epoch 77/300\n",
      "Average training loss: 0.15066333190600079\n",
      "Average test loss: 0.006073937141646941\n",
      "Epoch 78/300\n",
      "Average training loss: 0.15201192010773554\n",
      "Average test loss: 0.004609232344975074\n",
      "Epoch 79/300\n",
      "Average training loss: 0.14962813206513723\n",
      "Average test loss: 0.005623790738069349\n",
      "Epoch 80/300\n",
      "Average training loss: 0.149087709930208\n",
      "Average test loss: 0.004595840955153108\n",
      "Epoch 81/300\n",
      "Average training loss: 0.1484805039101177\n",
      "Average test loss: 0.004737844865148266\n",
      "Epoch 82/300\n",
      "Average training loss: 0.14855369595686593\n",
      "Average test loss: 0.005410918090492487\n",
      "Epoch 83/300\n",
      "Average training loss: 0.14745361471176147\n",
      "Average test loss: 0.004708665518090129\n",
      "Epoch 84/300\n",
      "Average training loss: 0.14832631378703648\n",
      "Average test loss: 0.004913629711502128\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1463824570973714\n",
      "Average test loss: 0.004631556693050596\n",
      "Epoch 86/300\n",
      "Average training loss: 0.14635461881425646\n",
      "Average test loss: 0.004721721274571287\n",
      "Epoch 87/300\n",
      "Average training loss: 0.146382908794615\n",
      "Average test loss: 0.004603312507271766\n",
      "Epoch 88/300\n",
      "Average training loss: 0.14547054452366298\n",
      "Average test loss: 0.005771029093199306\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1454765931367874\n",
      "Average test loss: 0.004515099972900417\n",
      "Epoch 90/300\n",
      "Average training loss: 0.15229471870263417\n",
      "Average test loss: 0.007555040271745788\n",
      "Epoch 91/300\n",
      "Average training loss: 123990.9528482799\n",
      "Average test loss: 151.1085404256185\n",
      "Epoch 92/300\n",
      "Average training loss: 21.727848822699652\n",
      "Average test loss: 1024.9529980370733\n",
      "Epoch 93/300\n",
      "Average training loss: 18.565035739474826\n",
      "Average test loss: 0.062464570896493064\n",
      "Epoch 94/300\n",
      "Average training loss: 16.73251089647081\n",
      "Average test loss: 0.02018582699282302\n",
      "Epoch 95/300\n",
      "Average training loss: 15.35977687327067\n",
      "Average test loss: 0.021277636314431827\n",
      "Epoch 96/300\n",
      "Average training loss: 14.18226163397895\n",
      "Average test loss: 11.416561544674966\n",
      "Epoch 97/300\n",
      "Average training loss: 13.140820572747124\n",
      "Average test loss: 0.06461777030179898\n",
      "Epoch 98/300\n",
      "Average training loss: 12.294753251817491\n",
      "Average test loss: 648.1491735194722\n",
      "Epoch 99/300\n",
      "Average training loss: 11.567981677585179\n",
      "Average test loss: 1357.4756036081315\n",
      "Epoch 100/300\n",
      "Average training loss: 10.87549952867296\n",
      "Average test loss: 39.24062610249056\n",
      "Epoch 101/300\n",
      "Average training loss: 10.282488577948676\n",
      "Average test loss: 4163.969162081262\n",
      "Epoch 102/300\n",
      "Average training loss: 9.640612113104925\n",
      "Average test loss: 0.008027026091598803\n",
      "Epoch 103/300\n",
      "Average training loss: 8.9702384830051\n",
      "Average test loss: 0.5004150108372172\n",
      "Epoch 104/300\n",
      "Average training loss: 8.354698550754124\n",
      "Average test loss: 0.04713403508729405\n",
      "Epoch 105/300\n",
      "Average training loss: 7.804864397260878\n",
      "Average test loss: 0.028631882491211097\n",
      "Epoch 106/300\n",
      "Average training loss: 7.253671214209662\n",
      "Average test loss: 0.006470475730382734\n",
      "Epoch 107/300\n",
      "Average training loss: 6.699237297481961\n",
      "Average test loss: 0.016323132063779566\n",
      "Epoch 108/300\n",
      "Average training loss: 6.213291980743408\n",
      "Average test loss: 0.006783300512366825\n",
      "Epoch 109/300\n",
      "Average training loss: 5.759795976003011\n",
      "Average test loss: 0.015471993568870756\n",
      "Epoch 110/300\n",
      "Average training loss: 5.336067451477051\n",
      "Average test loss: 0.007651704444239537\n",
      "Epoch 111/300\n",
      "Average training loss: 4.975895204755995\n",
      "Average test loss: 0.005562971666041348\n",
      "Epoch 112/300\n",
      "Average training loss: 4.664268443213569\n",
      "Average test loss: 0.07556751134991646\n",
      "Epoch 113/300\n",
      "Average training loss: 4.388632610321045\n",
      "Average test loss: 0.3989522449705336\n",
      "Epoch 114/300\n",
      "Average training loss: 4.118538900375366\n",
      "Average test loss: 0.005815424791226784\n",
      "Epoch 115/300\n",
      "Average training loss: 3.8572076822916666\n",
      "Average test loss: 0.005563788048923016\n",
      "Epoch 116/300\n",
      "Average training loss: 3.5854368135664196\n",
      "Average test loss: 0.0066786293453640405\n",
      "Epoch 117/300\n",
      "Average training loss: 3.3051567062801785\n",
      "Average test loss: 0.005441571635918485\n",
      "Epoch 118/300\n",
      "Average training loss: 3.03105165693495\n",
      "Average test loss: 0.0052218085891670655\n",
      "Epoch 119/300\n",
      "Average training loss: 2.756947452121311\n",
      "Average test loss: 0.005849118019557661\n",
      "Epoch 120/300\n",
      "Average training loss: 2.472143242518107\n",
      "Average test loss: 0.0056885872752302225\n",
      "Epoch 121/300\n",
      "Average training loss: 2.216113720787896\n",
      "Average test loss: 0.005040789141837094\n",
      "Epoch 122/300\n",
      "Average training loss: 1.9987174968719483\n",
      "Average test loss: 0.004923487529986435\n",
      "Epoch 123/300\n",
      "Average training loss: 1.7955288146336874\n",
      "Average test loss: 0.004974925858692991\n",
      "Epoch 124/300\n",
      "Average training loss: 1.580849067264133\n",
      "Average test loss: 0.0054795122597780495\n",
      "Epoch 125/300\n",
      "Average training loss: 1.322945186085171\n",
      "Average test loss: 0.005225679009738896\n",
      "Epoch 126/300\n",
      "Average training loss: 1.1131775734159681\n",
      "Average test loss: 0.0048866518528925046\n",
      "Epoch 127/300\n",
      "Average training loss: 0.943281026787228\n",
      "Average test loss: 0.004752086840363013\n",
      "Epoch 128/300\n",
      "Average training loss: 0.7938025294939677\n",
      "Average test loss: 0.004727847868783606\n",
      "Epoch 129/300\n",
      "Average training loss: 0.6516787991523743\n",
      "Average test loss: 0.004775442856260472\n",
      "Epoch 130/300\n",
      "Average training loss: 0.5423930655850304\n",
      "Average test loss: 0.005153920066232482\n",
      "Epoch 131/300\n",
      "Average training loss: 0.4633120988210042\n",
      "Average test loss: 0.004655906374462776\n",
      "Epoch 132/300\n",
      "Average training loss: 0.3990493812825945\n",
      "Average test loss: 0.004823784619155857\n",
      "Epoch 133/300\n",
      "Average training loss: 0.3485520313315921\n",
      "Average test loss: 0.004833782835553089\n",
      "Epoch 134/300\n",
      "Average training loss: 0.3106904811329312\n",
      "Average test loss: 0.004554514214396477\n",
      "Epoch 135/300\n",
      "Average training loss: 0.28362939625316197\n",
      "Average test loss: 0.005082775536924601\n",
      "Epoch 136/300\n",
      "Average training loss: 0.2585531982315911\n",
      "Average test loss: 0.007355472250531117\n",
      "Epoch 137/300\n",
      "Average training loss: 0.24205234151416355\n",
      "Average test loss: 0.004550059243208832\n",
      "Epoch 138/300\n",
      "Average training loss: 0.23972068696551851\n",
      "Average test loss: 0.004521330500849419\n",
      "Epoch 139/300\n",
      "Average training loss: 0.21306721578703985\n",
      "Average test loss: 0.004504076697346237\n",
      "Epoch 140/300\n",
      "Average training loss: 0.20276494048701393\n",
      "Average test loss: 0.004540237557556894\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1949718104865816\n",
      "Average test loss: 0.18915414450069268\n",
      "Epoch 142/300\n",
      "Average training loss: 0.18780235464043088\n",
      "Average test loss: 0.009505207136273385\n",
      "Epoch 143/300\n",
      "Average training loss: 0.1838369059032864\n",
      "Average test loss: 0.0045739328550795716\n",
      "Epoch 144/300\n",
      "Average training loss: 0.18092345323827533\n",
      "Average test loss: 0.007036988645998968\n",
      "Epoch 145/300\n",
      "Average training loss: 0.17424137818813323\n",
      "Average test loss: 0.019848302114340993\n",
      "Epoch 146/300\n",
      "Average training loss: 0.17874590881665547\n",
      "Average test loss: 0.004538666199271878\n",
      "Epoch 147/300\n",
      "Average training loss: 0.16891448023584155\n",
      "Average test loss: 0.004477296178332634\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1667378552224901\n",
      "Average test loss: 0.004474123035868009\n",
      "Epoch 149/300\n",
      "Average training loss: 0.16434032266669804\n",
      "Average test loss: 0.004507205689532889\n",
      "Epoch 150/300\n",
      "Average training loss: 0.16276749483744304\n",
      "Average test loss: 0.004474936592910025\n",
      "Epoch 151/300\n",
      "Average training loss: 0.16687470998366674\n",
      "Average test loss: 0.005393319063716465\n",
      "Epoch 152/300\n",
      "Average training loss: 0.15870268183284336\n",
      "Average test loss: 0.00824993471801281\n",
      "Epoch 153/300\n",
      "Average training loss: 0.15751297706365586\n",
      "Average test loss: 0.004491236994663875\n",
      "Epoch 154/300\n",
      "Average training loss: 0.15654289012485081\n",
      "Average test loss: 0.006175034018440379\n",
      "Epoch 155/300\n",
      "Average training loss: 0.15488768683539497\n",
      "Average test loss: 0.004471874930378463\n",
      "Epoch 156/300\n",
      "Average training loss: 0.1542437348233329\n",
      "Average test loss: 0.004487221717834473\n",
      "Epoch 157/300\n",
      "Average training loss: 625691.0373774698\n",
      "Average test loss: 347.6401083484888\n",
      "Epoch 158/300\n",
      "Average training loss: 26.027537855360244\n",
      "Average test loss: 1.1814026897433731\n",
      "Epoch 159/300\n",
      "Average training loss: 21.080438301934137\n",
      "Average test loss: 1143.4253863495721\n",
      "Epoch 160/300\n",
      "Average training loss: 18.60110728624132\n",
      "Average test loss: 327.5315066029297\n",
      "Epoch 161/300\n",
      "Average training loss: 16.6113680267334\n",
      "Average test loss: 4.445462974297504\n",
      "Epoch 162/300\n",
      "Average training loss: 15.109662073771158\n",
      "Average test loss: 1.3782050066838663\n",
      "Epoch 163/300\n",
      "Average training loss: 13.841333745320638\n",
      "Average test loss: 7647.118741611679\n",
      "Epoch 164/300\n",
      "Average training loss: 12.761553015814886\n",
      "Average test loss: 16.00036384854383\n",
      "Epoch 165/300\n",
      "Average training loss: 11.826796563890245\n",
      "Average test loss: 585.8510789607499\n",
      "Epoch 166/300\n",
      "Average training loss: 10.981114776611328\n",
      "Average test loss: 80.03641233465406\n",
      "Epoch 167/300\n",
      "Average training loss: 10.169491619533963\n",
      "Average test loss: 0.7661360106170177\n",
      "Epoch 168/300\n",
      "Average training loss: 9.375900145636665\n",
      "Average test loss: 969.4486829982334\n",
      "Epoch 169/300\n",
      "Average training loss: 8.632645969814725\n",
      "Average test loss: 259.7358625415895\n",
      "Epoch 170/300\n",
      "Average training loss: 7.9505422401428225\n",
      "Average test loss: 0.04256533773119251\n",
      "Epoch 171/300\n",
      "Average training loss: 7.383996215396457\n",
      "Average test loss: 0.037133194925884405\n",
      "Epoch 172/300\n",
      "Average training loss: 6.914550042470296\n",
      "Average test loss: 0.011342802254276143\n",
      "Epoch 173/300\n",
      "Average training loss: 6.500534030490451\n",
      "Average test loss: 0.017620317246764897\n",
      "Epoch 174/300\n",
      "Average training loss: 6.125902993943956\n",
      "Average test loss: 0.25386677803264723\n",
      "Epoch 175/300\n",
      "Average training loss: 5.747646396213107\n",
      "Average test loss: 0.006202980239358213\n",
      "Epoch 176/300\n",
      "Average training loss: 5.375700621710883\n",
      "Average test loss: 0.6465518973900212\n",
      "Epoch 177/300\n",
      "Average training loss: 4.9974198371039495\n",
      "Average test loss: 0.006476039718422625\n",
      "Epoch 178/300\n",
      "Average training loss: 4.6330932930840385\n",
      "Average test loss: 0.010837761191030344\n",
      "Epoch 179/300\n",
      "Average training loss: 4.302995750427246\n",
      "Average test loss: 0.025893330641918712\n",
      "Epoch 180/300\n",
      "Average training loss: 4.00568086751302\n",
      "Average test loss: 0.5040447854929501\n",
      "Epoch 181/300\n",
      "Average training loss: 3.7290051725175646\n",
      "Average test loss: 0.005392537817358971\n",
      "Epoch 182/300\n",
      "Average training loss: 3.472935094833374\n",
      "Average test loss: 0.005385995354710354\n",
      "Epoch 183/300\n",
      "Average training loss: 3.2248503415849474\n",
      "Average test loss: 0.009036473462565078\n",
      "Epoch 184/300\n",
      "Average training loss: 2.984126690970527\n",
      "Average test loss: 41.79616377766927\n",
      "Epoch 185/300\n",
      "Average training loss: 2.7451126611497667\n",
      "Average test loss: 0.0051073628374271925\n",
      "Epoch 186/300\n",
      "Average training loss: 2.5167915907965765\n",
      "Average test loss: 0.19140744975374804\n",
      "Epoch 187/300\n",
      "Average training loss: 2.301741068734063\n",
      "Average test loss: 0.004987691193819046\n",
      "Epoch 188/300\n",
      "Average training loss: 2.1059484089745415\n",
      "Average test loss: 0.005032703919129239\n",
      "Epoch 189/300\n",
      "Average training loss: 1.9543461689419217\n",
      "Average test loss: 0.01998020509671834\n",
      "Epoch 190/300\n",
      "Average training loss: 1.8303138377931383\n",
      "Average test loss: 0.005817140164060725\n",
      "Epoch 191/300\n",
      "Average training loss: 1.7167647043863932\n",
      "Average test loss: 0.004831591088117824\n",
      "Epoch 192/300\n",
      "Average training loss: 1.605129494243198\n",
      "Average test loss: 0.010721659087886413\n",
      "Epoch 193/300\n",
      "Average training loss: 1.4996709611680772\n",
      "Average test loss: 0.03628987909356753\n",
      "Epoch 194/300\n",
      "Average training loss: 1.386581882900662\n",
      "Average test loss: 0.07159698446322646\n",
      "Epoch 195/300\n",
      "Average training loss: 1.2738535461425782\n",
      "Average test loss: 0.00483607794261641\n",
      "Epoch 196/300\n",
      "Average training loss: 1.1634742832183838\n",
      "Average test loss: 0.0047060644299619725\n",
      "Epoch 197/300\n",
      "Average training loss: 1.0570075674057007\n",
      "Average test loss: 0.0046670941578017345\n",
      "Epoch 198/300\n",
      "Average training loss: 0.9573930405510797\n",
      "Average test loss: 1.75068541208903\n",
      "Epoch 199/300\n",
      "Average training loss: 0.8656301737361484\n",
      "Average test loss: 0.0046202370946606\n",
      "Epoch 200/300\n",
      "Average training loss: 0.7806675126287672\n",
      "Average test loss: 0.013399227185381784\n",
      "Epoch 201/300\n",
      "Average training loss: 0.7054129553371006\n",
      "Average test loss: 0.0054980501201417715\n",
      "Epoch 202/300\n",
      "Average training loss: 0.6325941061443753\n",
      "Average test loss: 30.750909637875026\n",
      "Epoch 203/300\n",
      "Average training loss: 0.5646262782414754\n",
      "Average test loss: 0.004497633170957366\n",
      "Epoch 204/300\n",
      "Average training loss: 0.5039991950458951\n",
      "Average test loss: 0.004568792204062144\n",
      "Epoch 205/300\n",
      "Average training loss: 0.44883500425020856\n",
      "Average test loss: 0.004522504494835933\n",
      "Epoch 206/300\n",
      "Average training loss: 0.4009129785431756\n",
      "Average test loss: 0.0045137470788839795\n",
      "Epoch 207/300\n",
      "Average training loss: 0.37030141168170505\n",
      "Average test loss: 0.026452284900678528\n",
      "Epoch 208/300\n",
      "Average training loss: 0.3200972456932068\n",
      "Average test loss: 0.004562939199308554\n",
      "Epoch 209/300\n",
      "Average training loss: 0.28134639750586615\n",
      "Average test loss: 0.05375276442368825\n",
      "Epoch 210/300\n",
      "Average training loss: 0.24441598961088393\n",
      "Average test loss: 2426214.62\n",
      "Epoch 211/300\n",
      "Average training loss: 0.22094426023960115\n",
      "Average test loss: 0.14165670517914825\n",
      "Epoch 212/300\n",
      "Average training loss: 0.20311977265940773\n",
      "Average test loss: 0.009405248378300004\n",
      "Epoch 213/300\n",
      "Average training loss: 0.19255609333515167\n",
      "Average test loss: 0.004638850337515274\n",
      "Epoch 214/300\n",
      "Average training loss: 0.1910611789756351\n",
      "Average test loss: 0.0044723684393490355\n",
      "Epoch 215/300\n",
      "Average training loss: 0.1783039363887575\n",
      "Average test loss: 0.005056737441155645\n",
      "Epoch 216/300\n",
      "Average training loss: 37957.53524279514\n",
      "Average test loss: 121.59577296339803\n",
      "Epoch 217/300\n",
      "Average training loss: 20.3163361070421\n",
      "Average test loss: 0.023226925421092247\n",
      "Epoch 218/300\n",
      "Average training loss: 18.256434870402018\n",
      "Average test loss: 0.013044798875848453\n",
      "Epoch 219/300\n",
      "Average training loss: 17.075635684543187\n",
      "Average test loss: 0.013866042700078752\n",
      "Epoch 220/300\n",
      "Average training loss: 16.156777217441135\n",
      "Average test loss: 0.011303940808607473\n",
      "Epoch 221/300\n",
      "Average training loss: 15.294952034844293\n",
      "Average test loss: 0.010211089252597757\n",
      "Epoch 222/300\n",
      "Average training loss: 14.462448448181153\n",
      "Average test loss: 0.011092124951382478\n",
      "Epoch 223/300\n",
      "Average training loss: 13.697397961934408\n",
      "Average test loss: 0.010624687861237261\n",
      "Epoch 224/300\n",
      "Average training loss: 12.948326510959202\n",
      "Average test loss: 0.007892557902468575\n",
      "Epoch 225/300\n",
      "Average training loss: 12.188717969258626\n",
      "Average test loss: 0.007954759755068356\n",
      "Epoch 226/300\n",
      "Average training loss: 11.44408977762858\n",
      "Average test loss: 0.023421566592322456\n",
      "Epoch 227/300\n",
      "Average training loss: 10.733602242363824\n",
      "Average test loss: 0.0072745281474457845\n",
      "Epoch 228/300\n",
      "Average training loss: 10.08709464433458\n",
      "Average test loss: 0.006641179264419608\n",
      "Epoch 229/300\n",
      "Average training loss: 9.496057885064019\n",
      "Average test loss: 0.006915646590292454\n",
      "Epoch 230/300\n",
      "Average training loss: 8.937333726671007\n",
      "Average test loss: 0.006413630631234911\n",
      "Epoch 231/300\n",
      "Average training loss: 8.40386446211073\n",
      "Average test loss: 0.006054977560622825\n",
      "Epoch 232/300\n",
      "Average training loss: 7.9019567565917965\n",
      "Average test loss: 0.005951273425999615\n",
      "Epoch 233/300\n",
      "Average training loss: 7.435534999423557\n",
      "Average test loss: 0.005741701976706585\n",
      "Epoch 234/300\n",
      "Average training loss: 6.993269276936849\n",
      "Average test loss: 0.00563359048207187\n",
      "Epoch 235/300\n",
      "Average training loss: 6.567242681715223\n",
      "Average test loss: 0.005586696341219876\n",
      "Epoch 236/300\n",
      "Average training loss: 6.148214285108779\n",
      "Average test loss: 0.005426783543700973\n",
      "Epoch 237/300\n",
      "Average training loss: 5.729082438151042\n",
      "Average test loss: 0.005255558955586619\n",
      "Epoch 238/300\n",
      "Average training loss: 5.304859887440999\n",
      "Average test loss: 0.005230511063916816\n",
      "Epoch 239/300\n",
      "Average training loss: 4.882985849592421\n",
      "Average test loss: 0.0051924044936895375\n",
      "Epoch 240/300\n",
      "Average training loss: 4.488353289710151\n",
      "Average test loss: 0.014901400823560026\n",
      "Epoch 241/300\n",
      "Average training loss: 4.111608034557767\n",
      "Average test loss: 0.02487645476228661\n",
      "Epoch 242/300\n",
      "Average training loss: 3.7395910720825194\n",
      "Average test loss: 0.006267062661962377\n",
      "Epoch 243/300\n",
      "Average training loss: 3.3776268049875897\n",
      "Average test loss: 0.004837414769869711\n",
      "Epoch 244/300\n",
      "Average training loss: 3.0238580481211343\n",
      "Average test loss: 0.007042113499508964\n",
      "Epoch 245/300\n",
      "Average training loss: 2.6804004033406574\n",
      "Average test loss: 0.3294669653036528\n",
      "Epoch 246/300\n",
      "Average training loss: 2.342346151775784\n",
      "Average test loss: 0.0048981866666840184\n",
      "Epoch 247/300\n",
      "Average training loss: 1.9854743423461914\n",
      "Average test loss: 0.004730260148230526\n",
      "Epoch 248/300\n",
      "Average training loss: 1.6529841853247749\n",
      "Average test loss: 0.005208984888262219\n",
      "Epoch 249/300\n",
      "Average training loss: 1.3692965269088746\n",
      "Average test loss: 0.006271567731681797\n",
      "Epoch 250/300\n",
      "Average training loss: 1.1070561321576435\n",
      "Average test loss: 0.023217822077787584\n",
      "Epoch 251/300\n",
      "Average training loss: 0.8456740569008722\n",
      "Average test loss: 0.0046115017498119006\n",
      "Epoch 252/300\n",
      "Average training loss: 0.6419488844871521\n",
      "Average test loss: 0.004563558720052242\n",
      "Epoch 253/300\n",
      "Average training loss: 0.5172880693806542\n",
      "Average test loss: 0.004683284874591563\n",
      "Epoch 254/300\n",
      "Average training loss: 0.4300441535313924\n",
      "Average test loss: 0.008327794689271185\n",
      "Epoch 255/300\n",
      "Average training loss: 0.3642475995487637\n",
      "Average test loss: 0.004753207256189651\n",
      "Epoch 256/300\n",
      "Average training loss: 0.3181423946221669\n",
      "Average test loss: 0.26372021601100765\n",
      "Epoch 257/300\n",
      "Average training loss: 0.2842004407511817\n",
      "Average test loss: 0.012261208389368321\n",
      "Epoch 258/300\n",
      "Average training loss: 0.2577315462430318\n",
      "Average test loss: 0.005838719359495574\n",
      "Epoch 259/300\n",
      "Average training loss: 0.23613211846351623\n",
      "Average test loss: 0.0050025741259257\n",
      "Epoch 260/300\n",
      "Average training loss: 0.22093125561873117\n",
      "Average test loss: 0.004559079435550504\n",
      "Epoch 261/300\n",
      "Average training loss: 0.20826228889491824\n",
      "Average test loss: 1.4232908106644948\n",
      "Epoch 262/300\n",
      "Average training loss: 0.19824922666284772\n",
      "Average test loss: 0.010496950069235431\n",
      "Epoch 263/300\n",
      "Average training loss: 0.19132152054044935\n",
      "Average test loss: 0.004551704016824564\n",
      "Epoch 264/300\n",
      "Average training loss: 0.185472138232655\n",
      "Average test loss: 0.004677363355540567\n",
      "Epoch 265/300\n",
      "Average training loss: 0.1805199946694904\n",
      "Average test loss: 0.004738699090563589\n",
      "Epoch 266/300\n",
      "Average training loss: 0.17799224502510494\n",
      "Average test loss: 0.004519046416299211\n",
      "Epoch 267/300\n",
      "Average training loss: 0.17577622176541222\n",
      "Average test loss: 0.004947182801862558\n",
      "Epoch 268/300\n",
      "Average training loss: 0.17050378953086004\n",
      "Average test loss: 0.004569156931506263\n",
      "Epoch 269/300\n",
      "Average training loss: 0.300957804746098\n",
      "Average test loss: 1127.2652145971801\n",
      "Epoch 270/300\n",
      "Average training loss: 0.1964198653300603\n",
      "Average test loss: 0.004586793696300851\n",
      "Epoch 271/300\n",
      "Average training loss: 0.17587150145901573\n",
      "Average test loss: 0.004534592914912435\n",
      "Epoch 272/300\n",
      "Average training loss: 0.17089745263258616\n",
      "Average test loss: 0.004484169188265999\n",
      "Epoch 273/300\n",
      "Average training loss: 0.16702036984761556\n",
      "Average test loss: 0.004475899740018778\n",
      "Epoch 274/300\n",
      "Average training loss: 0.16469102578030692\n",
      "Average test loss: 0.004516516269081169\n",
      "Epoch 275/300\n",
      "Average training loss: 0.16273031831449933\n",
      "Average test loss: 0.004484495023886362\n",
      "Epoch 276/300\n",
      "Average training loss: 0.1610498716301388\n",
      "Average test loss: 0.004461668662726879\n",
      "Epoch 277/300\n",
      "Average training loss: 0.15970659458637237\n",
      "Average test loss: 0.004479801463377144\n",
      "Epoch 278/300\n",
      "Average training loss: 0.18804642842875588\n",
      "Average test loss: 0.004554413281795051\n",
      "Epoch 279/300\n",
      "Average training loss: 0.16227161366409726\n",
      "Average test loss: 0.00447229770405425\n",
      "Epoch 280/300\n",
      "Average training loss: 0.1581945568058226\n",
      "Average test loss: 0.0047304472561097806\n",
      "Epoch 281/300\n",
      "Average training loss: 0.15632594397332933\n",
      "Average test loss: 71677283.11155556\n",
      "Epoch 282/300\n",
      "Average training loss: 0.15557659437921312\n",
      "Average test loss: 0.004583117092235221\n",
      "Epoch 283/300\n",
      "Average training loss: 0.15411599294344583\n",
      "Average test loss: 0.004449289018495215\n",
      "Epoch 284/300\n",
      "Average training loss: 0.15365832247998978\n",
      "Average test loss: 0.004740584261715412\n",
      "Epoch 285/300\n",
      "Average training loss: 0.15410034085644617\n",
      "Average test loss: 0.00446874514222145\n",
      "Epoch 286/300\n",
      "Average training loss: 0.15165323746204376\n",
      "Average test loss: 0.004451717262259788\n",
      "Epoch 287/300\n",
      "Average training loss: 0.15115337600972917\n",
      "Average test loss: 0.004493658238401015\n",
      "Epoch 288/300\n",
      "Average training loss: 0.15665817141532898\n",
      "Average test loss: 0.004400395493540499\n",
      "Epoch 289/300\n",
      "Average training loss: 0.15004696609576543\n",
      "Average test loss: 0.00470352247522937\n",
      "Epoch 290/300\n",
      "Average training loss: 0.14895935283766853\n",
      "Average test loss: 0.004468511592596769\n",
      "Epoch 291/300\n",
      "Average training loss: 0.16794179985258315\n",
      "Average test loss: 0.005028831690135929\n",
      "Epoch 292/300\n",
      "Average training loss: 0.1502472557094362\n",
      "Average test loss: 0.004614335026592017\n",
      "Epoch 293/300\n",
      "Average training loss: 0.1477536134057575\n",
      "Average test loss: 0.005786159042268991\n",
      "Epoch 294/300\n",
      "Average training loss: 0.14763356975714365\n",
      "Average test loss: 0.004537272209922473\n",
      "Epoch 295/300\n",
      "Average training loss: 19873.80968678256\n",
      "Average test loss: 0.13371700225936042\n",
      "Epoch 296/300\n",
      "Average training loss: 22.491704625447593\n",
      "Average test loss: 1.4450320127854746\n",
      "Epoch 297/300\n",
      "Average training loss: 19.75135226949056\n",
      "Average test loss: 0.021346615198585724\n",
      "Epoch 298/300\n",
      "Average training loss: 18.05294954087999\n",
      "Average test loss: 0.0211678810806738\n",
      "Epoch 299/300\n",
      "Average training loss: 16.610367136637368\n",
      "Average test loss: 1.3004969817267524\n",
      "Epoch 300/300\n",
      "Average training loss: 15.305855915493435\n",
      "Average test loss: 0.018734094871415034\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.295096528371175\n",
      "Average test loss: 4.9916273803214235\n",
      "Epoch 2/300\n",
      "Average training loss: 5.903145400153266\n",
      "Average test loss: 0.006661536399688986\n",
      "Epoch 3/300\n",
      "Average training loss: 4.351487295998467\n",
      "Average test loss: 0.005512786284916931\n",
      "Epoch 4/300\n",
      "Average training loss: 3.1448366288079157\n",
      "Average test loss: 0.015613214877744516\n",
      "Epoch 5/300\n",
      "Average training loss: 2.387611834420098\n",
      "Average test loss: 0.005077946924087074\n",
      "Epoch 6/300\n",
      "Average training loss: 1.9246564526028103\n",
      "Average test loss: 0.004776099777676993\n",
      "Epoch 7/300\n",
      "Average training loss: 1.5787965326309203\n",
      "Average test loss: 0.004578888414634599\n",
      "Epoch 8/300\n",
      "Average training loss: 1.3103910109202066\n",
      "Average test loss: 0.004268309140991833\n",
      "Epoch 9/300\n",
      "Average training loss: 1.0660679372151693\n",
      "Average test loss: 0.0042947535976353615\n",
      "Epoch 10/300\n",
      "Average training loss: 0.8882495895491705\n",
      "Average test loss: 0.00469463063735101\n",
      "Epoch 11/300\n",
      "Average training loss: 0.75130039495892\n",
      "Average test loss: 0.004136053871363401\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6477720481024848\n",
      "Average test loss: 0.0039860533165434996\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5660999619695876\n",
      "Average test loss: 0.0036722298922638097\n",
      "Epoch 14/300\n",
      "Average training loss: 0.5007303770913019\n",
      "Average test loss: 0.004054402873126997\n",
      "Epoch 15/300\n",
      "Average training loss: 0.44557700390285915\n",
      "Average test loss: 0.0034519079863611197\n",
      "Epoch 16/300\n",
      "Average training loss: 0.39980149825414024\n",
      "Average test loss: 0.004804360114451912\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3610644228193495\n",
      "Average test loss: 0.003540019076317549\n",
      "Epoch 18/300\n",
      "Average training loss: 0.3277113842964172\n",
      "Average test loss: 0.0032717024532871114\n",
      "Epoch 19/300\n",
      "Average training loss: 0.298571478260888\n",
      "Average test loss: 0.003542180815504657\n",
      "Epoch 20/300\n",
      "Average training loss: 0.27347403150134614\n",
      "Average test loss: 0.0034325541835278275\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2530962115658654\n",
      "Average test loss: 0.003482430133761631\n",
      "Epoch 22/300\n",
      "Average training loss: 0.23451970216963025\n",
      "Average test loss: 0.003076499108949469\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2168345755736033\n",
      "Average test loss: 0.0048086027354002\n",
      "Epoch 24/300\n",
      "Average training loss: 0.20617308051056332\n",
      "Average test loss: 0.002991502418493231\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1942967485586802\n",
      "Average test loss: 0.00341466172101597\n",
      "Epoch 26/300\n",
      "Average training loss: 0.18389295746220483\n",
      "Average test loss: 0.0030872301548305484\n",
      "Epoch 27/300\n",
      "Average training loss: 0.17549037653870053\n",
      "Average test loss: 0.003243548129995664\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1678911895884408\n",
      "Average test loss: 0.0028707546124027835\n",
      "Epoch 29/300\n",
      "Average training loss: 0.16133356805642446\n",
      "Average test loss: 0.003003545778079165\n",
      "Epoch 30/300\n",
      "Average training loss: 0.15454288335641225\n",
      "Average test loss: 0.00532793706552022\n",
      "Epoch 31/300\n",
      "Average training loss: 0.14987579339080387\n",
      "Average test loss: 0.003014867185821964\n",
      "Epoch 32/300\n",
      "Average training loss: 0.14478307298819224\n",
      "Average test loss: 0.0029187635048809977\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14048140108585358\n",
      "Average test loss: 0.003000710073651539\n",
      "Epoch 34/300\n",
      "Average training loss: 0.13702544664012062\n",
      "Average test loss: 0.0027059077926807934\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13295521868599786\n",
      "Average test loss: 0.0027957406983607344\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1282331993646092\n",
      "Average test loss: 0.0028104894602050384\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12706474388970268\n",
      "Average test loss: 0.0027357823670738275\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12281200410260094\n",
      "Average test loss: 0.002785701325784127\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11994381669494841\n",
      "Average test loss: 0.0026916789228303567\n",
      "Epoch 40/300\n",
      "Average training loss: 0.116102778395017\n",
      "Average test loss: 0.002895811994249622\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11443987324502733\n",
      "Average test loss: 0.0038375441630681354\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11239114818308088\n",
      "Average test loss: 0.0037780015543103216\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10924916060765584\n",
      "Average test loss: 0.5198748761018117\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1070969964199596\n",
      "Average test loss: 0.0026911129008771644\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11236019289493561\n",
      "Average test loss: 0.03503334201210075\n",
      "Epoch 46/300\n",
      "Average training loss: 18.280956353055107\n",
      "Average test loss: 0.005014468070740501\n",
      "Epoch 47/300\n",
      "Average training loss: 2.8406700701183745\n",
      "Average test loss: 0.004508082438260317\n",
      "Epoch 48/300\n",
      "Average training loss: 1.1951143362787036\n",
      "Average test loss: 0.003697143571658267\n",
      "Epoch 49/300\n",
      "Average training loss: 0.6714287091361152\n",
      "Average test loss: 0.0034480735959692135\n",
      "Epoch 50/300\n",
      "Average training loss: 0.4589074096149868\n",
      "Average test loss: 0.0036574494265433815\n",
      "Epoch 51/300\n",
      "Average training loss: 0.3619208086066776\n",
      "Average test loss: 0.0032394403028819297\n",
      "Epoch 52/300\n",
      "Average training loss: 0.3028417568736606\n",
      "Average test loss: 0.0031521374826422996\n",
      "Epoch 53/300\n",
      "Average training loss: 0.2631864337126414\n",
      "Average test loss: 0.003052025887908207\n",
      "Epoch 54/300\n",
      "Average training loss: 0.23481945637861887\n",
      "Average test loss: 0.0030090643757333356\n",
      "Epoch 55/300\n",
      "Average training loss: 0.2137805585861206\n",
      "Average test loss: 0.002962315766347779\n",
      "Epoch 56/300\n",
      "Average training loss: 0.19393621468544006\n",
      "Average test loss: 0.003183377234265208\n",
      "Epoch 57/300\n",
      "Average training loss: 0.17613360602325864\n",
      "Average test loss: 0.0028819562832100524\n",
      "Epoch 58/300\n",
      "Average training loss: 0.16161343673865\n",
      "Average test loss: 0.002871798405009839\n",
      "Epoch 59/300\n",
      "Average training loss: 0.15286529367499882\n",
      "Average test loss: 0.0028847291426112253\n",
      "Epoch 60/300\n",
      "Average training loss: 0.14701327652401394\n",
      "Average test loss: 0.0027920254779358706\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1417121130492952\n",
      "Average test loss: 0.0027755667293030356\n",
      "Epoch 62/300\n",
      "Average training loss: 0.13723747611045836\n",
      "Average test loss: 0.0027698406850298245\n",
      "Epoch 63/300\n",
      "Average training loss: 0.13309245216846466\n",
      "Average test loss: 0.002728552469776736\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12967825778325398\n",
      "Average test loss: 0.0027367572825815942\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12641629546880723\n",
      "Average test loss: 0.0030268711989952456\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12398855704731411\n",
      "Average test loss: 0.0035307720171080695\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1213164984120263\n",
      "Average test loss: 0.0027310125227603646\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11920411673519346\n",
      "Average test loss: 0.0026421330134487817\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11664728319644928\n",
      "Average test loss: 0.0027196224122825598\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11486929431226518\n",
      "Average test loss: 0.002864565209589071\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11304916586478551\n",
      "Average test loss: 0.002640085486281249\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11129176200760735\n",
      "Average test loss: 0.002607654757797718\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10999523545636071\n",
      "Average test loss: 0.0026095891748037603\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10850629420412912\n",
      "Average test loss: 0.0026547469520527456\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10712773269414902\n",
      "Average test loss: 0.002663570728670392\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10616712368859185\n",
      "Average test loss: 0.0031150494352396993\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10507780479722553\n",
      "Average test loss: 0.002666000648298197\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1043246123790741\n",
      "Average test loss: 0.002727084904495213\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10352301139963997\n",
      "Average test loss: 0.0029365014515610206\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10242603160606491\n",
      "Average test loss: 0.004142553730143441\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10164905337492625\n",
      "Average test loss: 0.0026061392482370138\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10078311387697855\n",
      "Average test loss: 0.002558945685418116\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10041785379913118\n",
      "Average test loss: 0.0026126877918011614\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1006309799750646\n",
      "Average test loss: 0.0026965897439254656\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09864008010758293\n",
      "Average test loss: 0.002580945587406556\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09817758034335243\n",
      "Average test loss: 0.008167441211640834\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09836750644445419\n",
      "Average test loss: 0.0025419585544409025\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09675905021693972\n",
      "Average test loss: 0.0025312099936935636\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09620018332534366\n",
      "Average test loss: 0.0030926916572368806\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09581041390366024\n",
      "Average test loss: 0.0027024075442718134\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09507624072167609\n",
      "Average test loss: 0.002652039070510202\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09462897138463126\n",
      "Average test loss: 0.002799182282346818\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09469248960415522\n",
      "Average test loss: 0.0035726702029092445\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09331632954544491\n",
      "Average test loss: 0.002504901115472118\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0928550946811835\n",
      "Average test loss: 0.01073993527640899\n",
      "Epoch 96/300\n",
      "Average training loss: 0.09295331261555353\n",
      "Average test loss: 0.0029769618126253288\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09272789300150341\n",
      "Average test loss: 0.00266300248934163\n",
      "Epoch 98/300\n",
      "Average training loss: 48.08738583015733\n",
      "Average test loss: 111420.22645273521\n",
      "Epoch 99/300\n",
      "Average training loss: 5.003064393785265\n",
      "Average test loss: 0.04434328249076174\n",
      "Epoch 100/300\n",
      "Average training loss: 3.1159691999223496\n",
      "Average test loss: 0.004084263428631756\n",
      "Epoch 101/300\n",
      "Average training loss: 2.230473284403483\n",
      "Average test loss: 0.018916348517768914\n",
      "Epoch 102/300\n",
      "Average training loss: 1.731090470314026\n",
      "Average test loss: 0.003167879033419821\n",
      "Epoch 103/300\n",
      "Average training loss: 1.369163291613261\n",
      "Average test loss: 0.0029694869472748704\n",
      "Epoch 104/300\n",
      "Average training loss: 1.0932935756577387\n",
      "Average test loss: 0.0069317029561433525\n",
      "Epoch 105/300\n",
      "Average training loss: 0.867281786388821\n",
      "Average test loss: 0.0028948138321025505\n",
      "Epoch 106/300\n",
      "Average training loss: 0.690980313565996\n",
      "Average test loss: 0.004880311062145564\n",
      "Epoch 107/300\n",
      "Average training loss: 0.5532508018811544\n",
      "Average test loss: 0.002992202171227998\n",
      "Epoch 108/300\n",
      "Average training loss: 0.4480080774890052\n",
      "Average test loss: 0.0034178097179780403\n",
      "Epoch 109/300\n",
      "Average training loss: 0.36542096032036675\n",
      "Average test loss: 0.002717083809379902\n",
      "Epoch 110/300\n",
      "Average training loss: 0.3022795921696557\n",
      "Average test loss: 0.0027251472518675856\n",
      "Epoch 111/300\n",
      "Average training loss: 0.2542974460389879\n",
      "Average test loss: 0.004744496148907476\n",
      "Epoch 112/300\n",
      "Average training loss: 0.21805017699135673\n",
      "Average test loss: 0.002681167059784962\n",
      "Epoch 113/300\n",
      "Average training loss: 0.19201475678549873\n",
      "Average test loss: 0.003706999510112736\n",
      "Epoch 114/300\n",
      "Average training loss: 0.17510523182815976\n",
      "Average test loss: 0.0042285330585307545\n",
      "Epoch 115/300\n",
      "Average training loss: 0.16209138723214467\n",
      "Average test loss: 0.0035382937296397155\n",
      "Epoch 116/300\n",
      "Average training loss: 0.15206942184766134\n",
      "Average test loss: 0.002831452435710364\n",
      "Epoch 117/300\n",
      "Average training loss: 0.14419503475560083\n",
      "Average test loss: 0.029495129235088825\n",
      "Epoch 118/300\n",
      "Average training loss: 0.13718099892139435\n",
      "Average test loss: 0.0039254999599523015\n",
      "Epoch 119/300\n",
      "Average training loss: 0.13123870599932141\n",
      "Average test loss: 0.003349872084127532\n",
      "Epoch 120/300\n",
      "Average training loss: 0.12384130574597253\n",
      "Average test loss: 0.0025710928037555683\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11824814723597632\n",
      "Average test loss: 0.0025432223421004083\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11533679918448131\n",
      "Average test loss: 0.002619889203045103\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11207200276851655\n",
      "Average test loss: 0.002511185383424163\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10970915246009827\n",
      "Average test loss: 0.0025190183371305466\n",
      "Epoch 125/300\n",
      "Average training loss: 0.1078228047688802\n",
      "Average test loss: 0.0025524183211641177\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10581686551041074\n",
      "Average test loss: 0.0027846623675690757\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10467266719871098\n",
      "Average test loss: 0.0025238480026730232\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10292471796274186\n",
      "Average test loss: 0.002815116535872221\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10178000530600548\n",
      "Average test loss: 0.002629632545221183\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10056173476907942\n",
      "Average test loss: 0.0025645329867386157\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0993101690808932\n",
      "Average test loss: 0.002603698648719324\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10123855517970191\n",
      "Average test loss: 0.0025839830835660297\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09770477314127816\n",
      "Average test loss: 0.003672637178459101\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09692008797327678\n",
      "Average test loss: 0.0051084412019699816\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09591502367125618\n",
      "Average test loss: 0.002645673486507601\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09525320796171824\n",
      "Average test loss: 0.002751204552542832\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09477888676855299\n",
      "Average test loss: 0.002494619131502178\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09426095089647506\n",
      "Average test loss: 0.0026961037568334076\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09380901670455932\n",
      "Average test loss: 0.006656469374067254\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09334997597005633\n",
      "Average test loss: 0.002512482343448533\n",
      "Epoch 141/300\n",
      "Average training loss: 0.12986111229658126\n",
      "Average test loss: 0.0026085873552494578\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09651179131534364\n",
      "Average test loss: 0.002545246530738142\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09409564216269387\n",
      "Average test loss: 0.002514114137221542\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09293265532122717\n",
      "Average test loss: 0.0025026478057520256\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09205000466770596\n",
      "Average test loss: 0.002523380644722945\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09186584683921602\n",
      "Average test loss: 0.0025373010707812177\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09357370342148674\n",
      "Average test loss: 0.0025068552498188285\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09108171696795357\n",
      "Average test loss: 0.005350194105257591\n",
      "Epoch 149/300\n",
      "Average training loss: 0.090637604534626\n",
      "Average test loss: 0.002849668797519472\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09066134901510345\n",
      "Average test loss: 0.002501486709755328\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09001388582918379\n",
      "Average test loss: 0.10340882326496972\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08973283243179321\n",
      "Average test loss: 0.002569856743224793\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08921848939524757\n",
      "Average test loss: 0.0028664730516158874\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08981273067328666\n",
      "Average test loss: 0.0024778211559686398\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08838518385754691\n",
      "Average test loss: 0.005921578584031926\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08859086798628171\n",
      "Average test loss: 0.005037574548895161\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08829213944408629\n",
      "Average test loss: 0.0027262883730646637\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0877988164027532\n",
      "Average test loss: 0.002643026455408997\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08733127248287201\n",
      "Average test loss: 0.002501339447374145\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08667815760771433\n",
      "Average test loss: 0.002491319145593378\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09029803204536438\n",
      "Average test loss: 0.0028943948172446754\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10169199984603458\n",
      "Average test loss: 0.0025580533139614595\n",
      "Epoch 163/300\n",
      "Average training loss: 0.12169832514392005\n",
      "Average test loss: 0.0033068647819260756\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10609989126523335\n",
      "Average test loss: 0.002631878755158848\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09328629911608166\n",
      "Average test loss: 0.002498373954660363\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09038240445984734\n",
      "Average test loss: 0.002694263338421782\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08854582159386741\n",
      "Average test loss: 0.0025245032579534585\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08792021989160113\n",
      "Average test loss: 0.002484390285383496\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08742266105943255\n",
      "Average test loss: 0.0028365498253454763\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08691305638684167\n",
      "Average test loss: 0.00250092800706625\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08842057361867693\n",
      "Average test loss: 0.0024791220606615147\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08718939707014296\n",
      "Average test loss: 0.002473093891930249\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08618265347679456\n",
      "Average test loss: 0.0024958195195843775\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0858352294365565\n",
      "Average test loss: 0.002540818602260616\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08528233105606503\n",
      "Average test loss: 0.0026542197863260904\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08559982994529935\n",
      "Average test loss: 0.002499537531286478\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08510322539011637\n",
      "Average test loss: 0.0025615424615227514\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08485460911194484\n",
      "Average test loss: 0.002521516163729959\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08515624977151552\n",
      "Average test loss: 0.03199990711423258\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0840600611699952\n",
      "Average test loss: 0.002531109389124645\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08456223186519411\n",
      "Average test loss: 0.0025050787286212046\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0851425990594758\n",
      "Average test loss: 0.002701830755298336\n",
      "Epoch 183/300\n",
      "Average training loss: 4.527779641787211\n",
      "Average test loss: 5.597099784798092\n",
      "Epoch 184/300\n",
      "Average training loss: 2.353635391553243\n",
      "Average test loss: 0.004247732248985105\n",
      "Epoch 185/300\n",
      "Average training loss: 1.554787213537428\n",
      "Average test loss: 42.95699381150057\n",
      "Epoch 186/300\n",
      "Average training loss: 1.1200123031404283\n",
      "Average test loss: 0.005306370500061247\n",
      "Epoch 187/300\n",
      "Average training loss: 0.8588840497334799\n",
      "Average test loss: 2.404525954169532\n",
      "Epoch 188/300\n",
      "Average training loss: 0.6644868607521057\n",
      "Average test loss: 0.0029971201059718925\n",
      "Epoch 189/300\n",
      "Average training loss: 0.5117817679246267\n",
      "Average test loss: 0.0028260785817272134\n",
      "Epoch 190/300\n",
      "Average training loss: 0.39964657775561013\n",
      "Average test loss: 0.020754594222539002\n",
      "Epoch 191/300\n",
      "Average training loss: 0.31899261514345806\n",
      "Average test loss: 0.002776718307700422\n",
      "Epoch 192/300\n",
      "Average training loss: 0.26052843590577446\n",
      "Average test loss: 0.0026792665786213345\n",
      "Epoch 193/300\n",
      "Average training loss: 0.2165295147895813\n",
      "Average test loss: 0.002762803304526541\n",
      "Epoch 194/300\n",
      "Average training loss: 0.18666020986768936\n",
      "Average test loss: 0.00755589465631379\n",
      "Epoch 195/300\n",
      "Average training loss: 0.16462975423865847\n",
      "Average test loss: 0.002633496298558182\n",
      "Epoch 196/300\n",
      "Average training loss: 0.1513921012878418\n",
      "Average test loss: 0.0026089731042997703\n",
      "Epoch 197/300\n",
      "Average training loss: 0.13675303202205233\n",
      "Average test loss: 0.0025958509383102258\n",
      "Epoch 198/300\n",
      "Average training loss: 0.12765297471814685\n",
      "Average test loss: 0.002519219179844691\n",
      "Epoch 199/300\n",
      "Average training loss: 0.12180379925833808\n",
      "Average test loss: 0.002873337069319354\n",
      "Epoch 200/300\n",
      "Average training loss: 0.11511442675855425\n",
      "Average test loss: 0.0025454575647082594\n",
      "Epoch 201/300\n",
      "Average training loss: 0.1111411613954438\n",
      "Average test loss: 0.0036512025956892307\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10866291866699855\n",
      "Average test loss: 25.001600458357068\n",
      "Epoch 203/300\n",
      "Average training loss: 0.1038680088851187\n",
      "Average test loss: 0.002499499744632178\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10093475311332278\n",
      "Average test loss: 0.0025411026868969204\n",
      "Epoch 205/300\n",
      "Average training loss: 0.13465617279873954\n",
      "Average test loss: 0.0025078815426677467\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09899727761745453\n",
      "Average test loss: 0.0025013471562415363\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0963689470688502\n",
      "Average test loss: 0.0024800366188089053\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09473682540655136\n",
      "Average test loss: 0.0025903698263896836\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09336529790030586\n",
      "Average test loss: 0.0024792162286531596\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09195006222195096\n",
      "Average test loss: 0.0025032760705798866\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09113406584660212\n",
      "Average test loss: 0.0025339714638474915\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09085192499889268\n",
      "Average test loss: 0.00250535066301624\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08884444071186913\n",
      "Average test loss: 0.002571236711719798\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09496441515286763\n",
      "Average test loss: 0.002492282261347605\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08754120484987894\n",
      "Average test loss: 0.0025162673925773964\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08664200743701723\n",
      "Average test loss: 0.002753977945074439\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08650689786010318\n",
      "Average test loss: 0.0029252086131730014\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08574586655365096\n",
      "Average test loss: 0.0025621054880321024\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08750395307275984\n",
      "Average test loss: 0.002558938048366043\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08538529889119996\n",
      "Average test loss: 0.0025683926304595336\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08423479722605812\n",
      "Average test loss: 0.002656623679213226\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08755711403489112\n",
      "Average test loss: 0.0024944073396424454\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08364140760236316\n",
      "Average test loss: 0.0038404456772324113\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08373697848452462\n",
      "Average test loss: 0.0032420282173487875\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08369291300243802\n",
      "Average test loss: 0.027180304952793652\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08337859655088849\n",
      "Average test loss: 0.002648150176430742\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08291648259427813\n",
      "Average test loss: 0.0025610635404785475\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08261288723680708\n",
      "Average test loss: 0.0025515403679261606\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08272280035416285\n",
      "Average test loss: 0.002836933410416047\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08303183578120338\n",
      "Average test loss: 0.005368150361710125\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08228408318758011\n",
      "Average test loss: 0.002504713095931543\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08173448912302653\n",
      "Average test loss: 0.002634773145843711\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08339475301570362\n",
      "Average test loss: 0.002866018821588821\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08118110203080707\n",
      "Average test loss: 0.0025161264627758\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08103944675127665\n",
      "Average test loss: 0.0024987957992901404\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08376070209344229\n",
      "Average test loss: 0.002523268283241325\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08072263074583477\n",
      "Average test loss: 0.002582755126266016\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08193564363982943\n",
      "Average test loss: 0.002522477494345771\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08018367719650268\n",
      "Average test loss: 0.0025397411716274088\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08021040703852972\n",
      "Average test loss: 0.002942182579388221\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08092016410827636\n",
      "Average test loss: 0.002595173489509357\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07990111184782452\n",
      "Average test loss: 0.002545063945982191\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07957086630000008\n",
      "Average test loss: 0.002500953844748437\n",
      "Epoch 244/300\n",
      "Average training loss: 0.17865603748295042\n",
      "Average test loss: 0.0025667019254631464\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09475664442777634\n",
      "Average test loss: 0.002625846212522851\n",
      "Epoch 246/300\n",
      "Average training loss: 0.11166205583016078\n",
      "Average test loss: 0.007154777993344599\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09327603475252787\n",
      "Average test loss: 0.0025041457238710587\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08537625881698396\n",
      "Average test loss: 0.0026918708752426837\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08326685697833697\n",
      "Average test loss: 0.0025640159563885797\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08159482516182794\n",
      "Average test loss: 0.00290290188541015\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08156807271970642\n",
      "Average test loss: 0.002728981295704014\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0800313096774949\n",
      "Average test loss: 0.0025529709204824436\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07987287626663844\n",
      "Average test loss: 0.0036784722469747066\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07958358583185408\n",
      "Average test loss: 0.0025463063785185415\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07954091692633099\n",
      "Average test loss: 0.002542180296757983\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07909868201613426\n",
      "Average test loss: 0.0025580450625469287\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07938181663221783\n",
      "Average test loss: 0.0027175688745660915\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08002862255109681\n",
      "Average test loss: 0.003248753045582109\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08066969331767825\n",
      "Average test loss: 0.0025565275626981423\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07846284665001763\n",
      "Average test loss: 0.002745918725306789\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07890389289458592\n",
      "Average test loss: 0.002971675719031029\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0789199980629815\n",
      "Average test loss: 0.0026146893525082204\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07848078601227866\n",
      "Average test loss: 0.0025933303931314085\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07882206041945351\n",
      "Average test loss: 0.0028235230683866476\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07811272104581198\n",
      "Average test loss: 0.002628526794517206\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07799551551871829\n",
      "Average test loss: 0.002764755640178919\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08164659402105544\n",
      "Average test loss: 0.002548880556287865\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07752043732669618\n",
      "Average test loss: 0.003644158175215125\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07724908059835434\n",
      "Average test loss: 0.0025505911492639116\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07746569380495283\n",
      "Average test loss: 0.003043527559687694\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07726397025585174\n",
      "Average test loss: 0.00259754841029644\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07975293636322021\n",
      "Average test loss: 0.0025945749620182646\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07801235170496834\n",
      "Average test loss: 0.0031477995759083164\n",
      "Epoch 274/300\n",
      "Average training loss: 0.2097359189192454\n",
      "Average test loss: 0.0025884946430515914\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0982068751917945\n",
      "Average test loss: 0.0024937015970547993\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08984117080105676\n",
      "Average test loss: 0.0025180271859798166\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08501989570260048\n",
      "Average test loss: 0.0025623586333046356\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08184196205271616\n",
      "Average test loss: 0.002876549546296398\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0799496746526824\n",
      "Average test loss: 0.0025456068880028195\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07867683623896705\n",
      "Average test loss: 0.0025577305680554775\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07770101350545883\n",
      "Average test loss: 0.002553732164411081\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07882737264037132\n",
      "Average test loss: 0.0026112233014363383\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07646739898787605\n",
      "Average test loss: 0.002541592897847295\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0826403090523349\n",
      "Average test loss: 0.0025579613770047825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07670234188768599\n",
      "Average test loss: 0.0025733394026756285\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07669173014826244\n",
      "Average test loss: 0.0025766414431855084\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07609351139598422\n",
      "Average test loss: 0.05365047010448244\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07684444502989451\n",
      "Average test loss: 0.002578317276098662\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07649408631192313\n",
      "Average test loss: 0.0025844667027187015\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07621957729260127\n",
      "Average test loss: 0.0026089764629594154\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07709931413994896\n",
      "Average test loss: 0.003227939594950941\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07691570350196626\n",
      "Average test loss: 0.0025570238698273897\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07655148906177944\n",
      "Average test loss: 0.0026480517290118667\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07670246042145623\n",
      "Average test loss: 0.0025389980507186717\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08919421586725447\n",
      "Average test loss: 0.00256271862031685\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07700414562887616\n",
      "Average test loss: 0.0025716507650083965\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08434545134835773\n",
      "Average test loss: 30.860853310679396\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10492919714914428\n",
      "Average test loss: 0.0025272621851828363\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07972204315662385\n",
      "Average test loss: 0.002519894936639402\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07672294021977319\n",
      "Average test loss: 0.002537163116658727\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.693656018998889\n",
      "Average test loss: 0.006501084682842096\n",
      "Epoch 2/300\n",
      "Average training loss: 6.33647338104248\n",
      "Average test loss: 0.005483038883656264\n",
      "Epoch 3/300\n",
      "Average training loss: 4.9167749799092615\n",
      "Average test loss: 0.004335495565500524\n",
      "Epoch 4/300\n",
      "Average training loss: 4.14593342060513\n",
      "Average test loss: 0.003949775984510779\n",
      "Epoch 5/300\n",
      "Average training loss: 3.3071487291124133\n",
      "Average test loss: 0.005208664894724885\n",
      "Epoch 6/300\n",
      "Average training loss: 2.7670505572424995\n",
      "Average test loss: 0.0037159771494981314\n",
      "Epoch 7/300\n",
      "Average training loss: 2.367129642274645\n",
      "Average test loss: 0.003656040364462468\n",
      "Epoch 8/300\n",
      "Average training loss: 2.0299283862643773\n",
      "Average test loss: 0.003273205200417174\n",
      "Epoch 9/300\n",
      "Average training loss: 1.7220028262668186\n",
      "Average test loss: 0.003995865145077308\n",
      "Epoch 10/300\n",
      "Average training loss: 1.5045565425025091\n",
      "Average test loss: 0.0031641403906461267\n",
      "Epoch 11/300\n",
      "Average training loss: 1.2932173181109958\n",
      "Average test loss: 0.003385640281356043\n",
      "Epoch 12/300\n",
      "Average training loss: 1.1386336353090074\n",
      "Average test loss: 0.003454457293368048\n",
      "Epoch 13/300\n",
      "Average training loss: 0.9931606156031291\n",
      "Average test loss: 0.002779833094113403\n",
      "Epoch 14/300\n",
      "Average training loss: 0.8641210505697462\n",
      "Average test loss: 0.002805281799286604\n",
      "Epoch 15/300\n",
      "Average training loss: 0.7522191350724962\n",
      "Average test loss: 0.0029907160167478852\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6552331495285034\n",
      "Average test loss: 0.002508706724478139\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5696297509405348\n",
      "Average test loss: 0.0028434399471928677\n",
      "Epoch 18/300\n",
      "Average training loss: 0.49683793777889673\n",
      "Average test loss: 0.004456491220742464\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4342468670739068\n",
      "Average test loss: 0.0030581271720843185\n",
      "Epoch 20/300\n",
      "Average training loss: 0.3817764259709252\n",
      "Average test loss: 0.0023594048608922297\n",
      "Epoch 21/300\n",
      "Average training loss: 0.33835447563065424\n",
      "Average test loss: 0.0027405911152147584\n",
      "Epoch 22/300\n",
      "Average training loss: 0.30240277716848585\n",
      "Average test loss: 0.002352972985762689\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2695504990021388\n",
      "Average test loss: 0.0022623100249717634\n",
      "Epoch 24/300\n",
      "Average training loss: 0.24309706166055467\n",
      "Average test loss: 0.003743610127311614\n",
      "Epoch 25/300\n",
      "Average training loss: 0.22155025015936958\n",
      "Average test loss: 0.0023855567063308424\n",
      "Epoch 26/300\n",
      "Average training loss: 0.202086191839642\n",
      "Average test loss: 0.0020625618058774204\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1853218537039227\n",
      "Average test loss: 0.016496259801917604\n",
      "Epoch 28/300\n",
      "Average training loss: 0.17133174578348795\n",
      "Average test loss: 0.0024406005462838543\n",
      "Epoch 29/300\n",
      "Average training loss: 0.16027850258350373\n",
      "Average test loss: 0.0021029119290825395\n",
      "Epoch 30/300\n",
      "Average training loss: 0.14964165792862574\n",
      "Average test loss: 19.918808328416613\n",
      "Epoch 31/300\n",
      "Average training loss: 0.14091482993629242\n",
      "Average test loss: 0.0020943785595397154\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13413183320893182\n",
      "Average test loss: 0.0021811302758546337\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12742608765761057\n",
      "Average test loss: 0.0018631756864488124\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1201934001578225\n",
      "Average test loss: 0.0021970887292797367\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11607192597786585\n",
      "Average test loss: 0.0018122550503661234\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10997594322098626\n",
      "Average test loss: 0.0023250048549638856\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10594902219374974\n",
      "Average test loss: 0.0019185289001713197\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1027382201883528\n",
      "Average test loss: 0.0019036671532731916\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0981616431540913\n",
      "Average test loss: 0.0026065323814335795\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09532297595342\n",
      "Average test loss: 0.0021346418294641706\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09208353609508939\n",
      "Average test loss: 0.002013015250261459\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08917587090863122\n",
      "Average test loss: 0.0021606008400105767\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0871053686870469\n",
      "Average test loss: 0.0017276208889153268\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09002035388681624\n",
      "Average test loss: 0.0019470501743877927\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08347266010112232\n",
      "Average test loss: 0.0020384688263552055\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08038807707362705\n",
      "Average test loss: 0.0017893996023469502\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07868320263094372\n",
      "Average test loss: 0.0017618545490420527\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0769133873184522\n",
      "Average test loss: 0.0017566569627573093\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07578285674916373\n",
      "Average test loss: 0.001666481423088246\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07397885311312145\n",
      "Average test loss: 0.001740934532549646\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07316617289516661\n",
      "Average test loss: 0.001740748546189732\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07386532409323586\n",
      "Average test loss: 0.0016735975495022204\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07963159109486474\n",
      "Average test loss: 0.0017220448452668884\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0739751168390115\n",
      "Average test loss: 0.0019450703605802522\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0718836382329464\n",
      "Average test loss: 0.002319122159025735\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06984175332056151\n",
      "Average test loss: 0.0016848361627716157\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06912914907601145\n",
      "Average test loss: 0.0016733845012883346\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06904350707266066\n",
      "Average test loss: 0.0016430731397122146\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06764619982242584\n",
      "Average test loss: 0.0016311592511418793\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06804247673352559\n",
      "Average test loss: 0.0017657176960880557\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06695658655630218\n",
      "Average test loss: 0.004218473877757788\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06639756802055571\n",
      "Average test loss: 0.001631242190591163\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06586820075247023\n",
      "Average test loss: 0.0016140282463489308\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06549964634743002\n",
      "Average test loss: 0.0016626067875574033\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06542622774177127\n",
      "Average test loss: 0.0018149848255432314\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06631078698900011\n",
      "Average test loss: 0.0016509967007570796\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06457210372885068\n",
      "Average test loss: 0.03558884637885624\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06374571000536283\n",
      "Average test loss: 0.0016453480820895897\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0638225831521882\n",
      "Average test loss: 0.0016132333657393853\n",
      "Epoch 70/300\n",
      "Average training loss: 0.063046520575881\n",
      "Average test loss: 0.0024630885426368977\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06265721546941333\n",
      "Average test loss: 0.0016362075349316\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06422228598263528\n",
      "Average test loss: 0.0025519855208694935\n",
      "Epoch 73/300\n",
      "Average training loss: 0.061920486013094585\n",
      "Average test loss: 0.0015750547941360208\n",
      "Epoch 74/300\n",
      "Average training loss: 0.061795913991000914\n",
      "Average test loss: 0.0022830936163663862\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06166064823667208\n",
      "Average test loss: 0.0016018515953587162\n",
      "Epoch 76/300\n",
      "Average training loss: 0.060914521820015374\n",
      "Average test loss: 0.001677463349679278\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06101931574609545\n",
      "Average test loss: 0.0015779390782117843\n",
      "Epoch 78/300\n",
      "Average training loss: 0.061449531578355365\n",
      "Average test loss: 0.0018648243257775902\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07045932657851113\n",
      "Average test loss: 0.0018361030687681502\n",
      "Epoch 80/300\n",
      "Average training loss: 0.061777856919500565\n",
      "Average test loss: 0.0015974320431964264\n",
      "Epoch 81/300\n",
      "Average training loss: 0.061082480549812315\n",
      "Average test loss: 0.0016610675788381033\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0602792545888159\n",
      "Average test loss: 0.021442461897929508\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05984852280550533\n",
      "Average test loss: 0.001636859408269326\n",
      "Epoch 84/300\n",
      "Average training loss: 0.059811921964089076\n",
      "Average test loss: 0.001666526454500854\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05958619910478592\n",
      "Average test loss: 0.001667861558807393\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05959641436073515\n",
      "Average test loss: 0.001666410160354442\n",
      "Epoch 87/300\n",
      "Average training loss: 0.061370158546500735\n",
      "Average test loss: 0.0017996897394251493\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05918456380897098\n",
      "Average test loss: 0.005173759566206071\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05853258497185177\n",
      "Average test loss: 0.005181992666174968\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05852396058042844\n",
      "Average test loss: 0.0017457712328889303\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05855555906229549\n",
      "Average test loss: 0.001751775488567849\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05821825199325879\n",
      "Average test loss: 0.0022253907372554145\n",
      "Epoch 93/300\n",
      "Average training loss: 0.057926112473011014\n",
      "Average test loss: 0.002568154655603899\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05807850795984268\n",
      "Average test loss: 0.0020740580355955497\n",
      "Epoch 95/300\n",
      "Average training loss: 0.058121175646781924\n",
      "Average test loss: 0.0016444819564413694\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05724882682826784\n",
      "Average test loss: 0.001627713751565251\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05729132791360219\n",
      "Average test loss: 0.00196388763634281\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05717595402730836\n",
      "Average test loss: 0.001721226803596235\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05995346496171421\n",
      "Average test loss: 0.010444342681931125\n",
      "Epoch 100/300\n",
      "Average training loss: 0.059942821572224296\n",
      "Average test loss: 0.001676187076502376\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05699971243407991\n",
      "Average test loss: 0.0036212443293382724\n",
      "Epoch 102/300\n",
      "Average training loss: 0.058314544369777045\n",
      "Average test loss: 0.0016445302083674404\n",
      "Epoch 103/300\n",
      "Average training loss: 0.056286729630496767\n",
      "Average test loss: 0.001661914865900245\n",
      "Epoch 104/300\n",
      "Average training loss: 0.056149088899294534\n",
      "Average test loss: 0.002141972841074069\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0561070892545912\n",
      "Average test loss: 0.0037312259431928397\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05593077153629727\n",
      "Average test loss: 0.0028016415401879284\n",
      "Epoch 107/300\n",
      "Average training loss: 0.055782250573237734\n",
      "Average test loss: 0.0016886345162573787\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05623865995142195\n",
      "Average test loss: 0.001641227155406442\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0563724999792046\n",
      "Average test loss: 0.00545349711096949\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0564870728817251\n",
      "Average test loss: 0.0016153646138393216\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05534293125073115\n",
      "Average test loss: 0.007034712707416879\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05535212537315157\n",
      "Average test loss: 0.0017896368089649413\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05517869337399801\n",
      "Average test loss: 0.0017139830977345506\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05477756258183056\n",
      "Average test loss: 0.0016674112726209893\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05494620285431544\n",
      "Average test loss: 0.0022949519496825005\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0548034465610981\n",
      "Average test loss: 0.001613652766113066\n",
      "Epoch 117/300\n",
      "Average training loss: 0.058374315900935066\n",
      "Average test loss: 0.0016867577241112789\n",
      "Epoch 118/300\n",
      "Average training loss: 0.054884823948144916\n",
      "Average test loss: 0.0017117229549007283\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05593154827422566\n",
      "Average test loss: 0.0038237618630131086\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05492312662800153\n",
      "Average test loss: 0.0016156979066630204\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05417574731508891\n",
      "Average test loss: 0.0016273932092719609\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0578074727124638\n",
      "Average test loss: 0.0016475083633429475\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0537726868821515\n",
      "Average test loss: 0.0016092441251708402\n",
      "Epoch 124/300\n",
      "Average training loss: 0.053754159602853985\n",
      "Average test loss: 0.0025226674963616663\n",
      "Epoch 125/300\n",
      "Average training loss: 0.053564392152759766\n",
      "Average test loss: 0.001626914319064882\n",
      "Epoch 126/300\n",
      "Average training loss: 0.053675278852383296\n",
      "Average test loss: 0.001779318222983016\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05383551160825623\n",
      "Average test loss: 0.0016399570608304607\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05338168059786161\n",
      "Average test loss: 0.0016373037468228075\n",
      "Epoch 129/300\n",
      "Average training loss: 0.053597778624958466\n",
      "Average test loss: 0.0016171354266504446\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05387319655219714\n",
      "Average test loss: 0.002694463872971634\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05338021659023232\n",
      "Average test loss: 0.0018526606454203527\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05306801310512755\n",
      "Average test loss: 0.0016240981726182832\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0533649090329806\n",
      "Average test loss: 0.0016764862600507008\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05313869919379552\n",
      "Average test loss: 0.0017283236275737485\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06056840433014764\n",
      "Average test loss: 0.0016701127842275632\n",
      "Epoch 136/300\n",
      "Average training loss: 0.053504981921778785\n",
      "Average test loss: 0.001750399621617463\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05380143309964074\n",
      "Average test loss: 0.0016515074782590899\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05276131280594402\n",
      "Average test loss: 0.01406971509485609\n",
      "Epoch 139/300\n",
      "Average training loss: 0.053433554490407306\n",
      "Average test loss: 0.0017625048786608709\n",
      "Epoch 140/300\n",
      "Average training loss: 0.052170104765229756\n",
      "Average test loss: 0.0016791543951258064\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05263530037800471\n",
      "Average test loss: 0.0016430131547773878\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05237911218073633\n",
      "Average test loss: 0.0016597608469633592\n",
      "Epoch 143/300\n",
      "Average training loss: 0.053317110419273374\n",
      "Average test loss: 0.0016189052573043026\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05360074199239413\n",
      "Average test loss: 0.0017332459662833975\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05190718184908231\n",
      "Average test loss: 0.0022106548570510415\n",
      "Epoch 146/300\n",
      "Average training loss: 0.051841139690743554\n",
      "Average test loss: 0.0016736437424810396\n",
      "Epoch 147/300\n",
      "Average training loss: 0.054463116069634755\n",
      "Average test loss: 0.0017574031117061773\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05168793494171566\n",
      "Average test loss: 0.001697762805244161\n",
      "Epoch 149/300\n",
      "Average training loss: 0.051693438629309335\n",
      "Average test loss: 0.0017030314197763801\n",
      "Epoch 150/300\n",
      "Average training loss: 0.051746068547169365\n",
      "Average test loss: 0.0016341604177012212\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05246067850788434\n",
      "Average test loss: 0.0019489544304087758\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05159301510453224\n",
      "Average test loss: 0.0017830237053955596\n",
      "Epoch 153/300\n",
      "Average training loss: 0.051485639105240504\n",
      "Average test loss: 0.0027675629841784634\n",
      "Epoch 154/300\n",
      "Average training loss: 0.053295466528998484\n",
      "Average test loss: 0.0017469269605353474\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05164578723907471\n",
      "Average test loss: 0.002811747093581491\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05125722010268106\n",
      "Average test loss: 0.0017849803383772572\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05171377087301678\n",
      "Average test loss: 0.002010662658864425\n",
      "Epoch 158/300\n",
      "Average training loss: 0.051716594692733556\n",
      "Average test loss: 0.002884480488383108\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05161769979861047\n",
      "Average test loss: 0.007738192359606425\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05263399250970947\n",
      "Average test loss: 0.001790025827785333\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05116682271493806\n",
      "Average test loss: 0.0019591164991466536\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0508973695702023\n",
      "Average test loss: 0.001653697123647564\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05157986472381486\n",
      "Average test loss: 0.0016616954519930813\n",
      "Epoch 164/300\n",
      "Average training loss: 0.050847594764497545\n",
      "Average test loss: 0.002311851361559497\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05284588939944903\n",
      "Average test loss: 0.002589385128993955\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05090403042899238\n",
      "Average test loss: 0.0017847747951745987\n",
      "Epoch 167/300\n",
      "Average training loss: 0.050639780577686096\n",
      "Average test loss: 0.0017870835074120098\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05105790123343468\n",
      "Average test loss: 0.00165917744477176\n",
      "Epoch 169/300\n",
      "Average training loss: 0.051317356102996405\n",
      "Average test loss: 0.0019885836102896265\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05054234688811832\n",
      "Average test loss: 0.0021990516437217595\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05073032224509451\n",
      "Average test loss: 0.0018362589543685318\n",
      "Epoch 172/300\n",
      "Average training loss: 0.050839314854807326\n",
      "Average test loss: 0.0018489721725280914\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05116741780108876\n",
      "Average test loss: 0.001813193819589085\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05055328068137169\n",
      "Average test loss: 0.0025607499554753306\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05147730848193169\n",
      "Average test loss: 0.0021220078188925982\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05022271051009496\n",
      "Average test loss: 0.0024516137659342753\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05019538611835903\n",
      "Average test loss: 0.001798584409782456\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05367827862666713\n",
      "Average test loss: 0.001825620060786605\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05020204416248533\n",
      "Average test loss: 0.001761558468764027\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05020862325694826\n",
      "Average test loss: 0.0021696471193184456\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05033414708574613\n",
      "Average test loss: 0.09584864991696344\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05003985314236747\n",
      "Average test loss: 0.0018773669401804607\n",
      "Epoch 183/300\n",
      "Average training loss: 0.050075692902008695\n",
      "Average test loss: 0.0018588065664387411\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08236536969078911\n",
      "Average test loss: 0.0017189289843663573\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05764241446057956\n",
      "Average test loss: 0.0016385771673586634\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05348899803227848\n",
      "Average test loss: 0.0017358701061457396\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05166889387865861\n",
      "Average test loss: 0.0019371670075795716\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0508530535267459\n",
      "Average test loss: 0.0017526644761156705\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05043898025486204\n",
      "Average test loss: 0.0041377446167171\n",
      "Epoch 190/300\n",
      "Average training loss: 0.050296558340390526\n",
      "Average test loss: 0.0016956648354729017\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05032176787654559\n",
      "Average test loss: 0.0024310402928127183\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04985626633630859\n",
      "Average test loss: 0.001688452577114933\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05039498580329948\n",
      "Average test loss: 0.0017028180236617725\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10289232564965883\n",
      "Average test loss: 0.001982474776916206\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08444467928674486\n",
      "Average test loss: 0.001662326857344144\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06711227789852355\n",
      "Average test loss: 0.001661452601974209\n",
      "Epoch 197/300\n",
      "Average training loss: 0.061791030993064246\n",
      "Average test loss: 0.0021616995870653126\n",
      "Epoch 198/300\n",
      "Average training loss: 0.058461764636966916\n",
      "Average test loss: 0.06876262457503213\n",
      "Epoch 199/300\n",
      "Average training loss: 0.055922679271962906\n",
      "Average test loss: 0.0025692181032564904\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05397400966948933\n",
      "Average test loss: 0.010118903935369518\n",
      "Epoch 201/300\n",
      "Average training loss: 0.052671672205130256\n",
      "Average test loss: 0.0016470137497203218\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05131675453649627\n",
      "Average test loss: 0.002674318440672424\n",
      "Epoch 203/300\n",
      "Average training loss: 0.052005383753114275\n",
      "Average test loss: 0.0021666980588601697\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04985478639271524\n",
      "Average test loss: 0.0017000473057851196\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04968027991718716\n",
      "Average test loss: 4.434353906154633\n",
      "Epoch 206/300\n",
      "Average training loss: 0.050130690859423746\n",
      "Average test loss: 0.00187410087366071\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04998802911904123\n",
      "Average test loss: 0.0017208705455478695\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04992329441507657\n",
      "Average test loss: 0.001824513830865423\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05100482987032996\n",
      "Average test loss: 0.0017860944925083054\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04996537816524506\n",
      "Average test loss: 0.0017048553329788976\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04945297975341479\n",
      "Average test loss: 0.001686724411478887\n",
      "Epoch 212/300\n",
      "Average training loss: 0.049748871544996895\n",
      "Average test loss: 0.001693112008480562\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05269668138358328\n",
      "Average test loss: 0.01448973809617261\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04957231073909336\n",
      "Average test loss: 0.0016548772658117944\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04988601420654191\n",
      "Average test loss: 0.0028787176004714437\n",
      "Epoch 216/300\n",
      "Average training loss: 0.050086673034562004\n",
      "Average test loss: 0.0019213820872828365\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04917101523942417\n",
      "Average test loss: 0.004516587652266026\n",
      "Epoch 218/300\n",
      "Average training loss: 0.049385255141390694\n",
      "Average test loss: 0.0023117363699194458\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05021167720688714\n",
      "Average test loss: 0.0016705586105171177\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05252503503031201\n",
      "Average test loss: 0.0031691278159204457\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04958136281702254\n",
      "Average test loss: 0.0019732888394759763\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04904287418060833\n",
      "Average test loss: 0.0018461471948151788\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04900876306825214\n",
      "Average test loss: 0.00173643892010053\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04947084688809183\n",
      "Average test loss: 0.0016751812738883827\n",
      "Epoch 225/300\n",
      "Average training loss: 0.048822867453098294\n",
      "Average test loss: 0.0018846443318244483\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0499617006248898\n",
      "Average test loss: 0.0019527276009838614\n",
      "Epoch 227/300\n",
      "Average training loss: 0.048965153843164445\n",
      "Average test loss: 0.0022002254424409733\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04909354201952616\n",
      "Average test loss: 0.0019188450071960688\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04936197688844469\n",
      "Average test loss: 0.0016821569969049759\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05170939213368628\n",
      "Average test loss: 0.0018915933647917376\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048762324700752896\n",
      "Average test loss: 0.0027221796400845053\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0488503236869971\n",
      "Average test loss: 0.0068405895413209995\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05250253797902001\n",
      "Average test loss: 0.0016801788819332919\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04863995690478219\n",
      "Average test loss: 0.001714027239009738\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04904724998937713\n",
      "Average test loss: 0.0035180243994626732\n",
      "Epoch 236/300\n",
      "Average training loss: 0.048712595217757754\n",
      "Average test loss: 0.0020713545673837265\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05089223689834277\n",
      "Average test loss: 0.0037499133843100735\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04876183099548022\n",
      "Average test loss: 0.001805575458229416\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04855418295992745\n",
      "Average test loss: 0.006848719226403369\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04902439591619703\n",
      "Average test loss: 0.0019015900259837508\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04856984723607699\n",
      "Average test loss: 0.0017264356949470109\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04902407028608852\n",
      "Average test loss: 0.0017961643095024758\n",
      "Epoch 243/300\n",
      "Average training loss: 0.048432860099607046\n",
      "Average test loss: 0.0024775295682872337\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04856281679868698\n",
      "Average test loss: 0.0016495285232603137\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04968035071425968\n",
      "Average test loss: 0.003018485183517138\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04838041635685497\n",
      "Average test loss: 0.006262149530773362\n",
      "Epoch 247/300\n",
      "Average training loss: 0.048693904764122435\n",
      "Average test loss: 0.0017748624063614342\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04931959921783871\n",
      "Average test loss: 0.003947253424053391\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04826543121205436\n",
      "Average test loss: 0.0016998372404939599\n",
      "Epoch 250/300\n",
      "Average training loss: 0.049443642718924416\n",
      "Average test loss: 0.001712364887818694\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04833220553398132\n",
      "Average test loss: 0.0017704456927668717\n",
      "Epoch 252/300\n",
      "Average training loss: 0.048307609611087376\n",
      "Average test loss: 0.0016985820569097995\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04845073770152198\n",
      "Average test loss: 0.0020282556314228308\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04866593780120214\n",
      "Average test loss: 0.001821037139536606\n",
      "Epoch 255/300\n",
      "Average training loss: 0.048526960296763316\n",
      "Average test loss: 0.0016847532074898482\n",
      "Epoch 256/300\n",
      "Average training loss: 0.049380959828694664\n",
      "Average test loss: 0.0017253424399014975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04826757116119067\n",
      "Average test loss: 0.0017874294405596123\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04867686470018493\n",
      "Average test loss: 0.0017312192546410693\n",
      "Epoch 259/300\n",
      "Average training loss: 0.047996899518701765\n",
      "Average test loss: 0.0018685123524111178\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04813978979984919\n",
      "Average test loss: 0.0018373102854109472\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04909354870186912\n",
      "Average test loss: 0.0016917579587962892\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0482879671090179\n",
      "Average test loss: 0.0017168191679649883\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04781343844532967\n",
      "Average test loss: 0.0017271218066000275\n",
      "Epoch 264/300\n",
      "Average training loss: 0.047987727059258356\n",
      "Average test loss: 0.0017252536436749828\n",
      "Epoch 265/300\n",
      "Average training loss: 0.050207676072915396\n",
      "Average test loss: 0.0017131833992898465\n",
      "Epoch 266/300\n",
      "Average training loss: 0.047664109001557035\n",
      "Average test loss: 0.001672754154437118\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04788466824094455\n",
      "Average test loss: 0.0017726979398479065\n",
      "Epoch 268/300\n",
      "Average training loss: 0.048326713310347665\n",
      "Average test loss: 0.0017740182321932581\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0486980874604649\n",
      "Average test loss: 0.018172123399873574\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04891409838861889\n",
      "Average test loss: 0.0022071652739412257\n",
      "Epoch 271/300\n",
      "Average training loss: 0.048223681721422405\n",
      "Average test loss: 0.010550556364158789\n",
      "Epoch 272/300\n",
      "Average training loss: 0.048553085734446846\n",
      "Average test loss: 0.0017524978404657708\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04887222336067094\n",
      "Average test loss: 0.001747386608272791\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0475723988711834\n",
      "Average test loss: 0.0016909148944541813\n",
      "Epoch 275/300\n",
      "Average training loss: 0.048020900703138775\n",
      "Average test loss: 0.0017331394596646228\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04774977829058965\n",
      "Average test loss: 0.0017462467091778915\n",
      "Epoch 277/300\n",
      "Average training loss: 0.048127571314573285\n",
      "Average test loss: 0.0017544791809179718\n",
      "Epoch 278/300\n",
      "Average training loss: 0.048314056932926175\n",
      "Average test loss: 0.0017665311354729863\n",
      "Epoch 279/300\n",
      "Average training loss: 0.047826388930281\n",
      "Average test loss: 0.0017368202166010936\n",
      "Epoch 280/300\n",
      "Average training loss: 0.048883504678805666\n",
      "Average test loss: 0.002123735738866445\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04759016112155384\n",
      "Average test loss: 0.0017617496459020508\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04778614792227745\n",
      "Average test loss: 0.00176089049430771\n",
      "Epoch 283/300\n",
      "Average training loss: 0.048244987779193456\n",
      "Average test loss: 0.0017733663079432314\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04734838280081749\n",
      "Average test loss: 0.0017345840221063959\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04767972768677606\n",
      "Average test loss: 0.001709593396116462\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0482841218246354\n",
      "Average test loss: 0.0017464507640235953\n",
      "Epoch 287/300\n",
      "Average training loss: 0.047911657018793956\n",
      "Average test loss: 0.0019117518583726552\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04783156389991442\n",
      "Average test loss: 0.0017537564944682849\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04792143964436319\n",
      "Average test loss: 0.001696896304272943\n",
      "Epoch 290/300\n",
      "Average training loss: 0.050972976270649166\n",
      "Average test loss: 0.001727037691614694\n",
      "Epoch 291/300\n",
      "Average training loss: 0.047366684574219914\n",
      "Average test loss: 0.07845458430134587\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0478870961036947\n",
      "Average test loss: 0.0030498739820387626\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04716143614384863\n",
      "Average test loss: 0.6312514060868157\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04834251512421502\n",
      "Average test loss: 0.007503016371072995\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04714986389875412\n",
      "Average test loss: 0.002190549397220214\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04739611724350187\n",
      "Average test loss: 0.00171829586636482\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04793386111325688\n",
      "Average test loss: 0.0018343802404900392\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04734310326311324\n",
      "Average test loss: 0.0018959893501467174\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04758786960442861\n",
      "Average test loss: 0.0017866303726202912\n",
      "Epoch 300/300\n",
      "Average training loss: 0.047267945739958024\n",
      "Average test loss: 0.001770921080476708\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7660.845700322469\n",
      "Average test loss: 167.31797152750028\n",
      "Epoch 2/300\n",
      "Average training loss: 14.825518615722656\n",
      "Average test loss: 226.36828956092728\n",
      "Epoch 3/300\n",
      "Average training loss: 15.02902421400282\n",
      "Average test loss: 15.087506551133261\n",
      "Epoch 4/300\n",
      "Average training loss: 12.947924169752334\n",
      "Average test loss: 0.012668712155686485\n",
      "Epoch 5/300\n",
      "Average training loss: 11.520181609259712\n",
      "Average test loss: 12.021670364194446\n",
      "Epoch 6/300\n",
      "Average training loss: 9.938513539632162\n",
      "Average test loss: 63.531209319164354\n",
      "Epoch 7/300\n",
      "Average training loss: 9.206779040866428\n",
      "Average test loss: 0.007567997059060467\n",
      "Epoch 8/300\n",
      "Average training loss: 8.270346538119846\n",
      "Average test loss: 0.0066169880073931485\n",
      "Epoch 9/300\n",
      "Average training loss: 7.3640366711086696\n",
      "Average test loss: 0.1254652697046598\n",
      "Epoch 10/300\n",
      "Average training loss: 6.87015512169732\n",
      "Average test loss: 0.005326101478603151\n",
      "Epoch 11/300\n",
      "Average training loss: 6.441118351406521\n",
      "Average test loss: 8.339110712428887\n",
      "Epoch 12/300\n",
      "Average training loss: 6.001786359151205\n",
      "Average test loss: 0.013252910299847523\n",
      "Epoch 13/300\n",
      "Average training loss: 5.15939904361301\n",
      "Average test loss: 0.7556307638540036\n",
      "Epoch 14/300\n",
      "Average training loss: 4.443865690443251\n",
      "Average test loss: 0.006545178508179056\n",
      "Epoch 15/300\n",
      "Average training loss: 3.7847030648125544\n",
      "Average test loss: 0.003710776875830359\n",
      "Epoch 16/300\n",
      "Average training loss: 3.321658184475369\n",
      "Average test loss: 0.02421113517218166\n",
      "Epoch 17/300\n",
      "Average training loss: 3.1134543972015383\n",
      "Average test loss: 0.0034121815249737765\n",
      "Epoch 18/300\n",
      "Average training loss: 2.8855624362097845\n",
      "Average test loss: 0.003215296402366625\n",
      "Epoch 19/300\n",
      "Average training loss: 2.5306096182929143\n",
      "Average test loss: 0.003362925215313832\n",
      "Epoch 20/300\n",
      "Average training loss: 2.2470220671759713\n",
      "Average test loss: 0.002927117785024974\n",
      "Epoch 21/300\n",
      "Average training loss: 1.9281344804763794\n",
      "Average test loss: 0.0028056681491434574\n",
      "Epoch 22/300\n",
      "Average training loss: 1.6699049776924981\n",
      "Average test loss: 0.002767464102142387\n",
      "Epoch 23/300\n",
      "Average training loss: 1.4591376457214356\n",
      "Average test loss: 0.0026739733713782494\n",
      "Epoch 24/300\n",
      "Average training loss: 1.2608789676030476\n",
      "Average test loss: 0.0025952651780098675\n",
      "Epoch 25/300\n",
      "Average training loss: 1.0898018380271064\n",
      "Average test loss: 0.0024872969546251827\n",
      "Epoch 26/300\n",
      "Average training loss: 0.9469863579008314\n",
      "Average test loss: 0.0029063238814059232\n",
      "Epoch 27/300\n",
      "Average training loss: 0.8251023870044284\n",
      "Average test loss: 0.0022604323791133034\n",
      "Epoch 28/300\n",
      "Average training loss: 0.7201681307156881\n",
      "Average test loss: 0.002722850976925757\n",
      "Epoch 29/300\n",
      "Average training loss: 0.6299202330907185\n",
      "Average test loss: 0.0021461899305383362\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5514068817032708\n",
      "Average test loss: 0.002130615616217256\n",
      "Epoch 31/300\n",
      "Average training loss: 0.4829809022479587\n",
      "Average test loss: 0.0021509934363679755\n",
      "Epoch 32/300\n",
      "Average training loss: 0.4225231458875868\n",
      "Average test loss: 0.0022316767037328746\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3688674550056458\n",
      "Average test loss: 0.0018730895380593008\n",
      "Epoch 34/300\n",
      "Average training loss: 0.322103717274136\n",
      "Average test loss: 0.001940021705503265\n",
      "Epoch 35/300\n",
      "Average training loss: 0.28188016623920864\n",
      "Average test loss: 0.001842059060310324\n",
      "Epoch 36/300\n",
      "Average training loss: 0.24906599722968206\n",
      "Average test loss: 0.0017271821809311707\n",
      "Epoch 37/300\n",
      "Average training loss: 0.22027800597084893\n",
      "Average test loss: 0.002954724901252323\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1980056365331014\n",
      "Average test loss: 0.0017327189492061734\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1773189002805286\n",
      "Average test loss: 0.0016845668809902336\n",
      "Epoch 40/300\n",
      "Average training loss: 0.16234765911102295\n",
      "Average test loss: 0.0019366976791578862\n",
      "Epoch 41/300\n",
      "Average training loss: 0.15027753233909608\n",
      "Average test loss: 0.002521969092181987\n",
      "Epoch 42/300\n",
      "Average training loss: 0.14537950128979152\n",
      "Average test loss: 0.0019301694488773743\n",
      "Epoch 43/300\n",
      "Average training loss: 0.16810393822193145\n",
      "Average test loss: 0.004199095739879542\n",
      "Epoch 44/300\n",
      "Average training loss: 0.14726300484604304\n",
      "Average test loss: 0.0019402262377035287\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12375192364056906\n",
      "Average test loss: 0.001820946285708083\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11423811770810022\n",
      "Average test loss: 0.0017384055427586038\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1166937393479877\n",
      "Average test loss: 0.0017953642924419708\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1044525713523229\n",
      "Average test loss: 0.0022555537332470217\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09816818943950865\n",
      "Average test loss: 0.001842279788520601\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09434342086977429\n",
      "Average test loss: 0.0023154659523732133\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0903116513689359\n",
      "Average test loss: 0.0014969428766311871\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08762376211418046\n",
      "Average test loss: 0.0016292930166754457\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08450981709692214\n",
      "Average test loss: 0.005147929410760602\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08089537831147511\n",
      "Average test loss: 0.001749034959408972\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07931999731726116\n",
      "Average test loss: 0.001832012076758676\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07690853654013739\n",
      "Average test loss: 0.001451108378979067\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08068975059522523\n",
      "Average test loss: 0.0028446335522457956\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12360713603099187\n",
      "Average test loss: 0.0016769304294139147\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08081908288929197\n",
      "Average test loss: 0.0015005610113342602\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07400819294320213\n",
      "Average test loss: 0.001434112049277044\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07099118004242579\n",
      "Average test loss: 0.0013947202631582817\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06852437163392702\n",
      "Average test loss: 0.0014613976627588271\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06653499238358604\n",
      "Average test loss: 0.0014556256908302505\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06498038347562154\n",
      "Average test loss: 0.0016669768285420205\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06409254713522064\n",
      "Average test loss: 0.0012836768742029866\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07941843331522412\n",
      "Average test loss: 0.001409307709791594\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06428374494446648\n",
      "Average test loss: 0.001302244224275152\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06198292408055729\n",
      "Average test loss: 0.0014069662906436456\n",
      "Epoch 69/300\n",
      "Average training loss: 0.060568765001164544\n",
      "Average test loss: 0.0012969034391765793\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06016522705223825\n",
      "Average test loss: 0.001286562049916635\n",
      "Epoch 71/300\n",
      "Average training loss: 0.058882605026165644\n",
      "Average test loss: 0.0013173950915742253\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05789117931657367\n",
      "Average test loss: 0.0012807153436458774\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05739619116650687\n",
      "Average test loss: 0.0012195568314443033\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05606950369477272\n",
      "Average test loss: 0.0011858291583549646\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05496529643403159\n",
      "Average test loss: 0.0011902849836171502\n",
      "Epoch 76/300\n",
      "Average training loss: 0.057687813692622714\n",
      "Average test loss: 0.0012334786243219343\n",
      "Epoch 77/300\n",
      "Average training loss: 0.053847738597128124\n",
      "Average test loss: 0.0011874226023856965\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0527848805156019\n",
      "Average test loss: 0.0014566517270480593\n",
      "Epoch 79/300\n",
      "Average training loss: 0.054186286674605476\n",
      "Average test loss: 0.0011539858202967378\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05216291484898991\n",
      "Average test loss: 0.001446384305651817\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05122487160894606\n",
      "Average test loss: 0.0015511214341450896\n",
      "Epoch 82/300\n",
      "Average training loss: 0.050720692694187164\n",
      "Average test loss: 0.001157939188596275\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05018894647889667\n",
      "Average test loss: 0.0012130659835206138\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04957112526893616\n",
      "Average test loss: 0.0050885310037475494\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04938617103298505\n",
      "Average test loss: 0.0011120630253313316\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04854024788737297\n",
      "Average test loss: 0.001162043958902359\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04878720097078217\n",
      "Average test loss: 0.1446167541626427\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04777650732960966\n",
      "Average test loss: 0.03614265919559532\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04722809092534913\n",
      "Average test loss: 0.0015416661792745192\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04691486858328184\n",
      "Average test loss: 0.0010864480527945691\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04644137752387259\n",
      "Average test loss: 0.007650092344317171\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04642172124650743\n",
      "Average test loss: 0.0016510487193655637\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04649433975749546\n",
      "Average test loss: 0.0010967088410527342\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04557138998309771\n",
      "Average test loss: 545.1410972764756\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04516716650459501\n",
      "Average test loss: 0.001110809099757009\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04468775633639759\n",
      "Average test loss: 0.0011007545670080516\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0446762028435866\n",
      "Average test loss: 0.0010811419857458936\n",
      "Epoch 98/300\n",
      "Average training loss: 0.060771860066387386\n",
      "Average test loss: 0.0013388864687230023\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05076928794052866\n",
      "Average test loss: 0.0013988386698895031\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04588490801387363\n",
      "Average test loss: 0.0010754584725428786\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04481795882516437\n",
      "Average test loss: 0.0010765889195414882\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04436282248629464\n",
      "Average test loss: 0.0012835070892340608\n",
      "Epoch 103/300\n",
      "Average training loss: 0.044149149729145895\n",
      "Average test loss: 0.0011230950163056454\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04387931330336465\n",
      "Average test loss: 0.0010868247465696185\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04373364140590032\n",
      "Average test loss: 0.0010849973643716011\n",
      "Epoch 106/300\n",
      "Average training loss: 0.044827629705270135\n",
      "Average test loss: 0.001066621879633102\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04338987844189008\n",
      "Average test loss: 0.0010686026777451238\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04323207570612431\n",
      "Average test loss: 0.0010915068156189388\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04322802173097928\n",
      "Average test loss: 0.0010705591099750664\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04307901108761628\n",
      "Average test loss: 0.0010989586469820805\n",
      "Epoch 111/300\n",
      "Average training loss: 0.042951518065399596\n",
      "Average test loss: 0.0010625824650956524\n",
      "Epoch 112/300\n",
      "Average training loss: 0.043579134159617956\n",
      "Average test loss: 0.0010826987849238017\n",
      "Epoch 113/300\n",
      "Average training loss: 0.042507178101274704\n",
      "Average test loss: 0.0010682708336454299\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04248781257536676\n",
      "Average test loss: 0.0065439532457126515\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04268973750869433\n",
      "Average test loss: 0.001523513115528557\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04216963925626543\n",
      "Average test loss: 0.0010783370085474518\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04207899341318343\n",
      "Average test loss: 0.0010456829824381404\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04216858670115471\n",
      "Average test loss: 0.02126224023765988\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04447930185000102\n",
      "Average test loss: 0.0011020789538613623\n",
      "Epoch 120/300\n",
      "Average training loss: 0.041855939755837124\n",
      "Average test loss: 0.001268855525749839\n",
      "Epoch 121/300\n",
      "Average training loss: 0.041784654625587996\n",
      "Average test loss: 0.0010853614866630072\n",
      "Epoch 122/300\n",
      "Average training loss: 0.041974907355176075\n",
      "Average test loss: 0.0011256198158177237\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04128060614731577\n",
      "Average test loss: 0.0011441745419676105\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04159845417406824\n",
      "Average test loss: 0.001058983124533875\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0410577228648795\n",
      "Average test loss: 0.001055615834788316\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04100739405221409\n",
      "Average test loss: 0.001042700404404766\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04570026253991657\n",
      "Average test loss: 0.00107638498623338\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04108406508134471\n",
      "Average test loss: 0.0010759426269990702\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04053809514972899\n",
      "Average test loss: 0.0015811774879693984\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04046359061698119\n",
      "Average test loss: 0.0011751664320213927\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04083714162475533\n",
      "Average test loss: 0.002493038615987947\n",
      "Epoch 132/300\n",
      "Average training loss: 0.040553745556208824\n",
      "Average test loss: 0.0013959342770071494\n",
      "Epoch 133/300\n",
      "Average training loss: 0.043424616740809545\n",
      "Average test loss: 0.007998431065016322\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04068956509563658\n",
      "Average test loss: 0.0017124706502072513\n",
      "Epoch 135/300\n",
      "Average training loss: 0.040298462566402224\n",
      "Average test loss: 0.0010876892288215459\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04028222974969281\n",
      "Average test loss: 0.0011462332642533713\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04007428839471605\n",
      "Average test loss: 0.0011338583263051179\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04073669575817055\n",
      "Average test loss: 0.0010929883030346699\n",
      "Epoch 139/300\n",
      "Average training loss: 0.039680103335115645\n",
      "Average test loss: 0.0010553489641493394\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04033368125557899\n",
      "Average test loss: 0.0010785113845227492\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04046622125638856\n",
      "Average test loss: 0.001785913980152044\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03947436621454027\n",
      "Average test loss: 0.0011737005567799012\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04002171708146731\n",
      "Average test loss: 0.0010928216472061143\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03925623273849487\n",
      "Average test loss: 0.0010833389390673901\n",
      "Epoch 145/300\n",
      "Average training loss: 0.039211734503507616\n",
      "Average test loss: 0.0011005617064009938\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04407515874173906\n",
      "Average test loss: 0.001225540118654155\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03906949300567309\n",
      "Average test loss: 0.0010949701213070916\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04120130466090308\n",
      "Average test loss: 0.00107917266825421\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03897687041428354\n",
      "Average test loss: 0.0012448062854301598\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03915796585215463\n",
      "Average test loss: 0.00104737619517578\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03899230502711402\n",
      "Average test loss: 0.018358197833100956\n",
      "Epoch 152/300\n",
      "Average training loss: 0.039014197455512156\n",
      "Average test loss: 0.00869722903188732\n",
      "Epoch 153/300\n",
      "Average training loss: 0.038889044337802464\n",
      "Average test loss: 0.0010991242710087035\n",
      "Epoch 154/300\n",
      "Average training loss: 0.038972894469896956\n",
      "Average test loss: 0.0010875305847471786\n",
      "Epoch 155/300\n",
      "Average training loss: 0.039677410930395125\n",
      "Average test loss: 0.0011166550499490566\n",
      "Epoch 156/300\n",
      "Average training loss: 0.038835335244735085\n",
      "Average test loss: 0.001118601870516108\n",
      "Epoch 157/300\n",
      "Average training loss: 0.038496825216544996\n",
      "Average test loss: 0.0010957330107274983\n",
      "Epoch 158/300\n",
      "Average training loss: 0.039083513038025965\n",
      "Average test loss: 0.0021332901396478214\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03842887151738008\n",
      "Average test loss: 0.0011326686779244079\n",
      "Epoch 160/300\n",
      "Average training loss: 0.038454868869649037\n",
      "Average test loss: 0.0014656309811398387\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03828632792664899\n",
      "Average test loss: 0.001147943151390387\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03809436868627866\n",
      "Average test loss: 0.00417155876962675\n",
      "Epoch 163/300\n",
      "Average training loss: 0.039191971502370304\n",
      "Average test loss: 0.0012277194230506817\n",
      "Epoch 164/300\n",
      "Average test loss: 0.001459329943276114\n",
      "Epoch 165/300\n",
      "Average training loss: 0.037872797946135205\n",
      "Average test loss: 0.0018704315309102336\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03800871610641479\n",
      "Average test loss: 0.0011929152422170673\n",
      "Epoch 167/300\n",
      "Average training loss: 0.039494114382399456\n",
      "Average test loss: 0.006369263786046455\n",
      "Epoch 168/300\n",
      "Average training loss: 0.037700437039136885\n",
      "Average test loss: 0.0013281561869920957\n",
      "Epoch 169/300\n",
      "Average training loss: 0.039592103117042116\n",
      "Average test loss: 0.0011287124909253583\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03754591743151347\n",
      "Average test loss: 0.0011107650031335651\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03736737563378281\n",
      "Average test loss: 0.0011204839595593513\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04204679929216703\n",
      "Average test loss: 0.0011159988674335182\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03745875460240576\n",
      "Average test loss: 0.004141876433872514\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03747994844781028\n",
      "Average test loss: 0.0010818332452327013\n",
      "Epoch 175/300\n",
      "Average training loss: 0.037773007773690755\n",
      "Average test loss: 0.0010918350444278782\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0377056238287025\n",
      "Average test loss: 0.0025529479258176355\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03777914447585742\n",
      "Average test loss: 0.0011453134410290254\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0374170151932372\n",
      "Average test loss: 0.0011952402629993029\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03851351151863734\n",
      "Average test loss: 0.0018921710316919618\n",
      "Epoch 180/300\n",
      "Average training loss: 0.037799598245157134\n",
      "Average test loss: 0.0011834653966749707\n",
      "Epoch 181/300\n",
      "Average training loss: 0.037263544304503335\n",
      "Average test loss: 0.001112162121054199\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03727036395668983\n",
      "Average test loss: 0.0011447041420679952\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0374715708643198\n",
      "Average test loss: 0.0011892539539581372\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03730267451206843\n",
      "Average test loss: 0.0012261051659782727\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03709968131780624\n",
      "Average test loss: 0.0011056404132395983\n",
      "Epoch 186/300\n",
      "Average training loss: 0.037852161001827984\n",
      "Average test loss: 0.001692791368191441\n",
      "Epoch 187/300\n",
      "Average training loss: 0.037127140634589724\n",
      "Average test loss: 0.0011010539997886452\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0413713946160343\n",
      "Average test loss: 0.0011253187725734379\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03678695393436485\n",
      "Average test loss: 0.0013516669454466965\n",
      "Epoch 190/300\n",
      "Average training loss: 0.037371935341093274\n",
      "Average test loss: 0.0011261132559739053\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03650475162267685\n",
      "Average test loss: 0.001434114284089042\n",
      "Epoch 192/300\n",
      "Average training loss: 0.037615194771024915\n",
      "Average test loss: 0.0012605295756624804\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03670187530583806\n",
      "Average test loss: 0.0037461913261148663\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03685486016339726\n",
      "Average test loss: 0.0023782593442334067\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03669636906849014\n",
      "Average test loss: 0.0011044336322229356\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0372431514163812\n",
      "Average test loss: 0.0013488486189291709\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03769037426677015\n",
      "Average test loss: 0.001109456301252875\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0367429799205727\n",
      "Average test loss: 0.0012305648838066392\n",
      "Epoch 199/300\n",
      "Average training loss: 0.037117783705393474\n",
      "Average test loss: 0.001191322234304001\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03640525808268123\n",
      "Average test loss: 0.09755467073784935\n",
      "Epoch 201/300\n",
      "Average training loss: 0.036460314101643035\n",
      "Average test loss: 0.0018277727702322106\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03646082655919923\n",
      "Average test loss: 0.0017497738708948923\n",
      "Epoch 203/300\n",
      "Average training loss: 0.040770106318924165\n",
      "Average test loss: 0.0011262736910850636\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0361075299249755\n",
      "Average test loss: 0.0011763676873201298\n",
      "Epoch 205/300\n",
      "Average training loss: 0.036202580842706895\n",
      "Average test loss: 0.0011561536653381256\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03660731043749385\n",
      "Average test loss: 0.0012294378293057284\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04765453908840815\n",
      "Average test loss: 0.0011973917391668591\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04115809870428509\n",
      "Average test loss: 0.0010956922399087085\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03750231546163559\n",
      "Average test loss: 0.001123203461472359\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03645392205235031\n",
      "Average test loss: 0.0016900882843054004\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03636825640334023\n",
      "Average test loss: 0.00395618687901232\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03624719754689269\n",
      "Average test loss: 0.001120859993311266\n",
      "Epoch 213/300\n",
      "Average training loss: 0.036132112676898635\n",
      "Average test loss: 0.003334715684668885\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03668388736248016\n",
      "Average test loss: 0.0010990175453739035\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03595850427614318\n",
      "Average test loss: 0.0011991841937415303\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03608927621444066\n",
      "Average test loss: 0.0010993021385123332\n",
      "Epoch 217/300\n",
      "Average training loss: 0.037168846292628184\n",
      "Average test loss: 0.0011243116648143364\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03595114027791553\n",
      "Average test loss: 0.00492012302722368\n",
      "Epoch 219/300\n",
      "Average training loss: 0.036201578358809154\n",
      "Average test loss: 0.0015725046739810042\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03585412193503645\n",
      "Average test loss: 0.0013574600180404053\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03638986767331759\n",
      "Average test loss: 0.001169990500797414\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0362566927936342\n",
      "Average test loss: 0.0012104243554381861\n",
      "Epoch 223/300\n",
      "Average training loss: 0.035690187017122905\n",
      "Average test loss: 0.0015210992171325616\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0360602656553189\n",
      "Average test loss: 0.0013849340811268322\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03651809139880869\n",
      "Average test loss: 0.0012161577062474356\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0355579593943225\n",
      "Average test loss: 0.0012789537516008649\n",
      "Epoch 227/300\n",
      "Average training loss: 0.042256154970990285\n",
      "Average test loss: 0.001167212163667298\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0361330676443047\n",
      "Average test loss: 0.0012020977548737493\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0361136987109979\n",
      "Average test loss: 0.0011484191834719645\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03578430238862832\n",
      "Average test loss: 0.001133667704416439\n",
      "Epoch 231/300\n",
      "Average training loss: 0.035463500953382916\n",
      "Average test loss: 0.00483910139857067\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03558332702351941\n",
      "Average test loss: 0.0013316657876388893\n",
      "Epoch 233/300\n",
      "Average training loss: 0.035743464118904535\n",
      "Average test loss: 0.0011595392980509333\n",
      "Epoch 234/300\n",
      "Average training loss: 0.035599618633588154\n",
      "Average test loss: 0.003691749243479636\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03582256096104781\n",
      "Average test loss: 0.0011275061471387744\n",
      "Epoch 236/300\n",
      "Average training loss: 0.035650698337290024\n",
      "Average test loss: 0.0011689913652630315\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03787936430176099\n",
      "Average test loss: 0.0013255049103043145\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03533066330353419\n",
      "Average test loss: 0.0012903674687258899\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03674924812217553\n",
      "Average test loss: 0.0015637700082734227\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03519131804837121\n",
      "Average test loss: 0.001152122524463468\n",
      "Epoch 241/300\n",
      "Average training loss: 0.035306362640526556\n",
      "Average test loss: 0.001677754259771771\n",
      "Epoch 242/300\n",
      "Average training loss: 0.035341573658916686\n",
      "Average test loss: 0.0011997470511123539\n",
      "Epoch 243/300\n",
      "Average training loss: 0.035913192596700456\n",
      "Average test loss: 0.0011192399880124463\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03595676390826702\n",
      "Average test loss: 0.001236457834020257\n",
      "Epoch 245/300\n",
      "Average training loss: 0.035268958303663465\n",
      "Average test loss: 0.001635017457107703\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03510230807960033\n",
      "Average test loss: 0.0015838553821668029\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03687241345975134\n",
      "Average test loss: 0.20822244752777946\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03514074892467923\n",
      "Average test loss: 0.0018929340462717746\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03500369375944137\n",
      "Average test loss: 0.0020471259080287484\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03544131823380788\n",
      "Average test loss: 0.0011909000677470531\n",
      "Epoch 251/300\n",
      "Average training loss: 0.036208941529194516\n",
      "Average test loss: 0.001687463499398695\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03615650255481402\n",
      "Average test loss: 0.00128280667681247\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03492550390296512\n",
      "Average test loss: 0.0011342679997906088\n",
      "Epoch 254/300\n",
      "Average training loss: 0.036450939426819484\n",
      "Average test loss: 0.007222317062525286\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03535696628689766\n",
      "Average test loss: 0.001204164055796961\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03512949621346262\n",
      "Average test loss: 0.0012175787828034824\n",
      "Epoch 257/300\n",
      "Average training loss: 0.034978787855969536\n",
      "Average test loss: 0.001185933340392593\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03530478199654155\n",
      "Average test loss: 0.0011442824474846324\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03951939128670427\n",
      "Average test loss: 0.001228933689598408\n",
      "Epoch 260/300\n",
      "Average training loss: 0.035391114455130365\n",
      "Average test loss: 0.0014976907949894667\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03493561223811573\n",
      "Average test loss: 0.001116875972909232\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03509829705953598\n",
      "Average test loss: 0.0019825191636466317\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03491719593273269\n",
      "Average test loss: 0.001191125602254437\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03743527950843175\n",
      "Average test loss: 0.0027779443034281334\n",
      "Epoch 265/300\n",
      "Average training loss: 0.035340001420842274\n",
      "Average test loss: 0.0011560715816190674\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03473893076181412\n",
      "Average test loss: 0.0011588948154821993\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03578456234269672\n",
      "Average test loss: 0.0011561911641102699\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0345977373652988\n",
      "Average test loss: 0.00583538485566775\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0350692616601785\n",
      "Average test loss: 0.0011294893220894866\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03518743713034524\n",
      "Average test loss: 0.0011826694617047906\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03484036345945464\n",
      "Average test loss: 0.0011228782031685114\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0354519840379556\n",
      "Average test loss: 0.0011393726102800832\n",
      "Epoch 273/300\n",
      "Average training loss: 0.034791012646423446\n",
      "Average test loss: 0.0013796482476302319\n",
      "Epoch 274/300\n",
      "Average training loss: 0.035067341185278364\n",
      "Average test loss: 0.0015695518316287134\n",
      "Epoch 275/300\n",
      "Average training loss: 0.036033251262373396\n",
      "Average test loss: 0.001208051739467515\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03445841109752655\n",
      "Average test loss: 0.0011470276274614863\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03620398328701655\n",
      "Average test loss: 0.001245408195329623\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03456134916014141\n",
      "Average test loss: 0.001130612138710502\n",
      "Epoch 279/300\n",
      "Average training loss: 0.034750295172135036\n",
      "Average test loss: 0.001139894504410525\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03542943141195509\n",
      "Average test loss: 0.0011498507952007154\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03448124705586168\n",
      "Average test loss: 0.001262590341994332\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03453997501068645\n",
      "Average test loss: 0.011684678634835613\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03471969653169314\n",
      "Average test loss: 0.001148119455203414\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03499486084447967\n",
      "Average test loss: 0.0011682139692000216\n",
      "Epoch 285/300\n",
      "Average training loss: 0.034934936900933584\n",
      "Average test loss: 0.0024408728142993317\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03455266374018457\n",
      "Average test loss: 0.0011427041354278723\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03456228315002388\n",
      "Average test loss: 0.0015911422623321415\n",
      "Epoch 288/300\n",
      "Average training loss: 0.035011370645629036\n",
      "Average test loss: 0.0011761131555669837\n",
      "Epoch 289/300\n",
      "Average training loss: 0.034591278182135685\n",
      "Average test loss: 0.0011406129859905276\n",
      "Epoch 290/300\n",
      "Average training loss: 0.034495358587967026\n",
      "Average test loss: 0.0011599073106836942\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03460663484864765\n",
      "Average test loss: 0.0012107665939256548\n",
      "Epoch 292/300\n",
      "Average training loss: 0.035472577823532955\n",
      "Average test loss: 0.0011334162418627078\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03432706597281827\n",
      "Average test loss: 0.001164568650122318\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03430530538492733\n",
      "Average test loss: 0.0011884586602552898\n",
      "Epoch 295/300\n",
      "Average training loss: 0.034863418526119655\n",
      "Average test loss: 0.0012582325999521548\n",
      "Epoch 296/300\n",
      "Average training loss: 0.034441530687941445\n",
      "Average test loss: 0.007537154230599602\n",
      "Epoch 297/300\n",
      "Average training loss: 0.034241172618336146\n",
      "Average test loss: 0.0011706554601486358\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03460372735891077\n",
      "Average test loss: 0.05116415845354398\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03413211967051029\n",
      "Average test loss: 0.001295212432121237\n",
      "Epoch 300/300\n",
      "Average training loss: 0.034951865510808096\n",
      "Average test loss: 0.0011397230167252321\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_64_Depth5/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.42\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.46\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.33\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.78\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.18\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.18\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.58\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.81\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.78\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.02\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.43\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.62\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.65\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.32\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.50\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.64\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.65\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.55\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.70\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.60\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.55\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.18\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.37\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.84\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.16\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.26\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.33\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.36\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.22\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.63\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.92\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.43\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.62\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.82\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 34.02\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.93\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 34.24\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 34.39\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 34.57\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 34.42\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 34.42\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 34.56\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 34.47\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 34.44\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 34.69\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 34.74\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 34.95\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 34.46\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 34.57\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 34.88\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 34.99\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
