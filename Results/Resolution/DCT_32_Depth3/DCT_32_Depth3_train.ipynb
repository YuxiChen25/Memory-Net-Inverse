{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_32x32.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.06418129229048888\n",
      "Average test loss: 0.005415191724896431\n",
      "Epoch 2/300\n",
      "Average training loss: 0.024969744129313364\n",
      "Average test loss: 0.005000198752515846\n",
      "Epoch 3/300\n",
      "Average training loss: 0.023734378533230887\n",
      "Average test loss: 0.004716288906004694\n",
      "Epoch 4/300\n",
      "Average training loss: 0.023202013316253822\n",
      "Average test loss: 0.004727475234203868\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022907073127726715\n",
      "Average test loss: 0.004699776099787818\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02268899698389901\n",
      "Average test loss: 0.004576726619982057\n",
      "Epoch 7/300\n",
      "Average training loss: 0.022536813404825\n",
      "Average test loss: 0.004572056194146475\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022413437300258212\n",
      "Average test loss: 0.0045796404803792636\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022307688963082103\n",
      "Average test loss: 0.004531814323117335\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02223064305053817\n",
      "Average test loss: 0.004535225064804156\n",
      "Epoch 11/300\n",
      "Average training loss: 0.022160035442974832\n",
      "Average test loss: 0.00452552606496546\n",
      "Epoch 12/300\n",
      "Average training loss: 0.022068192137612238\n",
      "Average test loss: 0.004470285941743189\n",
      "Epoch 13/300\n",
      "Average training loss: 0.022015331193804742\n",
      "Average test loss: 0.00444746221229434\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021943564921617507\n",
      "Average test loss: 0.004445193365630176\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021888876693116295\n",
      "Average test loss: 0.004431090779188606\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021842093139886855\n",
      "Average test loss: 0.0044353137997289495\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02178043589492639\n",
      "Average test loss: 0.004431351880853375\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02171819041834937\n",
      "Average test loss: 0.004395233241634236\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021674094474977916\n",
      "Average test loss: 0.004395149974773327\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02163074698381954\n",
      "Average test loss: 0.004389338029755487\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02158902375234498\n",
      "Average test loss: 0.004373620972037315\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021551871692140897\n",
      "Average test loss: 0.004359832289111283\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02150367379022969\n",
      "Average test loss: 0.004371341392811802\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021470750972628593\n",
      "Average test loss: 0.004365115335418119\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021425112277269363\n",
      "Average test loss: 0.004361371709240808\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02139827773306105\n",
      "Average test loss: 0.004343861060010062\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02137799755235513\n",
      "Average test loss: 0.004334742931028207\n",
      "Epoch 28/300\n",
      "Average training loss: 0.021332637445794212\n",
      "Average test loss: 0.0043261021628148025\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021296047324935594\n",
      "Average test loss: 0.004314847827371624\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0212722846004698\n",
      "Average test loss: 0.004354739161622193\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021252215714918244\n",
      "Average test loss: 0.004315001793619659\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021229008068641027\n",
      "Average test loss: 0.004319958741052283\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021199901208281517\n",
      "Average test loss: 0.004301390036526654\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02116932242943181\n",
      "Average test loss: 0.004305426645403107\n",
      "Epoch 35/300\n",
      "Average training loss: 0.021151030187805495\n",
      "Average test loss: 0.004289864496638377\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02111827869547738\n",
      "Average test loss: 0.004300022282120254\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021112266682916218\n",
      "Average test loss: 0.0043169067299200425\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02108807652029726\n",
      "Average test loss: 0.00429533754578895\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021060200868381396\n",
      "Average test loss: 0.004306140242765347\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02104310281905863\n",
      "Average test loss: 0.0042857554432832535\n",
      "Epoch 41/300\n",
      "Average training loss: 0.021019522009624377\n",
      "Average test loss: 0.004286713141120143\n",
      "Epoch 42/300\n",
      "Average training loss: 0.021010482155614428\n",
      "Average test loss: 0.004274184050659339\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02099118130074607\n",
      "Average test loss: 0.004285473769116733\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020981944772932266\n",
      "Average test loss: 0.004274611500195331\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02097273678912057\n",
      "Average test loss: 0.004283599087554548\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020944526518384615\n",
      "Average test loss: 0.004271153781976965\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02093791674077511\n",
      "Average test loss: 0.004279050013878279\n",
      "Epoch 48/300\n",
      "Average training loss: 0.020909848931762908\n",
      "Average test loss: 0.004270863650573624\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02088951453069846\n",
      "Average test loss: 0.0042646088649829226\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020876318201422692\n",
      "Average test loss: 0.004262402900805076\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02086710825148556\n",
      "Average test loss: 0.004267833110152019\n",
      "Epoch 52/300\n",
      "Average training loss: 0.020857261965672176\n",
      "Average test loss: 0.004266543014388945\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02084491981731521\n",
      "Average test loss: 0.004257394324367245\n",
      "Epoch 54/300\n",
      "Average training loss: 0.020823785069915984\n",
      "Average test loss: 0.004253377634618017\n",
      "Epoch 55/300\n",
      "Average training loss: 0.020815905527936086\n",
      "Average test loss: 0.004260159195297294\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020793826873103778\n",
      "Average test loss: 0.004274724722322491\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02077925258129835\n",
      "Average test loss: 0.004272405282697744\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020770932196742958\n",
      "Average test loss: 0.004258564803128441\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02076205895841122\n",
      "Average test loss: 0.004258304249909189\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020745066192415024\n",
      "Average test loss: 0.004264122296952539\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020723933727376992\n",
      "Average test loss: 0.004255199579728974\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020719154624475374\n",
      "Average test loss: 0.004249239493989282\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02070506527688768\n",
      "Average test loss: 0.004257672215501467\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02068891649113761\n",
      "Average test loss: 0.0042462771828803755\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02067378761867682\n",
      "Average test loss: 0.004272836241043276\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020671607555614577\n",
      "Average test loss: 0.0042777582829197245\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020659391664796405\n",
      "Average test loss: 0.004267959301256471\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02064801140129566\n",
      "Average test loss: 0.00425473921539055\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020631822547978827\n",
      "Average test loss: 0.004261900670412514\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020623017648028003\n",
      "Average test loss: 0.0042609506162504356\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02060725617243184\n",
      "Average test loss: 0.004243774522923761\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02058238260199626\n",
      "Average test loss: 0.004260255879826016\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020582824177212185\n",
      "Average test loss: 0.00423810363933444\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020574208272827996\n",
      "Average test loss: 0.00424901681434777\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02056614565021462\n",
      "Average test loss: 0.004248437825797332\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02054869158234861\n",
      "Average test loss: 0.004255518273967836\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02054101543294059\n",
      "Average test loss: 0.004264260813179943\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02053022806180848\n",
      "Average test loss: 0.004243781575105256\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02050939531624317\n",
      "Average test loss: 0.004273897133767605\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02049442955189281\n",
      "Average test loss: 0.004243956920587354\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020497435200545523\n",
      "Average test loss: 0.004254496810750828\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020494547972248662\n",
      "Average test loss: 0.004284653787604636\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020467184123065738\n",
      "Average test loss: 0.004257081198609538\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020458667208751044\n",
      "Average test loss: 0.004263382014611529\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020441995526353517\n",
      "Average test loss: 0.004249415853785144\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02043445612821314\n",
      "Average test loss: 0.004254671930438943\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02042843484050698\n",
      "Average test loss: 0.004258740335288975\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020420617375108932\n",
      "Average test loss: 0.004253646567463875\n",
      "Epoch 89/300\n",
      "Average training loss: 0.020406797063019542\n",
      "Average test loss: 0.0042837016551444925\n",
      "Epoch 90/300\n",
      "Average training loss: 0.020395312615566784\n",
      "Average test loss: 0.004261102027777169\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020383450898031393\n",
      "Average test loss: 0.004247483826552828\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020378898958365123\n",
      "Average test loss: 0.004270972513904174\n",
      "Epoch 93/300\n",
      "Average training loss: 0.020364604656895\n",
      "Average test loss: 0.0042615954735212855\n",
      "Epoch 94/300\n",
      "Average training loss: 0.020353163808584214\n",
      "Average test loss: 0.0042724108387612635\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020334439307451248\n",
      "Average test loss: 0.004260601115723451\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020329134848382738\n",
      "Average test loss: 0.004264018065192633\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02031666405333413\n",
      "Average test loss: 0.004243199938287338\n",
      "Epoch 98/300\n",
      "Average training loss: 0.020307536330487992\n",
      "Average test loss: 0.0044395430085973605\n",
      "Epoch 99/300\n",
      "Average training loss: 0.020301244411203595\n",
      "Average test loss: 0.004294005541337861\n",
      "Epoch 100/300\n",
      "Average training loss: 0.020284535199403764\n",
      "Average test loss: 0.0042825484350323675\n",
      "Epoch 101/300\n",
      "Average training loss: 0.020282676094108158\n",
      "Average test loss: 0.004278393897331423\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02025296930389272\n",
      "Average test loss: 0.00426736147246427\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02024259741604328\n",
      "Average test loss: 0.0043033811944640345\n",
      "Epoch 104/300\n",
      "Average training loss: 0.020244566334618463\n",
      "Average test loss: 0.00424963793448276\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02021922837694486\n",
      "Average test loss: 0.004265335149442156\n",
      "Epoch 106/300\n",
      "Average training loss: 0.020209330838587548\n",
      "Average test loss: 0.004322802810826236\n",
      "Epoch 107/300\n",
      "Average training loss: 0.020197249930765895\n",
      "Average test loss: 0.0043054442941728565\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02019522981180085\n",
      "Average test loss: 0.00427680108240909\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02019127255678177\n",
      "Average test loss: 0.004261110036530428\n",
      "Epoch 110/300\n",
      "Average training loss: 0.020176075981722937\n",
      "Average test loss: 0.004266928009274933\n",
      "Epoch 111/300\n",
      "Average training loss: 0.020161509265502295\n",
      "Average test loss: 0.004277015894237492\n",
      "Epoch 112/300\n",
      "Average training loss: 0.020164728845159213\n",
      "Average test loss: 0.004290736947208643\n",
      "Epoch 113/300\n",
      "Average training loss: 0.020141853630542755\n",
      "Average test loss: 0.004284334949114257\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02013553862273693\n",
      "Average test loss: 0.004319302591598696\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020119962682326634\n",
      "Average test loss: 0.004352437617050277\n",
      "Epoch 116/300\n",
      "Average training loss: 0.020120235802398787\n",
      "Average test loss: 0.004304624787014392\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02010017864406109\n",
      "Average test loss: 0.004253466512593958\n",
      "Epoch 118/300\n",
      "Average training loss: 0.020092964144216643\n",
      "Average test loss: 0.004286528490483761\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02008367584314611\n",
      "Average test loss: 0.004291895715105865\n",
      "Epoch 120/300\n",
      "Average training loss: 0.020072784940401715\n",
      "Average test loss: 0.004285204638623529\n",
      "Epoch 121/300\n",
      "Average training loss: 0.020059422334035237\n",
      "Average test loss: 0.004324666911943091\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02004582506749365\n",
      "Average test loss: 0.004304534012037847\n",
      "Epoch 123/300\n",
      "Average training loss: 0.020041277731458345\n",
      "Average test loss: 0.004296172220259905\n",
      "Epoch 124/300\n",
      "Average training loss: 0.020023279272847704\n",
      "Average test loss: 0.004341919219121337\n",
      "Epoch 125/300\n",
      "Average training loss: 0.020020342965920767\n",
      "Average test loss: 0.004328886017617252\n",
      "Epoch 126/300\n",
      "Average training loss: 0.020009550621112187\n",
      "Average test loss: 0.004430501249308387\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019999733545713955\n",
      "Average test loss: 0.004294376189923949\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019982684537768365\n",
      "Average test loss: 0.004303110993777712\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019986579498483075\n",
      "Average test loss: 0.004317074987416466\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01997399065643549\n",
      "Average test loss: 0.004297469208637873\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019952342798312504\n",
      "Average test loss: 0.004261332831242018\n",
      "Epoch 132/300\n",
      "Average training loss: 0.019956374428338475\n",
      "Average test loss: 0.004288558692981799\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019919934237996737\n",
      "Average test loss: 0.00429149059827129\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0199266567511691\n",
      "Average test loss: 0.004296360140459405\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019918064278033044\n",
      "Average test loss: 0.004298258591029379\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019904457656873598\n",
      "Average test loss: 0.004299708898489674\n",
      "Epoch 137/300\n",
      "Average training loss: 0.019878712372647392\n",
      "Average test loss: 0.004291328763796224\n",
      "Epoch 138/300\n",
      "Average training loss: 0.019890234379304778\n",
      "Average test loss: 0.004316351461741659\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019850705489516257\n",
      "Average test loss: 0.004341767315649324\n",
      "Epoch 140/300\n",
      "Average training loss: 0.019861094820830556\n",
      "Average test loss: 0.004411068155947659\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01984728474252754\n",
      "Average test loss: 0.004301883408178886\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01984991246461868\n",
      "Average test loss: 0.004342965286845963\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019829389327102237\n",
      "Average test loss: 0.004324080116632912\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01982635553015603\n",
      "Average test loss: 0.004332610598040952\n",
      "Epoch 145/300\n",
      "Average training loss: 0.019806277038322554\n",
      "Average test loss: 0.004331892611872818\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0198136865546306\n",
      "Average test loss: 0.004303258779562182\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01979733802212609\n",
      "Average test loss: 0.004282788266324335\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0197875830779473\n",
      "Average test loss: 0.004285194995088709\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0197665360454056\n",
      "Average test loss: 0.0043439941031651365\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019756686575710773\n",
      "Average test loss: 0.004379379903690682\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019742121370302308\n",
      "Average test loss: 0.004342694848568903\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019740615588095454\n",
      "Average test loss: 0.004318858589563105\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019736479655736022\n",
      "Average test loss: 0.00432750438609057\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019730857524606918\n",
      "Average test loss: 0.004312752573233511\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01970463172925843\n",
      "Average test loss: 0.004310690035422643\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019697319651643435\n",
      "Average test loss: 0.0043418522568212615\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019683486006326145\n",
      "Average test loss: 0.004373615675088432\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01968533013926612\n",
      "Average test loss: 0.004336174812167883\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019673070296645164\n",
      "Average test loss: 0.004365696513404449\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01965068432688713\n",
      "Average test loss: 0.004331423269791736\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019649965782132413\n",
      "Average test loss: 0.004416043160276281\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01964433858295282\n",
      "Average test loss: 0.004339666444808245\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019637761740220916\n",
      "Average test loss: 0.0044245225319431885\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019613667824202114\n",
      "Average test loss: 0.004370985777841674\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019611687118808428\n",
      "Average test loss: 0.0044847098824878535\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01961092232167721\n",
      "Average test loss: 0.004367259627208114\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019598595410585402\n",
      "Average test loss: 0.004422363732424047\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01958467071917322\n",
      "Average test loss: 0.004334193848901325\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019579498385389645\n",
      "Average test loss: 0.004320294131628341\n",
      "Epoch 170/300\n",
      "Average training loss: 0.019565495420661237\n",
      "Average test loss: 0.004348262130800221\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019567171838548447\n",
      "Average test loss: 0.004450991423593627\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019546375943554774\n",
      "Average test loss: 0.00435319979613026\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019537956530849138\n",
      "Average test loss: 0.0043681485681898065\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019524621579382156\n",
      "Average test loss: 0.004348018356909355\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01951666535768244\n",
      "Average test loss: 0.004315480606423484\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019508658276663888\n",
      "Average test loss: 0.0043711673855367636\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01951452963385317\n",
      "Average test loss: 0.004349056363519695\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01950040660632981\n",
      "Average test loss: 0.004364349579645528\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01948718712064955\n",
      "Average test loss: 0.004363596470405658\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019467685927947363\n",
      "Average test loss: 0.004344959326916271\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01947848624156581\n",
      "Average test loss: 0.0043368967877080045\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01947301805184947\n",
      "Average test loss: 0.004391927145421505\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01944559798306889\n",
      "Average test loss: 0.004344881078849236\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019436856754952006\n",
      "Average test loss: 0.004482909615668986\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019418978753189247\n",
      "Average test loss: 0.00439483115904861\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01942132163213359\n",
      "Average test loss: 0.0043889412192834746\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019408277716901567\n",
      "Average test loss: 0.004366506985078255\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01939440769288275\n",
      "Average test loss: 0.0043865387510094375\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01938835073262453\n",
      "Average test loss: 0.004423812470502324\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019392975573738418\n",
      "Average test loss: 0.004432656753394339\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01935961189534929\n",
      "Average test loss: 0.004333557477013933\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019370223328471183\n",
      "Average test loss: 0.0044506174125191235\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019357709154486657\n",
      "Average test loss: 0.004453697271231148\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01934048804557986\n",
      "Average test loss: 0.004496377114206552\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019321927645140225\n",
      "Average test loss: 0.004389453728786773\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01933329047428237\n",
      "Average test loss: 0.004416198646856679\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01931590563721127\n",
      "Average test loss: 0.004490624984519349\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01932120725264152\n",
      "Average test loss: 0.004387486312124465\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01930337287982305\n",
      "Average test loss: 0.0043935979368786016\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019299872477021484\n",
      "Average test loss: 0.004470080935706695\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019282421841389605\n",
      "Average test loss: 0.004422362662230929\n",
      "Epoch 202/300\n",
      "Average training loss: 0.019289315728677644\n",
      "Average test loss: 0.004445341881571544\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019269638299942018\n",
      "Average test loss: 0.004474914100021124\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019272355167402163\n",
      "Average test loss: 0.004477986119273636\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019260759573843746\n",
      "Average test loss: 0.00441076250664062\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019243971448805595\n",
      "Average test loss: 0.0044442892314659225\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019232049366666212\n",
      "Average test loss: 0.004518037381685443\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01924386270178689\n",
      "Average test loss: 0.00453325641196635\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019226528025335737\n",
      "Average test loss: 0.004597476691835457\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019218733214669756\n",
      "Average test loss: 0.004379860354380475\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019207106300526196\n",
      "Average test loss: 0.004407174524333742\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019190567079517575\n",
      "Average test loss: 0.004389235384762287\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019198997150692676\n",
      "Average test loss: 0.004403012370483743\n",
      "Epoch 214/300\n",
      "Average training loss: 0.019195454625619782\n",
      "Average test loss: 0.004352746314886543\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019169237272606957\n",
      "Average test loss: 0.004435044674823681\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01917830774684747\n",
      "Average test loss: 0.004388550719246268\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01914912737947371\n",
      "Average test loss: 0.004536560969634188\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019150762741764388\n",
      "Average test loss: 0.004459591796828641\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019154876000351376\n",
      "Average test loss: 0.0043916843309998515\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01914336770441797\n",
      "Average test loss: 0.004432590633630752\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019139779950181644\n",
      "Average test loss: 0.004373095102608204\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019109754004412227\n",
      "Average test loss: 0.00445927376838194\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019143711499041982\n",
      "Average test loss: 0.004524800929551323\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01910907275809182\n",
      "Average test loss: 0.0043954454238216084\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019084653311305575\n",
      "Average test loss: 0.004377616140163607\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019110442587070994\n",
      "Average test loss: 0.004494695055815908\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019068326437638866\n",
      "Average test loss: 0.00459358379120628\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019081297299928135\n",
      "Average test loss: 0.0044558702719708285\n",
      "Epoch 229/300\n",
      "Average training loss: 0.019083677815066445\n",
      "Average test loss: 0.004509742425133785\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019074948481387563\n",
      "Average test loss: 0.004411533881806665\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019069934553570218\n",
      "Average test loss: 0.004468943895565139\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01906588592297501\n",
      "Average test loss: 0.004439140511469709\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019047274036539927\n",
      "Average test loss: 0.00450567617184586\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01904630303217305\n",
      "Average test loss: 0.004484722045353717\n",
      "Epoch 235/300\n",
      "Average training loss: 0.019035547571049798\n",
      "Average test loss: 0.004437738750336899\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019016076937317848\n",
      "Average test loss: 0.004520579405956798\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0190142929189735\n",
      "Average test loss: 0.004507300344606241\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019010028180976708\n",
      "Average test loss: 0.004434635437197155\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0190045405841536\n",
      "Average test loss: 0.004477536716808875\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01899669241822428\n",
      "Average test loss: 0.004485576878198319\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01899371263053682\n",
      "Average test loss: 0.00447438844665885\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01898124162024922\n",
      "Average test loss: 0.004479878741419978\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01897610558735\n",
      "Average test loss: 0.004428694478339619\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018949237956768936\n",
      "Average test loss: 0.004476773215871718\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01895700356695387\n",
      "Average test loss: 0.004498923092252679\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01896143508454164\n",
      "Average test loss: 0.004436492469989591\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01895625973161724\n",
      "Average test loss: 0.004466628368323048\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018945462187131244\n",
      "Average test loss: 0.00444641386386421\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01893501975801256\n",
      "Average test loss: 0.0044496165033843784\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0189403414544132\n",
      "Average test loss: 0.004544624435818857\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018932902812957765\n",
      "Average test loss: 0.004443221001782351\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018915934586690532\n",
      "Average test loss: 0.004388515760294265\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018913190096616746\n",
      "Average test loss: 0.004478143822401762\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018900012829237513\n",
      "Average test loss: 0.004434831723984745\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018911448635160925\n",
      "Average test loss: 0.004423870688925187\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018893838663895924\n",
      "Average test loss: 0.004499603841867711\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01886515082915624\n",
      "Average test loss: 0.004596631992608308\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018874988340669207\n",
      "Average test loss: 0.004502534011585845\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018880742975407178\n",
      "Average test loss: 0.004449110001325607\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018865255470077197\n",
      "Average test loss: 0.004464689642915296\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018860046005083456\n",
      "Average test loss: 0.004530469713111719\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018852973280681504\n",
      "Average test loss: 0.004598677470452256\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018848360635340212\n",
      "Average test loss: 0.004631089773029089\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018843937580784162\n",
      "Average test loss: 0.004441710804899534\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01883623684446017\n",
      "Average test loss: 0.004489187770833572\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018816473343306116\n",
      "Average test loss: 0.0045139424010283416\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018820067884193527\n",
      "Average test loss: 0.0045457762585332\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01880873448153337\n",
      "Average test loss: 0.00454045298240251\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01880092400146855\n",
      "Average test loss: 0.004510584588059121\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018828493057025802\n",
      "Average test loss: 0.004583517970310317\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01880465946180953\n",
      "Average test loss: 0.004539285158117612\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018799659031960698\n",
      "Average test loss: 0.00456085455045104\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018779046836826536\n",
      "Average test loss: 0.0044527451433241366\n",
      "Epoch 274/300\n",
      "Average training loss: 0.018778686847951677\n",
      "Average test loss: 0.004559835951775312\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018780208482510514\n",
      "Average test loss: 0.00450301039384471\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01878569995943043\n",
      "Average test loss: 0.004518484798984395\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018762875760595005\n",
      "Average test loss: 0.004718855194333527\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01876396524326669\n",
      "Average test loss: 0.004836465261462662\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01875826101501783\n",
      "Average test loss: 0.004455715536450347\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01876076641016536\n",
      "Average test loss: 0.004461708790726132\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018756392089856997\n",
      "Average test loss: 0.004643679528186719\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01875030627184444\n",
      "Average test loss: 0.004472733872425225\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01872299333744579\n",
      "Average test loss: 0.0046269393935799595\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018723545180426703\n",
      "Average test loss: 0.00458731626946893\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01873905829588572\n",
      "Average test loss: 0.004541518659020464\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0187039345867104\n",
      "Average test loss: 0.004550182282510731\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01871560970445474\n",
      "Average test loss: 0.004543178905629449\n",
      "Epoch 288/300\n",
      "Average training loss: 0.018703982648750145\n",
      "Average test loss: 0.004527056893954675\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018711575840910277\n",
      "Average test loss: 0.004532650698390272\n",
      "Epoch 290/300\n",
      "Average training loss: 0.018701929498049948\n",
      "Average test loss: 0.0045235938988626\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018705785017046662\n",
      "Average test loss: 0.004571080370909638\n",
      "Epoch 292/300\n",
      "Average training loss: 0.018679399008552235\n",
      "Average test loss: 0.00448145885599984\n",
      "Epoch 293/300\n",
      "Average training loss: 0.018682262988554108\n",
      "Average test loss: 0.004616503056449194\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018662784210509722\n",
      "Average test loss: 0.004489863375822703\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018673762987057366\n",
      "Average test loss: 0.004536187859872977\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01867469986445374\n",
      "Average test loss: 0.004514095018514328\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01865691104448504\n",
      "Average test loss: 0.0046942882169451975\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018659915160801677\n",
      "Average test loss: 0.004575062181386683\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018633368686669403\n",
      "Average test loss: 0.004851368087447352\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018660137270887694\n",
      "Average test loss: 0.004533671960234642\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.0593593832121955\n",
      "Average test loss: 0.004718236457556486\n",
      "Epoch 2/300\n",
      "Average training loss: 0.021636600751015873\n",
      "Average test loss: 0.00431699904302756\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02019757705181837\n",
      "Average test loss: 0.00399645305549105\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01953528126411968\n",
      "Average test loss: 0.003932759247720242\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01912335864206155\n",
      "Average test loss: 0.003841687178032266\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01883748947415087\n",
      "Average test loss: 0.0037991074067023067\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01858614884648058\n",
      "Average test loss: 0.0037491318380667104\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018381159260869025\n",
      "Average test loss: 0.0037045039921585056\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018202642334832087\n",
      "Average test loss: 0.003664676488066713\n",
      "Epoch 10/300\n",
      "Average training loss: 0.018043272685673503\n",
      "Average test loss: 0.0036228105610029565\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017897415904535186\n",
      "Average test loss: 0.003613854699457685\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017791890035900804\n",
      "Average test loss: 0.0036006776796032986\n",
      "Epoch 13/300\n",
      "Average training loss: 0.017706330788632235\n",
      "Average test loss: 0.0035549232198132407\n",
      "Epoch 14/300\n",
      "Average training loss: 0.017594972013599344\n",
      "Average test loss: 0.0035265645583470663\n",
      "Epoch 15/300\n",
      "Average training loss: 0.017517651988400353\n",
      "Average test loss: 0.003525470520473189\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017411939430567954\n",
      "Average test loss: 0.003484371090100871\n",
      "Epoch 17/300\n",
      "Average training loss: 0.017340280122227138\n",
      "Average test loss: 0.003482792703434825\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01725733611567153\n",
      "Average test loss: 0.0034718649499118326\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01719599572237995\n",
      "Average test loss: 0.003439316172566679\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01710539424088266\n",
      "Average test loss: 0.0034323040385627083\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017045121263298723\n",
      "Average test loss: 0.00341007174210002\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016979850113391877\n",
      "Average test loss: 0.003424621404459079\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016898580603301527\n",
      "Average test loss: 0.00339236745900578\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016854858410855135\n",
      "Average test loss: 0.003382566671818495\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01679162382086118\n",
      "Average test loss: 0.0033781898067229326\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01673542559146881\n",
      "Average test loss: 0.0033573361612442465\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016688613650699458\n",
      "Average test loss: 0.0033256278579226798\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016639197279181745\n",
      "Average test loss: 0.0033509232960641385\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0165995981676711\n",
      "Average test loss: 0.0033305913058833945\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016547368408077294\n",
      "Average test loss: 0.0033663766694565613\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016517365640236273\n",
      "Average test loss: 0.0033370683493299616\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01648177643120289\n",
      "Average test loss: 0.003357788905294405\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016432658611072435\n",
      "Average test loss: 0.003317831000313163\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016405083273020055\n",
      "Average test loss: 0.003285875280284219\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016373445135851702\n",
      "Average test loss: 0.003277940210368898\n",
      "Epoch 36/300\n",
      "Average training loss: 0.016325600571102567\n",
      "Average test loss: 0.003271945466183954\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01630287664052513\n",
      "Average test loss: 0.003280616778259476\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016266843772596785\n",
      "Average test loss: 0.0032736333654158644\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016240340395934052\n",
      "Average test loss: 0.003282600000500679\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016207864437666204\n",
      "Average test loss: 0.003263635474567612\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01618311574144496\n",
      "Average test loss: 0.003257645739449395\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016153077725734974\n",
      "Average test loss: 0.0032474767160084513\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01613009402735366\n",
      "Average test loss: 0.0032797114991893373\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016102932633625135\n",
      "Average test loss: 0.0032505260428620708\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016073227628237673\n",
      "Average test loss: 0.0032384260696255497\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01606455942326122\n",
      "Average test loss: 0.003229024709098869\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01602769710951381\n",
      "Average test loss: 0.003240011228248477\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01599022993693749\n",
      "Average test loss: 0.0032724047721260125\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01597998359137111\n",
      "Average test loss: 0.003253530257071058\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015956804172860252\n",
      "Average test loss: 0.0032479537980010112\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015936471822361152\n",
      "Average test loss: 0.003304279978490538\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01591880833854278\n",
      "Average test loss: 0.003251047969278362\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01589646868324942\n",
      "Average test loss: 0.0032368993156900007\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01587572791096237\n",
      "Average test loss: 0.003221070167919\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015850347617434132\n",
      "Average test loss: 0.0032335656589517993\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015813087335891193\n",
      "Average test loss: 0.0032284956404732335\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01581543055259519\n",
      "Average test loss: 0.003250281289840738\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015801095344126225\n",
      "Average test loss: 0.003258663566990031\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015757445096969604\n",
      "Average test loss: 0.003215117854376634\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015739603492948746\n",
      "Average test loss: 0.0032713320934110216\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01572156955798467\n",
      "Average test loss: 0.0032211408680304886\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015701018827656905\n",
      "Average test loss: 0.0032164338827133177\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015683866049680445\n",
      "Average test loss: 0.0032843093731337123\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015667385226322546\n",
      "Average test loss: 0.0032017133637434908\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015639589996801484\n",
      "Average test loss: 0.00320214003448685\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015629597461058034\n",
      "Average test loss: 0.00451625236041016\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015604340816537539\n",
      "Average test loss: 0.0032506902669039036\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015595087096922927\n",
      "Average test loss: 0.003228486235563954\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01555715595351325\n",
      "Average test loss: 0.003234449911862612\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015539819882975684\n",
      "Average test loss: 0.003225034589361813\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015528076445890798\n",
      "Average test loss: 0.0032531511259989605\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015517024640407827\n",
      "Average test loss: 0.003226916157743997\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015500473373466068\n",
      "Average test loss: 0.0032373847253620624\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015478130975531208\n",
      "Average test loss: 0.0032455150317400694\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015453398201200697\n",
      "Average test loss: 0.003237822630339199\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015445973137186634\n",
      "Average test loss: 0.0032583045998795163\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01540781403829654\n",
      "Average test loss: 0.003209562132548955\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015401729233562946\n",
      "Average test loss: 0.0032785795242008236\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015389016608397166\n",
      "Average test loss: 0.0032380083836615085\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01538078242374791\n",
      "Average test loss: 0.0032177243541098304\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015346055464612113\n",
      "Average test loss: 0.003213003572076559\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015334458064701822\n",
      "Average test loss: 0.00324478428127865\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015309781490928596\n",
      "Average test loss: 0.003245718958063258\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015308146097593837\n",
      "Average test loss: 0.0032283509619947938\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015286390153070291\n",
      "Average test loss: 0.0032329729783038296\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01526594857374827\n",
      "Average test loss: 0.0032193923176576694\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01525546154131492\n",
      "Average test loss: 0.003232535413155953\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01523246335486571\n",
      "Average test loss: 0.0033317885676191913\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015223432910939057\n",
      "Average test loss: 0.0032612314307027394\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015200338377720779\n",
      "Average test loss: 0.0032349670111305184\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015189445931050512\n",
      "Average test loss: 0.003251645003341966\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015169523787167337\n",
      "Average test loss: 0.003239849574036068\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015150181549290816\n",
      "Average test loss: 0.003261503124402629\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015143375258478853\n",
      "Average test loss: 0.0032157772742211817\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015124859979583157\n",
      "Average test loss: 0.0032246325477543803\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015110973402029939\n",
      "Average test loss: 0.0032144556692284016\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01509014850606521\n",
      "Average test loss: 0.0032708875876333978\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015070951986644003\n",
      "Average test loss: 0.003247213303628895\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01505233397665951\n",
      "Average test loss: 0.0033036633926547235\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015049004144966602\n",
      "Average test loss: 0.0032633365498234827\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015042787762565745\n",
      "Average test loss: 0.003272503367314736\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015021483470168379\n",
      "Average test loss: 0.003273793096964558\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014998288420339425\n",
      "Average test loss: 0.0032863937912301884\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014999477265609636\n",
      "Average test loss: 0.003963909662846062\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015005189172095723\n",
      "Average test loss: 0.0032376436587009164\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014951382829911179\n",
      "Average test loss: 0.0033041430128117402\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014952125479777654\n",
      "Average test loss: 0.0032691173857698837\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014926788942681419\n",
      "Average test loss: 0.0032874003495607113\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014909623965620995\n",
      "Average test loss: 0.0033079837498565514\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014908292531967163\n",
      "Average test loss: 0.003296917320539554\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014884460200866064\n",
      "Average test loss: 0.003239058437032832\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014872473089231385\n",
      "Average test loss: 0.0032749459300604133\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014867163645724456\n",
      "Average test loss: 0.003273406594577763\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014848661308487256\n",
      "Average test loss: 0.0032616739653878743\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014834050460822052\n",
      "Average test loss: 0.0033019496649503706\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01482018263803588\n",
      "Average test loss: 0.0032868605525129373\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014806469005015161\n",
      "Average test loss: 0.0032977844648477105\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01478618400875065\n",
      "Average test loss: 0.00330333704708351\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014776018434100681\n",
      "Average test loss: 0.003271369928287135\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014771101603077517\n",
      "Average test loss: 0.003324904556075732\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014743707379533184\n",
      "Average test loss: 0.0032705317441787986\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014748388063576486\n",
      "Average test loss: 0.003270066971166266\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014714313342339462\n",
      "Average test loss: 0.003301213532479273\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014715962403350406\n",
      "Average test loss: 0.0033299410198297766\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014693359966079395\n",
      "Average test loss: 0.0032859669669220845\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014685788027114339\n",
      "Average test loss: 0.0032677248025106058\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014676469224194686\n",
      "Average test loss: 0.003332280364094509\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014658886487285297\n",
      "Average test loss: 0.0033224358581420447\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014658965094221963\n",
      "Average test loss: 0.0033005505551894506\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014639466524538066\n",
      "Average test loss: 0.003302965135830972\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014627248799635305\n",
      "Average test loss: 0.0032755203338132965\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01460649718095859\n",
      "Average test loss: 0.0033237463295873668\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014605220078594154\n",
      "Average test loss: 0.0033151964797741838\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014590245524214373\n",
      "Average test loss: 0.003281547444769078\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014597850879033406\n",
      "Average test loss: 0.0033738462169551186\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014561694107949734\n",
      "Average test loss: 0.003330041196404232\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014564081148968802\n",
      "Average test loss: 0.003295844044950273\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014553733564913273\n",
      "Average test loss: 0.0033856284480748905\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01452514525088999\n",
      "Average test loss: 0.00333469107767774\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014527748963899082\n",
      "Average test loss: 0.0032777019357308743\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014521690132717292\n",
      "Average test loss: 0.003363339905730552\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014513371406330003\n",
      "Average test loss: 0.003279549660368098\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014483430896782213\n",
      "Average test loss: 0.0034025522987875673\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014482781808409426\n",
      "Average test loss: 0.0033421492655244136\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014480330668389798\n",
      "Average test loss: 0.003298645580808322\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01446309709466166\n",
      "Average test loss: 0.003303055409962932\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014439316905207104\n",
      "Average test loss: 0.003306989597570565\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014437346031268438\n",
      "Average test loss: 0.0036189168182512122\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014442531860536999\n",
      "Average test loss: 0.0033000745079997513\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014413244658874141\n",
      "Average test loss: 0.003362533181077904\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014397249839372106\n",
      "Average test loss: 0.0034327834888050954\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014403083195288977\n",
      "Average test loss: 0.0033607024434540007\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014371490099363856\n",
      "Average test loss: 0.0033851114532185925\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01438300534006622\n",
      "Average test loss: 0.0035729044940736557\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014375422288974126\n",
      "Average test loss: 0.003267221421210302\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014359450631671482\n",
      "Average test loss: 0.0033223749498526257\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01436057172384527\n",
      "Average test loss: 0.003365711213193006\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014338630691998535\n",
      "Average test loss: 0.0033383266559491556\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014329870354798106\n",
      "Average test loss: 0.0033471817608094877\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014321202861766021\n",
      "Average test loss: 0.003412957649677992\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014306570059723325\n",
      "Average test loss: 0.0034492552058978214\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014316169881986246\n",
      "Average test loss: 0.0033606840533514817\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014289984131852786\n",
      "Average test loss: 0.003331353867633475\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014283897888329294\n",
      "Average test loss: 0.003295551196982463\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014266003596285978\n",
      "Average test loss: 0.003302813063065211\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014261530987090534\n",
      "Average test loss: 0.0033427459239545798\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01424193363967869\n",
      "Average test loss: 0.0033638456583850915\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014261316848297914\n",
      "Average test loss: 0.0033433295473870306\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014234881203207705\n",
      "Average test loss: 0.0033574236730734507\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014217552995516193\n",
      "Average test loss: 0.003349777735562788\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014220589735441738\n",
      "Average test loss: 0.003358135060303741\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014209187588757938\n",
      "Average test loss: 0.003385754947240154\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014209030191103617\n",
      "Average test loss: 0.003365735707183679\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014186031640403801\n",
      "Average test loss: 0.0033794672851347263\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014169698409736156\n",
      "Average test loss: 0.0033824877138766978\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014180780104464955\n",
      "Average test loss: 0.003568032389713658\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014173088808854421\n",
      "Average test loss: 0.003428804258091582\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014149939403765731\n",
      "Average test loss: 0.0034424196941157184\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014154257429970635\n",
      "Average test loss: 0.003352010947548681\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014138869137399727\n",
      "Average test loss: 0.003397857520936264\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014137128108077579\n",
      "Average test loss: 0.003419435318559408\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014121650085681014\n",
      "Average test loss: 0.003587774627738529\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014115935179094473\n",
      "Average test loss: 0.0034533799594889084\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014105328265991476\n",
      "Average test loss: 0.0033049665703955623\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014100085020065307\n",
      "Average test loss: 0.0033862892807357842\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014093678125904665\n",
      "Average test loss: 0.0033824168135308556\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014084263477888373\n",
      "Average test loss: 0.003355851496880253\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014079211877452003\n",
      "Average test loss: 0.003310120388865471\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014065128495295842\n",
      "Average test loss: 0.0035052785283575453\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01405646232350005\n",
      "Average test loss: 0.003390092013610734\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014065234964920416\n",
      "Average test loss: 0.003425414077523682\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014042728693948852\n",
      "Average test loss: 0.0034089004473967684\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014028657123446465\n",
      "Average test loss: 0.0033829605248239304\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014034859935442607\n",
      "Average test loss: 0.0034350500992602773\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01401410062279966\n",
      "Average test loss: 0.003408976066029734\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01401238311909967\n",
      "Average test loss: 0.0034371355738904742\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01402854155583514\n",
      "Average test loss: 0.0033770578627785046\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014020144266386828\n",
      "Average test loss: 0.0033630692965040602\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01398742984318071\n",
      "Average test loss: 0.0034203477481173144\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013985846544305484\n",
      "Average test loss: 0.0034658975346634783\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01397602723704444\n",
      "Average test loss: 0.003398284736192889\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013968631792399619\n",
      "Average test loss: 0.0034357412182208563\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013978553339838981\n",
      "Average test loss: 0.003362812535630332\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013965197030040953\n",
      "Average test loss: 0.0034665419064048263\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01397567359275288\n",
      "Average test loss: 0.0033988029068956773\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013966556600398488\n",
      "Average test loss: 0.003414585205622845\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013946365933451388\n",
      "Average test loss: 0.0035175807271152736\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013947442962063683\n",
      "Average test loss: 0.00335868741137286\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013940103702247143\n",
      "Average test loss: 0.0034564009087367192\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013921117811153332\n",
      "Average test loss: 0.0034472104381355973\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013906360180841551\n",
      "Average test loss: 0.0033568575413276754\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013904665252400769\n",
      "Average test loss: 0.0034054833880315222\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013908521685335372\n",
      "Average test loss: 0.003489745118758745\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013900661374959681\n",
      "Average test loss: 0.00342272419275509\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01388072226030959\n",
      "Average test loss: 0.0034433742908553944\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013871837308009465\n",
      "Average test loss: 0.003402983565090431\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013877749478651417\n",
      "Average test loss: 0.0033395901877019143\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013878729443583223\n",
      "Average test loss: 0.003449930438461403\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013872344887091053\n",
      "Average test loss: 0.003402653676768144\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013860479253033797\n",
      "Average test loss: 0.003369996891460485\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013853736786378754\n",
      "Average test loss: 0.003471028162683878\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013848040591511462\n",
      "Average test loss: 0.003459369262887372\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013831292624274889\n",
      "Average test loss: 0.0034862058870494365\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013830045727392038\n",
      "Average test loss: 0.003388477860018611\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013825634965466129\n",
      "Average test loss: 0.0035408541467040777\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013822539649075932\n",
      "Average test loss: 0.00354147659552594\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013805199132197432\n",
      "Average test loss: 0.003444333641893334\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013823813691735267\n",
      "Average test loss: 0.0033960516014032893\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013807649478316307\n",
      "Average test loss: 0.0034560727145936756\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01380002394815286\n",
      "Average test loss: 0.0034089934062212705\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013807489239507252\n",
      "Average test loss: 0.0034015092984255816\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013783002133998606\n",
      "Average test loss: 0.0034515314387778442\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013789718132052157\n",
      "Average test loss: 0.0035051130967007744\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013793355252179835\n",
      "Average test loss: 0.003501360466082891\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013768501134382353\n",
      "Average test loss: 0.003506771728810337\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013776030733353562\n",
      "Average test loss: 0.0034712894331249927\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013764780405494902\n",
      "Average test loss: 0.003428856111235089\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013748769102825058\n",
      "Average test loss: 0.0034106569192889663\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013747253917985492\n",
      "Average test loss: 0.0034355043094191286\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0137506534266803\n",
      "Average test loss: 0.0034425992754598457\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013747734498646524\n",
      "Average test loss: 0.0033762665148824454\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013751165890859233\n",
      "Average test loss: 0.0034192365848769745\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013735340889129373\n",
      "Average test loss: 0.003512301042262051\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013726945551733176\n",
      "Average test loss: 0.0034297406480958063\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013703619195355309\n",
      "Average test loss: 0.003409033175350891\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013713477482812273\n",
      "Average test loss: 0.00366176647403174\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013691968730754322\n",
      "Average test loss: 0.0035342246024972863\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013705421537988715\n",
      "Average test loss: 0.003490767490532663\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013696405972043673\n",
      "Average test loss: 0.0035065460095389024\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013699501544237137\n",
      "Average test loss: 0.0034602973291443452\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01369083781292041\n",
      "Average test loss: 0.0034227011243088377\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013691235666473707\n",
      "Average test loss: 0.00341030077305105\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013687070117228561\n",
      "Average test loss: 0.003454173899152213\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013660309737755193\n",
      "Average test loss: 0.003836880173948076\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013677708499961428\n",
      "Average test loss: 0.0034956354298111467\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013670757277972168\n",
      "Average test loss: 0.0033472807430144813\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013661083867152531\n",
      "Average test loss: 0.0034464624195049208\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013666826422015827\n",
      "Average test loss: 0.003547482124529779\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013654529218872388\n",
      "Average test loss: 0.0034670838612235254\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013636278184751669\n",
      "Average test loss: 0.0034507565622528395\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013645399370127254\n",
      "Average test loss: 0.003417530452625619\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013651633404195309\n",
      "Average test loss: 0.00340769695966608\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013643815101020865\n",
      "Average test loss: 0.003430451474049025\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01362546598745717\n",
      "Average test loss: 0.0034393781616042056\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013624550422860517\n",
      "Average test loss: 0.0035100458609975045\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013616000711090035\n",
      "Average test loss: 0.0034864151084588635\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013615639935765\n",
      "Average test loss: 0.003493482679956489\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013618956543505192\n",
      "Average test loss: 0.0034690468739718198\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013625622862743008\n",
      "Average test loss: 0.0036532963087989225\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013619106337428093\n",
      "Average test loss: 0.0033864452114535704\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013591011472874217\n",
      "Average test loss: 0.0035022435208989515\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013592846983008914\n",
      "Average test loss: 0.0035110624527765646\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013584890782005258\n",
      "Average test loss: 0.0034357180013838743\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013582740435997645\n",
      "Average test loss: 0.003423081417671508\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013573886864715152\n",
      "Average test loss: 0.00366881997945408\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013573161915772491\n",
      "Average test loss: 0.003471878739280833\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013574559181928635\n",
      "Average test loss: 0.0035043569687339994\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013569740252362357\n",
      "Average test loss: 0.0034660599055803486\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01356644390606218\n",
      "Average test loss: 0.0035718653657370145\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013566830679774284\n",
      "Average test loss: 0.0035490203441845046\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013543735809624195\n",
      "Average test loss: 0.0034539617606335216\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013539015645782153\n",
      "Average test loss: 0.003492133536376059\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013554908076922098\n",
      "Average test loss: 0.0034489567908975814\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013545846017698447\n",
      "Average test loss: 0.0034229536495274967\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013531923457980157\n",
      "Average test loss: 0.0035702494201767777\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013533825002610684\n",
      "Average test loss: 0.0034702241842945415\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01354246262460947\n",
      "Average test loss: 0.0034789093885984686\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013538845171531042\n",
      "Average test loss: 0.0034197187531325554\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013525066107511521\n",
      "Average test loss: 0.0035226601796845597\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013526052317685552\n",
      "Average test loss: 0.0034837002418935297\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01350262153065867\n",
      "Average test loss: 0.0034454767695731585\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013518884757326708\n",
      "Average test loss: 0.0035027319859299395\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013514079739236169\n",
      "Average test loss: 0.0035630533444798657\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013510094766815504\n",
      "Average test loss: 0.0034481919150178633\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013503073854578865\n",
      "Average test loss: 0.0037481745274530517\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01349520617723465\n",
      "Average test loss: 0.003486665913938648\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013498392020662626\n",
      "Average test loss: 0.0035803955677482817\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013476419986950027\n",
      "Average test loss: 0.003446323528678881\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0134767320735587\n",
      "Average test loss: 0.0035164499895440206\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013488448180258274\n",
      "Average test loss: 0.003480415169149637\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.056536763300498324\n",
      "Average test loss: 0.00431237216004067\n",
      "Epoch 2/300\n",
      "Average training loss: 0.019623931869864462\n",
      "Average test loss: 0.0038260311029023595\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01805403906438086\n",
      "Average test loss: 0.003573519060595168\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01730349559088548\n",
      "Average test loss: 0.003485720714761151\n",
      "Epoch 5/300\n",
      "Average training loss: 0.016796435093714132\n",
      "Average test loss: 0.0034346580447422136\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016434472053415247\n",
      "Average test loss: 0.0033098876596324976\n",
      "Epoch 7/300\n",
      "Average training loss: 0.016134035244584084\n",
      "Average test loss: 0.0032348045433561005\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015882076086269483\n",
      "Average test loss: 0.00313742274687522\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015659224004381234\n",
      "Average test loss: 0.003089875242776341\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015476464210285082\n",
      "Average test loss: 0.003066408815690213\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015305707138445642\n",
      "Average test loss: 0.0030306563764396642\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01515792700731092\n",
      "Average test loss: 0.0029932801512380443\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01501818987975518\n",
      "Average test loss: 0.0029705428029927943\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014902915690508153\n",
      "Average test loss: 0.002924195206620627\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014795210419429673\n",
      "Average test loss: 0.0029327636156231164\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014676203038129542\n",
      "Average test loss: 0.0029142225798633363\n",
      "Epoch 17/300\n",
      "Average training loss: 0.014582798529002402\n",
      "Average test loss: 0.0028888275395664905\n",
      "Epoch 18/300\n",
      "Average training loss: 0.014501604224244754\n",
      "Average test loss: 0.0028675176257060633\n",
      "Epoch 19/300\n",
      "Average training loss: 0.014389425796767075\n",
      "Average test loss: 0.0028301941893166967\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01431604410459598\n",
      "Average test loss: 0.0028243496492505074\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014230373533235656\n",
      "Average test loss: 0.002823746731608278\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014163559565113651\n",
      "Average test loss: 0.0027780439926104413\n",
      "Epoch 23/300\n",
      "Average training loss: 0.014087883981565635\n",
      "Average test loss: 0.002768852137029171\n",
      "Epoch 24/300\n",
      "Average training loss: 0.014018687864144643\n",
      "Average test loss: 0.0027488687932491302\n",
      "Epoch 25/300\n",
      "Average training loss: 0.013961527674562402\n",
      "Average test loss: 0.0027262724028486343\n",
      "Epoch 26/300\n",
      "Average training loss: 0.013893055361178186\n",
      "Average test loss: 0.0027269044663343163\n",
      "Epoch 27/300\n",
      "Average training loss: 0.013838117740220493\n",
      "Average test loss: 0.0027151067656361392\n",
      "Epoch 28/300\n",
      "Average training loss: 0.013802291097740333\n",
      "Average test loss: 0.002707900124291579\n",
      "Epoch 29/300\n",
      "Average training loss: 0.013738266676664352\n",
      "Average test loss: 0.0026951890610572363\n",
      "Epoch 30/300\n",
      "Average training loss: 0.013694396604266431\n",
      "Average test loss: 0.002696380796117915\n",
      "Epoch 31/300\n",
      "Average training loss: 0.013653288182285096\n",
      "Average test loss: 0.0026730878500060905\n",
      "Epoch 32/300\n",
      "Average training loss: 0.013599351486398114\n",
      "Average test loss: 0.002657820834053887\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01356058672732777\n",
      "Average test loss: 0.002685176219377253\n",
      "Epoch 34/300\n",
      "Average training loss: 0.013524377205305629\n",
      "Average test loss: 0.00265454821764595\n",
      "Epoch 35/300\n",
      "Average training loss: 0.013489428858790132\n",
      "Average test loss: 0.0026631103532595768\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01345188994705677\n",
      "Average test loss: 0.0026455537509173154\n",
      "Epoch 37/300\n",
      "Average training loss: 0.013432698101633125\n",
      "Average test loss: 0.0026580752537896236\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01338424852821562\n",
      "Average test loss: 0.002654158084549838\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013349839046597482\n",
      "Average test loss: 0.002647928161960509\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013317010239594513\n",
      "Average test loss: 0.0026290052723553447\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013285891929434406\n",
      "Average test loss: 0.0026148252346449427\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013249268259439204\n",
      "Average test loss: 0.0026111515879424082\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013237613145675925\n",
      "Average test loss: 0.002614454259475072\n",
      "Epoch 44/300\n",
      "Average training loss: 0.013203019007212586\n",
      "Average test loss: 0.002621041073774298\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01316735164821148\n",
      "Average test loss: 0.0026252032023751076\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013142976529068416\n",
      "Average test loss: 0.0026027206474294267\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013113874916401175\n",
      "Average test loss: 0.002599087901827362\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013093512605461809\n",
      "Average test loss: 0.0025906270111186636\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01306936810993486\n",
      "Average test loss: 0.0025997608196404245\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013042338202397028\n",
      "Average test loss: 0.0025851261984142994\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013017184562153287\n",
      "Average test loss: 0.0025755671157191196\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013009467813703749\n",
      "Average test loss: 0.002586459039606982\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012956428736448289\n",
      "Average test loss: 0.0026363521905408967\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012947392874293856\n",
      "Average test loss: 0.0025833733457451065\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012919287239511808\n",
      "Average test loss: 0.00257576128302349\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012898612918953101\n",
      "Average test loss: 0.002578713095229533\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012885772301918931\n",
      "Average test loss: 0.0025751894034652246\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01286709977603621\n",
      "Average test loss: 0.0025884143283797633\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012832611242930095\n",
      "Average test loss: 0.002579925587814715\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012822594947285122\n",
      "Average test loss: 0.0025752448679672345\n",
      "Epoch 61/300\n",
      "Average training loss: 0.012792394840882884\n",
      "Average test loss: 0.0025741249941703347\n",
      "Epoch 62/300\n",
      "Average training loss: 0.012781601150416665\n",
      "Average test loss: 0.00260483364864356\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012763406759334935\n",
      "Average test loss: 0.0026151904836297034\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012738849009904596\n",
      "Average test loss: 0.0025725477726923096\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012716887365612719\n",
      "Average test loss: 0.002570778111823731\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012702920987374253\n",
      "Average test loss: 0.002566569918559657\n",
      "Epoch 67/300\n",
      "Average training loss: 0.012687743763128917\n",
      "Average test loss: 0.0025646487221949632\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012664205902566512\n",
      "Average test loss: 0.002569261282682419\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01263728643208742\n",
      "Average test loss: 0.002560920684494906\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012645446734295952\n",
      "Average test loss: 0.002556350719804565\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01260865380242467\n",
      "Average test loss: 0.002632047046389845\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01259067222972711\n",
      "Average test loss: 0.0026010274066486294\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01256727794226673\n",
      "Average test loss: 0.002586810971506768\n",
      "Epoch 74/300\n",
      "Average training loss: 0.012553477610150974\n",
      "Average test loss: 0.002559744691890147\n",
      "Epoch 75/300\n",
      "Average training loss: 0.012535410987834136\n",
      "Average test loss: 0.0026068936518083017\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012524536220563782\n",
      "Average test loss: 0.0025608567405078146\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012495874090327158\n",
      "Average test loss: 0.002569427216011617\n",
      "Epoch 78/300\n",
      "Average training loss: 0.012497979337970417\n",
      "Average test loss: 0.0025763624045583937\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012474939046634569\n",
      "Average test loss: 0.0025719044264405967\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012459024451673031\n",
      "Average test loss: 0.002650820402221547\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012437459847165478\n",
      "Average test loss: 0.002567747784157594\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012427679275059039\n",
      "Average test loss: 0.0025612919417520364\n",
      "Epoch 83/300\n",
      "Average training loss: 0.012397835556831625\n",
      "Average test loss: 0.002633750994172361\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012390460064013798\n",
      "Average test loss: 0.002577037014067173\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012379360386066967\n",
      "Average test loss: 0.002594219686049554\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012356691783501043\n",
      "Average test loss: 0.0026069391606789497\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012358385479284657\n",
      "Average test loss: 0.0026073136154769194\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012329240038163132\n",
      "Average test loss: 0.002578155730747514\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012314979936513636\n",
      "Average test loss: 0.0025669634226295683\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012285473138093948\n",
      "Average test loss: 0.0025537296415617068\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012283521825240718\n",
      "Average test loss: 0.002570445959145824\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012273178978926606\n",
      "Average test loss: 0.002613027214176125\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012257957420415348\n",
      "Average test loss: 0.0025892087777869568\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012230549302366044\n",
      "Average test loss: 0.002595355806561808\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012223517265584734\n",
      "Average test loss: 0.0025562370186671614\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012212005464567079\n",
      "Average test loss: 0.0026040401632587114\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012202770171893968\n",
      "Average test loss: 0.0025718040563580064\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01218784794708093\n",
      "Average test loss: 0.0025938419078787166\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01217783921957016\n",
      "Average test loss: 0.002669564647600055\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012154609029491743\n",
      "Average test loss: 0.002580574251918329\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012139447477956613\n",
      "Average test loss: 0.002585134872338838\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012129381882647674\n",
      "Average test loss: 0.0025839473596877523\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012114645989404784\n",
      "Average test loss: 0.002624934849846694\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012098166558477614\n",
      "Average test loss: 0.0026205316125932666\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01208409408811066\n",
      "Average test loss: 0.0025918992910948066\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012084601797163487\n",
      "Average test loss: 0.00267796080869933\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012071658660140303\n",
      "Average test loss: 0.002591910858742065\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01204870289646917\n",
      "Average test loss: 0.002578243312943313\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012043412456081972\n",
      "Average test loss: 0.00262671505059633\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012017317394001616\n",
      "Average test loss: 0.002613568718648619\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012012951771832175\n",
      "Average test loss: 0.002635155407504903\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012004322631491555\n",
      "Average test loss: 0.002584474155886306\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011986320039464367\n",
      "Average test loss: 0.0027396183448533218\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011974068557222684\n",
      "Average test loss: 0.0026565202911280924\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011975843308700456\n",
      "Average test loss: 0.0025929030573202502\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011961760925749937\n",
      "Average test loss: 0.0026479227408352825\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01194489596535762\n",
      "Average test loss: 0.002605834802198741\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011926601399978002\n",
      "Average test loss: 0.0025967948229776486\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011923956625991397\n",
      "Average test loss: 0.0025997967566880917\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011920898550914393\n",
      "Average test loss: 0.0026469915019762184\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011898880757391452\n",
      "Average test loss: 0.0025783471398883396\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011890246070093579\n",
      "Average test loss: 0.0026423601160446802\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011880935283170807\n",
      "Average test loss: 0.002596989861586028\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011877418316072887\n",
      "Average test loss: 0.0025805316769207517\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011856791898608207\n",
      "Average test loss: 0.0026096042450517417\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01184565310469932\n",
      "Average test loss: 0.002652913086737196\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011833724544280106\n",
      "Average test loss: 0.0026941774766892196\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011825269472267893\n",
      "Average test loss: 0.002620925991071595\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011818866621288988\n",
      "Average test loss: 0.0026133897518739106\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011802200992074277\n",
      "Average test loss: 0.0026144658902453053\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011790545930465062\n",
      "Average test loss: 0.0026099192262109784\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01178719988051388\n",
      "Average test loss: 0.0026201086114678116\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011773240968585015\n",
      "Average test loss: 0.002633605586985747\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011763329859409068\n",
      "Average test loss: 0.0026356602309064734\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011757621771759458\n",
      "Average test loss: 0.0026586896436702873\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01174302380780379\n",
      "Average test loss: 0.0026046375981014637\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011729790074129899\n",
      "Average test loss: 0.002617609950713813\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011721491231686539\n",
      "Average test loss: 0.00263742409625815\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011711296246283583\n",
      "Average test loss: 0.0026100212149322032\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01171283399562041\n",
      "Average test loss: 0.0026602471166600784\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011702369098862013\n",
      "Average test loss: 0.002641525493003428\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011687031239271164\n",
      "Average test loss: 0.002631447225705617\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011666794573267301\n",
      "Average test loss: 0.002628334108947052\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011667175856729349\n",
      "Average test loss: 0.0026294413110655216\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011657456410427888\n",
      "Average test loss: 0.002629683340796166\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011656022144688501\n",
      "Average test loss: 0.0026191221974376177\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01164097975525591\n",
      "Average test loss: 0.002613852149910397\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011644919312662549\n",
      "Average test loss: 0.0026312170564714407\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01162511254929834\n",
      "Average test loss: 0.0026169590591970417\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011607043532033762\n",
      "Average test loss: 0.0026311568181133934\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011594349728690253\n",
      "Average test loss: 0.0026424493584781884\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011599042455355327\n",
      "Average test loss: 0.0026434550633033117\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011592864810592598\n",
      "Average test loss: 0.002627609822485182\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01158197453783618\n",
      "Average test loss: 0.002660251360076169\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01158357730963164\n",
      "Average test loss: 0.0026147966293825044\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011567486878898409\n",
      "Average test loss: 0.002734089751003517\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011552554815593693\n",
      "Average test loss: 0.002715703544103437\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011544720763961474\n",
      "Average test loss: 0.002657812761556771\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011547143110798465\n",
      "Average test loss: 0.00262912357536455\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01153450281255775\n",
      "Average test loss: 0.0026620941284216113\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011510368314882119\n",
      "Average test loss: 0.002619404236268666\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011508721566034687\n",
      "Average test loss: 0.0026088297025611005\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011508391611278057\n",
      "Average test loss: 0.002690298819086618\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011500585723668338\n",
      "Average test loss: 0.002651195207403766\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01148738787902726\n",
      "Average test loss: 0.0026561848328759273\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011480167664587497\n",
      "Average test loss: 0.002641324484617346\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011481752820312977\n",
      "Average test loss: 0.0026682608605672915\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011465198984990517\n",
      "Average test loss: 0.002686876564286649\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011469858371549183\n",
      "Average test loss: 0.0026735138001127375\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011456450705726942\n",
      "Average test loss: 0.0027107660046054257\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011436704773041937\n",
      "Average test loss: 0.002616276884244548\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011451884626514381\n",
      "Average test loss: 0.002649528661846287\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011426895836989085\n",
      "Average test loss: 0.0026480508394953277\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011422230808271303\n",
      "Average test loss: 0.002688830755857958\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011425583312080967\n",
      "Average test loss: 0.0026491514758931268\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01140936412413915\n",
      "Average test loss: 0.0026156353743539917\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011402949604723189\n",
      "Average test loss: 0.0026729318170497815\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011400636475119326\n",
      "Average test loss: 0.0026569631843724184\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011395046871569421\n",
      "Average test loss: 0.002702039850875735\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011385146777662966\n",
      "Average test loss: 0.002689380041220122\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011378867838531733\n",
      "Average test loss: 0.0030936420491586127\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011377517727514108\n",
      "Average test loss: 0.002678151578331987\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011355646624333329\n",
      "Average test loss: 0.002655002769496706\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01136271721124649\n",
      "Average test loss: 0.00264964883422686\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011350636444158024\n",
      "Average test loss: 0.0026927179296811423\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011342149305674764\n",
      "Average test loss: 0.0026599269489654236\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01133531999753581\n",
      "Average test loss: 0.002710204642266035\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011331420342127483\n",
      "Average test loss: 0.002613580853264365\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011337340012192726\n",
      "Average test loss: 0.0027509309077221487\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011323995201124086\n",
      "Average test loss: 0.0026832961527009807\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011308981764647695\n",
      "Average test loss: 0.002792471149108476\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011318894756337006\n",
      "Average test loss: 0.002646855785407954\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01131051995108525\n",
      "Average test loss: 0.0026800488388786714\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011288458836575348\n",
      "Average test loss: 0.002721574404069947\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01128904370798005\n",
      "Average test loss: 0.002682204092128409\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011284672339343363\n",
      "Average test loss: 0.002735585725141896\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011274936166902384\n",
      "Average test loss: 0.0026534795403066608\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011274529841211108\n",
      "Average test loss: 0.002689761481558283\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01126999664803346\n",
      "Average test loss: 0.002709431927651167\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011261009326411619\n",
      "Average test loss: 0.002671449084455768\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01126009885304504\n",
      "Average test loss: 0.0026782100616643827\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011260457942055331\n",
      "Average test loss: 0.00271082689302663\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011246776659455565\n",
      "Average test loss: 0.002695797629861368\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011242566309869289\n",
      "Average test loss: 0.0027020842662702003\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011215258145497904\n",
      "Average test loss: 0.002677206810770763\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011218925163149833\n",
      "Average test loss: 0.0026867346207921704\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011220452696084977\n",
      "Average test loss: 0.002737386248902314\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011215309142238564\n",
      "Average test loss: 0.0026988907636453707\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011225207640065087\n",
      "Average test loss: 0.0026843747339314883\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011191106049550904\n",
      "Average test loss: 0.0027145439758896827\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011199669353663922\n",
      "Average test loss: 0.002761689176162084\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011187229285637538\n",
      "Average test loss: 0.0028077413108613756\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011183017359839545\n",
      "Average test loss: 0.0026880578580829833\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011186205389599005\n",
      "Average test loss: 0.0027421651003468367\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01118317985534668\n",
      "Average test loss: 0.0027324112088729937\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011167751925686996\n",
      "Average test loss: 0.002723756324913767\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011184633197883764\n",
      "Average test loss: 0.002729372517309255\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011168820708162254\n",
      "Average test loss: 0.0028587302253064184\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011155405202673541\n",
      "Average test loss: 0.002707529174784819\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01115547741866774\n",
      "Average test loss: 0.002761590198096302\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011150609485805034\n",
      "Average test loss: 0.002765735067634119\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011143114099899928\n",
      "Average test loss: 0.0027421416701335047\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011145436607301235\n",
      "Average test loss: 0.002691629889110724\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01113172197341919\n",
      "Average test loss: 0.00277123274281621\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01112907010068496\n",
      "Average test loss: 0.002696487563972672\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011135598747266664\n",
      "Average test loss: 0.0027302358324329057\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0111166260689497\n",
      "Average test loss: 0.0027203817564166255\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011110345098707411\n",
      "Average test loss: 0.0027638765347914565\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011099976994097233\n",
      "Average test loss: 0.0027590625120533837\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01110436762869358\n",
      "Average test loss: 0.0030509364211724864\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011105887159705162\n",
      "Average test loss: 0.0026887425509177975\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011087883069283433\n",
      "Average test loss: 0.002807120806020167\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011090036200980346\n",
      "Average test loss: 0.0027236469363172847\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011089488720728291\n",
      "Average test loss: 0.00279611989379757\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011070908383362823\n",
      "Average test loss: 0.002762331746518612\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011076766748395231\n",
      "Average test loss: 0.0029282468099974925\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01107337548997667\n",
      "Average test loss: 0.0030238257170551353\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01108186875945992\n",
      "Average test loss: 0.0027327883354284696\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011065825611766842\n",
      "Average test loss: 0.0027495907959010865\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011052642887665165\n",
      "Average test loss: 0.00278188097394175\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011057837444874975\n",
      "Average test loss: 0.00281405583769083\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011055361384318934\n",
      "Average test loss: 0.0027547738957736227\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01104676099287139\n",
      "Average test loss: 0.002675550104636285\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011041534401062461\n",
      "Average test loss: 0.002814235513098538\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01104889542857806\n",
      "Average test loss: 0.002673776524141431\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011037482619285584\n",
      "Average test loss: 0.0027597517140934037\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011027901776134968\n",
      "Average test loss: 0.0028027225937694313\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01101937778459655\n",
      "Average test loss: 0.0027032676608198217\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011026977614396149\n",
      "Average test loss: 0.0027180073507544066\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011011499925620026\n",
      "Average test loss: 0.002858935442028774\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011013355371852716\n",
      "Average test loss: 0.0027210342393567165\n",
      "Epoch 252/300\n",
      "Average training loss: 0.011005319675637617\n",
      "Average test loss: 0.002759822267418106\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011008054422007666\n",
      "Average test loss: 0.0027597573232940502\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01100279735442665\n",
      "Average test loss: 0.002736986864565147\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010998162246412702\n",
      "Average test loss: 0.00269209715579119\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011002647332019276\n",
      "Average test loss: 0.002726958754989836\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010991076562967566\n",
      "Average test loss: 0.0027618725998327137\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010990574037035307\n",
      "Average test loss: 0.002729238883488708\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010972827525602446\n",
      "Average test loss: 0.0027834121535221736\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010986790376404921\n",
      "Average test loss: 0.0027611208103804127\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010979998083578215\n",
      "Average test loss: 0.0026939436619480453\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010971628598868848\n",
      "Average test loss: 0.002753972665924165\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01097057969785399\n",
      "Average test loss: 0.0029289293978363275\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01097452365896768\n",
      "Average test loss: 0.0027611986231058836\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010959190712206893\n",
      "Average test loss: 0.002773839928830663\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010961302909586164\n",
      "Average test loss: 0.002749497340578172\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01094921352631516\n",
      "Average test loss: 0.0027156238068516054\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010946002959377235\n",
      "Average test loss: 0.0027317152244763243\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010943442023462719\n",
      "Average test loss: 0.0027340797192106645\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010947278911868731\n",
      "Average test loss: 0.002846366619070371\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010945897042751312\n",
      "Average test loss: 0.0027383731632596915\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010940446246829298\n",
      "Average test loss: 0.002715314630005095\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010937927342123455\n",
      "Average test loss: 0.002804602700803015\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010919589195814398\n",
      "Average test loss: 0.0029655154511953395\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010924438667794068\n",
      "Average test loss: 0.0028173762183222504\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010921746342132489\n",
      "Average test loss: 0.002853324037252201\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010919163288341627\n",
      "Average test loss: 0.0028151682023372913\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010918410215112898\n",
      "Average test loss: 0.002735648682858381\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010909971897800763\n",
      "Average test loss: 0.002699999781118499\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010916754798756706\n",
      "Average test loss: 0.00273382781052755\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010906605328950617\n",
      "Average test loss: 0.0028087405264377594\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010903984343840016\n",
      "Average test loss: 0.002785236160788271\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010906493881096442\n",
      "Average test loss: 0.002769538028165698\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010893326916628414\n",
      "Average test loss: 0.002760574642361866\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010907452461620172\n",
      "Average test loss: 0.0027932141822659307\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010883229908015993\n",
      "Average test loss: 0.0027850018000851076\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010886447640756766\n",
      "Average test loss: 0.0027182062088201443\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01088617109424538\n",
      "Average test loss: 0.002741089049519764\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01086797347250912\n",
      "Average test loss: 0.0027502844870711364\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010885784045689635\n",
      "Average test loss: 0.0027798873202668295\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010880912103586727\n",
      "Average test loss: 0.002782861295984023\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01087017954678999\n",
      "Average test loss: 0.0027864597369399334\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01085793625563383\n",
      "Average test loss: 0.002809931190373997\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010863567845688926\n",
      "Average test loss: 0.0027638440082470577\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010856670998864703\n",
      "Average test loss: 0.0027716081279019517\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010849479633900854\n",
      "Average test loss: 0.0027645985043297213\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010852267124586635\n",
      "Average test loss: 0.0027318103810151416\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010851381372246477\n",
      "Average test loss: 0.002788218012286557\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010848237379143635\n",
      "Average test loss: 0.0029097949156744614\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010848099856740899\n",
      "Average test loss: 0.002762758328372406\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05337825071480539\n",
      "Average test loss: 0.003690800973110729\n",
      "Epoch 2/300\n",
      "Average training loss: 0.016489111476474336\n",
      "Average test loss: 0.0032486995280616815\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014826243833535247\n",
      "Average test loss: 0.002959993723159035\n",
      "Epoch 4/300\n",
      "Average training loss: 0.014023502219054435\n",
      "Average test loss: 0.0027794291648185914\n",
      "Epoch 5/300\n",
      "Average training loss: 0.013493580780095524\n",
      "Average test loss: 0.0026632088511768315\n",
      "Epoch 6/300\n",
      "Average training loss: 0.013063429249657525\n",
      "Average test loss: 0.0026141987331211565\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012736527079509364\n",
      "Average test loss: 0.00246062498456902\n",
      "Epoch 8/300\n",
      "Average training loss: 0.012469629223148027\n",
      "Average test loss: 0.002466907114928795\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01223809157560269\n",
      "Average test loss: 0.00237734916806221\n",
      "Epoch 10/300\n",
      "Average training loss: 0.012047428011480305\n",
      "Average test loss: 0.0023611560923357806\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011869864209658569\n",
      "Average test loss: 0.00229381978760163\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011728516749209827\n",
      "Average test loss: 0.002292764611956146\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011576343619989025\n",
      "Average test loss: 0.00222614655798922\n",
      "Epoch 14/300\n",
      "Average training loss: 0.011452311066289743\n",
      "Average test loss: 0.002199925212189555\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011336543309191862\n",
      "Average test loss: 0.0023328196979645224\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011230322024888462\n",
      "Average test loss: 0.002141288367824422\n",
      "Epoch 17/300\n",
      "Average training loss: 0.011128870493008031\n",
      "Average test loss: 0.0023061173365761835\n",
      "Epoch 18/300\n",
      "Average training loss: 0.011028752468526363\n",
      "Average test loss: 0.002102934923229946\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01093411214152972\n",
      "Average test loss: 0.002083970866786937\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010846371312936146\n",
      "Average test loss: 0.0020956920217722655\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010772330097854138\n",
      "Average test loss: 0.0020789090730249883\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01069589478108618\n",
      "Average test loss: 0.002021485664157404\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010625445830325285\n",
      "Average test loss: 0.002020836930618518\n",
      "Epoch 24/300\n",
      "Average training loss: 0.010582007103496128\n",
      "Average test loss: 0.0020021238850636614\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010510002381271786\n",
      "Average test loss: 0.001979624715840651\n",
      "Epoch 26/300\n",
      "Average training loss: 0.010480713240388366\n",
      "Average test loss: 0.00198567672756811\n",
      "Epoch 27/300\n",
      "Average training loss: 0.010415738641387886\n",
      "Average test loss: 0.0019580345149669384\n",
      "Epoch 28/300\n",
      "Average training loss: 0.010354401095045937\n",
      "Average test loss: 0.0019521985546582276\n",
      "Epoch 29/300\n",
      "Average training loss: 0.010330462608072492\n",
      "Average test loss: 0.00197267634069754\n",
      "Epoch 30/300\n",
      "Average training loss: 0.010269362097813024\n",
      "Average test loss: 0.001979260525873138\n",
      "Epoch 31/300\n",
      "Average training loss: 0.010233416435619196\n",
      "Average test loss: 0.001963304039090872\n",
      "Epoch 32/300\n",
      "Average training loss: 0.010202929376728004\n",
      "Average test loss: 0.0019237631818072663\n",
      "Epoch 33/300\n",
      "Average training loss: 0.010149649877515104\n",
      "Average test loss: 0.001912032440925638\n",
      "Epoch 34/300\n",
      "Average training loss: 0.010132384594115945\n",
      "Average test loss: 0.001916662155650556\n",
      "Epoch 35/300\n",
      "Average training loss: 0.010094591641177734\n",
      "Average test loss: 0.0019159070180935992\n",
      "Epoch 36/300\n",
      "Average training loss: 0.010062248446047306\n",
      "Average test loss: 0.0019010843999890817\n",
      "Epoch 37/300\n",
      "Average training loss: 0.010033165319098366\n",
      "Average test loss: 0.0018927182868743936\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009994774143728945\n",
      "Average test loss: 0.001899610800254676\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009965798761281702\n",
      "Average test loss: 0.0018838668349716399\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009931787984238731\n",
      "Average test loss: 0.0018766978056066566\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009915352606938944\n",
      "Average test loss: 0.0018763315402385262\n",
      "Epoch 42/300\n",
      "Average training loss: 0.009888415308462248\n",
      "Average test loss: 0.0018682272674308883\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009864212474061383\n",
      "Average test loss: 0.0019713842858456905\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009843524202704429\n",
      "Average test loss: 0.0018668236537939973\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009804892662498685\n",
      "Average test loss: 0.0018508210600250298\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009787211333711942\n",
      "Average test loss: 0.001884690225124359\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009768705764578448\n",
      "Average test loss: 0.0018737121087809405\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009741876265241041\n",
      "Average test loss: 0.0018821079477460848\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009721946644286314\n",
      "Average test loss: 0.0018550226984338628\n",
      "Epoch 50/300\n",
      "Average training loss: 0.00970273820269439\n",
      "Average test loss: 0.0018425296178708475\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009669188649290138\n",
      "Average test loss: 0.0018659308755563364\n",
      "Epoch 52/300\n",
      "Average training loss: 0.009655535519123078\n",
      "Average test loss: 0.0018683370285564\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009642282233470016\n",
      "Average test loss: 0.001883284073892153\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009614681522051494\n",
      "Average test loss: 0.001840659162029624\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009593891176084677\n",
      "Average test loss: 0.0018444858577309382\n",
      "Epoch 56/300\n",
      "Average training loss: 0.00958659786855181\n",
      "Average test loss: 0.0018704530347345603\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009553636653555764\n",
      "Average test loss: 0.001839372212274207\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009546814674304592\n",
      "Average test loss: 0.0018351198531066377\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009526461742818355\n",
      "Average test loss: 0.0018103701607841585\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009511146144320567\n",
      "Average test loss: 0.001828865692536864\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009483832710319095\n",
      "Average test loss: 0.0018250942426837153\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009469913442101744\n",
      "Average test loss: 0.0018527431952663593\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009456358344604572\n",
      "Average test loss: 0.0018197782446319859\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009443311578697629\n",
      "Average test loss: 0.001822667699928085\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009423903877950376\n",
      "Average test loss: 0.0018321076658450895\n",
      "Epoch 66/300\n",
      "Average training loss: 0.00940143997925851\n",
      "Average test loss: 0.0018314663068287901\n",
      "Epoch 67/300\n",
      "Average training loss: 0.00938341036438942\n",
      "Average test loss: 0.001827947976274623\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009381632240696085\n",
      "Average test loss: 0.001843981152607335\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00936892240991195\n",
      "Average test loss: 0.0018084988157368368\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009338658832013607\n",
      "Average test loss: 0.0018214553888473246\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009337350928948985\n",
      "Average test loss: 0.0018520653667445813\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009320201627082294\n",
      "Average test loss: 0.001830182199780312\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009303502327038183\n",
      "Average test loss: 0.0018258582113517656\n",
      "Epoch 74/300\n",
      "Average training loss: 0.009282460360891288\n",
      "Average test loss: 0.0018107284878691038\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009269787879453765\n",
      "Average test loss: 0.0018151037969316045\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00926708447933197\n",
      "Average test loss: 0.001838535040203068\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009240169212222099\n",
      "Average test loss: 0.001808942297163109\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009230213101953268\n",
      "Average test loss: 0.0026761434219984546\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009210658056454527\n",
      "Average test loss: 0.0018175186280988984\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0092072868289219\n",
      "Average test loss: 0.0018581870968143145\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00919785383509265\n",
      "Average test loss: 0.0018213781003529827\n",
      "Epoch 82/300\n",
      "Average training loss: 0.009175612571752733\n",
      "Average test loss: 0.0018824219192481704\n",
      "Epoch 83/300\n",
      "Average training loss: 0.009164838247001171\n",
      "Average test loss: 0.001824101879571875\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009147198939488994\n",
      "Average test loss: 0.0018178042263413468\n",
      "Epoch 85/300\n",
      "Average training loss: 0.009147325861785146\n",
      "Average test loss: 0.0018123762659314605\n",
      "Epoch 86/300\n",
      "Average training loss: 0.009126618987984128\n",
      "Average test loss: 0.0018129585145248307\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009123515491684277\n",
      "Average test loss: 0.0018722344781789515\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009096929502569967\n",
      "Average test loss: 0.0018699725441013774\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009086902377506097\n",
      "Average test loss: 0.0018463989859446883\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009087855715718534\n",
      "Average test loss: 0.0018097125650900934\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009066233755399783\n",
      "Average test loss: 0.0018140800734981896\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009050820954971844\n",
      "Average test loss: 0.0018233729394980603\n",
      "Epoch 93/300\n",
      "Average training loss: 0.009039605353027582\n",
      "Average test loss: 0.0018065098218826784\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009040654751989577\n",
      "Average test loss: 0.001819352346058521\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009019593829082118\n",
      "Average test loss: 0.0018294634284037683\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009012352084120115\n",
      "Average test loss: 0.0018252672312988175\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008994096791992584\n",
      "Average test loss: 0.0018383057663838068\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008990330158422391\n",
      "Average test loss: 0.0018077693336332838\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008984064714362224\n",
      "Average test loss: 0.0018210544493049382\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008972284580270449\n",
      "Average test loss: 0.0018481254362397723\n",
      "Epoch 101/300\n",
      "Average training loss: 0.00895448110335403\n",
      "Average test loss: 0.0018208711444296772\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008942051077468528\n",
      "Average test loss: 0.0018440341821147335\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008944165954987208\n",
      "Average test loss: 0.0018318001700358259\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008925688515106837\n",
      "Average test loss: 0.00181478052565621\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008918357763025495\n",
      "Average test loss: 0.002052459100468291\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008903305715156925\n",
      "Average test loss: 0.0018123365358139077\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008906057139237722\n",
      "Average test loss: 0.0018609685561516219\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008883210027383433\n",
      "Average test loss: 0.0018155112657178608\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00887072948863109\n",
      "Average test loss: 0.0018604540371646484\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008865172789328628\n",
      "Average test loss: 0.0018292726492509246\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008858948373960123\n",
      "Average test loss: 0.0018048724212890697\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008852205876260996\n",
      "Average test loss: 0.0018329251779036389\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008842324497799078\n",
      "Average test loss: 0.0018252256086303128\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00882724327262905\n",
      "Average test loss: 0.0018344317273133331\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008830461882468726\n",
      "Average test loss: 0.0018334691010208593\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008818439812295967\n",
      "Average test loss: 0.0018938679782052835\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008809330984122224\n",
      "Average test loss: 0.0018187627587467432\n",
      "Epoch 118/300\n",
      "Average training loss: 0.00879717342928052\n",
      "Average test loss: 0.0018563761444141467\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008786950108905633\n",
      "Average test loss: 0.0018480590349063278\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008782549329929881\n",
      "Average test loss: 0.001833256425232523\n",
      "Epoch 121/300\n",
      "Average training loss: 0.00877103976905346\n",
      "Average test loss: 0.0018460342883028918\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008766252821932236\n",
      "Average test loss: 0.001824405865640276\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008751275300151773\n",
      "Average test loss: 0.0018210188844758604\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008748090659992562\n",
      "Average test loss: 0.001826684788076414\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008739070114162233\n",
      "Average test loss: 0.0018956843837060861\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008731353203455607\n",
      "Average test loss: 0.0018306026159682208\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008728457384639315\n",
      "Average test loss: 0.001844151224113173\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008713816011117564\n",
      "Average test loss: 0.001868116308728026\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008705315478146076\n",
      "Average test loss: 0.0018539118779202302\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008692036561667919\n",
      "Average test loss: 0.0018106171242478822\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008689893609533707\n",
      "Average test loss: 0.001839603100799852\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008678430006735854\n",
      "Average test loss: 0.0018255907737960418\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008671553677982754\n",
      "Average test loss: 0.0018442806678099764\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008671193342241977\n",
      "Average test loss: 0.0018256596556554237\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008655834587911766\n",
      "Average test loss: 0.0018479311648342345\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008650485863288244\n",
      "Average test loss: 0.0018452965634771518\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008643184740096331\n",
      "Average test loss: 0.001857093245204952\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008645506941195992\n",
      "Average test loss: 0.0018598118697603543\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008624778486374352\n",
      "Average test loss: 0.0018437691080487437\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008617296182447009\n",
      "Average test loss: 0.0018582937875762581\n",
      "Epoch 141/300\n",
      "Average training loss: 0.00863228374843796\n",
      "Average test loss: 0.0018581304028630256\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008609952644755442\n",
      "Average test loss: 0.0019149503925194344\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008613627921375964\n",
      "Average test loss: 0.0018911002511158585\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008597560798128446\n",
      "Average test loss: 0.0018541565970000293\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0085834620979925\n",
      "Average test loss: 0.0018490755697712301\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008586532366772493\n",
      "Average test loss: 0.0018313116368113293\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00857684326171875\n",
      "Average test loss: 0.0019051370339261161\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008568546881278356\n",
      "Average test loss: 0.0018379864138033655\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008564601752079196\n",
      "Average test loss: 0.0018716462888858385\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008563076061920987\n",
      "Average test loss: 0.001854568696477347\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008557918642958006\n",
      "Average test loss: 0.0018847903030821019\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008547518308377928\n",
      "Average test loss: 0.0018532650496603715\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008536747955613665\n",
      "Average test loss: 0.0018354518780898717\n",
      "Epoch 154/300\n",
      "Average training loss: 0.00853048975020647\n",
      "Average test loss: 0.0018288054685625765\n",
      "Epoch 155/300\n",
      "Average training loss: 0.00853637138257424\n",
      "Average test loss: 0.0018800947992648515\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008519581774870555\n",
      "Average test loss: 0.0018410322235690223\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008518907285398907\n",
      "Average test loss: 0.0018320157188508246\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00850446990546253\n",
      "Average test loss: 0.001896986446550323\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00850553551564614\n",
      "Average test loss: 0.0018665656288050943\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008505157816741202\n",
      "Average test loss: 0.0019026508137790694\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008484668019745085\n",
      "Average test loss: 0.0018321354039427308\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008486747133235136\n",
      "Average test loss: 0.001856000709036986\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008476219133370452\n",
      "Average test loss: 0.0018528373969925775\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008479030800362428\n",
      "Average test loss: 0.0018323325583090384\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008468250676161714\n",
      "Average test loss: 0.0018778515835810039\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008472662034961912\n",
      "Average test loss: 0.0018566716606211332\n",
      "Epoch 167/300\n",
      "Average training loss: 0.00844776464212272\n",
      "Average test loss: 0.0018618732826370331\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008455426562163564\n",
      "Average test loss: 0.0018965048829300535\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008453675992579924\n",
      "Average test loss: 0.001914057994675305\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00842998712675439\n",
      "Average test loss: 0.0018880301949878533\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008439970518151919\n",
      "Average test loss: 0.0018411875810577637\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008426783700784047\n",
      "Average test loss: 0.0018737114930732382\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008433195370766852\n",
      "Average test loss: 0.0019061955714391337\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008414970498532057\n",
      "Average test loss: 0.0018505206749671035\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00841842651905285\n",
      "Average test loss: 0.0018663294343277811\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008415261052134965\n",
      "Average test loss: 0.0019050107122295433\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008400433001004987\n",
      "Average test loss: 0.0019010604692416058\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00839841628819704\n",
      "Average test loss: 0.0018750459448330932\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008396908601952923\n",
      "Average test loss: 0.0018562171270863877\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008391579149083958\n",
      "Average test loss: 0.0018776345920438569\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008376876858373484\n",
      "Average test loss: 0.0018620404601097107\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008375227015051577\n",
      "Average test loss: 0.0018329833302025995\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008378273237082694\n",
      "Average test loss: 0.001849260953358478\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008365261382526822\n",
      "Average test loss: 0.0018399968600004083\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008360147242744764\n",
      "Average test loss: 0.0018656573970284728\n",
      "Epoch 186/300\n",
      "Average training loss: 0.008372187194724878\n",
      "Average test loss: 0.001868748462965919\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008356832505100303\n",
      "Average test loss: 0.0018742365820540322\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008352210747698943\n",
      "Average test loss: 0.0018681204594257804\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008336030030002196\n",
      "Average test loss: 0.001870370875009232\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008345651938683456\n",
      "Average test loss: 0.0018931909781984156\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008340844014452564\n",
      "Average test loss: 0.0018577014666257632\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008339496307902866\n",
      "Average test loss: 0.0019149733408250743\n",
      "Epoch 193/300\n",
      "Average training loss: 0.00832823888791932\n",
      "Average test loss: 0.0018679265970778134\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008317169596751531\n",
      "Average test loss: 0.0018699681825108\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008327704102628761\n",
      "Average test loss: 0.0018648584650622474\n",
      "Epoch 196/300\n",
      "Average training loss: 0.00831623213407066\n",
      "Average test loss: 0.0019393267754672303\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008313750847760175\n",
      "Average test loss: 0.0018917192819838723\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008313407416972849\n",
      "Average test loss: 0.0018810669434153372\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008304922502073977\n",
      "Average test loss: 0.001884317096736696\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008294992600464159\n",
      "Average test loss: 0.0018868469512090087\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008299073203984235\n",
      "Average test loss: 0.0018483941064526637\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008298690026832952\n",
      "Average test loss: 0.001889019536682301\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008279772332145108\n",
      "Average test loss: 0.001907703333844741\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008281696474800508\n",
      "Average test loss: 0.0018905195575207472\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008272966057889991\n",
      "Average test loss: 0.0018817479426248207\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008277683604094718\n",
      "Average test loss: 0.001987887781423827\n",
      "Epoch 207/300\n",
      "Average training loss: 0.00826940220180485\n",
      "Average test loss: 0.0019406672739941213\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008267932727519009\n",
      "Average test loss: 0.0019030240945931939\n",
      "Epoch 209/300\n",
      "Average training loss: 0.00826159226604634\n",
      "Average test loss: 0.0020011582432521713\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008255004771881633\n",
      "Average test loss: 0.0018891595900058747\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008250114509214958\n",
      "Average test loss: 0.0019034745699415604\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008243914908419053\n",
      "Average test loss: 0.001894113492531081\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008239546281596025\n",
      "Average test loss: 0.0019096385290225347\n",
      "Epoch 214/300\n",
      "Average training loss: 0.00825187338454028\n",
      "Average test loss: 0.002084158254787326\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008242793943732977\n",
      "Average test loss: 0.0019413579659950401\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008231100317504672\n",
      "Average test loss: 0.0018900515029413832\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008234636266198423\n",
      "Average test loss: 0.0018969621474130287\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008231493640691041\n",
      "Average test loss: 0.001898126942726473\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00822143656305141\n",
      "Average test loss: 0.0019105422912786404\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00821650653001335\n",
      "Average test loss: 0.0018877449922470582\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008210072833630774\n",
      "Average test loss: 0.0019050539450512992\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008210281865464316\n",
      "Average test loss: 0.0020200679800990557\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008206129950781663\n",
      "Average test loss: 0.0018791701583605674\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008200441090597047\n",
      "Average test loss: 0.0019545251027577454\n",
      "Epoch 225/300\n",
      "Average training loss: 0.00820401342999604\n",
      "Average test loss: 0.0018785198171002169\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008189567263755533\n",
      "Average test loss: 0.0019099888785017861\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008203120418306854\n",
      "Average test loss: 0.0019163297942529122\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008182890807588895\n",
      "Average test loss: 0.0018596207943434517\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008181587058636877\n",
      "Average test loss: 0.0019301993327422275\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008183084957301617\n",
      "Average test loss: 0.00188849911507633\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008178575354731744\n",
      "Average test loss: 0.0019300107647561365\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008174972312731876\n",
      "Average test loss: 0.0019421648592170741\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008169891972922616\n",
      "Average test loss: 0.0019348689635387726\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008163923400971624\n",
      "Average test loss: 0.001896048671203769\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00816427072096202\n",
      "Average test loss: 0.0018946017796794574\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008168525950362285\n",
      "Average test loss: 0.001933063367381692\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008158498610887263\n",
      "Average test loss: 0.002006124890719851\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008156940297534068\n",
      "Average test loss: 0.0019161947593092918\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008140737123787403\n",
      "Average test loss: 0.001896164814527664\n",
      "Epoch 240/300\n",
      "Average training loss: 0.00816047704178426\n",
      "Average test loss: 0.0019224193083743255\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008137300996316803\n",
      "Average test loss: 0.0019298311752370663\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008146457470125622\n",
      "Average test loss: 0.0018967286468380027\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00813643285466565\n",
      "Average test loss: 0.0018928462641520632\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008135213961203893\n",
      "Average test loss: 0.0019083831301993793\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008132351227518585\n",
      "Average test loss: 0.0019214904248300525\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008135236391590701\n",
      "Average test loss: 0.0019022021705491674\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008122703817569547\n",
      "Average test loss: 0.0019194031469523907\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008130862273275852\n",
      "Average test loss: 0.0020570436318715414\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008127021923661232\n",
      "Average test loss: 0.0019037200572590033\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008109828867846065\n",
      "Average test loss: 0.0019440590332572658\n",
      "Epoch 251/300\n",
      "Average training loss: 0.00811816222841541\n",
      "Average test loss: 0.0019118087360014517\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008115067025025685\n",
      "Average test loss: 0.0019152730618500048\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008109590971635447\n",
      "Average test loss: 0.0020104696680274276\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008114561279614766\n",
      "Average test loss: 0.001973101017272307\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008096411967443096\n",
      "Average test loss: 0.0019646330951816507\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008096856938054165\n",
      "Average test loss: 0.0019483835210816727\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008095979136725266\n",
      "Average test loss: 0.0018960640891972516\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008094109372132355\n",
      "Average test loss: 0.0019234725642535422\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008088676638073392\n",
      "Average test loss: 0.00191106459695018\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008089366907874743\n",
      "Average test loss: 0.0019713597864740424\n",
      "Epoch 261/300\n",
      "Average training loss: 0.00809625270879931\n",
      "Average test loss: 0.0019102144627314475\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008084728334926896\n",
      "Average test loss: 0.0019376163418508238\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008077544121278656\n",
      "Average test loss: 0.0019057330241840746\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008074385720822546\n",
      "Average test loss: 0.0019055806537055307\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008068208848022753\n",
      "Average test loss: 0.001977100028759903\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008070949107408524\n",
      "Average test loss: 0.0019114512271351284\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008067309892839856\n",
      "Average test loss: 0.001892784981160528\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008067340576814281\n",
      "Average test loss: 0.0019915381719668706\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008059909273352888\n",
      "Average test loss: 0.001953592081243793\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008072473723855283\n",
      "Average test loss: 0.0019452097398332425\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008066142758561505\n",
      "Average test loss: 0.001948708813327054\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008059277498059802\n",
      "Average test loss: 0.0018923520327856144\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008050339293148783\n",
      "Average test loss: 0.0019172141007665132\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008055750430872043\n",
      "Average test loss: 0.001930746762185461\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008045662879943847\n",
      "Average test loss: 0.001972103853399555\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008041818469762802\n",
      "Average test loss: 0.0019348585990568003\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008047322759197818\n",
      "Average test loss: 0.0018799633988075787\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008033676808079085\n",
      "Average test loss: 0.002009038793957896\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008039055700103442\n",
      "Average test loss: 0.0019061755809622507\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008030672358555925\n",
      "Average test loss: 0.0020474310192383\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008031921193003654\n",
      "Average test loss: 0.0019172274908050896\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008035346957958408\n",
      "Average test loss: 0.0019314943993878034\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008034687915196022\n",
      "Average test loss: 0.001919593184099843\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008027077984064818\n",
      "Average test loss: 0.0019181755751164424\n",
      "Epoch 285/300\n",
      "Average training loss: 0.00801992145470447\n",
      "Average test loss: 0.0019547276937713224\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008015908977637688\n",
      "Average test loss: 0.001938027348679801\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008030262539370192\n",
      "Average test loss: 0.00195995503767497\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008017870899703768\n",
      "Average test loss: 0.001926565489421288\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008020448344863124\n",
      "Average test loss: 0.001962931241529683\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008003829976750745\n",
      "Average test loss: 0.001907070028078225\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008010756534834703\n",
      "Average test loss: 0.002001566373432676\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008025079868319962\n",
      "Average test loss: 0.0019484843391304216\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008005237710972626\n",
      "Average test loss: 0.001937221513233251\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007997486210531658\n",
      "Average test loss: 0.0024132817562462555\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007995123268829452\n",
      "Average test loss: 0.0019177791223757797\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007998095607592\n",
      "Average test loss: 0.0019060864698969655\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007992945830855105\n",
      "Average test loss: 0.0019046118967235089\n",
      "Epoch 298/300\n",
      "Average training loss: 0.007997724446985457\n",
      "Average test loss: 0.0019395433490475018\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007995284982439545\n",
      "Average test loss: 0.0019454492685488528\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007980443001621299\n",
      "Average test loss: 0.0019420760865840646\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'DCT_32_Depth3/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.68\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7695808939668868\n",
      "Average test loss: 0.005545967624833187\n",
      "Epoch 2/300\n",
      "Average training loss: 0.11104126081201765\n",
      "Average test loss: 0.005355503422932492\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08801818732420603\n",
      "Average test loss: 0.00495861479267478\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08040466605292426\n",
      "Average test loss: 0.004848967238846752\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07660557452837626\n",
      "Average test loss: 0.004757156478448046\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07410141434934404\n",
      "Average test loss: 0.00474011301000913\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07240807752145662\n",
      "Average test loss: 0.004651318369226323\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0711130766040749\n",
      "Average test loss: 0.004637566035820378\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07006636644734276\n",
      "Average test loss: 0.004796256786833207\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06927605301141739\n",
      "Average test loss: 0.0045872509202195535\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06859623254007763\n",
      "Average test loss: 0.004542839431928264\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06808974270025889\n",
      "Average test loss: 0.004544588706145684\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06756541208757294\n",
      "Average test loss: 0.004521651833007733\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06720697032080757\n",
      "Average test loss: 0.004487259312222401\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06681213129560153\n",
      "Average test loss: 0.0044780940396918194\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06658269939157697\n",
      "Average test loss: 0.0044590103733870715\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06622210862239201\n",
      "Average test loss: 0.004478474036066068\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06594698910580742\n",
      "Average test loss: 0.004456186553670301\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06571566753586133\n",
      "Average test loss: 0.004437165103438828\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06547465203040176\n",
      "Average test loss: 0.004402299691819482\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06531441820992363\n",
      "Average test loss: 0.004408889221648375\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06505323904090457\n",
      "Average test loss: 0.004398875110679203\n",
      "Epoch 23/300\n",
      "Average training loss: 0.064800783806377\n",
      "Average test loss: 0.004360484525147411\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0646140095392863\n",
      "Average test loss: 0.004349460255768564\n",
      "Epoch 25/300\n",
      "Average training loss: 0.064464457495345\n",
      "Average test loss: 0.004366796725740035\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06429365654455291\n",
      "Average test loss: 0.004364137753844261\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06409707335962189\n",
      "Average test loss: 0.004333076525893476\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06398457041051653\n",
      "Average test loss: 0.004342715388784806\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06377436407407125\n",
      "Average test loss: 0.00434222681965265\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0637033940354983\n",
      "Average test loss: 0.004303755610146456\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06355405321054988\n",
      "Average test loss: 0.004305949852698379\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0633559810022513\n",
      "Average test loss: 0.004306450598355796\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0632270092036989\n",
      "Average test loss: 0.004290009851877888\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06312988821334309\n",
      "Average test loss: 0.004322864401257701\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06300386739770572\n",
      "Average test loss: 0.004274069775723748\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06297107564409574\n",
      "Average test loss: 0.0042636797337068455\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06282208475470542\n",
      "Average test loss: 0.004265526622740759\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06270245723923047\n",
      "Average test loss: 0.004270354970875714\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06261662249763807\n",
      "Average test loss: 0.004263513755881124\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06250850301318699\n",
      "Average test loss: 0.004260147595571147\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06241105943918228\n",
      "Average test loss: 0.004259256194863055\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06235749115877681\n",
      "Average test loss: 0.004269789947817723\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06225973395837678\n",
      "Average test loss: 0.004233700503077772\n",
      "Epoch 44/300\n",
      "Average training loss: 0.062181807660394245\n",
      "Average test loss: 0.004246143635362387\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0620735423233774\n",
      "Average test loss: 0.004253698115133577\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06204830270674493\n",
      "Average test loss: 0.00424512470803327\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06194156814614932\n",
      "Average test loss: 0.004224098673918181\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06191397968928019\n",
      "Average test loss: 0.0042512675608611765\n",
      "Epoch 49/300\n",
      "Average training loss: 0.061785735232962505\n",
      "Average test loss: 0.004256083788143264\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06175902791486846\n",
      "Average test loss: 0.004389168527184261\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06166966855194834\n",
      "Average test loss: 0.004235798178033696\n",
      "Epoch 52/300\n",
      "Average training loss: 0.061601651748021444\n",
      "Average test loss: 0.004264910191297531\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06154202166861958\n",
      "Average test loss: 0.004221820151640309\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06149233251478937\n",
      "Average test loss: 0.0042217266276064844\n",
      "Epoch 55/300\n",
      "Average training loss: 0.061388637708293066\n",
      "Average test loss: 0.0042195399658133586\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06134890923897425\n",
      "Average test loss: 0.004224720161822107\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06124334526724286\n",
      "Average test loss: 0.004218174695554707\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06128248703810904\n",
      "Average test loss: 0.004231974415481091\n",
      "Epoch 59/300\n",
      "Average training loss: 0.061163049363427695\n",
      "Average test loss: 0.004229153401321835\n",
      "Epoch 60/300\n",
      "Average training loss: 0.061121061225732166\n",
      "Average test loss: 0.004220469622562329\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06105041072434849\n",
      "Average test loss: 0.004216334950592783\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06097426943977674\n",
      "Average test loss: 0.004218831021752622\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0608912834127744\n",
      "Average test loss: 0.004214843391130368\n",
      "Epoch 64/300\n",
      "Average training loss: 0.060839886211686664\n",
      "Average test loss: 0.00422795672280093\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06074857214424345\n",
      "Average test loss: 0.004299457092045082\n",
      "Epoch 66/300\n",
      "Average training loss: 0.060743345204326844\n",
      "Average test loss: 0.004242906959934367\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0606805738111337\n",
      "Average test loss: 0.00422327074698276\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06057014023595386\n",
      "Average test loss: 0.004229463623629676\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06057942357990477\n",
      "Average test loss: 0.004198011442604992\n",
      "Epoch 70/300\n",
      "Average training loss: 0.060522244582573576\n",
      "Average test loss: 0.004211239908407959\n",
      "Epoch 71/300\n",
      "Average training loss: 0.060461855858564374\n",
      "Average test loss: 0.004203005893776814\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06036438011791971\n",
      "Average test loss: 0.004209207945399814\n",
      "Epoch 73/300\n",
      "Average training loss: 0.060325753649075824\n",
      "Average test loss: 0.004223168670924173\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06025022209352917\n",
      "Average test loss: 0.004277462285425928\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06023748277624448\n",
      "Average test loss: 0.004222786926147011\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06012029375632604\n",
      "Average test loss: 0.0042170007543431386\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0600459959242079\n",
      "Average test loss: 0.004235240870051914\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05999135747551918\n",
      "Average test loss: 0.004240311877905495\n",
      "Epoch 79/300\n",
      "Average training loss: 0.059950633380148145\n",
      "Average test loss: 0.004218095013250907\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05993365371227265\n",
      "Average test loss: 0.004215835499680704\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05982438487145636\n",
      "Average test loss: 0.004224156964156363\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05976807248923514\n",
      "Average test loss: 0.004203360168263316\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05970561825897958\n",
      "Average test loss: 0.00422871263532175\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05962760196791755\n",
      "Average test loss: 0.004412222940060827\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05958796440230475\n",
      "Average test loss: 0.004213106895486513\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05952299925684929\n",
      "Average test loss: 0.004244234467339184\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05941611282693015\n",
      "Average test loss: 0.004310840487894084\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05939036892520057\n",
      "Average test loss: 0.004236006984901097\n",
      "Epoch 89/300\n",
      "Average training loss: 0.059345821168687606\n",
      "Average test loss: 0.004273985744350487\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05933328689138095\n",
      "Average test loss: 0.004256129163006941\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05918430068757799\n",
      "Average test loss: 0.004210570312622521\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05911994442343712\n",
      "Average test loss: 0.00422024371723334\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05908787515428331\n",
      "Average test loss: 0.004225333195179701\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05902482704321543\n",
      "Average test loss: 0.004222823451790545\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05893116559584936\n",
      "Average test loss: 0.004229276476220952\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05886530102292697\n",
      "Average test loss: 0.004227317102874319\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0588517591158549\n",
      "Average test loss: 0.004231053724471066\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05878357601993614\n",
      "Average test loss: 0.004275612752574186\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05866853799422582\n",
      "Average test loss: 0.004273967742919922\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05860787830750148\n",
      "Average test loss: 0.00425035644757251\n",
      "Epoch 101/300\n",
      "Average training loss: 0.058565273645851346\n",
      "Average test loss: 0.004275437826911609\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05850004362397724\n",
      "Average test loss: 0.004280629610228869\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05845195720593135\n",
      "Average test loss: 0.004242657690205508\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05843656772706244\n",
      "Average test loss: 0.004283617932349443\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05825768306851387\n",
      "Average test loss: 0.004288656516828471\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05822036852439245\n",
      "Average test loss: 0.004256835308339861\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05819492988453971\n",
      "Average test loss: 0.004262879454220335\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05815494940016005\n",
      "Average test loss: 0.004252171826445394\n",
      "Epoch 109/300\n",
      "Average training loss: 0.058104467223087944\n",
      "Average test loss: 0.0042444095988240504\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05798390566971567\n",
      "Average test loss: 0.004248786932064427\n",
      "Epoch 111/300\n",
      "Average training loss: 0.057886610872215694\n",
      "Average test loss: 0.004270214067151149\n",
      "Epoch 112/300\n",
      "Average training loss: 0.057958221031559835\n",
      "Average test loss: 0.004257524981887804\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05774907276696629\n",
      "Average test loss: 0.004248871436963479\n",
      "Epoch 114/300\n",
      "Average training loss: 0.057751941081550386\n",
      "Average test loss: 0.0042492546139078005\n",
      "Epoch 115/300\n",
      "Average training loss: 0.057673436681429546\n",
      "Average test loss: 0.004243461954303913\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05765547384818395\n",
      "Average test loss: 0.004285049694900712\n",
      "Epoch 117/300\n",
      "Average training loss: 0.057562154862615796\n",
      "Average test loss: 0.004293205883353949\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0574728924466504\n",
      "Average test loss: 0.004260566248661942\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05740729429986742\n",
      "Average test loss: 0.0043415259598857826\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05734869978162978\n",
      "Average test loss: 0.0043420349645117914\n",
      "Epoch 121/300\n",
      "Average training loss: 0.057206568837165836\n",
      "Average test loss: 0.004329004613475667\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05719829045732816\n",
      "Average test loss: 0.004331044169763724\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05716807671719127\n",
      "Average test loss: 0.004298186428017086\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05712268641259935\n",
      "Average test loss: 0.00445824063445131\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05713911574747827\n",
      "Average test loss: 0.004300607218096653\n",
      "Epoch 126/300\n",
      "Average training loss: 0.057038627283440696\n",
      "Average test loss: 0.004298547821740309\n",
      "Epoch 127/300\n",
      "Average training loss: 0.056913974278503\n",
      "Average test loss: 0.004346830056980252\n",
      "Epoch 128/300\n",
      "Average training loss: 0.056844147606028454\n",
      "Average test loss: 0.004323990498565965\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05677549930744701\n",
      "Average test loss: 0.004346163026988506\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0566540161702368\n",
      "Average test loss: 0.004383694443023867\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05669921524326007\n",
      "Average test loss: 0.00430244162823591\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05661802097161611\n",
      "Average test loss: 0.004342495651294788\n",
      "Epoch 133/300\n",
      "Average training loss: 0.056554914861917494\n",
      "Average test loss: 0.004346938198018405\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05648997900221083\n",
      "Average test loss: 0.00435360525879595\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05646617510252529\n",
      "Average test loss: 0.0043709836701552075\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05634263307187292\n",
      "Average test loss: 0.004349841942803728\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05630266331301795\n",
      "Average test loss: 0.004366483351422681\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05618191290895144\n",
      "Average test loss: 0.004334267798811197\n",
      "Epoch 139/300\n",
      "Average training loss: 0.056138316839933394\n",
      "Average test loss: 0.004304835764070352\n",
      "Epoch 140/300\n",
      "Average training loss: 0.056147521184550395\n",
      "Average test loss: 0.004388643928700023\n",
      "Epoch 141/300\n",
      "Average training loss: 0.056115699966748554\n",
      "Average test loss: 0.004302860743262702\n",
      "Epoch 142/300\n",
      "Average training loss: 0.055928446233272554\n",
      "Average test loss: 0.004424030876821942\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05590089429749383\n",
      "Average test loss: 0.004456480720390876\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05594020183881124\n",
      "Average test loss: 0.004320114448459612\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05585947595702277\n",
      "Average test loss: 0.004364573792037037\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05572770024008221\n",
      "Average test loss: 0.004301727561901013\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0556940280298392\n",
      "Average test loss: 0.004375757440096802\n",
      "Epoch 148/300\n",
      "Average training loss: 0.055647014372878605\n",
      "Average test loss: 0.004405965091867579\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0556146538456281\n",
      "Average test loss: 0.0043263815695212945\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05565664634770817\n",
      "Average test loss: 0.004407305224488179\n",
      "Epoch 151/300\n",
      "Average training loss: 0.055475456492768395\n",
      "Average test loss: 0.004461157049569819\n",
      "Epoch 152/300\n",
      "Average training loss: 0.055405631058745915\n",
      "Average test loss: 0.0043470935593876575\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05538093569543626\n",
      "Average test loss: 0.00440529377779199\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0552254125641452\n",
      "Average test loss: 0.004458567427885201\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05524861976835463\n",
      "Average test loss: 0.00429832798242569\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05525703418254852\n",
      "Average test loss: 0.0044941917281183935\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05519806350270907\n",
      "Average test loss: 0.0043995973360207346\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05507469529575772\n",
      "Average test loss: 0.004339438094240096\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05507670469085375\n",
      "Average test loss: 0.0044616657768686615\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05500166687369347\n",
      "Average test loss: 0.004429689698956079\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05496107577946451\n",
      "Average test loss: 0.004421812259281675\n",
      "Epoch 162/300\n",
      "Average training loss: 0.054914776417944164\n",
      "Average test loss: 0.004342426139447424\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05489056110050943\n",
      "Average test loss: 0.004381564605049789\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05480656043688456\n",
      "Average test loss: 0.00434185941144824\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0547069386906094\n",
      "Average test loss: 0.0044580132495611905\n",
      "Epoch 166/300\n",
      "Average training loss: 0.054709219740496744\n",
      "Average test loss: 0.004359619376973973\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05492757562465138\n",
      "Average test loss: 0.004472827189705438\n",
      "Epoch 168/300\n",
      "Average training loss: 0.054686650031142765\n",
      "Average test loss: 0.004545573439656033\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05455175999469227\n",
      "Average test loss: 0.004467193333432078\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05443033741248979\n",
      "Average test loss: 0.0044793145650376875\n",
      "Epoch 171/300\n",
      "Average training loss: 0.054513800491889315\n",
      "Average test loss: 0.0044907263447013165\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05446515041589737\n",
      "Average test loss: 0.004395327541563246\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05432461435265011\n",
      "Average test loss: 0.004420923507668905\n",
      "Epoch 174/300\n",
      "Average training loss: 0.054357326106892694\n",
      "Average test loss: 0.0045187408675750095\n",
      "Epoch 175/300\n",
      "Average training loss: 0.054343965864843795\n",
      "Average test loss: 0.004478894895563523\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05426105917162365\n",
      "Average test loss: 0.004501692757424381\n",
      "Epoch 177/300\n",
      "Average training loss: 0.054127980450789134\n",
      "Average test loss: 0.00444010375191768\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05407661558853255\n",
      "Average test loss: 0.004524794732530912\n",
      "Epoch 179/300\n",
      "Average training loss: 0.054170678950018356\n",
      "Average test loss: 0.0044616550219555694\n",
      "Epoch 180/300\n",
      "Average training loss: 0.054061574048466156\n",
      "Average test loss: 0.0044205514354010425\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05406731369098028\n",
      "Average test loss: 0.00443342600390315\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05392424087723096\n",
      "Average test loss: 0.004431318400427699\n",
      "Epoch 183/300\n",
      "Average training loss: 0.053905299150281485\n",
      "Average test loss: 0.004499321447892322\n",
      "Epoch 184/300\n",
      "Average training loss: 0.053910864912801316\n",
      "Average test loss: 0.004511496661437882\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05385725199845102\n",
      "Average test loss: 0.004558881899962823\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05380441940161917\n",
      "Average test loss: 0.00439810077763266\n",
      "Epoch 187/300\n",
      "Average training loss: 0.053747451653083166\n",
      "Average test loss: 0.004468877903289265\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05369706302550104\n",
      "Average test loss: 0.004450780593272713\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05366633349988196\n",
      "Average test loss: 0.0044608872005095085\n",
      "Epoch 190/300\n",
      "Average training loss: 0.053684150685866676\n",
      "Average test loss: 0.004467926079200373\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05359601897001266\n",
      "Average test loss: 0.004527121851427687\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05356036411391364\n",
      "Average test loss: 0.004574182374609841\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0534482363694244\n",
      "Average test loss: 0.004524025376886129\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05350325541363822\n",
      "Average test loss: 0.004413463277949227\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05339459341930018\n",
      "Average test loss: 0.004561422222811314\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05343543012109068\n",
      "Average test loss: 0.004528573052750694\n",
      "Epoch 197/300\n",
      "Average training loss: 0.053351521713866125\n",
      "Average test loss: 0.004541063667999373\n",
      "Epoch 198/300\n",
      "Average training loss: 0.053409018920527566\n",
      "Average test loss: 0.004542894329047865\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05326277993122737\n",
      "Average test loss: 0.004470505783541335\n",
      "Epoch 200/300\n",
      "Average training loss: 0.053206705609957376\n",
      "Average test loss: 0.004431786181198226\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05322053061260117\n",
      "Average test loss: 0.0045458835706942615\n",
      "Epoch 202/300\n",
      "Average training loss: 0.053105143563614954\n",
      "Average test loss: 0.004434393426610364\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05316914012034734\n",
      "Average test loss: 0.004530061231719123\n",
      "Epoch 204/300\n",
      "Average training loss: 0.053138386925061544\n",
      "Average test loss: 0.004496017007985049\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05299955220354928\n",
      "Average test loss: 0.004530947061462535\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05303208996852239\n",
      "Average test loss: 0.004505430354426305\n",
      "Epoch 207/300\n",
      "Average training loss: 0.052946285423305296\n",
      "Average test loss: 0.004562447030304207\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05304347960816489\n",
      "Average test loss: 0.004685013746635781\n",
      "Epoch 209/300\n",
      "Average training loss: 0.052820033305221135\n",
      "Average test loss: 0.004471761835945977\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05291474786069658\n",
      "Average test loss: 0.004443942685921987\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052809724072615305\n",
      "Average test loss: 0.004613170249387622\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05282579596506225\n",
      "Average test loss: 0.004484434239566326\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05275872947441207\n",
      "Average test loss: 0.004520848360740477\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05268661592735185\n",
      "Average test loss: 0.00451463406946924\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05273271567953958\n",
      "Average test loss: 0.004501834157854319\n",
      "Epoch 216/300\n",
      "Average training loss: 0.052637930234273274\n",
      "Average test loss: 0.004465204341544045\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05266616830229759\n",
      "Average test loss: 0.004556086646599902\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05254997199111515\n",
      "Average test loss: 0.004530395171708531\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05258722138073709\n",
      "Average test loss: 0.004448590180526177\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05255487468838692\n",
      "Average test loss: 0.004582270180599557\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05251980216635598\n",
      "Average test loss: 0.0044657989034636155\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05241656941837735\n",
      "Average test loss: 0.004488061026566559\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05234922599130207\n",
      "Average test loss: 0.004569409071985218\n",
      "Epoch 224/300\n",
      "Average training loss: 0.052348668641514245\n",
      "Average test loss: 0.0045478310026228425\n",
      "Epoch 225/300\n",
      "Average training loss: 0.052361205018228954\n",
      "Average test loss: 0.004548561105918553\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05233148837420676\n",
      "Average test loss: 0.004485929232918554\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05217745636900266\n",
      "Average test loss: 0.0045190858377350705\n",
      "Epoch 228/300\n",
      "Average training loss: 0.052305164145098795\n",
      "Average test loss: 0.004697205972340372\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05224196773436334\n",
      "Average test loss: 0.004487785040504403\n",
      "Epoch 230/300\n",
      "Average training loss: 0.052259072807100085\n",
      "Average test loss: 0.004664545021537277\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05222440771924125\n",
      "Average test loss: 0.004458265067388614\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05208707320690155\n",
      "Average test loss: 0.004781152579519484\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05209260447157754\n",
      "Average test loss: 0.00450383045979672\n",
      "Epoch 234/300\n",
      "Average training loss: 0.051984803603755105\n",
      "Average test loss: 0.004452462476367752\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05206228827436765\n",
      "Average test loss: 0.004563901452761557\n",
      "Epoch 236/300\n",
      "Average training loss: 0.052048721002207865\n",
      "Average test loss: 0.004638232696801424\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05215577600068516\n",
      "Average test loss: 0.004541822312192785\n",
      "Epoch 238/300\n",
      "Average training loss: 0.051938836216926576\n",
      "Average test loss: 0.00444928575017386\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05186526703503397\n",
      "Average test loss: 0.0045078809348245465\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05193480205867026\n",
      "Average test loss: 0.004571741390145487\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05184484580159187\n",
      "Average test loss: 0.004464929908514023\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0518126221994559\n",
      "Average test loss: 0.004707878667033381\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05185997300677829\n",
      "Average test loss: 0.0045181890639166035\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0517904104159938\n",
      "Average test loss: 0.004579926135225429\n",
      "Epoch 245/300\n",
      "Average training loss: 0.051719216214285955\n",
      "Average test loss: 0.004521998206774394\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05167636414203379\n",
      "Average test loss: 0.0045975042914764746\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05174869458211793\n",
      "Average test loss: 0.00459417351293895\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05158951113455825\n",
      "Average test loss: 0.004507341814951764\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0517352457344532\n",
      "Average test loss: 0.004690095852232641\n",
      "Epoch 250/300\n",
      "Average training loss: 0.051669554120964474\n",
      "Average test loss: 0.004547078550275829\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05172467805610763\n",
      "Average test loss: 0.0045294405337837\n",
      "Epoch 252/300\n",
      "Average training loss: 0.051516937815480765\n",
      "Average test loss: 0.0045585357782741386\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05153681428564919\n",
      "Average test loss: 0.004733315258804295\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05148653176426887\n",
      "Average test loss: 0.004588063268611829\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05157159607774681\n",
      "Average test loss: 0.004518015880137682\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05145221393638187\n",
      "Average test loss: 0.004701653895899653\n",
      "Epoch 257/300\n",
      "Average training loss: 0.051409939722882374\n",
      "Average test loss: 0.0046025918032974004\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05144390081365903\n",
      "Average test loss: 0.004722160678770807\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05134127811590831\n",
      "Average test loss: 0.00511155402329233\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05143458136253887\n",
      "Average test loss: 0.004667413309630421\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05131571276982625\n",
      "Average test loss: 0.004501198792623149\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05126734331581328\n",
      "Average test loss: 0.004502653126708335\n",
      "Epoch 263/300\n",
      "Average training loss: 0.051257337937752404\n",
      "Average test loss: 0.0046205124552879066\n",
      "Epoch 264/300\n",
      "Average training loss: 0.051295526686641904\n",
      "Average test loss: 0.00461656924088796\n",
      "Epoch 265/300\n",
      "Average training loss: 0.051304645939005744\n",
      "Average test loss: 0.004546219195342726\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05117450926038954\n",
      "Average test loss: 0.0045695693633622594\n",
      "Epoch 267/300\n",
      "Average training loss: 0.051245554871029324\n",
      "Average test loss: 0.004491891959061225\n",
      "Epoch 268/300\n",
      "Average training loss: 0.051106825874911416\n",
      "Average test loss: 0.00471095629201995\n",
      "Epoch 269/300\n",
      "Average training loss: 0.051114655911922456\n",
      "Average test loss: 0.004646872420898742\n",
      "Epoch 270/300\n",
      "Average training loss: 0.051190500812398065\n",
      "Average test loss: 0.00465226179568304\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05116029268503189\n",
      "Average test loss: 0.00456278752328621\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05101601751976543\n",
      "Average test loss: 0.004673735743181573\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05103537855876816\n",
      "Average test loss: 0.004593683274048898\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05102053997251722\n",
      "Average test loss: 0.004591111428207821\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0509738124344084\n",
      "Average test loss: 0.004603592965958847\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05099020471506648\n",
      "Average test loss: 0.004622421447394623\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05085986473825243\n",
      "Average test loss: 0.004593111807687415\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05092438998321692\n",
      "Average test loss: 0.004639646739595466\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05079028955764241\n",
      "Average test loss: 0.004563129704652561\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05088044855660862\n",
      "Average test loss: 0.00457513715657923\n",
      "Epoch 281/300\n",
      "Average training loss: 0.050911884824434914\n",
      "Average test loss: 0.004620850713095731\n",
      "Epoch 282/300\n",
      "Average training loss: 0.050829754799604415\n",
      "Average test loss: 0.0045740139645834765\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05074476199348767\n",
      "Average test loss: 0.0047182311531570225\n",
      "Epoch 284/300\n",
      "Average training loss: 0.050772817250755095\n",
      "Average test loss: 0.00460531600813071\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05073875327242745\n",
      "Average test loss: 0.004646550325055917\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05075173719061746\n",
      "Average test loss: 0.004657330519623227\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05071836391422484\n",
      "Average test loss: 0.004549487635493278\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05055190669165717\n",
      "Average test loss: 0.004574958143548833\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05065354640285174\n",
      "Average test loss: 0.004587818983114428\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05067612655626403\n",
      "Average test loss: 0.0045374915339052675\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05067077846659555\n",
      "Average test loss: 0.0045030631836917665\n",
      "Epoch 292/300\n",
      "Average training loss: 0.050554495404164\n",
      "Average test loss: 0.004665851061128908\n",
      "Epoch 293/300\n",
      "Average training loss: 0.050625752872890895\n",
      "Average test loss: 0.004703573166082302\n",
      "Epoch 294/300\n",
      "Average training loss: 0.050582070264551374\n",
      "Average test loss: 0.004480313449684117\n",
      "Epoch 295/300\n",
      "Average training loss: 0.050530568265252644\n",
      "Average test loss: 0.0045394807751807905\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05050288799073961\n",
      "Average test loss: 0.0045887407826052775\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05045858756701151\n",
      "Average test loss: 0.00462104924250808\n",
      "Epoch 298/300\n",
      "Average training loss: 0.050489977462424174\n",
      "Average test loss: 0.004569000986301237\n",
      "Epoch 299/300\n",
      "Average training loss: 0.050424455079767436\n",
      "Average test loss: 0.004550332883993784\n",
      "Epoch 300/300\n",
      "Average training loss: 0.050458051671584445\n",
      "Average test loss: 0.004664125837178694\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7028881642685996\n",
      "Average test loss: 0.005558334234274096\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0997906263868014\n",
      "Average test loss: 0.0047865456371671625\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07998587293757332\n",
      "Average test loss: 0.004472629719103376\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07214345202181074\n",
      "Average test loss: 0.00431672412984901\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06755966193477313\n",
      "Average test loss: 0.004211630968583954\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06456964625252617\n",
      "Average test loss: 0.0041262323341021935\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06232740742961566\n",
      "Average test loss: 0.004044045629186763\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06057561576035288\n",
      "Average test loss: 0.0038970663100481033\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05928089011377759\n",
      "Average test loss: 0.003839386475375957\n",
      "Epoch 10/300\n",
      "Average training loss: 0.058065362867381835\n",
      "Average test loss: 0.0037810507886525658\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05709217266572846\n",
      "Average test loss: 0.0037221144123209845\n",
      "Epoch 12/300\n",
      "Average training loss: 0.056218846572770015\n",
      "Average test loss: 0.003692471603138579\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05551894940932592\n",
      "Average test loss: 0.003666622168694933\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05487758803036478\n",
      "Average test loss: 0.0036337339085423283\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05420707593692674\n",
      "Average test loss: 0.003600820540967915\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05365208954612414\n",
      "Average test loss: 0.003536748656589124\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05309093360768424\n",
      "Average test loss: 0.0035123055941528743\n",
      "Epoch 18/300\n",
      "Average training loss: 0.052645594467719395\n",
      "Average test loss: 0.0034860604494396184\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05220197171966235\n",
      "Average test loss: 0.0035584146376285287\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05167932252585888\n",
      "Average test loss: 0.0034095171900052164\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05137811656130685\n",
      "Average test loss: 0.0033930951634214986\n",
      "Epoch 22/300\n",
      "Average training loss: 0.050936101095543966\n",
      "Average test loss: 0.0034502306669536562\n",
      "Epoch 23/300\n",
      "Average training loss: 0.050559222926696144\n",
      "Average test loss: 0.0033589359579814806\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05023316397269567\n",
      "Average test loss: 0.0033688164204359054\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04999804832041264\n",
      "Average test loss: 0.00331369398906827\n",
      "Epoch 26/300\n",
      "Average training loss: 0.049666012747420205\n",
      "Average test loss: 0.003326827646336622\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04936459529731009\n",
      "Average test loss: 0.0032820997919059463\n",
      "Epoch 28/300\n",
      "Average training loss: 0.049111793521377775\n",
      "Average test loss: 0.003262018050791489\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04886824742621846\n",
      "Average test loss: 0.0033031058876464765\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04860773322648472\n",
      "Average test loss: 0.003253803893406358\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04839905509021547\n",
      "Average test loss: 0.003305515711299247\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04815892741746373\n",
      "Average test loss: 0.003221677465157376\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04798622545599938\n",
      "Average test loss: 0.003232114106416702\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04778226191136572\n",
      "Average test loss: 0.003202521578098337\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04762173980143335\n",
      "Average test loss: 0.0031933930903259252\n",
      "Epoch 36/300\n",
      "Average training loss: 0.047418940462999874\n",
      "Average test loss: 0.0032203779061221413\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04731262975103325\n",
      "Average test loss: 0.003174897212121222\n",
      "Epoch 38/300\n",
      "Average training loss: 0.047102707866165376\n",
      "Average test loss: 0.0032211719976945057\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04695690581699213\n",
      "Average test loss: 0.0031771537065505982\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04679417205188009\n",
      "Average test loss: 0.003182941370540195\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04663635101252132\n",
      "Average test loss: 0.0031842196813474098\n",
      "Epoch 42/300\n",
      "Average training loss: 0.046557400756412085\n",
      "Average test loss: 0.0031724141279442443\n",
      "Epoch 43/300\n",
      "Average training loss: 0.046396722180975805\n",
      "Average test loss: 0.0031412134127070507\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04624537174900373\n",
      "Average test loss: 0.0031701366721342008\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04606740283966065\n",
      "Average test loss: 0.0031385810784995555\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04593344204293357\n",
      "Average test loss: 0.003147862452806698\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04583141780561871\n",
      "Average test loss: 0.003150517420636283\n",
      "Epoch 48/300\n",
      "Average training loss: 0.045731972210937076\n",
      "Average test loss: 0.003144491973850462\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04558942656053437\n",
      "Average test loss: 0.003142789482863413\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04546321408616172\n",
      "Average test loss: 0.0031234872815095716\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04537575137284067\n",
      "Average test loss: 0.003153082744942771\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04525227798024813\n",
      "Average test loss: 0.003134707638786899\n",
      "Epoch 53/300\n",
      "Average training loss: 0.045162498546971214\n",
      "Average test loss: 0.00313542635138664\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04496352888478173\n",
      "Average test loss: 0.003126952644230591\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04486602606541581\n",
      "Average test loss: 0.003165642202107443\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04475250736044513\n",
      "Average test loss: 0.0031554078221735027\n",
      "Epoch 57/300\n",
      "Average training loss: 0.044633366091383825\n",
      "Average test loss: 0.003129801027269827\n",
      "Epoch 58/300\n",
      "Average training loss: 0.044545734365781145\n",
      "Average test loss: 0.0031120804430296023\n",
      "Epoch 59/300\n",
      "Average training loss: 0.044385613209671444\n",
      "Average test loss: 0.0031606952862607107\n",
      "Epoch 60/300\n",
      "Average training loss: 0.044302865935696496\n",
      "Average test loss: 0.003150422771771749\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0441587009065681\n",
      "Average test loss: 0.0031310112114167877\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04411532138784727\n",
      "Average test loss: 0.0031553978100419045\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04397271494070689\n",
      "Average test loss: 0.003255280921856562\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04386818475524584\n",
      "Average test loss: 0.003156144214587079\n",
      "Epoch 65/300\n",
      "Average training loss: 0.043685413622193865\n",
      "Average test loss: 0.003128030591333906\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04353460091021326\n",
      "Average test loss: 0.003126253521276845\n",
      "Epoch 67/300\n",
      "Average training loss: 0.043505498689081934\n",
      "Average test loss: 0.003183668047810594\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04344170192629099\n",
      "Average test loss: 0.003184908759469787\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04331129667990737\n",
      "Average test loss: 0.0031318680860309136\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04321294791168637\n",
      "Average test loss: 0.0031190588331470887\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04310502081778314\n",
      "Average test loss: 0.003146059743853079\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04294783903327253\n",
      "Average test loss: 0.0031764380424800847\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04281457074483236\n",
      "Average test loss: 0.00315010418701503\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04273416161868308\n",
      "Average test loss: 0.003188781903228826\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04257994521657626\n",
      "Average test loss: 0.003162260231251518\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04263679427405198\n",
      "Average test loss: 0.003167867102349798\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04249146295090516\n",
      "Average test loss: 0.003144999694079161\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04233739130198955\n",
      "Average test loss: 0.0031657874839794305\n",
      "Epoch 79/300\n",
      "Average training loss: 0.042304527461528775\n",
      "Average test loss: 0.0031823086080451806\n",
      "Epoch 80/300\n",
      "Average training loss: 0.042146064185433915\n",
      "Average test loss: 0.003167015604260895\n",
      "Epoch 81/300\n",
      "Average training loss: 0.042052164225114715\n",
      "Average test loss: 0.0032705827903830344\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04195010410083665\n",
      "Average test loss: 0.003221038902592328\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04189987249672413\n",
      "Average test loss: 0.0031718251779675483\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04177157797747188\n",
      "Average test loss: 0.003159556834027171\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04166613655951288\n",
      "Average test loss: 0.003136432574854957\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04152451029419899\n",
      "Average test loss: 0.0032420138139277696\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0414903914531072\n",
      "Average test loss: 0.0031812254562973976\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04141578092177709\n",
      "Average test loss: 0.003204004834923479\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04128922285305129\n",
      "Average test loss: 0.003206994502287772\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04122155495815807\n",
      "Average test loss: 0.0031561197363254097\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04118006267812517\n",
      "Average test loss: 0.0031992149288869566\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04103790537185139\n",
      "Average test loss: 0.0034068055198424392\n",
      "Epoch 93/300\n",
      "Average training loss: 0.040947275476323236\n",
      "Average test loss: 0.003162901257061296\n",
      "Epoch 94/300\n",
      "Average training loss: 0.040902373628483875\n",
      "Average test loss: 0.003250043545746141\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04073319414920277\n",
      "Average test loss: 0.003272273628248109\n",
      "Epoch 96/300\n",
      "Average training loss: 0.040734862579239736\n",
      "Average test loss: 0.0031680966899212864\n",
      "Epoch 97/300\n",
      "Average training loss: 0.040593134987685416\n",
      "Average test loss: 0.0032619729015148347\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04056909066438675\n",
      "Average test loss: 0.0032070028624600834\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0404472854534785\n",
      "Average test loss: 0.0032586107905954124\n",
      "Epoch 100/300\n",
      "Average training loss: 0.040350669234991074\n",
      "Average test loss: 0.003176968678832054\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04027645364072588\n",
      "Average test loss: 0.003161456787855261\n",
      "Epoch 102/300\n",
      "Average training loss: 0.040192165248923835\n",
      "Average test loss: 0.0031991358395251964\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04010992635952102\n",
      "Average test loss: 0.0031563928013460503\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04002642481691308\n",
      "Average test loss: 0.0031917351000010966\n",
      "Epoch 105/300\n",
      "Average training loss: 0.039939211830496785\n",
      "Average test loss: 0.003319328198209405\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03990761591328515\n",
      "Average test loss: 0.0032577223107218742\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03983122861716482\n",
      "Average test loss: 0.0032712253199683296\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03977625155117777\n",
      "Average test loss: 0.0032104932771374784\n",
      "Epoch 109/300\n",
      "Average training loss: 0.039624318457312055\n",
      "Average test loss: 0.0032348812073469163\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03958199614286423\n",
      "Average test loss: 0.0032196484505095414\n",
      "Epoch 111/300\n",
      "Average training loss: 0.039502799342075984\n",
      "Average test loss: 0.0032732895144985783\n",
      "Epoch 112/300\n",
      "Average training loss: 0.039456425729725096\n",
      "Average test loss: 0.003337179006594751\n",
      "Epoch 113/300\n",
      "Average training loss: 0.039445096141762205\n",
      "Average test loss: 0.003235611379560497\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03934936045275794\n",
      "Average test loss: 0.0032921383320871325\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03927715515096982\n",
      "Average test loss: 0.00320918155544334\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03919277350273397\n",
      "Average test loss: 0.003352550860494375\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0391221770743529\n",
      "Average test loss: 0.003310236037605339\n",
      "Epoch 118/300\n",
      "Average training loss: 0.039018353746996984\n",
      "Average test loss: 0.003220502593864997\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03905040504038334\n",
      "Average test loss: 0.0032290916554629802\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03894541159272194\n",
      "Average test loss: 0.0033145120547463497\n",
      "Epoch 121/300\n",
      "Average training loss: 0.038810646790597175\n",
      "Average test loss: 0.0032591112765173115\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03875809572802649\n",
      "Average test loss: 0.0032491797612359125\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03874794340133667\n",
      "Average test loss: 0.0033252722376750574\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03870640052027172\n",
      "Average test loss: 0.0032540759983369044\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03860880800088247\n",
      "Average test loss: 0.0034159235254757935\n",
      "Epoch 126/300\n",
      "Average training loss: 0.038546314302417964\n",
      "Average test loss: 0.0032916346366206805\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03849501910143428\n",
      "Average test loss: 0.003271116520381636\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03849523053732183\n",
      "Average test loss: 0.0033316090293228624\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03837839100758235\n",
      "Average test loss: 0.0032515758064885934\n",
      "Epoch 130/300\n",
      "Average training loss: 0.038282448222239814\n",
      "Average test loss: 0.003282280147075653\n",
      "Epoch 131/300\n",
      "Average training loss: 0.038223153687185714\n",
      "Average test loss: 0.0032820219304412605\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0381613805178139\n",
      "Average test loss: 0.003324289624889692\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03821528707941373\n",
      "Average test loss: 0.003376982095133927\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0381058393948608\n",
      "Average test loss: 0.0032876489888876677\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03796876907348633\n",
      "Average test loss: 0.0033144083244519102\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03799858078360557\n",
      "Average test loss: 0.0032937979667137067\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03795789902408918\n",
      "Average test loss: 0.0033212155107822682\n",
      "Epoch 138/300\n",
      "Average training loss: 0.037809449235598244\n",
      "Average test loss: 0.00334441448851592\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03783636633223957\n",
      "Average test loss: 0.0033557479807900057\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0378519260485967\n",
      "Average test loss: 0.003264633130696085\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03768799204627673\n",
      "Average test loss: 0.003267123281127877\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03764805416266123\n",
      "Average test loss: 0.003276363149492277\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037644856904943784\n",
      "Average test loss: 0.003337339856972297\n",
      "Epoch 144/300\n",
      "Average training loss: 0.037607571138275994\n",
      "Average test loss: 0.0034779548624323474\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03742924561765459\n",
      "Average test loss: 0.003314853006352981\n",
      "Epoch 146/300\n",
      "Average training loss: 0.037555269565847184\n",
      "Average test loss: 0.003366484632094701\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03740681412981616\n",
      "Average test loss: 0.003392817294328577\n",
      "Epoch 148/300\n",
      "Average training loss: 0.037402983874082564\n",
      "Average test loss: 0.0032685572974797752\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0373626630720165\n",
      "Average test loss: 0.003457776402226753\n",
      "Epoch 150/300\n",
      "Average training loss: 0.037293937063879436\n",
      "Average test loss: 0.0034009241405874493\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03719154439038701\n",
      "Average test loss: 0.003480077945730752\n",
      "Epoch 152/300\n",
      "Average training loss: 0.037156087092227404\n",
      "Average test loss: 0.0033326458506700066\n",
      "Epoch 153/300\n",
      "Average training loss: 0.037189537041717104\n",
      "Average test loss: 0.00336481783611493\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03710461304916276\n",
      "Average test loss: 0.003410742176696658\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03705507661236657\n",
      "Average test loss: 0.0033380265643613207\n",
      "Epoch 156/300\n",
      "Average training loss: 0.037075320952468446\n",
      "Average test loss: 0.003415775402966473\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03705093749695354\n",
      "Average test loss: 0.0034413489069168766\n",
      "Epoch 158/300\n",
      "Average training loss: 0.036912927614318\n",
      "Average test loss: 0.003315002317022946\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03698034396105342\n",
      "Average test loss: 0.003278150217814578\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03682334752380848\n",
      "Average test loss: 0.0034218186320116124\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0368349270787504\n",
      "Average test loss: 0.0033574706843743723\n",
      "Epoch 162/300\n",
      "Average training loss: 0.036762937759359675\n",
      "Average test loss: 0.003516035209513373\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036701304333077535\n",
      "Average test loss: 0.0034098621952450936\n",
      "Epoch 164/300\n",
      "Average training loss: 0.036674644531475176\n",
      "Average test loss: 0.0034116800369487867\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03667436792121993\n",
      "Average test loss: 0.0036429906642685334\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03665765401721001\n",
      "Average test loss: 0.00330785781931546\n",
      "Epoch 167/300\n",
      "Average training loss: 0.036612864742676415\n",
      "Average test loss: 0.003341582805642651\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03657425535387463\n",
      "Average test loss: 0.0033923047778920993\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03654421487119463\n",
      "Average test loss: 0.0033839321045411957\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03647735063234965\n",
      "Average test loss: 0.0033690737390683757\n",
      "Epoch 171/300\n",
      "Average training loss: 0.036437843991650475\n",
      "Average test loss: 0.003581696868977613\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03632068053715759\n",
      "Average test loss: 0.003434531351344453\n",
      "Epoch 173/300\n",
      "Average training loss: 0.036394185092714096\n",
      "Average test loss: 0.0033591230875915953\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03628760043448872\n",
      "Average test loss: 0.0033068348446653947\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03628648805618286\n",
      "Average test loss: 0.0033099189293053417\n",
      "Epoch 176/300\n",
      "Average training loss: 0.036291817237933476\n",
      "Average test loss: 0.0034377187558760247\n",
      "Epoch 177/300\n",
      "Average training loss: 0.036232329179843266\n",
      "Average test loss: 0.003298370363811652\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03617538355456458\n",
      "Average test loss: 0.0033549198963575893\n",
      "Epoch 179/300\n",
      "Average training loss: 0.036078654824859564\n",
      "Average test loss: 0.003444886173535552\n",
      "Epoch 180/300\n",
      "Average training loss: 0.036175757269064586\n",
      "Average test loss: 0.0034494586305485833\n",
      "Epoch 181/300\n",
      "Average training loss: 0.036055785059928895\n",
      "Average test loss: 0.0034699203111231325\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0360382930735747\n",
      "Average test loss: 0.0033249615968929396\n",
      "Epoch 183/300\n",
      "Average training loss: 0.036003293954663805\n",
      "Average test loss: 0.0034305051623119247\n",
      "Epoch 184/300\n",
      "Average training loss: 0.036046191271808414\n",
      "Average test loss: 0.0034256271635078723\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03591292817725076\n",
      "Average test loss: 0.0035011333664879203\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03593795072866811\n",
      "Average test loss: 0.003423748635583454\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03588927710221873\n",
      "Average test loss: 0.0034952701228774257\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03587676282061471\n",
      "Average test loss: 0.0034102211613208054\n",
      "Epoch 189/300\n",
      "Average training loss: 0.035907624004615675\n",
      "Average test loss: 0.0033767777499225404\n",
      "Epoch 190/300\n",
      "Average training loss: 0.035757871669199734\n",
      "Average test loss: 0.003390114460968309\n",
      "Epoch 191/300\n",
      "Average training loss: 0.035726485755708484\n",
      "Average test loss: 0.0033853062900404134\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03576838822497262\n",
      "Average test loss: 0.003430643234401941\n",
      "Epoch 193/300\n",
      "Average training loss: 0.035670130282640454\n",
      "Average test loss: 0.0034173574663905635\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03566978495981958\n",
      "Average test loss: 0.0033501417593409617\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03566831363240878\n",
      "Average test loss: 0.003383060620476802\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03562103917698065\n",
      "Average test loss: 0.003435189328259892\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03560262371930811\n",
      "Average test loss: 0.0034609275721013545\n",
      "Epoch 198/300\n",
      "Average training loss: 0.035600086152553556\n",
      "Average test loss: 0.003433246840619379\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03556157182653745\n",
      "Average test loss: 0.003678351932515701\n",
      "Epoch 200/300\n",
      "Average training loss: 0.035507116142246456\n",
      "Average test loss: 0.0034005898603548606\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03552875742647383\n",
      "Average test loss: 0.0034259643658167784\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03547624331381586\n",
      "Average test loss: 0.0033237412265605397\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03547785762945811\n",
      "Average test loss: 0.003357277748278446\n",
      "Epoch 204/300\n",
      "Average training loss: 0.035409014809462756\n",
      "Average test loss: 0.0034097235629128087\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03540661158826616\n",
      "Average test loss: 0.0034193475459598833\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03535536012550195\n",
      "Average test loss: 0.0034248947174184852\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03535890904565652\n",
      "Average test loss: 0.0033818983141746784\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0352523495124446\n",
      "Average test loss: 0.0034324949040181107\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03531782934731907\n",
      "Average test loss: 0.0033591700175570117\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03529553315374586\n",
      "Average test loss: 0.0034575612358748913\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03520086096723874\n",
      "Average test loss: 0.0035090725566777918\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03526110526588228\n",
      "Average test loss: 0.0034637091401964427\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03515394988987181\n",
      "Average test loss: 0.0033906945782817073\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03508269886506928\n",
      "Average test loss: 0.003500691358620922\n",
      "Epoch 215/300\n",
      "Average training loss: 0.035134368615017994\n",
      "Average test loss: 0.0034376018674423295\n",
      "Epoch 216/300\n",
      "Average training loss: 0.035084415679176646\n",
      "Average test loss: 0.0036924432669248847\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03510567294723458\n",
      "Average test loss: 0.0034450178725851906\n",
      "Epoch 218/300\n",
      "Average training loss: 0.035074789106845854\n",
      "Average test loss: 0.003403531934859024\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0350174214227332\n",
      "Average test loss: 0.0033773418847057556\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03511218085885048\n",
      "Average test loss: 0.0033792129837804372\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03489467161231571\n",
      "Average test loss: 0.003497928691820966\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03496101436846786\n",
      "Average test loss: 0.003533522329189711\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03490062778525882\n",
      "Average test loss: 0.00351182374647922\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03492609728707208\n",
      "Average test loss: 0.003537173320021894\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03490447516242663\n",
      "Average test loss: 0.003344936203935908\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03491977698604266\n",
      "Average test loss: 0.0036063732612464164\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03491376459101836\n",
      "Average test loss: 0.0033787336821357408\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03472148871752951\n",
      "Average test loss: 0.003374076852678425\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03484724414679739\n",
      "Average test loss: 0.0035362250440650516\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03479950837625398\n",
      "Average test loss: 0.0034518367987540033\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03476750870876842\n",
      "Average test loss: 0.0034948351027237045\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0347120967970954\n",
      "Average test loss: 0.003414637805066175\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03469600235256884\n",
      "Average test loss: 0.0035625479950880013\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03468594818148348\n",
      "Average test loss: 0.0036030125971883535\n",
      "Epoch 235/300\n",
      "Average training loss: 0.034698445641332204\n",
      "Average test loss: 0.0035001208221332896\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03471057474778758\n",
      "Average test loss: 0.0035138992470585637\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03463731077975697\n",
      "Average test loss: 0.003438129331295689\n",
      "Epoch 238/300\n",
      "Average training loss: 0.034587114526165856\n",
      "Average test loss: 0.0034558046586397623\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03456247673432032\n",
      "Average test loss: 0.0034590887218299837\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03460091787742244\n",
      "Average test loss: 0.0034501544371661214\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03454777511954307\n",
      "Average test loss: 0.0035422908183601167\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03457810047268867\n",
      "Average test loss: 0.0034778366848412486\n",
      "Epoch 243/300\n",
      "Average training loss: 0.034585408469041186\n",
      "Average test loss: 0.0034468472328864864\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03444287290672461\n",
      "Average test loss: 0.003632940541125006\n",
      "Epoch 245/300\n",
      "Average training loss: 0.034438867804076935\n",
      "Average test loss: 0.0034343220678468545\n",
      "Epoch 246/300\n",
      "Average training loss: 0.034421227721704376\n",
      "Average test loss: 0.0036147116675145094\n",
      "Epoch 247/300\n",
      "Average training loss: 0.034369531457622844\n",
      "Average test loss: 0.0035008081729627316\n",
      "Epoch 248/300\n",
      "Average training loss: 0.034500546185506714\n",
      "Average test loss: 0.0034114861118917663\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03436130502157741\n",
      "Average test loss: 0.0036012996037801106\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03437166779736678\n",
      "Average test loss: 0.0034141097894559306\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03433463216821352\n",
      "Average test loss: 0.0034966896093553967\n",
      "Epoch 252/300\n",
      "Average training loss: 0.034354167994525696\n",
      "Average test loss: 0.0034528570767078134\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03434423884418276\n",
      "Average test loss: 0.003595016833394766\n",
      "Epoch 254/300\n",
      "Average training loss: 0.034288540340132186\n",
      "Average test loss: 0.0034556059162649845\n",
      "Epoch 255/300\n",
      "Average training loss: 0.034286025252607134\n",
      "Average test loss: 0.003437925046309829\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03422703525755141\n",
      "Average test loss: 0.00339273683167994\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03423159641855293\n",
      "Average test loss: 0.0034578649171938497\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03421758579876688\n",
      "Average test loss: 0.0033771514050248597\n",
      "Epoch 259/300\n",
      "Average training loss: 0.034184757923086485\n",
      "Average test loss: 0.0035879434790048333\n",
      "Epoch 260/300\n",
      "Average training loss: 0.034179931988318764\n",
      "Average test loss: 0.003516130640481909\n",
      "Epoch 261/300\n",
      "Average training loss: 0.034156017402807874\n",
      "Average test loss: 0.003387029702257779\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03419676543441084\n",
      "Average test loss: 0.0035963699401666722\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03411349469423294\n",
      "Average test loss: 0.003431286371830437\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0341312464343177\n",
      "Average test loss: 0.003511474795639515\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03415470492177539\n",
      "Average test loss: 0.0035918609316771227\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03411826338536209\n",
      "Average test loss: 0.003530812023414506\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03403536222378413\n",
      "Average test loss: 0.003404029110653533\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03412162405749162\n",
      "Average test loss: 0.003368232924077246\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03407286495632596\n",
      "Average test loss: 0.0035683074051307308\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03404036938481861\n",
      "Average test loss: 0.003504886797732777\n",
      "Epoch 271/300\n",
      "Average training loss: 0.033956008268727196\n",
      "Average test loss: 0.003515072222178181\n",
      "Epoch 272/300\n",
      "Average training loss: 0.033937065995401805\n",
      "Average test loss: 0.0034734673001286056\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03395254820916388\n",
      "Average test loss: 0.0035378679310282073\n",
      "Epoch 274/300\n",
      "Average training loss: 0.033934224963188174\n",
      "Average test loss: 0.0035139666785382563\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0339087950868739\n",
      "Average test loss: 0.0035434927071134248\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03384978768891758\n",
      "Average test loss: 0.0034963914934131833\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03396778619951672\n",
      "Average test loss: 0.003502028253653811\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03389543015923765\n",
      "Average test loss: 0.0034736631899658175\n",
      "Epoch 279/300\n",
      "Average training loss: 0.033867760659919845\n",
      "Average test loss: 0.0035047015734016897\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03385399955925014\n",
      "Average test loss: 0.0035261552561488417\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03385762189163102\n",
      "Average test loss: 0.0035061815596289106\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03379649468594127\n",
      "Average test loss: 0.0033960481356415484\n",
      "Epoch 283/300\n",
      "Average training loss: 0.033700987494654125\n",
      "Average test loss: 0.003515882171276543\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03380655058224996\n",
      "Average test loss: 0.00345929590633346\n",
      "Epoch 285/300\n",
      "Average training loss: 0.033799006655812265\n",
      "Average test loss: 0.0035015933385325803\n",
      "Epoch 286/300\n",
      "Average training loss: 0.033811225563287736\n",
      "Average test loss: 0.0035143959706442226\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03376946622133255\n",
      "Average test loss: 0.003461484644562006\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03372285389900208\n",
      "Average test loss: 0.0035259739926291837\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03367823192477226\n",
      "Average test loss: 0.0035381185648341975\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03372835719585419\n",
      "Average test loss: 0.0035098941415134402\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03368863626983431\n",
      "Average test loss: 0.003535796655341983\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03364918797545963\n",
      "Average test loss: 0.0035197268130464686\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03362049880623817\n",
      "Average test loss: 0.003578408169042733\n",
      "Epoch 294/300\n",
      "Average training loss: 0.033671346111430064\n",
      "Average test loss: 0.0034728343532317214\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03363350085748566\n",
      "Average test loss: 0.0035093317216055263\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03365702148278554\n",
      "Average test loss: 0.0034317498004270927\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03361157214641571\n",
      "Average test loss: 0.0035740183792594407\n",
      "Epoch 298/300\n",
      "Average training loss: 0.033604609688123066\n",
      "Average test loss: 0.0035165851302444936\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03356688866350386\n",
      "Average test loss: 0.003600220479692022\n",
      "Epoch 300/300\n",
      "Average training loss: 0.033605920596255195\n",
      "Average test loss: 0.003432684709628423\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7376144856876797\n",
      "Average test loss: 0.005281322453998857\n",
      "Epoch 2/300\n",
      "Average training loss: 0.10057887077331543\n",
      "Average test loss: 0.004426143348423971\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07775290870666504\n",
      "Average test loss: 0.004250533839273784\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06847839774688085\n",
      "Average test loss: 0.0038686273623671796\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06306233320964708\n",
      "Average test loss: 0.003908903075175153\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05935679574476348\n",
      "Average test loss: 0.0036504373720122707\n",
      "Epoch 7/300\n",
      "Average training loss: 0.056640990095006095\n",
      "Average test loss: 0.0035588228799816636\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05451381362146802\n",
      "Average test loss: 0.0035226471150914826\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05284112772345543\n",
      "Average test loss: 0.0033815305079850887\n",
      "Epoch 10/300\n",
      "Average training loss: 0.051388037868671944\n",
      "Average test loss: 0.003370658508191506\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05013052100936572\n",
      "Average test loss: 0.0032378551014181642\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04904331455959214\n",
      "Average test loss: 0.0032140111207134194\n",
      "Epoch 13/300\n",
      "Average training loss: 0.048125660475757386\n",
      "Average test loss: 0.0032625699150893423\n",
      "Epoch 14/300\n",
      "Average training loss: 0.047284456587500044\n",
      "Average test loss: 0.0030718525720553266\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04651051643821928\n",
      "Average test loss: 0.003040819723572996\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04573202074567477\n",
      "Average test loss: 0.003003240427830153\n",
      "Epoch 17/300\n",
      "Average training loss: 0.045109344197644126\n",
      "Average test loss: 0.003181874048585693\n",
      "Epoch 18/300\n",
      "Average training loss: 0.044501577811108695\n",
      "Average test loss: 0.0028760474823001357\n",
      "Epoch 19/300\n",
      "Average training loss: 0.043847820318407484\n",
      "Average test loss: 0.0028797711073938344\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04338837489320172\n",
      "Average test loss: 0.002803223158750269\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04279826395048036\n",
      "Average test loss: 0.0027903121068245836\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04232880548967256\n",
      "Average test loss: 0.0027395620524055427\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04186867823865679\n",
      "Average test loss: 0.0027600581554902926\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04151119129690859\n",
      "Average test loss: 0.002693310129766663\n",
      "Epoch 25/300\n",
      "Average training loss: 0.041091432372728984\n",
      "Average test loss: 0.0026903061183790366\n",
      "Epoch 26/300\n",
      "Average training loss: 0.040749650518099464\n",
      "Average test loss: 0.0026826968737360506\n",
      "Epoch 27/300\n",
      "Average training loss: 0.040433594066235756\n",
      "Average test loss: 0.0026343805752694605\n",
      "Epoch 28/300\n",
      "Average training loss: 0.040075910571548676\n",
      "Average test loss: 0.0026434185202750893\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039823750245902276\n",
      "Average test loss: 0.002620149481213755\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0395140004803737\n",
      "Average test loss: 0.0026125653514431583\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03929901523060269\n",
      "Average test loss: 0.0026238844589226776\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03904024547338486\n",
      "Average test loss: 0.0025803258867106503\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03885780893431769\n",
      "Average test loss: 0.0025865921585096255\n",
      "Epoch 34/300\n",
      "Average training loss: 0.038565203054083716\n",
      "Average test loss: 0.002646053843614128\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03841085939937168\n",
      "Average test loss: 0.0025451138392090797\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03821521237823698\n",
      "Average test loss: 0.0025358719740890792\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03807930588059955\n",
      "Average test loss: 0.0025198708031740455\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03785629241168499\n",
      "Average test loss: 0.00252812506361968\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03768450363477071\n",
      "Average test loss: 0.002522073585747017\n",
      "Epoch 40/300\n",
      "Average training loss: 0.037556672119432026\n",
      "Average test loss: 0.002529985347141822\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03737691924141513\n",
      "Average test loss: 0.00251098139045967\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03719423002170192\n",
      "Average test loss: 0.0025226746729264658\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03710649321807755\n",
      "Average test loss: 0.0025465734645517338\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03703064063025845\n",
      "Average test loss: 0.0025101399748689596\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03681635262403223\n",
      "Average test loss: 0.0025004472093035778\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03666857635478179\n",
      "Average test loss: 0.0024714836097425885\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03651910328865051\n",
      "Average test loss: 0.002492994278151956\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0364107096079323\n",
      "Average test loss: 0.0024796104039996863\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03623331124087175\n",
      "Average test loss: 0.0025223702895972463\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03613885494735506\n",
      "Average test loss: 0.0024797829404059383\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03606068113280667\n",
      "Average test loss: 0.002496943352743983\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03590254895554648\n",
      "Average test loss: 0.002609361594542861\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03578126697076692\n",
      "Average test loss: 0.0035859313839011722\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03567048120829794\n",
      "Average test loss: 0.002465677488077846\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03560402736895614\n",
      "Average test loss: 0.002474336599310239\n",
      "Epoch 56/300\n",
      "Average training loss: 0.035492304280400275\n",
      "Average test loss: 0.002501361695428689\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03530303452577856\n",
      "Average test loss: 0.002532661725559996\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03520718279315366\n",
      "Average test loss: 0.002462078483982219\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03509459758632713\n",
      "Average test loss: 0.002471043223929074\n",
      "Epoch 60/300\n",
      "Average training loss: 0.034980827608042296\n",
      "Average test loss: 0.0024767027772549127\n",
      "Epoch 61/300\n",
      "Average training loss: 0.034878746756249\n",
      "Average test loss: 0.002489250940995084\n",
      "Epoch 62/300\n",
      "Average training loss: 0.034789547729823324\n",
      "Average test loss: 0.0024902012339896628\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0346499402390586\n",
      "Average test loss: 0.0024697203372294704\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03456410202715132\n",
      "Average test loss: 0.002489304081226389\n",
      "Epoch 65/300\n",
      "Average training loss: 0.034439832816521326\n",
      "Average test loss: 0.0024924708056367107\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03436351427767012\n",
      "Average test loss: 0.0024857738208439615\n",
      "Epoch 67/300\n",
      "Average training loss: 0.034281742324431735\n",
      "Average test loss: 0.002446413835717572\n",
      "Epoch 68/300\n",
      "Average training loss: 0.034148758666382895\n",
      "Average test loss: 0.0024954341877665783\n",
      "Epoch 69/300\n",
      "Average training loss: 0.034092865569723976\n",
      "Average test loss: 0.002461263239280217\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03390436828467581\n",
      "Average test loss: 0.0024575391235864825\n",
      "Epoch 71/300\n",
      "Average training loss: 0.033795852272046935\n",
      "Average test loss: 0.002467369198385212\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03374827655322022\n",
      "Average test loss: 0.0024537505793074768\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03364499779542287\n",
      "Average test loss: 0.002497442551992006\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03352796298927731\n",
      "Average test loss: 0.0025550079118046498\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03346380091044638\n",
      "Average test loss: 0.0024858533814549446\n",
      "Epoch 76/300\n",
      "Average training loss: 0.033423997630675634\n",
      "Average test loss: 0.0024866157627354067\n",
      "Epoch 77/300\n",
      "Average training loss: 0.033276447690195506\n",
      "Average test loss: 0.0025101392548531293\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03319858269724581\n",
      "Average test loss: 0.002487386154735254\n",
      "Epoch 79/300\n",
      "Average training loss: 0.033027908330162364\n",
      "Average test loss: 0.002524388045693437\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03310770221054554\n",
      "Average test loss: 0.0024755108271621996\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03290943706532319\n",
      "Average test loss: 0.002497537098618017\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03283136875099606\n",
      "Average test loss: 0.002524870632423295\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03271259037322468\n",
      "Average test loss: 0.002462705808174279\n",
      "Epoch 84/300\n",
      "Average training loss: 0.032691747213403385\n",
      "Average test loss: 0.002502387969651156\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03264535833895207\n",
      "Average test loss: 0.0025670027364459304\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03248678522308668\n",
      "Average test loss: 0.0024870135670320855\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03240636727710565\n",
      "Average test loss: 0.002507185272975928\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03232226958870888\n",
      "Average test loss: 0.0024921295947084826\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03215601093073686\n",
      "Average test loss: 0.0025071409228775237\n",
      "Epoch 90/300\n",
      "Average training loss: 0.032139352276921274\n",
      "Average test loss: 0.002515726200201445\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03211270188457436\n",
      "Average test loss: 0.0024855428538802598\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03196838049921725\n",
      "Average test loss: 0.002545516547022594\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03191397305329641\n",
      "Average test loss: 0.002527012451655335\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03187512982885043\n",
      "Average test loss: 0.002497025322789947\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03178628528780408\n",
      "Average test loss: 0.0024881010330799554\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03168196411265267\n",
      "Average test loss: 0.0024939455698347753\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03156170802646213\n",
      "Average test loss: 0.0025195340210985807\n",
      "Epoch 98/300\n",
      "Average training loss: 0.031507672985394794\n",
      "Average test loss: 0.002533726742077205\n",
      "Epoch 99/300\n",
      "Average training loss: 0.031511577528383995\n",
      "Average test loss: 0.0025200794724126656\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0314108550813463\n",
      "Average test loss: 0.002553257728409436\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03137014205422666\n",
      "Average test loss: 0.002612017538605465\n",
      "Epoch 102/300\n",
      "Average training loss: 0.031240547961658902\n",
      "Average test loss: 0.002540601881634858\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03119259898695681\n",
      "Average test loss: 0.002628007861268189\n",
      "Epoch 104/300\n",
      "Average training loss: 0.031110578945941395\n",
      "Average test loss: 0.00252882923786011\n",
      "Epoch 105/300\n",
      "Average training loss: 0.031052011652125254\n",
      "Average test loss: 0.002589533584399356\n",
      "Epoch 106/300\n",
      "Average training loss: 0.030983475027812853\n",
      "Average test loss: 0.002562920482518772\n",
      "Epoch 107/300\n",
      "Average training loss: 0.030889336650570233\n",
      "Average test loss: 0.0025384843077303635\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03086585620542367\n",
      "Average test loss: 0.0025559517428692846\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0307903556873401\n",
      "Average test loss: 0.0025164986718446015\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03068549516465929\n",
      "Average test loss: 0.002575776866947611\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030681043022208743\n",
      "Average test loss: 0.0025201031894733507\n",
      "Epoch 112/300\n",
      "Average training loss: 0.030649857088923455\n",
      "Average test loss: 0.002536020214772887\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03054565386970838\n",
      "Average test loss: 0.002548194279273351\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03046126753754086\n",
      "Average test loss: 0.0025274465835342804\n",
      "Epoch 115/300\n",
      "Average training loss: 0.030397270490725836\n",
      "Average test loss: 0.002515232864456872\n",
      "Epoch 116/300\n",
      "Average training loss: 0.030335436610711947\n",
      "Average test loss: 0.002566914605183734\n",
      "Epoch 117/300\n",
      "Average training loss: 0.030266742401652866\n",
      "Average test loss: 0.0025241643252472083\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03023947390417258\n",
      "Average test loss: 0.002574525619132651\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030179298165771695\n",
      "Average test loss: 0.002549938814093669\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03012076263791985\n",
      "Average test loss: 0.002645341916009784\n",
      "Epoch 121/300\n",
      "Average training loss: 0.030116093513038424\n",
      "Average test loss: 0.0025734115762429104\n",
      "Epoch 122/300\n",
      "Average training loss: 0.030019382725159326\n",
      "Average test loss: 0.0025943410421411196\n",
      "Epoch 123/300\n",
      "Average training loss: 0.029952854942944314\n",
      "Average test loss: 0.0026338835664921336\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02989637781845199\n",
      "Average test loss: 0.002601785412368675\n",
      "Epoch 125/300\n",
      "Average training loss: 0.029835184931755066\n",
      "Average test loss: 0.0025875476563556325\n",
      "Epoch 126/300\n",
      "Average training loss: 0.029768460702564982\n",
      "Average test loss: 0.002553306229205595\n",
      "Epoch 127/300\n",
      "Average training loss: 0.029729469277792506\n",
      "Average test loss: 0.0026193149708625342\n",
      "Epoch 128/300\n",
      "Average training loss: 0.029621425948209233\n",
      "Average test loss: 0.0026230035643610688\n",
      "Epoch 129/300\n",
      "Average training loss: 0.029601748805907036\n",
      "Average test loss: 0.0026007820351256266\n",
      "Epoch 130/300\n",
      "Average training loss: 0.029569351802269616\n",
      "Average test loss: 0.002711042159133487\n",
      "Epoch 131/300\n",
      "Average training loss: 0.029565084076590007\n",
      "Average test loss: 0.0027066123129593\n",
      "Epoch 132/300\n",
      "Average training loss: 0.029509981979926426\n",
      "Average test loss: 0.0026225819581498703\n",
      "Epoch 133/300\n",
      "Average training loss: 0.029456868830654355\n",
      "Average test loss: 0.0030571983945038585\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02941464966038863\n",
      "Average test loss: 0.0025406927385677896\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02932849113808738\n",
      "Average test loss: 0.0026965157602810196\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02939883698191908\n",
      "Average test loss: 0.0026198890291982226\n",
      "Epoch 137/300\n",
      "Average training loss: 0.029183758700887363\n",
      "Average test loss: 0.0025579114065816004\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02920632167822785\n",
      "Average test loss: 0.0025984194839580193\n",
      "Epoch 139/300\n",
      "Average training loss: 0.029156896238525708\n",
      "Average test loss: 0.002614922861258189\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02913239484031995\n",
      "Average test loss: 0.0026100264330291088\n",
      "Epoch 141/300\n",
      "Average training loss: 0.029048845796121492\n",
      "Average test loss: 0.0026704213472290173\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02899319051537249\n",
      "Average test loss: 0.0025548952950371637\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0289785365694099\n",
      "Average test loss: 0.002666918311888973\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028912707757618693\n",
      "Average test loss: 0.0025955468996738396\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02887066269086467\n",
      "Average test loss: 0.0025960016635557014\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02881580610573292\n",
      "Average test loss: 0.0026557825919654634\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02876767489976353\n",
      "Average test loss: 0.0025979813028954796\n",
      "Epoch 148/300\n",
      "Average training loss: 0.028749909579753877\n",
      "Average test loss: 0.0025794196664873097\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02873809088104301\n",
      "Average test loss: 0.002589896345097158\n",
      "Epoch 150/300\n",
      "Average training loss: 0.028735946964886454\n",
      "Average test loss: 0.0026341403927654028\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02860452995532089\n",
      "Average test loss: 0.0026169074719978704\n",
      "Epoch 152/300\n",
      "Average training loss: 0.028593907399310007\n",
      "Average test loss: 0.002583549428731203\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0285996197346184\n",
      "Average test loss: 0.002653806407418516\n",
      "Epoch 154/300\n",
      "Average training loss: 0.028560152863462765\n",
      "Average test loss: 0.00263067341554496\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02846890603337023\n",
      "Average test loss: 0.002641859636745519\n",
      "Epoch 156/300\n",
      "Average training loss: 0.028486521826850044\n",
      "Average test loss: 0.002590894821099937\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02844695178667704\n",
      "Average test loss: 0.0027392644149561722\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028347379161251917\n",
      "Average test loss: 0.0026675602299057774\n",
      "Epoch 159/300\n",
      "Average training loss: 0.028334473880628746\n",
      "Average test loss: 0.0027238058787253166\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02829982171456019\n",
      "Average test loss: 0.0026213673657427233\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02830888017018636\n",
      "Average test loss: 0.002684969083406031\n",
      "Epoch 162/300\n",
      "Average training loss: 0.028225012640158337\n",
      "Average test loss: 0.002645037955707974\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02824677287869983\n",
      "Average test loss: 0.0025984050787778365\n",
      "Epoch 164/300\n",
      "Average training loss: 0.028139049635993108\n",
      "Average test loss: 0.002608258857495255\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02815876799656285\n",
      "Average test loss: 0.0026558754986359015\n",
      "Epoch 166/300\n",
      "Average training loss: 0.028156865381532246\n",
      "Average test loss: 0.0026600081614322132\n",
      "Epoch 167/300\n",
      "Average training loss: 0.028074599718054136\n",
      "Average test loss: 0.0027019604005747372\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02798619765208827\n",
      "Average test loss: 0.002631483983972834\n",
      "Epoch 169/300\n",
      "Average training loss: 0.028107692948646016\n",
      "Average test loss: 0.0026039169174101617\n",
      "Epoch 170/300\n",
      "Average training loss: 0.027960775290926297\n",
      "Average test loss: 0.002715125279708041\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02789685565398799\n",
      "Average test loss: 0.002770216114405129\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02794174992872609\n",
      "Average test loss: 0.002687872967786259\n",
      "Epoch 173/300\n",
      "Average training loss: 0.027883283966117434\n",
      "Average test loss: 0.002647561028185818\n",
      "Epoch 174/300\n",
      "Average training loss: 0.027829999642239676\n",
      "Average test loss: 0.0026597405407163834\n",
      "Epoch 175/300\n",
      "Average training loss: 0.027861694908804364\n",
      "Average test loss: 0.00264999317119105\n",
      "Epoch 176/300\n",
      "Average training loss: 0.027809197453988924\n",
      "Average test loss: 0.0026603067270997496\n",
      "Epoch 177/300\n",
      "Average training loss: 0.027765087061458162\n",
      "Average test loss: 0.0027034545971287623\n",
      "Epoch 178/300\n",
      "Average training loss: 0.027719747535056537\n",
      "Average test loss: 0.002718760653812852\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02766036597556538\n",
      "Average test loss: 0.0027206711874653896\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02766068552268876\n",
      "Average test loss: 0.0027150304834875796\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02763904058933258\n",
      "Average test loss: 0.0028498580397831067\n",
      "Epoch 182/300\n",
      "Average training loss: 0.027559007321794827\n",
      "Average test loss: 0.0027194181595825485\n",
      "Epoch 183/300\n",
      "Average training loss: 0.027586449419458706\n",
      "Average test loss: 0.0026780469650402663\n",
      "Epoch 184/300\n",
      "Average training loss: 0.027617650411195224\n",
      "Average test loss: 0.0026600749745137164\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027593812371293703\n",
      "Average test loss: 0.0027087406350506677\n",
      "Epoch 186/300\n",
      "Average training loss: 0.027489610801140468\n",
      "Average test loss: 0.0026633660441471472\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02744443373878797\n",
      "Average test loss: 0.0027280981433060432\n",
      "Epoch 188/300\n",
      "Average training loss: 0.027463089068730673\n",
      "Average test loss: 0.0026978660693081718\n",
      "Epoch 189/300\n",
      "Average training loss: 0.027439456815520923\n",
      "Average test loss: 0.002692164341194762\n",
      "Epoch 190/300\n",
      "Average training loss: 0.027377267708381017\n",
      "Average test loss: 0.002646763563156128\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027307929744323094\n",
      "Average test loss: 0.0028058339156624345\n",
      "Epoch 192/300\n",
      "Average training loss: 0.027307593983080653\n",
      "Average test loss: 0.0027370210132665104\n",
      "Epoch 193/300\n",
      "Average training loss: 0.027307828709483145\n",
      "Average test loss: 0.002655958008228077\n",
      "Epoch 194/300\n",
      "Average training loss: 0.027338792784346474\n",
      "Average test loss: 0.0027336738486256865\n",
      "Epoch 195/300\n",
      "Average training loss: 0.027267515839801894\n",
      "Average test loss: 0.0027236031798852814\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02725462627576457\n",
      "Average test loss: 0.0027670538150187996\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0272163012938367\n",
      "Average test loss: 0.002709062488335702\n",
      "Epoch 198/300\n",
      "Average training loss: 0.027190570556455188\n",
      "Average test loss: 0.002689211974851787\n",
      "Epoch 199/300\n",
      "Average training loss: 0.027237896167569692\n",
      "Average test loss: 0.0026156300951002374\n",
      "Epoch 200/300\n",
      "Average training loss: 0.027127051199475925\n",
      "Average test loss: 0.0026629527877602313\n",
      "Epoch 201/300\n",
      "Average training loss: 0.027174349925584262\n",
      "Average test loss: 0.0027443853926120533\n",
      "Epoch 202/300\n",
      "Average training loss: 0.027090399409333867\n",
      "Average test loss: 0.0027700106752001578\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027121602329942915\n",
      "Average test loss: 0.0027677445323723887\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02705398911072148\n",
      "Average test loss: 0.0027044944680399367\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02700140082339446\n",
      "Average test loss: 0.0026764308756424323\n",
      "Epoch 206/300\n",
      "Average training loss: 0.026950348223249116\n",
      "Average test loss: 0.0027024924808906185\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02700100637641218\n",
      "Average test loss: 0.002751133860399326\n",
      "Epoch 208/300\n",
      "Average training loss: 0.026996224224567412\n",
      "Average test loss: 0.002690983921703365\n",
      "Epoch 209/300\n",
      "Average training loss: 0.027030607043041124\n",
      "Average test loss: 0.0026758835406766996\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026962442710995675\n",
      "Average test loss: 0.0026708548826475938\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02688735041519006\n",
      "Average test loss: 0.0027350942531807554\n",
      "Epoch 212/300\n",
      "Average training loss: 0.026886376962065698\n",
      "Average test loss: 0.0027066956764707963\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026813487658898037\n",
      "Average test loss: 0.0027853558195961845\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02680100792977545\n",
      "Average test loss: 0.002681124878840314\n",
      "Epoch 215/300\n",
      "Average training loss: 0.026783489061726465\n",
      "Average test loss: 0.0027257890999317167\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026789131081766553\n",
      "Average test loss: 0.0026937385346326565\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02675708649886979\n",
      "Average test loss: 0.0026803995807551675\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02673520470990075\n",
      "Average test loss: 0.002752447757249077\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026721656440032854\n",
      "Average test loss: 0.002700379790841705\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02665687089496189\n",
      "Average test loss: 0.002768493935672773\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026672646171516844\n",
      "Average test loss: 0.0027232390638026924\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02672287664645248\n",
      "Average test loss: 0.002669375577734576\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0266594893021716\n",
      "Average test loss: 0.002740084226346678\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02669630275335577\n",
      "Average test loss: 0.002682067969193061\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026636470071143573\n",
      "Average test loss: 0.002700095397937629\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026653511012593906\n",
      "Average test loss: 0.002783203748986125\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026638742725054423\n",
      "Average test loss: 0.0027676384647687276\n",
      "Epoch 228/300\n",
      "Average training loss: 0.026518385466602113\n",
      "Average test loss: 0.0027138799180587135\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026498228581415283\n",
      "Average test loss: 0.002745532825175259\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026534816458821296\n",
      "Average test loss: 0.0027004784459455145\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02650228028661675\n",
      "Average test loss: 0.0027283355974488787\n",
      "Epoch 232/300\n",
      "Average training loss: 0.026437253546383647\n",
      "Average test loss: 0.0027272043673114642\n",
      "Epoch 233/300\n",
      "Average training loss: 0.026443557942907016\n",
      "Average test loss: 0.0027210880163022213\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02641979624165429\n",
      "Average test loss: 0.0026748138374338547\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026443563489450348\n",
      "Average test loss: 0.0027642640312098795\n",
      "Epoch 236/300\n",
      "Average training loss: 0.026471069672041467\n",
      "Average test loss: 0.0028182555178387298\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026382364930378065\n",
      "Average test loss: 0.0027293839688516328\n",
      "Epoch 238/300\n",
      "Average training loss: 0.026399173787898488\n",
      "Average test loss: 0.002753783594403002\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026311058266295328\n",
      "Average test loss: 0.0028132068800429504\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026338891373740302\n",
      "Average test loss: 0.002829879213693655\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026286004407538307\n",
      "Average test loss: 0.002717120143170986\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026367695881260766\n",
      "Average test loss: 0.002720234761428502\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026242321204808024\n",
      "Average test loss: 0.0026375791469795836\n",
      "Epoch 244/300\n",
      "Average training loss: 0.026257040300303035\n",
      "Average test loss: 0.0027386581409308644\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02627711650232474\n",
      "Average test loss: 0.0026985870295514664\n",
      "Epoch 246/300\n",
      "Average training loss: 0.026213935270905493\n",
      "Average test loss: 0.002696305323599113\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026177228833238284\n",
      "Average test loss: 0.0027482306723379426\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02616933232711421\n",
      "Average test loss: 0.0027155304634943604\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026154921129345893\n",
      "Average test loss: 0.002745773181112276\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026185082919067806\n",
      "Average test loss: 0.002787695421319869\n",
      "Epoch 251/300\n",
      "Average training loss: 0.026167971854408584\n",
      "Average test loss: 0.002821364945007695\n",
      "Epoch 252/300\n",
      "Average training loss: 0.026163157410091824\n",
      "Average test loss: 0.00290525574889034\n",
      "Epoch 253/300\n",
      "Average training loss: 0.026173818866411847\n",
      "Average test loss: 0.0027377116647031574\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026113334437211356\n",
      "Average test loss: 0.002791185709958275\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02614155569507016\n",
      "Average test loss: 0.002740555102626483\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026127114224765036\n",
      "Average test loss: 0.0026717118439781996\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026021608480148846\n",
      "Average test loss: 0.0026970756612718105\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02602606396211518\n",
      "Average test loss: 0.002684270135023528\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026044288977980615\n",
      "Average test loss: 0.002719148413795564\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02600088133911292\n",
      "Average test loss: 0.002744550972763035\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026034888492690193\n",
      "Average test loss: 0.00280554943304095\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02597128845916854\n",
      "Average test loss: 0.0027461958639323713\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025999211213654942\n",
      "Average test loss: 0.002699603951225678\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025996777104006875\n",
      "Average test loss: 0.002753118274733424\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025949047176374328\n",
      "Average test loss: 0.0027491958375192352\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026006564097272027\n",
      "Average test loss: 0.0026626663881664475\n",
      "Epoch 267/300\n",
      "Average training loss: 0.025895292027129067\n",
      "Average test loss: 0.0027516764405493936\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025861259324683084\n",
      "Average test loss: 0.002677713222387764\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025901305480135813\n",
      "Average test loss: 0.0028679461379845935\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02587818945116467\n",
      "Average test loss: 0.002767757565507458\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025886925847993958\n",
      "Average test loss: 0.002813696938049462\n",
      "Epoch 272/300\n",
      "Average training loss: 0.025817489812771478\n",
      "Average test loss: 0.00284396737958822\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0258195886661609\n",
      "Average test loss: 0.0027846806610210073\n",
      "Epoch 274/300\n",
      "Average training loss: 0.025874870523810385\n",
      "Average test loss: 0.002790860759301318\n",
      "Epoch 275/300\n",
      "Average training loss: 0.025846943323810895\n",
      "Average test loss: 0.0027543068275683457\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025779201267494094\n",
      "Average test loss: 0.002752355534893771\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02576533987455898\n",
      "Average test loss: 0.0028045034321645896\n",
      "Epoch 278/300\n",
      "Average training loss: 0.025778132329384487\n",
      "Average test loss: 0.0027917857898606194\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025810974367790753\n",
      "Average test loss: 0.002776295622810721\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025657753446035915\n",
      "Average test loss: 0.002828370247864061\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025716990439428225\n",
      "Average test loss: 0.002760308012780216\n",
      "Epoch 282/300\n",
      "Average training loss: 0.025726473089721466\n",
      "Average test loss: 0.0027615283127460214\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025732186492946413\n",
      "Average test loss: 0.0027186296789182556\n",
      "Epoch 284/300\n",
      "Average training loss: 0.025662257624997034\n",
      "Average test loss: 0.002777460506806771\n",
      "Epoch 285/300\n",
      "Average training loss: 0.025595798134803772\n",
      "Average test loss: 0.0027734021716233756\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02570729944606622\n",
      "Average test loss: 0.0028321945856635768\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02569396930270725\n",
      "Average test loss: 0.0027536568947964246\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02564823905626933\n",
      "Average test loss: 0.0027036233213212756\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0256080628219578\n",
      "Average test loss: 0.0027291456926614045\n",
      "Epoch 290/300\n",
      "Average training loss: 0.025616189670231607\n",
      "Average test loss: 0.0028046096470206974\n",
      "Epoch 291/300\n",
      "Average training loss: 0.025578862824373775\n",
      "Average test loss: 0.0028079509682332475\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02555803607735369\n",
      "Average test loss: 0.002811366644791431\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025528603083557554\n",
      "Average test loss: 0.0028302713417344624\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025576234471466806\n",
      "Average test loss: 0.00288097508230971\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025530948764748044\n",
      "Average test loss: 0.002738135603360004\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025556546673178672\n",
      "Average test loss: 0.002749105892351104\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025535717520448895\n",
      "Average test loss: 0.0027637474454111524\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02548043995598952\n",
      "Average test loss: 0.0027912430312070583\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025495299201872615\n",
      "Average test loss: 0.002763235016001595\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02544689105782244\n",
      "Average test loss: 0.0027244921875082782\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.678271552628941\n",
      "Average test loss: 0.005006479233503342\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09137926767932045\n",
      "Average test loss: 0.00383026339362065\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06827539409531487\n",
      "Average test loss: 0.003506240143130223\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05858912115295728\n",
      "Average test loss: 0.003484463570846452\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0527791123688221\n",
      "Average test loss: 0.0031758173608945474\n",
      "Epoch 6/300\n",
      "Average training loss: 0.048964575876792274\n",
      "Average test loss: 0.002995781814886464\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04620395682255427\n",
      "Average test loss: 0.002885550354917844\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04407077740960651\n",
      "Average test loss: 0.00281418486767345\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04239770233962271\n",
      "Average test loss: 0.002692415338423517\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04085256228513188\n",
      "Average test loss: 0.002621237211343315\n",
      "Epoch 11/300\n",
      "Average training loss: 0.039576815406481426\n",
      "Average test loss: 0.0026588654135250383\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03846665990021494\n",
      "Average test loss: 0.0024696286304129495\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03729337242907948\n",
      "Average test loss: 0.0023934825230389834\n",
      "Epoch 14/300\n",
      "Average training loss: 0.036343704289860196\n",
      "Average test loss: 0.0022903681109762855\n",
      "Epoch 15/300\n",
      "Average training loss: 0.035438661318686275\n",
      "Average test loss: 0.002240506239959763\n",
      "Epoch 16/300\n",
      "Average training loss: 0.034588613579670587\n",
      "Average test loss: 0.0022776080631754466\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03391024418009652\n",
      "Average test loss: 0.0021325731430616645\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03320157766011026\n",
      "Average test loss: 0.002121942038130429\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03259613959987958\n",
      "Average test loss: 0.0020752347889873715\n",
      "Epoch 20/300\n",
      "Average training loss: 0.032019582657350436\n",
      "Average test loss: 0.002013118717767712\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03158541894290182\n",
      "Average test loss: 0.0020051725257395044\n",
      "Epoch 22/300\n",
      "Average training loss: 0.031178050867385336\n",
      "Average test loss: 0.001966130297837986\n",
      "Epoch 23/300\n",
      "Average training loss: 0.030795963103572528\n",
      "Average test loss: 0.002014339881793906\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03046613975862662\n",
      "Average test loss: 0.001947122324258089\n",
      "Epoch 25/300\n",
      "Average training loss: 0.030098362310065165\n",
      "Average test loss: 0.0019586385175999668\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0297702983253532\n",
      "Average test loss: 0.0019078543455236488\n",
      "Epoch 27/300\n",
      "Average training loss: 0.029545036216576896\n",
      "Average test loss: 0.0019433401551925474\n",
      "Epoch 28/300\n",
      "Average training loss: 0.029263244072596232\n",
      "Average test loss: 0.0018757780946584211\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02901892322136296\n",
      "Average test loss: 0.001848364652082738\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028801303401589394\n",
      "Average test loss: 0.0018555908127584392\n",
      "Epoch 31/300\n",
      "Average training loss: 0.028586885343823167\n",
      "Average test loss: 0.0018576265824958682\n",
      "Epoch 32/300\n",
      "Average training loss: 0.028339650644196406\n",
      "Average test loss: 0.0018430453375395802\n",
      "Epoch 33/300\n",
      "Average training loss: 0.028192771254314318\n",
      "Average test loss: 0.0018213412455386586\n",
      "Epoch 34/300\n",
      "Average training loss: 0.028004539928502506\n",
      "Average test loss: 0.0018110163667135768\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027787525047858558\n",
      "Average test loss: 0.0018262153583475285\n",
      "Epoch 36/300\n",
      "Average training loss: 0.027715911471181447\n",
      "Average test loss: 0.0018465467086579237\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02757511869735188\n",
      "Average test loss: 0.0017960761584755447\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02734349986579683\n",
      "Average test loss: 0.0017895223287244639\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02720714388291041\n",
      "Average test loss: 0.0018006473088430033\n",
      "Epoch 40/300\n",
      "Average training loss: 0.027129561051726342\n",
      "Average test loss: 0.0017930092082048456\n",
      "Epoch 41/300\n",
      "Average training loss: 0.026990422949194907\n",
      "Average test loss: 0.0017852387834961215\n",
      "Epoch 42/300\n",
      "Average training loss: 0.026844298664894368\n",
      "Average test loss: 0.0017672039372846485\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02667978867722882\n",
      "Average test loss: 0.0017623560798044006\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0265679815808932\n",
      "Average test loss: 0.001756542215330733\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02649652722146776\n",
      "Average test loss: 0.0017646472804869215\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02640190865430567\n",
      "Average test loss: 0.0017398331118747591\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02621553254789776\n",
      "Average test loss: 0.0017667543969841467\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0261000371525685\n",
      "Average test loss: 0.0017397381824751695\n",
      "Epoch 49/300\n",
      "Average training loss: 0.026075694766309526\n",
      "Average test loss: 0.001774078766297963\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025921416214770742\n",
      "Average test loss: 0.0017428492251783609\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025806189154585202\n",
      "Average test loss: 0.0017348888735804293\n",
      "Epoch 52/300\n",
      "Average training loss: 0.025775906645589406\n",
      "Average test loss: 0.0019144217827253872\n",
      "Epoch 53/300\n",
      "Average training loss: 0.025646417521768147\n",
      "Average test loss: 0.001738590300962743\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025555060446262358\n",
      "Average test loss: 0.0017548930009620056\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025429059172670045\n",
      "Average test loss: 0.0017466468529568778\n",
      "Epoch 56/300\n",
      "Average training loss: 0.025345893889665603\n",
      "Average test loss: 0.001737612202246156\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025249911958972613\n",
      "Average test loss: 0.0017361286882725028\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02517687270210849\n",
      "Average test loss: 0.0017276050748510493\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02510987789266639\n",
      "Average test loss: 0.0017788657966173358\n",
      "Epoch 60/300\n",
      "Average training loss: 0.025026050885518392\n",
      "Average test loss: 0.0017280198379109302\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024946150968472164\n",
      "Average test loss: 0.0017820433708321717\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02481761902074019\n",
      "Average test loss: 0.0017253289562132624\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02478853786819511\n",
      "Average test loss: 0.0017317191145072382\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024626149616307682\n",
      "Average test loss: 0.0017186072876469957\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024615555327799587\n",
      "Average test loss: 0.0017319373925113015\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024496277458137938\n",
      "Average test loss: 0.0017215938990314801\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024399146523740558\n",
      "Average test loss: 0.0017298056070382396\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02431462261080742\n",
      "Average test loss: 0.0017275874877555502\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024242038205266\n",
      "Average test loss: 0.0017488887371081446\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024188136191831695\n",
      "Average test loss: 0.0017492815488432017\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024088017774952782\n",
      "Average test loss: 0.001734008225457122\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024030087497499256\n",
      "Average test loss: 0.0017229361317844854\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02395889065331883\n",
      "Average test loss: 0.0017276108473953274\n",
      "Epoch 74/300\n",
      "Average training loss: 0.023839287102222443\n",
      "Average test loss: 0.001757501032927798\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023787742003798486\n",
      "Average test loss: 0.0017455012081190944\n",
      "Epoch 76/300\n",
      "Average training loss: 0.023747474817766085\n",
      "Average test loss: 0.0017991170289201868\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02363256523675389\n",
      "Average test loss: 0.0017227863113706311\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023561204153630468\n",
      "Average test loss: 0.0017429827604856756\n",
      "Epoch 79/300\n",
      "Average training loss: 0.023515250934494865\n",
      "Average test loss: 0.0017489014383819368\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023445068890849748\n",
      "Average test loss: 0.0017494491634166074\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023391735442810587\n",
      "Average test loss: 0.0017516352239375314\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023311356392171648\n",
      "Average test loss: 0.0017734731352991528\n",
      "Epoch 83/300\n",
      "Average training loss: 0.023232504056559668\n",
      "Average test loss: 0.0017462170322736104\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02316276728113492\n",
      "Average test loss: 0.0017753817046889\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023056319130791557\n",
      "Average test loss: 0.0017978245427625046\n",
      "Epoch 86/300\n",
      "Average training loss: 0.023072609007358552\n",
      "Average test loss: 0.0017305115935289197\n",
      "Epoch 87/300\n",
      "Average training loss: 0.023040160752005046\n",
      "Average test loss: 0.0017231508953910735\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022928937652044828\n",
      "Average test loss: 0.0017457819169180262\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022804082670145564\n",
      "Average test loss: 0.0018253050384422144\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022790372634927433\n",
      "Average test loss: 0.0017390657824774584\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022704007064302763\n",
      "Average test loss: 0.001731778773996565\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022635323660241232\n",
      "Average test loss: 0.0017373678667677774\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02261426302128368\n",
      "Average test loss: 0.0017295643456487191\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022544586559136708\n",
      "Average test loss: 0.0017410991624411609\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022471456254522005\n",
      "Average test loss: 0.0017852533782521884\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022435798459582858\n",
      "Average test loss: 0.0017532072528782818\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02241265678074625\n",
      "Average test loss: 0.001815517875779834\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02230206856628259\n",
      "Average test loss: 0.0017528041661199595\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02227793439063761\n",
      "Average test loss: 0.0018060780976795487\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022177889769275982\n",
      "Average test loss: 0.001795741170955201\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02207740845117304\n",
      "Average test loss: 0.0017795881740748883\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022106440618634225\n",
      "Average test loss: 0.0017700154650956393\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022079500360621347\n",
      "Average test loss: 0.0017668708285523786\n",
      "Epoch 104/300\n",
      "Average training loss: 0.021983768027689723\n",
      "Average test loss: 0.0017463380114899742\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02193891516327858\n",
      "Average test loss: 0.0018244633761545022\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021894465306566822\n",
      "Average test loss: 0.0017857608405045338\n",
      "Epoch 107/300\n",
      "Average training loss: 0.021821175182859105\n",
      "Average test loss: 0.0018111037777529823\n",
      "Epoch 108/300\n",
      "Average training loss: 0.021816659177343052\n",
      "Average test loss: 0.001852973318244848\n",
      "Epoch 109/300\n",
      "Average training loss: 0.021813282819257842\n",
      "Average test loss: 0.0017936848151601022\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021638932580749195\n",
      "Average test loss: 0.0018378521135697763\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0216328770402405\n",
      "Average test loss: 0.0017994491457939148\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02157205725709597\n",
      "Average test loss: 0.0017786880274199778\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021551277142432\n",
      "Average test loss: 0.0017800044788875513\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021513942764865027\n",
      "Average test loss: 0.0017917651479235953\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02144215455154578\n",
      "Average test loss: 0.0017722855819803146\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021362035094035996\n",
      "Average test loss: 0.0018160597610597808\n",
      "Epoch 117/300\n",
      "Average training loss: 0.021355896254380544\n",
      "Average test loss: 0.00184837657192515\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021314810641937784\n",
      "Average test loss: 0.0018118792625351084\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02126357317301962\n",
      "Average test loss: 0.0018110935234775145\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021220374771290355\n",
      "Average test loss: 0.0017782558687031269\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021185265894565316\n",
      "Average test loss: 0.0017880431207724743\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021089265839921104\n",
      "Average test loss: 0.0017908988332168924\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02107671581705411\n",
      "Average test loss: 0.001819279956113961\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02101940856211715\n",
      "Average test loss: 0.0017473816889234715\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02097116586069266\n",
      "Average test loss: 0.0017900233317373528\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02094569618337684\n",
      "Average test loss: 0.0017646143887605932\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02093535533050696\n",
      "Average test loss: 0.001774537926332818\n",
      "Epoch 128/300\n",
      "Average training loss: 0.020895095467567446\n",
      "Average test loss: 0.001905071291451653\n",
      "Epoch 129/300\n",
      "Average training loss: 0.020910822338528104\n",
      "Average test loss: 0.0017637041367383466\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02080696156124274\n",
      "Average test loss: 0.0019163641691621806\n",
      "Epoch 131/300\n",
      "Average training loss: 0.020763943433761596\n",
      "Average test loss: 0.00179663709199263\n",
      "Epoch 132/300\n",
      "Average training loss: 0.020768938718570603\n",
      "Average test loss: 0.0017651443988498715\n",
      "Epoch 133/300\n",
      "Average training loss: 0.020672726181646187\n",
      "Average test loss: 0.0018063782343847885\n",
      "Epoch 134/300\n",
      "Average training loss: 0.020694378945562576\n",
      "Average test loss: 0.001773919764906168\n",
      "Epoch 135/300\n",
      "Average training loss: 0.020590109772152372\n",
      "Average test loss: 0.001836298815906048\n",
      "Epoch 136/300\n",
      "Average training loss: 0.020579984793232548\n",
      "Average test loss: 0.001824389164439506\n",
      "Epoch 137/300\n",
      "Average training loss: 0.020546722774704297\n",
      "Average test loss: 0.0017884825720555252\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020541306518846086\n",
      "Average test loss: 0.0018051313302583165\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02043989133503702\n",
      "Average test loss: 0.0017737407482539614\n",
      "Epoch 140/300\n",
      "Average training loss: 0.020452929145760006\n",
      "Average test loss: 0.001825613684952259\n",
      "Epoch 141/300\n",
      "Average training loss: 0.020381494355698426\n",
      "Average test loss: 0.001792656736344927\n",
      "Epoch 142/300\n",
      "Average training loss: 0.020417139649391174\n",
      "Average test loss: 0.0018278204103310903\n",
      "Epoch 143/300\n",
      "Average training loss: 0.020356694837411246\n",
      "Average test loss: 0.001847510242420766\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020337020532952416\n",
      "Average test loss: 0.0018723794248782927\n",
      "Epoch 145/300\n",
      "Average training loss: 0.020318392189012633\n",
      "Average test loss: 0.0018057908908360534\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0202818397399452\n",
      "Average test loss: 0.0017845158477624258\n",
      "Epoch 147/300\n",
      "Average training loss: 0.020244844771921636\n",
      "Average test loss: 0.001847288716584444\n",
      "Epoch 148/300\n",
      "Average training loss: 0.020227838357289633\n",
      "Average test loss: 0.001838997714428438\n",
      "Epoch 149/300\n",
      "Average training loss: 0.020144189698828592\n",
      "Average test loss: 0.001793078074645665\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020121499586436484\n",
      "Average test loss: 0.0020324523150920867\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02016130884985129\n",
      "Average test loss: 0.0019421821186422475\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02008674986826049\n",
      "Average test loss: 0.0017817691730128394\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02000970754524072\n",
      "Average test loss: 0.0019521612682276302\n",
      "Epoch 154/300\n",
      "Average training loss: 0.020019968366457357\n",
      "Average test loss: 0.0018678966506073872\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019976468784941567\n",
      "Average test loss: 0.0018445491375815538\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01996340319679843\n",
      "Average test loss: 0.0018232566552857559\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01991213808622625\n",
      "Average test loss: 0.0018093437561765313\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019886947870254516\n",
      "Average test loss: 0.0018668913684992327\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019917838075094754\n",
      "Average test loss: 0.0018583389148116112\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019838750693533157\n",
      "Average test loss: 0.0018223469849261973\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019801064640283583\n",
      "Average test loss: 0.0018274451955738995\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019805221317542924\n",
      "Average test loss: 0.0018702490217983722\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01980974323219723\n",
      "Average test loss: 0.0018458750726034244\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019729582414031027\n",
      "Average test loss: 0.001824918603317605\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019703636109828948\n",
      "Average test loss: 0.001819846849785083\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01968512757619222\n",
      "Average test loss: 0.0018239680412742826\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019715047255158425\n",
      "Average test loss: 0.0018698374157150586\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019693248661028014\n",
      "Average test loss: 0.0018181936128530652\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019676046001414458\n",
      "Average test loss: 0.0018846888069270385\n",
      "Epoch 170/300\n",
      "Average training loss: 0.019554720603757434\n",
      "Average test loss: 0.001858531022651328\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0195839843104283\n",
      "Average test loss: 0.00182743722676403\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019499276058541404\n",
      "Average test loss: 0.0018269242945955031\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019506124324268764\n",
      "Average test loss: 0.0018270393675193192\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01952366018295288\n",
      "Average test loss: 0.0018669286222300595\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019480013267861473\n",
      "Average test loss: 0.001837022691240741\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019439866849945652\n",
      "Average test loss: 0.0018725180946704415\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019436444604562387\n",
      "Average test loss: 0.0018970467909756633\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019444370612502097\n",
      "Average test loss: 0.0018046529314791162\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019396535080340175\n",
      "Average test loss: 0.0018360830077694523\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019354117658403183\n",
      "Average test loss: 0.002806638802298241\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019343546738227208\n",
      "Average test loss: 0.001854399344469938\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019348380337158837\n",
      "Average test loss: 0.00198575660555313\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019274812555975383\n",
      "Average test loss: 0.0018597983641342984\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019274691602422133\n",
      "Average test loss: 0.0018798853779832523\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019273690129319825\n",
      "Average test loss: 0.0019156349268224504\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019333263422879908\n",
      "Average test loss: 0.0019203112261990706\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019245656478736137\n",
      "Average test loss: 0.0018460794045693344\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01913843326105012\n",
      "Average test loss: 0.0019342289362102747\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019156558212306763\n",
      "Average test loss: 0.0018995898661928045\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019187298385633364\n",
      "Average test loss: 0.0019380227414270241\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01911493126882447\n",
      "Average test loss: 0.0019012059149228864\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019102772831088966\n",
      "Average test loss: 0.0019130165189918545\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01910450831717915\n",
      "Average test loss: 0.00196839929641121\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019097696757978865\n",
      "Average test loss: 0.0018427165469361677\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019074468442135388\n",
      "Average test loss: 0.0020269288619359335\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019061452767915196\n",
      "Average test loss: 0.0018622353286999796\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01902045248117712\n",
      "Average test loss: 0.0018380060450484356\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01896605745371845\n",
      "Average test loss: 0.0018695461160192887\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01901862090660466\n",
      "Average test loss: 0.001917562099173665\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01892132336397966\n",
      "Average test loss: 0.00186663625917087\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01896838717824883\n",
      "Average test loss: 0.0018603960215631459\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01891323581089576\n",
      "Average test loss: 0.001941014536242518\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01892155863179101\n",
      "Average test loss: 0.0018927366040233109\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01889284216529793\n",
      "Average test loss: 0.0018883091006428004\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018918648070759242\n",
      "Average test loss: 0.0019203865122463968\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018866122163832186\n",
      "Average test loss: 0.001899308475562268\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01879725967016485\n",
      "Average test loss: 0.0018938281344663766\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018839762379725775\n",
      "Average test loss: 0.0018687845131175385\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018791679792933995\n",
      "Average test loss: 0.0019197118048452668\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018805829538239373\n",
      "Average test loss: 0.0018764226875371403\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018796370230615138\n",
      "Average test loss: 0.0019167574166009823\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018754113181597656\n",
      "Average test loss: 0.001976271543134418\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018742659368448787\n",
      "Average test loss: 0.0020271066034005746\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018752637919452454\n",
      "Average test loss: 0.0019109117571885388\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01870332699848546\n",
      "Average test loss: 0.0019040341362771061\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01872130579666959\n",
      "Average test loss: 0.002002701470835341\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01865640338924196\n",
      "Average test loss: 0.0018929978377289242\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01865531565083398\n",
      "Average test loss: 0.0018827663889775673\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018628266979422835\n",
      "Average test loss: 0.0019124448720572722\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018608687079615063\n",
      "Average test loss: 0.0018601796157244178\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018625886532995437\n",
      "Average test loss: 0.0019148504287004472\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01864299404952261\n",
      "Average test loss: 0.0019309099963348772\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018592655207547878\n",
      "Average test loss: 0.0019428367940708994\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018562144307626618\n",
      "Average test loss: 0.0019286727176772223\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018539695135421225\n",
      "Average test loss: 0.0018821440457055965\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018548361885878776\n",
      "Average test loss: 0.0018530593518581654\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018610441607733567\n",
      "Average test loss: 0.0019566978675623734\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018486692171129915\n",
      "Average test loss: 0.0019446463239275746\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018466037218769393\n",
      "Average test loss: 0.0018614436549444994\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018486357462075022\n",
      "Average test loss: 0.0018780483592094647\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018462988192836444\n",
      "Average test loss: 0.0018978495614396201\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018457404861019718\n",
      "Average test loss: 0.0019839387165589465\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018415953064958253\n",
      "Average test loss: 0.0019197627896856931\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018456191495888764\n",
      "Average test loss: 0.0019435813507686058\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018445843358834584\n",
      "Average test loss: 0.0018873995787774523\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018381205855144394\n",
      "Average test loss: 0.0019031186957533162\n",
      "Epoch 237/300\n",
      "Average training loss: 0.018424499066339597\n",
      "Average test loss: 0.0018921073596510623\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01839237827476528\n",
      "Average test loss: 0.0019522070582542155\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018366745640834174\n",
      "Average test loss: 0.0019334120688339073\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018375869808925523\n",
      "Average test loss: 0.0019134494883732663\n",
      "Epoch 241/300\n",
      "Average training loss: 0.018364729785256915\n",
      "Average test loss: 0.001886999440482921\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01829909763402409\n",
      "Average test loss: 0.001968198348147174\n",
      "Epoch 243/300\n",
      "Average training loss: 0.018305461916658612\n",
      "Average test loss: 0.0019374681438008945\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018322625348965327\n",
      "Average test loss: 0.0019011512141053875\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018282667674952083\n",
      "Average test loss: 0.0019365635230723355\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018264720287587907\n",
      "Average test loss: 0.0019034014298684068\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018275535991622342\n",
      "Average test loss: 0.0019476251751184464\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01826185916033056\n",
      "Average test loss: 0.001892302815698915\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018248571051491632\n",
      "Average test loss: 0.001857312004806267\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01825105697909991\n",
      "Average test loss: 0.001992468977967898\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018239224036534628\n",
      "Average test loss: 0.0019860147260543372\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018242053533593813\n",
      "Average test loss: 0.0018887262759316298\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01824045651240481\n",
      "Average test loss: 0.00202142101381388\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018175343164967165\n",
      "Average test loss: 0.0019713715653763048\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018161650258633824\n",
      "Average test loss: 0.0019231265319718256\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018186503756377433\n",
      "Average test loss: 0.0018939177624674307\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018196167790227466\n",
      "Average test loss: 0.0018998927725479007\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01809702514111996\n",
      "Average test loss: 0.001900968462539216\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018113693334990077\n",
      "Average test loss: 0.001940483316158255\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01813221707443396\n",
      "Average test loss: 0.001982483904291358\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018185441472464137\n",
      "Average test loss: 0.0019411911583609052\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018091804592145815\n",
      "Average test loss: 0.0018985967091802094\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018055928606126044\n",
      "Average test loss: 0.0019166605165228247\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0180781902058257\n",
      "Average test loss: 0.0018859724233754808\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018030440292424624\n",
      "Average test loss: 0.0019385868892487553\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018043702617287636\n",
      "Average test loss: 0.0019332207181594438\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018067492711875174\n",
      "Average test loss: 0.0019588502798643377\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01801067524817255\n",
      "Average test loss: 0.0019258537176582548\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01806639234556092\n",
      "Average test loss: 0.002026896729444464\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018014439814620548\n",
      "Average test loss: 0.0019339283059040706\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018000373053881857\n",
      "Average test loss: 0.0019165096990764142\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017984244480729102\n",
      "Average test loss: 0.0019778474835782417\n",
      "Epoch 273/300\n",
      "Average training loss: 0.017976725221508078\n",
      "Average test loss: 0.0019959641004809077\n",
      "Epoch 274/300\n",
      "Average training loss: 0.017963683053851126\n",
      "Average test loss: 0.001920960289426148\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017950733870267868\n",
      "Average test loss: 0.0019428669756485356\n",
      "Epoch 276/300\n",
      "Average training loss: 0.017975380973683463\n",
      "Average test loss: 0.0019523923383611772\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017919169391194978\n",
      "Average test loss: 0.001995449476564924\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017949775851435132\n",
      "Average test loss: 0.0018800924992602732\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017891197024120224\n",
      "Average test loss: 0.00192370640558915\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017907120510935782\n",
      "Average test loss: 0.0018666472254941862\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017916890383594567\n",
      "Average test loss: 0.0019277093397039506\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01792052160700162\n",
      "Average test loss: 0.0019472231394093897\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017915178784065778\n",
      "Average test loss: 0.0019468118585646154\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017900642532441353\n",
      "Average test loss: 0.0018946479826958643\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017831603705883027\n",
      "Average test loss: 0.001964570774593287\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017852417522006565\n",
      "Average test loss: 0.0018546298988577393\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017855956136352488\n",
      "Average test loss: 0.001988953330243627\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017876264289849335\n",
      "Average test loss: 0.0019357501004512111\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017833495173189374\n",
      "Average test loss: 0.0019158858988020154\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017803497696916262\n",
      "Average test loss: 0.0020948039252931872\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017772880472242833\n",
      "Average test loss: 0.00200659734217657\n",
      "Epoch 292/300\n",
      "Average training loss: 0.017768014113936158\n",
      "Average test loss: 0.0019957701600053245\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017820240348577498\n",
      "Average test loss: 0.001901296729635861\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017776265889406204\n",
      "Average test loss: 0.0019277865482080314\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01778089041594002\n",
      "Average test loss: 0.0019152211621403694\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0177702609292335\n",
      "Average test loss: 0.0019411762247069015\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017768635486563048\n",
      "Average test loss: 0.001989543488042222\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017747029089265398\n",
      "Average test loss: 0.0019463428382037414\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017727874343593915\n",
      "Average test loss: 0.0019572271210038\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01773698270486461\n",
      "Average test loss: 0.001927042258799904\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'DCT_32_Depth3/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.19\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.09\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.83\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.49\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.55\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.14\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.78\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 33.05\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.11\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.15\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.11\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.22\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.087573679394192\n",
      "Average test loss: 0.006398959897458554\n",
      "Epoch 2/300\n",
      "Average training loss: 1.4162166576915318\n",
      "Average test loss: 0.0053993851703902085\n",
      "Epoch 3/300\n",
      "Average training loss: 0.763440039952596\n",
      "Average test loss: 0.005135431492080291\n",
      "Epoch 4/300\n",
      "Average training loss: 0.5182973598374261\n",
      "Average test loss: 0.005120927290577028\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3885908721288045\n",
      "Average test loss: 0.00493100327004989\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3115955499675539\n",
      "Average test loss: 0.004860558043751451\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2576725203461117\n",
      "Average test loss: 0.004748975516607364\n",
      "Epoch 8/300\n",
      "Average training loss: 0.22050654575559828\n",
      "Average test loss: 0.004731450767152839\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1962039288414849\n",
      "Average test loss: 0.004697231823785437\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18003238530953725\n",
      "Average test loss: 0.004691159567485253\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1684196182092031\n",
      "Average test loss: 0.004638272555338012\n",
      "Epoch 12/300\n",
      "Average training loss: 0.15983043999142116\n",
      "Average test loss: 0.004578463800665405\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15397772645288044\n",
      "Average test loss: 0.00467024424754911\n",
      "Epoch 14/300\n",
      "Average training loss: 0.14967434225479762\n",
      "Average test loss: 0.004560116736839215\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1462711245086458\n",
      "Average test loss: 0.004542368355724547\n",
      "Epoch 16/300\n",
      "Average training loss: 0.14361073821120793\n",
      "Average test loss: 0.004493324405617184\n",
      "Epoch 17/300\n",
      "Average training loss: 0.14138172804647023\n",
      "Average test loss: 0.004491635185976823\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1397426451974445\n",
      "Average test loss: 0.004455675881356001\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13798009151220322\n",
      "Average test loss: 0.00445931120754944\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13686725081337822\n",
      "Average test loss: 0.004483079527608223\n",
      "Epoch 21/300\n",
      "Average training loss: 0.13563498363892237\n",
      "Average test loss: 0.004441906896730264\n",
      "Epoch 22/300\n",
      "Average training loss: 0.13464579195446438\n",
      "Average test loss: 0.004454091341131263\n",
      "Epoch 23/300\n",
      "Average training loss: 0.13369274018870458\n",
      "Average test loss: 0.004435836907890108\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1331472654276424\n",
      "Average test loss: 0.004469217840168211\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1323429622716374\n",
      "Average test loss: 0.004395644448490606\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13180440583493974\n",
      "Average test loss: 0.004393248550179932\n",
      "Epoch 27/300\n",
      "Average training loss: 0.13130019683970345\n",
      "Average test loss: 0.004365767132904794\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13073715638452105\n",
      "Average test loss: 0.004360314088149203\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13036770287487243\n",
      "Average test loss: 0.0043707921935452355\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13001859301328658\n",
      "Average test loss: 0.004352266324890984\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12964500688181982\n",
      "Average test loss: 0.004327595182591014\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12907343703508378\n",
      "Average test loss: 0.004316023323684931\n",
      "Epoch 33/300\n",
      "Average training loss: 0.128814586735434\n",
      "Average test loss: 0.004322609300414722\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12845631499422921\n",
      "Average test loss: 0.004383760596935948\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1280881490972307\n",
      "Average test loss: 0.0042709367705716025\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12776356167263456\n",
      "Average test loss: 0.004271632042609983\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12742100681198967\n",
      "Average test loss: 0.0043043639517078795\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12707865312364366\n",
      "Average test loss: 0.004274581746922599\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12687829202413559\n",
      "Average test loss: 0.004276467922247118\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12667096503575642\n",
      "Average test loss: 0.004270935583445761\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12621360088719263\n",
      "Average test loss: 0.0042785914525803595\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1259514595799976\n",
      "Average test loss: 0.0042556290783815915\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12575479021337296\n",
      "Average test loss: 0.004270217812309663\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12549127664168674\n",
      "Average test loss: 0.004275455171656278\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12526551655928295\n",
      "Average test loss: 0.004333531670893232\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1250103415052096\n",
      "Average test loss: 0.00427783806497852\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12482181643777424\n",
      "Average test loss: 0.004233835386112332\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1245590156449212\n",
      "Average test loss: 0.004230930636947353\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12441194681326548\n",
      "Average test loss: 0.004221387340169814\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12422666612598632\n",
      "Average test loss: 0.004243612522259355\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12392256314224667\n",
      "Average test loss: 0.004222471047606733\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12380073803663254\n",
      "Average test loss: 0.004239645675238636\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12370566608508428\n",
      "Average test loss: 0.004226843693190151\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12344080076615016\n",
      "Average test loss: 0.0042201333157718185\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1232765344646242\n",
      "Average test loss: 0.004222872919092575\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12321958712074492\n",
      "Average test loss: 0.0042635776595109035\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1229655192428165\n",
      "Average test loss: 0.004200972506983413\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12274328847063912\n",
      "Average test loss: 0.004243934376786152\n",
      "Epoch 59/300\n",
      "Average training loss: 0.12274975232283274\n",
      "Average test loss: 0.004209071790178617\n",
      "Epoch 60/300\n",
      "Average training loss: 0.1225585278140174\n",
      "Average test loss: 0.004220974331514703\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12230676137076484\n",
      "Average test loss: 0.00419976687990129\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12210739070177078\n",
      "Average test loss: 0.0042125399650798905\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12201674925618701\n",
      "Average test loss: 0.004210382777783606\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12186956910292307\n",
      "Average test loss: 0.004208094221022394\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12169609937402937\n",
      "Average test loss: 0.004290849616544115\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12161577402220831\n",
      "Average test loss: 0.0041817653216421604\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12140396933423148\n",
      "Average test loss: 0.004222833078354597\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12127624232901467\n",
      "Average test loss: 0.004179531648755073\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12106123354699877\n",
      "Average test loss: 0.00419669453592764\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12096137557427089\n",
      "Average test loss: 0.004230799754046732\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12088035413953993\n",
      "Average test loss: 0.004193069315618939\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1206658150686158\n",
      "Average test loss: 0.004216755826026201\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12068100121948454\n",
      "Average test loss: 0.004193891756650474\n",
      "Epoch 74/300\n",
      "Average training loss: 0.1203826003505124\n",
      "Average test loss: 0.004192090206262138\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12036402242713504\n",
      "Average test loss: 0.004196777128511005\n",
      "Epoch 76/300\n",
      "Average training loss: 0.12013848634560903\n",
      "Average test loss: 0.0041918743463853995\n",
      "Epoch 77/300\n",
      "Average training loss: 0.12002555272314283\n",
      "Average test loss: 0.004233652822673321\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1198949322104454\n",
      "Average test loss: 0.00422033397335973\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11963480610979928\n",
      "Average test loss: 0.004265865126003822\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11948698263698154\n",
      "Average test loss: 0.004177765151692762\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11942476269933912\n",
      "Average test loss: 0.004208963190308876\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11925201555093129\n",
      "Average test loss: 0.00421412076221572\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11910455965995789\n",
      "Average test loss: 0.004189649271571802\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11897910837332408\n",
      "Average test loss: 0.0041975432290799084\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11882770997285842\n",
      "Average test loss: 0.0042246256094012\n",
      "Epoch 86/300\n",
      "Average training loss: 0.1186966404583719\n",
      "Average test loss: 0.004203391129357947\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11852301307519277\n",
      "Average test loss: 0.004202380815727843\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11838795788420571\n",
      "Average test loss: 0.004189495911200841\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11816785538858837\n",
      "Average test loss: 0.0042105616260733865\n",
      "Epoch 90/300\n",
      "Average training loss: 0.1181032339864307\n",
      "Average test loss: 0.004253716593194339\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11790877250830333\n",
      "Average test loss: 0.004243994116783142\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11777717804246479\n",
      "Average test loss: 0.004225938151900967\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11764289564556546\n",
      "Average test loss: 0.004207312246163686\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11740984322627386\n",
      "Average test loss: 0.004212172159097261\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11736079125934178\n",
      "Average test loss: 0.004220451340286268\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11715271288818783\n",
      "Average test loss: 0.004264490141636795\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11693199175596238\n",
      "Average test loss: 0.004241544874384999\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1168949295282364\n",
      "Average test loss: 0.0042309468773504095\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11673872519863976\n",
      "Average test loss: 0.004217910964869791\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11640992771916919\n",
      "Average test loss: 0.004252841264423397\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1163415604962243\n",
      "Average test loss: 0.004235257859445281\n",
      "Epoch 102/300\n",
      "Average training loss: 0.11625175758865144\n",
      "Average test loss: 0.004259580856810013\n",
      "Epoch 103/300\n",
      "Average training loss: 0.11599863090117772\n",
      "Average test loss: 0.00423454047491153\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11579260571135415\n",
      "Average test loss: 0.004265515425966845\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11565294349193574\n",
      "Average test loss: 0.004258367110250725\n",
      "Epoch 106/300\n",
      "Average training loss: 0.1155534681280454\n",
      "Average test loss: 0.004286409113556147\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11553965122832192\n",
      "Average test loss: 0.004296669219103124\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11512548404932021\n",
      "Average test loss: 0.004214066149046024\n",
      "Epoch 109/300\n",
      "Average training loss: 0.1151303499672148\n",
      "Average test loss: 0.004261127477925685\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11481040015485551\n",
      "Average test loss: 0.004277435406214661\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11492028659582137\n",
      "Average test loss: 0.004224578005158239\n",
      "Epoch 112/300\n",
      "Average training loss: 0.11452967978848351\n",
      "Average test loss: 0.005127198465582398\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11445233234432009\n",
      "Average test loss: 0.004258367087278101\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11414967273341285\n",
      "Average test loss: 0.004256567489355803\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11400820055272844\n",
      "Average test loss: 0.004246864864395725\n",
      "Epoch 116/300\n",
      "Average training loss: 0.11397747364971372\n",
      "Average test loss: 0.004926919666843282\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1136952987048361\n",
      "Average test loss: 0.004238348642985026\n",
      "Epoch 118/300\n",
      "Average training loss: 0.11355697413285573\n",
      "Average test loss: 0.00427752034780052\n",
      "Epoch 119/300\n",
      "Average training loss: 0.1133274378048049\n",
      "Average test loss: 0.0044622428918050395\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11331161539422141\n",
      "Average test loss: 0.004280382785325249\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11313959112432267\n",
      "Average test loss: 0.004297241470466058\n",
      "Epoch 122/300\n",
      "Average training loss: 0.1129470552139812\n",
      "Average test loss: 0.004282614562660456\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11268969615300496\n",
      "Average test loss: 0.004307968020025227\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11252444412310918\n",
      "Average test loss: 0.004460491889466842\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11229489875502056\n",
      "Average test loss: 0.004319516291220983\n",
      "Epoch 126/300\n",
      "Average training loss: 0.11217748960521486\n",
      "Average test loss: 0.004341716807542576\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11220780848132239\n",
      "Average test loss: 0.004312739135904444\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1121147752404213\n",
      "Average test loss: 0.004263788688927889\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11177171630991829\n",
      "Average test loss: 0.004635485110597478\n",
      "Epoch 130/300\n",
      "Average training loss: 0.11159698802232743\n",
      "Average test loss: 0.004294156442913744\n",
      "Epoch 131/300\n",
      "Average training loss: 0.11141410565376282\n",
      "Average test loss: 0.0043957499609225325\n",
      "Epoch 132/300\n",
      "Average training loss: 0.11113302245404985\n",
      "Average test loss: 0.004397367996474107\n",
      "Epoch 133/300\n",
      "Average training loss: 0.1110857789516449\n",
      "Average test loss: 0.004362601407286193\n",
      "Epoch 134/300\n",
      "Average training loss: 0.1109074824584855\n",
      "Average test loss: 0.0043141804304387835\n",
      "Epoch 135/300\n",
      "Average training loss: 0.11071674431032605\n",
      "Average test loss: 0.004279358689569765\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11064520231882731\n",
      "Average test loss: 0.004305758302824364\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11051140384541618\n",
      "Average test loss: 0.004671034882052077\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11058205517795351\n",
      "Average test loss: 0.004411828888373243\n",
      "Epoch 139/300\n",
      "Average training loss: 0.11021869787904952\n",
      "Average test loss: 0.004449961295144425\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10988553988933564\n",
      "Average test loss: 0.004430808922482861\n",
      "Epoch 141/300\n",
      "Average training loss: 0.11008308564954333\n",
      "Average test loss: 0.004480668507516384\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1096691932214631\n",
      "Average test loss: 0.0043377330588797725\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10936113505893283\n",
      "Average test loss: 0.004391268021530575\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10910729951990976\n",
      "Average test loss: 0.004303807018117772\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1093115160200331\n",
      "Average test loss: 0.004490341956830687\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10894779745075438\n",
      "Average test loss: 0.004358697505460845\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10910669774479337\n",
      "Average test loss: 0.004428333444313871\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10867571018801796\n",
      "Average test loss: 0.004322393082082271\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10869488455851872\n",
      "Average test loss: 0.004388080557601319\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10850789203246435\n",
      "Average test loss: 0.004676442339188523\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10857987381352319\n",
      "Average test loss: 0.004417701224072112\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10811714037921694\n",
      "Average test loss: 0.004454238745073477\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10812801293532054\n",
      "Average test loss: 0.004333218975199594\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10799086127016279\n",
      "Average test loss: 0.004389945757885774\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10776166359583537\n",
      "Average test loss: 0.004398757226351234\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10771748931540383\n",
      "Average test loss: 0.004408308347480165\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10751607721381717\n",
      "Average test loss: 0.004372450658016735\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10730735428465737\n",
      "Average test loss: 0.004363280659541488\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10697691345214844\n",
      "Average test loss: 0.004362191706895828\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10699449362357458\n",
      "Average test loss: 0.004479275241080258\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10694288412729899\n",
      "Average test loss: 0.004437919191602204\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10684940387474166\n",
      "Average test loss: 0.004646179375549157\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10675063877635532\n",
      "Average test loss: 0.00442883632497655\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10667021958695518\n",
      "Average test loss: 0.004546832482640942\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10637074322832955\n",
      "Average test loss: 0.004461450510140922\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10647128860155741\n",
      "Average test loss: 0.004437358886210455\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1060313035051028\n",
      "Average test loss: 0.004640086811656753\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10591617404090034\n",
      "Average test loss: 0.00455643062127961\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1058885140882598\n",
      "Average test loss: 0.004366862660066949\n",
      "Epoch 170/300\n",
      "Average training loss: 0.10565568360355165\n",
      "Average test loss: 0.004507689004557\n",
      "Epoch 171/300\n",
      "Average training loss: 0.105653957426548\n",
      "Average test loss: 0.004425274839831723\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10530015467272864\n",
      "Average test loss: 0.004455382551170057\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10552145990398196\n",
      "Average test loss: 0.004379189080662198\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10506965208715863\n",
      "Average test loss: 0.004434435524046421\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10498307724793753\n",
      "Average test loss: 0.00446223083179858\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10525581680403816\n",
      "Average test loss: 0.004366173064957062\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10483527385526233\n",
      "Average test loss: 0.004437474492937327\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10483234926727084\n",
      "Average test loss: 0.00448502085937394\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10453113312191434\n",
      "Average test loss: 0.0045669624788893595\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10474992524252998\n",
      "Average test loss: 0.004397075802087784\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10444440189997355\n",
      "Average test loss: 0.00445709103345871\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10423205923371845\n",
      "Average test loss: 0.004514243157994416\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10392197344700495\n",
      "Average test loss: 0.004464727841110693\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10428603219985962\n",
      "Average test loss: 0.0045219825866321724\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10411591405338712\n",
      "Average test loss: 0.004465080134156678\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10377034956216812\n",
      "Average test loss: 0.004544548401816024\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1040241519411405\n",
      "Average test loss: 0.00448469513696101\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10359566851788098\n",
      "Average test loss: 0.004569848479496108\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10388721795876821\n",
      "Average test loss: 0.004430564539713992\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10324777204460568\n",
      "Average test loss: 0.004473308600485325\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10350888031058841\n",
      "Average test loss: 0.0045114332772791385\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1032574088441001\n",
      "Average test loss: 0.004487723810391294\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1031612877978219\n",
      "Average test loss: 0.00455091755092144\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10308829940027661\n",
      "Average test loss: 0.004493640690214104\n",
      "Epoch 195/300\n",
      "Average training loss: 0.10279687755637698\n",
      "Average test loss: 0.004615465294776691\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10306072934468587\n",
      "Average test loss: 0.00454913259897795\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10261945563554764\n",
      "Average test loss: 0.004765230885396401\n",
      "Epoch 198/300\n",
      "Average training loss: 0.1027335381772783\n",
      "Average test loss: 0.0045848678832666745\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10250466089778476\n",
      "Average test loss: 0.004599160693378912\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1022487641705407\n",
      "Average test loss: 0.00446634956739015\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10238545932372412\n",
      "Average test loss: 0.004464641506059302\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10223978469106886\n",
      "Average test loss: 0.004537436841469672\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10195980982648002\n",
      "Average test loss: 0.004431734819793039\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10214236199193531\n",
      "Average test loss: 0.004499532202879588\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10187692303789987\n",
      "Average test loss: 0.004554090722981427\n",
      "Epoch 206/300\n",
      "Average training loss: 0.10184053030278947\n",
      "Average test loss: 0.004548530646496349\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10186935142676036\n",
      "Average test loss: 0.004621363134019904\n",
      "Epoch 208/300\n",
      "Average training loss: 0.10179350086715486\n",
      "Average test loss: 0.004565898323638571\n",
      "Epoch 209/300\n",
      "Average training loss: 0.10145023233360714\n",
      "Average test loss: 0.00460569953173399\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10140233801470863\n",
      "Average test loss: 0.0045575971930391255\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10121781051158905\n",
      "Average test loss: 0.004585006879021724\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10121099981996748\n",
      "Average test loss: 0.0045383711738718885\n",
      "Epoch 213/300\n",
      "Average training loss: 0.10128321601284875\n",
      "Average test loss: 0.004581936378859812\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10117881930536694\n",
      "Average test loss: 0.004433200269523594\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10104222509596084\n",
      "Average test loss: 0.004538211123603914\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10079999783966276\n",
      "Average test loss: 0.004534680094156\n",
      "Epoch 217/300\n",
      "Average training loss: 0.10074930211570528\n",
      "Average test loss: 0.0045867558494210245\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10092598901192347\n",
      "Average test loss: 0.0044811882629162735\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10057318772210015\n",
      "Average test loss: 0.004670823709418377\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10052055431736841\n",
      "Average test loss: 0.004943050653156307\n",
      "Epoch 221/300\n",
      "Average training loss: 0.10034397234519322\n",
      "Average test loss: 0.0045653288910786315\n",
      "Epoch 222/300\n",
      "Average training loss: 0.10039170222812228\n",
      "Average test loss: 0.004554668046947982\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10042676527632607\n",
      "Average test loss: 0.004469494152400229\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10023254642883937\n",
      "Average test loss: 0.0044581279431780176\n",
      "Epoch 225/300\n",
      "Average training loss: 0.10010890710353851\n",
      "Average test loss: 0.0044682306990855266\n",
      "Epoch 226/300\n",
      "Average training loss: 0.10011147177219391\n",
      "Average test loss: 0.004546744588555561\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09985841839843326\n",
      "Average test loss: 0.004627828444871637\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10031852753294838\n",
      "Average test loss: 0.00455895044385559\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09976019361284044\n",
      "Average test loss: 0.004491724394261837\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09944391676452424\n",
      "Average test loss: 0.004648623837365045\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09973514523108801\n",
      "Average test loss: 0.004558631591084931\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09948138231039047\n",
      "Average test loss: 0.004592683064233926\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09945060333278444\n",
      "Average test loss: 0.004420956069810523\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09945266969336404\n",
      "Average test loss: 0.004641594099087848\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09946073901984427\n",
      "Average test loss: 0.0045859822930975095\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09910946518182755\n",
      "Average test loss: 0.0045300237865497665\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09955657258298661\n",
      "Average test loss: 0.0046506272395037945\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09909300598171022\n",
      "Average test loss: 0.004623012995347381\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09892785513401031\n",
      "Average test loss: 0.004544390249169535\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09896924348672231\n",
      "Average test loss: 0.004590411213537057\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09993797544638316\n",
      "Average test loss: 0.00471028869971633\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0990443224840694\n",
      "Average test loss: 0.00454315779503021\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09852827307250765\n",
      "Average test loss: 0.004511626622329156\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09836199411087566\n",
      "Average test loss: 0.0045723320589297345\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0984473876953125\n",
      "Average test loss: 0.004633412440203958\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09870662089851168\n",
      "Average test loss: 0.004541432895180252\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09836588155561023\n",
      "Average test loss: 0.00475924246923791\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09850451324383418\n",
      "Average test loss: 0.004649472763554917\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09839738102091683\n",
      "Average test loss: 0.004521355321423875\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09816011117564308\n",
      "Average test loss: 0.004507596499597033\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09800379893845983\n",
      "Average test loss: 0.004555655677285459\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09823533498578602\n",
      "Average test loss: 0.004585994922659463\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09810987586445279\n",
      "Average test loss: 0.004573531146264739\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09779197814729479\n",
      "Average test loss: 0.004528343395226532\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09804269740978877\n",
      "Average test loss: 0.00462530848549472\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09808409672313266\n",
      "Average test loss: 0.004458327990853124\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09757391966051525\n",
      "Average test loss: 0.0051118529749413335\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09805563047859404\n",
      "Average test loss: 0.004630905307001538\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09763157957130009\n",
      "Average test loss: 0.004888081725686788\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09742868787050248\n",
      "Average test loss: 0.004715444912927018\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09744964233371947\n",
      "Average test loss: 0.00459305970536338\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09736678369177712\n",
      "Average test loss: 0.004625383067876101\n",
      "Epoch 263/300\n",
      "Average training loss: 0.097354288968775\n",
      "Average test loss: 0.004551850033923984\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09716738953855303\n",
      "Average test loss: 0.0046636573132127525\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09785569950607088\n",
      "Average test loss: 0.004647162307053804\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09735612131489647\n",
      "Average test loss: 0.004574425184064442\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09727922930651241\n",
      "Average test loss: 0.004726393279516035\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09667761454317304\n",
      "Average test loss: 0.004637955357217127\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09709099698728985\n",
      "Average test loss: 0.004650823192877902\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0967619801958402\n",
      "Average test loss: 0.004612391423020098\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09674241367313596\n",
      "Average test loss: 0.00469397145551112\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09686684270037545\n",
      "Average test loss: 0.004660181211100684\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09666813627878824\n",
      "Average test loss: 0.004631703599459595\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09642747721076012\n",
      "Average test loss: 0.0046078964836067625\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09666826462745666\n",
      "Average test loss: 0.0045735334538751175\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09651543589433034\n",
      "Average test loss: 0.004532261947790782\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09651165105899175\n",
      "Average test loss: 0.004558557139295671\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09650641619496875\n",
      "Average test loss: 0.004624867350690894\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09660359267393748\n",
      "Average test loss: 0.004643127460860544\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09655180773470137\n",
      "Average test loss: 0.004744448317421807\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0961829800274637\n",
      "Average test loss: 0.004640119727700949\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09625986669129795\n",
      "Average test loss: 0.00464388660258717\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09604234678215451\n",
      "Average test loss: 0.004545395285305049\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09612728467914793\n",
      "Average test loss: 0.004756251270779305\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0960095253388087\n",
      "Average test loss: 0.004520305881069766\n",
      "Epoch 286/300\n",
      "Average training loss: 0.09601981380912993\n",
      "Average test loss: 0.004614372555580404\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09602950191497803\n",
      "Average test loss: 0.00465626242549883\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09581960533724891\n",
      "Average test loss: 0.004651043291307158\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09578999400801129\n",
      "Average test loss: 0.004679254679009318\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09568207137452231\n",
      "Average test loss: 0.004671129003581073\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09564519684182272\n",
      "Average test loss: 0.004667675088677142\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09563490488794114\n",
      "Average test loss: 0.004757590561070376\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09571049675014284\n",
      "Average test loss: 0.00466270333983832\n",
      "Epoch 294/300\n",
      "Average training loss: 0.09539991282754474\n",
      "Average test loss: 0.0049355641475154295\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09522406697935529\n",
      "Average test loss: 0.004574139556123151\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0956162042286661\n",
      "Average test loss: 0.004644155675752296\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09569898351695802\n",
      "Average test loss: 0.004581995634155141\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09482908486657672\n",
      "Average test loss: 0.004719063992301623\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0952960825363795\n",
      "Average test loss: 0.004495563235547807\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09505835712618298\n",
      "Average test loss: 0.004696372086803118\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.683212968084547\n",
      "Average test loss: 0.0065184454611606065\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2054908079571194\n",
      "Average test loss: 0.005186641551554203\n",
      "Epoch 3/300\n",
      "Average training loss: 0.627230110698276\n",
      "Average test loss: 0.004796693212663134\n",
      "Epoch 4/300\n",
      "Average training loss: 0.4165512197812398\n",
      "Average test loss: 0.004466702876405584\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3071477881007724\n",
      "Average test loss: 0.0043685852165023485\n",
      "Epoch 6/300\n",
      "Average training loss: 0.24348088849915397\n",
      "Average test loss: 0.0043519568439159126\n",
      "Epoch 7/300\n",
      "Average training loss: 0.20622638238800897\n",
      "Average test loss: 0.004297359565272927\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18255516101254357\n",
      "Average test loss: 0.004320617583683795\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1664210480981403\n",
      "Average test loss: 0.004075551391061809\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15428374998437033\n",
      "Average test loss: 0.003981087012216449\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14525431621074678\n",
      "Average test loss: 0.0038726681328068177\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13835091556443108\n",
      "Average test loss: 0.003847867485963636\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1329110776450899\n",
      "Average test loss: 0.0038040388599038123\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1283894399603208\n",
      "Average test loss: 0.003714847121801641\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12466480561097464\n",
      "Average test loss: 0.003677822918941577\n",
      "Epoch 16/300\n",
      "Average training loss: 0.12160629434718026\n",
      "Average test loss: 0.0036836166354931063\n",
      "Epoch 17/300\n",
      "Average training loss: 0.11872458260589176\n",
      "Average test loss: 0.003605325629727708\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11649548845158683\n",
      "Average test loss: 0.00361817548237741\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11436920126279194\n",
      "Average test loss: 0.003595688722613785\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11262003066804674\n",
      "Average test loss: 0.003500598295488291\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11095318967766232\n",
      "Average test loss: 0.0034813748660186927\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10961899609698189\n",
      "Average test loss: 0.00343491047144764\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10826784518692228\n",
      "Average test loss: 0.0034304725960310964\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10714659324619505\n",
      "Average test loss: 0.003478978507841627\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1060376124911838\n",
      "Average test loss: 0.0034461120820293823\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10497470704052184\n",
      "Average test loss: 0.003394329605417119\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10428363484144211\n",
      "Average test loss: 0.0033515858803358344\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10319345627890693\n",
      "Average test loss: 0.003311977473191089\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10227707256873449\n",
      "Average test loss: 0.0033147783589859803\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10168207867277992\n",
      "Average test loss: 0.003276646093568868\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10082321546806229\n",
      "Average test loss: 0.003299325099421872\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10008402280012767\n",
      "Average test loss: 0.0032888734572463565\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09930333200428221\n",
      "Average test loss: 0.003239077323012882\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0986763723426395\n",
      "Average test loss: 0.0032479940745979547\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09816609085930718\n",
      "Average test loss: 0.003213919420830078\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09733856424358156\n",
      "Average test loss: 0.003209212358420094\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09680952905284033\n",
      "Average test loss: 0.003214494321909216\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09627165210247039\n",
      "Average test loss: 0.003293541798988978\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09568008607625961\n",
      "Average test loss: 0.003203666026600533\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09539281556341384\n",
      "Average test loss: 0.003165999978573786\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09468470415804121\n",
      "Average test loss: 0.0031617150859286386\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09454563184579214\n",
      "Average test loss: 0.0031969846333894463\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09394496060079999\n",
      "Average test loss: 0.003176024618041184\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09366458705398771\n",
      "Average test loss: 0.0031646823531223667\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09316496012939347\n",
      "Average test loss: 0.003128853000079592\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09284142951832877\n",
      "Average test loss: 0.0031331583408431875\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09289274496833483\n",
      "Average test loss: 0.0031349819145268863\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09195805858903461\n",
      "Average test loss: 0.003139148004146086\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09173721641633245\n",
      "Average test loss: 0.0031346846119397215\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09135151197512945\n",
      "Average test loss: 0.0031016251428259744\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09100840708282258\n",
      "Average test loss: 0.0031231101589898266\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09072791212797165\n",
      "Average test loss: 0.0031255511680824887\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09047210833099154\n",
      "Average test loss: 0.0031447526030242445\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09005101066827774\n",
      "Average test loss: 0.003136035780318909\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08991176741321881\n",
      "Average test loss: 0.003090753652776281\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08947941090663274\n",
      "Average test loss: 0.0030958712816031445\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08930321886142095\n",
      "Average test loss: 0.003118312562712365\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08895104295015335\n",
      "Average test loss: 0.0031718074207504592\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0886417517264684\n",
      "Average test loss: 0.0030978609037895996\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08829314092795054\n",
      "Average test loss: 0.0030961078566809496\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08815520429776774\n",
      "Average test loss: 0.00309538549102015\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08772241053316328\n",
      "Average test loss: 0.003088502435427573\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08752185460262829\n",
      "Average test loss: 0.003085533944889903\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08714631519714991\n",
      "Average test loss: 0.0030847346184568274\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08685268086857266\n",
      "Average test loss: 0.0031816136000884904\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08669018712970945\n",
      "Average test loss: 0.0031007656074232524\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08624965094195472\n",
      "Average test loss: 0.003129925578004784\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08591053763363096\n",
      "Average test loss: 0.003095183701461388\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08559554485811127\n",
      "Average test loss: 0.0031117918705567717\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08541504277785619\n",
      "Average test loss: 0.0030757674783882167\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08514048347870509\n",
      "Average test loss: 0.003162049004808068\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08497596850660112\n",
      "Average test loss: 0.003127332102921274\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0845616558889548\n",
      "Average test loss: 0.003122024084544844\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08430446986688508\n",
      "Average test loss: 0.003082079152059224\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0839422260456615\n",
      "Average test loss: 0.003115730868238542\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08378896706634098\n",
      "Average test loss: 0.003079915308083097\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08345520904329087\n",
      "Average test loss: 0.003098880742573076\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08339194907744725\n",
      "Average test loss: 0.003119734979752037\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08301155764526791\n",
      "Average test loss: 0.003118002483414279\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08266321363051732\n",
      "Average test loss: 0.0031210295081966453\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08250792570577728\n",
      "Average test loss: 0.0031341928475432925\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08222687072224087\n",
      "Average test loss: 0.0031041691912752057\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08183914598822593\n",
      "Average test loss: 0.0031577833890914916\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0816429321302308\n",
      "Average test loss: 0.0030934063605964185\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08143305559621918\n",
      "Average test loss: 0.0031288703327170676\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08114912112885052\n",
      "Average test loss: 0.0032264291644096377\n",
      "Epoch 87/300\n",
      "Average training loss: 0.080920754565133\n",
      "Average test loss: 0.0031473535737022757\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0807725959122181\n",
      "Average test loss: 0.003287247327880727\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08037759451733695\n",
      "Average test loss: 0.003464529542459382\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08029706588718626\n",
      "Average test loss: 0.0031288387318038277\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07975191043814023\n",
      "Average test loss: 0.003141239506089025\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07988606525129742\n",
      "Average test loss: 0.0031819533637414375\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07949862071871758\n",
      "Average test loss: 0.0031741068044470414\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07920827817254596\n",
      "Average test loss: 0.0034457186895112198\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0790539130502277\n",
      "Average test loss: 0.003141261106150018\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07868077094356218\n",
      "Average test loss: 0.0031362981054310997\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07847325934966405\n",
      "Average test loss: 0.0031808000695374276\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07833604594402843\n",
      "Average test loss: 0.0031518158918867507\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0780724046362771\n",
      "Average test loss: 0.003144928762482272\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07786883982684877\n",
      "Average test loss: 0.0031715087640202706\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07756855273909039\n",
      "Average test loss: 0.0031783685259934927\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07734397619300419\n",
      "Average test loss: 0.0032205586104343334\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0773249793847402\n",
      "Average test loss: 0.0031714756122479836\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07704135043753518\n",
      "Average test loss: 0.0032286456250068215\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07669266261988215\n",
      "Average test loss: 0.0033609161731890506\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07667724630898899\n",
      "Average test loss: 0.00314470258106788\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07620756872826152\n",
      "Average test loss: 0.003173262050582303\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07615454649594094\n",
      "Average test loss: 0.0037960953582078217\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07595525197519197\n",
      "Average test loss: 0.003227784065736665\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07586628326773644\n",
      "Average test loss: 0.003340858434016506\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07561268985271453\n",
      "Average test loss: 0.003152904276115199\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07538076415326861\n",
      "Average test loss: 0.003203861455536551\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07504454459415542\n",
      "Average test loss: 0.0032599197557816903\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07500870668225819\n",
      "Average test loss: 0.003186678521335125\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07476675569680002\n",
      "Average test loss: 0.003242717154324055\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07462981568773587\n",
      "Average test loss: 0.00319730393961072\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07442356057961781\n",
      "Average test loss: 0.003218672034227186\n",
      "Epoch 118/300\n",
      "Average training loss: 0.074438041528066\n",
      "Average test loss: 0.004266078711797794\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07425033600462808\n",
      "Average test loss: 0.0032473711046493714\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07402591371867392\n",
      "Average test loss: 0.0032701446569214265\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0737829839322302\n",
      "Average test loss: 0.0032183176988942754\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07355192929506302\n",
      "Average test loss: 0.0032813870662616357\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0734182107216782\n",
      "Average test loss: 0.003198210559785366\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07304102576441235\n",
      "Average test loss: 0.0032317830165848136\n",
      "Epoch 125/300\n",
      "Average training loss: 0.073058566696114\n",
      "Average test loss: 0.0032712160913894576\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07296013266510433\n",
      "Average test loss: 0.0033451427368240225\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07267154009474648\n",
      "Average test loss: 0.003297018326405022\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07258305035034815\n",
      "Average test loss: 0.003214708571839664\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07249741209877862\n",
      "Average test loss: 0.0032848622120089\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07231284595198101\n",
      "Average test loss: 0.003269321360728807\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07225528214706314\n",
      "Average test loss: 0.0032418250348418953\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0718169932199849\n",
      "Average test loss: 0.0032715943112141556\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07179411777191692\n",
      "Average test loss: 0.0033052476530687678\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07176202305820253\n",
      "Average test loss: 0.0032694110570268497\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07143720085422198\n",
      "Average test loss: 0.0033498870402367577\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07127627310487959\n",
      "Average test loss: 0.0032729562752776675\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07134841873248418\n",
      "Average test loss: 0.0032611332742704287\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07107581826382213\n",
      "Average test loss: 0.0033447080053802996\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07104308507177565\n",
      "Average test loss: 0.0032551723081204625\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07086957877212101\n",
      "Average test loss: 0.00331080861762166\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07073812640375561\n",
      "Average test loss: 0.0032795663281447356\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07082283422350884\n",
      "Average test loss: 0.003414401238784194\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07044468951887554\n",
      "Average test loss: 0.0034118891735043792\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07039212548732758\n",
      "Average test loss: 0.0032724505006853075\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07003464769654803\n",
      "Average test loss: 0.003325688477605581\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07016933414671156\n",
      "Average test loss: 0.0032426851274859575\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06994804969761106\n",
      "Average test loss: 0.003277481050334043\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0698391327030129\n",
      "Average test loss: 0.0033094910230073663\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0697362705303563\n",
      "Average test loss: 0.0032780313729825947\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06961425184541278\n",
      "Average test loss: 0.0033361296966258024\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0693447886010011\n",
      "Average test loss: 0.003366641428735521\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06932125193542904\n",
      "Average test loss: 0.003338713616443177\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0691160255405638\n",
      "Average test loss: 0.003345703586108155\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06903363655010859\n",
      "Average test loss: 0.003294118793267343\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06893466207053926\n",
      "Average test loss: 0.0032749498116059436\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06891363697250684\n",
      "Average test loss: 0.003348212924475471\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06868641236093309\n",
      "Average test loss: 0.0033468551764057742\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06873162687155936\n",
      "Average test loss: 0.0033395103868097066\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06855591866374015\n",
      "Average test loss: 0.0035164375888804594\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06853094685739941\n",
      "Average test loss: 0.0033131237828897104\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06806775297721226\n",
      "Average test loss: 0.0033236562427547242\n",
      "Epoch 162/300\n",
      "Average training loss: 0.068215343710449\n",
      "Average test loss: 0.0033558477498590944\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06801692790455288\n",
      "Average test loss: 0.0032764206214083566\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06807844617300564\n",
      "Average test loss: 0.0033076420968605412\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06793824085262086\n",
      "Average test loss: 0.003308243543530504\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06766626812352075\n",
      "Average test loss: 0.0033383847545418476\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06784608112441169\n",
      "Average test loss: 0.0033633029527134367\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06751636570360925\n",
      "Average test loss: 0.003385999799809522\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06753183049294684\n",
      "Average test loss: 0.003325827861411704\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06759198819266425\n",
      "Average test loss: 0.003389915814416276\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06733689965142144\n",
      "Average test loss: 0.003319717125967145\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06732710864808825\n",
      "Average test loss: 0.0033323020432144404\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06717329289515814\n",
      "Average test loss: 0.0034767829887568952\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06701631721191936\n",
      "Average test loss: 0.0033830157638423974\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06699760417805778\n",
      "Average test loss: 0.003402765919557876\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06702611854672431\n",
      "Average test loss: 0.0034884954216993515\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06665246924426821\n",
      "Average test loss: 0.0034098408948630096\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06688477049271266\n",
      "Average test loss: 0.003283411487109131\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06658189319239723\n",
      "Average test loss: 0.0033118662914882104\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06644400552577442\n",
      "Average test loss: 0.0034753050843460693\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06621368563175202\n",
      "Average test loss: 0.0034141703988942837\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06636076019207636\n",
      "Average test loss: 0.0033591032777395514\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06629892992973328\n",
      "Average test loss: 0.003384295941847894\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06630323439174228\n",
      "Average test loss: 0.00336854268444909\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06610098097059462\n",
      "Average test loss: 0.0034923946551150744\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06604288152853648\n",
      "Average test loss: 0.0033678603315105042\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06600377069248094\n",
      "Average test loss: 0.003315731099082364\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06617891238795387\n",
      "Average test loss: 0.003389999147504568\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06573810218109025\n",
      "Average test loss: 0.003401040756661031\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06568954799572627\n",
      "Average test loss: 0.003421336894027061\n",
      "Epoch 191/300\n",
      "Average training loss: 0.065661830753088\n",
      "Average test loss: 0.003344478408081664\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06548057489593824\n",
      "Average test loss: 0.00343050478812721\n",
      "Epoch 193/300\n",
      "Average training loss: 0.065372546699312\n",
      "Average test loss: 0.003392360831093457\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06548237228724692\n",
      "Average test loss: 0.0033573066290054055\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06529607502288289\n",
      "Average test loss: 0.0034731348851281736\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06539186229308447\n",
      "Average test loss: 0.003402691278606653\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06523917337258657\n",
      "Average test loss: 0.0035381635460588665\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06511339061790042\n",
      "Average test loss: 0.003338965134281251\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06507111958993805\n",
      "Average test loss: 0.0033326698827246823\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06471756814585791\n",
      "Average test loss: 0.0034163347151544357\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06481777129570643\n",
      "Average test loss: 0.0034670057074270315\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06478853262795342\n",
      "Average test loss: 0.0034662333651342327\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06471225354406569\n",
      "Average test loss: 0.0034220274363954864\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06476675519678328\n",
      "Average test loss: 0.0033853032922165262\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0646187910967403\n",
      "Average test loss: 0.0033899138952708904\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06450520122051238\n",
      "Average test loss: 0.003398476185897986\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06442714177237617\n",
      "Average test loss: 0.0034062817436125545\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06442527896496984\n",
      "Average test loss: 0.003365538850219713\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06426850153009096\n",
      "Average test loss: 0.0034834648718436558\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06410316105352508\n",
      "Average test loss: 0.0034712720254643096\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0641424975030952\n",
      "Average test loss: 0.0035301976104577383\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06416418919960658\n",
      "Average test loss: 0.0033844933732309276\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06408801433112886\n",
      "Average test loss: 0.0036996515091094707\n",
      "Epoch 214/300\n",
      "Average training loss: 0.064215290400717\n",
      "Average test loss: 0.00356636178224451\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0640042732987139\n",
      "Average test loss: 0.0035123071488406925\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06371392661001947\n",
      "Average test loss: 0.0034414240285340282\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06386378465427292\n",
      "Average test loss: 0.0034994329354829257\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06372127648856905\n",
      "Average test loss: 0.003404254738241434\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06366029860244857\n",
      "Average test loss: 0.00342864284126295\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06382531556487084\n",
      "Average test loss: 0.0035100834597316054\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06344531166553498\n",
      "Average test loss: 0.003504337364393804\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06361203098297119\n",
      "Average test loss: 0.003382202425557706\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06344266129864587\n",
      "Average test loss: 0.0034835439136044846\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06337761966056293\n",
      "Average test loss: 0.003422308107631074\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06315937842263115\n",
      "Average test loss: 0.0033932894520047638\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0631478735241625\n",
      "Average test loss: 0.0034409726682222553\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06319160231616762\n",
      "Average test loss: 0.0033544294813440904\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0631769109831916\n",
      "Average test loss: 0.0034558180743414495\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06311352259582943\n",
      "Average test loss: 0.0033877410733451447\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06296043119827906\n",
      "Average test loss: 0.0034351980752415127\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06298899587492147\n",
      "Average test loss: 0.0034160467171006734\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06288516628742218\n",
      "Average test loss: 0.003495020473479397\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0628507179816564\n",
      "Average test loss: 0.003427328839484188\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06276748024423917\n",
      "Average test loss: 0.0035482967450386946\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06269717353251246\n",
      "Average test loss: 0.003519261216123899\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06265912445055113\n",
      "Average test loss: 0.003494746680888865\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06267285077108277\n",
      "Average test loss: 0.0035863977616859806\n",
      "Epoch 238/300\n",
      "Average training loss: 0.062419302930434545\n",
      "Average test loss: 0.003593785606117712\n",
      "Epoch 239/300\n",
      "Average training loss: 0.062485499448246426\n",
      "Average test loss: 0.003372505420611964\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06240945656100909\n",
      "Average test loss: 0.003398990038368437\n",
      "Epoch 241/300\n",
      "Average training loss: 0.062425653792089884\n",
      "Average test loss: 0.003479456582417091\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06227913528349664\n",
      "Average test loss: 0.0034671345961590607\n",
      "Epoch 243/300\n",
      "Average training loss: 0.062340352856450613\n",
      "Average test loss: 0.0035193754525648224\n",
      "Epoch 244/300\n",
      "Average training loss: 0.062251884626017676\n",
      "Average test loss: 0.0034355082447744077\n",
      "Epoch 245/300\n",
      "Average training loss: 0.062191779944631785\n",
      "Average test loss: 0.003422927056853142\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06209047803613875\n",
      "Average test loss: 0.0035124807227402926\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06206139896313349\n",
      "Average test loss: 0.005556664028929339\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06209228570924865\n",
      "Average test loss: 0.00342108382905523\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06184606275955836\n",
      "Average test loss: 0.003403721525437302\n",
      "Epoch 250/300\n",
      "Average training loss: 0.062047847165001765\n",
      "Average test loss: 0.0035270884162228967\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06188808621300591\n",
      "Average test loss: 0.003459187252033088\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06191097591320674\n",
      "Average test loss: 0.0035350355832941003\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06190354147553444\n",
      "Average test loss: 0.003501021613056461\n",
      "Epoch 254/300\n",
      "Average training loss: 0.061818574484851625\n",
      "Average test loss: 0.0034750165759275357\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06180756025513013\n",
      "Average test loss: 0.0034715060349553824\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06159536614351802\n",
      "Average test loss: 0.0034577837818198735\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06160745777686437\n",
      "Average test loss: 0.0034479934790482124\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06160077370206515\n",
      "Average test loss: 0.003429039979353547\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0614292700820499\n",
      "Average test loss: 0.003542895069345832\n",
      "Epoch 260/300\n",
      "Average training loss: 0.061409043974346586\n",
      "Average test loss: 0.003456771558150649\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06136178715030352\n",
      "Average test loss: 0.0034352579907410675\n",
      "Epoch 262/300\n",
      "Average training loss: 0.061310221049520705\n",
      "Average test loss: 0.0035456784601426788\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06138989661137263\n",
      "Average test loss: 0.0035070459875795575\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06131010170115365\n",
      "Average test loss: 0.0034825723382333916\n",
      "Epoch 265/300\n",
      "Average training loss: 0.061293861584530936\n",
      "Average test loss: 0.003492799753323197\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06124489657746421\n",
      "Average test loss: 0.00389943924629026\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0611372789144516\n",
      "Average test loss: 0.0034871832989156245\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06113265349136458\n",
      "Average test loss: 0.0034592475733823246\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06108601951599121\n",
      "Average test loss: 0.003470732461247179\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06112735396292474\n",
      "Average test loss: 0.0034163576198948755\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06102762236528927\n",
      "Average test loss: 0.00350825411039922\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06122446717156304\n",
      "Average test loss: 0.003469240021581451\n",
      "Epoch 273/300\n",
      "Average training loss: 0.060814406484365466\n",
      "Average test loss: 0.0034928015016226304\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06094325470262103\n",
      "Average test loss: 0.0034648422681623037\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06076049326525794\n",
      "Average test loss: 0.003509609550444616\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06071741960114903\n",
      "Average test loss: 0.0036719320433007347\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06088042844004101\n",
      "Average test loss: 0.003661506843235758\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06075696400139067\n",
      "Average test loss: 0.00347638602161573\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06069051522678799\n",
      "Average test loss: 0.003420238790826665\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06069859129190445\n",
      "Average test loss: 0.0034378217965778377\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06083217858937052\n",
      "Average test loss: 0.00349144056170351\n",
      "Epoch 282/300\n",
      "Average training loss: 0.060467985735999215\n",
      "Average test loss: 0.0035087483152747155\n",
      "Epoch 283/300\n",
      "Average training loss: 0.060437509467204414\n",
      "Average test loss: 0.0034674752385665973\n",
      "Epoch 284/300\n",
      "Average training loss: 0.060334894475009705\n",
      "Average test loss: 0.0037707577124238014\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06037508810228771\n",
      "Average test loss: 0.003555740817346507\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06041870114538404\n",
      "Average test loss: 0.003498792889010575\n",
      "Epoch 287/300\n",
      "Average training loss: 0.060358349677589206\n",
      "Average test loss: 0.0034985579941421746\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06016001076168484\n",
      "Average test loss: 0.003427021780361732\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06025207797023985\n",
      "Average test loss: 0.003473433625159992\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06004535567098194\n",
      "Average test loss: 0.003505850719494952\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06017731778158082\n",
      "Average test loss: 0.003435321126340164\n",
      "Epoch 292/300\n",
      "Average training loss: 0.060239680770370695\n",
      "Average test loss: 0.00347289046479596\n",
      "Epoch 293/300\n",
      "Average training loss: 0.060246727536122\n",
      "Average test loss: 0.0034777145783106487\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06003843493262927\n",
      "Average test loss: 0.0034691093323959243\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06010979473590851\n",
      "Average test loss: 0.003451966678102811\n",
      "Epoch 296/300\n",
      "Average training loss: 0.059921294245455003\n",
      "Average test loss: 0.003576869473275211\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06001425594753689\n",
      "Average test loss: 0.0034851359371095895\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05998849445581436\n",
      "Average test loss: 0.0034825910108370914\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05974696795807945\n",
      "Average test loss: 0.0034752748906612396\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05999507592784034\n",
      "Average test loss: 0.0035522106211218567\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.669083362579346\n",
      "Average test loss: 0.006486024440576632\n",
      "Epoch 2/300\n",
      "Average training loss: 0.973791842089759\n",
      "Average test loss: 0.005121151717172728\n",
      "Epoch 3/300\n",
      "Average training loss: 0.505132601737976\n",
      "Average test loss: 0.004659395185609659\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3321045218308767\n",
      "Average test loss: 0.004200573677404059\n",
      "Epoch 5/300\n",
      "Average training loss: 0.24474953954749637\n",
      "Average test loss: 0.0039643096580273575\n",
      "Epoch 6/300\n",
      "Average training loss: 0.19887249834007686\n",
      "Average test loss: 0.003862344566318724\n",
      "Epoch 7/300\n",
      "Average training loss: 0.17296752762794496\n",
      "Average test loss: 0.0041888883082817\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15588453274303013\n",
      "Average test loss: 0.0037387368985348276\n",
      "Epoch 9/300\n",
      "Average training loss: 0.143810085925791\n",
      "Average test loss: 0.0036719555511242814\n",
      "Epoch 10/300\n",
      "Average training loss: 0.13457027409474054\n",
      "Average test loss: 0.003591529393568635\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12736149587896134\n",
      "Average test loss: 0.0033979089266310135\n",
      "Epoch 12/300\n",
      "Average training loss: 0.12136704143550661\n",
      "Average test loss: 0.0033420171605216134\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11640764736466938\n",
      "Average test loss: 0.003325590643617842\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11208639875385497\n",
      "Average test loss: 0.003182111395522952\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10823173041476143\n",
      "Average test loss: 0.0031364956256002188\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10496544143226412\n",
      "Average test loss: 0.0032141871551672617\n",
      "Epoch 17/300\n",
      "Average training loss: 0.10214551376303037\n",
      "Average test loss: 0.0030411356487828824\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09980867318974601\n",
      "Average test loss: 0.002976096131735378\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09746763641966714\n",
      "Average test loss: 0.002931028516549203\n",
      "Epoch 20/300\n",
      "Average training loss: 0.09564716821577814\n",
      "Average test loss: 0.0028650559410452843\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09372214523620076\n",
      "Average test loss: 0.002838530519563291\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09208962708049351\n",
      "Average test loss: 0.002813833878065149\n",
      "Epoch 23/300\n",
      "Average training loss: 0.09070548712544971\n",
      "Average test loss: 0.002758588515015112\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08935930808054077\n",
      "Average test loss: 0.002765707977530029\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08806180574827724\n",
      "Average test loss: 0.002739738503057096\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0868402654197481\n",
      "Average test loss: 0.002660406544390652\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08579264779223336\n",
      "Average test loss: 0.00265216990850038\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08466061243746016\n",
      "Average test loss: 0.0026736459536477923\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08368331343597836\n",
      "Average test loss: 0.002724245954511894\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08273292605082194\n",
      "Average test loss: 0.002677815509960055\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08193763544162115\n",
      "Average test loss: 0.0026007858828331035\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08094152814480994\n",
      "Average test loss: 0.0025885899398061965\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08040554080406825\n",
      "Average test loss: 0.0026009445074531767\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07957327437731955\n",
      "Average test loss: 0.0025849909087022147\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07888152205944061\n",
      "Average test loss: 0.0025942045110795234\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07827846931086646\n",
      "Average test loss: 0.0025939293443742726\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07783966869778103\n",
      "Average test loss: 0.002558513593963451\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07723853144049644\n",
      "Average test loss: 0.0025244419997971917\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07651598351531559\n",
      "Average test loss: 0.002522774318854014\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07606919897927178\n",
      "Average test loss: 0.0025432226763417326\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07548941366208924\n",
      "Average test loss: 0.0025381747992295357\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07510752161343892\n",
      "Average test loss: 0.0024937505568895075\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07466274895270665\n",
      "Average test loss: 0.002526697904078497\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07454350405931473\n",
      "Average test loss: 0.002509050141192145\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07371381284793219\n",
      "Average test loss: 0.0024612156295528015\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07339075152079265\n",
      "Average test loss: 0.0025082335689415535\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07294041572014491\n",
      "Average test loss: 0.002492772827959723\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07271445101168421\n",
      "Average test loss: 0.002467391044832766\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07223597475886345\n",
      "Average test loss: 0.0024827058665040464\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07198536556959152\n",
      "Average test loss: 0.0024653416278047693\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07146313692463768\n",
      "Average test loss: 0.002465547439745731\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07112088144487805\n",
      "Average test loss: 0.002445408878227075\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07095022001862526\n",
      "Average test loss: 0.002438907140969402\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07054611804419093\n",
      "Average test loss: 0.0024369999949509897\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0700716001590093\n",
      "Average test loss: 0.002480533859692514\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06991533039013545\n",
      "Average test loss: 0.0024774881809846395\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06955393614371617\n",
      "Average test loss: 0.002424964140479763\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06920587217145496\n",
      "Average test loss: 0.002422652079827256\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06902509056197273\n",
      "Average test loss: 0.00245546690447049\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06868084508180618\n",
      "Average test loss: 0.002423759677343898\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06812683573034074\n",
      "Average test loss: 0.002428725077253249\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06797749619351492\n",
      "Average test loss: 0.0024345378530108265\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06772066789865494\n",
      "Average test loss: 0.0024878187500354315\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06744420516159799\n",
      "Average test loss: 0.002426477479748428\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0670529946949747\n",
      "Average test loss: 0.0024359044200844234\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06679369225766923\n",
      "Average test loss: 0.0024230289520281883\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0664938076933225\n",
      "Average test loss: 0.002517076585648788\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06645202176438438\n",
      "Average test loss: 0.002446072498129474\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0660530955294768\n",
      "Average test loss: 0.0025103653309245903\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06570811363723543\n",
      "Average test loss: 0.0024657102398988272\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06543909408317672\n",
      "Average test loss: 0.0024722626221676666\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06512671260701286\n",
      "Average test loss: 0.00243688141544246\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06479843156867557\n",
      "Average test loss: 0.0024475681293341846\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06456771452228229\n",
      "Average test loss: 0.0024328495804220437\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06414720820718342\n",
      "Average test loss: 0.0024499663605044284\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06417148468560643\n",
      "Average test loss: 0.0024706697058346537\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06390833271543185\n",
      "Average test loss: 0.002536809535490142\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06371669554048115\n",
      "Average test loss: 0.002438900668380989\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06323967358469963\n",
      "Average test loss: 0.0024294598752425778\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0629884394440386\n",
      "Average test loss: 0.0024563505061798627\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0627835341360834\n",
      "Average test loss: 0.0024509246783951917\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06262395192186038\n",
      "Average test loss: 0.002466372839278645\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06231339221861627\n",
      "Average test loss: 0.002443669793092542\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06205493100484212\n",
      "Average test loss: 0.002431830940147241\n",
      "Epoch 85/300\n",
      "Average training loss: 0.061808391551176706\n",
      "Average test loss: 0.0024881904926151036\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06170625446240107\n",
      "Average test loss: 0.0024925813507288693\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0612618907392025\n",
      "Average test loss: 0.0024987426853428285\n",
      "Epoch 88/300\n",
      "Average training loss: 0.061180215679936936\n",
      "Average test loss: 0.002451550314202905\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06078593808743689\n",
      "Average test loss: 0.0025474723981072506\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06056331600414382\n",
      "Average test loss: 0.0024754152624971337\n",
      "Epoch 91/300\n",
      "Average training loss: 0.060430447008874684\n",
      "Average test loss: 0.0024889278261818817\n",
      "Epoch 92/300\n",
      "Average training loss: 0.060238272640440196\n",
      "Average test loss: 0.0026916123028430674\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05989737896455659\n",
      "Average test loss: 0.0024666049608753787\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05970579085747401\n",
      "Average test loss: 0.002476047436396281\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05955739340186119\n",
      "Average test loss: 0.0024845817560950915\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05911201839976841\n",
      "Average test loss: 0.0025587694628371135\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05906229009562069\n",
      "Average test loss: 0.002474105624895957\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05893684912059042\n",
      "Average test loss: 0.0024778687600046398\n",
      "Epoch 99/300\n",
      "Average training loss: 0.058630264659722646\n",
      "Average test loss: 0.0024874861681212983\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05846305066015985\n",
      "Average test loss: 0.0024729224604864916\n",
      "Epoch 101/300\n",
      "Average training loss: 0.058344160394536125\n",
      "Average test loss: 0.0024848160518126357\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05799875172641542\n",
      "Average test loss: 0.0025270593696170383\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05778864954908689\n",
      "Average test loss: 0.002502959962313374\n",
      "Epoch 104/300\n",
      "Average training loss: 0.057717076013485594\n",
      "Average test loss: 0.0025242093563493757\n",
      "Epoch 105/300\n",
      "Average training loss: 0.057439886997143426\n",
      "Average test loss: 0.00256440145874189\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05736379410492049\n",
      "Average test loss: 0.0024900602698326112\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05721974525186751\n",
      "Average test loss: 0.0025722781237628727\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05705435791942808\n",
      "Average test loss: 0.0025233265903467934\n",
      "Epoch 109/300\n",
      "Average training loss: 0.056618301077021496\n",
      "Average test loss: 0.0026120422035455703\n",
      "Epoch 110/300\n",
      "Average training loss: 0.056610355546077094\n",
      "Average test loss: 0.002518345924715201\n",
      "Epoch 111/300\n",
      "Average training loss: 0.056595395866367554\n",
      "Average test loss: 0.0026283001037728454\n",
      "Epoch 112/300\n",
      "Average training loss: 0.056330001602570214\n",
      "Average test loss: 0.0025163379001120726\n",
      "Epoch 113/300\n",
      "Average training loss: 0.056197761363453336\n",
      "Average test loss: 0.0025063411533418627\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05593158702055613\n",
      "Average test loss: 0.0025168338211046326\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05574539507097668\n",
      "Average test loss: 0.0025343125340425306\n",
      "Epoch 116/300\n",
      "Average training loss: 0.055568049281835556\n",
      "Average test loss: 0.0025325684669531053\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05556722178724077\n",
      "Average test loss: 0.002572939924067921\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05559863085548083\n",
      "Average test loss: 0.002541845979169011\n",
      "Epoch 119/300\n",
      "Average training loss: 0.055183202938901055\n",
      "Average test loss: 0.0025315591959903638\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05498476326796744\n",
      "Average test loss: 0.002490444393000669\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05476687898569637\n",
      "Average test loss: 0.002533706967201498\n",
      "Epoch 122/300\n",
      "Average training loss: 0.054681031958924396\n",
      "Average test loss: 0.0025446521643963126\n",
      "Epoch 123/300\n",
      "Average training loss: 0.054548511726988685\n",
      "Average test loss: 0.0026381230894476177\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05449278224839105\n",
      "Average test loss: 0.002524773164962729\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05439857444498274\n",
      "Average test loss: 0.0032090485476785236\n",
      "Epoch 126/300\n",
      "Average training loss: 0.054226920770274266\n",
      "Average test loss: 0.0026194590586755013\n",
      "Epoch 127/300\n",
      "Average training loss: 0.054153314501047135\n",
      "Average test loss: 0.0025141180781647564\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05400536508361498\n",
      "Average test loss: 0.002607532701972458\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05373317923810747\n",
      "Average test loss: 0.002558868618061145\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05371028713219696\n",
      "Average test loss: 0.0025882759253597924\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05353246105048391\n",
      "Average test loss: 0.0026286317960669595\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05361769910984569\n",
      "Average test loss: 0.002568501760562261\n",
      "Epoch 133/300\n",
      "Average training loss: 0.053287000659439296\n",
      "Average test loss: 0.002553367663588789\n",
      "Epoch 134/300\n",
      "Average training loss: 0.053086543838183085\n",
      "Average test loss: 0.0025306065403338934\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05299273815419939\n",
      "Average test loss: 0.0026179929675741328\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05278167266978158\n",
      "Average test loss: 0.002573844194205271\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05262110736966133\n",
      "Average test loss: 0.0025841299212641185\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05267035807503594\n",
      "Average test loss: 0.0025630129153529804\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05270385597480668\n",
      "Average test loss: 0.002618126779794693\n",
      "Epoch 140/300\n",
      "Average training loss: 0.052275821301672194\n",
      "Average test loss: 0.0025504337257395188\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05235065412852499\n",
      "Average test loss: 0.002567576111190849\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05240071990092596\n",
      "Average test loss: 0.002599423868995574\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05209984930687481\n",
      "Average test loss: 0.00262947604412006\n",
      "Epoch 144/300\n",
      "Average training loss: 0.052036223868529005\n",
      "Average test loss: 0.0025691370524259078\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05186950586239497\n",
      "Average test loss: 0.0025937398448586464\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05173939830395911\n",
      "Average test loss: 0.002571369940415025\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05177011574639214\n",
      "Average test loss: 0.002606880701871382\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0514855018986596\n",
      "Average test loss: 0.00259234841250711\n",
      "Epoch 149/300\n",
      "Average training loss: 0.051669941521353194\n",
      "Average test loss: 0.00257992251858943\n",
      "Epoch 150/300\n",
      "Average training loss: 0.051300894657770796\n",
      "Average test loss: 0.0026341109497265685\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05127929649419255\n",
      "Average test loss: 0.002611107982074221\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05117612802320057\n",
      "Average test loss: 0.0025936852501084406\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05101756347881423\n",
      "Average test loss: 0.0027535849016987616\n",
      "Epoch 154/300\n",
      "Average training loss: 0.050893597904178835\n",
      "Average test loss: 0.0026572279036045075\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05085266393091944\n",
      "Average test loss: 0.002599059787889322\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0508226004242897\n",
      "Average test loss: 0.0025726845080239905\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050662984258598755\n",
      "Average test loss: 0.002828350275961889\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05054127230577999\n",
      "Average test loss: 0.002683994090805451\n",
      "Epoch 159/300\n",
      "Average training loss: 0.050475213544236286\n",
      "Average test loss: 0.002619371383761366\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05044806347290675\n",
      "Average test loss: 0.002607175528175301\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05030496374103758\n",
      "Average test loss: 0.0026768665802147655\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05029206195473671\n",
      "Average test loss: 0.002635099041689601\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05019370475742552\n",
      "Average test loss: 0.002588617076786856\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05005519292751948\n",
      "Average test loss: 0.002698216724106007\n",
      "Epoch 165/300\n",
      "Average training loss: 0.050140153292152614\n",
      "Average test loss: 0.0026570910325066912\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04995656151904\n",
      "Average test loss: 0.0026250369192825423\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04977142090929879\n",
      "Average test loss: 0.002649225895189577\n",
      "Epoch 168/300\n",
      "Average training loss: 0.049749386294020544\n",
      "Average test loss: 0.0026534027121961116\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04947206728988224\n",
      "Average test loss: 0.0027148359343409536\n",
      "Epoch 170/300\n",
      "Average training loss: 0.049645509544346066\n",
      "Average test loss: 0.002676706948938469\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04947867510219415\n",
      "Average test loss: 0.002650885432337721\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04952511085073153\n",
      "Average test loss: 0.00261732882178492\n",
      "Epoch 173/300\n",
      "Average training loss: 0.049350430240233736\n",
      "Average test loss: 0.002754289981391695\n",
      "Epoch 174/300\n",
      "Average training loss: 0.049200564589765336\n",
      "Average test loss: 0.0027093008584860296\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04921717249353727\n",
      "Average test loss: 0.002698507081717253\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049079535017410914\n",
      "Average test loss: 0.0026370887108561067\n",
      "Epoch 177/300\n",
      "Average training loss: 0.049026272346576055\n",
      "Average test loss: 0.002687075557601121\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04900321410761939\n",
      "Average test loss: 0.00262652201205492\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04903360660208596\n",
      "Average test loss: 0.0027096535644183556\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04881831227739652\n",
      "Average test loss: 0.0026658446194810998\n",
      "Epoch 181/300\n",
      "Average training loss: 0.048650899479786555\n",
      "Average test loss: 0.0026316998668221964\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0488239749206437\n",
      "Average test loss: 0.002655172136508756\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04867543294032415\n",
      "Average test loss: 0.0027385736354109316\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0485076763232549\n",
      "Average test loss: 0.002639743045800262\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04836260058151351\n",
      "Average test loss: 0.0027084541183172\n",
      "Epoch 186/300\n",
      "Average training loss: 0.048374492039283117\n",
      "Average test loss: 0.0025988905786847073\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04840926757786009\n",
      "Average test loss: 0.00262071800014625\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04824695592456394\n",
      "Average test loss: 0.002754424494173792\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0483893863691224\n",
      "Average test loss: 0.0026489890557196403\n",
      "Epoch 190/300\n",
      "Average training loss: 0.048154984480804866\n",
      "Average test loss: 0.002678338393362032\n",
      "Epoch 191/300\n",
      "Average training loss: 0.047942411210801866\n",
      "Average test loss: 0.0029648734538091554\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04815942976540989\n",
      "Average test loss: 0.0026262089850174054\n",
      "Epoch 193/300\n",
      "Average training loss: 0.048029533127943676\n",
      "Average test loss: 0.00265740187631713\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04805106407072809\n",
      "Average test loss: 0.0026635320540517567\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0478249505062898\n",
      "Average test loss: 0.0026416535569975773\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0476539122859637\n",
      "Average test loss: 0.0027654970377269717\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04771166182557742\n",
      "Average test loss: 0.0026554437915070187\n",
      "Epoch 198/300\n",
      "Average training loss: 0.047795046223534476\n",
      "Average test loss: 0.0026735893988774884\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04763940754532814\n",
      "Average test loss: 0.0026749566027687655\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04752438139584329\n",
      "Average test loss: 0.0028787436299026014\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0474196079770724\n",
      "Average test loss: 0.0026824917271733285\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04754579065905677\n",
      "Average test loss: 0.0027516187250407205\n",
      "Epoch 203/300\n",
      "Average training loss: 0.047657595270209845\n",
      "Average test loss: 0.002663813037590848\n",
      "Epoch 204/300\n",
      "Average training loss: 0.047222287038962044\n",
      "Average test loss: 0.0026290151935277712\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04716139749354786\n",
      "Average test loss: 0.0026903381830909187\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04721544080310398\n",
      "Average test loss: 0.0026893206658876604\n",
      "Epoch 207/300\n",
      "Average training loss: 0.047292026860846416\n",
      "Average test loss: 0.002695966070310937\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04710726460814476\n",
      "Average test loss: 0.0028069641625301706\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04722871582210064\n",
      "Average test loss: 0.002759637828916311\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04692557275957531\n",
      "Average test loss: 0.002666105046661364\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04699525479806794\n",
      "Average test loss: 0.0027308864469329517\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04688289221127828\n",
      "Average test loss: 0.0026770848840889003\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04691535093055831\n",
      "Average test loss: 0.0027376611123068467\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04673039749264717\n",
      "Average test loss: 0.0026438069043474067\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04691823627551397\n",
      "Average test loss: 0.00277970418230527\n",
      "Epoch 216/300\n",
      "Average training loss: 0.046787638780143526\n",
      "Average test loss: 0.002745992355255617\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04668866344789664\n",
      "Average test loss: 0.0027736506342060036\n",
      "Epoch 218/300\n",
      "Average training loss: 0.046587117082542845\n",
      "Average test loss: 0.0027841211590501997\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04653676101234224\n",
      "Average test loss: 0.0028000407724951704\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04657009010844761\n",
      "Average test loss: 0.0027950893061028585\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04649057661824756\n",
      "Average test loss: 0.002679757001395855\n",
      "Epoch 222/300\n",
      "Average training loss: 0.046380840609471\n",
      "Average test loss: 0.0031367664293696484\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04630355322692129\n",
      "Average test loss: 0.00276659385404653\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04632075882951418\n",
      "Average test loss: 0.00267245307057682\n",
      "Epoch 225/300\n",
      "Average training loss: 0.046213001588980354\n",
      "Average test loss: 0.0027831638724439673\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04619431120157242\n",
      "Average test loss: 0.0028092793062743215\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04619954816831483\n",
      "Average test loss: 0.0026718082796368336\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04604588569535149\n",
      "Average test loss: 0.002980055102457603\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04637389894326528\n",
      "Average test loss: 0.0026929127863711782\n",
      "Epoch 230/300\n",
      "Average training loss: 0.045873406168487334\n",
      "Average test loss: 0.002732191051873896\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04594964895149072\n",
      "Average test loss: 0.0028152569178491833\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04592008521821764\n",
      "Average test loss: 0.00271660079061985\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04585943380660481\n",
      "Average test loss: 0.002737683160851399\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04583675343791644\n",
      "Average test loss: 0.0027270392666881283\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04589863192703989\n",
      "Average test loss: 0.0027730152010917663\n",
      "Epoch 236/300\n",
      "Average training loss: 0.045725928233729465\n",
      "Average test loss: 0.002736686724652019\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04577982063094775\n",
      "Average test loss: 0.0026836863023539386\n",
      "Epoch 238/300\n",
      "Average training loss: 0.045703768253326414\n",
      "Average test loss: 0.0027201936536779005\n",
      "Epoch 239/300\n",
      "Average training loss: 0.045620995425515704\n",
      "Average test loss: 0.002701626467828949\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04555044839117262\n",
      "Average test loss: 0.0028096754343973265\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04557171454363399\n",
      "Average test loss: 0.002767839398442043\n",
      "Epoch 242/300\n",
      "Average training loss: 0.045387886573870974\n",
      "Average test loss: 0.0026887733466509317\n",
      "Epoch 243/300\n",
      "Average training loss: 0.045481904870933954\n",
      "Average test loss: 0.002748752572677202\n",
      "Epoch 244/300\n",
      "Average training loss: 0.045507523155874675\n",
      "Average test loss: 0.0026741116618116696\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04540494260523054\n",
      "Average test loss: 0.0027369818857146634\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04532977718114853\n",
      "Average test loss: 0.00285503241357704\n",
      "Epoch 247/300\n",
      "Average training loss: 0.045384243786334995\n",
      "Average test loss: 0.0027072137194789118\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04527622915142112\n",
      "Average test loss: 0.0027163149770349265\n",
      "Epoch 249/300\n",
      "Average training loss: 0.045196867913007734\n",
      "Average test loss: 0.002699250359709064\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04521063698993789\n",
      "Average test loss: 0.002681863246899512\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04514092511600918\n",
      "Average test loss: 0.002732777959977587\n",
      "Epoch 252/300\n",
      "Average training loss: 0.045064781384335624\n",
      "Average test loss: 0.0028525128602567645\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04502044008837806\n",
      "Average test loss: 0.002788611053385668\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04515277641018232\n",
      "Average test loss: 0.002690601043506629\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04498536808623208\n",
      "Average test loss: 0.002757842342472739\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04494592826565107\n",
      "Average test loss: 0.002794997453689575\n",
      "Epoch 257/300\n",
      "Average training loss: 0.045019818401998944\n",
      "Average test loss: 0.002794033808633685\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04489739686912961\n",
      "Average test loss: 0.0027823544225345056\n",
      "Epoch 259/300\n",
      "Average training loss: 0.044806361969974305\n",
      "Average test loss: 0.002733194770084487\n",
      "Epoch 260/300\n",
      "Average training loss: 0.044873735308647154\n",
      "Average test loss: 0.0027159495806942384\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04477275751365556\n",
      "Average test loss: 0.0026877028385384216\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04472829596532716\n",
      "Average test loss: 0.0028302130645347966\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04480336963799265\n",
      "Average test loss: 0.002666062479010887\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0446936502241426\n",
      "Average test loss: 0.002766979188347856\n",
      "Epoch 265/300\n",
      "Average training loss: 0.044550604325201776\n",
      "Average test loss: 0.0027726082828723724\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04474109753635194\n",
      "Average test loss: 0.002677227064760195\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04449611141946581\n",
      "Average test loss: 0.0026978576156414216\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04453697637385792\n",
      "Average test loss: 0.002762490050142838\n",
      "Epoch 269/300\n",
      "Average training loss: 0.044433171050416095\n",
      "Average test loss: 0.0027219866149955324\n",
      "Epoch 270/300\n",
      "Average training loss: 0.044379675570461484\n",
      "Average test loss: 0.002754517327994108\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04443862348794937\n",
      "Average test loss: 0.002755641994376977\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0442874554031425\n",
      "Average test loss: 0.0027485078459398614\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04447038426333003\n",
      "Average test loss: 0.002926238356779019\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04425754866500695\n",
      "Average test loss: 0.0028320855401042437\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0442203515138891\n",
      "Average test loss: 0.0027366120578307244\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04433810043666098\n",
      "Average test loss: 0.002739501970095767\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0440864728225602\n",
      "Average test loss: 0.0027481585576509438\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04406922634442647\n",
      "Average test loss: 0.0026899962332099675\n",
      "Epoch 279/300\n",
      "Average training loss: 0.044209773768981296\n",
      "Average test loss: 0.002824255982827809\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04423195881644885\n",
      "Average test loss: 0.0027856807640443243\n",
      "Epoch 281/300\n",
      "Average training loss: 0.044138251738415825\n",
      "Average test loss: 0.0027246162609921563\n",
      "Epoch 282/300\n",
      "Average training loss: 0.044091391960779824\n",
      "Average test loss: 0.002752132566852702\n",
      "Epoch 283/300\n",
      "Average training loss: 0.043888488352298735\n",
      "Average test loss: 0.002712763853992025\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04399226025905874\n",
      "Average test loss: 0.0028321240786463022\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04387240543961525\n",
      "Average test loss: 0.0028018127528743613\n",
      "Epoch 286/300\n",
      "Average training loss: 0.043989093154668805\n",
      "Average test loss: 0.002837590145981974\n",
      "Epoch 287/300\n",
      "Average training loss: 0.043914745837450024\n",
      "Average test loss: 0.002757659129694932\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04376173134975963\n",
      "Average test loss: 0.002727567828984724\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04388017475936148\n",
      "Average test loss: 0.002768301872226099\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04382151323225763\n",
      "Average test loss: 0.0027174782970299325\n",
      "Epoch 291/300\n",
      "Average training loss: 0.043786164965894485\n",
      "Average test loss: 0.002824990183942848\n",
      "Epoch 292/300\n",
      "Average training loss: 0.043791775667005116\n",
      "Average test loss: 0.002721219227856232\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04365316461192237\n",
      "Average test loss: 0.0028294675089418886\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04373537975880835\n",
      "Average test loss: 0.0029164253667824797\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04361083561513159\n",
      "Average test loss: 0.0028940335841228563\n",
      "Epoch 296/300\n",
      "Average training loss: 0.043667834361394244\n",
      "Average test loss: 0.002847661286178562\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04369405690497822\n",
      "Average test loss: 0.002871665934514668\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04348766286174456\n",
      "Average test loss: 0.0028065330665558576\n",
      "Epoch 299/300\n",
      "Average training loss: 0.043460676577356125\n",
      "Average test loss: 0.0026838137358427046\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04369850578241878\n",
      "Average test loss: 0.0028250946830958128\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.457715850194295\n",
      "Average test loss: 0.007369314965274599\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7624023493660821\n",
      "Average test loss: 0.004400344155728817\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4075381452507443\n",
      "Average test loss: 0.0038463385885374414\n",
      "Epoch 4/300\n",
      "Average training loss: 0.273658038271798\n",
      "Average test loss: 0.0037523770078809727\n",
      "Epoch 5/300\n",
      "Average training loss: 0.20666928909884558\n",
      "Average test loss: 0.0033775277859013943\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17103161607186\n",
      "Average test loss: 0.0032717752346975936\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1486685644785563\n",
      "Average test loss: 0.0031048626175357234\n",
      "Epoch 8/300\n",
      "Average training loss: 0.13309821429517535\n",
      "Average test loss: 0.0030487129990425374\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1215064373281267\n",
      "Average test loss: 0.0029723849240690468\n",
      "Epoch 10/300\n",
      "Average training loss: 0.11260770561960008\n",
      "Average test loss: 0.002844626944512129\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1054299648337894\n",
      "Average test loss: 0.0028431327531321183\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09954916302363077\n",
      "Average test loss: 0.002626986933251222\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09457414563496908\n",
      "Average test loss: 0.002533305617256297\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09015937740272946\n",
      "Average test loss: 0.0024608719397543207\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0862559212214417\n",
      "Average test loss: 0.002399661454682549\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0830419996910625\n",
      "Average test loss: 0.002351602295206653\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08010159363349278\n",
      "Average test loss: 0.002262873713961906\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07751275416215261\n",
      "Average test loss: 0.0022076750946127704\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07516734815968408\n",
      "Average test loss: 0.0021698136864643958\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07313624470763737\n",
      "Average test loss: 0.0021227156427792377\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07120096239778731\n",
      "Average test loss: 0.0021847374760028387\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06941821385423343\n",
      "Average test loss: 0.002039458577003744\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06813252334793408\n",
      "Average test loss: 0.002025431488537126\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06667350523670515\n",
      "Average test loss: 0.0019550682412874366\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06540849966804187\n",
      "Average test loss: 0.00202924079199632\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06412257646520933\n",
      "Average test loss: 0.0019091658727783296\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0632978222237693\n",
      "Average test loss: 0.001911604536904229\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06218011772963736\n",
      "Average test loss: 0.0019166551852184866\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06132634632454978\n",
      "Average test loss: 0.0018827359181725317\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06048586260941294\n",
      "Average test loss: 0.0018437299674583807\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05972233575251367\n",
      "Average test loss: 0.0018945210738521483\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05910319025317828\n",
      "Average test loss: 0.0018481349584956964\n",
      "Epoch 33/300\n",
      "Average training loss: 0.058399590028656856\n",
      "Average test loss: 0.001815719549026754\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0576165298918883\n",
      "Average test loss: 0.0018121136685626375\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05696704713834656\n",
      "Average test loss: 0.0018158723902371194\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05635554109348191\n",
      "Average test loss: 0.0018050926509830688\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05619189802474446\n",
      "Average test loss: 0.001780007783220046\n",
      "Epoch 38/300\n",
      "Average training loss: 0.055479820317692226\n",
      "Average test loss: 0.001766616090718243\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05515968111819691\n",
      "Average test loss: 0.0017744395155459643\n",
      "Epoch 40/300\n",
      "Average training loss: 0.054530934616923335\n",
      "Average test loss: 0.00174683875652651\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05415307162536515\n",
      "Average test loss: 0.001738330613821745\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05374687255091137\n",
      "Average test loss: 0.00177621712990933\n",
      "Epoch 43/300\n",
      "Average training loss: 0.053524242715703114\n",
      "Average test loss: 0.0017393479317219722\n",
      "Epoch 44/300\n",
      "Average training loss: 0.053157834834522674\n",
      "Average test loss: 0.001741959056403074\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05264989669455422\n",
      "Average test loss: 0.001720629369840026\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05226504236459732\n",
      "Average test loss: 0.001739700676769846\n",
      "Epoch 47/300\n",
      "Average training loss: 0.052101481864849725\n",
      "Average test loss: 0.0017320797882146306\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05183484724495146\n",
      "Average test loss: 0.0017171494951471686\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05140819215112262\n",
      "Average test loss: 0.0017240972546860576\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05100594262613191\n",
      "Average test loss: 0.0017217278393606345\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05084004974696371\n",
      "Average test loss: 0.0017057336368080643\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05045674073696137\n",
      "Average test loss: 0.0016950348485261202\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05012495440244675\n",
      "Average test loss: 0.001708637037831876\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04978752581940757\n",
      "Average test loss: 0.0017389276987459096\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04973616054654122\n",
      "Average test loss: 0.0017022639766542448\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04930842131707403\n",
      "Average test loss: 0.001708686631379856\n",
      "Epoch 57/300\n",
      "Average training loss: 0.049095288647545705\n",
      "Average test loss: 0.001688275743379361\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0488173258834415\n",
      "Average test loss: 0.0017253192367239132\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0486099257386393\n",
      "Average test loss: 0.0017159383971658017\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04814757263660431\n",
      "Average test loss: 0.001691549595962796\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04806927380959193\n",
      "Average test loss: 0.0016931435937682787\n",
      "Epoch 62/300\n",
      "Average training loss: 0.047962335546811424\n",
      "Average test loss: 0.0017006217716261744\n",
      "Epoch 63/300\n",
      "Average training loss: 0.047643762406375675\n",
      "Average test loss: 0.0017077004508529272\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04734246784779761\n",
      "Average test loss: 0.0017388235392669837\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04708720072110494\n",
      "Average test loss: 0.0016704400566199588\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04676205486390326\n",
      "Average test loss: 0.0017137899867569408\n",
      "Epoch 67/300\n",
      "Average training loss: 0.046517035914791956\n",
      "Average test loss: 0.0016829066967798604\n",
      "Epoch 68/300\n",
      "Average training loss: 0.046387568771839145\n",
      "Average test loss: 0.0016954902524335517\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04618071170979076\n",
      "Average test loss: 0.001754781728506916\n",
      "Epoch 70/300\n",
      "Average training loss: 0.045970883945624035\n",
      "Average test loss: 0.0016880723234886925\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0455058649248547\n",
      "Average test loss: 0.0016924773983450399\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04541860556271341\n",
      "Average test loss: 0.0017049057263260087\n",
      "Epoch 73/300\n",
      "Average training loss: 0.045224465376800964\n",
      "Average test loss: 0.0017085265436520179\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04501562099158764\n",
      "Average test loss: 0.0017132394968842467\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04494439889325036\n",
      "Average test loss: 0.0016823701701230472\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04448650064898862\n",
      "Average test loss: 0.001694837360125449\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04435537983311547\n",
      "Average test loss: 0.0016894344485675295\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04419949571622742\n",
      "Average test loss: 0.0017525193051745494\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0440412606994311\n",
      "Average test loss: 0.001699043968381981\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04373327043983671\n",
      "Average test loss: 0.001693336226977408\n",
      "Epoch 81/300\n",
      "Average training loss: 0.043515186147557366\n",
      "Average test loss: 0.001696869958916472\n",
      "Epoch 82/300\n",
      "Average training loss: 0.043335414595074125\n",
      "Average test loss: 0.0016929198450719317\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04315282621317439\n",
      "Average test loss: 0.00169540452118963\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04311524316668511\n",
      "Average test loss: 0.0017351541198376151\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04313179086976581\n",
      "Average test loss: 0.0017205352135416534\n",
      "Epoch 86/300\n",
      "Average training loss: 0.042581398520204754\n",
      "Average test loss: 0.0017305322972436747\n",
      "Epoch 87/300\n",
      "Average training loss: 0.042388437282707955\n",
      "Average test loss: 0.001757176061367823\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04222508012255033\n",
      "Average test loss: 0.0017314193578850893\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04198408297366566\n",
      "Average test loss: 0.0017647909621397655\n",
      "Epoch 90/300\n",
      "Average training loss: 0.042049157056543565\n",
      "Average test loss: 0.0017405321827779214\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04160196218556828\n",
      "Average test loss: 0.001739037834521797\n",
      "Epoch 92/300\n",
      "Average training loss: 0.041525120791461735\n",
      "Average test loss: 0.001736573720143901\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04133476262622409\n",
      "Average test loss: 0.0017385309206114875\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04123551307949755\n",
      "Average test loss: 0.0017318946115362148\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04105675822827551\n",
      "Average test loss: 0.001727993822346131\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04104511182175742\n",
      "Average test loss: 0.0017669248794102007\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0406259553068214\n",
      "Average test loss: 0.001724248695290751\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04070552125573158\n",
      "Average test loss: 0.0017253839735769563\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04055551928944058\n",
      "Average test loss: 0.0017139038178655835\n",
      "Epoch 100/300\n",
      "Average training loss: 0.040243939012289044\n",
      "Average test loss: 0.0016984007561372386\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04012522779239549\n",
      "Average test loss: 0.0017164284612776505\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03981716238790088\n",
      "Average test loss: 0.0017118223655141062\n",
      "Epoch 103/300\n",
      "Average training loss: 0.039681498494413166\n",
      "Average test loss: 0.0018050668332725763\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03962016059954961\n",
      "Average test loss: 0.0017512131427518196\n",
      "Epoch 105/300\n",
      "Average training loss: 0.039444623400767646\n",
      "Average test loss: 0.001832683392903871\n",
      "Epoch 106/300\n",
      "Average training loss: 0.039407421658436456\n",
      "Average test loss: 0.001763160082201163\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03933175163467725\n",
      "Average test loss: 0.0017430505685301291\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03909923060735067\n",
      "Average test loss: 0.0017614112702301807\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03893357936044534\n",
      "Average test loss: 0.0017979894982029995\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03871151657568084\n",
      "Average test loss: 0.0017914850649734337\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03872380873560905\n",
      "Average test loss: 0.0017915605900602207\n",
      "Epoch 112/300\n",
      "Average training loss: 0.038597140178084376\n",
      "Average test loss: 0.0017606897316873073\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03871968277957704\n",
      "Average test loss: 0.001728592259809375\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03823055478930473\n",
      "Average test loss: 0.0017646504264945786\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03822079811162419\n",
      "Average test loss: 0.0017598515221228202\n",
      "Epoch 116/300\n",
      "Average training loss: 0.038044784092240866\n",
      "Average test loss: 0.001777089121337566\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03782865931921535\n",
      "Average test loss: 0.0017467571452466977\n",
      "Epoch 118/300\n",
      "Average training loss: 0.037813326453169184\n",
      "Average test loss: 0.001759118310486277\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03763535217444102\n",
      "Average test loss: 0.0017710937391966582\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03790977702538172\n",
      "Average test loss: 0.001784271714122345\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0375383845484919\n",
      "Average test loss: 0.0017895029860859116\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03730278633700477\n",
      "Average test loss: 0.0018270833801685108\n",
      "Epoch 123/300\n",
      "Average training loss: 0.037302341894971\n",
      "Average test loss: 0.0018017140539983909\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0371415090461572\n",
      "Average test loss: 0.001811406940428747\n",
      "Epoch 125/300\n",
      "Average training loss: 0.036963733280698456\n",
      "Average test loss: 0.0017326838044035766\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03690792416532834\n",
      "Average test loss: 0.0017587439523388943\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03687364367312855\n",
      "Average test loss: 0.00177254560103433\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03673377474314637\n",
      "Average test loss: 0.0017874375754553412\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03669912930495209\n",
      "Average test loss: 0.0018065080930375391\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0364222006963359\n",
      "Average test loss: 0.001814123223328756\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03642719714509116\n",
      "Average test loss: 0.0017830431871116161\n",
      "Epoch 132/300\n",
      "Average training loss: 0.036257469433877206\n",
      "Average test loss: 0.001831591200704376\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03614969143602583\n",
      "Average test loss: 0.0017642857969428101\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03610706957181295\n",
      "Average test loss: 0.0018147452696640459\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03613218096229765\n",
      "Average test loss: 0.0017999551249668001\n",
      "Epoch 136/300\n",
      "Average training loss: 0.035959561460547976\n",
      "Average test loss: 0.0017993912988652785\n",
      "Epoch 137/300\n",
      "Average training loss: 0.035829815482099854\n",
      "Average test loss: 0.0017975653021906812\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03580736200511456\n",
      "Average test loss: 0.0018902298419011963\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0356431442366706\n",
      "Average test loss: 0.0018009919217891162\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03556908869908915\n",
      "Average test loss: 0.0018250067986340986\n",
      "Epoch 141/300\n",
      "Average training loss: 0.035663664748271304\n",
      "Average test loss: 0.0017813364145242505\n",
      "Epoch 142/300\n",
      "Average training loss: 0.035411167588498854\n",
      "Average test loss: 0.0018439526295082438\n",
      "Epoch 143/300\n",
      "Average training loss: 0.035389100223779675\n",
      "Average test loss: 0.001803177304772867\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03537462352050675\n",
      "Average test loss: 0.0018446453395817015\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03520895281434059\n",
      "Average test loss: 0.0017919171880930662\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03495060459772746\n",
      "Average test loss: 0.001776313088213404\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03496095822254817\n",
      "Average test loss: 0.0018215334571691022\n",
      "Epoch 148/300\n",
      "Average training loss: 0.034878810581233766\n",
      "Average test loss: 0.0018312399718496534\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03482054793172412\n",
      "Average test loss: 0.0017924010002364715\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03471912887030178\n",
      "Average test loss: 0.0018068321035760973\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03476317898763551\n",
      "Average test loss: 0.001804359116488033\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03461004626750946\n",
      "Average test loss: 0.0017717854413721297\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03465061236421267\n",
      "Average test loss: 0.0018480832416357265\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0346054578324159\n",
      "Average test loss: 0.0018048741567052073\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03428559963570701\n",
      "Average test loss: 0.0018851676641239061\n",
      "Epoch 156/300\n",
      "Average training loss: 0.034352979231211875\n",
      "Average test loss: 0.0018449773531821038\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0342512682278951\n",
      "Average test loss: 0.0018338911169105106\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03422996709412999\n",
      "Average test loss: 0.0019731406510497132\n",
      "Epoch 159/300\n",
      "Average training loss: 0.034083009405268566\n",
      "Average test loss: 0.0018360464834711618\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03417224368453026\n",
      "Average test loss: 0.0018055095888363818\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0339321570644776\n",
      "Average test loss: 0.0018773665924866993\n",
      "Epoch 162/300\n",
      "Average training loss: 0.033907334157162246\n",
      "Average test loss: 0.0018715990467203989\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03388115657369296\n",
      "Average test loss: 0.0018386570006195043\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03381235288580259\n",
      "Average test loss: 0.0018445013374504115\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03372663397590319\n",
      "Average test loss: 0.0018002620905948183\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03367059812280867\n",
      "Average test loss: 0.0018207647941178746\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03366032654709286\n",
      "Average test loss: 0.0018636315355284347\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0336615594840712\n",
      "Average test loss: 0.0019000059155126414\n",
      "Epoch 169/300\n",
      "Average training loss: 0.033476260191864435\n",
      "Average test loss: 0.0018281979533947177\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03342800298333168\n",
      "Average test loss: 0.002298923003176848\n",
      "Epoch 171/300\n",
      "Average training loss: 0.033399592113163734\n",
      "Average test loss: 0.0018223877816150587\n",
      "Epoch 172/300\n",
      "Average training loss: 0.033231583174731995\n",
      "Average test loss: 0.0018634649419950115\n",
      "Epoch 173/300\n",
      "Average training loss: 0.033147280669874615\n",
      "Average test loss: 0.0018467130685846011\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03330704098608759\n",
      "Average test loss: 0.0018584976229402755\n",
      "Epoch 176/300\n",
      "Average training loss: 0.033148373656802704\n",
      "Average test loss: 0.0018238760367449787\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03300394318501155\n",
      "Average test loss: 0.0018710655723181036\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03291997993323538\n",
      "Average test loss: 0.0018345465779097544\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03290589919024044\n",
      "Average test loss: 0.001903857531853848\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03288367649912834\n",
      "Average test loss: 0.0018559593823738395\n",
      "Epoch 181/300\n",
      "Average training loss: 0.032920327122012774\n",
      "Average test loss: 0.0018935810034680697\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032811097572247185\n",
      "Average test loss: 0.002062676512118843\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03275080035958025\n",
      "Average test loss: 0.0018542713741254476\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03260345196227232\n",
      "Average test loss: 0.001876054004455606\n",
      "Epoch 185/300\n",
      "Average training loss: 0.032703302557269734\n",
      "Average test loss: 0.0018347539506438706\n",
      "Epoch 186/300\n",
      "Average training loss: 0.032576302031675976\n",
      "Average test loss: 0.0018266173504913846\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03245363972087701\n",
      "Average test loss: 0.0018745923685944742\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0324768477347162\n",
      "Average test loss: 0.0018963041375908588\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03257168431083361\n",
      "Average test loss: 0.0018484704765594669\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03244881259070503\n",
      "Average test loss: 0.001830139129422605\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03227258709900909\n",
      "Average test loss: 0.0018248668665894203\n",
      "Epoch 192/300\n",
      "Average training loss: 0.032305485578046904\n",
      "Average test loss: 0.001923200824815366\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03229392458001772\n",
      "Average test loss: 0.0018706644533409012\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03226994989646806\n",
      "Average test loss: 0.0018641931400116946\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03224525483946006\n",
      "Average test loss: 0.0019260230474174023\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03224526008963585\n",
      "Average test loss: 0.0018464233725228243\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03213208639125029\n",
      "Average test loss: 0.0020481163528230455\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031992995311816534\n",
      "Average test loss: 0.0018886501122679976\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03191277781294452\n",
      "Average test loss: 0.001869925572226445\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03210428275167942\n",
      "Average test loss: 0.0018713374212384224\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031858944276968636\n",
      "Average test loss: 0.0018836562373778887\n",
      "Epoch 202/300\n",
      "Average training loss: 0.031760420242945356\n",
      "Average test loss: 0.0018717012881922226\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03183208007613818\n",
      "Average test loss: 0.0019189126243193944\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03184863892859883\n",
      "Average test loss: 0.001910010255045361\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03169237430228127\n",
      "Average test loss: 0.0018457406409498717\n",
      "Epoch 206/300\n",
      "Average training loss: 0.031681244510743355\n",
      "Average test loss: 0.001888684140621788\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03162497150235706\n",
      "Average test loss: 0.0018783073734699024\n",
      "Epoch 208/300\n",
      "Average training loss: 0.031685183879401946\n",
      "Average test loss: 0.0018441824511521393\n",
      "Epoch 209/300\n",
      "Average training loss: 0.031529093969199395\n",
      "Average test loss: 0.0018579668393358588\n",
      "Epoch 210/300\n",
      "Average training loss: 0.031536242458555436\n",
      "Average test loss: 0.001920409912450446\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03147448334428999\n",
      "Average test loss: 0.0019115745261725453\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03141329683694575\n",
      "Average test loss: 0.0018776428347660435\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031509714075260695\n",
      "Average test loss: 0.0019592423252761362\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03142093206279808\n",
      "Average test loss: 0.0018644377938989137\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03131982115738922\n",
      "Average test loss: 0.0019700966458767654\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0313071454597844\n",
      "Average test loss: 0.002167254101898935\n",
      "Epoch 217/300\n",
      "Average training loss: 0.031355575430724356\n",
      "Average test loss: 0.0018835475494464239\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031135646929343543\n",
      "Average test loss: 0.0018551453430619505\n",
      "Epoch 219/300\n",
      "Average training loss: 0.031172042967544663\n",
      "Average test loss: 0.0018788191191852093\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031211863709820643\n",
      "Average test loss: 0.0025390580314108065\n",
      "Epoch 221/300\n",
      "Average training loss: 0.031094215739104484\n",
      "Average test loss: 0.0019129548360490136\n",
      "Epoch 222/300\n",
      "Average training loss: 0.031033981710672377\n",
      "Average test loss: 0.0018584087920478648\n",
      "Epoch 223/300\n",
      "Average training loss: 0.030977099140485128\n",
      "Average test loss: 0.001962787338429027\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03100362573729621\n",
      "Average test loss: 0.0018990275118913914\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03091415351629257\n",
      "Average test loss: 0.0018643702684591214\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030925193670723175\n",
      "Average test loss: 0.001876191640065776\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03124367230468326\n",
      "Average test loss: 0.001955099831024806\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030876025625401073\n",
      "Average test loss: 0.0018677754850230284\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030788497497638068\n",
      "Average test loss: 0.001903243358557423\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03084743412004577\n",
      "Average test loss: 0.0018988710905735692\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03069449699918429\n",
      "Average test loss: 0.0018511138788113992\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030912370276119976\n",
      "Average test loss: 0.0018787706271848744\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030732426855299207\n",
      "Average test loss: 0.0019003664000580708\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03063820039894846\n",
      "Average test loss: 0.001895913781080809\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03059750896361139\n",
      "Average test loss: 0.001974303110088739\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030624801087710592\n",
      "Average test loss: 0.001866880371959673\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030570058680242963\n",
      "Average test loss: 0.0019519653364809023\n",
      "Epoch 238/300\n",
      "Average training loss: 0.030538951562510595\n",
      "Average test loss: 0.0019103490236318775\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0305356169309881\n",
      "Average test loss: 0.001898055270469437\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03045161812669701\n",
      "Average test loss: 0.001978359672654834\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03054856342739529\n",
      "Average test loss: 0.0018770816156433688\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030438855152991082\n",
      "Average test loss: 0.0018867889617880185\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03030354959103796\n",
      "Average test loss: 0.001931240589875314\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030313025168246695\n",
      "Average test loss: 0.0019258079489486086\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03030410267247094\n",
      "Average test loss: 0.0019566324410132236\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03028491646382544\n",
      "Average test loss: 0.0018750174533472292\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03027002315885491\n",
      "Average test loss: 0.001951921416239606\n",
      "Epoch 248/300\n",
      "Average training loss: 0.030217842146754263\n",
      "Average test loss: 0.0018905399212510222\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03023136687113179\n",
      "Average test loss: 0.0021025426077346007\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03016801901327239\n",
      "Average test loss: 0.0018681457284837962\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030164870146248076\n",
      "Average test loss: 0.001923941183835268\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030163410309288236\n",
      "Average test loss: 0.0019364314928857816\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030092550590634346\n",
      "Average test loss: 0.0019197843466988867\n",
      "Epoch 254/300\n",
      "Average training loss: 0.030129451450374392\n",
      "Average test loss: 0.001920483871259623\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03009909265405602\n",
      "Average test loss: 0.0019043059911992815\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03000219663977623\n",
      "Average test loss: 0.0019363333344873455\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030042485071553125\n",
      "Average test loss: 0.0018851031925943163\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02995117134021388\n",
      "Average test loss: 0.0018798006428405642\n",
      "Epoch 259/300\n",
      "Average training loss: 0.029911107836498155\n",
      "Average test loss: 0.0019236252829432487\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02997761698232757\n",
      "Average test loss: 0.001969189169816673\n",
      "Epoch 261/300\n",
      "Average training loss: 0.029846672256787617\n",
      "Average test loss: 0.0019210599304901228\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029827095025115543\n",
      "Average test loss: 0.0019211050196447307\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02987533647980955\n",
      "Average test loss: 0.0019398451587185263\n",
      "Epoch 264/300\n",
      "Average training loss: 0.029822989328040016\n",
      "Average test loss: 0.002014161084468166\n",
      "Epoch 265/300\n",
      "Average training loss: 0.029842106853922207\n",
      "Average test loss: 0.0019279480111888713\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02975461074709892\n",
      "Average test loss: 0.0019126991173252464\n",
      "Epoch 267/300\n",
      "Average training loss: 0.029853961436284913\n",
      "Average test loss: 0.0019249048180257282\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029742271115382513\n",
      "Average test loss: 0.0019021279276866053\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029598111669222514\n",
      "Average test loss: 0.001889135058865779\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029636761203408243\n",
      "Average test loss: 0.0018793308999803331\n",
      "Epoch 271/300\n",
      "Average training loss: 0.029618215400311683\n",
      "Average test loss: 0.0019854669896885754\n",
      "Epoch 272/300\n",
      "Average training loss: 0.029549226596951486\n",
      "Average test loss: 0.001914794216139449\n",
      "Epoch 273/300\n",
      "Average training loss: 0.029512269833021694\n",
      "Average test loss: 0.001954186069882578\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02954918112523026\n",
      "Average test loss: 0.0018858772152000004\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02961891810099284\n",
      "Average test loss: 0.0018895432248504625\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029611047998070718\n",
      "Average test loss: 0.0019060495893677904\n",
      "Epoch 277/300\n",
      "Average training loss: 0.029565254676673147\n",
      "Average test loss: 0.0020271751563996077\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029417980237139596\n",
      "Average test loss: 0.0019517165922249356\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029434509712788794\n",
      "Average test loss: 0.001973810992721054\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02934566658900844\n",
      "Average test loss: 0.0018986172620207072\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029414371889498497\n",
      "Average test loss: 0.0018804090187574426\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029451366919610235\n",
      "Average test loss: 0.001890490454932054\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029427585523989466\n",
      "Average test loss: 0.0019655035824204486\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029261995534102123\n",
      "Average test loss: 0.0019960241220477555\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0293226292166445\n",
      "Average test loss: 0.0019928257873074876\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029199357794390784\n",
      "Average test loss: 0.0019986512060794566\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029329679807027182\n",
      "Average test loss: 0.0018908387173691558\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02935802761713664\n",
      "Average test loss: 0.0023600477160265047\n",
      "Epoch 289/300\n",
      "Average training loss: 0.029138697036438517\n",
      "Average test loss: 0.0020087624933156703\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029168313110868135\n",
      "Average test loss: 0.0018995265598512358\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029191180777218606\n",
      "Average test loss: 0.0019340735977101658\n",
      "Epoch 292/300\n",
      "Average training loss: 0.029183473338683447\n",
      "Average test loss: 0.002269833216547138\n",
      "Epoch 293/300\n",
      "Average training loss: 0.029114354527658887\n",
      "Average test loss: 0.002056453235861328\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029081067028972837\n",
      "Average test loss: 0.0019167697698705727\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02901414189239343\n",
      "Average test loss: 0.0019350951114255521\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02905880666275819\n",
      "Average test loss: 0.0019169199185238943\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029055508943067656\n",
      "Average test loss: 0.0019330366257992056\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029035764417714542\n",
      "Average test loss: 0.0019107287243629496\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028935382490356765\n",
      "Average test loss: 0.0019722974823994767\n",
      "Epoch 300/300\n",
      "Average training loss: 0.028939655065536498\n",
      "Average test loss: 0.002034261488252216\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'DCT_32_Depth3/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.81\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.33\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.48\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.72\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.47\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.70\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.70\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.49\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.89\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.20\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.28\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.05\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.38\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.51\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.56\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.28\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.37\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.63\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.95\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.64\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.34\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.39\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.46\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.84\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.89\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.90\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.05\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.14\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.08\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.98\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.09\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.26\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.29\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.24\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616b8d7-2b92-46b0-a2a7-1f900d59eb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
