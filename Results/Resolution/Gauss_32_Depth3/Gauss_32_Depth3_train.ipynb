{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/32 x 32 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_32x32.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.23304819700453017\n",
      "Average test loss: 0.011221452561517556\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06227529554234611\n",
      "Average test loss: 0.01021574701120456\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05588855416907205\n",
      "Average test loss: 0.00928563543740246\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05275441288285785\n",
      "Average test loss: 0.009258719948430857\n",
      "Epoch 5/300\n",
      "Average training loss: 0.051206742992003756\n",
      "Average test loss: 0.009214807185033957\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04945191240310669\n",
      "Average test loss: 0.010051587741408083\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04777595759762658\n",
      "Average test loss: 0.009789224464860228\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04686785924765799\n",
      "Average test loss: 0.008604504448672135\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04607993717988332\n",
      "Average test loss: 0.008516477635337246\n",
      "Epoch 10/300\n",
      "Average training loss: 0.045280993547704484\n",
      "Average test loss: 0.008311309374041027\n",
      "Epoch 11/300\n",
      "Average training loss: 0.044835892226960924\n",
      "Average test loss: 0.008416808119250668\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04422231403986613\n",
      "Average test loss: 0.008192226808932092\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04383457048071755\n",
      "Average test loss: 0.008102261195580164\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04335300981998444\n",
      "Average test loss: 0.007919531159930758\n",
      "Epoch 15/300\n",
      "Average training loss: 0.042860447701480654\n",
      "Average test loss: 0.008038167309429911\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04242132901151975\n",
      "Average test loss: 0.007755218959516949\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04202634032898479\n",
      "Average test loss: 0.00781688960062133\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04164081483417087\n",
      "Average test loss: 0.007598200123343203\n",
      "Epoch 19/300\n",
      "Average training loss: 0.041433112568325464\n",
      "Average test loss: 0.007657111279666424\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04095840709739261\n",
      "Average test loss: 0.0077038534747229685\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04085990388525857\n",
      "Average test loss: 0.007613476247009304\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04049163353112009\n",
      "Average test loss: 0.0077348408496214285\n",
      "Epoch 23/300\n",
      "Average training loss: 0.040145485921038525\n",
      "Average test loss: 0.0073619839751886\n",
      "Epoch 24/300\n",
      "Average training loss: 0.040007706304391225\n",
      "Average test loss: 0.007787889221476184\n",
      "Epoch 25/300\n",
      "Average training loss: 0.039772465381357404\n",
      "Average test loss: 0.007293004194895427\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03959044686125385\n",
      "Average test loss: 0.0072565647338827454\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03930866132179896\n",
      "Average test loss: 0.007159402166803678\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039106701251533295\n",
      "Average test loss: 0.007152914945450094\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03890211612317297\n",
      "Average test loss: 0.007390359543677834\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03879222815235456\n",
      "Average test loss: 0.007323742870655325\n",
      "Epoch 31/300\n",
      "Average training loss: 0.038673080414533614\n",
      "Average test loss: 0.007309543524144424\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03849292973346181\n",
      "Average test loss: 0.007551142049332459\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03831831098596255\n",
      "Average test loss: 0.007155938364565372\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03805083281795184\n",
      "Average test loss: 0.007043649739689298\n",
      "Epoch 35/300\n",
      "Average training loss: 0.037961207538843154\n",
      "Average test loss: 0.007006685007777479\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03779762076669269\n",
      "Average test loss: 0.006953253091209465\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03765738423334228\n",
      "Average test loss: 0.00703747042392691\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03762586573097441\n",
      "Average test loss: 0.007138041476822562\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03746733652220832\n",
      "Average test loss: 0.007088350835359759\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03735902906457583\n",
      "Average test loss: 0.006959849571188291\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03724071854352951\n",
      "Average test loss: 0.006882922405997912\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03704024797346857\n",
      "Average test loss: 0.006812256558073892\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03706521634923087\n",
      "Average test loss: 0.007049093939363957\n",
      "Epoch 44/300\n",
      "Average training loss: 0.036848407510254116\n",
      "Average test loss: 0.006834649800840351\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03681346288654539\n",
      "Average test loss: 0.006859714877274301\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03674815527598063\n",
      "Average test loss: 0.0068483874632252585\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03658914676639769\n",
      "Average test loss: 0.0067407183941039775\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03650762832495901\n",
      "Average test loss: 0.006746828885955943\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03644676888982455\n",
      "Average test loss: 0.007088027126259274\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03636966212756104\n",
      "Average test loss: 0.006998033812476529\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03627165973186493\n",
      "Average test loss: 0.00668665592206849\n",
      "Epoch 52/300\n",
      "Average training loss: 0.036281870106856025\n",
      "Average test loss: 0.006779420553396145\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03619042768412166\n",
      "Average test loss: 0.00676007229834795\n",
      "Epoch 54/300\n",
      "Average training loss: 0.036014496273464626\n",
      "Average test loss: 0.006728918628560172\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03597386686669456\n",
      "Average test loss: 0.006785606823861599\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03589349280463325\n",
      "Average test loss: 0.006676765775101052\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03590855609046088\n",
      "Average test loss: 0.00680624730574588\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03571828256216314\n",
      "Average test loss: 0.0066858314486841364\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03568862592511707\n",
      "Average test loss: 0.006764164381143119\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03562780049443245\n",
      "Average test loss: 0.0067887333184480664\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03557104751136568\n",
      "Average test loss: 0.0071507690975235565\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03550369672311677\n",
      "Average test loss: 0.006679653481062916\n",
      "Epoch 63/300\n",
      "Average training loss: 0.035421718541118835\n",
      "Average test loss: 0.006606939165542523\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03546713010470073\n",
      "Average test loss: 0.006764657413793934\n",
      "Epoch 65/300\n",
      "Average training loss: 0.035324385695987275\n",
      "Average test loss: 0.006793497185740206\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0353636006878482\n",
      "Average test loss: 0.007632730265458425\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03527015328076151\n",
      "Average test loss: 0.0066418009479012755\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03519192921121915\n",
      "Average test loss: 0.006589493612034454\n",
      "Epoch 69/300\n",
      "Average training loss: 0.035142471979061764\n",
      "Average test loss: 0.006744922040651242\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03503918178545104\n",
      "Average test loss: 0.0070729158139891096\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03503251217636797\n",
      "Average test loss: 0.006765474740829733\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03500231911738714\n",
      "Average test loss: 0.006677819333556626\n",
      "Epoch 73/300\n",
      "Average training loss: 0.034915870408217115\n",
      "Average test loss: 0.006652907256036997\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03499177155726486\n",
      "Average test loss: 0.006694905262440443\n",
      "Epoch 75/300\n",
      "Average training loss: 0.035293690244356794\n",
      "Average test loss: 0.006675069573852751\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03476674852106306\n",
      "Average test loss: 0.006654058207861252\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03473229431112607\n",
      "Average test loss: 0.006820882365521457\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03472170082065794\n",
      "Average test loss: 0.006883039851776428\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03467457667986552\n",
      "Average test loss: 0.007162947036739853\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03460897048148844\n",
      "Average test loss: 0.006649406272917986\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03461712473299768\n",
      "Average test loss: 0.006605982043262985\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034573307717839875\n",
      "Average test loss: 0.006660802457067702\n",
      "Epoch 83/300\n",
      "Average training loss: 0.034507176341281996\n",
      "Average test loss: 0.0066440303052465125\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0344825618300173\n",
      "Average test loss: 0.006650230846678217\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03441792545384831\n",
      "Average test loss: 0.006544549519403114\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0344168803907103\n",
      "Average test loss: 0.006521895957903729\n",
      "Epoch 87/300\n",
      "Average training loss: 0.034350542227427165\n",
      "Average test loss: 0.006647488962858915\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03432201753722297\n",
      "Average test loss: 0.006888954125758674\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03433336489730411\n",
      "Average test loss: 0.006904911684907145\n",
      "Epoch 90/300\n",
      "Average training loss: 0.034246731779641576\n",
      "Average test loss: 0.006570095255143113\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03420309236976835\n",
      "Average test loss: 0.006708028271380398\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03422620733247863\n",
      "Average test loss: 0.00658354813978076\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03413078732622994\n",
      "Average test loss: 0.00659502223547962\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0341930808176597\n",
      "Average test loss: 0.006780296184536483\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03409538238247235\n",
      "Average test loss: 0.006672094868289099\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03406869358486599\n",
      "Average test loss: 0.0066475241283575695\n",
      "Epoch 97/300\n",
      "Average training loss: 0.033926768236690094\n",
      "Average test loss: 0.006613858782996734\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03400900694727898\n",
      "Average test loss: 0.030558027966154946\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0340074007180002\n",
      "Average test loss: 0.0065914801967640714\n",
      "Epoch 100/300\n",
      "Average training loss: 0.033895679006973906\n",
      "Average test loss: 0.006650662348088291\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03391340424285995\n",
      "Average test loss: 0.007013463031914499\n",
      "Epoch 102/300\n",
      "Average training loss: 0.033851320410768194\n",
      "Average test loss: 0.006747847096787558\n",
      "Epoch 103/300\n",
      "Average training loss: 0.033767695440186395\n",
      "Average test loss: 0.006745921538521846\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03379378962185648\n",
      "Average test loss: 0.0065311592287487456\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03381743158896764\n",
      "Average test loss: 0.006557314027514723\n",
      "Epoch 106/300\n",
      "Average training loss: 0.033718399925364385\n",
      "Average test loss: 0.006790747566355599\n",
      "Epoch 107/300\n",
      "Average training loss: 0.033665594104263515\n",
      "Average test loss: 0.006729822530928585\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0336633426282141\n",
      "Average test loss: 0.0065861695661313\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03367543048163255\n",
      "Average test loss: 0.006618647103301353\n",
      "Epoch 110/300\n",
      "Average training loss: 0.033656499279869925\n",
      "Average test loss: 0.006660698907656802\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03358593711256981\n",
      "Average test loss: 0.006903912188278304\n",
      "Epoch 112/300\n",
      "Average training loss: 0.033561794724729324\n",
      "Average test loss: 0.006614649193154441\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03350315742029084\n",
      "Average test loss: 0.006711041621863842\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03346788823935721\n",
      "Average test loss: 0.006698373054050737\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03358032005528609\n",
      "Average test loss: 0.007428497311555677\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03339746209813489\n",
      "Average test loss: 0.006616220065909955\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03339404040243891\n",
      "Average test loss: 0.006658814800696241\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03343956548968951\n",
      "Average test loss: 0.0066532803417907824\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03333840640054809\n",
      "Average test loss: 0.006718569990247488\n",
      "Epoch 120/300\n",
      "Average training loss: 0.033326050731870865\n",
      "Average test loss: 0.0066338019250995585\n",
      "Epoch 121/300\n",
      "Average training loss: 0.033321545395586225\n",
      "Average test loss: 0.006717420932319429\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03328837176495128\n",
      "Average test loss: 0.006861258772926198\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0332721708714962\n",
      "Average test loss: 0.006822602750526534\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03329527016977469\n",
      "Average test loss: 0.006975926473322842\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03317892325586743\n",
      "Average test loss: 0.006729222739322318\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03334524950053957\n",
      "Average test loss: 0.0325870401478476\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03312929523984591\n",
      "Average test loss: 0.006647728763934639\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03312401918901337\n",
      "Average test loss: 0.0065943225286901\n",
      "Epoch 129/300\n",
      "Average training loss: 0.033182546148697534\n",
      "Average test loss: 0.006633756585419178\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03312850987745656\n",
      "Average test loss: 0.006596938847667641\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03306547164585855\n",
      "Average test loss: 0.006896599851962593\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03308864873316553\n",
      "Average test loss: 0.006586808854920996\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03298071086406708\n",
      "Average test loss: 0.006744416202108065\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03303944450285699\n",
      "Average test loss: 0.00679834860480494\n",
      "Epoch 135/300\n",
      "Average training loss: 0.032957713037729264\n",
      "Average test loss: 0.006651936694151825\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03297148245241907\n",
      "Average test loss: 0.006700293582346704\n",
      "Epoch 137/300\n",
      "Average training loss: 0.032912238041559856\n",
      "Average test loss: 0.0066030366350379255\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03294078630705675\n",
      "Average test loss: 0.0066449633323484\n",
      "Epoch 139/300\n",
      "Average training loss: 0.032901734819014865\n",
      "Average test loss: 0.006785137189759149\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03282215365933047\n",
      "Average test loss: 0.011142201186882125\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03285879275865025\n",
      "Average test loss: 0.006678311628599961\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03281707321769661\n",
      "Average test loss: 0.006761095047824912\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03276960834529665\n",
      "Average test loss: 0.006783311249067386\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03277626774542861\n",
      "Average test loss: 0.006694281527151664\n",
      "Epoch 145/300\n",
      "Average training loss: 0.032783288078175654\n",
      "Average test loss: 0.006702190274579658\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03271514400508669\n",
      "Average test loss: 0.006697108624709977\n",
      "Epoch 147/300\n",
      "Average training loss: 0.032689807703097665\n",
      "Average test loss: 0.006688461599250635\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03266463442809052\n",
      "Average test loss: 0.0066445526563458975\n",
      "Epoch 149/300\n",
      "Average training loss: 0.032748098380035826\n",
      "Average test loss: 0.006770318225853973\n",
      "Epoch 150/300\n",
      "Average training loss: 0.032677523069911535\n",
      "Average test loss: 0.006747080022676124\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03258128010233243\n",
      "Average test loss: 0.006613654704971446\n",
      "Epoch 152/300\n",
      "Average training loss: 0.032655861392617225\n",
      "Average test loss: 0.006757034910221895\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03259871881537967\n",
      "Average test loss: 0.006740437049832609\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0326388381421566\n",
      "Average test loss: 0.0066384530746274525\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03251261005136702\n",
      "Average test loss: 0.006989816843635506\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03271046495106485\n",
      "Average test loss: 0.006861484754002756\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03246394461062219\n",
      "Average test loss: 0.009782691832217905\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03248476422164175\n",
      "Average test loss: 0.0066951682046055795\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03247397734555933\n",
      "Average test loss: 0.006881892976661523\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03242293369438913\n",
      "Average test loss: 0.006746647459765275\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03247156844205327\n",
      "Average test loss: 0.006679121697114573\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03238347390956349\n",
      "Average test loss: 0.006713899661269453\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03239732873108652\n",
      "Average test loss: 0.007231941976067093\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03240726343128416\n",
      "Average test loss: 0.007145527704722351\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03241641584038735\n",
      "Average test loss: 4.967317439609103\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03281958227356275\n",
      "Average test loss: 0.0066693010611666576\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03223335204521815\n",
      "Average test loss: 0.006718971834000614\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03227226867609554\n",
      "Average test loss: 0.006985436506983306\n",
      "Epoch 169/300\n",
      "Average training loss: 0.032304048909081354\n",
      "Average test loss: 0.006641864386283689\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03224732211563322\n",
      "Average test loss: 0.006719249123500453\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03230681051810583\n",
      "Average test loss: 0.007399224585956998\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03226434149510331\n",
      "Average test loss: 0.006651973712361522\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03223225591248936\n",
      "Average test loss: 0.006935502539078394\n",
      "Epoch 174/300\n",
      "Average training loss: 0.032149669562776886\n",
      "Average test loss: 0.006710611866373155\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03213650369644165\n",
      "Average test loss: 0.006693072818219662\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03212598396341006\n",
      "Average test loss: 0.006834043609599273\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03217114518086116\n",
      "Average test loss: 0.006708159831249051\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03214023820724752\n",
      "Average test loss: 0.006900740483982696\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0320923666689131\n",
      "Average test loss: 0.0066921212193038725\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03208348218599955\n",
      "Average test loss: 0.006771733610166444\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03208744453887145\n",
      "Average test loss: 0.006672861051435272\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032127185970544814\n",
      "Average test loss: 0.00694853901449177\n",
      "Epoch 183/300\n",
      "Average training loss: 0.032107802513572904\n",
      "Average test loss: 0.006705232631001207\n",
      "Epoch 184/300\n",
      "Average training loss: 0.032035997190409234\n",
      "Average test loss: 0.006811050823165311\n",
      "Epoch 185/300\n",
      "Average training loss: 0.031970403616627056\n",
      "Average test loss: 0.006866677575641208\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03209166652626461\n",
      "Average test loss: 0.006877385094347928\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03201819495856762\n",
      "Average test loss: 0.006677664246824052\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03201096146305402\n",
      "Average test loss: 0.006723754218882985\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03194315503537655\n",
      "Average test loss: 0.0068061733920541075\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03192392644617292\n",
      "Average test loss: 0.006973951588074366\n",
      "Epoch 191/300\n",
      "Average training loss: 0.031948043935828736\n",
      "Average test loss: 0.006692536498523421\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0318909399178293\n",
      "Average test loss: 0.006746960283981429\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03187315080563227\n",
      "Average test loss: 0.006967885761625237\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03192096204724577\n",
      "Average test loss: 0.006814780973311928\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0318689286477036\n",
      "Average test loss: 0.006736403037276533\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03180557436744372\n",
      "Average test loss: 0.006732522618025542\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03187506539788511\n",
      "Average test loss: 0.006705550049328142\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031825817288623916\n",
      "Average test loss: 0.006773695134454303\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03179275189174546\n",
      "Average test loss: 0.006808116361498833\n",
      "Epoch 200/300\n",
      "Average training loss: 0.031764224612050584\n",
      "Average test loss: 0.012206575986411837\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03175272567073504\n",
      "Average test loss: 0.0067022824283275335\n",
      "Epoch 202/300\n",
      "Average training loss: 0.031789163281520205\n",
      "Average test loss: 0.006723368395947748\n",
      "Epoch 203/300\n",
      "Average training loss: 0.031796816983156735\n",
      "Average test loss: 0.006667453341186047\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03170434674951765\n",
      "Average test loss: 0.006785973497149017\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03174482782350646\n",
      "Average test loss: 0.006752187100549539\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03174096463455094\n",
      "Average test loss: 0.007198701214459207\n",
      "Epoch 207/300\n",
      "Average training loss: 0.031659293558862475\n",
      "Average test loss: 0.006963612514651484\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03168480448259248\n",
      "Average test loss: 0.006916925460928016\n",
      "Epoch 209/300\n",
      "Average training loss: 0.031736943561169836\n",
      "Average test loss: 0.0069144492579831015\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03164400508503119\n",
      "Average test loss: 0.007243031316747268\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03175116174088584\n",
      "Average test loss: 0.006819784792347087\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03161959505412314\n",
      "Average test loss: 0.006764992434117529\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031603670939803125\n",
      "Average test loss: 0.0067215265867610775\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0316451188425223\n",
      "Average test loss: 0.006754533810747994\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03158865848845906\n",
      "Average test loss: 0.0070064064156677985\n",
      "Epoch 216/300\n",
      "Average training loss: 0.031616788228352864\n",
      "Average test loss: 0.006725386991269059\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03162324408276213\n",
      "Average test loss: 0.006867193800707658\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031590579675303564\n",
      "Average test loss: 0.007083965793251991\n",
      "Epoch 219/300\n",
      "Average training loss: 0.031521708811322845\n",
      "Average test loss: 0.006985038799958097\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031476691404978434\n",
      "Average test loss: 0.006775111769222551\n",
      "Epoch 221/300\n",
      "Average training loss: 0.031493446060352856\n",
      "Average test loss: 0.006789425222410096\n",
      "Epoch 222/300\n",
      "Average training loss: 0.031480727854702205\n",
      "Average test loss: 0.006785456950051917\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03157531378335423\n",
      "Average test loss: 0.006905474825865693\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03150169614122974\n",
      "Average test loss: 0.007925303863154516\n",
      "Epoch 225/300\n",
      "Average training loss: 0.031453251557217704\n",
      "Average test loss: 0.006978098466164536\n",
      "Epoch 226/300\n",
      "Average training loss: 0.031535536714726024\n",
      "Average test loss: 0.006795959683342113\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03143345398041937\n",
      "Average test loss: 0.006832947788553105\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03137672933936119\n",
      "Average test loss: 0.00685618705012732\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03146544706159168\n",
      "Average test loss: 0.00683136892484294\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0314425713982847\n",
      "Average test loss: 0.006872375389354097\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03142171412540807\n",
      "Average test loss: 0.0068177111273010575\n",
      "Epoch 232/300\n",
      "Average training loss: 0.031376179572608735\n",
      "Average test loss: 0.006810802714278301\n",
      "Epoch 233/300\n",
      "Average training loss: 0.031357691983381904\n",
      "Average test loss: 0.006811281592481666\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03134645160039266\n",
      "Average test loss: 0.0070817001606855124\n",
      "Epoch 235/300\n",
      "Average training loss: 0.031384595930576326\n",
      "Average test loss: 0.006760829886628522\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03129962064160241\n",
      "Average test loss: 0.006863095845199294\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03145201492971844\n",
      "Average test loss: 0.006798297529419263\n",
      "Epoch 238/300\n",
      "Average training loss: 0.031283090912633475\n",
      "Average test loss: 0.006721934557788902\n",
      "Epoch 239/300\n",
      "Average training loss: 0.031346394615040886\n",
      "Average test loss: 0.006765914672364791\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03129758259985182\n",
      "Average test loss: 0.007017411530017853\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03127382111880515\n",
      "Average test loss: 0.006772730179131031\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03120172224773301\n",
      "Average test loss: 0.006736155304643843\n",
      "Epoch 243/300\n",
      "Average training loss: 0.031215484529733658\n",
      "Average test loss: 0.006842590119689703\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031262445522679226\n",
      "Average test loss: 0.0069379675458702775\n",
      "Epoch 245/300\n",
      "Average training loss: 0.031158826874362097\n",
      "Average test loss: 0.007120807062834501\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03123886335392793\n",
      "Average test loss: 0.006947301491681072\n",
      "Epoch 247/300\n",
      "Average training loss: 0.031165364038613107\n",
      "Average test loss: 0.006797260007096661\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03122811574074957\n",
      "Average test loss: 0.006854529234270255\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03122084411316448\n",
      "Average test loss: 0.006906471069074339\n",
      "Epoch 250/300\n",
      "Average training loss: 0.031273128936688106\n",
      "Average test loss: 0.0070132095565398535\n",
      "Epoch 251/300\n",
      "Average training loss: 0.031209769434399074\n",
      "Average test loss: 0.006753476592815584\n",
      "Epoch 252/300\n",
      "Average training loss: 0.031099393061465686\n",
      "Average test loss: 0.006828296694076723\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03115960035721461\n",
      "Average test loss: 0.006777306562496557\n",
      "Epoch 254/300\n",
      "Average training loss: 0.031137389848629633\n",
      "Average test loss: 0.006938904398017459\n",
      "Epoch 255/300\n",
      "Average training loss: 0.031125857111480502\n",
      "Average test loss: 0.006876087880382935\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03105565818813112\n",
      "Average test loss: 0.006970320738852024\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03135068588952224\n",
      "Average test loss: 0.006827896147138543\n",
      "Epoch 258/300\n",
      "Average training loss: 0.031076989099383355\n",
      "Average test loss: 0.006792365787757768\n",
      "Epoch 259/300\n",
      "Average training loss: 0.031056344411439365\n",
      "Average test loss: 0.0068671067564023865\n",
      "Epoch 260/300\n",
      "Average training loss: 0.031027116848362818\n",
      "Average test loss: 0.006969730814091034\n",
      "Epoch 261/300\n",
      "Average training loss: 0.031040370855066512\n",
      "Average test loss: 0.006835541633268197\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03104038096467654\n",
      "Average test loss: 0.006790240931428141\n",
      "Epoch 263/300\n",
      "Average training loss: 0.031044060786565145\n",
      "Average test loss: 0.0068677920065820216\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03103845943013827\n",
      "Average test loss: 0.00681507639752494\n",
      "Epoch 265/300\n",
      "Average training loss: 0.031048772401279873\n",
      "Average test loss: 0.006897376247164276\n",
      "Epoch 266/300\n",
      "Average training loss: 0.031006031294663748\n",
      "Average test loss: 0.007264891031715605\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03102667655216323\n",
      "Average test loss: 0.006975321095436812\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030953301512532765\n",
      "Average test loss: 0.006797282543033361\n",
      "Epoch 269/300\n",
      "Average training loss: 0.030957543392976124\n",
      "Average test loss: 0.006821240507894092\n",
      "Epoch 270/300\n",
      "Average training loss: 0.031018591043021945\n",
      "Average test loss: 0.007147320792906814\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030923838537600307\n",
      "Average test loss: 0.0069562367043561404\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03097651906973786\n",
      "Average test loss: 0.006849017380426327\n",
      "Epoch 273/300\n",
      "Average training loss: 0.030940952936808267\n",
      "Average test loss: 0.006810822653687662\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030907831365863482\n",
      "Average test loss: 0.0069966584456463655\n",
      "Epoch 275/300\n",
      "Average training loss: 0.030916770577430724\n",
      "Average test loss: 0.007002023280494743\n",
      "Epoch 276/300\n",
      "Average training loss: 0.030943557300501398\n",
      "Average test loss: 0.006980404988759094\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03093175064192878\n",
      "Average test loss: 0.00811575775841872\n",
      "Epoch 278/300\n",
      "Average training loss: 0.030891517084505823\n",
      "Average test loss: 0.007199238369034396\n",
      "Epoch 279/300\n",
      "Average training loss: 0.030895175258318582\n",
      "Average test loss: 0.006932114235642883\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03090661722752783\n",
      "Average test loss: 0.006837038531899453\n",
      "Epoch 281/300\n",
      "Average training loss: 0.030857778824037976\n",
      "Average test loss: 0.0072002356400092445\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03096004507276747\n",
      "Average test loss: 0.006788172692888313\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030802077415916656\n",
      "Average test loss: 0.0070260133966803555\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030795200518435904\n",
      "Average test loss: 0.006959404450737768\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030802253531085122\n",
      "Average test loss: 0.006854473225772381\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030836724787950517\n",
      "Average test loss: 0.006928459779669841\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0307970759206348\n",
      "Average test loss: 0.006861066195699904\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03072352594137192\n",
      "Average test loss: 0.006974824386338393\n",
      "Epoch 289/300\n",
      "Average training loss: 0.030896900276343026\n",
      "Average test loss: 0.006921553271926111\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03084316207551294\n",
      "Average test loss: 0.006976930891474088\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03074357939594322\n",
      "Average test loss: 0.00709120010294848\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030802053332328796\n",
      "Average test loss: 0.006846778251644638\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030808536738157272\n",
      "Average test loss: 0.007192452704120013\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03072837429245313\n",
      "Average test loss: 0.007107422754996353\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03073779912458526\n",
      "Average test loss: 0.006871404559248024\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030716532488663992\n",
      "Average test loss: 0.006906382819637656\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03072145254909992\n",
      "Average test loss: 0.0070659011912842596\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03069690502517753\n",
      "Average test loss: 0.007038563497778442\n",
      "Epoch 299/300\n",
      "Average training loss: 0.031622454630004036\n",
      "Average test loss: 0.006949587796297338\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03060641468067964\n",
      "Average test loss: 0.006780940440793832\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.20499353583653768\n",
      "Average test loss: 0.008344990654951995\n",
      "Epoch 2/300\n",
      "Average training loss: 0.050284072041511536\n",
      "Average test loss: 0.007235097110271454\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04403562900092867\n",
      "Average test loss: 0.006973227159844504\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04103859441479047\n",
      "Average test loss: 0.006532215080327457\n",
      "Epoch 5/300\n",
      "Average training loss: 0.038730118933651185\n",
      "Average test loss: 0.006330360948625538\n",
      "Epoch 6/300\n",
      "Average training loss: 0.037250028587049906\n",
      "Average test loss: 0.0060292085301544935\n",
      "Epoch 7/300\n",
      "Average training loss: 0.035880648805035484\n",
      "Average test loss: 0.006218544231106838\n",
      "Epoch 8/300\n",
      "Average training loss: 0.034951593319574994\n",
      "Average test loss: 0.005889145311382082\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03404749590655168\n",
      "Average test loss: 0.005674281027168036\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03342081172102027\n",
      "Average test loss: 0.005925931825406022\n",
      "Epoch 11/300\n",
      "Average training loss: 0.032717721045017245\n",
      "Average test loss: 0.005531598952909311\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03212305261525843\n",
      "Average test loss: 0.005297263932310873\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03159773258037037\n",
      "Average test loss: 0.005396099228825834\n",
      "Epoch 14/300\n",
      "Average training loss: 0.031079567664199406\n",
      "Average test loss: 0.005302723737640513\n",
      "Epoch 15/300\n",
      "Average training loss: 0.030627078958683544\n",
      "Average test loss: 0.005189490008685324\n",
      "Epoch 16/300\n",
      "Average training loss: 0.030256434319747817\n",
      "Average test loss: 0.005036112201296621\n",
      "Epoch 17/300\n",
      "Average training loss: 0.029866548231906362\n",
      "Average test loss: 0.004976919414268599\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02942370092537668\n",
      "Average test loss: 0.004873016658342547\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02916157282061047\n",
      "Average test loss: 0.0048533108615212974\n",
      "Epoch 20/300\n",
      "Average training loss: 0.028826381471421983\n",
      "Average test loss: 0.00473216374611689\n",
      "Epoch 21/300\n",
      "Average training loss: 0.028575455236766075\n",
      "Average test loss: 0.0046844780995614\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02829613482289844\n",
      "Average test loss: 0.004798985826886363\n",
      "Epoch 23/300\n",
      "Average training loss: 0.028080141888724434\n",
      "Average test loss: 0.004596151389388575\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02778827923039595\n",
      "Average test loss: 0.0045451233167615205\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02766444009873602\n",
      "Average test loss: 0.00452360446130236\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02739944619768196\n",
      "Average test loss: 0.004529486688060893\n",
      "Epoch 27/300\n",
      "Average training loss: 0.027240120983786052\n",
      "Average test loss: 0.004480261000287202\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02705038252969583\n",
      "Average test loss: 0.004367879508270158\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026886209863755436\n",
      "Average test loss: 0.004422672507249647\n",
      "Epoch 30/300\n",
      "Average training loss: 0.026737909946176742\n",
      "Average test loss: 0.00456437158708771\n",
      "Epoch 31/300\n",
      "Average training loss: 0.026556430081526437\n",
      "Average test loss: 0.0043705628361139035\n",
      "Epoch 32/300\n",
      "Average training loss: 0.026504679926567607\n",
      "Average test loss: 0.004452239244348473\n",
      "Epoch 33/300\n",
      "Average training loss: 0.026303553010026615\n",
      "Average test loss: 0.004313311131464111\n",
      "Epoch 34/300\n",
      "Average training loss: 0.026188860578669444\n",
      "Average test loss: 0.00421154400623507\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02609167866077688\n",
      "Average test loss: 0.004288725077278084\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025976781028840278\n",
      "Average test loss: 0.004221158393348257\n",
      "Epoch 37/300\n",
      "Average training loss: 0.025844680648710993\n",
      "Average test loss: 0.004250875118085079\n",
      "Epoch 38/300\n",
      "Average training loss: 0.025767348006367683\n",
      "Average test loss: 0.0041451285831216305\n",
      "Epoch 39/300\n",
      "Average training loss: 0.025692035143574078\n",
      "Average test loss: 0.004155863193174203\n",
      "Epoch 40/300\n",
      "Average training loss: 0.025612366929650306\n",
      "Average test loss: 0.004175015997969442\n",
      "Epoch 41/300\n",
      "Average training loss: 0.025519909863670667\n",
      "Average test loss: 0.004173572905775574\n",
      "Epoch 42/300\n",
      "Average training loss: 0.025417016158501306\n",
      "Average test loss: 0.004223293747132023\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02539452605942885\n",
      "Average test loss: 0.004119475559227996\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025372320547699927\n",
      "Average test loss: 0.004138581187774737\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02521846329503589\n",
      "Average test loss: 0.004055966466251347\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02513981355064445\n",
      "Average test loss: 0.004073716503050592\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0250778749767277\n",
      "Average test loss: 0.0040869417188482155\n",
      "Epoch 48/300\n",
      "Average training loss: 0.025026025997267828\n",
      "Average test loss: 0.004166937695816159\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024953411989741856\n",
      "Average test loss: 0.0040731559315075475\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024934254965848394\n",
      "Average test loss: 0.0040310767843491505\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02482016237411234\n",
      "Average test loss: 0.00422259184345603\n",
      "Epoch 52/300\n",
      "Average training loss: 0.024811992958188058\n",
      "Average test loss: 0.004541676921149095\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02473268880115615\n",
      "Average test loss: 0.004014845949080255\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02470688125325574\n",
      "Average test loss: 0.004024514612431328\n",
      "Epoch 55/300\n",
      "Average training loss: 0.024706142998403974\n",
      "Average test loss: 0.004117293302176727\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02459996250602934\n",
      "Average test loss: 0.004111299015995529\n",
      "Epoch 57/300\n",
      "Average training loss: 0.024540365083350075\n",
      "Average test loss: 0.004103299307119515\n",
      "Epoch 58/300\n",
      "Average training loss: 0.024582455136709744\n",
      "Average test loss: 0.0041038870000176956\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024505383716689217\n",
      "Average test loss: 0.00395374355899791\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02445632494158215\n",
      "Average test loss: 0.004068060377819671\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024441986522740787\n",
      "Average test loss: 0.0041699213164134155\n",
      "Epoch 62/300\n",
      "Average training loss: 0.024378288477659226\n",
      "Average test loss: 0.0040094546870225005\n",
      "Epoch 63/300\n",
      "Average training loss: 0.024364048161440426\n",
      "Average test loss: 0.0040613721584280334\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02430632268720203\n",
      "Average test loss: 0.004157649888760513\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024243800721234745\n",
      "Average test loss: 0.003944339576280779\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024245370929439864\n",
      "Average test loss: 0.26692651269170975\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024700341241227258\n",
      "Average test loss: 0.003995271175685856\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024153933674097062\n",
      "Average test loss: 0.004041585930933555\n",
      "Epoch 69/300\n",
      "Average training loss: 0.024088719165987437\n",
      "Average test loss: 0.003954075144396888\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024059861251049572\n",
      "Average test loss: 0.004002401637120379\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02405477659900983\n",
      "Average test loss: 0.004210897482931614\n",
      "Epoch 72/300\n",
      "Average training loss: 0.023999763867921298\n",
      "Average test loss: 0.003940988049531976\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02399695639477836\n",
      "Average test loss: 0.0040220417773558035\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02397170087363985\n",
      "Average test loss: 0.0039383472506370806\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023935794088575574\n",
      "Average test loss: 0.0040581343565136195\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02389870929883586\n",
      "Average test loss: 0.00394801662200027\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023881090364522402\n",
      "Average test loss: 0.003966827250189251\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02388320825000604\n",
      "Average test loss: 0.004060874371892876\n",
      "Epoch 79/300\n",
      "Average training loss: 0.023820920459098286\n",
      "Average test loss: 0.003968292481576403\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023807112316290537\n",
      "Average test loss: 0.003929201297048065\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023772088082300292\n",
      "Average test loss: 0.003994947358965874\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023736565116378994\n",
      "Average test loss: 0.003960884477943182\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02375129344397121\n",
      "Average test loss: 0.003977535885241297\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02368637846575843\n",
      "Average test loss: 0.0039986338207705155\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023671906891796324\n",
      "Average test loss: 0.003985359338422616\n",
      "Epoch 86/300\n",
      "Average training loss: 0.023636257214678658\n",
      "Average test loss: 0.003941659617755148\n",
      "Epoch 87/300\n",
      "Average training loss: 0.024014106588231193\n",
      "Average test loss: 0.003992167757617102\n",
      "Epoch 88/300\n",
      "Average training loss: 0.023589854559964603\n",
      "Average test loss: 0.003977929275068972\n",
      "Epoch 89/300\n",
      "Average training loss: 0.023584310645030606\n",
      "Average test loss: 0.003999829043944677\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023518957228296333\n",
      "Average test loss: 0.003977539259319504\n",
      "Epoch 91/300\n",
      "Average training loss: 0.023553532363639936\n",
      "Average test loss: 0.004009973102973567\n",
      "Epoch 92/300\n",
      "Average training loss: 0.023512604223357308\n",
      "Average test loss: 0.003938401244580746\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023470556939641635\n",
      "Average test loss: 0.003998507840351926\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02346527648799949\n",
      "Average test loss: 0.003944291122050749\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02343316120405992\n",
      "Average test loss: 0.003925179162580106\n",
      "Epoch 96/300\n",
      "Average training loss: 0.023464300783144102\n",
      "Average test loss: 0.003976897302187151\n",
      "Epoch 97/300\n",
      "Average training loss: 0.023430783212184905\n",
      "Average test loss: 0.0039851325212253465\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023379057308038076\n",
      "Average test loss: 0.003987125241094166\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023355690015686884\n",
      "Average test loss: 0.004286027051922348\n",
      "Epoch 100/300\n",
      "Average training loss: 0.023346030755175485\n",
      "Average test loss: 0.003993838242358632\n",
      "Epoch 101/300\n",
      "Average training loss: 0.023314888085756037\n",
      "Average test loss: 0.004028466406795714\n",
      "Epoch 102/300\n",
      "Average training loss: 0.023381392551792994\n",
      "Average test loss: 0.0039434808604419235\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02328726002905104\n",
      "Average test loss: 0.003994936832744214\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02333015717731582\n",
      "Average test loss: 0.0532342165377405\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02373632699913449\n",
      "Average test loss: 0.003973718308947153\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02321174885167016\n",
      "Average test loss: 0.003931833033553428\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023186164576146338\n",
      "Average test loss: 0.00392352311209672\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023181332195798557\n",
      "Average test loss: 0.00404185412141184\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023203102505869336\n",
      "Average test loss: 0.003979461008268926\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02314850722418891\n",
      "Average test loss: 0.003994417049197687\n",
      "Epoch 111/300\n",
      "Average training loss: 0.023136807655294737\n",
      "Average test loss: 0.003940475050359964\n",
      "Epoch 112/300\n",
      "Average training loss: 0.023145284497075612\n",
      "Average test loss: 0.004026267593105634\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023085060313344\n",
      "Average test loss: 0.00398017900188764\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023130321729514333\n",
      "Average test loss: 0.00396037288237777\n",
      "Epoch 115/300\n",
      "Average training loss: 0.023098085173302228\n",
      "Average test loss: 0.003999541842689117\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02307060963743263\n",
      "Average test loss: 0.0040077776941988205\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023045407170222864\n",
      "Average test loss: 0.003948309262386627\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023041857533984713\n",
      "Average test loss: 0.00413702918547723\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023010227746433683\n",
      "Average test loss: 0.004009736976689763\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023015116391910447\n",
      "Average test loss: 0.004005356428937779\n",
      "Epoch 121/300\n",
      "Average training loss: 0.022974695364634198\n",
      "Average test loss: 0.003997127305302355\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023015645436114736\n",
      "Average test loss: 0.004047605798890194\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022940117951896457\n",
      "Average test loss: 0.003956127838335104\n",
      "Epoch 124/300\n",
      "Average training loss: 0.022953194780482186\n",
      "Average test loss: 0.004022000398486852\n",
      "Epoch 125/300\n",
      "Average training loss: 0.022913427031702467\n",
      "Average test loss: 0.003983281266565124\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0229207065668371\n",
      "Average test loss: 0.003992861693518029\n",
      "Epoch 127/300\n",
      "Average training loss: 0.022893342266480127\n",
      "Average test loss: 0.00405681521900826\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023083090274698206\n",
      "Average test loss: 0.0040346365223328275\n",
      "Epoch 129/300\n",
      "Average training loss: 0.022852477421363194\n",
      "Average test loss: 0.00403177064905564\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02282800895886289\n",
      "Average test loss: 0.004009364918495218\n",
      "Epoch 131/300\n",
      "Average training loss: 0.022825181227591303\n",
      "Average test loss: 0.003959080582691564\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02282138899465402\n",
      "Average test loss: 0.003991682700398896\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02280942697574695\n",
      "Average test loss: 0.004101649985959133\n",
      "Epoch 134/300\n",
      "Average training loss: 0.022793525038494005\n",
      "Average test loss: 0.004106373630671038\n",
      "Epoch 135/300\n",
      "Average training loss: 0.022797835825218094\n",
      "Average test loss: 0.004026609977707267\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022777146010763114\n",
      "Average test loss: 0.0040391230556286045\n",
      "Epoch 137/300\n",
      "Average training loss: 0.022763048519690833\n",
      "Average test loss: 0.0039632081708146465\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022730165267984073\n",
      "Average test loss: 0.004063845393351383\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02274611563152737\n",
      "Average test loss: 0.00424099292854468\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022711873004833856\n",
      "Average test loss: 0.003936630363886555\n",
      "Epoch 141/300\n",
      "Average training loss: 0.022698057250844107\n",
      "Average test loss: 0.00396003557762338\n",
      "Epoch 142/300\n",
      "Average training loss: 0.022674025893211366\n",
      "Average test loss: 0.003969230852607223\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022662242884437245\n",
      "Average test loss: 0.0040476131882104605\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022680159687995912\n",
      "Average test loss: 0.004097820384220944\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022670752435922623\n",
      "Average test loss: 0.003950049261666006\n",
      "Epoch 146/300\n",
      "Average training loss: 0.022634478623668353\n",
      "Average test loss: 0.003985602345731524\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02262729638649358\n",
      "Average test loss: 0.004015790327969525\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022606639603773755\n",
      "Average test loss: 0.004231557933406698\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022586293056607247\n",
      "Average test loss: 0.0040417704602910415\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022619838120208846\n",
      "Average test loss: 0.004862233901396394\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02256831030216482\n",
      "Average test loss: 0.004173517098029454\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02257218962907791\n",
      "Average test loss: 0.0039945725517140495\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022559033448497454\n",
      "Average test loss: 0.004005427875452571\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022555541336536408\n",
      "Average test loss: 0.004051654249015782\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02255773504409525\n",
      "Average test loss: 0.004159243787121442\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022500786170363427\n",
      "Average test loss: 0.003980170490013229\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02251586265199714\n",
      "Average test loss: 0.003979997744473318\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02250424089531104\n",
      "Average test loss: 0.004019209023358093\n",
      "Epoch 159/300\n",
      "Average training loss: 0.022575622608264286\n",
      "Average test loss: 0.00401880638094412\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02245246262848377\n",
      "Average test loss: 0.0039995096255507736\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02247725672357612\n",
      "Average test loss: 0.004160518048952023\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02257931749936607\n",
      "Average test loss: 0.004020061019394133\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022449493476086192\n",
      "Average test loss: 0.004148217286914587\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02241208326154285\n",
      "Average test loss: 0.003993119384265608\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02240644870698452\n",
      "Average test loss: 0.004009834631863568\n",
      "Epoch 166/300\n",
      "Average training loss: 0.022438784667187267\n",
      "Average test loss: 0.0040231898077246215\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0223943646715747\n",
      "Average test loss: 0.0040956250861701035\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022386782250470586\n",
      "Average test loss: 0.004138113659289148\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02238303688665231\n",
      "Average test loss: 0.004010602735603849\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022366309344768524\n",
      "Average test loss: 0.004038977650304636\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022439825892448424\n",
      "Average test loss: 0.004006014715259274\n",
      "Epoch 172/300\n",
      "Average training loss: 0.022345186962021722\n",
      "Average test loss: 0.004080844569123453\n",
      "Epoch 173/300\n",
      "Average training loss: 0.022325413731237254\n",
      "Average test loss: 0.004019927048434814\n",
      "Epoch 174/300\n",
      "Average training loss: 0.022312334823111693\n",
      "Average test loss: 0.004026062383419938\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02230691418548425\n",
      "Average test loss: 0.004065487667504284\n",
      "Epoch 176/300\n",
      "Average training loss: 0.022335258319973947\n",
      "Average test loss: 0.004100300318458014\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022319248661398887\n",
      "Average test loss: 0.004076725450240903\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02226918954650561\n",
      "Average test loss: 0.004015650566667318\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022300810638401242\n",
      "Average test loss: 0.004074493938436111\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02228374228378137\n",
      "Average test loss: 0.004007901993476682\n",
      "Epoch 181/300\n",
      "Average training loss: 0.022230851294265854\n",
      "Average test loss: 0.003993964732107189\n",
      "Epoch 182/300\n",
      "Average training loss: 0.022250221868356068\n",
      "Average test loss: 0.004054628358532985\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02222543318155739\n",
      "Average test loss: 0.004102661789912317\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022257579675979085\n",
      "Average test loss: 0.003955355787649751\n",
      "Epoch 185/300\n",
      "Average training loss: 0.022244460451934074\n",
      "Average test loss: 0.004101363771077659\n",
      "Epoch 186/300\n",
      "Average training loss: 0.022217950180172922\n",
      "Average test loss: 0.004071999672800302\n",
      "Epoch 187/300\n",
      "Average training loss: 0.022181489997439914\n",
      "Average test loss: 0.004062586880185538\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022209391393595273\n",
      "Average test loss: 0.004040904177973668\n",
      "Epoch 189/300\n",
      "Average training loss: 0.022194201519091922\n",
      "Average test loss: 0.00411677971502973\n",
      "Epoch 190/300\n",
      "Average training loss: 0.022196478898326554\n",
      "Average test loss: 0.004007843362374438\n",
      "Epoch 191/300\n",
      "Average training loss: 0.022193794752160708\n",
      "Average test loss: 0.0040497086031569375\n",
      "Epoch 192/300\n",
      "Average training loss: 0.022162004886402024\n",
      "Average test loss: 0.004096322758123279\n",
      "Epoch 193/300\n",
      "Average training loss: 0.022140341993835237\n",
      "Average test loss: 0.004035139431556066\n",
      "Epoch 194/300\n",
      "Average training loss: 0.022135460590322814\n",
      "Average test loss: 0.0042902363191048305\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02215693196323183\n",
      "Average test loss: 0.00412408057310515\n",
      "Epoch 196/300\n",
      "Average training loss: 0.022139392503433757\n",
      "Average test loss: 0.004132938009583287\n",
      "Epoch 197/300\n",
      "Average training loss: 0.022128754713469083\n",
      "Average test loss: 0.0040417026144762835\n",
      "Epoch 198/300\n",
      "Average training loss: 0.022093889536129105\n",
      "Average test loss: 0.003996185115228096\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02209585715499189\n",
      "Average test loss: 0.004019873715109295\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02210251248876254\n",
      "Average test loss: 0.004112290453786652\n",
      "Epoch 201/300\n",
      "Average training loss: 0.022057221876250373\n",
      "Average test loss: 0.004110375709417793\n",
      "Epoch 202/300\n",
      "Average training loss: 0.022086246179209814\n",
      "Average test loss: 0.004048867138309611\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02211709593402015\n",
      "Average test loss: 0.004004294532454676\n",
      "Epoch 204/300\n",
      "Average training loss: 0.022040909227397706\n",
      "Average test loss: 0.0040696058387143744\n",
      "Epoch 205/300\n",
      "Average training loss: 0.022051731412609417\n",
      "Average test loss: 0.004160788180513514\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0220738871710168\n",
      "Average test loss: 0.004039926983830002\n",
      "Epoch 207/300\n",
      "Average training loss: 0.022050666977961857\n",
      "Average test loss: 0.004230750784070955\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022034697910149892\n",
      "Average test loss: 0.0040458301641047\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022064720913767814\n",
      "Average test loss: 0.004043218137075504\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021984147061904272\n",
      "Average test loss: 0.004083722407619159\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021979481466942362\n",
      "Average test loss: 0.004007224618146817\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02200156155559752\n",
      "Average test loss: 0.004061781936428613\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022007394840319952\n",
      "Average test loss: 0.004128846817132499\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021941336904962857\n",
      "Average test loss: 0.0041371733273069065\n",
      "Epoch 215/300\n",
      "Average training loss: 0.021965460374951364\n",
      "Average test loss: 0.0040909249016808135\n",
      "Epoch 216/300\n",
      "Average training loss: 0.021950985785987642\n",
      "Average test loss: 0.004063792040157649\n",
      "Epoch 217/300\n",
      "Average training loss: 0.021960456762048934\n",
      "Average test loss: 0.004080893019421233\n",
      "Epoch 218/300\n",
      "Average training loss: 0.022000525110297733\n",
      "Average test loss: 0.0040614791962628564\n",
      "Epoch 219/300\n",
      "Average training loss: 0.021918375492095948\n",
      "Average test loss: 0.004069779468493329\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021933951791789797\n",
      "Average test loss: 0.004124088465753529\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021905468811591465\n",
      "Average test loss: 0.004165603182589015\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02194428222709232\n",
      "Average test loss: 0.004085418009095722\n",
      "Epoch 223/300\n",
      "Average training loss: 0.021884325017531712\n",
      "Average test loss: 0.004152958375505275\n",
      "Epoch 224/300\n",
      "Average training loss: 0.021922802093956206\n",
      "Average test loss: 0.004066973288026121\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021910827160709433\n",
      "Average test loss: 0.004248930668251382\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021890982334812483\n",
      "Average test loss: 0.004138702110283905\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021865741168459258\n",
      "Average test loss: 0.0040313981819070045\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021877524680561488\n",
      "Average test loss: 0.004100098665803671\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02187405972348319\n",
      "Average test loss: 0.004071340351055066\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02186097291111946\n",
      "Average test loss: 0.004103378529556923\n",
      "Epoch 231/300\n",
      "Average training loss: 0.021879081262482537\n",
      "Average test loss: 0.004044575783941481\n",
      "Epoch 232/300\n",
      "Average training loss: 0.021828964763217502\n",
      "Average test loss: 0.004122921794239017\n",
      "Epoch 233/300\n",
      "Average training loss: 0.021854533223642243\n",
      "Average test loss: 0.004516806290381484\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02181356680641572\n",
      "Average test loss: 0.004170502653966348\n",
      "Epoch 235/300\n",
      "Average training loss: 0.021871696280108557\n",
      "Average test loss: 0.0040769203218321\n",
      "Epoch 236/300\n",
      "Average training loss: 0.021880851727392937\n",
      "Average test loss: 0.004094734508751167\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02181757885052098\n",
      "Average test loss: 0.004118370771822002\n",
      "Epoch 238/300\n",
      "Average training loss: 0.021784254626267485\n",
      "Average test loss: 0.0042306682063887515\n",
      "Epoch 239/300\n",
      "Average training loss: 0.021825038709574276\n",
      "Average test loss: 0.0041156112353007\n",
      "Epoch 240/300\n",
      "Average training loss: 0.021794917040401034\n",
      "Average test loss: 0.004043868222584327\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02177352421813541\n",
      "Average test loss: 0.004063577058828539\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021792534175846312\n",
      "Average test loss: 0.004037843527065383\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021816341461406815\n",
      "Average test loss: 0.004128054190840986\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021763029913107555\n",
      "Average test loss: 0.004106571487875448\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02175505244235198\n",
      "Average test loss: 0.004102476850358977\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021754593213399252\n",
      "Average test loss: 0.004312834315001964\n",
      "Epoch 247/300\n",
      "Average training loss: 0.021760299146175385\n",
      "Average test loss: 0.004362194902987944\n",
      "Epoch 248/300\n",
      "Average training loss: 0.021767312485310768\n",
      "Average test loss: 0.004240907739847899\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021732805634538332\n",
      "Average test loss: 0.004084431011643674\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021761721200413173\n",
      "Average test loss: 0.004056251003924343\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021734576803114677\n",
      "Average test loss: 0.004097532432940271\n",
      "Epoch 252/300\n",
      "Average training loss: 0.021695486328668065\n",
      "Average test loss: 0.004069696959315075\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021731572709149785\n",
      "Average test loss: 0.004115726113112436\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021733770453267628\n",
      "Average test loss: 0.0042846495616767144\n",
      "Epoch 255/300\n",
      "Average training loss: 0.021719580478138392\n",
      "Average test loss: 0.004960685188157691\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021715963080525397\n",
      "Average test loss: 0.004117751146770186\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021708340133229892\n",
      "Average test loss: 0.004113041914378604\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021705494064423774\n",
      "Average test loss: 0.004283229558107754\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02168081267674764\n",
      "Average test loss: 0.0040994552216596074\n",
      "Epoch 260/300\n",
      "Average training loss: 0.021677236694428656\n",
      "Average test loss: 0.004085221653390262\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021690049424767495\n",
      "Average test loss: 0.0040143452630274825\n",
      "Epoch 262/300\n",
      "Average training loss: 0.021677069620953667\n",
      "Average test loss: 0.004168349556624889\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021661715366774137\n",
      "Average test loss: 0.004141837808820936\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02166103800634543\n",
      "Average test loss: 0.004069989073607657\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021654487237334253\n",
      "Average test loss: 0.004092946219568451\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021617257503999603\n",
      "Average test loss: 0.004220128607625762\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021662962530222204\n",
      "Average test loss: 0.004239739777520299\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0216400296025806\n",
      "Average test loss: 0.004254629781262742\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021634718098574215\n",
      "Average test loss: 0.004147926464262936\n",
      "Epoch 270/300\n",
      "Average training loss: 0.021677754253149033\n",
      "Average test loss: 0.004075128651741478\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021603075161576273\n",
      "Average test loss: 0.0041342671751562095\n",
      "Epoch 272/300\n",
      "Average training loss: 0.021605409064226682\n",
      "Average test loss: 0.004142836773147186\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021613739874627856\n",
      "Average test loss: 0.004088028387063079\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021585106424159475\n",
      "Average test loss: 0.004097531124949455\n",
      "Epoch 275/300\n",
      "Average training loss: 0.021603795318139923\n",
      "Average test loss: 0.0041619082405749295\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021588695754607517\n",
      "Average test loss: 0.004128886726167467\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021570582904749447\n",
      "Average test loss: 0.004194101333204243\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02158882379697429\n",
      "Average test loss: 0.004135505706692735\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02159431098235978\n",
      "Average test loss: 0.004936512702455123\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021648168991009395\n",
      "Average test loss: 0.004163336700449387\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02154438717332151\n",
      "Average test loss: 0.004095849904956089\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0215298695349031\n",
      "Average test loss: 0.004151244364161458\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021577651558650865\n",
      "Average test loss: 0.004134355156371991\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021584437605407503\n",
      "Average test loss: 0.004090600732300016\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021563373732897972\n",
      "Average test loss: 0.004192055341891116\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02152096222506629\n",
      "Average test loss: 0.0041027127914130684\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02156020314494769\n",
      "Average test loss: 0.004210281201534801\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02153867935637633\n",
      "Average test loss: 0.004078447973355651\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02149132231871287\n",
      "Average test loss: 0.004171577617112133\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02150360575152768\n",
      "Average test loss: 0.004276213602887259\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0215055819766389\n",
      "Average test loss: 0.004118947966438201\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0215415472123358\n",
      "Average test loss: 0.004130371531264649\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02149495519200961\n",
      "Average test loss: 0.0041422266355819175\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021492878028088146\n",
      "Average test loss: 0.00410754607597159\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021510174741347633\n",
      "Average test loss: 0.005145816116697259\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02150708577202426\n",
      "Average test loss: 0.004120591504085395\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02150350227786435\n",
      "Average test loss: 0.004242515002687772\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02143372106552124\n",
      "Average test loss: 0.004124421836187442\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02149157641331355\n",
      "Average test loss: 0.004148964518888129\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02149322507613235\n",
      "Average test loss: 0.0041434233374893666\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.18745981168084674\n",
      "Average test loss: 0.0063997663801742925\n",
      "Epoch 2/300\n",
      "Average training loss: 0.043485457009739346\n",
      "Average test loss: 0.005512573172234827\n",
      "Epoch 3/300\n",
      "Average training loss: 0.037862205064959\n",
      "Average test loss: 0.005309184257768922\n",
      "Epoch 4/300\n",
      "Average training loss: 0.034733094301488666\n",
      "Average test loss: 0.005219314250888096\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03249956773718198\n",
      "Average test loss: 0.004892866016676029\n",
      "Epoch 6/300\n",
      "Average training loss: 0.031007235921091503\n",
      "Average test loss: 0.004911862492975262\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02965157980720202\n",
      "Average test loss: 0.004653845580501689\n",
      "Epoch 8/300\n",
      "Average training loss: 0.028524170781175296\n",
      "Average test loss: 0.004426341137538353\n",
      "Epoch 9/300\n",
      "Average training loss: 0.027704503585894904\n",
      "Average test loss: 0.004230107309503688\n",
      "Epoch 10/300\n",
      "Average training loss: 0.026915964441166985\n",
      "Average test loss: 0.004181327398866415\n",
      "Epoch 11/300\n",
      "Average training loss: 0.026224993452429773\n",
      "Average test loss: 0.004093006814933486\n",
      "Epoch 12/300\n",
      "Average training loss: 0.025665811308556132\n",
      "Average test loss: 0.004274742967552609\n",
      "Epoch 13/300\n",
      "Average training loss: 0.025082130991750294\n",
      "Average test loss: 0.003858995818429523\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02464129548602634\n",
      "Average test loss: 0.00379779298355182\n",
      "Epoch 15/300\n",
      "Average training loss: 0.024218084921439488\n",
      "Average test loss: 0.003696596468074454\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02378770541316933\n",
      "Average test loss: 0.0035936647028558784\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02342853534552786\n",
      "Average test loss: 0.003589994648678435\n",
      "Epoch 18/300\n",
      "Average training loss: 0.023089675476153693\n",
      "Average test loss: 0.0035231418820718926\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022733529403805734\n",
      "Average test loss: 0.0035681804846972227\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022445431608292793\n",
      "Average test loss: 0.0034128677629762225\n",
      "Epoch 21/300\n",
      "Average training loss: 0.022231509364313548\n",
      "Average test loss: 0.003313180591290196\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021967314210202957\n",
      "Average test loss: 0.0036949130485041276\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021798764291736814\n",
      "Average test loss: 0.003332101347649263\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021546695955925517\n",
      "Average test loss: 0.003122595434801446\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02134901992397176\n",
      "Average test loss: 0.003208404410423504\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021200881380173894\n",
      "Average test loss: 0.0031031969338655473\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021050745932592287\n",
      "Average test loss: 0.003116851583744089\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02089339058101177\n",
      "Average test loss: 0.003176343284547329\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02074179519381788\n",
      "Average test loss: 0.00301307859002716\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02064537764257855\n",
      "Average test loss: 0.0032278782274160122\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020503166026539274\n",
      "Average test loss: 0.0029290633497552738\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020391393040617305\n",
      "Average test loss: 0.0029983887978725964\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020315851360559464\n",
      "Average test loss: 0.0029700064381791486\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020218193501234053\n",
      "Average test loss: 0.002855610378707449\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02008716387881173\n",
      "Average test loss: 0.0029390164274308416\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020012850178612603\n",
      "Average test loss: 0.002953987225683199\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019952515459722944\n",
      "Average test loss: 0.003315178906545043\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01988441197739707\n",
      "Average test loss: 0.002841170260889663\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019799562036991118\n",
      "Average test loss: 0.002846596625737018\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01971849949989054\n",
      "Average test loss: 0.0028403314167840614\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019703707464039325\n",
      "Average test loss: 0.002846095038577914\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019613395993908246\n",
      "Average test loss: 0.0028090490866452458\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01957011242542002\n",
      "Average test loss: 0.0027851315540158086\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019473697798119652\n",
      "Average test loss: 0.0029168848912749026\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01943064844359954\n",
      "Average test loss: 0.0027722202266256013\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01939373248649968\n",
      "Average test loss: 0.0027563094353924194\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01933531365129683\n",
      "Average test loss: 0.002831939238227076\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01927940203415023\n",
      "Average test loss: 0.0027377954760773315\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019243201893236903\n",
      "Average test loss: 0.002849817435567578\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019199903330869145\n",
      "Average test loss: 0.0027324130243311325\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01915379950735304\n",
      "Average test loss: 0.0028250636940615045\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019111389893624518\n",
      "Average test loss: 0.0027260405454370713\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0190676807794306\n",
      "Average test loss: 0.002796660138294101\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019027673290835486\n",
      "Average test loss: 0.0027725408434246977\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018991161523593796\n",
      "Average test loss: 0.002747867135124074\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018963507963551417\n",
      "Average test loss: 0.0027327647801074716\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018935707277721828\n",
      "Average test loss: 0.0027079653705780705\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01890253325800101\n",
      "Average test loss: 0.0027390039109935365\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0188493372797966\n",
      "Average test loss: 0.0027348052786870134\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018837686700953378\n",
      "Average test loss: 0.0026810166794392796\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01882395236690839\n",
      "Average test loss: 0.002730059679183695\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018788909652994738\n",
      "Average test loss: 0.002831584678341945\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018742701239056057\n",
      "Average test loss: 0.002757856708019972\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018712522317965826\n",
      "Average test loss: 0.002686142586171627\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01868441926108466\n",
      "Average test loss: 0.0027082682963874604\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018654067900445725\n",
      "Average test loss: 0.002729374395372967\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018630498117870754\n",
      "Average test loss: 0.002678598799639278\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018596896150873767\n",
      "Average test loss: 0.002668370236849619\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018587301357752748\n",
      "Average test loss: 0.0026923167662074167\n",
      "Epoch 70/300\n",
      "Average training loss: 0.018589639842510223\n",
      "Average test loss: 0.002750986163607902\n",
      "Epoch 71/300\n",
      "Average training loss: 0.018547598960498967\n",
      "Average test loss: 0.003220954306837585\n",
      "Epoch 72/300\n",
      "Average training loss: 0.018499067828059195\n",
      "Average test loss: 0.0030342707956830662\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01851858435985115\n",
      "Average test loss: 0.002721694993476073\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018467215448617937\n",
      "Average test loss: 0.0027102122989793617\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018416095188922354\n",
      "Average test loss: 0.0026689500069866576\n",
      "Epoch 76/300\n",
      "Average training loss: 0.018414499163627626\n",
      "Average test loss: 0.0026933858452571763\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01840737165013949\n",
      "Average test loss: 0.0027630410763538546\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018379762361447016\n",
      "Average test loss: 0.002739518854353163\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018365544996327823\n",
      "Average test loss: 0.0026600707806646823\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018765916541218758\n",
      "Average test loss: 0.002744004096628891\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018306221394075287\n",
      "Average test loss: 0.0026810230821785\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018283867725067668\n",
      "Average test loss: 0.002653027747033371\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01827236267262035\n",
      "Average test loss: 0.0028320793606754807\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018231907836265035\n",
      "Average test loss: 0.002682387239817116\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01824334249397119\n",
      "Average test loss: 0.0028295909652693405\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01825014408843385\n",
      "Average test loss: 0.0026774242137455277\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01817994898143742\n",
      "Average test loss: 0.0026874010081713398\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01817941804147429\n",
      "Average test loss: 0.0026918453714913793\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01819260559976101\n",
      "Average test loss: 0.0026748979820145503\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018133912634518412\n",
      "Average test loss: 0.0026899830918345187\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018131728019979265\n",
      "Average test loss: 0.0026643325398779576\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018141649674210284\n",
      "Average test loss: 0.0027123188957985903\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018122889131307603\n",
      "Average test loss: 0.002730502175995045\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018079434229267968\n",
      "Average test loss: 0.0027643778565236265\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018092761519882414\n",
      "Average test loss: 0.0027060820328899554\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018053707955612078\n",
      "Average test loss: 0.002710157266507546\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018041110512283114\n",
      "Average test loss: 0.0026898847267859513\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018015013307332992\n",
      "Average test loss: 0.002683264528711637\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018001076785226663\n",
      "Average test loss: 0.0027489747878991894\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0179957189510266\n",
      "Average test loss: 0.002664327761365308\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018001625390516387\n",
      "Average test loss: 0.0026812030596451626\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017973979391157628\n",
      "Average test loss: 0.0026963565105365384\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017939946017331548\n",
      "Average test loss: 0.0027143275326945715\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017954669126205976\n",
      "Average test loss: 0.002688404339262181\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017909482581747904\n",
      "Average test loss: 0.0027031554501089784\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01790613954514265\n",
      "Average test loss: 0.0027472715679970052\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017909620804919136\n",
      "Average test loss: 0.0026978045132839018\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0178733214015762\n",
      "Average test loss: 0.0027004526570025417\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017850384447309707\n",
      "Average test loss: 0.004128785330802202\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017862387356658776\n",
      "Average test loss: 0.0027040139910661514\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0178133525600036\n",
      "Average test loss: 0.0027442258563306595\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017824294411473804\n",
      "Average test loss: 0.0027557934270136886\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017818844565086894\n",
      "Average test loss: 0.0027650362085551022\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017803348960975806\n",
      "Average test loss: 0.002735146467677421\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017789361935522822\n",
      "Average test loss: 0.0026688376224289338\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017762446287605496\n",
      "Average test loss: 0.002788306987947888\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017777156838112408\n",
      "Average test loss: 0.0029514435426228574\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017739136282768516\n",
      "Average test loss: 0.0026813804532090824\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017716700086990993\n",
      "Average test loss: 0.002680473520110051\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01771401728524102\n",
      "Average test loss: 0.0027439893735572694\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017727714190880456\n",
      "Average test loss: 0.0026949355699535872\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017692879345681933\n",
      "Average test loss: 0.0027168711485962074\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01770107298427158\n",
      "Average test loss: 0.002693913992908266\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01771495024694337\n",
      "Average test loss: 0.0026889256817392176\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01765789387209548\n",
      "Average test loss: 0.002682302995481425\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01765640837285254\n",
      "Average test loss: 0.0027092952109459374\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017635664951470162\n",
      "Average test loss: 0.002724127729319864\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01765035917196009\n",
      "Average test loss: 0.0026749325049006275\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017621489039725727\n",
      "Average test loss: 0.00279537290148437\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017633497760527663\n",
      "Average test loss: 0.0027511310186237098\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017601438840230306\n",
      "Average test loss: 0.0027238783937775426\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017593665862249003\n",
      "Average test loss: 0.0026976541632579435\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017574413569437133\n",
      "Average test loss: 0.0027018128672821654\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01759227356976933\n",
      "Average test loss: 0.0027974442943102785\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01758244921763738\n",
      "Average test loss: 0.002710800104153653\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017586573994821973\n",
      "Average test loss: 0.002720833890967899\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01752727625436253\n",
      "Average test loss: 0.002694090069892506\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017540383067395953\n",
      "Average test loss: 0.0027261724008454218\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01751894992755519\n",
      "Average test loss: 0.0027386628455585904\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017528152086668546\n",
      "Average test loss: 0.002821416570701533\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01750803513162666\n",
      "Average test loss: 0.002702767289450599\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017508721005585457\n",
      "Average test loss: 0.0027566272788163686\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0174727493673563\n",
      "Average test loss: 0.0027990034317804707\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01749626854724354\n",
      "Average test loss: 0.002752040363434288\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017485577179325952\n",
      "Average test loss: 0.0027149016824033526\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017464652314782142\n",
      "Average test loss: 0.0026939305503749186\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01745201152149174\n",
      "Average test loss: 0.0028002654779702425\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01745932304693593\n",
      "Average test loss: 0.0026736794917119875\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017429637398984697\n",
      "Average test loss: 0.002702707028223409\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017425079981486003\n",
      "Average test loss: 0.0027518146404375632\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017393441549605794\n",
      "Average test loss: 0.0027234586040592854\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017398306551906797\n",
      "Average test loss: 0.0027656733439200454\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01742128204057614\n",
      "Average test loss: 0.0027439219483898747\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017431153755221102\n",
      "Average test loss: 0.0026938789448597367\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017379582587215635\n",
      "Average test loss: 0.00275006710489591\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01739199985149834\n",
      "Average test loss: 0.002785451378259394\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017350429221159883\n",
      "Average test loss: 0.002736697838538223\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017355432480573654\n",
      "Average test loss: 0.0027175743822008373\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017321613791916105\n",
      "Average test loss: 0.0027511369929545454\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01733626489672396\n",
      "Average test loss: 0.002762788988649845\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017319846590360007\n",
      "Average test loss: 0.0027281402107328175\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017323438535133996\n",
      "Average test loss: 0.002706188611065348\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01732401769442691\n",
      "Average test loss: 0.002721125392243266\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01730447913126813\n",
      "Average test loss: 0.002793292955805858\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017297475733690792\n",
      "Average test loss: 0.0027782998558961683\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017311865573955906\n",
      "Average test loss: 0.0027888463886661663\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017265021526151232\n",
      "Average test loss: 0.0028008136689249012\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017280813809070324\n",
      "Average test loss: 0.0027964770841515728\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01727833415236738\n",
      "Average test loss: 0.002749481745271219\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017273795433342457\n",
      "Average test loss: 0.002896365407647358\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017254527265826863\n",
      "Average test loss: 0.002702954881100191\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01732154964407285\n",
      "Average test loss: 0.0027390980672919087\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01723478983177079\n",
      "Average test loss: 0.0027945394619471498\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01724645100037257\n",
      "Average test loss: 0.002858569157620271\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017234572246670722\n",
      "Average test loss: 0.003026289398678475\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017229298212462\n",
      "Average test loss: 0.002808239062834117\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017216677655776343\n",
      "Average test loss: 0.0027434060667744942\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01721490218407578\n",
      "Average test loss: 0.002724748342504932\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01720887115266588\n",
      "Average test loss: 0.003355647924873564\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017198725140757033\n",
      "Average test loss: 0.0027989359746376672\n",
      "Epoch 181/300\n",
      "Average training loss: 0.017188686822023656\n",
      "Average test loss: 0.07220238253143099\n",
      "Epoch 182/300\n",
      "Average training loss: 0.017254702425665326\n",
      "Average test loss: 0.0027425511330366136\n",
      "Epoch 183/300\n",
      "Average training loss: 0.017176437680919966\n",
      "Average test loss: 0.0027543871585900587\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017164731744262907\n",
      "Average test loss: 0.0027655168496486214\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01714542607880301\n",
      "Average test loss: 0.002747738346250521\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01715989980267154\n",
      "Average test loss: 0.002703616135960652\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017160322757230864\n",
      "Average test loss: 0.0027354055895573563\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017145919925636716\n",
      "Average test loss: 0.0027680220417678355\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01714661189996534\n",
      "Average test loss: 0.002859207010310557\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017144329683648214\n",
      "Average test loss: 0.0027165924116141268\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017099400830765565\n",
      "Average test loss: 0.002785323602457841\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017131894538799923\n",
      "Average test loss: 0.0027317336986048355\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017129314202401373\n",
      "Average test loss: 0.0027955838286628327\n",
      "Epoch 194/300\n",
      "Average training loss: 0.017087589389748043\n",
      "Average test loss: 0.0028007530190257562\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017105856148733032\n",
      "Average test loss: 0.0027658742459283933\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01710386866496669\n",
      "Average test loss: 0.0028409714504248566\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0170766989853647\n",
      "Average test loss: 0.0027077867856456174\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017113023714058928\n",
      "Average test loss: 0.0029224457059883408\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01706106453306145\n",
      "Average test loss: 0.0027699796886493763\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017047394489248593\n",
      "Average test loss: 0.0027666298538032504\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01705798858238591\n",
      "Average test loss: 0.002737211648167835\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01706254384915034\n",
      "Average test loss: 0.0027784018983236618\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01703586593767007\n",
      "Average test loss: 0.0027710440376152593\n",
      "Epoch 204/300\n",
      "Average training loss: 0.017065435712536176\n",
      "Average test loss: 0.0027599634759955935\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017014406816826925\n",
      "Average test loss: 0.0027631960871318975\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017047210678458213\n",
      "Average test loss: 0.0027304609377558032\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017036317880782817\n",
      "Average test loss: 0.0028640162045550015\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01701787745455901\n",
      "Average test loss: 0.002770805017194814\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01701973328822189\n",
      "Average test loss: 0.002783723088602225\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017025859740045335\n",
      "Average test loss: 0.0027404392775562075\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01701507605529494\n",
      "Average test loss: 0.002839619357759754\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016982446408934062\n",
      "Average test loss: 0.0028731272253725263\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01700685297118293\n",
      "Average test loss: 0.0028380085232978065\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0169742722188433\n",
      "Average test loss: 0.002807000255315668\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017003380373120307\n",
      "Average test loss: 0.002805355484907826\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01697290383858813\n",
      "Average test loss: 0.002841010395437479\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017001071985397073\n",
      "Average test loss: 0.0028919813881317777\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017015203788876533\n",
      "Average test loss: 0.002882525441133314\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01695923283861743\n",
      "Average test loss: 0.0027364399762203296\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016945448128713504\n",
      "Average test loss: 0.0027523837209575707\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01693530184775591\n",
      "Average test loss: 0.0028915327421079077\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017003765665822557\n",
      "Average test loss: 0.0027805056707519626\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01694964135189851\n",
      "Average test loss: 0.0027732473011646006\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016944019198417662\n",
      "Average test loss: 0.0031331565040681095\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01693270381788413\n",
      "Average test loss: 0.002751047772148417\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016915381832255258\n",
      "Average test loss: 0.002768362997720639\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01691172344651487\n",
      "Average test loss: 0.002782995226068629\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01692700663043393\n",
      "Average test loss: 0.002832859310011069\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016914283022284506\n",
      "Average test loss: 0.0028138017061476906\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016907959911558364\n",
      "Average test loss: 0.10310831447442373\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01702772207227018\n",
      "Average test loss: 0.0028127394811146788\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016875296784771815\n",
      "Average test loss: 0.0028459717805186906\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0168714616894722\n",
      "Average test loss: 0.0027908420724173386\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01689060179475281\n",
      "Average test loss: 0.002835926639330056\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016868883661097952\n",
      "Average test loss: 0.002768733388640814\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0168662617256244\n",
      "Average test loss: 0.0028641356418116224\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01690271564076344\n",
      "Average test loss: 0.0034576736874878406\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01688348200586107\n",
      "Average test loss: 0.0028240959187969565\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01685101553797722\n",
      "Average test loss: 0.0028398686154848998\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016849647363026937\n",
      "Average test loss: 0.002798025686914722\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016862841349509026\n",
      "Average test loss: 0.0028408563031504554\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016864835171235933\n",
      "Average test loss: 0.002821303103843497\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01684215712547302\n",
      "Average test loss: 0.0027701754321654637\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016824435552789105\n",
      "Average test loss: 0.0028446040831299293\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016853922297557195\n",
      "Average test loss: 0.002758723260834813\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016842934937940705\n",
      "Average test loss: 0.0028191547287731533\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016826910893122354\n",
      "Average test loss: 0.0027931003229071695\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01682599060071839\n",
      "Average test loss: 0.0027785448216729692\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016816710551579794\n",
      "Average test loss: 0.002775912483326263\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01681766371180614\n",
      "Average test loss: 0.0029009668562147354\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01683371256788572\n",
      "Average test loss: 0.002831148919338981\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016798710472053953\n",
      "Average test loss: 0.0027981991389145456\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01679862340622478\n",
      "Average test loss: 0.002774041637260881\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0167963039610121\n",
      "Average test loss: 0.002754636532937487\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01679797685643037\n",
      "Average test loss: 0.0027606881429544754\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016790120732453135\n",
      "Average test loss: 0.002819991138039364\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01679460115234057\n",
      "Average test loss: 0.0028248576160727276\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016769961151811813\n",
      "Average test loss: 0.002822743749866883\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016780780161420504\n",
      "Average test loss: 0.002819694166175193\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016782605469226837\n",
      "Average test loss: 0.002759727431047294\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016779057370291815\n",
      "Average test loss: 0.002814249704281489\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01675650226076444\n",
      "Average test loss: 0.002816414725656311\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01678395778271887\n",
      "Average test loss: 0.002819615675136447\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016778101270397503\n",
      "Average test loss: 0.002863685381081369\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016752606969740656\n",
      "Average test loss: 0.0028106761154615217\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016777585039536157\n",
      "Average test loss: 0.002807107340958383\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016727334193057483\n",
      "Average test loss: 0.002804616911854181\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016740079436037277\n",
      "Average test loss: 0.0027826748771799933\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016758644780351058\n",
      "Average test loss: 0.0028206280606488388\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016761698603216146\n",
      "Average test loss: 0.0028872760908885135\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01673716066363785\n",
      "Average test loss: 0.002784973902420865\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016729226700133746\n",
      "Average test loss: 0.0027982337646600274\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016724227094815837\n",
      "Average test loss: 0.0028257784006289308\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01673983368443118\n",
      "Average test loss: 0.0029434734200023942\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01671466378370921\n",
      "Average test loss: 0.0027960668195866874\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016708052659200296\n",
      "Average test loss: 0.0029354577201108137\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016689948404000864\n",
      "Average test loss: 0.003138464858341548\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016710764549672602\n",
      "Average test loss: 0.002810070813323061\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01670016221370962\n",
      "Average test loss: 0.002780247686430812\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016699098704589736\n",
      "Average test loss: 0.002985300809558895\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016697204935881827\n",
      "Average test loss: 0.0028137960825115444\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016694444613324273\n",
      "Average test loss: 0.0027836569495913056\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016695080956651103\n",
      "Average test loss: 0.0028909189458936452\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016690859162145192\n",
      "Average test loss: 0.002960497006980909\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016677980068657134\n",
      "Average test loss: 0.0028058769365565646\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016677519518468114\n",
      "Average test loss: 0.00294437354678909\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016657853005660906\n",
      "Average test loss: 0.0028284458308998083\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016682295546763475\n",
      "Average test loss: 0.002868963775328464\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016679218244221476\n",
      "Average test loss: 0.002789365585272511\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016666031618913016\n",
      "Average test loss: 0.0028247419132126704\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01670433247420523\n",
      "Average test loss: 0.002899076799551646\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016642988741397857\n",
      "Average test loss: 0.0028254344235691758\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01663600509779321\n",
      "Average test loss: 0.00285341025578479\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01664708279652728\n",
      "Average test loss: 0.0028483484966887367\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01662944973839654\n",
      "Average test loss: 0.002792303818381495\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016631023176842265\n",
      "Average test loss: 0.0028296223539445134\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016640679813921453\n",
      "Average test loss: 0.0028322470895946025\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016642986161841285\n",
      "Average test loss: 0.0028455525040626526\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016632417872548103\n",
      "Average test loss: 0.0028723445180803536\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016614673811528417\n",
      "Average test loss: 0.0028007929145048064\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.16767533610926735\n",
      "Average test loss: 0.005411261267132229\n",
      "Epoch 2/300\n",
      "Average training loss: 0.038209129518932766\n",
      "Average test loss: 0.004817327508909835\n",
      "Epoch 3/300\n",
      "Average training loss: 0.033427146789100436\n",
      "Average test loss: 0.0051377251123388605\n",
      "Epoch 4/300\n",
      "Average training loss: 0.030498837918043135\n",
      "Average test loss: 0.0039028287813481356\n",
      "Epoch 5/300\n",
      "Average training loss: 0.028501676900519263\n",
      "Average test loss: 0.0039054737111760512\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02679878165324529\n",
      "Average test loss: 0.00400737864130901\n",
      "Epoch 7/300\n",
      "Average training loss: 0.025585173797276286\n",
      "Average test loss: 0.003601076385834151\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02446176585720645\n",
      "Average test loss: 0.003477813094233473\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02356044210659133\n",
      "Average test loss: 0.0032705293840004338\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02275871295730273\n",
      "Average test loss: 0.0032063772392769653\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02206365978386667\n",
      "Average test loss: 0.003045539800905519\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02142119946744707\n",
      "Average test loss: 0.0030571373301661675\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02096336656808853\n",
      "Average test loss: 0.0029500549621880056\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02046752342498965\n",
      "Average test loss: 0.002866420570554005\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020075837882028685\n",
      "Average test loss: 0.0030893224792348014\n",
      "Epoch 16/300\n",
      "Average training loss: 0.019687519722514684\n",
      "Average test loss: 0.00264119683785571\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01936593296378851\n",
      "Average test loss: 0.004127753599236409\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01904850317372216\n",
      "Average test loss: 0.0024965706875340805\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018748309559292263\n",
      "Average test loss: 0.002444902697371112\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01850912858049075\n",
      "Average test loss: 0.002501566628408101\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018296632048156528\n",
      "Average test loss: 0.0024549876225905287\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018045357363091576\n",
      "Average test loss: 0.0023382814027782944\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017879279823766813\n",
      "Average test loss: 0.0023320290793975196\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017741925314068794\n",
      "Average test loss: 0.002292679351651006\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017603757268852657\n",
      "Average test loss: 0.0022399648245837954\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01743469813797209\n",
      "Average test loss: 0.002211569868442085\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01729605818953779\n",
      "Average test loss: 0.002264518863831957\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0171660055551264\n",
      "Average test loss: 0.0021752451810364924\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017087733136283027\n",
      "Average test loss: 0.002307372488288416\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01695915163225598\n",
      "Average test loss: 0.0021440741216970814\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016871900377174218\n",
      "Average test loss: 0.0021436124700638984\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016767550959355303\n",
      "Average test loss: 0.0021248412713822393\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016669298686914975\n",
      "Average test loss: 0.0021259725718862483\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016624225677715408\n",
      "Average test loss: 0.0021413642815831637\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016529920776685077\n",
      "Average test loss: 0.0020970551017671824\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0164657167121768\n",
      "Average test loss: 0.002063656953887807\n",
      "Epoch 37/300\n",
      "Average training loss: 0.016400819718837736\n",
      "Average test loss: 0.002065294152332677\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016323628571298387\n",
      "Average test loss: 0.0020388769023120404\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016274268329143523\n",
      "Average test loss: 0.0020865410475267303\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016209852966997357\n",
      "Average test loss: 0.002024827680653996\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01616940036912759\n",
      "Average test loss: 0.0020313812577062184\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0161286408752203\n",
      "Average test loss: 0.0020577903555499184\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0160956881708569\n",
      "Average test loss: 0.0042008814021117155\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01600482650183969\n",
      "Average test loss: 0.002007079003482229\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01597434126916859\n",
      "Average test loss: 0.0019970100956658524\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015930162097016972\n",
      "Average test loss: 0.0020202841276509894\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015895873733692698\n",
      "Average test loss: 0.0020410087207953136\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015842201218008996\n",
      "Average test loss: 0.001988395905536082\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015806649012698067\n",
      "Average test loss: 0.0020619202856388356\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015789197050035\n",
      "Average test loss: 0.0020228505473997857\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015734729895989102\n",
      "Average test loss: 0.00206320826181521\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015696739768816366\n",
      "Average test loss: 0.001974792490609818\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015657203922669092\n",
      "Average test loss: 0.0020096867349930106\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015630154456529353\n",
      "Average test loss: 0.0019585606886280906\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015597500937680404\n",
      "Average test loss: 0.0020551444126500023\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015581544513503711\n",
      "Average test loss: 0.003917042883733908\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015545991994440556\n",
      "Average test loss: 0.002007557752645678\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015527174615197712\n",
      "Average test loss: 0.0019690731027060086\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015480235255426831\n",
      "Average test loss: 0.001970712295215991\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015445394929084514\n",
      "Average test loss: 0.001961338452477422\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015418846817480193\n",
      "Average test loss: 0.0019562271655433707\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015405232636464967\n",
      "Average test loss: 0.0020196350790146326\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015399175707664754\n",
      "Average test loss: 0.001945479473616514\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015351392929752667\n",
      "Average test loss: 0.001960485252034333\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015343268748786715\n",
      "Average test loss: 0.0019772830146054425\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015319109379417366\n",
      "Average test loss: 0.001977916478903757\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015288863801293903\n",
      "Average test loss: 0.001955242961024245\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015272070722447502\n",
      "Average test loss: 0.0020468368698946305\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015258260484370921\n",
      "Average test loss: 0.001982935367566016\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015212979655298923\n",
      "Average test loss: 0.0019298828992371758\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015229556909865802\n",
      "Average test loss: 0.001965132085761676\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015170685280528333\n",
      "Average test loss: 0.001942859751275844\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015178082654045688\n",
      "Average test loss: 0.0019359036174913247\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01513179997354746\n",
      "Average test loss: 0.0019429184790286753\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015115489416652255\n",
      "Average test loss: 0.0019459936494628587\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015103243386579885\n",
      "Average test loss: 0.002026638929007782\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015084391565786467\n",
      "Average test loss: 0.001953207413252029\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015111634013553461\n",
      "Average test loss: 0.09316738841268751\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015183901976380084\n",
      "Average test loss: 0.0019193607806745503\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015006229514049159\n",
      "Average test loss: 0.0019599763910389608\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015018634941014979\n",
      "Average test loss: 0.0019373922928546865\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015011922539936171\n",
      "Average test loss: 0.002092964040322436\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014981027758783764\n",
      "Average test loss: 0.001935763129240109\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014955237568252617\n",
      "Average test loss: 0.0019191009242915445\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014967920354670948\n",
      "Average test loss: 0.0019018317018118169\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014937245974938075\n",
      "Average test loss: 0.0019283637218177319\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014940025173955493\n",
      "Average test loss: 0.0019954703352931475\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014894881897502476\n",
      "Average test loss: 0.0019598304453409383\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014900535952713755\n",
      "Average test loss: 0.0026503221105991137\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014899136717120806\n",
      "Average test loss: 0.0019383734018645353\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014862260956731108\n",
      "Average test loss: 0.001929865337908268\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014846241522994307\n",
      "Average test loss: 0.001917583093771504\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014833934196167521\n",
      "Average test loss: 0.0019704617843445803\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014841152570313878\n",
      "Average test loss: 0.0019360004591031208\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014801010858681466\n",
      "Average test loss: 0.0019324642958947354\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014801060267620616\n",
      "Average test loss: 0.0019417304951283667\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014781069548593626\n",
      "Average test loss: 0.001996418851117293\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014774517017934058\n",
      "Average test loss: 0.0019109270037669275\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014769105431934198\n",
      "Average test loss: 0.001957429264154699\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014758479957779248\n",
      "Average test loss: 0.0019310920071891613\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01472920819123586\n",
      "Average test loss: 0.00195853803658651\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014723326179716322\n",
      "Average test loss: 0.001938514331355691\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014723067507147788\n",
      "Average test loss: 0.0019472293219943014\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014715599183407094\n",
      "Average test loss: 0.0019364421470090746\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014683025283945932\n",
      "Average test loss: 0.0028848572112619877\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014682859677407477\n",
      "Average test loss: 0.0019780031317431068\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014663643197880851\n",
      "Average test loss: 0.0019328155170919166\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014663302637636662\n",
      "Average test loss: 0.0019165014643222093\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014641804745627774\n",
      "Average test loss: 0.0020693249261627593\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014636601557334265\n",
      "Average test loss: 0.0020012440376190677\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014614527160094844\n",
      "Average test loss: 0.001970316977188405\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014614994771778583\n",
      "Average test loss: 0.0019486867820637094\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014600612693362765\n",
      "Average test loss: 0.001922007436864078\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01459286574108733\n",
      "Average test loss: 0.0019770128946337436\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014593326629036002\n",
      "Average test loss: 0.0019629201825915113\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014577848027149837\n",
      "Average test loss: 0.002000782487500045\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014578033989502324\n",
      "Average test loss: 0.0019334069848474529\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014568885134326087\n",
      "Average test loss: 0.001959596263244748\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014537648362418016\n",
      "Average test loss: 0.0019400112657911247\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014536662608385086\n",
      "Average test loss: 0.0019309029484478135\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014510827041334577\n",
      "Average test loss: 0.0019436907979349296\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01451092663821247\n",
      "Average test loss: 0.0019452008094845547\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014516925386256641\n",
      "Average test loss: 0.001908030659167303\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01449840505917867\n",
      "Average test loss: 0.0019441973763621517\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014487970620393753\n",
      "Average test loss: 0.002416395523274938\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014479426344235739\n",
      "Average test loss: 0.0019604168214524785\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014465979027251403\n",
      "Average test loss: 0.0019501521279000574\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014465735377536879\n",
      "Average test loss: 0.0019536681769208777\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014446800269186497\n",
      "Average test loss: 0.0019430626175469823\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01444296533945534\n",
      "Average test loss: 0.0019146980719847813\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014414416255222426\n",
      "Average test loss: 0.0019305121188776361\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014439936864707206\n",
      "Average test loss: 0.001944627534598112\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014425333191123274\n",
      "Average test loss: 0.001958211642069121\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014406094396279918\n",
      "Average test loss: 0.0019200563977162042\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014393770375185543\n",
      "Average test loss: 0.002010200282662279\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014380525218115912\n",
      "Average test loss: 0.001977372474451032\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014382233015365072\n",
      "Average test loss: 0.001963629671268993\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014393449376854632\n",
      "Average test loss: 0.00195522340780331\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014369428423543771\n",
      "Average test loss: 0.0019604636397626664\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014349266962872611\n",
      "Average test loss: 0.0020596407797808447\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014371170757545366\n",
      "Average test loss: 0.0019363651935838991\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014350462050901519\n",
      "Average test loss: 0.001970174859588345\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014329872242278522\n",
      "Average test loss: 0.002121698414389458\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01433832784742117\n",
      "Average test loss: 0.0019444618696967762\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014306672588818603\n",
      "Average test loss: 0.0020069487291491696\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014328054140839312\n",
      "Average test loss: 0.0019405110114150577\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014298069599601957\n",
      "Average test loss: 0.001966028697374794\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014296119755340947\n",
      "Average test loss: 0.0019537673448212444\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014292861792776319\n",
      "Average test loss: 0.001981337255384359\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014318909029165904\n",
      "Average test loss: 0.001941274928756886\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014293977912929324\n",
      "Average test loss: 0.0019769683664457665\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01426467926055193\n",
      "Average test loss: 0.0019840703960508107\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014280350065065756\n",
      "Average test loss: 0.001951899615323378\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014249373894598748\n",
      "Average test loss: 0.0019391201590705249\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014253812010089557\n",
      "Average test loss: 0.001993732452496058\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01423605079203844\n",
      "Average test loss: 0.0019722398420174915\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01422537766645352\n",
      "Average test loss: 0.001947309801975886\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014239298914041784\n",
      "Average test loss: 0.001978176364364723\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014233027706543605\n",
      "Average test loss: 0.0019381354517406888\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014218458182281919\n",
      "Average test loss: 0.001969979009177122\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01421339669658078\n",
      "Average test loss: 0.001964934254065156\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014196767612463898\n",
      "Average test loss: 0.0019612158296836747\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014184487452109654\n",
      "Average test loss: 0.00199120181819631\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014205257720417447\n",
      "Average test loss: 0.0020441390321486526\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01418530464834637\n",
      "Average test loss: 0.0019295967168485124\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01419467668235302\n",
      "Average test loss: 0.0019392702409790621\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01416006582395898\n",
      "Average test loss: 0.0019979383665033514\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014170113589200709\n",
      "Average test loss: 0.0019855312508427433\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014173169665866427\n",
      "Average test loss: 0.0019875906778292524\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014165287502937846\n",
      "Average test loss: 0.001951156924168269\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01414541362474362\n",
      "Average test loss: 0.0020330895334482193\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014153856437239381\n",
      "Average test loss: 0.001959426762950089\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01413569330672423\n",
      "Average test loss: 0.0020285273471122816\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014127590787079598\n",
      "Average test loss: 0.001937073710374534\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01412366793056329\n",
      "Average test loss: 0.0019879781857339873\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014130168732669618\n",
      "Average test loss: 0.002003937140107155\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01412496080663469\n",
      "Average test loss: 0.001968311619013548\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014105728894472123\n",
      "Average test loss: 0.0019818710792395802\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014093484347893133\n",
      "Average test loss: 0.001937979318201542\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014108829778929551\n",
      "Average test loss: 0.001968236352937917\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014079509993394216\n",
      "Average test loss: 0.001978158636432555\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014100333547426595\n",
      "Average test loss: 0.001979442542522318\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014088115998440318\n",
      "Average test loss: 0.0019331010298596488\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014090044415659374\n",
      "Average test loss: 0.002056056665774021\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014069654150969453\n",
      "Average test loss: 0.0020141125358641146\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01406788943376806\n",
      "Average test loss: 0.0019534088293504384\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014074429204894436\n",
      "Average test loss: 0.002024912119325664\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014061064042978817\n",
      "Average test loss: 0.0019884060482598014\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01405493346022235\n",
      "Average test loss: 0.0020410226210951805\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01405597041050593\n",
      "Average test loss: 0.0019981122477394013\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014057680582834615\n",
      "Average test loss: 0.0019923064298927786\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014044971002472772\n",
      "Average test loss: 0.0020446488766206637\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014035557788279321\n",
      "Average test loss: 0.0019884304211785398\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014017944048676225\n",
      "Average test loss: 0.0020121249442713127\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014072037301957607\n",
      "Average test loss: 0.001997669539310866\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014002575991882218\n",
      "Average test loss: 0.001996473359151019\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014020217718349564\n",
      "Average test loss: 0.0020083598709768717\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014022871445450518\n",
      "Average test loss: 0.0020008115342093837\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014008653703663084\n",
      "Average test loss: 0.0019740009945299892\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013989453145199351\n",
      "Average test loss: 0.001996891796692378\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013990272506243653\n",
      "Average test loss: 0.001958650101804071\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014023417155775758\n",
      "Average test loss: 0.0019476354190458853\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01398083043595155\n",
      "Average test loss: 0.0019977631081516544\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013991344920463031\n",
      "Average test loss: 0.0020105914577013915\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013989622590442499\n",
      "Average test loss: 0.0020125886973821456\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013980236161914137\n",
      "Average test loss: 0.0020662817739778095\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013968762640737825\n",
      "Average test loss: 0.0020238999439817335\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013970529010726346\n",
      "Average test loss: 0.002046895650732848\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013994155143698056\n",
      "Average test loss: 0.0022017944893903203\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013943440110319191\n",
      "Average test loss: 0.0019726094887074497\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013959904006785817\n",
      "Average test loss: 0.002004973048551215\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01395618876732058\n",
      "Average test loss: 0.002055853174585435\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013942041187650628\n",
      "Average test loss: 0.001998588722613123\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013945860432667864\n",
      "Average test loss: 0.0021849348954856396\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013936786995165878\n",
      "Average test loss: 0.003411274532890982\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01393056854357322\n",
      "Average test loss: 0.002104570126781861\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013924418305357298\n",
      "Average test loss: 0.002006168915786677\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013921578344371584\n",
      "Average test loss: 0.002041948557520906\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013911228430767855\n",
      "Average test loss: 0.001975716431521707\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01390263666295343\n",
      "Average test loss: 0.0021261964789074327\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013917386791772313\n",
      "Average test loss: 0.0020007331882499987\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01391165175454484\n",
      "Average test loss: 0.002061696620244119\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013893208473920822\n",
      "Average test loss: 0.0019817699160840775\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01389266750547621\n",
      "Average test loss: 0.0020177118797890014\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013895467994113763\n",
      "Average test loss: 0.0019811827132167914\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013881947312090132\n",
      "Average test loss: 0.0020192167748593623\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013889318996005588\n",
      "Average test loss: 0.0020074699169231785\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013889866195619107\n",
      "Average test loss: 0.00196078516718828\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013874429860048824\n",
      "Average test loss: 0.0020449300829528108\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013861059952113364\n",
      "Average test loss: 0.001992730205671655\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013876657990117868\n",
      "Average test loss: 0.0019868951715115045\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01386994192832046\n",
      "Average test loss: 0.0020844919128964346\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01386895244485802\n",
      "Average test loss: 0.0020164386502777537\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013849166824585861\n",
      "Average test loss: 0.002120218149645047\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013858304080863793\n",
      "Average test loss: 0.0020123961514068975\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013848307002749707\n",
      "Average test loss: 0.002103249836092194\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013867961257696152\n",
      "Average test loss: 0.002098170222921504\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013855709372295274\n",
      "Average test loss: 0.0019702140842047\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013841261119478279\n",
      "Average test loss: 0.002049276024827527\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013846034699016147\n",
      "Average test loss: 0.0020591287416302497\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01382214143872261\n",
      "Average test loss: 0.002099596502362854\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01382691724681192\n",
      "Average test loss: 0.0020929351368298133\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01383146865831481\n",
      "Average test loss: 0.0019982806146144868\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013838888811568419\n",
      "Average test loss: 0.0019903545625921753\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01383394183880753\n",
      "Average test loss: 0.002024469380370445\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013821953061554168\n",
      "Average test loss: 0.0020485684499144553\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013799670793943935\n",
      "Average test loss: 0.001983821076961855\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013805772816969288\n",
      "Average test loss: 0.0023850352480593653\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01380865406493346\n",
      "Average test loss: 0.002085856638020939\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013790461085736751\n",
      "Average test loss: 0.002038646461131672\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013804193261596891\n",
      "Average test loss: 0.001995540656770269\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013808950620392958\n",
      "Average test loss: 0.001981749194363753\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013783657224641905\n",
      "Average test loss: 0.002105173089231054\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01379471653699875\n",
      "Average test loss: 0.002109603064548638\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013792971511681875\n",
      "Average test loss: 0.0020995620151774753\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013779118637243907\n",
      "Average test loss: 0.0020312416322736276\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01378306335873074\n",
      "Average test loss: 0.0019429904377708833\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01377884947674142\n",
      "Average test loss: 0.0020477597865586478\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01377941957116127\n",
      "Average test loss: 0.0020092770195462638\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013766199986139933\n",
      "Average test loss: 0.0021133909004016054\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013984265696671274\n",
      "Average test loss: 0.0020010173065173956\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013756431510878934\n",
      "Average test loss: 0.0020908497004873223\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0137520168547829\n",
      "Average test loss: 0.0019885520940232607\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013760041790703932\n",
      "Average test loss: 0.001972332663834095\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013738928053941992\n",
      "Average test loss: 0.0020589239657339124\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01374937873830398\n",
      "Average test loss: 0.0019819061184922853\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01374572957555453\n",
      "Average test loss: 0.0019735231888997886\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01375107859985696\n",
      "Average test loss: 0.0021384003872258798\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013738021709024906\n",
      "Average test loss: 0.001992453923034999\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013735233270459704\n",
      "Average test loss: 0.002035553953092959\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01374268550839689\n",
      "Average test loss: 0.002061737167648971\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013750916765795814\n",
      "Average test loss: 0.002001310229094492\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013733777102496889\n",
      "Average test loss: 0.0019983306324316396\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013729700216816531\n",
      "Average test loss: 0.001976880154779388\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013721580522755782\n",
      "Average test loss: 0.002048535324840082\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013725381553173065\n",
      "Average test loss: 0.0020569699990252654\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013716846477654246\n",
      "Average test loss: 0.001999420228207277\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013713882077899245\n",
      "Average test loss: 0.0020840001120749447\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013714694611728192\n",
      "Average test loss: 0.001989529956028693\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013728326939874224\n",
      "Average test loss: 0.002066637358110812\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013709188058972359\n",
      "Average test loss: 0.0021238617858745984\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01371551724192169\n",
      "Average test loss: 0.0020425300739912522\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013704971000552178\n",
      "Average test loss: 0.0020063617805329463\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013696747679677275\n",
      "Average test loss: 0.002093998544849455\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01369670802851518\n",
      "Average test loss: 0.002027094426150951\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013698910781078868\n",
      "Average test loss: 0.0020592043918247023\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01370415053019921\n",
      "Average test loss: 0.0020980725228372547\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01370505464076996\n",
      "Average test loss: 0.0020150706735957\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013686635577844249\n",
      "Average test loss: 0.0020601707643104926\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01367898381749789\n",
      "Average test loss: 0.002118900589644909\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013674761693510744\n",
      "Average test loss: 0.002077405092202955\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013682816621330049\n",
      "Average test loss: 0.002089744700636301\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01367493134405878\n",
      "Average test loss: 0.0020435635573748086\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013672667441268762\n",
      "Average test loss: 0.00498375420148174\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013682330374088553\n",
      "Average test loss: 0.0020691844541579486\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013678944860067632\n",
      "Average test loss: 0.002092844500102931\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013666083690192964\n",
      "Average test loss: 0.0020546959574437806\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01365133867247237\n",
      "Average test loss: 0.002024976034855677\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013660812504589557\n",
      "Average test loss: 0.0020673832387352984\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013649992463489373\n",
      "Average test loss: 0.0020532816517063314\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Gauss_32_Depth3/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 23.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.54\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.05\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.15\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.41\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.57\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 25.31\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.88\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.3067069873544903\n",
      "Average test loss: 0.012932419581545724\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2985579195022583\n",
      "Average test loss: 0.011364502048326864\n",
      "Epoch 3/300\n",
      "Average training loss: 0.22052503871917725\n",
      "Average test loss: 0.011642657533288001\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1884397186173333\n",
      "Average test loss: 0.010150295515027311\n",
      "Epoch 5/300\n",
      "Average training loss: 0.17145768801371256\n",
      "Average test loss: 0.008745640499310361\n",
      "Epoch 6/300\n",
      "Average training loss: 0.16139398063553703\n",
      "Average test loss: 0.010583000069691074\n",
      "Epoch 7/300\n",
      "Average training loss: 0.15427545403109658\n",
      "Average test loss: 0.009459867051906056\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1501234157151646\n",
      "Average test loss: 0.008151637530989117\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14400315545664893\n",
      "Average test loss: 0.012817076995140977\n",
      "Epoch 10/300\n",
      "Average training loss: 0.14015921898682912\n",
      "Average test loss: 0.010841750411523713\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13650885932975346\n",
      "Average test loss: 0.007832803111109468\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13356558714972602\n",
      "Average test loss: 0.007554334282875061\n",
      "Epoch 13/300\n",
      "Average training loss: 0.12973755735821194\n",
      "Average test loss: 0.007431809348778592\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12845217700137032\n",
      "Average test loss: 0.007929201062768699\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12549387138419682\n",
      "Average test loss: 0.008001327476153772\n",
      "Epoch 16/300\n",
      "Average training loss: 0.12342707375685374\n",
      "Average test loss: 0.00708926004005803\n",
      "Epoch 17/300\n",
      "Average training loss: 0.12172269517845577\n",
      "Average test loss: 0.007208994028882848\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11954127797815535\n",
      "Average test loss: 0.00781536969004406\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11829078384902743\n",
      "Average test loss: 0.006803755995300081\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11629515283968714\n",
      "Average test loss: 0.0071422235531111555\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11499623554944992\n",
      "Average test loss: 0.006765268790225188\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11389052603642146\n",
      "Average test loss: 0.00687474064115021\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11253033941321902\n",
      "Average test loss: 0.006736592089964284\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1113554742468728\n",
      "Average test loss: 0.006654019562320577\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11048024185498556\n",
      "Average test loss: 0.006659130524016089\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10922049269411299\n",
      "Average test loss: 0.006552767472962538\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10832639869054159\n",
      "Average test loss: 0.006483622610569\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10741017261478636\n",
      "Average test loss: 0.006789495015309917\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10698970634407468\n",
      "Average test loss: 0.006579926641450988\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10569892274008857\n",
      "Average test loss: 0.0068961474245621095\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10505537022484673\n",
      "Average test loss: 0.006368877109554079\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10434759667184618\n",
      "Average test loss: 0.006414999469286866\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10386666725079219\n",
      "Average test loss: 0.006490170336018006\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10339473400513331\n",
      "Average test loss: 0.006742817463560237\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10232762154605654\n",
      "Average test loss: 0.006287283349368307\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10204562973313862\n",
      "Average test loss: 0.006256714712414476\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10160833297835456\n",
      "Average test loss: 0.006274299897667435\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10090858521726397\n",
      "Average test loss: 0.006195200391527679\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10059509103165733\n",
      "Average test loss: 0.006236171173138751\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10019863449202643\n",
      "Average test loss: 0.006291071515944269\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09973492254151238\n",
      "Average test loss: 0.006149737898674276\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09928755096594492\n",
      "Average test loss: 0.006428768299933937\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09886829013294644\n",
      "Average test loss: 0.006185609586536884\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09831013080808851\n",
      "Average test loss: 0.006287046843932735\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09789238240983751\n",
      "Average test loss: 0.006274963133451011\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09755764267841975\n",
      "Average test loss: 0.0063538101841178205\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09698862497674095\n",
      "Average test loss: 0.006102880381875568\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09673324860466852\n",
      "Average test loss: 0.007059920207907756\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09696886355347104\n",
      "Average test loss: 0.006524565282381243\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09605333211686876\n",
      "Average test loss: 534.716223578559\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09621844726469782\n",
      "Average test loss: 0.006203835500197278\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09554634073707792\n",
      "Average test loss: 0.05174586480193668\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0953322340713607\n",
      "Average test loss: 0.008268736561967267\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0947786462770568\n",
      "Average test loss: 0.006132420946947403\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09487598607937495\n",
      "Average test loss: 0.0060586963631212715\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0942051491273774\n",
      "Average test loss: 0.006049396065374216\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09401161848836474\n",
      "Average test loss: 0.006160669316434198\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09411286505725648\n",
      "Average test loss: 0.006349855141093334\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09360640647013982\n",
      "Average test loss: 0.006363732271310356\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09334408463372125\n",
      "Average test loss: 0.006078868429693911\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09292523013883167\n",
      "Average test loss: 0.0061408237574828994\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09255992395348019\n",
      "Average test loss: 0.006249871004372835\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09275585202376048\n",
      "Average test loss: 0.006191424020462566\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09215527407990562\n",
      "Average test loss: 0.00608701041009691\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09201838954289754\n",
      "Average test loss: 0.006300856930928098\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09270932117435668\n",
      "Average test loss: 0.006317019775095913\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09145471725198957\n",
      "Average test loss: 0.006101035793622335\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09118571914566888\n",
      "Average test loss: 0.006213287607249286\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0910218995279736\n",
      "Average test loss: 0.006069090346495311\n",
      "Epoch 70/300\n",
      "Average training loss: 1802708.7425482331\n",
      "Average test loss: 1.931423834853702\n",
      "Epoch 71/300\n",
      "Average training loss: 11.079663765801325\n",
      "Average test loss: 3.02115769261784\n",
      "Epoch 72/300\n",
      "Average training loss: 10.013540015326607\n",
      "Average test loss: 9.673353532049392\n",
      "Epoch 73/300\n",
      "Average training loss: 9.463604178534613\n",
      "Average test loss: 0.4878654376135932\n",
      "Epoch 74/300\n",
      "Average training loss: 9.018852277119954\n",
      "Average test loss: 0.6910846121178733\n",
      "Epoch 75/300\n",
      "Average training loss: 8.615225453694661\n",
      "Average test loss: 0.09819788016213311\n",
      "Epoch 76/300\n",
      "Average training loss: 8.216824917263455\n",
      "Average test loss: 0.358725158823861\n",
      "Epoch 77/300\n",
      "Average training loss: 7.813320302751329\n",
      "Average test loss: 2.249882302628623\n",
      "Epoch 78/300\n",
      "Average training loss: 7.392593261294895\n",
      "Average test loss: 0.09307775337497393\n",
      "Epoch 79/300\n",
      "Average training loss: 6.981000271691216\n",
      "Average test loss: 0.1222707745830218\n",
      "Epoch 80/300\n",
      "Average training loss: 6.586089772118463\n",
      "Average test loss: 0.38279539716243743\n",
      "Epoch 81/300\n",
      "Average training loss: 6.24350701268514\n",
      "Average test loss: 0.03919792624149058\n",
      "Epoch 82/300\n",
      "Average training loss: 5.927924040900336\n",
      "Average test loss: 0.027779927409357495\n",
      "Epoch 83/300\n",
      "Average training loss: 5.6270841348436145\n",
      "Average test loss: 0.02708746489385764\n",
      "Epoch 84/300\n",
      "Average training loss: 5.329521664089627\n",
      "Average test loss: 0.5190568502015538\n",
      "Epoch 85/300\n",
      "Average training loss: 5.00284860780504\n",
      "Average test loss: 0.03507041168544028\n",
      "Epoch 86/300\n",
      "Average training loss: 4.699926400926378\n",
      "Average test loss: 0.036205667306979494\n",
      "Epoch 87/300\n",
      "Average training loss: 4.428446242862278\n",
      "Average test loss: 0.03149456577830845\n",
      "Epoch 88/300\n",
      "Average training loss: 4.175156185997857\n",
      "Average test loss: 0.01580394245766931\n",
      "Epoch 89/300\n",
      "Average training loss: 3.9380111673143174\n",
      "Average test loss: 0.016301691606640814\n",
      "Epoch 90/300\n",
      "Average training loss: 3.709585673014323\n",
      "Average test loss: 0.010997781286636988\n",
      "Epoch 91/300\n",
      "Average training loss: 3.492030116399129\n",
      "Average test loss: 0.011480615802937084\n",
      "Epoch 92/300\n",
      "Average training loss: 3.2865304942660862\n",
      "Average test loss: 0.009531694723500145\n",
      "Epoch 93/300\n",
      "Average training loss: 3.0883674810197617\n",
      "Average test loss: 0.008997838751309448\n",
      "Epoch 94/300\n",
      "Average training loss: 2.8975390446980795\n",
      "Average test loss: 0.008574344659845034\n",
      "Epoch 95/300\n",
      "Average training loss: 2.712685690773858\n",
      "Average test loss: 0.008754472988347212\n",
      "Epoch 96/300\n",
      "Average training loss: 2.5338393026987713\n",
      "Average test loss: 0.008562557125671043\n",
      "Epoch 97/300\n",
      "Average training loss: 2.3625776884290905\n",
      "Average test loss: 0.007794234950509336\n",
      "Epoch 98/300\n",
      "Average training loss: 2.2060892748302883\n",
      "Average test loss: 0.008208071133121848\n",
      "Epoch 99/300\n",
      "Average training loss: 2.063364276568095\n",
      "Average test loss: 0.007493775981995795\n",
      "Epoch 100/300\n",
      "Average training loss: 1.9332701336542766\n",
      "Average test loss: 0.007406898481564389\n",
      "Epoch 101/300\n",
      "Average training loss: 1.8145201617346869\n",
      "Average test loss: 0.0073905905774898\n",
      "Epoch 102/300\n",
      "Average training loss: 1.7029366909662882\n",
      "Average test loss: 0.007093191312832965\n",
      "Epoch 103/300\n",
      "Average training loss: 1.5946598626242743\n",
      "Average test loss: 0.007049413701726331\n",
      "Epoch 104/300\n",
      "Average training loss: 1.4848014147016737\n",
      "Average test loss: 0.007055620039088858\n",
      "Epoch 105/300\n",
      "Average training loss: 1.3738304234610663\n",
      "Average test loss: 0.006804325831847058\n",
      "Epoch 106/300\n",
      "Average training loss: 1.253874431822035\n",
      "Average test loss: 0.006765326868742704\n",
      "Epoch 107/300\n",
      "Average training loss: 1.1240783975389268\n",
      "Average test loss: 0.0066719353778494726\n",
      "Epoch 108/300\n",
      "Average training loss: 0.997515170150333\n",
      "Average test loss: 0.00658377522478501\n",
      "Epoch 109/300\n",
      "Average training loss: 0.8788991106351217\n",
      "Average test loss: 0.0065765177831053736\n",
      "Epoch 110/300\n",
      "Average training loss: 0.7740131605996026\n",
      "Average test loss: 0.006553066587696473\n",
      "Epoch 111/300\n",
      "Average training loss: 0.6842017191780938\n",
      "Average test loss: 0.0064956135091682275\n",
      "Epoch 112/300\n",
      "Average training loss: 0.5951216790411208\n",
      "Average test loss: 0.0064818041763371895\n",
      "Epoch 113/300\n",
      "Average training loss: 0.5101978721088833\n",
      "Average test loss: 0.006676240339875222\n",
      "Epoch 114/300\n",
      "Average training loss: 0.4341479388342963\n",
      "Average test loss: 0.006316226702183485\n",
      "Epoch 115/300\n",
      "Average training loss: 0.3700364357365502\n",
      "Average test loss: 0.006245295009679265\n",
      "Epoch 116/300\n",
      "Average training loss: 0.31745259597566394\n",
      "Average test loss: 0.0069950626099275215\n",
      "Epoch 117/300\n",
      "Average training loss: 0.27229470194710625\n",
      "Average test loss: 0.006219012544386917\n",
      "Epoch 118/300\n",
      "Average training loss: 0.23547030500570934\n",
      "Average test loss: 0.006188714988529682\n",
      "Epoch 119/300\n",
      "Average training loss: 0.20577827797995674\n",
      "Average test loss: 0.006347799725002713\n",
      "Epoch 120/300\n",
      "Average training loss: 0.1826572348276774\n",
      "Average test loss: 0.0070483030598196715\n",
      "Epoch 121/300\n",
      "Average training loss: 0.16543739216857486\n",
      "Average test loss: 0.0061658213382793795\n",
      "Epoch 122/300\n",
      "Average training loss: 0.15326829634772407\n",
      "Average test loss: 0.007032540642552905\n",
      "Epoch 123/300\n",
      "Average training loss: 0.144203638765547\n",
      "Average test loss: 0.006124636175317897\n",
      "Epoch 124/300\n",
      "Average training loss: 0.1362407151858012\n",
      "Average test loss: 0.0077223895291487375\n",
      "Epoch 125/300\n",
      "Average training loss: 0.1302375100188785\n",
      "Average test loss: 0.006076939140549964\n",
      "Epoch 126/300\n",
      "Average training loss: 0.12455236144860586\n",
      "Average test loss: 0.006063838424781958\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11925264989667468\n",
      "Average test loss: 0.006285329177975654\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11502435004048878\n",
      "Average test loss: 0.006050525035709142\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11139424530665079\n",
      "Average test loss: 0.006274981910983721\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10833435835440954\n",
      "Average test loss: 0.006102537951121727\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10592679182026121\n",
      "Average test loss: 0.005994016746679941\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10398730183309979\n",
      "Average test loss: 0.00614778169327312\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10249354763163461\n",
      "Average test loss: 0.006117112676095632\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10079989578988817\n",
      "Average test loss: 0.006078172932896349\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09981541774670283\n",
      "Average test loss: 0.0059512806625829804\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0988272323343489\n",
      "Average test loss: 0.006024761997991138\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09801895056168239\n",
      "Average test loss: 0.006230182111263275\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09777080412043465\n",
      "Average test loss: 0.0061710129744476745\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09621331828832626\n",
      "Average test loss: 0.006067646091183027\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09548825069930818\n",
      "Average test loss: 0.00634954739196433\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09495922458171845\n",
      "Average test loss: 0.006712725659211476\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09413342108329137\n",
      "Average test loss: 0.005960367520857188\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09369918306006325\n",
      "Average test loss: 0.006413869568043285\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09314616455634435\n",
      "Average test loss: 0.010002537052664491\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09269116420216031\n",
      "Average test loss: 0.006000457181284825\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09201302821768655\n",
      "Average test loss: 0.006095704067084524\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09176762407355839\n",
      "Average test loss: 0.006074414757804738\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09178900648487939\n",
      "Average test loss: 0.006065668032401138\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09361477361122768\n",
      "Average test loss: 0.006071206806434525\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09077005375093884\n",
      "Average test loss: 0.006120050208850039\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09050323187642627\n",
      "Average test loss: 0.00690960859383146\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0902110141714414\n",
      "Average test loss: 0.006108258019718859\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09031662661499447\n",
      "Average test loss: 0.0065026873594356905\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08985210469696257\n",
      "Average test loss: 0.00623120637382898\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08983228888776568\n",
      "Average test loss: 0.00604159640851948\n",
      "Epoch 156/300\n",
      "Average training loss: 347.86311944107877\n",
      "Average test loss: 0.205069092935986\n",
      "Epoch 157/300\n",
      "Average training loss: 17.240556864420572\n",
      "Average test loss: 0.0386543843779299\n",
      "Epoch 158/300\n",
      "Average training loss: 15.002140200297038\n",
      "Average test loss: 0.017632810961869028\n",
      "Epoch 159/300\n",
      "Average training loss: 13.577105643378363\n",
      "Average test loss: 0.021408157866862086\n",
      "Epoch 160/300\n",
      "Average training loss: 12.553634941101075\n",
      "Average test loss: 0.016945471903516186\n",
      "Epoch 161/300\n",
      "Average training loss: 11.458656500074598\n",
      "Average test loss: 0.02552014898426003\n",
      "Epoch 162/300\n",
      "Average training loss: 10.35960686577691\n",
      "Average test loss: 0.012795031364593241\n",
      "Epoch 163/300\n",
      "Average training loss: 9.4605779715644\n",
      "Average test loss: 0.010746787022385332\n",
      "Epoch 164/300\n",
      "Average training loss: 8.689911384582519\n",
      "Average test loss: 0.010628955612579982\n",
      "Epoch 165/300\n",
      "Average training loss: 8.012194652133518\n",
      "Average test loss: 0.010843173200057612\n",
      "Epoch 166/300\n",
      "Average training loss: 7.407819327460395\n",
      "Average test loss: 0.009259202994406223\n",
      "Epoch 167/300\n",
      "Average training loss: 6.810946531083848\n",
      "Average test loss: 0.013163854338228703\n",
      "Epoch 168/300\n",
      "Average training loss: 6.196651059044732\n",
      "Average test loss: 0.012869652301073075\n",
      "Epoch 169/300\n",
      "Average training loss: 5.555284575568305\n",
      "Average test loss: 0.008435460457785262\n",
      "Epoch 170/300\n",
      "Average training loss: 4.893075180053711\n",
      "Average test loss: 0.008503235627379683\n",
      "Epoch 171/300\n",
      "Average training loss: 4.233953845130073\n",
      "Average test loss: 0.008071004989246528\n",
      "Epoch 172/300\n",
      "Average training loss: 3.618117112053765\n",
      "Average test loss: 0.007926057153277927\n",
      "Epoch 173/300\n",
      "Average training loss: 3.0845582196977404\n",
      "Average test loss: 0.00809366700384352\n",
      "Epoch 174/300\n",
      "Average training loss: 2.6073135108947754\n",
      "Average test loss: 0.007819804266095161\n",
      "Epoch 175/300\n",
      "Average training loss: 2.2007642806371055\n",
      "Average test loss: 0.007328975708948241\n",
      "Epoch 176/300\n",
      "Average training loss: 1.8767168520821464\n",
      "Average test loss: 0.007565662266479598\n",
      "Epoch 177/300\n",
      "Average training loss: 1.6188389409383137\n",
      "Average test loss: 0.007374209431310495\n",
      "Epoch 178/300\n",
      "Average training loss: 1.3987196369171142\n",
      "Average test loss: 0.00713190976695882\n",
      "Epoch 179/300\n",
      "Average training loss: 1.2036247295803495\n",
      "Average test loss: 0.0069705583655999765\n",
      "Epoch 180/300\n",
      "Average training loss: 1.0195625199211968\n",
      "Average test loss: 0.007111470284561316\n",
      "Epoch 181/300\n",
      "Average training loss: 0.8421246028476291\n",
      "Average test loss: 0.006967840755565299\n",
      "Epoch 182/300\n",
      "Average training loss: 0.6653540285958184\n",
      "Average test loss: 0.006781664091679785\n",
      "Epoch 183/300\n",
      "Average training loss: 0.4840598395665487\n",
      "Average test loss: 0.006796503188709418\n",
      "Epoch 184/300\n",
      "Average training loss: 0.37603392865922713\n",
      "Average test loss: 0.006503384667552179\n",
      "Epoch 185/300\n",
      "Average training loss: 0.3009324350886875\n",
      "Average test loss: 0.00645459693711665\n",
      "Epoch 186/300\n",
      "Average training loss: 0.24541445371839735\n",
      "Average test loss: 0.0062939334296517905\n",
      "Epoch 187/300\n",
      "Average training loss: 0.20401807355880738\n",
      "Average test loss: 0.006254191746314366\n",
      "Epoch 188/300\n",
      "Average training loss: 0.17603206657038795\n",
      "Average test loss: 0.0064320310855077376\n",
      "Epoch 189/300\n",
      "Average training loss: 0.15711572534508175\n",
      "Average test loss: 0.006131851518733634\n",
      "Epoch 190/300\n",
      "Average training loss: 0.14326256114906735\n",
      "Average test loss: 0.006135555807914999\n",
      "Epoch 191/300\n",
      "Average training loss: 0.13262561606698567\n",
      "Average test loss: 0.006174263800183932\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1249756144285202\n",
      "Average test loss: 0.007179889888813099\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1189205760690901\n",
      "Average test loss: 0.0062261182127727405\n",
      "Epoch 194/300\n",
      "Average training loss: 0.11410167959001329\n",
      "Average test loss: 0.006105809423658583\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1103904321855969\n",
      "Average test loss: 0.00612395607183377\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10767809868521161\n",
      "Average test loss: 0.006092795244521565\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10538177665074666\n",
      "Average test loss: 0.00613528334018257\n",
      "Epoch 198/300\n",
      "Average training loss: 0.10377884861495759\n",
      "Average test loss: 0.007352123940570487\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10190320314301385\n",
      "Average test loss: 0.006173603272686402\n",
      "Epoch 200/300\n",
      "Average training loss: 0.10037693883313073\n",
      "Average test loss: 0.005958324509776301\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0989324323270056\n",
      "Average test loss: 0.00603596840136581\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0977059156762229\n",
      "Average test loss: 0.0059560724575486446\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0971104135579533\n",
      "Average test loss: 0.006013915522230996\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09566811316543156\n",
      "Average test loss: 0.0061435755582319364\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09450621399614546\n",
      "Average test loss: 0.006149248481624656\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09353868744770685\n",
      "Average test loss: 0.006030356973823574\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09297937495178646\n",
      "Average test loss: 0.0073732392121520305\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09203755773438348\n",
      "Average test loss: 0.006047181661758158\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0914944348666403\n",
      "Average test loss: 0.006242607629547516\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09109075616465674\n",
      "Average test loss: 0.005998163846631844\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09061783479981952\n",
      "Average test loss: 0.006272801109279196\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0902215774655342\n",
      "Average test loss: 0.005953559510823753\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09059013950162463\n",
      "Average test loss: 0.006052217968222168\n",
      "Epoch 214/300\n",
      "Average training loss: 10396.581685424791\n",
      "Average test loss: 2.0333910668691\n",
      "Epoch 215/300\n",
      "Average training loss: 13.207660465664334\n",
      "Average test loss: 0.8320672715306282\n",
      "Epoch 216/300\n",
      "Average training loss: 11.853008772956\n",
      "Average test loss: 0.06649047272735172\n",
      "Epoch 217/300\n",
      "Average training loss: 11.156296441819933\n",
      "Average test loss: 0.04588212936123212\n",
      "Epoch 218/300\n",
      "Average training loss: 10.63876055823432\n",
      "Average test loss: 0.0417116873231199\n",
      "Epoch 219/300\n",
      "Average training loss: 10.192444832695855\n",
      "Average test loss: 0.02414498336116473\n",
      "Epoch 220/300\n",
      "Average training loss: 9.771616516960991\n",
      "Average test loss: 0.025352744748195013\n",
      "Epoch 221/300\n",
      "Average training loss: 9.356931351555719\n",
      "Average test loss: 0.01743831519368622\n",
      "Epoch 222/300\n",
      "Average training loss: 8.93235536617703\n",
      "Average test loss: 0.016658708292577002\n",
      "Epoch 223/300\n",
      "Average training loss: 8.491496592203776\n",
      "Average test loss: 0.016050514669881926\n",
      "Epoch 224/300\n",
      "Average training loss: 8.029991437276204\n",
      "Average test loss: 0.014312200613319875\n",
      "Epoch 225/300\n",
      "Average training loss: 7.559389056735569\n",
      "Average test loss: 0.012636315305199888\n",
      "Epoch 226/300\n",
      "Average training loss: 7.099341010199653\n",
      "Average test loss: 0.01144422872364521\n",
      "Epoch 227/300\n",
      "Average training loss: 6.64920240910848\n",
      "Average test loss: 0.011595501825213432\n",
      "Epoch 228/300\n",
      "Average training loss: 6.205941944122315\n",
      "Average test loss: 0.0108436069207059\n",
      "Epoch 229/300\n",
      "Average training loss: 5.770821848127577\n",
      "Average test loss: 0.010145671720306079\n",
      "Epoch 230/300\n",
      "Average training loss: 5.347650560590956\n",
      "Average test loss: 0.01010426138424211\n",
      "Epoch 231/300\n",
      "Average training loss: 4.9430895021226675\n",
      "Average test loss: 0.009487221783234013\n",
      "Epoch 232/300\n",
      "Average training loss: 4.567770315806071\n",
      "Average test loss: 0.011005727158652412\n",
      "Epoch 233/300\n",
      "Average training loss: 4.219818526373969\n",
      "Average test loss: 0.009599532512740956\n",
      "Epoch 234/300\n",
      "Average training loss: 3.884310392591688\n",
      "Average test loss: 0.009617978221012487\n",
      "Epoch 235/300\n",
      "Average training loss: 3.55767830212911\n",
      "Average test loss: 0.008897120549447007\n",
      "Epoch 236/300\n",
      "Average training loss: 3.233067214541965\n",
      "Average test loss: 0.009709171375052797\n",
      "Epoch 237/300\n",
      "Average training loss: 2.911891278584798\n",
      "Average test loss: 0.009731667281024986\n",
      "Epoch 238/300\n",
      "Average training loss: 2.59895531739129\n",
      "Average test loss: 0.008434685616857476\n",
      "Epoch 239/300\n",
      "Average training loss: 2.3014370886484783\n",
      "Average test loss: 0.008503971306814087\n",
      "Epoch 240/300\n",
      "Average training loss: 2.0174013458887736\n",
      "Average test loss: 0.007816734489467408\n",
      "Epoch 241/300\n",
      "Average training loss: 1.7342138862609864\n",
      "Average test loss: 0.007730209371281995\n",
      "Epoch 242/300\n",
      "Average training loss: 1.4683699259228176\n",
      "Average test loss: 0.0075552537995907994\n",
      "Epoch 243/300\n",
      "Average training loss: 1.2252706025441487\n",
      "Average test loss: 0.007294739327083031\n",
      "Epoch 244/300\n",
      "Average training loss: 1.0247373414569432\n",
      "Average test loss: 0.0070966738983988765\n",
      "Epoch 245/300\n",
      "Average training loss: 0.8514904722107781\n",
      "Average test loss: 0.007132074848645263\n",
      "Epoch 246/300\n",
      "Average training loss: 0.7001733699374729\n",
      "Average test loss: 0.007411392490069071\n",
      "Epoch 247/300\n",
      "Average training loss: 0.5716236436102126\n",
      "Average test loss: 0.006546774352590243\n",
      "Epoch 248/300\n",
      "Average training loss: 0.46100718757841325\n",
      "Average test loss: 0.00647470716221465\n",
      "Epoch 249/300\n",
      "Average training loss: 0.37614310921563043\n",
      "Average test loss: 0.006489470165222883\n",
      "Epoch 250/300\n",
      "Average training loss: 0.30988058235910204\n",
      "Average test loss: 0.0062997129853400915\n",
      "Epoch 251/300\n",
      "Average training loss: 0.2598639696704017\n",
      "Average test loss: 0.0062460496073795686\n",
      "Epoch 252/300\n",
      "Average training loss: 0.22246148573027716\n",
      "Average test loss: 0.006286343484703037\n",
      "Epoch 253/300\n",
      "Average training loss: 0.1932080518802007\n",
      "Average test loss: 0.00638024228811264\n",
      "Epoch 254/300\n",
      "Average training loss: 0.1695960501432419\n",
      "Average test loss: 0.006502607219335105\n",
      "Epoch 255/300\n",
      "Average training loss: 0.15173877007431455\n",
      "Average test loss: 0.006318853163056903\n",
      "Epoch 256/300\n",
      "Average training loss: 0.13914876684877608\n",
      "Average test loss: 0.006101538910427027\n",
      "Epoch 257/300\n",
      "Average training loss: 0.13035065433714124\n",
      "Average test loss: 0.0060515284136765535\n",
      "Epoch 258/300\n",
      "Average training loss: 0.12357574175463783\n",
      "Average test loss: 0.006138423902293046\n",
      "Epoch 259/300\n",
      "Average training loss: 0.11813429450326496\n",
      "Average test loss: 0.006649635511140029\n",
      "Epoch 260/300\n",
      "Average training loss: 0.11362058599127664\n",
      "Average test loss: 0.00848297967430618\n",
      "Epoch 261/300\n",
      "Average training loss: 0.10971453555425008\n",
      "Average test loss: 0.005990776562649343\n",
      "Epoch 262/300\n",
      "Average training loss: 0.10662029783593284\n",
      "Average test loss: 0.006102170258760452\n",
      "Epoch 263/300\n",
      "Average training loss: 0.10421490077177684\n",
      "Average test loss: 0.024182624590065743\n",
      "Epoch 264/300\n",
      "Average training loss: 0.10218216482135985\n",
      "Average test loss: 0.007211124466525184\n",
      "Epoch 265/300\n",
      "Average training loss: 0.10019000017642975\n",
      "Average test loss: 0.006785739147000843\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09851923398176829\n",
      "Average test loss: 0.0060485239070322775\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09731280925538804\n",
      "Average test loss: 0.0061320042680535055\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09594142246246339\n",
      "Average test loss: 0.007558952312502596\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0946674024131563\n",
      "Average test loss: 0.006163544992191924\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09371971495283975\n",
      "Average test loss: 0.006073019318696526\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09276589763826794\n",
      "Average test loss: 0.006050602060639196\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09203263488743041\n",
      "Average test loss: 0.0063138500398231875\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0913864573041598\n",
      "Average test loss: 0.006803621285905441\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09064018597867754\n",
      "Average test loss: 0.006026103504416016\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09037200736999512\n",
      "Average test loss: 0.06481287743316756\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09016053216324912\n",
      "Average test loss: 0.006099704934077131\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08972385497225656\n",
      "Average test loss: 0.006046161990198824\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08950440829661158\n",
      "Average test loss: 0.006259849686589506\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08926911920971341\n",
      "Average test loss: 0.0060229132763213586\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0889195574257109\n",
      "Average test loss: 0.0059946131246785325\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08893354924519857\n",
      "Average test loss: 0.006052025924333268\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08852554392814636\n",
      "Average test loss: 0.006066551866630713\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0884013757440779\n",
      "Average test loss: 0.0062444254793226715\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08803395011027654\n",
      "Average test loss: 0.006077271652718385\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08818520596954557\n",
      "Average test loss: 0.006265238511893484\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0875629536178377\n",
      "Average test loss: 0.006115706163148086\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08749500136242973\n",
      "Average test loss: 0.006067289496875471\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08703820812702179\n",
      "Average test loss: 0.006029005447195636\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08704129618406296\n",
      "Average test loss: 0.00604171016977893\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08702868149677913\n",
      "Average test loss: 0.006013571390261253\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10083707702159882\n",
      "Average test loss: 0.006084237691015005\n",
      "Epoch 292/300\n",
      "Average training loss: 0.1546331598824925\n",
      "Average test loss: 0.0553969255288442\n",
      "Epoch 293/300\n",
      "Average training loss: 15377.544160718282\n",
      "Average test loss: 0.09959528098834886\n",
      "Epoch 294/300\n",
      "Average training loss: 15.08572083791097\n",
      "Average test loss: 3.1592346675131058\n",
      "Epoch 295/300\n",
      "Average training loss: 14.138334749857584\n",
      "Average test loss: 3.259465357012219\n",
      "Epoch 296/300\n",
      "Average training loss: 13.510627504136828\n",
      "Average test loss: 0.06214096173809634\n",
      "Epoch 297/300\n",
      "Average training loss: 12.869976927863227\n",
      "Average test loss: 0.032904630868799155\n",
      "Epoch 298/300\n",
      "Average training loss: 12.236403383890789\n",
      "Average test loss: 0.06664861784213119\n",
      "Epoch 299/300\n",
      "Average training loss: 11.655320911831327\n",
      "Average test loss: 0.028810039657685492\n",
      "Epoch 300/300\n",
      "Average training loss: 11.11089948018392\n",
      "Average test loss: 0.08561639575163524\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.0076487477090623\n",
      "Average test loss: 0.010824105675021808\n",
      "Epoch 2/300\n",
      "Average training loss: 0.24289895096090106\n",
      "Average test loss: 0.007202672882626454\n",
      "Epoch 3/300\n",
      "Average training loss: 0.16921955670250788\n",
      "Average test loss: 0.006551546179172065\n",
      "Epoch 4/300\n",
      "Average training loss: 0.14273994318644206\n",
      "Average test loss: 0.00644737627191676\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1291287910607126\n",
      "Average test loss: 0.00648677983507514\n",
      "Epoch 6/300\n",
      "Average training loss: 0.12120898481210073\n",
      "Average test loss: 0.007462061570750343\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11502520245313644\n",
      "Average test loss: 0.005828398845261998\n",
      "Epoch 8/300\n",
      "Average training loss: 0.11004370029767355\n",
      "Average test loss: 0.005448637523584896\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10622320543395149\n",
      "Average test loss: 0.0051873448400033845\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10170972550577588\n",
      "Average test loss: 0.008575966159502665\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09801450133323669\n",
      "Average test loss: 0.0054868094362318515\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09433683483468162\n",
      "Average test loss: 0.006057337802317407\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09143228273921543\n",
      "Average test loss: 0.00727256370956699\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08831377168165312\n",
      "Average test loss: 0.004724248349252674\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08615711122751236\n",
      "Average test loss: 0.004595386166953378\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08366054395834605\n",
      "Average test loss: 0.00447915352611906\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08134990185168055\n",
      "Average test loss: 0.004473756056485904\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07958132546477847\n",
      "Average test loss: 0.004802928186746107\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07803547394937939\n",
      "Average test loss: 0.004798968412395981\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07628830067978964\n",
      "Average test loss: 0.004909352807535065\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07528681570291519\n",
      "Average test loss: 0.0041436023203035195\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07361156584819158\n",
      "Average test loss: 0.004295252970937226\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07241793088118235\n",
      "Average test loss: 0.004027094824032651\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07141538117991554\n",
      "Average test loss: 0.004022191221101416\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07058484478791555\n",
      "Average test loss: 0.0045547104639311635\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06981264775329166\n",
      "Average test loss: 0.003967441461359461\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06859455029831993\n",
      "Average test loss: 0.003942743316292763\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06802508398228221\n",
      "Average test loss: 0.003888331301924255\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06732991367578506\n",
      "Average test loss: 0.010579420620368587\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06684999975893233\n",
      "Average test loss: 0.0037842140226728386\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06612473457389408\n",
      "Average test loss: 0.003854603770499428\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06549939459562301\n",
      "Average test loss: 0.003741598037795888\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06506969355874591\n",
      "Average test loss: 0.003793166740073098\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06454355485240619\n",
      "Average test loss: 0.0037244318328383897\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06388288123740091\n",
      "Average test loss: 0.00363795263837609\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0634797534081671\n",
      "Average test loss: 0.0036925039568709004\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06304641123612721\n",
      "Average test loss: 0.0036895902968115276\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06294820889499453\n",
      "Average test loss: 0.003683398504017128\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06251977949341138\n",
      "Average test loss: 0.003604443187514941\n",
      "Epoch 40/300\n",
      "Average training loss: 0.062112787001662784\n",
      "Average test loss: 0.0037988873716029857\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06205826686819394\n",
      "Average test loss: 0.00379478629761272\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06150391468074587\n",
      "Average test loss: 0.003578701630027758\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06189230228463809\n",
      "Average test loss: 0.003649116565576858\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06105184734861056\n",
      "Average test loss: 0.003584845025092363\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06545300137665537\n",
      "Average test loss: 0.003581903084077769\n",
      "Epoch 46/300\n",
      "Average training loss: 0.563905536532402\n",
      "Average test loss: 0.00487153177936044\n",
      "Epoch 47/300\n",
      "Average training loss: 0.3211512191560533\n",
      "Average test loss: 0.004125999870813555\n",
      "Epoch 48/300\n",
      "Average training loss: 0.15325093966060216\n",
      "Average test loss: 0.004566479670504729\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1097090720600552\n",
      "Average test loss: 0.003929158345071806\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09223435311847263\n",
      "Average test loss: 0.003762593321502209\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08404124389754401\n",
      "Average test loss: 0.0036853108329491483\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07914546435740259\n",
      "Average test loss: 0.003729190104951461\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07605310755968094\n",
      "Average test loss: 0.003650154247879982\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07367054631974962\n",
      "Average test loss: 0.0036179400564481817\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07170921242237091\n",
      "Average test loss: 0.003824390543624759\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07012407045894199\n",
      "Average test loss: 0.0036243690270930527\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0689638917181227\n",
      "Average test loss: 0.0036313995988004736\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06773947848545181\n",
      "Average test loss: 0.003648722733888361\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0669524498714341\n",
      "Average test loss: 0.0037583636484212345\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06599879780080584\n",
      "Average test loss: 0.0036846524653956292\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06528988756073846\n",
      "Average test loss: 0.0035740455322795443\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06462173492709795\n",
      "Average test loss: 0.003810203584531943\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06404608025153478\n",
      "Average test loss: 0.0037170426377819642\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06360957627826266\n",
      "Average test loss: 0.003594934901636508\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06306941759917471\n",
      "Average test loss: 0.0035852563619199725\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06257406060563193\n",
      "Average test loss: 0.00354205016605556\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0622393786476718\n",
      "Average test loss: 0.0035743799878077374\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06223213137189547\n",
      "Average test loss: 0.00362163997731275\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06155908507439825\n",
      "Average test loss: 0.0035323196924808955\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0611494469874435\n",
      "Average test loss: 0.0036070200947837697\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0611936861442195\n",
      "Average test loss: 0.0035289878810031545\n",
      "Epoch 72/300\n",
      "Average training loss: 0.060709248860677086\n",
      "Average test loss: 0.003556117731663916\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06089501196808285\n",
      "Average test loss: 0.0035983381490740513\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06038854186733564\n",
      "Average test loss: 0.0036091334650086033\n",
      "Epoch 75/300\n",
      "Average training loss: 0.060150800744692486\n",
      "Average test loss: 0.0035531764069779053\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06004501466618644\n",
      "Average test loss: 0.0035401026287840472\n",
      "Epoch 77/300\n",
      "Average training loss: 0.059803689741426046\n",
      "Average test loss: 0.003583516367193725\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05949666291475296\n",
      "Average test loss: 0.003521071210814019\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05921331533127361\n",
      "Average test loss: 0.003535931519543131\n",
      "Epoch 80/300\n",
      "Average training loss: 0.058847843044333985\n",
      "Average test loss: 0.0036254533732102976\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05898281384176678\n",
      "Average test loss: 0.0037624256776438818\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07303171581361029\n",
      "Average test loss: 0.0035835276436474587\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06002839502029949\n",
      "Average test loss: 0.0035248881868190237\n",
      "Epoch 84/300\n",
      "Average training loss: 0.058918035440974764\n",
      "Average test loss: 0.003482226368246807\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05858239499065611\n",
      "Average test loss: 0.0035077613157530625\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05833902891476949\n",
      "Average test loss: 0.0034872455888738233\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05825506778226958\n",
      "Average test loss: 0.0035040335973931685\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0580140263206429\n",
      "Average test loss: 0.003499935010448098\n",
      "Epoch 89/300\n",
      "Average training loss: 0.058122767080863315\n",
      "Average test loss: 0.0036011267412039967\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05763403820329242\n",
      "Average test loss: 0.0034646706701152853\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05751655983924866\n",
      "Average test loss: 0.007991379531721274\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05743805415100522\n",
      "Average test loss: 0.008552882090210915\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05823755622572369\n",
      "Average test loss: 0.0034639775454998016\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05740852430793974\n",
      "Average test loss: 0.0037726109470758175\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05693982161084811\n",
      "Average test loss: 0.003599929378678401\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05727708357572556\n",
      "Average test loss: 0.003483367897156212\n",
      "Epoch 97/300\n",
      "Average training loss: 0.056719977577527364\n",
      "Average test loss: 0.0035775009017023776\n",
      "Epoch 98/300\n",
      "Average training loss: 0.056366559045182334\n",
      "Average test loss: 0.003608593085159858\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05641084334254265\n",
      "Average test loss: 0.0035036281496286393\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05628508142630259\n",
      "Average test loss: 0.0034522834269122946\n",
      "Epoch 101/300\n",
      "Average training loss: 0.056245598889059494\n",
      "Average test loss: 0.0035651151925946275\n",
      "Epoch 102/300\n",
      "Average training loss: 0.055991515974203744\n",
      "Average test loss: 0.0035462696303923923\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05569330888655451\n",
      "Average test loss: 0.0034527404345571994\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05581486962901221\n",
      "Average test loss: 0.0034767139852046967\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05557859689328406\n",
      "Average test loss: 0.003561573768655459\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05547360952364074\n",
      "Average test loss: 0.0034879636162271103\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05620263529817263\n",
      "Average test loss: 0.0034756453037261964\n",
      "Epoch 108/300\n",
      "Average training loss: 0.055152851813369326\n",
      "Average test loss: 0.0034825750305834745\n",
      "Epoch 109/300\n",
      "Average training loss: 0.054890323860777746\n",
      "Average test loss: 0.0035756215862929822\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05484369314710299\n",
      "Average test loss: 0.003495327355547084\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05485539984371927\n",
      "Average test loss: 0.00368549443822768\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05499443045258522\n",
      "Average test loss: 0.0036800097746567595\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05457930053936111\n",
      "Average test loss: 0.00356153247712387\n",
      "Epoch 114/300\n",
      "Average training loss: 0.054535340044233535\n",
      "Average test loss: 0.0037983303672323625\n",
      "Epoch 115/300\n",
      "Average training loss: 0.054304931140608255\n",
      "Average test loss: 0.0034988474022183153\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05428319595588578\n",
      "Average test loss: 0.005999372236430645\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05416452844937642\n",
      "Average test loss: 0.004154431555834081\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05426178875896666\n",
      "Average test loss: 0.0034609648812976147\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0539059171643522\n",
      "Average test loss: 0.003481830622173018\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0538984489440918\n",
      "Average test loss: 0.0035088773363580305\n",
      "Epoch 121/300\n",
      "Average training loss: 0.053619467198848726\n",
      "Average test loss: 0.003500201366427872\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05402951443195343\n",
      "Average test loss: 0.004037198698148131\n",
      "Epoch 123/300\n",
      "Average training loss: 0.054137165033155014\n",
      "Average test loss: 0.00364712978568342\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05330125887857543\n",
      "Average test loss: 0.003558465963539978\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05332638242840767\n",
      "Average test loss: 0.003520013954490423\n",
      "Epoch 126/300\n",
      "Average training loss: 0.053234925776720045\n",
      "Average test loss: 0.0035007936155630483\n",
      "Epoch 127/300\n",
      "Average training loss: 0.053213428497314454\n",
      "Average test loss: 0.003601376798831754\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05305760699841711\n",
      "Average test loss: 0.0037390345198412738\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05299520376986928\n",
      "Average test loss: 0.003490682761495312\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05297969784339269\n",
      "Average test loss: 0.0035320454289515814\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05320375538865725\n",
      "Average test loss: 0.0038280804440793065\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05332288797365294\n",
      "Average test loss: 0.003578341486553351\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05278063812189632\n",
      "Average test loss: 0.0036397449214839272\n",
      "Epoch 134/300\n",
      "Average training loss: 0.053082333005136914\n",
      "Average test loss: 0.003716907286188669\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05263342097732756\n",
      "Average test loss: 0.003530534968400995\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05235076264871491\n",
      "Average test loss: 0.003563786880320145\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05260249603125784\n",
      "Average test loss: 0.003836093975024091\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05204974260926247\n",
      "Average test loss: 0.004050140560087231\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05238707876867718\n",
      "Average test loss: 0.003824654824203915\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05214855596754286\n",
      "Average test loss: 0.0036041411674684947\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05193776641951667\n",
      "Average test loss: 0.003505931225294868\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05187029243840112\n",
      "Average test loss: 0.0035387213735116854\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05186529224448734\n",
      "Average test loss: 0.003924955889582634\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0519192665749126\n",
      "Average test loss: 0.004211536150011751\n",
      "Epoch 145/300\n",
      "Average training loss: 0.051697250223822065\n",
      "Average test loss: 0.003500875580434998\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05160570482744111\n",
      "Average test loss: 0.003557890187948942\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05158602437376976\n",
      "Average test loss: 0.006546347058067719\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05175026376710998\n",
      "Average test loss: 0.00348837327481144\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05139207894603411\n",
      "Average test loss: 0.004109608129701681\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05133703738451004\n",
      "Average test loss: 0.0036373180786354676\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05133156920141644\n",
      "Average test loss: 0.0035860329456627368\n",
      "Epoch 152/300\n",
      "Average training loss: 0.051201298531558775\n",
      "Average test loss: 0.0035799843430933025\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05107845352424516\n",
      "Average test loss: 0.003580366981112295\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05154001935323079\n",
      "Average test loss: 0.0036137838999016417\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05119585965739356\n",
      "Average test loss: 0.0036006937753409146\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05091580950220426\n",
      "Average test loss: 0.003862790385468139\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050734508607122634\n",
      "Average test loss: 0.0035779787452063627\n",
      "Epoch 158/300\n",
      "Average training loss: 0.051020333651039335\n",
      "Average test loss: 0.0035196455077578623\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05094189112716251\n",
      "Average test loss: 0.003540805368994673\n",
      "Epoch 160/300\n",
      "Average training loss: 0.050723131358623504\n",
      "Average test loss: 0.0035574945658445357\n",
      "Epoch 161/300\n",
      "Average training loss: 0.050694993240965734\n",
      "Average test loss: 0.0035511755442453755\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05056594924132029\n",
      "Average test loss: 0.0037084277332242992\n",
      "Epoch 163/300\n",
      "Average training loss: 0.050462348484330705\n",
      "Average test loss: 0.0036727703228178954\n",
      "Epoch 164/300\n",
      "Average training loss: 0.050326653778553006\n",
      "Average test loss: 0.003896373096232613\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05058613869216707\n",
      "Average test loss: 0.00475777043402195\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05051120324267282\n",
      "Average test loss: 0.00359402895387676\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05024940146009127\n",
      "Average test loss: 0.0036472154829858077\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0500869985057248\n",
      "Average test loss: 0.0036860621900608142\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0499861228350136\n",
      "Average test loss: 0.004402576740003295\n",
      "Epoch 170/300\n",
      "Average training loss: 0.050276876174741324\n",
      "Average test loss: 0.0036545616930557623\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05033036301864518\n",
      "Average test loss: 0.004353135271204843\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04994324559304449\n",
      "Average test loss: 0.003718237570176522\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0497837823232015\n",
      "Average test loss: 0.0036025563006599746\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04982609002788862\n",
      "Average test loss: 0.004405056368559599\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04978383156326082\n",
      "Average test loss: 0.0035716473278072144\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0497322485976749\n",
      "Average test loss: 0.0037866960579736367\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0496804001265102\n",
      "Average test loss: 0.003634026754647493\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0498850977619489\n",
      "Average test loss: 0.003588577962997887\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0495743469281329\n",
      "Average test loss: 0.003706407046980328\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04979433111349742\n",
      "Average test loss: 0.003762672576432427\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04939350013931592\n",
      "Average test loss: 0.0036555674527254368\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04958482849266794\n",
      "Average test loss: 0.0035634212527010174\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04938881321085824\n",
      "Average test loss: 0.003753042588631312\n",
      "Epoch 184/300\n",
      "Average training loss: 0.049489987480971546\n",
      "Average test loss: 0.0036173973213881254\n",
      "Epoch 185/300\n",
      "Average training loss: 0.049213988482952116\n",
      "Average test loss: 0.0038411364942375157\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04945960125658247\n",
      "Average test loss: 0.005395339743337698\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04907651474078496\n",
      "Average test loss: 0.003765712028576268\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04925539925032192\n",
      "Average test loss: 0.0035920539885345433\n",
      "Epoch 189/300\n",
      "Average training loss: 0.049484825919071836\n",
      "Average test loss: 0.0037219909698598913\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04932164112726847\n",
      "Average test loss: 0.0036608360612557993\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04888950817783674\n",
      "Average test loss: 0.0036087448170615565\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04925555507673158\n",
      "Average test loss: 0.0036137122996151446\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04918968839446704\n",
      "Average test loss: 0.00364886727090925\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04892626037862566\n",
      "Average test loss: 0.0036646927988363636\n",
      "Epoch 195/300\n",
      "Average training loss: 0.049003843668434355\n",
      "Average test loss: 0.0036744877360761164\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04897717221577962\n",
      "Average test loss: 0.0036054385846687686\n",
      "Epoch 197/300\n",
      "Average training loss: 0.048774218953318065\n",
      "Average test loss: 0.0037426187112513517\n",
      "Epoch 198/300\n",
      "Average training loss: 0.048648639692200554\n",
      "Average test loss: 0.003654155001872116\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04881500294142299\n",
      "Average test loss: 0.05493501743343141\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04862791811757618\n",
      "Average test loss: 0.0036365127745601866\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04879162543680933\n",
      "Average test loss: 0.0036796667294369803\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04840133847461806\n",
      "Average test loss: 0.003616618402302265\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04842932565344704\n",
      "Average test loss: 0.005517126200513707\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04860751764973004\n",
      "Average test loss: 0.0036590093655718696\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04893585461709234\n",
      "Average test loss: 0.0036562807404746612\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04859706735279825\n",
      "Average test loss: 0.0038086309197048348\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04844854557183054\n",
      "Average test loss: 0.003707736739474866\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04846791842910979\n",
      "Average test loss: 0.003645443834985296\n",
      "Epoch 209/300\n",
      "Average training loss: 0.048218631701336966\n",
      "Average test loss: 0.0037972339511745507\n",
      "Epoch 210/300\n",
      "Average training loss: 0.048526215857929655\n",
      "Average test loss: 0.003733538480061624\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04810474371910095\n",
      "Average test loss: 0.0037072827073021068\n",
      "Epoch 212/300\n",
      "Average training loss: 0.048278391275140976\n",
      "Average test loss: 0.0037906629163771866\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04815240763624509\n",
      "Average test loss: 0.0038793619035018814\n",
      "Epoch 214/300\n",
      "Average training loss: 0.048016136709186766\n",
      "Average test loss: 0.003813340893636147\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04797244130571683\n",
      "Average test loss: 0.0036688480416519776\n",
      "Epoch 216/300\n",
      "Average training loss: 0.048058465752336715\n",
      "Average test loss: 0.003786858638127645\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04809838114182154\n",
      "Average test loss: 0.0036817277752690847\n",
      "Epoch 218/300\n",
      "Average training loss: 0.048263873835404716\n",
      "Average test loss: 0.006144683755313357\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04811515403787295\n",
      "Average test loss: 0.0036881563017765683\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04778383774227566\n",
      "Average test loss: 0.0036461962511142097\n",
      "Epoch 221/300\n",
      "Average training loss: 0.048017101334200965\n",
      "Average test loss: 0.0037435219366517333\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04797351893782616\n",
      "Average test loss: 0.003721223020926118\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04798802568846279\n",
      "Average test loss: 0.0039520470282683766\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04775298309657309\n",
      "Average test loss: 0.0038559374449153743\n",
      "Epoch 225/300\n",
      "Average training loss: 0.047695951736635635\n",
      "Average test loss: 0.003746590449474752\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04782810843653149\n",
      "Average test loss: 0.0039433011425038175\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04793859600689676\n",
      "Average test loss: 0.003645161249157455\n",
      "Epoch 228/300\n",
      "Average training loss: 0.047528045568201276\n",
      "Average test loss: 0.0036775027664585247\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04768354487915834\n",
      "Average test loss: 0.003774198221663634\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04817066029376454\n",
      "Average test loss: 0.0037346026003360747\n",
      "Epoch 231/300\n",
      "Average training loss: 0.047633945087591806\n",
      "Average test loss: 0.0037872443524085814\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04740269531144036\n",
      "Average test loss: 0.00601690807669527\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04744249036245876\n",
      "Average test loss: 0.0038357131148998936\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04732737611730894\n",
      "Average test loss: 0.003928751221340563\n",
      "Epoch 235/300\n",
      "Average training loss: 0.047697274380260044\n",
      "Average test loss: 0.052339697225226293\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04752657656868299\n",
      "Average test loss: 0.003717302791567312\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04763019117381838\n",
      "Average test loss: 0.003673001882309715\n",
      "Epoch 238/300\n",
      "Average training loss: 0.047293454584148194\n",
      "Average test loss: 0.0036679169318328303\n",
      "Epoch 239/300\n",
      "Average training loss: 0.047353178166680866\n",
      "Average test loss: 0.0037249453984614877\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04739147316084968\n",
      "Average test loss: 0.0037005681689414715\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04722797814673848\n",
      "Average test loss: 0.003966896686702966\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04734378068976932\n",
      "Average test loss: 0.003977811003310813\n",
      "Epoch 243/300\n",
      "Average training loss: 0.047355346047215995\n",
      "Average test loss: 0.003851997625082731\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04736312504609426\n",
      "Average test loss: 0.0036890346507231393\n",
      "Epoch 245/300\n",
      "Average training loss: 0.047363232615921236\n",
      "Average test loss: 0.0036760980159872107\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04715945246650113\n",
      "Average test loss: 0.004199767150398758\n",
      "Epoch 247/300\n",
      "Average training loss: 0.046994740297396975\n",
      "Average test loss: 0.0037052872156103454\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0469886151154836\n",
      "Average test loss: 0.003691575997405582\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04687040467063586\n",
      "Average test loss: 0.003941168996401959\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04715606198377079\n",
      "Average test loss: 0.004032736269550191\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04715414874090089\n",
      "Average test loss: 0.0037211535463316575\n",
      "Epoch 252/300\n",
      "Average training loss: 0.046855005357000566\n",
      "Average test loss: 0.0037242502379748557\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04682295903563499\n",
      "Average test loss: 0.0037837592667589585\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04724090940091345\n",
      "Average test loss: 0.004065884224449595\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0468490392267704\n",
      "Average test loss: 0.0036627839538786147\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04682899237010214\n",
      "Average test loss: 0.0037386787459254265\n",
      "Epoch 257/300\n",
      "Average training loss: 0.046795602192481356\n",
      "Average test loss: 0.003771715325406856\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04697788941529062\n",
      "Average test loss: 0.004663161203886072\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04666621460186111\n",
      "Average test loss: 0.0038813634138140413\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0467696724500921\n",
      "Average test loss: 0.003880971742173036\n",
      "Epoch 261/300\n",
      "Average training loss: 0.046939091347985795\n",
      "Average test loss: 0.003729410940574275\n",
      "Epoch 262/300\n",
      "Average training loss: 0.046823810564147104\n",
      "Average test loss: 0.003813701324785749\n",
      "Epoch 263/300\n",
      "Average training loss: 0.046546401613288456\n",
      "Average test loss: 0.0036934529408398603\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04667410055134032\n",
      "Average test loss: 0.003911911185002989\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04685586933626069\n",
      "Average test loss: 0.0073881960519486005\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04657401179936197\n",
      "Average test loss: 0.007307843519581689\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04643925381700198\n",
      "Average test loss: 0.0036538379275136523\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04659741027818786\n",
      "Average test loss: 0.004056681586222516\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04652883454826143\n",
      "Average test loss: 0.004040855522371001\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04671324022610982\n",
      "Average test loss: 0.0038254138552066353\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0464068240125974\n",
      "Average test loss: 0.003981282136299544\n",
      "Epoch 272/300\n",
      "Average training loss: 0.046521142452955244\n",
      "Average test loss: 0.004366335963830352\n",
      "Epoch 273/300\n",
      "Average training loss: 0.046359553598695334\n",
      "Average test loss: 0.003771716695692804\n",
      "Epoch 274/300\n",
      "Average training loss: 0.046379668368233574\n",
      "Average test loss: 0.004029780298885372\n",
      "Epoch 275/300\n",
      "Average training loss: 0.046307447099023395\n",
      "Average test loss: 0.003798404464291202\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04646770633922683\n",
      "Average test loss: 0.0038139512406455147\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04628879168629646\n",
      "Average test loss: 0.0038628200796536273\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04653264154659377\n",
      "Average test loss: 0.0037326643307589824\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04642831705013911\n",
      "Average test loss: 0.003847139864746067\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0462772385229667\n",
      "Average test loss: 0.0037722751225034395\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04624414629075262\n",
      "Average test loss: 0.004227365497499704\n",
      "Epoch 282/300\n",
      "Average training loss: 0.046119144370158516\n",
      "Average test loss: 0.003727273585481776\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04628599824176894\n",
      "Average test loss: 0.003999882494409879\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04613355669048098\n",
      "Average test loss: 0.0039348771027806735\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0462203338444233\n",
      "Average test loss: 0.0038349529136386183\n",
      "Epoch 286/300\n",
      "Average training loss: 0.046099972075886196\n",
      "Average test loss: 0.003882864481459061\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04637572870983018\n",
      "Average test loss: 0.0037692149895139867\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04596060811811023\n",
      "Average test loss: 0.0039956833625005355\n",
      "Epoch 289/300\n",
      "Average training loss: 0.046577715784311295\n",
      "Average test loss: 0.02619300596250428\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04621536682049433\n",
      "Average test loss: 0.003970757454012831\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04582872805330489\n",
      "Average test loss: 0.003800718152274688\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04606337582071622\n",
      "Average test loss: 0.0037985736613886222\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04618497360414929\n",
      "Average test loss: 0.003883422767329547\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04597130363848474\n",
      "Average test loss: 0.003915259868320492\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04591734613643752\n",
      "Average test loss: 0.003874076676865419\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04619523224896855\n",
      "Average test loss: 0.014355188957519001\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04572485404875543\n",
      "Average test loss: 0.003877495147080885\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04599983974628978\n",
      "Average test loss: 0.003949935223700272\n",
      "Epoch 299/300\n",
      "Average training loss: 0.046016955859131285\n",
      "Average test loss: 0.0039621256070418494\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04596131368809276\n",
      "Average test loss: 0.00369030664405889\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6732093043592242\n",
      "Average test loss: 0.007473177533596754\n",
      "Epoch 2/300\n",
      "Average training loss: 0.19601950124899548\n",
      "Average test loss: 0.0056006531028283965\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1377073876592848\n",
      "Average test loss: 0.005366948809060785\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1152026058766577\n",
      "Average test loss: 0.004573542307234473\n",
      "Epoch 5/300\n",
      "Average training loss: 0.10363337782356474\n",
      "Average test loss: 0.0048700677239232595\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09639817439847523\n",
      "Average test loss: 0.004271490722480747\n",
      "Epoch 7/300\n",
      "Average training loss: 0.09048875610033671\n",
      "Average test loss: 0.004751822312879893\n",
      "Epoch 8/300\n",
      "Average training loss: 0.086234908454948\n",
      "Average test loss: 0.004380787689238786\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08240272841188642\n",
      "Average test loss: 0.004266661177492804\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0788748102751043\n",
      "Average test loss: 0.004105415501942237\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07511939320630498\n",
      "Average test loss: 0.003882401984391941\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07208054181271129\n",
      "Average test loss: 0.003890637043449614\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06931157638629278\n",
      "Average test loss: 0.003527177129768663\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06657792295681106\n",
      "Average test loss: 0.0034239017959270214\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06397256841262182\n",
      "Average test loss: 0.003379104365491205\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0619858301281929\n",
      "Average test loss: 0.0030903000612225798\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05992655186189545\n",
      "Average test loss: 0.003896522634766168\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05841218707296583\n",
      "Average test loss: 0.003011199714822902\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05663854830463727\n",
      "Average test loss: 0.0029175349144885936\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05543945160011451\n",
      "Average test loss: 0.0028933910857886075\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05420338280995687\n",
      "Average test loss: 0.0028641229305002426\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05314801908201641\n",
      "Average test loss: 0.0028338335404793423\n",
      "Epoch 23/300\n",
      "Average training loss: 0.051995185212956534\n",
      "Average test loss: 0.002745071482948131\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05119974884721968\n",
      "Average test loss: 0.0026969187092036008\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05035748014516301\n",
      "Average test loss: 0.0026387416229893762\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04946725481086307\n",
      "Average test loss: 0.0025382736157625913\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04893219329913457\n",
      "Average test loss: 0.0025455447249114515\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04826361828711298\n",
      "Average test loss: 0.0025349962171167134\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04768692466451062\n",
      "Average test loss: 0.0025807521471546756\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04706832571824392\n",
      "Average test loss: 0.002559404784399602\n",
      "Epoch 31/300\n",
      "Average training loss: 0.046706279085742104\n",
      "Average test loss: 0.0025470447573396893\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04650356565250291\n",
      "Average test loss: 0.0025036923938120403\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05014934072891871\n",
      "Average test loss: 0.0026778506925329566\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04759131539861361\n",
      "Average test loss: 0.002433197559788823\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04578533293141259\n",
      "Average test loss: 0.002524802238576942\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04554740729265743\n",
      "Average test loss: 0.0023900732731239665\n",
      "Epoch 37/300\n",
      "Average training loss: 0.045373144338528314\n",
      "Average test loss: 0.00239006854655842\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04485507493548923\n",
      "Average test loss: 0.002394754999420709\n",
      "Epoch 39/300\n",
      "Average training loss: 0.044798810412486394\n",
      "Average test loss: 0.002364832527935505\n",
      "Epoch 40/300\n",
      "Average training loss: 0.044463299337360596\n",
      "Average test loss: 0.002389612067490816\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04422501411040624\n",
      "Average test loss: 0.0023473064868400496\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0440840202305052\n",
      "Average test loss: 0.0023542819939967657\n",
      "Epoch 43/300\n",
      "Average training loss: 0.043802622879544895\n",
      "Average test loss: 0.0023327797330502005\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04370496032966508\n",
      "Average test loss: 0.0027575307664357953\n",
      "Epoch 45/300\n",
      "Average training loss: 0.043537514431609046\n",
      "Average test loss: 0.0023358485301335653\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04313978803488943\n",
      "Average test loss: 0.0023176385015249252\n",
      "Epoch 47/300\n",
      "Average training loss: 0.042965177516142525\n",
      "Average test loss: 0.0023178496405275332\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04266796670357386\n",
      "Average test loss: 0.0027427634698235326\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04255091900626818\n",
      "Average test loss: 0.0024956783290124604\n",
      "Epoch 50/300\n",
      "Average training loss: 0.042379362417591944\n",
      "Average test loss: 0.002328681734494037\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04230465303526984\n",
      "Average test loss: 0.0023344221119251515\n",
      "Epoch 52/300\n",
      "Average training loss: 0.042244756498270565\n",
      "Average test loss: 0.0026723617619524398\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04174844832883941\n",
      "Average test loss: 0.009376579408016469\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04164530397454898\n",
      "Average test loss: 0.00227526857631488\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04158163507448302\n",
      "Average test loss: 0.00237630250170413\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04354657779799567\n",
      "Average test loss: 0.0022905558314588333\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04157309796743923\n",
      "Average test loss: 0.002691406572237611\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04111985855963495\n",
      "Average test loss: 0.0022668170964138374\n",
      "Epoch 59/300\n",
      "Average training loss: 0.041049870951308146\n",
      "Average test loss: 0.0022830393790370888\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04093272996942202\n",
      "Average test loss: 0.002360274570890599\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04078683121005694\n",
      "Average test loss: 0.002327788104613622\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04072786160144541\n",
      "Average test loss: 0.0025037019316934877\n",
      "Epoch 63/300\n",
      "Average training loss: 0.040646910111109415\n",
      "Average test loss: 0.002298199836889075\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0406038399901655\n",
      "Average test loss: 0.0027674817088991404\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04045558854606417\n",
      "Average test loss: 0.0022794689850674735\n",
      "Epoch 66/300\n",
      "Average training loss: 0.040288130743636026\n",
      "Average test loss: 0.0023463980046411355\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04023867977824476\n",
      "Average test loss: 0.0023361127266867295\n",
      "Epoch 68/300\n",
      "Average training loss: 0.040116202334562936\n",
      "Average test loss: 0.002476442720947994\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04003606355852551\n",
      "Average test loss: 0.0022725801024999883\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03993609774443838\n",
      "Average test loss: 0.002294570565016733\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03986724950207604\n",
      "Average test loss: 0.0022771431083480516\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03988691105445226\n",
      "Average test loss: 0.002264960427561568\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03961700730522474\n",
      "Average test loss: 0.0022715503516503506\n",
      "Epoch 74/300\n",
      "Average training loss: 0.039464319992396565\n",
      "Average test loss: 0.0027317660744819376\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03931139466497633\n",
      "Average test loss: 0.002333789471950796\n",
      "Epoch 76/300\n",
      "Average training loss: 0.039553611937496395\n",
      "Average test loss: 0.0022719687982979747\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03927770448062155\n",
      "Average test loss: 0.014250209991302755\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039488399624824526\n",
      "Average test loss: 0.00232620134525415\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03892800806131628\n",
      "Average test loss: 0.0023908692273414797\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03902612586153878\n",
      "Average test loss: 0.0028887018010848096\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03892848668495814\n",
      "Average test loss: 0.0022870257006337245\n",
      "Epoch 82/300\n",
      "Average training loss: 0.038969758823513986\n",
      "Average test loss: 0.002536718714982271\n",
      "Epoch 83/300\n",
      "Average training loss: 0.038599063866668275\n",
      "Average test loss: 0.002232514334635602\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03864095862706502\n",
      "Average test loss: 0.002226969839591119\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03884948448671235\n",
      "Average test loss: 0.8354896515508493\n",
      "Epoch 86/300\n",
      "Average training loss: 0.038595154616567824\n",
      "Average test loss: 0.0022848797070069444\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03840541218386756\n",
      "Average test loss: 0.0022924409405742255\n",
      "Epoch 88/300\n",
      "Average training loss: 0.038303323503997594\n",
      "Average test loss: 0.002381082691873113\n",
      "Epoch 89/300\n",
      "Average training loss: 0.038367584480179684\n",
      "Average test loss: 0.002557209104299545\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03831253169808123\n",
      "Average test loss: 0.0022955894948293765\n",
      "Epoch 91/300\n",
      "Average training loss: 0.038435191010435424\n",
      "Average test loss: 0.002337870575487614\n",
      "Epoch 92/300\n",
      "Average training loss: 0.037846572644180725\n",
      "Average test loss: 0.002655523473189937\n",
      "Epoch 93/300\n",
      "Average training loss: 0.037891704592439866\n",
      "Average test loss: 0.0022812284899668563\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03787289584345288\n",
      "Average test loss: 0.002262029314827588\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03783420579300986\n",
      "Average test loss: 0.0022724402906994026\n",
      "Epoch 96/300\n",
      "Average training loss: 0.038012026454011597\n",
      "Average test loss: 0.002296493959095743\n",
      "Epoch 97/300\n",
      "Average training loss: 0.037733980463610754\n",
      "Average test loss: 0.0022627950047867168\n",
      "Epoch 98/300\n",
      "Average training loss: 0.037676100552082065\n",
      "Average test loss: 0.0024444028130835957\n",
      "Epoch 99/300\n",
      "Average training loss: 0.037833108140362635\n",
      "Average test loss: 0.002412582847600182\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03749044440852271\n",
      "Average test loss: 0.0022722472827881573\n",
      "Epoch 101/300\n",
      "Average training loss: 0.037499308048023115\n",
      "Average test loss: 0.0022707937403271595\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03736592706044515\n",
      "Average test loss: 0.0024511072493882645\n",
      "Epoch 103/300\n",
      "Average training loss: 0.037371509154637654\n",
      "Average test loss: 30.50499732451969\n",
      "Epoch 104/300\n",
      "Average training loss: 0.037164855231841404\n",
      "Average test loss: 0.0022840917091816663\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03730723692476749\n",
      "Average test loss: 0.0023709508505546386\n",
      "Epoch 106/300\n",
      "Average training loss: 0.037875084569056826\n",
      "Average test loss: 0.0022995430786783497\n",
      "Epoch 107/300\n",
      "Average training loss: 0.037028755403227274\n",
      "Average test loss: 0.0022684673892023662\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03702181639273961\n",
      "Average test loss: 0.0023501430502575306\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03715384176704618\n",
      "Average test loss: 0.0022572454690105385\n",
      "Epoch 110/300\n",
      "Average training loss: 0.036971543305450015\n",
      "Average test loss: 0.0022652285233553913\n",
      "Epoch 111/300\n",
      "Average training loss: 0.036918491310543484\n",
      "Average test loss: 0.0022838071864098313\n",
      "Epoch 112/300\n",
      "Average training loss: 0.036813467762536474\n",
      "Average test loss: 0.002296093989784519\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0369589238497946\n",
      "Average test loss: 0.002272512371548348\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03664502332607905\n",
      "Average test loss: 0.0023040977358404135\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03677410339978006\n",
      "Average test loss: 0.0022865022498493393\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0367667676690552\n",
      "Average test loss: 0.002313667017966509\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03642688421739472\n",
      "Average test loss: 0.0024192211888730527\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03667028886410925\n",
      "Average test loss: 0.0023064414080646305\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03645881088078022\n",
      "Average test loss: 0.0023412839215662742\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03641015779972077\n",
      "Average test loss: 0.0023651412179072698\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03628820265663995\n",
      "Average test loss: 0.002319778298959136\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03677086331612534\n",
      "Average test loss: 0.0024015714706232152\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0364223375055525\n",
      "Average test loss: 0.0023371553630050688\n",
      "Epoch 124/300\n",
      "Average training loss: 0.036301584972275626\n",
      "Average test loss: 0.0023867404985552032\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03631523504522112\n",
      "Average test loss: 0.002389120089924998\n",
      "Epoch 126/300\n",
      "Average training loss: 0.036240538169940315\n",
      "Average test loss: 0.002339370322103302\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03598050463199615\n",
      "Average test loss: 0.0023381390779589613\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03602371755739053\n",
      "Average test loss: 0.002481346197012398\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03598624470498827\n",
      "Average test loss: 0.0023282637453327574\n",
      "Epoch 130/300\n",
      "Average training loss: 0.036041519476307765\n",
      "Average test loss: 0.0030904977296789485\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03597998055815697\n",
      "Average test loss: 0.0035811681056188214\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03615171788798438\n",
      "Average test loss: 0.002718640152985851\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03600700669652886\n",
      "Average test loss: 0.0025702004364381234\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03578717055254512\n",
      "Average test loss: 0.002344235518015921\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03581699286897977\n",
      "Average test loss: 0.006864669714536932\n",
      "Epoch 136/300\n",
      "Average training loss: 0.035778066515922545\n",
      "Average test loss: 0.0023271066012481847\n",
      "Epoch 137/300\n",
      "Average training loss: 0.035743388884597355\n",
      "Average test loss: 0.0023374185180291534\n",
      "Epoch 138/300\n",
      "Average training loss: 0.035707600752512614\n",
      "Average test loss: 0.0022881103874080712\n",
      "Epoch 139/300\n",
      "Average training loss: 0.035667040271891486\n",
      "Average test loss: 0.0023113884344283077\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0355155896809366\n",
      "Average test loss: 0.0022857346451944775\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03591113690700796\n",
      "Average test loss: 0.0023302374198618863\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03536754022373093\n",
      "Average test loss: 0.00231421516276896\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03551516258716583\n",
      "Average test loss: 0.021002607739633986\n",
      "Epoch 144/300\n",
      "Average training loss: 0.035418440573745304\n",
      "Average test loss: 0.0023309755165957743\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03529012222091357\n",
      "Average test loss: 0.0025816626877834398\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03525653271211518\n",
      "Average test loss: 0.002324078363676866\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03535583424237039\n",
      "Average test loss: 0.0023411049014992183\n",
      "Epoch 148/300\n",
      "Average training loss: 0.035294309609466126\n",
      "Average test loss: 0.0023066340105401145\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03533659902877278\n",
      "Average test loss: 0.002328270968256725\n",
      "Epoch 150/300\n",
      "Average training loss: 0.035449083013667\n",
      "Average test loss: 0.002333813289180398\n",
      "Epoch 151/300\n",
      "Average training loss: 0.035072673302557734\n",
      "Average test loss: 0.002430892216041684\n",
      "Epoch 152/300\n",
      "Average training loss: 0.035113918360736636\n",
      "Average test loss: 0.0023360551417701773\n",
      "Epoch 153/300\n",
      "Average training loss: 0.035302006403605146\n",
      "Average test loss: 0.005079989477164215\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03503713488578796\n",
      "Average test loss: 0.0023824771802044577\n",
      "Epoch 155/300\n",
      "Average training loss: 0.034989143499069744\n",
      "Average test loss: 0.0023681176128900715\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03508542430069712\n",
      "Average test loss: 0.0024217476632653013\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03508683780166838\n",
      "Average test loss: 0.0024822704245646794\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03500520914958583\n",
      "Average test loss: 0.0023533572782244946\n",
      "Epoch 159/300\n",
      "Average training loss: 0.034830386247899796\n",
      "Average test loss: 0.0027875917942987547\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03491575271553463\n",
      "Average test loss: 0.002331471270364192\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03502821093632115\n",
      "Average test loss: 0.002408807996556991\n",
      "Epoch 162/300\n",
      "Average training loss: 0.034837472442123624\n",
      "Average test loss: 0.0024035897619194453\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03502839132812288\n",
      "Average test loss: 0.0024138272777199745\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03487819930911064\n",
      "Average test loss: 0.002329169426527288\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03470607998636034\n",
      "Average test loss: 0.002961576781753037\n",
      "Epoch 166/300\n",
      "Average training loss: 0.034706734899017544\n",
      "Average test loss: 0.002454084538233777\n",
      "Epoch 167/300\n",
      "Average training loss: 0.034770669475197795\n",
      "Average test loss: 0.002397992234987517\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03453134535087479\n",
      "Average test loss: 0.0023133102539512848\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03468414072526826\n",
      "Average test loss: 0.0023301461082365776\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03465599244667424\n",
      "Average test loss: 0.0038540098360843127\n",
      "Epoch 171/300\n",
      "Average training loss: 0.034698422653807535\n",
      "Average test loss: 0.0026337834230313697\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03468914531005753\n",
      "Average test loss: 0.02470994897517893\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03456382223632601\n",
      "Average test loss: 0.0025630407196780046\n",
      "Epoch 174/300\n",
      "Average training loss: 0.034444304815597\n",
      "Average test loss: 0.0023601743309862085\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0345450348522928\n",
      "Average test loss: 0.002589414175496333\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03441776520013809\n",
      "Average test loss: 0.004190061683042182\n",
      "Epoch 177/300\n",
      "Average training loss: 0.034576334062549806\n",
      "Average test loss: 0.0023578494298789237\n",
      "Epoch 178/300\n",
      "Average training loss: 0.034597445295916664\n",
      "Average test loss: 0.002480579431168735\n",
      "Epoch 179/300\n",
      "Average training loss: 0.034340933624241086\n",
      "Average test loss: 0.002742977939546108\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0342478486597538\n",
      "Average test loss: 0.00849676257620255\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03443381014300717\n",
      "Average test loss: 0.0025204469348407456\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03439082081119219\n",
      "Average test loss: 0.00237202784501844\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03422874162097772\n",
      "Average test loss: 0.002788500183261931\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03419471021824413\n",
      "Average test loss: 0.0023335748863303\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03415491727822357\n",
      "Average test loss: 0.0025215811791519326\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03417771594391929\n",
      "Average test loss: 0.002424692957248125\n",
      "Epoch 187/300\n",
      "Average training loss: 0.034155739749471344\n",
      "Average test loss: 0.002973198463105493\n",
      "Epoch 188/300\n",
      "Average training loss: 0.034182975149816934\n",
      "Average test loss: 0.0025232754697402317\n",
      "Epoch 189/300\n",
      "Average training loss: 0.034296021772755515\n",
      "Average test loss: 0.002376668618991971\n",
      "Epoch 190/300\n",
      "Average training loss: 0.034698747283882565\n",
      "Average test loss: 0.002443205308996969\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03406132952537801\n",
      "Average test loss: 0.002536328267926971\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03394228294822905\n",
      "Average test loss: 0.0024732355481634537\n",
      "Epoch 193/300\n",
      "Average training loss: 0.033946832441621354\n",
      "Average test loss: 0.0024558623453809153\n",
      "Epoch 194/300\n",
      "Average training loss: 0.034098119581739104\n",
      "Average test loss: 0.002436904091387987\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03392132686244117\n",
      "Average test loss: 0.002407938140651418\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03414216878679063\n",
      "Average test loss: 0.002398744414250056\n",
      "Epoch 197/300\n",
      "Average training loss: 0.034097981519169276\n",
      "Average test loss: 0.002345879191946652\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03412490333451165\n",
      "Average test loss: 0.0024460544127763973\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03388254299759865\n",
      "Average test loss: 0.0025641238808424937\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03379538778132862\n",
      "Average test loss: 0.002595295462343428\n",
      "Epoch 201/300\n",
      "Average training loss: 0.033967159067591034\n",
      "Average test loss: 0.0024276151043466396\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03410931631922722\n",
      "Average test loss: 0.0024522659056302573\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03372651290231281\n",
      "Average test loss: 0.0031612247027870684\n",
      "Epoch 204/300\n",
      "Average training loss: 0.033731151753001745\n",
      "Average test loss: 0.0027613215628597473\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03392164303527938\n",
      "Average test loss: 0.0024387357187353902\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03363770725991991\n",
      "Average test loss: 0.0027530029076668953\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03376339727640152\n",
      "Average test loss: 0.00258906139081551\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03379817189441787\n",
      "Average test loss: 0.0034845993208388487\n",
      "Epoch 209/300\n",
      "Average training loss: 0.033799091794424585\n",
      "Average test loss: 0.0023895380304505428\n",
      "Epoch 210/300\n",
      "Average training loss: 0.033636398262447784\n",
      "Average test loss: 0.002536724336652292\n",
      "Epoch 211/300\n",
      "Average training loss: 0.033780827891495495\n",
      "Average test loss: 0.00246782090411418\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03351652813454469\n",
      "Average test loss: 0.005364596755554279\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03373161692420642\n",
      "Average test loss: 0.0025069943196657633\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03359997577468554\n",
      "Average test loss: 0.002410460582623879\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0335310560464859\n",
      "Average test loss: 0.004912832249783807\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03368538727031814\n",
      "Average test loss: 0.0023820738906247746\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03345197583569421\n",
      "Average test loss: 0.002355047734454274\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03357536465095149\n",
      "Average test loss: 0.0031540835984051226\n",
      "Epoch 219/300\n",
      "Average training loss: 0.033591629106137486\n",
      "Average test loss: 0.0024995854794979096\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03349235544105371\n",
      "Average test loss: 0.0026368996494760114\n",
      "Epoch 221/300\n",
      "Average training loss: 0.033754866364929415\n",
      "Average test loss: 0.002617367587983608\n",
      "Epoch 222/300\n",
      "Average training loss: 0.033769503803716766\n",
      "Average test loss: 0.002372438676551812\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03357811705271403\n",
      "Average test loss: 0.002515644816060861\n",
      "Epoch 224/300\n",
      "Average training loss: 0.033302733573648664\n",
      "Average test loss: 0.0024806183657298487\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03338748613993327\n",
      "Average test loss: 0.002687690380650262\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03335831240978506\n",
      "Average test loss: 0.0027193638533353805\n",
      "Epoch 227/300\n",
      "Average training loss: 0.033308613103297025\n",
      "Average test loss: 0.002444477634918359\n",
      "Epoch 228/300\n",
      "Average training loss: 0.033378668546676635\n",
      "Average test loss: 0.0024356280450398725\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03348244149817361\n",
      "Average test loss: 0.002446034338946144\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03318052595853806\n",
      "Average test loss: 0.0026384483396800027\n",
      "Epoch 231/300\n",
      "Average training loss: 0.033231196575694615\n",
      "Average test loss: 0.0025402637442780867\n",
      "Epoch 232/300\n",
      "Average training loss: 0.033343500410517056\n",
      "Average test loss: 0.0024221433594615923\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03320851120352745\n",
      "Average test loss: 0.002715739856370621\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03327928727699651\n",
      "Average test loss: 0.0024340808254977066\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03356939517127143\n",
      "Average test loss: 0.0024483842826965784\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03343771773080031\n",
      "Average test loss: 0.002549750092956755\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03309493331114451\n",
      "Average test loss: 0.002442857726632307\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03319702877600988\n",
      "Average test loss: 0.0025286013975532517\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03317373288671176\n",
      "Average test loss: 0.0025026217287199364\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03318931480248769\n",
      "Average test loss: 0.002503731260024425\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03311317108405961\n",
      "Average test loss: 0.002442382537966801\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03328961029317644\n",
      "Average test loss: 0.0024271091806391874\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0330126603477531\n",
      "Average test loss: 0.0024084551547550494\n",
      "Epoch 244/300\n",
      "Average training loss: 0.033138500357667605\n",
      "Average test loss: 0.002575617199763656\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03312342045373387\n",
      "Average test loss: 0.0023751348739282953\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03299282262391514\n",
      "Average test loss: 0.002951397971560558\n",
      "Epoch 247/300\n",
      "Average training loss: 0.033164180192682476\n",
      "Average test loss: 0.0024968157093971967\n",
      "Epoch 248/300\n",
      "Average training loss: 0.032908056169748304\n",
      "Average test loss: 0.002427397256406645\n",
      "Epoch 249/300\n",
      "Average training loss: 0.033070890525976816\n",
      "Average test loss: 0.002413832797565394\n",
      "Epoch 250/300\n",
      "Average training loss: 0.033150030235449476\n",
      "Average test loss: 0.0024772396634022394\n",
      "Epoch 251/300\n",
      "Average training loss: 0.032992252147859995\n",
      "Average test loss: 0.01826076740399003\n",
      "Epoch 252/300\n",
      "Average training loss: 0.032802677634689546\n",
      "Average test loss: 0.004645062008251747\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03295722056428591\n",
      "Average test loss: 0.002428069439302716\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03294200926025709\n",
      "Average test loss: 0.01011320583191183\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03321781456470489\n",
      "Average test loss: 0.00882356602864133\n",
      "Epoch 256/300\n",
      "Average training loss: 0.032901604917314316\n",
      "Average test loss: 0.002563444312661886\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03285526482760906\n",
      "Average test loss: 0.002467570782535606\n",
      "Epoch 258/300\n",
      "Average training loss: 0.032912406073676216\n",
      "Average test loss: 0.003109377440272106\n",
      "Epoch 259/300\n",
      "Average training loss: 0.033066869818502\n",
      "Average test loss: 0.003951006262252728\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03269039717482196\n",
      "Average test loss: 0.0025137706141298015\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03287437108490202\n",
      "Average test loss: 0.002471749567737182\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03273220097687509\n",
      "Average test loss: 0.002433667798423105\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03290285890466637\n",
      "Average test loss: 0.004279180523215069\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03294919891489877\n",
      "Average test loss: 0.0024346567616901466\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03304485430320104\n",
      "Average test loss: 0.004510071905743745\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03318213944964939\n",
      "Average test loss: 0.002384439940150413\n",
      "Epoch 267/300\n",
      "Average training loss: 0.032672803552614316\n",
      "Average test loss: 0.0024850143443586096\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0326450971364975\n",
      "Average test loss: 0.002366153451717562\n",
      "Epoch 269/300\n",
      "Average training loss: 0.032631505691342884\n",
      "Average test loss: 0.002543682025124629\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0327954417069753\n",
      "Average test loss: 0.0026271567669593626\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03274451583789455\n",
      "Average test loss: 0.002549818434442083\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03255989716284805\n",
      "Average test loss: 0.0025339602711093095\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03266266004410055\n",
      "Average test loss: 0.002974662382983499\n",
      "Epoch 274/300\n",
      "Average training loss: 0.032691046259469454\n",
      "Average test loss: 0.0024453254849132566\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03264036091168721\n",
      "Average test loss: 0.00247514689527452\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03260247781872749\n",
      "Average test loss: 0.00315550256261809\n",
      "Epoch 277/300\n",
      "Average training loss: 0.032561337358421746\n",
      "Average test loss: 0.0024499920405861406\n",
      "Epoch 278/300\n",
      "Average training loss: 0.032920663156443174\n",
      "Average test loss: 0.022957736485534244\n",
      "Epoch 279/300\n",
      "Average training loss: 0.032754685451587044\n",
      "Average test loss: 0.0024475080888304445\n",
      "Epoch 280/300\n",
      "Average training loss: 0.032478516118394\n",
      "Average test loss: 0.0029787344622115293\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03247755069865121\n",
      "Average test loss: 0.002449196842395597\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03258726265364223\n",
      "Average test loss: 0.002443300204144584\n",
      "Epoch 283/300\n",
      "Average training loss: 0.032597650519675676\n",
      "Average test loss: 0.002908290039334032\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03274119712246789\n",
      "Average test loss: 0.0025020075221028593\n",
      "Epoch 285/300\n",
      "Average training loss: 0.032472165554761886\n",
      "Average test loss: 0.0024679812784824105\n",
      "Epoch 286/300\n",
      "Average training loss: 0.032502477142545914\n",
      "Average test loss: 0.0028665196011877724\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03249657447470559\n",
      "Average test loss: 0.002447533978149295\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03245837849213017\n",
      "Average test loss: 0.003509620145584146\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03246598616904683\n",
      "Average test loss: 0.0025884057904283207\n",
      "Epoch 290/300\n",
      "Average training loss: 0.032366334213150875\n",
      "Average test loss: 0.002505004632493688\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03242506719960107\n",
      "Average test loss: 0.01172599477155341\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03256037691235542\n",
      "Average test loss: 0.0025226407336692014\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03241969458593263\n",
      "Average test loss: 0.0025201531959076723\n",
      "Epoch 294/300\n",
      "Average training loss: 0.032322723044289486\n",
      "Average test loss: 0.012520987709363302\n",
      "Epoch 295/300\n",
      "Average training loss: 0.032378153880437216\n",
      "Average test loss: 0.002461302128310005\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03241110259956784\n",
      "Average test loss: 0.0024884513816278843\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03351839694711897\n",
      "Average test loss: 0.0028716206589920654\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03261438526875443\n",
      "Average test loss: 0.0024892568757964506\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03224359725912412\n",
      "Average test loss: 0.0025211625192314386\n",
      "Epoch 300/300\n",
      "Average training loss: 0.032181223647461996\n",
      "Average test loss: 0.0025324992692718905\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4397177419927385\n",
      "Average test loss: 0.006344504151493311\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16000069206290776\n",
      "Average test loss: 0.004620233250574933\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11254338667790095\n",
      "Average test loss: 0.004008635671188434\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09486852150824335\n",
      "Average test loss: 0.004135542193634643\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08501438592539894\n",
      "Average test loss: 0.003801611832446522\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07837757034434213\n",
      "Average test loss: 0.0035071299597620965\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07333338797754711\n",
      "Average test loss: 0.003477940678803457\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06965281447437074\n",
      "Average test loss: 0.0032476114645186398\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0660463421344757\n",
      "Average test loss: 0.003669600885775354\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06309764486551285\n",
      "Average test loss: 0.00373062031591932\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05987608320845498\n",
      "Average test loss: 0.002913898698157734\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05776855023039712\n",
      "Average test loss: 0.002668806498663293\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05493097782466147\n",
      "Average test loss: 0.0029139742623600696\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05244992003838221\n",
      "Average test loss: 0.0027417915891855956\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05029473200440407\n",
      "Average test loss: 0.002387175433234208\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04808056675394376\n",
      "Average test loss: 0.002439614501574801\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04643622890445921\n",
      "Average test loss: 0.0023079843811897767\n",
      "Epoch 18/300\n",
      "Average training loss: 0.044953432665930854\n",
      "Average test loss: 0.00254904362724887\n",
      "Epoch 19/300\n",
      "Average training loss: 0.043360345400042\n",
      "Average test loss: 0.0020892233069365225\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04226599443289969\n",
      "Average test loss: 0.001985802848202487\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04125004039870368\n",
      "Average test loss: 0.001985363512299955\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0402391090757317\n",
      "Average test loss: 0.0019227808219277197\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03947848561406136\n",
      "Average test loss: 0.002043447496990363\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0385920270813836\n",
      "Average test loss: 0.0018597693669920167\n",
      "Epoch 25/300\n",
      "Average training loss: 0.038100830202301345\n",
      "Average test loss: 0.0019066298570897843\n",
      "Epoch 26/300\n",
      "Average training loss: 0.037485748327440685\n",
      "Average test loss: 0.0018606438284946812\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03705326450036632\n",
      "Average test loss: 0.0026849049068987368\n",
      "Epoch 28/300\n",
      "Average training loss: 0.036483804937866\n",
      "Average test loss: 0.001842703965285586\n",
      "Epoch 29/300\n",
      "Average training loss: 0.036036816732751\n",
      "Average test loss: 0.001762584034094794\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03562935866912206\n",
      "Average test loss: 0.0018581117532836895\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03536243941220972\n",
      "Average test loss: 0.001738838480371568\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0348813157296843\n",
      "Average test loss: 0.001717291189254158\n",
      "Epoch 33/300\n",
      "Average training loss: 0.034689729962084026\n",
      "Average test loss: 0.001683861612652739\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0342501617587275\n",
      "Average test loss: 0.0016890480904839932\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03392853431238069\n",
      "Average test loss: 0.0016851528622210026\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03382444291644626\n",
      "Average test loss: 0.00198050885161178\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033663473417361575\n",
      "Average test loss: 0.0016254693081395493\n",
      "Epoch 38/300\n",
      "Average training loss: 0.034636222735047344\n",
      "Average test loss: 0.001625757366211878\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03327757602764501\n",
      "Average test loss: 0.0016509119388129975\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03310206994579898\n",
      "Average test loss: 0.001731565995245344\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03288549588455094\n",
      "Average test loss: 0.0016233220527259013\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03264505438175466\n",
      "Average test loss: 0.0017258305325586762\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03250178881320689\n",
      "Average test loss: 0.0015873565882340902\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03235694551467896\n",
      "Average test loss: 0.0016509278487000202\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03234329222639402\n",
      "Average test loss: 0.0015867592598208123\n",
      "Epoch 46/300\n",
      "Average training loss: 0.032032474663522506\n",
      "Average test loss: 0.0016451927347936564\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03200605661173662\n",
      "Average test loss: 0.0017739033399977618\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03247623966965411\n",
      "Average test loss: 0.0019322180319577456\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03192118349671364\n",
      "Average test loss: 0.0016254407842126158\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03161016432609823\n",
      "Average test loss: 0.0015943390654606952\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0314921655356884\n",
      "Average test loss: 0.0015933419240431654\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03128202827109231\n",
      "Average test loss: 0.002776495997276571\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03141360212531355\n",
      "Average test loss: 0.001642645574795703\n",
      "Epoch 54/300\n",
      "Average training loss: 0.031096139333314367\n",
      "Average test loss: 0.0015984132536169555\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03105937955611282\n",
      "Average test loss: 0.0016266010072496203\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03083889164361689\n",
      "Average test loss: 0.001570193398433427\n",
      "Epoch 57/300\n",
      "Average training loss: 0.031040777610407935\n",
      "Average test loss: 0.0015612658956605527\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030691146937509378\n",
      "Average test loss: 0.0015456352543292773\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030571423257390658\n",
      "Average test loss: 0.0015700372131541372\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030485827811890177\n",
      "Average test loss: 0.0016088952593919304\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030908229379190338\n",
      "Average test loss: 0.0021521513213713962\n",
      "Epoch 62/300\n",
      "Average training loss: 0.030227777888377508\n",
      "Average test loss: 0.0016296705889205139\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03028465917209784\n",
      "Average test loss: 0.0015354265356436372\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03036969075269169\n",
      "Average test loss: 0.0015329050257181127\n",
      "Epoch 65/300\n",
      "Average training loss: 0.029962830731438267\n",
      "Average test loss: 0.0015698263842819466\n",
      "Epoch 66/300\n",
      "Average training loss: 0.029925779362519583\n",
      "Average test loss: 0.0016451326074699562\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03062446784145302\n",
      "Average test loss: 0.0015320754173315234\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02970253325667646\n",
      "Average test loss: 0.0015398296722107464\n",
      "Epoch 69/300\n",
      "Average training loss: 0.029663505408498977\n",
      "Average test loss: 0.0015606356518757013\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0296443118436469\n",
      "Average test loss: 0.001552980858936078\n",
      "Epoch 71/300\n",
      "Average training loss: 0.030020306419995094\n",
      "Average test loss: 0.0017214963524084953\n",
      "Epoch 72/300\n",
      "Average training loss: 0.029767950202027958\n",
      "Average test loss: 0.0015327080235713057\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02967704214817948\n",
      "Average test loss: 0.0015231329017422266\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02942720803949568\n",
      "Average test loss: 0.0017512660765399536\n",
      "Epoch 75/300\n",
      "Average training loss: 0.029359645888209343\n",
      "Average test loss: 0.001557596354538368\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02945397844248348\n",
      "Average test loss: 0.0015886638760566711\n",
      "Epoch 77/300\n",
      "Average training loss: 0.029332413292593427\n",
      "Average test loss: 0.0015427016182285216\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029134549483656885\n",
      "Average test loss: 0.001628015373316076\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0290556740678019\n",
      "Average test loss: 0.0015940478514466022\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02904312127497461\n",
      "Average test loss: 0.001546441942349904\n",
      "Epoch 81/300\n",
      "Average training loss: 0.028886754843923782\n",
      "Average test loss: 0.002053249653739234\n",
      "Epoch 82/300\n",
      "Average training loss: 0.029022488254639836\n",
      "Average test loss: 0.0015988870546635653\n",
      "Epoch 83/300\n",
      "Average training loss: 0.029010320271054903\n",
      "Average test loss: 0.0018176087385250463\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028905328611532846\n",
      "Average test loss: 0.0018687214679602119\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02882418851223257\n",
      "Average test loss: 0.002126071117611395\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028625412626398936\n",
      "Average test loss: 0.001769595755264163\n",
      "Epoch 87/300\n",
      "Average training loss: 0.028509817373421457\n",
      "Average test loss: 0.0015561018104975422\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028686486419704227\n",
      "Average test loss: 0.0015372789421429236\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02885909660988384\n",
      "Average test loss: 0.0020281239253365333\n",
      "Epoch 90/300\n",
      "Average training loss: 0.028543414296375382\n",
      "Average test loss: 0.0015094416290521622\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02847179431716601\n",
      "Average test loss: 0.0017870988322214948\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0283548933416605\n",
      "Average test loss: 0.0015697436694366237\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0282568382455243\n",
      "Average test loss: 0.00153146173297945\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02836860004895263\n",
      "Average test loss: 0.001532941615002023\n",
      "Epoch 95/300\n",
      "Average training loss: 0.028284594976239735\n",
      "Average test loss: 0.0016602308853632874\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02824609236088064\n",
      "Average test loss: 0.001571889606821868\n",
      "Epoch 97/300\n",
      "Average training loss: 0.028245481164919005\n",
      "Average test loss: 0.0016049243804688255\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0279887572824955\n",
      "Average test loss: 0.001528893443238404\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028104969925350613\n",
      "Average test loss: 0.0107009693764978\n",
      "Epoch 100/300\n",
      "Average training loss: 0.028354345488879416\n",
      "Average test loss: 0.0015735825391279326\n",
      "Epoch 101/300\n",
      "Average training loss: 0.027843803721997473\n",
      "Average test loss: 0.0015510142930886812\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02787535270717409\n",
      "Average test loss: 0.0015714380205091502\n",
      "Epoch 103/300\n",
      "Average training loss: 0.027817753516965443\n",
      "Average test loss: 0.0015960626078562604\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0279391110042731\n",
      "Average test loss: 0.0015410212581563327\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02797553293241395\n",
      "Average test loss: 0.0018745446668730842\n",
      "Epoch 106/300\n",
      "Average training loss: 0.027879101710187065\n",
      "Average test loss: 0.0015463879670326909\n",
      "Epoch 107/300\n",
      "Average training loss: 0.027610565405752924\n",
      "Average test loss: 0.001674537170678377\n",
      "Epoch 108/300\n",
      "Average training loss: 0.027648879408836365\n",
      "Average test loss: 0.0018047438221466211\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02762923190991084\n",
      "Average test loss: 0.0016152386656031012\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027502079379227426\n",
      "Average test loss: 0.0015630339377870161\n",
      "Epoch 111/300\n",
      "Average training loss: 0.027721697396702237\n",
      "Average test loss: 0.0015835813968959782\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02747872901459535\n",
      "Average test loss: 0.0024440678160430654\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027502550029092365\n",
      "Average test loss: 0.0015402199464539686\n",
      "Epoch 114/300\n",
      "Average training loss: 0.027650058567523957\n",
      "Average test loss: 0.001600250665201909\n",
      "Epoch 115/300\n",
      "Average training loss: 0.027314477951990233\n",
      "Average test loss: 0.0015724655757140783\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02744962867597739\n",
      "Average test loss: 0.0016261740065076284\n",
      "Epoch 117/300\n",
      "Average training loss: 0.027445659526520304\n",
      "Average test loss: 0.0019453336296396124\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02735612191590998\n",
      "Average test loss: 0.001638898768876162\n",
      "Epoch 119/300\n",
      "Average training loss: 0.027229583208759626\n",
      "Average test loss: 0.001938738603869246\n",
      "Epoch 120/300\n",
      "Average training loss: 0.027189077269699838\n",
      "Average test loss: 0.001585283794440329\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027159663687149685\n",
      "Average test loss: 0.0017121081435018115\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027379935764604145\n",
      "Average test loss: 0.001554021732053823\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027077135271496244\n",
      "Average test loss: 0.0015685169235285785\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027036220755841996\n",
      "Average test loss: 0.001564548997208476\n",
      "Epoch 125/300\n",
      "Average training loss: 0.027037239140934415\n",
      "Average test loss: 0.0015695770349767474\n",
      "Epoch 126/300\n",
      "Average training loss: 0.026965079706576134\n",
      "Average test loss: 0.0015437919176701043\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027151411493619285\n",
      "Average test loss: 0.0015997978760343458\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027059275643693075\n",
      "Average test loss: 0.0015964223937027983\n",
      "Epoch 129/300\n",
      "Average training loss: 0.026897574527396096\n",
      "Average test loss: 0.001547986538356377\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02678949303097195\n",
      "Average test loss: 0.0016196906603872776\n",
      "Epoch 131/300\n",
      "Average training loss: 0.027024030316207145\n",
      "Average test loss: 0.0015705315148354403\n",
      "Epoch 132/300\n",
      "Average training loss: 0.026833064696855015\n",
      "Average test loss: 0.0015541104163146681\n",
      "Epoch 133/300\n",
      "Average training loss: 0.026897493238250414\n",
      "Average test loss: 0.0016768427465317978\n",
      "Epoch 134/300\n",
      "Average training loss: 0.026716067598925697\n",
      "Average test loss: 0.001593121578089065\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02661476993560791\n",
      "Average test loss: 0.001608403750591808\n",
      "Epoch 136/300\n",
      "Average training loss: 0.026938882034685876\n",
      "Average test loss: 0.0016015723896109395\n",
      "Epoch 137/300\n",
      "Average training loss: 0.026782649242215687\n",
      "Average test loss: 0.0015857120509155922\n",
      "Epoch 138/300\n",
      "Average training loss: 0.026649889018800523\n",
      "Average test loss: 0.0023163323698358403\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02659297246237596\n",
      "Average test loss: 0.0015550988756327165\n",
      "Epoch 140/300\n",
      "Average training loss: 0.026584015612800917\n",
      "Average test loss: 0.0016211632194204463\n",
      "Epoch 141/300\n",
      "Average training loss: 0.026628762114379142\n",
      "Average test loss: 0.0015965788097431263\n",
      "Epoch 142/300\n",
      "Average training loss: 0.026621223409970602\n",
      "Average test loss: 0.001607258779824608\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02675476852721638\n",
      "Average test loss: 0.0022954851920819943\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02666709335313903\n",
      "Average test loss: 0.0016048275442380043\n",
      "Epoch 145/300\n",
      "Average training loss: 0.026481772937708432\n",
      "Average test loss: 0.001567954981389145\n",
      "Epoch 146/300\n",
      "Average training loss: 0.026396488782432344\n",
      "Average test loss: 0.0015809473753389384\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02672033385766877\n",
      "Average test loss: 0.0016037150393757556\n",
      "Epoch 148/300\n",
      "Average training loss: 0.026386232847968736\n",
      "Average test loss: 0.0016098641918765174\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026341195336646504\n",
      "Average test loss: 0.0020644733947184353\n",
      "Epoch 150/300\n",
      "Average training loss: 0.026349294622739158\n",
      "Average test loss: 0.0017673522915898098\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02646208708981673\n",
      "Average test loss: 0.001832676347810775\n",
      "Epoch 152/300\n",
      "Average training loss: 0.026472867122954793\n",
      "Average test loss: 0.0015788918945213986\n",
      "Epoch 153/300\n",
      "Average training loss: 0.026358327756325402\n",
      "Average test loss: 0.0016640385564209686\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02643238457871808\n",
      "Average test loss: 0.001633312243864768\n",
      "Epoch 155/300\n",
      "Average training loss: 0.026374973686205016\n",
      "Average test loss: 0.0016191034686441222\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0260571974615256\n",
      "Average test loss: 0.0015984047981393006\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02620657688544856\n",
      "Average test loss: 0.0015880534710983436\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02617146283470922\n",
      "Average test loss: 0.0018328940315792959\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02610543416440487\n",
      "Average test loss: 0.002460010777744982\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02632871082259549\n",
      "Average test loss: 0.0015761639408560263\n",
      "Epoch 161/300\n",
      "Average training loss: 0.026103403344750406\n",
      "Average test loss: 0.0016807305151596665\n",
      "Epoch 162/300\n",
      "Average training loss: 0.026090161005655924\n",
      "Average test loss: 0.001582414622935984\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026047610996498\n",
      "Average test loss: 0.002306355156832271\n",
      "Epoch 164/300\n",
      "Average training loss: 0.025931542404823833\n",
      "Average test loss: 0.0015969716482278373\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02605899765756395\n",
      "Average test loss: 0.0016202962037382854\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02607220372060935\n",
      "Average test loss: 0.0015917674383769433\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026308516672915883\n",
      "Average test loss: 0.0016220645738972558\n",
      "Epoch 168/300\n",
      "Average training loss: 0.025950083762407303\n",
      "Average test loss: 0.0016046897416882632\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02582877396709389\n",
      "Average test loss: 0.0016998277209285234\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02605120576752557\n",
      "Average test loss: 0.0016939245731466346\n",
      "Epoch 171/300\n",
      "Average training loss: 0.025915993988513946\n",
      "Average test loss: 0.0015890069702226256\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02594629397491614\n",
      "Average test loss: 0.0016459579219938153\n",
      "Epoch 173/300\n",
      "Average training loss: 0.025790190479821628\n",
      "Average test loss: 0.0018818520080919067\n",
      "Epoch 174/300\n",
      "Average training loss: 0.025839933107296625\n",
      "Average test loss: 0.0015989400457797778\n",
      "Epoch 175/300\n",
      "Average training loss: 0.025974183801147674\n",
      "Average test loss: 0.0016068084821001523\n",
      "Epoch 176/300\n",
      "Average training loss: 0.025769136771559717\n",
      "Average test loss: 0.0016301449741133384\n",
      "Epoch 177/300\n",
      "Average training loss: 0.025808827547563445\n",
      "Average test loss: 0.0016351078901853827\n",
      "Epoch 178/300\n",
      "Average training loss: 0.025971523576312595\n",
      "Average test loss: 0.0017900275428675942\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02571454162067837\n",
      "Average test loss: 0.0016077858624565932\n",
      "Epoch 180/300\n",
      "Average training loss: 0.025815628762046496\n",
      "Average test loss: 0.00858391200978723\n",
      "Epoch 181/300\n",
      "Average training loss: 0.025662237414055402\n",
      "Average test loss: 0.0017354103582393793\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026783792805340555\n",
      "Average test loss: 0.0021112318774685265\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02551713842484686\n",
      "Average test loss: 0.0016325132840623458\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0255655009696881\n",
      "Average test loss: 0.0016891935755395228\n",
      "Epoch 185/300\n",
      "Average training loss: 0.025549143397973643\n",
      "Average test loss: 0.0017027796517229742\n",
      "Epoch 186/300\n",
      "Average training loss: 0.025757273813088736\n",
      "Average test loss: 0.0016949787384106054\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02564161476492882\n",
      "Average test loss: 0.0016280890757011043\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0255800315555599\n",
      "Average test loss: 0.010823793688582049\n",
      "Epoch 189/300\n",
      "Average training loss: 0.025554669689801003\n",
      "Average test loss: 0.0016147758785842194\n",
      "Epoch 190/300\n",
      "Average training loss: 0.025606822962562243\n",
      "Average test loss: 0.001758409978925354\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02551319986416234\n",
      "Average test loss: 0.0016782332215872077\n",
      "Epoch 192/300\n",
      "Average training loss: 0.025500517341825697\n",
      "Average test loss: 0.0016633001815320716\n",
      "Epoch 193/300\n",
      "Average training loss: 0.025647113157643214\n",
      "Average test loss: 0.0018127079717814923\n",
      "Epoch 194/300\n",
      "Average training loss: 0.025600372537970543\n",
      "Average test loss: 0.0016410812548258238\n",
      "Epoch 195/300\n",
      "Average training loss: 0.025470154134763612\n",
      "Average test loss: 0.0022256555794220832\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02553256491654449\n",
      "Average test loss: 0.0016532342523957293\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026147997248503897\n",
      "Average test loss: 0.0016025989860710172\n",
      "Epoch 198/300\n",
      "Average training loss: 0.025237203339735666\n",
      "Average test loss: 0.0016477344050589535\n",
      "Epoch 199/300\n",
      "Average training loss: 0.025393330421712663\n",
      "Average test loss: 0.0017066160364904337\n",
      "Epoch 200/300\n",
      "Average training loss: 0.025290664189391666\n",
      "Average test loss: 0.0016344537748437789\n",
      "Epoch 201/300\n",
      "Average training loss: 0.025318755100170773\n",
      "Average test loss: 0.0017341732229623529\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02545137068629265\n",
      "Average test loss: 0.0017081679836329488\n",
      "Epoch 203/300\n",
      "Average training loss: 0.025348907970719866\n",
      "Average test loss: 0.0016472764130578273\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02531912594371372\n",
      "Average test loss: 0.0017011036227146785\n",
      "Epoch 205/300\n",
      "Average training loss: 0.025494569294982488\n",
      "Average test loss: 0.0016964368313137027\n",
      "Epoch 206/300\n",
      "Average training loss: 0.025320341365204918\n",
      "Average test loss: 0.0017617615467558304\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02557593911058373\n",
      "Average test loss: 0.0016339632603857251\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02519630370868577\n",
      "Average test loss: 0.0016745885194589694\n",
      "Epoch 209/300\n",
      "Average training loss: 0.025210704596506223\n",
      "Average test loss: 0.0017997176715483269\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025165379390120507\n",
      "Average test loss: 0.0016408900446775888\n",
      "Epoch 211/300\n",
      "Average training loss: 0.025173097441593805\n",
      "Average test loss: 0.0016870114263147116\n",
      "Epoch 212/300\n",
      "Average training loss: 0.025336413241095014\n",
      "Average test loss: 0.2559758173359765\n",
      "Epoch 213/300\n",
      "Average training loss: 0.025165241183506118\n",
      "Average test loss: 0.0017724995678290725\n",
      "Epoch 214/300\n",
      "Average training loss: 0.025315565251641803\n",
      "Average test loss: 0.0018496707420175274\n",
      "Epoch 215/300\n",
      "Average training loss: 0.025177992618746228\n",
      "Average test loss: 0.0016615485122634306\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02517235814779997\n",
      "Average test loss: 0.0022663459055539633\n",
      "Epoch 217/300\n",
      "Average training loss: 0.025148104867173567\n",
      "Average test loss: 0.0016867713097275959\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02508731887075636\n",
      "Average test loss: 0.0016573385060247448\n",
      "Epoch 219/300\n",
      "Average training loss: 0.025128545315729247\n",
      "Average test loss: 0.001689796123239729\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02511984473466873\n",
      "Average test loss: 0.001632751563977864\n",
      "Epoch 221/300\n",
      "Average training loss: 0.025000876582331126\n",
      "Average test loss: 0.0016591292416366437\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02507491436600685\n",
      "Average test loss: 0.001689539301623073\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0250286272863547\n",
      "Average test loss: 0.001726497452809579\n",
      "Epoch 224/300\n",
      "Average training loss: 0.025117821726534102\n",
      "Average test loss: 0.0016725979378033017\n",
      "Epoch 225/300\n",
      "Average training loss: 0.025042053590218227\n",
      "Average test loss: 0.0016580005415404836\n",
      "Epoch 226/300\n",
      "Average training loss: 0.025058010157611634\n",
      "Average test loss: 0.0016964143896475434\n",
      "Epoch 227/300\n",
      "Average training loss: 0.024951262157824304\n",
      "Average test loss: 0.0016903970112196274\n",
      "Epoch 228/300\n",
      "Average training loss: 0.024975044003791278\n",
      "Average test loss: 0.0016804389118527372\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0249622220777803\n",
      "Average test loss: 0.0016274863419433435\n",
      "Epoch 230/300\n",
      "Average training loss: 0.024927901042832268\n",
      "Average test loss: 0.0016509634083033437\n",
      "Epoch 231/300\n",
      "Average training loss: 0.025108480153812302\n",
      "Average test loss: 0.0016574131823662254\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024910641321705448\n",
      "Average test loss: 0.001759934702163769\n",
      "Epoch 233/300\n",
      "Average training loss: 0.024865218290024332\n",
      "Average test loss: 0.0016999767180532217\n",
      "Epoch 234/300\n",
      "Average training loss: 0.024966185786657862\n",
      "Average test loss: 0.001730492673193415\n",
      "Epoch 235/300\n",
      "Average training loss: 0.024892385724518035\n",
      "Average test loss: 0.0017346754945918089\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02484776185784075\n",
      "Average test loss: 0.0018312365006034573\n",
      "Epoch 237/300\n",
      "Average training loss: 0.024852507531642913\n",
      "Average test loss: 0.001698179357788629\n",
      "Epoch 238/300\n",
      "Average training loss: 0.024856774705151716\n",
      "Average test loss: 0.0017356144424734844\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025305368304252625\n",
      "Average test loss: 0.007143163036968973\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02480637654827701\n",
      "Average test loss: 0.0016610842547896834\n",
      "Epoch 241/300\n",
      "Average training loss: 0.024849209303657215\n",
      "Average test loss: 0.001681513671659761\n",
      "Epoch 242/300\n",
      "Average training loss: 0.024774558418326906\n",
      "Average test loss: 0.0017156522083613607\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02488971446371741\n",
      "Average test loss: 0.0016749140392574998\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02484813146955437\n",
      "Average test loss: 0.0017168299439880583\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02476825081308683\n",
      "Average test loss: 0.0017959571377270751\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02478275770611233\n",
      "Average test loss: 0.001665477989655402\n",
      "Epoch 247/300\n",
      "Average training loss: 0.024741709333327082\n",
      "Average test loss: 0.0017114583804375596\n",
      "Epoch 248/300\n",
      "Average training loss: 0.024929686400625442\n",
      "Average test loss: 0.0016826926723846958\n",
      "Epoch 249/300\n",
      "Average training loss: 0.024696015866266356\n",
      "Average test loss: 0.0017337087533540196\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02472039156489902\n",
      "Average test loss: 0.0017358686638892524\n",
      "Epoch 251/300\n",
      "Average training loss: 0.024737618327140807\n",
      "Average test loss: 0.0019851420132650267\n",
      "Epoch 252/300\n",
      "Average training loss: 0.024839629727933143\n",
      "Average test loss: 0.0017093097348180083\n",
      "Epoch 253/300\n",
      "Average training loss: 0.024685152024858527\n",
      "Average test loss: 0.0017843266698635287\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02482367190718651\n",
      "Average test loss: 0.0017524401395995583\n",
      "Epoch 255/300\n",
      "Average training loss: 0.024727149287859598\n",
      "Average test loss: 0.001852745843016439\n",
      "Epoch 256/300\n",
      "Average training loss: 0.024680696479148334\n",
      "Average test loss: 0.0016373455295753147\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02476730893717872\n",
      "Average test loss: 0.0016853892522760564\n",
      "Epoch 258/300\n",
      "Average training loss: 0.024589687022897933\n",
      "Average test loss: 0.001638077503277196\n",
      "Epoch 259/300\n",
      "Average training loss: 0.024758396968245507\n",
      "Average test loss: 0.0019006573474034667\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02471694345606698\n",
      "Average test loss: 0.001704924117670291\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02468281554679076\n",
      "Average test loss: 0.0016545524057000876\n",
      "Epoch 262/300\n",
      "Average training loss: 0.024561487868428232\n",
      "Average test loss: 0.0017089701847483715\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02453324812153975\n",
      "Average test loss: 0.0017373408517903752\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02459059982498487\n",
      "Average test loss: 0.0016638443068497711\n",
      "Epoch 265/300\n",
      "Average training loss: 0.024652482446697023\n",
      "Average test loss: 0.0017412702303586734\n",
      "Epoch 266/300\n",
      "Average training loss: 0.024584128338429663\n",
      "Average test loss: 0.0017057416842112111\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02464938339094321\n",
      "Average test loss: 0.0017049563226497007\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02447076342172093\n",
      "Average test loss: 0.0020377044121010434\n",
      "Epoch 269/300\n",
      "Average training loss: 0.024605739677945774\n",
      "Average test loss: 0.004969125671519173\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02447495743466748\n",
      "Average test loss: 0.0016835199244734315\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02450363301071856\n",
      "Average test loss: 0.0017960690795961354\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02478498286008835\n",
      "Average test loss: 0.0017742357895088692\n",
      "Epoch 273/300\n",
      "Average training loss: 0.024470930531620978\n",
      "Average test loss: 0.0018459131933955682\n",
      "Epoch 274/300\n",
      "Average training loss: 0.024458223771717812\n",
      "Average test loss: 0.0016783446609559987\n",
      "Epoch 275/300\n",
      "Average training loss: 0.024456375963158077\n",
      "Average test loss: 0.0016838170507301887\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02448588942322466\n",
      "Average test loss: 0.0017845882714932991\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02449272131588724\n",
      "Average test loss: 0.00178355579264462\n",
      "Epoch 278/300\n",
      "Average training loss: 0.024438961929745144\n",
      "Average test loss: 0.0016705572590645817\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02447273880077733\n",
      "Average test loss: 0.0016960084112361074\n",
      "Epoch 280/300\n",
      "Average training loss: 0.024454464076293838\n",
      "Average test loss: 0.001638081717511846\n",
      "Epoch 281/300\n",
      "Average training loss: 0.024643433580795925\n",
      "Average test loss: 0.0018067064903055628\n",
      "Epoch 282/300\n",
      "Average training loss: 0.024422815543082024\n",
      "Average test loss: 0.0017757840730870764\n",
      "Epoch 283/300\n",
      "Average training loss: 0.024389704619844754\n",
      "Average test loss: 0.0017342724876685275\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02445564771360821\n",
      "Average test loss: 0.001692395334959858\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02444906757440832\n",
      "Average test loss: 0.0017634071817414628\n",
      "Epoch 286/300\n",
      "Average training loss: 0.024389397773477765\n",
      "Average test loss: 0.0016653890512469743\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02436138752930694\n",
      "Average test loss: 0.0017711743141214052\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02441992275747988\n",
      "Average test loss: 0.0016946273175999522\n",
      "Epoch 289/300\n",
      "Average training loss: 0.024656238085693782\n",
      "Average test loss: 0.0016864332675726877\n",
      "Epoch 290/300\n",
      "Average training loss: 0.024266062481535806\n",
      "Average test loss: 0.0018448376581072807\n",
      "Epoch 291/300\n",
      "Average training loss: 0.024293652263780434\n",
      "Average test loss: 0.0018324901840339105\n",
      "Epoch 292/300\n",
      "Average training loss: 0.024322284206748007\n",
      "Average test loss: 0.0017892642489944896\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02451547154287497\n",
      "Average test loss: 0.0016719490970588393\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02423663701613744\n",
      "Average test loss: 0.0017188067567638224\n",
      "Epoch 295/300\n",
      "Average training loss: 0.024253588537375134\n",
      "Average test loss: 0.0017039033149679503\n",
      "Epoch 296/300\n",
      "Average training loss: 0.024307930611901812\n",
      "Average test loss: 0.0018096503025541702\n",
      "Epoch 297/300\n",
      "Average training loss: 0.024314377614193493\n",
      "Average test loss: 0.001789255863469508\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02446849534826146\n",
      "Average test loss: 0.0017095381579258376\n",
      "Epoch 299/300\n",
      "Average training loss: 0.024192590730057823\n",
      "Average test loss: 0.001709515186233653\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02429236364364624\n",
      "Average test loss: 0.0020388762701509726\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Gauss_32_Depth3/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.35\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.37\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.25\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.41\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.65\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.67\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.88\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.42\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.77\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.48\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.61\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.42\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.58\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.12\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.79\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.62\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.89\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.65\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.96\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.01\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.47\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.95\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 33.10\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 33.32\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 33.49\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.67\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.75\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.812407128228081\n",
      "Average test loss: 0.015466776776644918\n",
      "Epoch 2/300\n",
      "Average training loss: 3.027272459665934\n",
      "Average test loss: 0.011317533581621117\n",
      "Epoch 3/300\n",
      "Average training loss: 2.0759976784388225\n",
      "Average test loss: 0.010136892686287561\n",
      "Epoch 4/300\n",
      "Average training loss: 1.5001522464752197\n",
      "Average test loss: 0.010465128757473496\n",
      "Epoch 5/300\n",
      "Average training loss: 1.1080536184310914\n",
      "Average test loss: 0.009938770949012703\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8065482357343038\n",
      "Average test loss: 0.009661958229210642\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6180477359029982\n",
      "Average test loss: 0.008218090627756384\n",
      "Epoch 8/300\n",
      "Average training loss: 0.5065022172398037\n",
      "Average test loss: 0.008052422858774663\n",
      "Epoch 9/300\n",
      "Average training loss: 0.43924794250064425\n",
      "Average test loss: 0.009142038827968968\n",
      "Epoch 10/300\n",
      "Average training loss: 0.395476623667611\n",
      "Average test loss: 0.008251712554030947\n",
      "Epoch 11/300\n",
      "Average training loss: 0.3660706377294328\n",
      "Average test loss: 0.00949735506458415\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3467454934120178\n",
      "Average test loss: 0.00821910253746642\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3273179238372379\n",
      "Average test loss: 0.009000774227910572\n",
      "Epoch 14/300\n",
      "Average training loss: 0.31439394688606265\n",
      "Average test loss: 0.008235583623250326\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30140657912360297\n",
      "Average test loss: 0.0074494391216172115\n",
      "Epoch 16/300\n",
      "Average training loss: 0.29328691160678866\n",
      "Average test loss: 0.008220310505893495\n",
      "Epoch 17/300\n",
      "Average training loss: 0.28384505528873866\n",
      "Average test loss: 0.00851324664718575\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2779835917949677\n",
      "Average test loss: 0.007683338375555145\n",
      "Epoch 19/300\n",
      "Average training loss: 0.26780799628628627\n",
      "Average test loss: 0.00694880483382278\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2635002525250117\n",
      "Average test loss: 0.0076207623647318945\n",
      "Epoch 21/300\n",
      "Average training loss: 0.25636493537161087\n",
      "Average test loss: 0.0070040298178792\n",
      "Epoch 22/300\n",
      "Average training loss: 0.2508343879646725\n",
      "Average test loss: 0.006644960701879528\n",
      "Epoch 23/300\n",
      "Average training loss: 0.24429678556654189\n",
      "Average test loss: 0.006701113258384996\n",
      "Epoch 24/300\n",
      "Average training loss: 0.24361491027143267\n",
      "Average test loss: 0.00692717232555151\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23863812812169394\n",
      "Average test loss: 0.007481339469965961\n",
      "Epoch 26/300\n",
      "Average training loss: 0.23526375558641222\n",
      "Average test loss: 0.00774511068355706\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2303413586086697\n",
      "Average test loss: 0.006557029836293724\n",
      "Epoch 28/300\n",
      "Average training loss: 0.22562027678224775\n",
      "Average test loss: 0.006561963784197967\n",
      "Epoch 29/300\n",
      "Average training loss: 0.22521269473764632\n",
      "Average test loss: 0.006808139922304286\n",
      "Epoch 30/300\n",
      "Average training loss: 0.2219684310224321\n",
      "Average test loss: 0.0064724789526727465\n",
      "Epoch 31/300\n",
      "Average training loss: 0.21767959009276497\n",
      "Average test loss: 0.0064457917006479366\n",
      "Epoch 32/300\n",
      "Average training loss: 0.21550316711266834\n",
      "Average test loss: 0.00629655189646615\n",
      "Epoch 33/300\n",
      "Average training loss: 0.2142687151233355\n",
      "Average test loss: 0.006455265039785041\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2115498546494378\n",
      "Average test loss: 0.006475783259504371\n",
      "Epoch 35/300\n",
      "Average training loss: 0.21008218918906318\n",
      "Average test loss: 0.006216810502939754\n",
      "Epoch 36/300\n",
      "Average training loss: 0.20690540046162076\n",
      "Average test loss: 0.0061610085326764315\n",
      "Epoch 37/300\n",
      "Average training loss: 0.20550527327590518\n",
      "Average test loss: 0.006146404883513848\n",
      "Epoch 38/300\n",
      "Average training loss: 0.20394958237806957\n",
      "Average test loss: 0.006396940573636029\n",
      "Epoch 39/300\n",
      "Average training loss: 0.20249981710645887\n",
      "Average test loss: 0.00626480358093977\n",
      "Epoch 40/300\n",
      "Average training loss: 0.20195922870106167\n",
      "Average test loss: 0.006142143098430501\n",
      "Epoch 41/300\n",
      "Average training loss: 0.19966678951846228\n",
      "Average test loss: 0.006143373579407732\n",
      "Epoch 42/300\n",
      "Average training loss: 0.19806218763192496\n",
      "Average test loss: 0.006210059038880798\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1961580365233951\n",
      "Average test loss: 0.006120366235574087\n",
      "Epoch 44/300\n",
      "Average training loss: 0.19499370696809556\n",
      "Average test loss: 0.006039038364671998\n",
      "Epoch 45/300\n",
      "Average training loss: 0.19510334311591254\n",
      "Average test loss: 0.0064383267838921815\n",
      "Epoch 46/300\n",
      "Average training loss: 0.193753956662284\n",
      "Average test loss: 0.006324824725381202\n",
      "Epoch 47/300\n",
      "Average training loss: 0.19193855065107346\n",
      "Average test loss: 0.006227477821624941\n",
      "Epoch 48/300\n",
      "Average training loss: 0.19122767273585\n",
      "Average test loss: 0.006242954109691911\n",
      "Epoch 49/300\n",
      "Average training loss: 0.19078886388407812\n",
      "Average test loss: 0.006385276089732846\n",
      "Epoch 50/300\n",
      "Average training loss: 0.19009083700180054\n",
      "Average test loss: 0.006001427097866933\n",
      "Epoch 51/300\n",
      "Average training loss: 0.18894564322630564\n",
      "Average test loss: 0.0068905407355891336\n",
      "Epoch 52/300\n",
      "Average training loss: 0.18774305721124013\n",
      "Average test loss: 0.0064809020236134525\n",
      "Epoch 53/300\n",
      "Average training loss: 0.18753339876068964\n",
      "Average test loss: 0.006592280169328054\n",
      "Epoch 54/300\n",
      "Average training loss: 0.18782110460599263\n",
      "Average test loss: 0.005979360358582602\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1850702293978797\n",
      "Average test loss: 0.006378665311882893\n",
      "Epoch 56/300\n",
      "Average training loss: 0.18479483958085377\n",
      "Average test loss: 0.009715470862885316\n",
      "Epoch 57/300\n",
      "Average training loss: 0.18565566453668805\n",
      "Average test loss: 0.0059053251443223825\n",
      "Epoch 58/300\n",
      "Average training loss: 0.18292053339216444\n",
      "Average test loss: 0.821268581840727\n",
      "Epoch 59/300\n",
      "Average training loss: 0.18251566874980926\n",
      "Average test loss: 0.00607899385649297\n",
      "Epoch 60/300\n",
      "Average training loss: 0.18168015355534023\n",
      "Average test loss: 0.0060206054329044285\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1809435998333825\n",
      "Average test loss: 0.0058968613164292446\n",
      "Epoch 62/300\n",
      "Average training loss: 0.18028870794508192\n",
      "Average test loss: 0.006205918039712641\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1800471894343694\n",
      "Average test loss: 0.006069790681617128\n",
      "Epoch 64/300\n",
      "Average training loss: 0.17926004298528037\n",
      "Average test loss: 0.005986405598206653\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1784520149893231\n",
      "Average test loss: 0.006610803543279568\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1776673832204607\n",
      "Average test loss: 0.0061297775051660005\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1769695749150382\n",
      "Average test loss: 0.006349788529591428\n",
      "Epoch 68/300\n",
      "Average training loss: 0.17676964269744025\n",
      "Average test loss: 0.005872173960010211\n",
      "Epoch 69/300\n",
      "Average training loss: 0.17642806853188409\n",
      "Average test loss: 0.005869268657018741\n",
      "Epoch 70/300\n",
      "Average training loss: 0.17592491862508985\n",
      "Average test loss: 5.177893647193908\n",
      "Epoch 71/300\n",
      "Average training loss: 3.1429728164407944\n",
      "Average test loss: 0.01075890350424581\n",
      "Epoch 72/300\n",
      "Average training loss: 5.319102387958103\n",
      "Average test loss: 0.008012068237695428\n",
      "Epoch 73/300\n",
      "Average training loss: 2.9876727833218046\n",
      "Average test loss: 0.007138143880913655\n",
      "Epoch 74/300\n",
      "Average training loss: 2.080683025677999\n",
      "Average test loss: 0.006744360998272896\n",
      "Epoch 75/300\n",
      "Average training loss: 1.5302360810173883\n",
      "Average test loss: 0.006589019195901023\n",
      "Epoch 76/300\n",
      "Average training loss: 1.2137196168899536\n",
      "Average test loss: 0.007757234654078881\n",
      "Epoch 77/300\n",
      "Average training loss: 1.0103314997884962\n",
      "Average test loss: 0.0064539819848206306\n",
      "Epoch 78/300\n",
      "Average training loss: 0.8640921896298727\n",
      "Average test loss: 0.006213519585629304\n",
      "Epoch 79/300\n",
      "Average training loss: 0.7505442095862495\n",
      "Average test loss: 0.00715079938661721\n",
      "Epoch 80/300\n",
      "Average training loss: 0.6620032359759013\n",
      "Average test loss: 0.006194268607430988\n",
      "Epoch 81/300\n",
      "Average training loss: 0.5870407675637139\n",
      "Average test loss: 0.0062608957369294435\n",
      "Epoch 82/300\n",
      "Average training loss: 0.5265789927641551\n",
      "Average test loss: 0.0064762943581574495\n",
      "Epoch 83/300\n",
      "Average training loss: 0.47451283115810816\n",
      "Average test loss: 0.006469605657996403\n",
      "Epoch 84/300\n",
      "Average training loss: 0.431015311188168\n",
      "Average test loss: 0.006067711867805984\n",
      "Epoch 85/300\n",
      "Average training loss: 0.39402911615371705\n",
      "Average test loss: 0.005985789537015888\n",
      "Epoch 86/300\n",
      "Average training loss: 0.36276368835237294\n",
      "Average test loss: 0.006638619163797961\n",
      "Epoch 87/300\n",
      "Average training loss: 0.3331420679092407\n",
      "Average test loss: 0.005948445073018472\n",
      "Epoch 88/300\n",
      "Average training loss: 0.3113422102663252\n",
      "Average test loss: 0.006460876069962978\n",
      "Epoch 89/300\n",
      "Average training loss: 0.2924667665958405\n",
      "Average test loss: 0.028791034317678876\n",
      "Epoch 90/300\n",
      "Average training loss: 0.2803752558628718\n",
      "Average test loss: 0.03832638961325089\n",
      "Epoch 91/300\n",
      "Average training loss: 0.2868281466431088\n",
      "Average test loss: 9.239241844971975\n",
      "Epoch 92/300\n",
      "Average training loss: 0.25709963681962755\n",
      "Average test loss: 0.005969472967916065\n",
      "Epoch 93/300\n",
      "Average training loss: 0.2590077271593942\n",
      "Average test loss: 0.008168828019665347\n",
      "Epoch 94/300\n",
      "Average training loss: 0.23959366697735257\n",
      "Average test loss: 20.08519070752793\n",
      "Epoch 95/300\n",
      "Average training loss: 0.23458917032347784\n",
      "Average test loss: 0.02529401114417447\n",
      "Epoch 96/300\n",
      "Average training loss: 0.23200514052973853\n",
      "Average test loss: 0.7240045700867971\n",
      "Epoch 97/300\n",
      "Average training loss: 0.22171709371937645\n",
      "Average test loss: 0.006799312475240893\n",
      "Epoch 98/300\n",
      "Average training loss: 21306853.639907558\n",
      "Average test loss: 0.7067465726534525\n",
      "Epoch 99/300\n",
      "Average training loss: 34.889975755479604\n",
      "Average test loss: 0.6051677429940966\n",
      "Epoch 100/300\n",
      "Average training loss: 34.340463768853084\n",
      "Average test loss: 0.7241675623787774\n",
      "Epoch 101/300\n",
      "Average training loss: 33.94695844862196\n",
      "Average test loss: 0.5073898871209886\n",
      "Epoch 102/300\n",
      "Average training loss: 33.33698018052843\n",
      "Average test loss: 0.33813238157166375\n",
      "Epoch 103/300\n",
      "Average training loss: 32.84522880554199\n",
      "Average test loss: 0.232560928384463\n",
      "Epoch 104/300\n",
      "Average training loss: 32.371290313720706\n",
      "Average test loss: 0.2947861351834403\n",
      "Epoch 105/300\n",
      "Average training loss: 31.842481770833334\n",
      "Average test loss: 0.15721105771594576\n",
      "Epoch 106/300\n",
      "Average training loss: 31.138925886366103\n",
      "Average test loss: 0.1564156966275639\n",
      "Epoch 107/300\n",
      "Average training loss: 30.569914443969726\n",
      "Average test loss: 0.13805480355024338\n",
      "Epoch 108/300\n",
      "Average training loss: 29.859185268825954\n",
      "Average test loss: 0.04997571622994211\n",
      "Epoch 109/300\n",
      "Average training loss: 29.072548590766058\n",
      "Average test loss: 0.04670743324028121\n",
      "Epoch 110/300\n",
      "Average training loss: 28.297051245795355\n",
      "Average test loss: 0.04406037680308024\n",
      "Epoch 111/300\n",
      "Average training loss: 27.46118514166938\n",
      "Average test loss: 0.07046660494804383\n",
      "Epoch 112/300\n",
      "Average training loss: 26.566881184895834\n",
      "Average test loss: 0.030513010885980393\n",
      "Epoch 113/300\n",
      "Average training loss: 25.579857452392577\n",
      "Average test loss: 0.022453827227155367\n",
      "Epoch 114/300\n",
      "Average training loss: 24.60620910135905\n",
      "Average test loss: 0.02114983561138312\n",
      "Epoch 115/300\n",
      "Average training loss: 23.63874772474501\n",
      "Average test loss: 0.024997900325391027\n",
      "Epoch 116/300\n",
      "Average training loss: 22.64365337117513\n",
      "Average test loss: 0.02215299864444468\n",
      "Epoch 117/300\n",
      "Average training loss: 21.670215274386937\n",
      "Average test loss: 0.013966977344618903\n",
      "Epoch 118/300\n",
      "Average training loss: 20.745719129774304\n",
      "Average test loss: 0.01322367662191391\n",
      "Epoch 119/300\n",
      "Average training loss: 19.847579087999133\n",
      "Average test loss: 0.012264929679532846\n",
      "Epoch 120/300\n",
      "Average training loss: 18.96496967909071\n",
      "Average test loss: 0.011568008302814431\n",
      "Epoch 121/300\n",
      "Average training loss: 18.126655822753907\n",
      "Average test loss: 0.0107447995212343\n",
      "Epoch 122/300\n",
      "Average training loss: 17.273586698744033\n",
      "Average test loss: 0.08345360162440274\n",
      "Epoch 123/300\n",
      "Average training loss: 16.445670102437337\n",
      "Average test loss: 0.010253231669465701\n",
      "Epoch 124/300\n",
      "Average training loss: 15.732612058003744\n",
      "Average test loss: 0.011430131504105196\n",
      "Epoch 125/300\n",
      "Average training loss: 15.106993169148764\n",
      "Average test loss: 0.009399867706000806\n",
      "Epoch 126/300\n",
      "Average training loss: 14.518121003892686\n",
      "Average test loss: 0.009544644505199458\n",
      "Epoch 127/300\n",
      "Average training loss: 13.99988870493571\n",
      "Average test loss: 0.00942362618777487\n",
      "Epoch 128/300\n",
      "Average training loss: 13.48188498433431\n",
      "Average test loss: 0.008834994160466724\n",
      "Epoch 129/300\n",
      "Average training loss: 13.039468101501464\n",
      "Average test loss: 0.009634604141943984\n",
      "Epoch 130/300\n",
      "Average training loss: 12.602543029785156\n",
      "Average test loss: 0.009645770370132393\n",
      "Epoch 131/300\n",
      "Average training loss: 12.23683721499973\n",
      "Average test loss: 0.008846665427916579\n",
      "Epoch 132/300\n",
      "Average training loss: 11.887889704386394\n",
      "Average test loss: 0.008295542833705743\n",
      "Epoch 133/300\n",
      "Average training loss: 11.600942703247071\n",
      "Average test loss: 0.008708047834535439\n",
      "Epoch 134/300\n",
      "Average training loss: 11.308067409939236\n",
      "Average test loss: 0.007905185216002994\n",
      "Epoch 135/300\n",
      "Average training loss: 11.011630819532606\n",
      "Average test loss: 0.007800760745174355\n",
      "Epoch 136/300\n",
      "Average training loss: 10.755695705837674\n",
      "Average test loss: 0.007643229643917746\n",
      "Epoch 137/300\n",
      "Average training loss: 10.492055324130588\n",
      "Average test loss: 0.0075376606004105675\n",
      "Epoch 138/300\n",
      "Average training loss: 10.238997579786513\n",
      "Average test loss: 0.007865020527607864\n",
      "Epoch 139/300\n",
      "Average training loss: 9.989511409335666\n",
      "Average test loss: 0.007281449420998494\n",
      "Epoch 140/300\n",
      "Average training loss: 9.711135240342882\n",
      "Average test loss: 0.0071891005958120025\n",
      "Epoch 141/300\n",
      "Average training loss: 9.409610683017307\n",
      "Average test loss: 0.007189547217140595\n",
      "Epoch 142/300\n",
      "Average training loss: 9.097783083597818\n",
      "Average test loss: 0.0069320125414265524\n",
      "Epoch 143/300\n",
      "Average training loss: 8.754282084994847\n",
      "Average test loss: 0.006775285194317499\n",
      "Epoch 144/300\n",
      "Average training loss: 8.324289505004883\n",
      "Average test loss: 0.007046433593663904\n",
      "Epoch 145/300\n",
      "Average training loss: 7.8165845247904455\n",
      "Average test loss: 0.007038989767432213\n",
      "Epoch 146/300\n",
      "Average training loss: 7.281768267313639\n",
      "Average test loss: 0.006585081767704752\n",
      "Epoch 147/300\n",
      "Average training loss: 6.128226794348823\n",
      "Average test loss: 0.006656559272772736\n",
      "Epoch 148/300\n",
      "Average training loss: 5.118035583496094\n",
      "Average test loss: 0.008541715923696756\n",
      "Epoch 149/300\n",
      "Average training loss: 4.608546195560032\n",
      "Average test loss: 0.006302207632197274\n",
      "Epoch 150/300\n",
      "Average training loss: 4.11164694680108\n",
      "Average test loss: 0.010683309939172533\n",
      "Epoch 151/300\n",
      "Average training loss: 3.6664684427049425\n",
      "Average test loss: 0.006228630284054412\n",
      "Epoch 152/300\n",
      "Average training loss: 3.2716974466111926\n",
      "Average test loss: 0.008317176611059242\n",
      "Epoch 153/300\n",
      "Average training loss: 2.9288061464097765\n",
      "Average test loss: 0.006308588543699848\n",
      "Epoch 154/300\n",
      "Average training loss: 2.6339760117001005\n",
      "Average test loss: 0.00882137288732661\n",
      "Epoch 155/300\n",
      "Average training loss: 2.3823136840396457\n",
      "Average test loss: 0.0063642696502308055\n",
      "Epoch 156/300\n",
      "Average training loss: 2.1553622873094347\n",
      "Average test loss: 0.006173868671887451\n",
      "Epoch 157/300\n",
      "Average training loss: 1.9389691886901856\n",
      "Average test loss: 0.006110478875537713\n",
      "Epoch 158/300\n",
      "Average training loss: 1.7238189970652262\n",
      "Average test loss: 0.00609682619985607\n",
      "Epoch 159/300\n",
      "Average training loss: 1.5227923985587226\n",
      "Average test loss: 0.006232135251164436\n",
      "Epoch 160/300\n",
      "Average training loss: 1.336338468975491\n",
      "Average test loss: 0.006231439929041597\n",
      "Epoch 161/300\n",
      "Average training loss: 1.1576739588843452\n",
      "Average test loss: 0.005976752739399672\n",
      "Epoch 162/300\n",
      "Average training loss: 0.9872712732950847\n",
      "Average test loss: 0.006085564941581753\n",
      "Epoch 163/300\n",
      "Average training loss: 0.830573925336202\n",
      "Average test loss: 0.005940080759425958\n",
      "Epoch 164/300\n",
      "Average training loss: 0.6968335404396057\n",
      "Average test loss: 0.006161731692237987\n",
      "Epoch 165/300\n",
      "Average training loss: 0.5845730000072056\n",
      "Average test loss: 0.013334577687084674\n",
      "Epoch 166/300\n",
      "Average training loss: 0.49447551867696976\n",
      "Average test loss: 0.012506551418039533\n",
      "Epoch 167/300\n",
      "Average training loss: 0.4204038329919179\n",
      "Average test loss: 0.006204032882634137\n",
      "Epoch 168/300\n",
      "Average training loss: 0.36266662152608237\n",
      "Average test loss: 0.01012546285510891\n",
      "Epoch 169/300\n",
      "Average training loss: 0.32284506927596196\n",
      "Average test loss: 0.006205531536084083\n",
      "Epoch 170/300\n",
      "Average training loss: 0.29545351583427854\n",
      "Average test loss: 0.010618566396749681\n",
      "Epoch 171/300\n",
      "Average training loss: 0.27543230019675363\n",
      "Average test loss: 0.005997828828377856\n",
      "Epoch 172/300\n",
      "Average training loss: 0.26049303073353236\n",
      "Average test loss: 0.008641309708356857\n",
      "Epoch 173/300\n",
      "Average training loss: 29445705.773584355\n",
      "Average test loss: 0.2752334112856123\n",
      "Epoch 174/300\n",
      "Average training loss: 24.14784725104438\n",
      "Average test loss: 3.3781377555529275\n",
      "Epoch 175/300\n",
      "Average training loss: 23.430999745686847\n",
      "Average test loss: 5758.775421440972\n",
      "Epoch 176/300\n",
      "Average training loss: 22.93954099867079\n",
      "Average test loss: 5.888861999458737\n",
      "Epoch 177/300\n",
      "Average training loss: 22.578019253200956\n",
      "Average test loss: 0.4239537823332681\n",
      "Epoch 178/300\n",
      "Average training loss: 22.225287221272787\n",
      "Average test loss: 1.531050809118483\n",
      "Epoch 179/300\n",
      "Average training loss: 22.013398512098526\n",
      "Average test loss: 14.599684008724159\n",
      "Epoch 180/300\n",
      "Average training loss: 21.76492059495714\n",
      "Average test loss: 0.2834365716377894\n",
      "Epoch 181/300\n",
      "Average training loss: 21.558052071465386\n",
      "Average test loss: 0.08820066817601521\n",
      "Epoch 182/300\n",
      "Average training loss: 21.403518171522354\n",
      "Average test loss: 128.80626008478137\n",
      "Epoch 183/300\n",
      "Average training loss: 21.220900095621744\n",
      "Average test loss: 0.07252495695485009\n",
      "Epoch 184/300\n",
      "Average training loss: 21.016503836737737\n",
      "Average test loss: 0.046128866298331156\n",
      "Epoch 185/300\n",
      "Average training loss: 20.849456022474502\n",
      "Average test loss: 0.07471664412816366\n",
      "Epoch 186/300\n",
      "Average training loss: 20.675449396769206\n",
      "Average test loss: 0.1090586421986421\n",
      "Epoch 187/300\n",
      "Average training loss: 20.46394042799208\n",
      "Average test loss: 0.09193919099039502\n",
      "Epoch 188/300\n",
      "Average training loss: 20.29398058403863\n",
      "Average test loss: 0.05809105355044206\n",
      "Epoch 189/300\n",
      "Average training loss: 20.088820622762043\n",
      "Average test loss: 0.04948248836563693\n",
      "Epoch 190/300\n",
      "Average training loss: 19.892558066474066\n",
      "Average test loss: 3.625522102885776\n",
      "Epoch 191/300\n",
      "Average training loss: 19.6303559044732\n",
      "Average test loss: 0.40444157948096593\n",
      "Epoch 192/300\n",
      "Average training loss: 19.417722466362846\n",
      "Average test loss: 2.7731001718242965\n",
      "Epoch 193/300\n",
      "Average training loss: 19.161158635457358\n",
      "Average test loss: 0.059043746368752585\n",
      "Epoch 194/300\n",
      "Average training loss: 18.882561280992295\n",
      "Average test loss: 1.7276155837641822\n",
      "Epoch 195/300\n",
      "Average training loss: 18.597812116834852\n",
      "Average test loss: 27.76624341837565\n",
      "Epoch 196/300\n",
      "Average training loss: 18.304551005045575\n",
      "Average test loss: 0.12696127853294215\n",
      "Epoch 197/300\n",
      "Average training loss: 17.96617703077528\n",
      "Average test loss: 0.05695709889464908\n",
      "Epoch 198/300\n",
      "Average training loss: 17.628463907877602\n",
      "Average test loss: 0.012805086760885185\n",
      "Epoch 199/300\n",
      "Average training loss: 17.254556257459853\n",
      "Average test loss: 0.8537681529919307\n",
      "Epoch 200/300\n",
      "Average training loss: 16.860110477023653\n",
      "Average test loss: 0.011425726484921243\n",
      "Epoch 201/300\n",
      "Average training loss: 16.41522955830892\n",
      "Average test loss: 0.015413159757852554\n",
      "Epoch 202/300\n",
      "Average training loss: 15.948286026848686\n",
      "Average test loss: 0.027013102125790386\n",
      "Epoch 203/300\n",
      "Average training loss: 15.445450012207031\n",
      "Average test loss: 0.014883326667050521\n",
      "Epoch 204/300\n",
      "Average training loss: 14.878887136671278\n",
      "Average test loss: 0.013058381179968517\n",
      "Epoch 205/300\n",
      "Average training loss: 14.295034016927083\n",
      "Average test loss: 0.010470419475601778\n",
      "Epoch 206/300\n",
      "Average training loss: 13.707531979031033\n",
      "Average test loss: 0.6148774803744422\n",
      "Epoch 207/300\n",
      "Average training loss: 13.252092140197753\n",
      "Average test loss: 0.010551294806516833\n",
      "Epoch 208/300\n",
      "Average training loss: 12.848778058369954\n",
      "Average test loss: 0.010800838803251585\n",
      "Epoch 209/300\n",
      "Average training loss: 12.542536079406739\n",
      "Average test loss: 0.007168614940510856\n",
      "Epoch 210/300\n",
      "Average training loss: 12.23901837158203\n",
      "Average test loss: 0.015901729668180146\n",
      "Epoch 211/300\n",
      "Average training loss: 11.888266917758518\n",
      "Average test loss: 0.0072335170254939135\n",
      "Epoch 212/300\n",
      "Average training loss: 11.470703925238714\n",
      "Average test loss: 0.007331043166418871\n",
      "Epoch 213/300\n",
      "Average training loss: 11.080259086608887\n",
      "Average test loss: 0.013550722051825788\n",
      "Epoch 214/300\n",
      "Average training loss: 10.783201459248861\n",
      "Average test loss: 0.007663397849847873\n",
      "Epoch 215/300\n",
      "Average training loss: 10.51954874420166\n",
      "Average test loss: 0.006645181917895873\n",
      "Epoch 216/300\n",
      "Average training loss: 10.261094631618924\n",
      "Average test loss: 0.006615819736487336\n",
      "Epoch 217/300\n",
      "Average training loss: 9.978281034681531\n",
      "Average test loss: 0.007131274078455236\n",
      "Epoch 218/300\n",
      "Average training loss: 9.69046184794108\n",
      "Average test loss: 0.009079583194520739\n",
      "Epoch 219/300\n",
      "Average training loss: 9.371806197272406\n",
      "Average test loss: 0.00869421229759852\n",
      "Epoch 220/300\n",
      "Average training loss: 9.010094843546549\n",
      "Average test loss: 0.006419229479713572\n",
      "Epoch 221/300\n",
      "Average training loss: 8.60570972612169\n",
      "Average test loss: 0.006949842509710127\n",
      "Epoch 222/300\n",
      "Average training loss: 8.170686240302192\n",
      "Average test loss: 0.006365692350185579\n",
      "Epoch 223/300\n",
      "Average training loss: 7.6666817682054305\n",
      "Average test loss: 0.006406302807231744\n",
      "Epoch 224/300\n",
      "Average training loss: 7.126321803622775\n",
      "Average test loss: 0.006193650706774659\n",
      "Epoch 225/300\n",
      "Average training loss: 6.536088903215196\n",
      "Average test loss: 0.0064922183197405605\n",
      "Epoch 226/300\n",
      "Average training loss: 5.930291743384467\n",
      "Average test loss: 0.006242602014086313\n",
      "Epoch 227/300\n",
      "Average training loss: 5.320379723442925\n",
      "Average test loss: 0.006767358353154527\n",
      "Epoch 228/300\n",
      "Average training loss: 4.771783941480849\n",
      "Average test loss: 0.006714407225035959\n",
      "Epoch 229/300\n",
      "Average training loss: 4.267337212456598\n",
      "Average test loss: 0.006262070734881693\n",
      "Epoch 230/300\n",
      "Average training loss: 3.804541908899943\n",
      "Average test loss: 0.006499748891840378\n",
      "Epoch 231/300\n",
      "Average training loss: 3.3728054394192166\n",
      "Average test loss: 0.006224377604408396\n",
      "Epoch 232/300\n",
      "Average training loss: 2.9737261871761747\n",
      "Average test loss: 0.01974082353214423\n",
      "Epoch 233/300\n",
      "Average training loss: 2.5544060948689777\n",
      "Average test loss: 0.010113291547530228\n",
      "Epoch 234/300\n",
      "Average training loss: 2.220504912376404\n",
      "Average test loss: 0.006946355996032556\n",
      "Epoch 235/300\n",
      "Average training loss: 1.9094920097986856\n",
      "Average test loss: 0.016859675938884416\n",
      "Epoch 236/300\n",
      "Average training loss: 1.6167040882110595\n",
      "Average test loss: 0.01875515160150826\n",
      "Epoch 237/300\n",
      "Average training loss: 1.3478315160539416\n",
      "Average test loss: 0.005981399404505889\n",
      "Epoch 238/300\n",
      "Average training loss: 1.1053576607174342\n",
      "Average test loss: 0.053999027142094244\n",
      "Epoch 239/300\n",
      "Average training loss: 0.9086280204984877\n",
      "Average test loss: 0.01037416988114516\n",
      "Epoch 240/300\n",
      "Average training loss: 0.7577740081681146\n",
      "Average test loss: 0.006433731277783712\n",
      "Epoch 241/300\n",
      "Average training loss: 0.6381532066663106\n",
      "Average test loss: 0.3677900758981705\n",
      "Epoch 242/300\n",
      "Average training loss: 0.5369303994178772\n",
      "Average test loss: 0.0059482524353596895\n",
      "Epoch 243/300\n",
      "Average training loss: 0.4568264731566111\n",
      "Average test loss: 0.005869548540976313\n",
      "Epoch 244/300\n",
      "Average training loss: 0.3936728746361203\n",
      "Average test loss: 0.005876894794404506\n",
      "Epoch 245/300\n",
      "Average training loss: 0.3479872017701467\n",
      "Average test loss: 0.0059452639702293605\n",
      "Epoch 246/300\n",
      "Average training loss: 0.31244206833839416\n",
      "Average test loss: 0.005952883575525549\n",
      "Epoch 247/300\n",
      "Average training loss: 0.284406670305464\n",
      "Average test loss: 0.006879631268481414\n",
      "Epoch 248/300\n",
      "Average training loss: 0.2645119607448578\n",
      "Average test loss: 0.005909859911849101\n",
      "Epoch 249/300\n",
      "Average training loss: 0.2507480579217275\n",
      "Average test loss: 0.005922483878003227\n",
      "Epoch 250/300\n",
      "Average training loss: 0.23873139882087707\n",
      "Average test loss: 0.005893415127777391\n",
      "Epoch 251/300\n",
      "Average training loss: 0.22982841491699219\n",
      "Average test loss: 0.005787378013134003\n",
      "Epoch 252/300\n",
      "Average training loss: 0.22322500971953074\n",
      "Average test loss: 0.005828717963563071\n",
      "Epoch 253/300\n",
      "Average training loss: 0.21697616181108687\n",
      "Average test loss: 0.005884790900266832\n",
      "Epoch 254/300\n",
      "Average training loss: 0.21171650646792517\n",
      "Average test loss: 0.005875109225511551\n",
      "Epoch 255/300\n",
      "Average training loss: 0.20699772977828979\n",
      "Average test loss: 0.005829765503605207\n",
      "Epoch 256/300\n",
      "Average training loss: 0.20427669741047752\n",
      "Average test loss: 0.00588063378756245\n",
      "Epoch 257/300\n",
      "Average training loss: 0.20134086787700653\n",
      "Average test loss: 0.11891126963827345\n",
      "Epoch 258/300\n",
      "Average training loss: 0.19901147978835634\n",
      "Average test loss: 0.0058551070503890515\n",
      "Epoch 259/300\n",
      "Average training loss: 0.19663315387566885\n",
      "Average test loss: 0.0063430064084629215\n",
      "Epoch 260/300\n",
      "Average training loss: 0.19456620485252804\n",
      "Average test loss: 0.007996305253356695\n",
      "Epoch 261/300\n",
      "Average training loss: 0.19274748827351465\n",
      "Average test loss: 0.00589681952032778\n",
      "Epoch 262/300\n",
      "Average training loss: 0.19094813170697955\n",
      "Average test loss: 0.0058327869114776455\n",
      "Epoch 263/300\n",
      "Average training loss: 0.18946540802054934\n",
      "Average test loss: 0.005841933632890383\n",
      "Epoch 264/300\n",
      "Average training loss: 0.18789172195063697\n",
      "Average test loss: 0.006036848824471235\n",
      "Epoch 265/300\n",
      "Average training loss: 0.1866971433295144\n",
      "Average test loss: 0.00586441054691871\n",
      "Epoch 266/300\n",
      "Average training loss: 0.18570681868659125\n",
      "Average test loss: 0.006014140122466617\n",
      "Epoch 267/300\n",
      "Average training loss: 0.18395688601997165\n",
      "Average test loss: 0.005920410249796179\n",
      "Epoch 268/300\n",
      "Average training loss: 0.18263609674241807\n",
      "Average test loss: 0.0059267002683546805\n",
      "Epoch 269/300\n",
      "Average training loss: 0.1818106908665763\n",
      "Average test loss: 0.005773608593684104\n",
      "Epoch 270/300\n",
      "Average training loss: 0.1804594337410397\n",
      "Average test loss: 0.005810004949569702\n",
      "Epoch 271/300\n",
      "Average training loss: 0.1800416169696384\n",
      "Average test loss: 0.00591214044060972\n",
      "Epoch 272/300\n",
      "Average training loss: 0.17901794394519593\n",
      "Average test loss: 0.005885851008610593\n",
      "Epoch 273/300\n",
      "Average training loss: 0.17806961860921647\n",
      "Average test loss: 0.005854838091466162\n",
      "Epoch 274/300\n",
      "Average training loss: 0.17766910572184458\n",
      "Average test loss: 0.005836490761074755\n",
      "Epoch 275/300\n",
      "Average training loss: 0.17692556104395124\n",
      "Average test loss: 0.005812792222946882\n",
      "Epoch 276/300\n",
      "Average training loss: 0.17642243560155232\n",
      "Average test loss: 0.0058213785551488395\n",
      "Epoch 277/300\n",
      "Average training loss: 0.1745056439505683\n",
      "Average test loss: 0.005874649848788977\n",
      "Epoch 278/300\n",
      "Average training loss: 0.17415069709221523\n",
      "Average test loss: 0.005818248545544015\n",
      "Epoch 279/300\n",
      "Average training loss: 0.17318469387955135\n",
      "Average test loss: 0.005975443205485742\n",
      "Epoch 280/300\n",
      "Average training loss: 0.1731610651810964\n",
      "Average test loss: 0.006061492593338092\n",
      "Epoch 281/300\n",
      "Average training loss: 0.17297686460283068\n",
      "Average test loss: 0.005870168114701907\n",
      "Epoch 282/300\n",
      "Average training loss: 0.1724513916571935\n",
      "Average test loss: 0.005927967722217242\n",
      "Epoch 283/300\n",
      "Average training loss: 0.1716950310866038\n",
      "Average test loss: 0.005925823083354367\n",
      "Epoch 284/300\n",
      "Average training loss: 0.17009014413091872\n",
      "Average test loss: 0.00582200261267523\n",
      "Epoch 285/300\n",
      "Average training loss: 0.17001089742448594\n",
      "Average test loss: 0.005837113334486882\n",
      "Epoch 286/300\n",
      "Average training loss: 0.16949893565972646\n",
      "Average test loss: 0.005877484863003095\n",
      "Epoch 287/300\n",
      "Average training loss: 0.16907196875413258\n",
      "Average test loss: 0.006894601398872004\n",
      "Epoch 288/300\n",
      "Average training loss: 0.16852294276158014\n",
      "Average test loss: 0.005985644432819552\n",
      "Epoch 289/300\n",
      "Average training loss: 0.16768084058496688\n",
      "Average test loss: 0.005909010603196091\n",
      "Epoch 290/300\n",
      "Average training loss: 0.16827341246604918\n",
      "Average test loss: 0.007413285747170448\n",
      "Epoch 291/300\n",
      "Average training loss: 0.16690325538979636\n",
      "Average test loss: 0.00601958055752847\n",
      "Epoch 292/300\n",
      "Average training loss: 0.16631852385732862\n",
      "Average test loss: 0.017070917651057244\n",
      "Epoch 293/300\n",
      "Average training loss: 0.16848937735292646\n",
      "Average test loss: 0.006021230735712581\n",
      "Epoch 294/300\n",
      "Average training loss: 0.16525296068191528\n",
      "Average test loss: 0.005928303374805384\n",
      "Epoch 295/300\n",
      "Average training loss: 0.16462061018413968\n",
      "Average test loss: 0.005852730735308594\n",
      "Epoch 296/300\n",
      "Average training loss: 0.16471731445524426\n",
      "Average test loss: 0.005838317286637094\n",
      "Epoch 297/300\n",
      "Average training loss: 2772906161.8846793\n",
      "Average test loss: 311.0334568397072\n",
      "Epoch 298/300\n",
      "Average training loss: 17.432302124023437\n",
      "Average test loss: 11924.332884262667\n",
      "Epoch 299/300\n",
      "Average training loss: 17.132394894070096\n",
      "Average test loss: 3251.71931837972\n",
      "Epoch 300/300\n",
      "Average training loss: 16.77487545776367\n",
      "Average test loss: 433.94147551997503\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.64621917851766\n",
      "Average test loss: 0.01096320625891288\n",
      "Epoch 2/300\n",
      "Average training loss: 2.514839965926276\n",
      "Average test loss: 0.008043759990897443\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4725601267284818\n",
      "Average test loss: 0.007256317324522469\n",
      "Epoch 4/300\n",
      "Average training loss: 0.9952376321686639\n",
      "Average test loss: 0.006594881277531385\n",
      "Epoch 5/300\n",
      "Average training loss: 0.7316068523195055\n",
      "Average test loss: 0.006231676133970419\n",
      "Epoch 6/300\n",
      "Average training loss: 0.5610390025244819\n",
      "Average test loss: 0.006210355538047023\n",
      "Epoch 7/300\n",
      "Average training loss: 0.4553354740407732\n",
      "Average test loss: 0.005994052117069562\n",
      "Epoch 8/300\n",
      "Average training loss: 0.38598652338981626\n",
      "Average test loss: 0.0059703138834900325\n",
      "Epoch 9/300\n",
      "Average training loss: 0.3364757737848494\n",
      "Average test loss: 0.005578195449378756\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2990412914355596\n",
      "Average test loss: 0.005396295085549355\n",
      "Epoch 11/300\n",
      "Average training loss: 0.2715689335664113\n",
      "Average test loss: 0.0056849789044095414\n",
      "Epoch 12/300\n",
      "Average training loss: 0.24995777299669053\n",
      "Average test loss: 0.00518330583969752\n",
      "Epoch 13/300\n",
      "Average training loss: 0.23436321822802225\n",
      "Average test loss: 0.005261612775425116\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2209897265036901\n",
      "Average test loss: 0.004768840193748474\n",
      "Epoch 15/300\n",
      "Average training loss: 0.21262673907809787\n",
      "Average test loss: 0.005196339669326941\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2053108761443032\n",
      "Average test loss: 0.004741084533226159\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1979981592098872\n",
      "Average test loss: 0.005457402043044567\n",
      "Epoch 18/300\n",
      "Average training loss: 0.19084193650881448\n",
      "Average test loss: 0.005135773941046662\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1861447423034244\n",
      "Average test loss: 0.004792306050658226\n",
      "Epoch 20/300\n",
      "Average training loss: 0.17955682643254597\n",
      "Average test loss: 0.00441105990173916\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1735812060435613\n",
      "Average test loss: 0.004286299234463109\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1700244349638621\n",
      "Average test loss: 0.004233795541442103\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16509767615795135\n",
      "Average test loss: 0.004131862326214711\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15965838883982764\n",
      "Average test loss: 0.004327182709756825\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1560706161260605\n",
      "Average test loss: 0.003866561892338925\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1512999288174841\n",
      "Average test loss: 0.003999950589198205\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14885444943772422\n",
      "Average test loss: 0.0038872937531107\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14502600114213096\n",
      "Average test loss: 0.00472374487378531\n",
      "Epoch 29/300\n",
      "Average training loss: 0.14128204394711388\n",
      "Average test loss: 0.0038075404287212424\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13892685467667049\n",
      "Average test loss: 0.004059229677336083\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13623620608117845\n",
      "Average test loss: 0.0037437025807383986\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13502055057552126\n",
      "Average test loss: 0.00391299046162102\n",
      "Epoch 33/300\n",
      "Average training loss: 0.13172479881180657\n",
      "Average test loss: 0.0037166956182983187\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12936837527487013\n",
      "Average test loss: 0.003783332933775253\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12871129727363587\n",
      "Average test loss: 0.0036349493997792403\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1276345427831014\n",
      "Average test loss: 0.00357719886302948\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12499998972813288\n",
      "Average test loss: 0.003556375395092699\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12357598544491662\n",
      "Average test loss: 0.004303729688127836\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12236717417505052\n",
      "Average test loss: 0.0037433949359175233\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12128721990187963\n",
      "Average test loss: 0.0035377909332099887\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12084030323558384\n",
      "Average test loss: 0.004632312224557003\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11946324235863155\n",
      "Average test loss: 0.003918518114421103\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1190866126815478\n",
      "Average test loss: 0.0035954956110152933\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11708969123495949\n",
      "Average test loss: 0.0038725873935553765\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11631264995866351\n",
      "Average test loss: 0.0034844560254779127\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11582890456252629\n",
      "Average test loss: 0.0035548441666695806\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11476977696021398\n",
      "Average test loss: 0.0035718540584461556\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11404469413227505\n",
      "Average test loss: 0.003682733065552182\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11360258150762982\n",
      "Average test loss: 0.003953360248770979\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1138187139497863\n",
      "Average test loss: 0.003575432077878051\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11162983126772774\n",
      "Average test loss: 0.004117729355684585\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11444670165909661\n",
      "Average test loss: 0.0034518222755028144\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11177516467703713\n",
      "Average test loss: 0.014651835175024138\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11003572132190069\n",
      "Average test loss: 0.0036876107714035444\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11721522045797772\n",
      "Average test loss: 0.02802674831615554\n",
      "Epoch 56/300\n",
      "Average training loss: 0.7950121771494547\n",
      "Average test loss: 0.0038667289097276\n",
      "Epoch 57/300\n",
      "Average training loss: 0.24427049197090997\n",
      "Average test loss: 0.0036907113935384485\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1850791857375039\n",
      "Average test loss: 0.0037035225079291396\n",
      "Epoch 59/300\n",
      "Average training loss: 0.16059113850858475\n",
      "Average test loss: 0.003566614729662736\n",
      "Epoch 60/300\n",
      "Average training loss: 0.1475399219526185\n",
      "Average test loss: 0.003512423208190335\n",
      "Epoch 61/300\n",
      "Average training loss: 0.13938249511520068\n",
      "Average test loss: 0.003489463496539328\n",
      "Epoch 62/300\n",
      "Average training loss: 0.13401379811763764\n",
      "Average test loss: 0.003466690127013458\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12988010408480963\n",
      "Average test loss: 0.00343391216525601\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12664046782917446\n",
      "Average test loss: 0.003569450011476874\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12422629410690732\n",
      "Average test loss: 0.003988419027378161\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12194361526436276\n",
      "Average test loss: 0.003409571473590202\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12054989219374126\n",
      "Average test loss: 0.0035685534748352236\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11905972798665365\n",
      "Average test loss: 0.003667175470954842\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11728071280320486\n",
      "Average test loss: 0.003608874267277618\n",
      "Epoch 70/300\n",
      "Average training loss: 0.15796029263072545\n",
      "Average test loss: 0.003409944223240018\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12274630470408333\n",
      "Average test loss: 0.003553914986136887\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11816069972515106\n",
      "Average test loss: 0.004305050148111251\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11635006812877126\n",
      "Average test loss: 0.0035901011344459323\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11471159235305256\n",
      "Average test loss: 0.003494328499254253\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11366071818272273\n",
      "Average test loss: 0.0036075081241627536\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11248161666260825\n",
      "Average test loss: 0.003384790939382381\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11217881665627162\n",
      "Average test loss: 0.003398604599138101\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11148486561907663\n",
      "Average test loss: 0.003401680572165383\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11111887676848306\n",
      "Average test loss: 0.0034309539610726967\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11015235841274261\n",
      "Average test loss: 0.0033906828148497475\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10889501551787059\n",
      "Average test loss: 0.0033590998664084406\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11145851276980506\n",
      "Average test loss: 0.0036139921720864045\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10857377348343532\n",
      "Average test loss: 0.0036456225477159024\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10762258701854283\n",
      "Average test loss: 0.0033439777518312138\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10775950654347738\n",
      "Average test loss: 0.0033882862120452853\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10730583399865362\n",
      "Average test loss: 0.0037292720560605326\n",
      "Epoch 87/300\n",
      "Average training loss: 0.1068913294341829\n",
      "Average test loss: 0.003416452327432732\n",
      "Epoch 88/300\n",
      "Average training loss: 0.106816440237893\n",
      "Average test loss: 0.006133588038798835\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1056565916604466\n",
      "Average test loss: 0.003522953451714582\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10551816853549745\n",
      "Average test loss: 0.003468030768757065\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10460646828677919\n",
      "Average test loss: 0.003360566722643044\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10529684240950478\n",
      "Average test loss: 0.003422766770960556\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10385725438594819\n",
      "Average test loss: 0.003422590377637082\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10335322030385335\n",
      "Average test loss: 1.342167913224962\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10407517100042767\n",
      "Average test loss: 0.003368701441006528\n",
      "Epoch 96/300\n",
      "Average training loss: 10.04442298090458\n",
      "Average test loss: 0.005478310856347283\n",
      "Epoch 97/300\n",
      "Average training loss: 3.4129093975490994\n",
      "Average test loss: 0.005073276852567991\n",
      "Epoch 98/300\n",
      "Average training loss: 2.285421987109714\n",
      "Average test loss: 0.004920301743265655\n",
      "Epoch 99/300\n",
      "Average training loss: 1.673014482498169\n",
      "Average test loss: 0.004266222116847833\n",
      "Epoch 100/300\n",
      "Average training loss: 1.2835421232647366\n",
      "Average test loss: 0.004194390326324436\n",
      "Epoch 101/300\n",
      "Average training loss: 0.9881576659944322\n",
      "Average test loss: 0.0050013663383821644\n",
      "Epoch 102/300\n",
      "Average training loss: 0.75398224512736\n",
      "Average test loss: 0.003877868253737688\n",
      "Epoch 103/300\n",
      "Average training loss: 0.585666082435184\n",
      "Average test loss: 0.0037862766049802303\n",
      "Epoch 104/300\n",
      "Average training loss: 0.4653308357662625\n",
      "Average test loss: 0.0037933223475184706\n",
      "Epoch 105/300\n",
      "Average training loss: 0.3774368210633596\n",
      "Average test loss: 0.0038371904862837657\n",
      "Epoch 106/300\n",
      "Average training loss: 0.3120579800340864\n",
      "Average test loss: 0.003541759948349661\n",
      "Epoch 107/300\n",
      "Average training loss: 0.26253175716929966\n",
      "Average test loss: 0.004905967711574501\n",
      "Epoch 108/300\n",
      "Average training loss: 0.22756177847915227\n",
      "Average test loss: 0.006329132789952887\n",
      "Epoch 109/300\n",
      "Average training loss: 0.2018196743329366\n",
      "Average test loss: 0.00352925933069653\n",
      "Epoch 110/300\n",
      "Average training loss: 0.18227962842252518\n",
      "Average test loss: 0.0034625502592987483\n",
      "Epoch 111/300\n",
      "Average training loss: 0.16753695911831326\n",
      "Average test loss: 0.0034437334479557142\n",
      "Epoch 112/300\n",
      "Average training loss: 0.15692907679080964\n",
      "Average test loss: 0.0034523141686287192\n",
      "Epoch 113/300\n",
      "Average training loss: 0.1487245004442003\n",
      "Average test loss: 0.024874334293107193\n",
      "Epoch 114/300\n",
      "Average training loss: 0.14166061067581176\n",
      "Average test loss: 0.003393806761337651\n",
      "Epoch 115/300\n",
      "Average training loss: 0.13587550061278872\n",
      "Average test loss: 0.0034407608531829385\n",
      "Epoch 116/300\n",
      "Average training loss: 0.13098983006344903\n",
      "Average test loss: 0.0037607527228279247\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1276501921084192\n",
      "Average test loss: 0.0034378903224650354\n",
      "Epoch 118/300\n",
      "Average training loss: 0.12487636050250796\n",
      "Average test loss: 0.005441619796885384\n",
      "Epoch 119/300\n",
      "Average training loss: 0.12221275959412257\n",
      "Average test loss: 0.004036104386258456\n",
      "Epoch 120/300\n",
      "Average training loss: 0.12020894575781292\n",
      "Average test loss: 0.0034481267848362526\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11854380407598283\n",
      "Average test loss: 0.0033641207110550668\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11696792056163152\n",
      "Average test loss: 0.0035600277628335688\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11526981966363059\n",
      "Average test loss: 0.0033770403704709477\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11381788593530655\n",
      "Average test loss: 0.0033147913361382154\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11252743431594636\n",
      "Average test loss: 0.0033735891085945897\n",
      "Epoch 126/300\n",
      "Average training loss: 0.11165395808882184\n",
      "Average test loss: 0.0033554219500058226\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11056115251117282\n",
      "Average test loss: 0.003736002614514695\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11485124948951933\n",
      "Average test loss: 0.0033268120934565864\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11014963806337781\n",
      "Average test loss: 0.0033808441315260197\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10885495772626665\n",
      "Average test loss: 0.003339297656176819\n",
      "Epoch 131/300\n",
      "Average training loss: 0.1072877995636728\n",
      "Average test loss: 0.0033336731897046167\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10754161340329382\n",
      "Average test loss: 0.003384509229618642\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10605493415726555\n",
      "Average test loss: 0.0038182288575917484\n",
      "Epoch 134/300\n",
      "Average training loss: 0.105625477804078\n",
      "Average test loss: 0.003348350684468945\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10525214817126592\n",
      "Average test loss: 0.003441072983977695\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10466140643755595\n",
      "Average test loss: 0.0035490870229485964\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10469465217987696\n",
      "Average test loss: 0.0033860177917199004\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11417862396770054\n",
      "Average test loss: 0.0033837112666418156\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10897395522726906\n",
      "Average test loss: 0.0034135238259202904\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10532106290260951\n",
      "Average test loss: 0.0034019019583033192\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1030706521736251\n",
      "Average test loss: 0.0035536430674708553\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1025878932873408\n",
      "Average test loss: 0.003391426020198398\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10210316621594959\n",
      "Average test loss: 0.003451766862637467\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10205396428373124\n",
      "Average test loss: 0.003657786256323258\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10158755581908756\n",
      "Average test loss: 0.0034022400797241266\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10124634904993905\n",
      "Average test loss: 0.003377901681491898\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10125329080555175\n",
      "Average test loss: 0.0033476923091544045\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10047575565179188\n",
      "Average test loss: 0.0033515317340691883\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10008832464615504\n",
      "Average test loss: 0.0033546368825352856\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10011055249638028\n",
      "Average test loss: 0.0034067399015443194\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09979727382130094\n",
      "Average test loss: 0.004497148776219951\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0992109079029825\n",
      "Average test loss: 0.003364739212104016\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0991068697836664\n",
      "Average test loss: 0.003402954896291097\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09839706377188365\n",
      "Average test loss: 0.00341396961423258\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0980788781510459\n",
      "Average test loss: 0.004275074988189671\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09821889709101783\n",
      "Average test loss: 0.003458728363323543\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09740610659122467\n",
      "Average test loss: 0.0036057959486626916\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09755175201098124\n",
      "Average test loss: 7.791810347239177\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09692720124456618\n",
      "Average test loss: 0.0035001153430591027\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09704949939250947\n",
      "Average test loss: 0.003419597168970439\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09648265257808897\n",
      "Average test loss: 0.0035000717838605247\n",
      "Epoch 162/300\n",
      "Average training loss: 12.95013557983107\n",
      "Average test loss: 2286973.9104246995\n",
      "Epoch 163/300\n",
      "Average training loss: 3.9010231554243298\n",
      "Average test loss: 350.24903051055844\n",
      "Epoch 164/300\n",
      "Average training loss: 2.7660567851596407\n",
      "Average test loss: 317.31203895822745\n",
      "Epoch 165/300\n",
      "Average training loss: 2.202239167107476\n",
      "Average test loss: 0.014125220484203763\n",
      "Epoch 166/300\n",
      "Average training loss: 1.8042645146052043\n",
      "Average test loss: 0.029859848169402944\n",
      "Epoch 167/300\n",
      "Average training loss: 1.495445869339837\n",
      "Average test loss: 0.3676512274179194\n",
      "Epoch 168/300\n",
      "Average training loss: 1.2199476312001547\n",
      "Average test loss: 0.006211349735036492\n",
      "Epoch 169/300\n",
      "Average training loss: 0.9957483718660143\n",
      "Average test loss: 0.003633093064443933\n",
      "Epoch 170/300\n",
      "Average training loss: 0.8256126339170667\n",
      "Average test loss: 0.007395579062816169\n",
      "Epoch 171/300\n",
      "Average training loss: 0.6928882728682624\n",
      "Average test loss: 0.0036218192560805215\n",
      "Epoch 172/300\n",
      "Average training loss: 0.5795833687782288\n",
      "Average test loss: 0.003581066600564453\n",
      "Epoch 173/300\n",
      "Average training loss: 0.47643129573927984\n",
      "Average test loss: 0.005078350779910882\n",
      "Epoch 174/300\n",
      "Average training loss: 0.39194665392239886\n",
      "Average test loss: 0.004049816189540757\n",
      "Epoch 175/300\n",
      "Average training loss: 0.32384060186809965\n",
      "Average test loss: 0.003426916436602672\n",
      "Epoch 176/300\n",
      "Average training loss: 0.2709283113479614\n",
      "Average test loss: 0.0033998877325405676\n",
      "Epoch 177/300\n",
      "Average training loss: 0.22914636347028944\n",
      "Average test loss: 0.0038320899409138493\n",
      "Epoch 178/300\n",
      "Average training loss: 0.19700939321517943\n",
      "Average test loss: 0.0034260329438580406\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1732737338675393\n",
      "Average test loss: 0.003352965904606713\n",
      "Epoch 180/300\n",
      "Average training loss: 0.15586241545942095\n",
      "Average test loss: 0.003350035533308983\n",
      "Epoch 181/300\n",
      "Average training loss: 0.14247789102130465\n",
      "Average test loss: 0.0033600623375839656\n",
      "Epoch 182/300\n",
      "Average training loss: 0.1327382664018207\n",
      "Average test loss: 0.0033330278388328024\n",
      "Epoch 183/300\n",
      "Average training loss: 0.12612472055355708\n",
      "Average test loss: 0.0033529316323498886\n",
      "Epoch 184/300\n",
      "Average training loss: 0.12179692511426078\n",
      "Average test loss: 0.0033066562480396694\n",
      "Epoch 185/300\n",
      "Average training loss: 0.11847570065657298\n",
      "Average test loss: 0.003513182036992576\n",
      "Epoch 186/300\n",
      "Average training loss: 0.11576636425654094\n",
      "Average test loss: 0.0033368118378437226\n",
      "Epoch 187/300\n",
      "Average training loss: 0.11371425661775801\n",
      "Average test loss: 0.0033737228251993654\n",
      "Epoch 188/300\n",
      "Average training loss: 0.11136321255233553\n",
      "Average test loss: 0.0033652180447760554\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10963248711824417\n",
      "Average test loss: 0.003321227889508009\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10789251599709193\n",
      "Average test loss: 0.003323377067844073\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10692433833413655\n",
      "Average test loss: 0.003345595924390687\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1053588021463818\n",
      "Average test loss: 0.003369286044190327\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1038127908375528\n",
      "Average test loss: 0.0035591875223649873\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10278929568661584\n",
      "Average test loss: 0.003372258381297191\n",
      "Epoch 195/300\n",
      "Average training loss: 0.10168610257572598\n",
      "Average test loss: 0.003373326357661022\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10069540515873167\n",
      "Average test loss: 0.003403797146967716\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0999946826895078\n",
      "Average test loss: 0.003402960876209868\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09862527994314829\n",
      "Average test loss: 0.0033892182879563834\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0980007790658209\n",
      "Average test loss: 0.0034758814552591907\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09815096153153313\n",
      "Average test loss: 0.0034263761107706362\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0971933440234926\n",
      "Average test loss: 0.0036042824142302077\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0967141170170572\n",
      "Average test loss: 0.0034780031595793037\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09640707931253645\n",
      "Average test loss: 0.003406832731432385\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09618346597750982\n",
      "Average test loss: 0.02772096945842107\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09773099717828962\n",
      "Average test loss: 0.0034291823061390054\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0955684941344791\n",
      "Average test loss: 0.003554947314783931\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09510183668136597\n",
      "Average test loss: 0.0035348854338129363\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09525636018647088\n",
      "Average test loss: 0.0034727141712274814\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09487364846467972\n",
      "Average test loss: 0.0034551755719714693\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09425259708695942\n",
      "Average test loss: 0.003479250721426474\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09402846208545897\n",
      "Average test loss: 0.007402802878783809\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09372183952066633\n",
      "Average test loss: 0.004472358229880531\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09729700833559037\n",
      "Average test loss: 0.0034566683202154106\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09402046572499805\n",
      "Average test loss: 0.003673442269364993\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09372770054472818\n",
      "Average test loss: 0.003731490993872285\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09269660683472951\n",
      "Average test loss: 0.00342987245031529\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09272828867038091\n",
      "Average test loss: 0.0051583365665541754\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09282906422350141\n",
      "Average test loss: 0.00350171045669251\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09213648471567366\n",
      "Average test loss: 0.0034381432249728176\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0927690304087268\n",
      "Average test loss: 0.003475024135990275\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09151840142409007\n",
      "Average test loss: 0.003937819282213846\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09233919215202331\n",
      "Average test loss: 0.003487034820020199\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09128039922648006\n",
      "Average test loss: 0.0044572946603099505\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09158722514576383\n",
      "Average test loss: 0.0034823540680938296\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0910621702671051\n",
      "Average test loss: 0.003420754521050387\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09102497050497267\n",
      "Average test loss: 0.003516148545468847\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09127281720108456\n",
      "Average test loss: 0.0034666698272857403\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09018651129139794\n",
      "Average test loss: 0.0034899721136316657\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09053276997804642\n",
      "Average test loss: 0.003728561647236347\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09074702715211444\n",
      "Average test loss: 0.0034922249687628615\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08956649911072519\n",
      "Average test loss: 0.0035480777443283136\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09016657591528363\n",
      "Average test loss: 0.003721683542451097\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0897550474802653\n",
      "Average test loss: 0.0042148715982006655\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08930957346492344\n",
      "Average test loss: 0.0036450577125781113\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09022863719198439\n",
      "Average test loss: 0.0035214051989217597\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09019596242904664\n",
      "Average test loss: 0.0034670146662327977\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08871972788704766\n",
      "Average test loss: 0.005133666263686286\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08843287889162699\n",
      "Average test loss: 0.0035174717158079146\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08878689438766903\n",
      "Average test loss: 0.0036222921452588506\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08821841493580077\n",
      "Average test loss: 0.0035890959973136584\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08867193212774065\n",
      "Average test loss: 0.0035248901471495626\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0882508930961291\n",
      "Average test loss: 0.003669671938651138\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0879890779654185\n",
      "Average test loss: 0.003521525123053127\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08775479340553284\n",
      "Average test loss: 0.0035525197020421424\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08755741324689653\n",
      "Average test loss: 0.0034978263751707143\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08777139783567853\n",
      "Average test loss: 0.011149063249842987\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08751853618356917\n",
      "Average test loss: 0.004630582239478827\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08747451341152192\n",
      "Average test loss: 0.003518072942478789\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08798786482546064\n",
      "Average test loss: 0.0036157706261922915\n",
      "Epoch 250/300\n",
      "Average training loss: 0.10490392494863934\n",
      "Average test loss: 0.003450709563162592\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08895981117751864\n",
      "Average test loss: 0.0034455118723627593\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08645842622386085\n",
      "Average test loss: 0.004034046937608057\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08624680025047726\n",
      "Average test loss: 0.0036637245772613417\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08619098573260837\n",
      "Average test loss: 0.003529770536555184\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08684560027387407\n",
      "Average test loss: 0.003607111207726929\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08609004280964533\n",
      "Average test loss: 0.004200859594262308\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0860056128402551\n",
      "Average test loss: 0.0038683715286768144\n",
      "Epoch 258/300\n",
      "Average training loss: 0.086111045744684\n",
      "Average test loss: 0.003620413399197989\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08605072357919481\n",
      "Average test loss: 0.01536510407510731\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08594763197501501\n",
      "Average test loss: 0.0035497089239458243\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0860329562889205\n",
      "Average test loss: 0.003529446269488997\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0861506971485085\n",
      "Average test loss: 0.0036389285810291766\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08535066186057197\n",
      "Average test loss: 0.0037007916938099597\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08657465486394035\n",
      "Average test loss: 0.003559926806638638\n",
      "Epoch 265/300\n",
      "Average training loss: 0.085027686311139\n",
      "Average test loss: 0.0034942033720306224\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08630261348353492\n",
      "Average test loss: 0.005883492387831211\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08505737502707375\n",
      "Average test loss: 0.003545628483924601\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08523794728517532\n",
      "Average test loss: 0.0039018127152489293\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08538309989372889\n",
      "Average test loss: 0.003514854509797361\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08478878033823437\n",
      "Average test loss: 0.0035864310334953998\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08500278438462151\n",
      "Average test loss: 0.003572100335939063\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08428281372785568\n",
      "Average test loss: 0.0034934133076005513\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08427136094040341\n",
      "Average test loss: 0.0035851709680217838\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08442518935600916\n",
      "Average test loss: 0.0035303446042040986\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08483181195788914\n",
      "Average test loss: 0.003949789547878835\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08391417682170868\n",
      "Average test loss: 0.0037522282149228784\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08418996179766125\n",
      "Average test loss: 0.004252721830167704\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08322830967108409\n",
      "Average test loss: 0.003514126271630327\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08435845053858228\n",
      "Average test loss: 0.0035730177226165932\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08470180991623137\n",
      "Average test loss: 0.0036118249841448334\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08319108280870649\n",
      "Average test loss: 0.004158770808329185\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08376580739021301\n",
      "Average test loss: 0.0035893195565376017\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08353412747714255\n",
      "Average test loss: 0.0036381114961372483\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08391414515177409\n",
      "Average test loss: 0.003646834612927503\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0828403119775984\n",
      "Average test loss: 0.003691621063070165\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08294546967082553\n",
      "Average test loss: 0.0035824907401369676\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08258846551511023\n",
      "Average test loss: 0.0043943180280427135\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08275572727786171\n",
      "Average test loss: 0.0036039677837656605\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08335209271642897\n",
      "Average test loss: 0.003567811249858803\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08274914424949222\n",
      "Average test loss: 0.0035740063695444\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08232439457707935\n",
      "Average test loss: 0.004054930446462499\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0827712946401702\n",
      "Average test loss: 0.003576727142350541\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08238660216993755\n",
      "Average test loss: 0.003714339876547456\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08248163101408217\n",
      "Average test loss: 0.0037699049682252935\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08238164936171638\n",
      "Average test loss: 0.0036340176477614376\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08209450346893735\n",
      "Average test loss: 0.0036548054044445357\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08305261352327135\n",
      "Average test loss: 0.003642514021653268\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08147761303186417\n",
      "Average test loss: 0.0035581662352714275\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08230757047070397\n",
      "Average test loss: 0.00367799078548948\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08169637172751956\n",
      "Average test loss: 0.0036512741049130755\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.884026310814752\n",
      "Average test loss: 0.009837921775049633\n",
      "Epoch 2/300\n",
      "Average training loss: 1.733005075454712\n",
      "Average test loss: 0.00738563216643201\n",
      "Epoch 3/300\n",
      "Average training loss: 1.0428685215314228\n",
      "Average test loss: 0.006457846309161849\n",
      "Epoch 4/300\n",
      "Average training loss: 0.711411448902554\n",
      "Average test loss: 0.005114365439862013\n",
      "Epoch 5/300\n",
      "Average training loss: 0.527986537138621\n",
      "Average test loss: 0.004750431785153018\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4200357868936327\n",
      "Average test loss: 0.004534734859234757\n",
      "Epoch 7/300\n",
      "Average training loss: 0.347957544459237\n",
      "Average test loss: 0.005144703541364935\n",
      "Epoch 8/300\n",
      "Average training loss: 0.29640992617607115\n",
      "Average test loss: 0.00425957198937734\n",
      "Epoch 9/300\n",
      "Average training loss: 0.25836869555049474\n",
      "Average test loss: 0.004080300871489776\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2297075992292828\n",
      "Average test loss: 0.003978968881691495\n",
      "Epoch 11/300\n",
      "Average training loss: 0.2077639131148656\n",
      "Average test loss: 0.003737309446144435\n",
      "Epoch 12/300\n",
      "Average training loss: 0.19121743698914845\n",
      "Average test loss: 0.003944057841681772\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1784957148498959\n",
      "Average test loss: 0.004383986011354459\n",
      "Epoch 14/300\n",
      "Average training loss: 0.16841068179739846\n",
      "Average test loss: 0.003428769280306167\n",
      "Epoch 15/300\n",
      "Average training loss: 0.15968109570609199\n",
      "Average test loss: 0.006188915097879039\n",
      "Epoch 16/300\n",
      "Average training loss: 0.15276099905702803\n",
      "Average test loss: 0.006185574648280939\n",
      "Epoch 17/300\n",
      "Average training loss: 0.14760891995165082\n",
      "Average test loss: 0.0034055543479820094\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1424927091134919\n",
      "Average test loss: 0.0030461011388235623\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13673318317201402\n",
      "Average test loss: 0.003140587247701155\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1328724002506998\n",
      "Average test loss: 0.0028526103993256886\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1279920347399182\n",
      "Average test loss: 0.0028219769156227507\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1227300618092219\n",
      "Average test loss: 0.0027604411399612823\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11887233279811012\n",
      "Average test loss: 0.002856910805321402\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1149838052590688\n",
      "Average test loss: 0.00296297627252837\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11145201356212298\n",
      "Average test loss: 0.0026201827271531025\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10797496171792348\n",
      "Average test loss: 0.002580446678524216\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10507975307438108\n",
      "Average test loss: 0.002576155449367232\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10207109424140719\n",
      "Average test loss: 0.00262156844459888\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09931917226976819\n",
      "Average test loss: 0.003092504336188237\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09703861764404509\n",
      "Average test loss: 0.0025850199916296534\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09479241640700234\n",
      "Average test loss: 0.0024408552375518614\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09401836630370881\n",
      "Average test loss: 0.0024467605041960873\n",
      "Epoch 33/300\n",
      "Average training loss: 0.091333534611596\n",
      "Average test loss: 0.00237547041537861\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08943872991535398\n",
      "Average test loss: 0.002973757123367654\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08915355514155494\n",
      "Average test loss: 0.002437149322591722\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0867429406841596\n",
      "Average test loss: 0.0024788129571825268\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08547282523579068\n",
      "Average test loss: 0.002426687652866046\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08774936217069626\n",
      "Average test loss: 0.0024701556871748634\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08370963148276012\n",
      "Average test loss: 0.002345759719817175\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08284999230172899\n",
      "Average test loss: 0.002414169169134564\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0818148917886946\n",
      "Average test loss: 0.0026020303724540606\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08114348211553361\n",
      "Average test loss: 0.0022768388278782367\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08224798385964499\n",
      "Average test loss: 0.002307876670733094\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07970267690552606\n",
      "Average test loss: 0.002338206785213616\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07968827076090707\n",
      "Average test loss: 0.002297186674964097\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08094880937867695\n",
      "Average test loss: 0.0023308911464280553\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07769220985306634\n",
      "Average test loss: 0.002281559202613102\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08147111861573325\n",
      "Average test loss: 0.0021991858324036002\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1483132913675573\n",
      "Average test loss: 0.0023560738991945983\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09741908980409304\n",
      "Average test loss: 0.002249566633047329\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08827903522716628\n",
      "Average test loss: 0.0022570221449972855\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08452092910475201\n",
      "Average test loss: 0.002233447711707817\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08243356051378779\n",
      "Average test loss: 0.0022672917752837142\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08090572634670469\n",
      "Average test loss: 0.002307293857447803\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08005029227998521\n",
      "Average test loss: 0.002256044570563568\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07936993749274147\n",
      "Average test loss: 0.0022058730487090846\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0779974162346787\n",
      "Average test loss: 0.002238622797032197\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0777175975839297\n",
      "Average test loss: 0.002213190337229106\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07677380539973577\n",
      "Average test loss: 0.002241848215357297\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07778283370865716\n",
      "Average test loss: 0.002212496430716581\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07578426580296622\n",
      "Average test loss: 0.002255136248552137\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07611923321750429\n",
      "Average test loss: 0.0022205210400538314\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0749655336605178\n",
      "Average test loss: 0.0022280827777253257\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07617753818962308\n",
      "Average test loss: 0.0021426005388299626\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07797919303509925\n",
      "Average test loss: 0.002217571293728219\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07419338593218061\n",
      "Average test loss: 0.002174503578080071\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07379414803120825\n",
      "Average test loss: 0.0022677344350765147\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0735453875594669\n",
      "Average test loss: 0.0023087137188348506\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07582938928074306\n",
      "Average test loss: 0.0023703049001180462\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07302105766534805\n",
      "Average test loss: 0.002551651368331578\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07267961318625345\n",
      "Average test loss: 0.002547254693383972\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07245205010639297\n",
      "Average test loss: 0.002343634525934855\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0723625455647707\n",
      "Average test loss: 0.0021871852963748907\n",
      "Epoch 74/300\n",
      "Average training loss: 0.073128219217062\n",
      "Average test loss: 0.0023110932478060324\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07101788632074992\n",
      "Average test loss: 0.002265765520847506\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07217840366562207\n",
      "Average test loss: 0.002173313187331789\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0715265455543995\n",
      "Average test loss: 0.09860531982779502\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07138607999682427\n",
      "Average test loss: 0.002131087629434963\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07099279060628678\n",
      "Average test loss: 0.002255073944520619\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07013954158624014\n",
      "Average test loss: 0.0021576363663706514\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06990506023168563\n",
      "Average test loss: 0.0041175400788585345\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06917844373650021\n",
      "Average test loss: 0.002143264510979255\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06961940191189447\n",
      "Average test loss: 0.0021553533147606586\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07171141229073207\n",
      "Average test loss: 0.0024182953211582367\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06866445964574813\n",
      "Average test loss: 0.0021533387299213147\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06848121247026655\n",
      "Average test loss: 0.004843453534361389\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0695817433198293\n",
      "Average test loss: 0.002220192046318617\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06886550481451882\n",
      "Average test loss: 0.002172675136787196\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06821251877148946\n",
      "Average test loss: 0.0022814998765372566\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06878839666313595\n",
      "Average test loss: 0.002160697614877588\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06702445245782535\n",
      "Average test loss: 0.0022200465803552004\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06762774748272365\n",
      "Average test loss: 0.003275459432353576\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06711924398938815\n",
      "Average test loss: 0.002157020737313562\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06750081720617082\n",
      "Average test loss: 0.0021994248888351853\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06699427759978506\n",
      "Average test loss: 0.0021362988827750085\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06634460499551562\n",
      "Average test loss: 0.0023279079171932408\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06614576735099156\n",
      "Average test loss: 0.0021439452287223605\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06613657452993922\n",
      "Average test loss: 1.0165988147954146\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06718131106098493\n",
      "Average test loss: 0.0025478320997208356\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06584049350685543\n",
      "Average test loss: 0.002153832456097007\n",
      "Epoch 101/300\n",
      "Average training loss: 0.065358991795116\n",
      "Average test loss: 0.0022443274073302745\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06591709932022624\n",
      "Average test loss: 0.002239064495596621\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06488103733128972\n",
      "Average test loss: 0.0022842222691203157\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06501416796776983\n",
      "Average test loss: 0.002157349831321173\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0640377959675259\n",
      "Average test loss: 0.002225881948446234\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0647523187233342\n",
      "Average test loss: 0.00240684784659081\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06474089838398828\n",
      "Average test loss: 0.0028198613532715374\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06461602635847198\n",
      "Average test loss: 0.0022528732407631146\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06395877152019076\n",
      "Average test loss: 0.0059906754259847934\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06441351773010359\n",
      "Average test loss: 0.002147738624467618\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06397534368104404\n",
      "Average test loss: 0.002289608234229187\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06332745205031501\n",
      "Average test loss: 0.00234277467822863\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06581516668862766\n",
      "Average test loss: 0.002171736623574462\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06271404632925988\n",
      "Average test loss: 0.0022318908367305996\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06265210557646221\n",
      "Average test loss: 0.0025890840244375997\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0631202637652556\n",
      "Average test loss: 0.002189284660336044\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06379426824384266\n",
      "Average test loss: 0.013074379882878727\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06229088652796216\n",
      "Average test loss: 0.002192649849793977\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0657081553141276\n",
      "Average test loss: 0.002186307748262253\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06165818252166112\n",
      "Average test loss: 0.002396152375990318\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06292508860760265\n",
      "Average test loss: 0.002876793900090787\n",
      "Epoch 122/300\n",
      "Average training loss: 0.061812317947546644\n",
      "Average test loss: 0.0023190409783273936\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06239028149180942\n",
      "Average test loss: 0.0023644746732380656\n",
      "Epoch 124/300\n",
      "Average training loss: 0.061593703587849935\n",
      "Average test loss: 0.0023317842663576204\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06180867505735821\n",
      "Average test loss: 0.013919094460705916\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06147231258617507\n",
      "Average test loss: 0.0022624609749764203\n",
      "Epoch 127/300\n",
      "Average training loss: 0.061412373122241765\n",
      "Average test loss: 0.0022821932942089108\n",
      "Epoch 128/300\n",
      "Average training loss: 0.061026363525125714\n",
      "Average test loss: 0.002254976297832198\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0615869190328651\n",
      "Average test loss: 0.0022246148126820724\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06086492941611343\n",
      "Average test loss: 0.00224075749837276\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06172990537352032\n",
      "Average test loss: 0.005899082915650474\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06036432013577885\n",
      "Average test loss: 0.0021937308872325553\n",
      "Epoch 133/300\n",
      "Average training loss: 0.061325970364941494\n",
      "Average test loss: 0.002227145074866712\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0604376219842169\n",
      "Average test loss: 0.0022467684700257246\n",
      "Epoch 135/300\n",
      "Average training loss: 0.060572629974948035\n",
      "Average test loss: 0.0022561905142954655\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06069245715936025\n",
      "Average test loss: 0.002219961818721559\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05990850956903564\n",
      "Average test loss: 0.0024195975955161784\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06006134557723999\n",
      "Average test loss: 0.002270969707518816\n",
      "Epoch 139/300\n",
      "Average training loss: 0.059669536176655026\n",
      "Average test loss: 0.0025005556108016105\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05975692425502671\n",
      "Average test loss: 0.002661588417366147\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05977971587247319\n",
      "Average test loss: 0.0022856313857353397\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05956433405809932\n",
      "Average test loss: 0.0025909674827837284\n",
      "Epoch 143/300\n",
      "Average training loss: 0.059728684998220864\n",
      "Average test loss: 0.002246097733784053\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06005238819453451\n",
      "Average test loss: 0.002991333333051039\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05966745223601659\n",
      "Average test loss: 0.0025121163291235765\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05881006917026308\n",
      "Average test loss: 0.0022773486071576677\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0594983633922206\n",
      "Average test loss: 0.0026070233426160284\n",
      "Epoch 148/300\n",
      "Average training loss: 0.058534336189428965\n",
      "Average test loss: 0.0022705027270648213\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05952813129292594\n",
      "Average test loss: 0.0024515447014321883\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0586306146548854\n",
      "Average test loss: 0.0023313632879613175\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05879822255174319\n",
      "Average test loss: 0.003334191156551242\n",
      "Epoch 152/300\n",
      "Average training loss: 0.058328745918141474\n",
      "Average test loss: 0.0023680286910384893\n",
      "Epoch 153/300\n",
      "Average training loss: 0.058290270729197395\n",
      "Average test loss: 0.0024207657407969234\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05843354921208488\n",
      "Average test loss: 0.0023587139944235484\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05818060972955492\n",
      "Average test loss: 0.002242010847147968\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05870137743155161\n",
      "Average test loss: 0.0022585570646656886\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05838588251173496\n",
      "Average test loss: 0.005528130298687352\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05788816300034523\n",
      "Average test loss: 0.0022921516413076056\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05823539951774809\n",
      "Average test loss: 0.002328637093046887\n",
      "Epoch 160/300\n",
      "Average training loss: 0.057615613195631236\n",
      "Average test loss: 0.002357641369932228\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05890636579857932\n",
      "Average test loss: 0.0029026219855166143\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05736975319186846\n",
      "Average test loss: 0.002285428648504118\n",
      "Epoch 163/300\n",
      "Average training loss: 0.057439933604664276\n",
      "Average test loss: 0.002274539100523624\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05796698883506987\n",
      "Average test loss: 0.014899838616036706\n",
      "Epoch 165/300\n",
      "Average training loss: 0.057329259746604494\n",
      "Average test loss: 0.0035142510218752754\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0580688792069753\n",
      "Average test loss: 0.0022477785588966475\n",
      "Epoch 167/300\n",
      "Average training loss: 0.057367444071504806\n",
      "Average test loss: 0.0022841737390392356\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05721019193530083\n",
      "Average test loss: 0.0022989993122302823\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05712538268831041\n",
      "Average test loss: 0.002757680549596747\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05675176874134276\n",
      "Average test loss: 0.0023615872949982685\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05710198717812697\n",
      "Average test loss: 0.002395488596624798\n",
      "Epoch 172/300\n",
      "Average training loss: 0.057057901740074156\n",
      "Average test loss: 0.0026868942091241477\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05679554009437561\n",
      "Average test loss: 0.0023165427835451233\n",
      "Epoch 174/300\n",
      "Average training loss: 0.057026724196142624\n",
      "Average test loss: 0.0030969905151675145\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0566505287554529\n",
      "Average test loss: 0.0024123546607378457\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05635767913526959\n",
      "Average test loss: 0.002861328024831083\n",
      "Epoch 177/300\n",
      "Average training loss: 0.056156376236014896\n",
      "Average test loss: 0.002311703648625149\n",
      "Epoch 178/300\n",
      "Average training loss: 0.056732264538606005\n",
      "Average test loss: 0.0023629201563696066\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05787108045816421\n",
      "Average test loss: 0.0022969113766319223\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05654439420501391\n",
      "Average test loss: 0.002389181272851096\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05613209355539746\n",
      "Average test loss: 0.002351875560047726\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05578566230667962\n",
      "Average test loss: 0.0023321502572960324\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05615684568881989\n",
      "Average test loss: 0.0030363825915588273\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05575233393907547\n",
      "Average test loss: 0.0023974749568021958\n",
      "Epoch 185/300\n",
      "Average training loss: 0.056266111307673984\n",
      "Average test loss: 0.002544144815765321\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05579942645629247\n",
      "Average test loss: 0.002346285517534448\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0566465106010437\n",
      "Average test loss: 0.002271187486127019\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05567297288444307\n",
      "Average test loss: 1.935217229730553\n",
      "Epoch 189/300\n",
      "Average training loss: 0.055695851849185095\n",
      "Average test loss: 0.002435628595865435\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05704453611705038\n",
      "Average test loss: 0.0023189388977156744\n",
      "Epoch 191/300\n",
      "Average training loss: 0.055539265851179756\n",
      "Average test loss: 0.0031293111668071814\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05520243407951461\n",
      "Average test loss: 0.0022789302891534236\n",
      "Epoch 193/300\n",
      "Average training loss: 0.055708931790457834\n",
      "Average test loss: 0.0023717441438800757\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05599472353193495\n",
      "Average test loss: 0.0023467902448028326\n",
      "Epoch 195/300\n",
      "Average training loss: 0.055323610580629776\n",
      "Average test loss: 0.002351564988079998\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05564622502856784\n",
      "Average test loss: 0.002556602316805058\n",
      "Epoch 197/300\n",
      "Average training loss: 0.055165875020954346\n",
      "Average test loss: 0.002348199038248923\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05503198163045777\n",
      "Average test loss: 0.0023331618468380635\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0550825166437361\n",
      "Average test loss: 0.0046679607671168116\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05576247841450903\n",
      "Average test loss: 0.0056290345434099436\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05518650650315814\n",
      "Average test loss: 0.011151669580075477\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05513414798180262\n",
      "Average test loss: 1317123751.8377779\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0558084768752257\n",
      "Average test loss: 0.005674179971218109\n",
      "Epoch 204/300\n",
      "Average training loss: 0.054991332074006395\n",
      "Average test loss: 300.1761302421798\n",
      "Epoch 205/300\n",
      "Average training loss: 0.055031641440259085\n",
      "Average test loss: 0.0023866376342872777\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05463889690240224\n",
      "Average test loss: 0.0025058483936720426\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05488891186979082\n",
      "Average test loss: 0.0026188269723206757\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05470043884714444\n",
      "Average test loss: 0.0025401486917916273\n",
      "Epoch 209/300\n",
      "Average training loss: 0.054478666332032945\n",
      "Average test loss: 0.0024403449677758747\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05536615969075097\n",
      "Average test loss: 0.002425813018033902\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05461226099729538\n",
      "Average test loss: 0.019035432499316003\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0551361754933993\n",
      "Average test loss: 0.0023201003157430223\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05411324328515265\n",
      "Average test loss: 0.0048180457237694\n",
      "Epoch 214/300\n",
      "Average training loss: 0.054549379375245836\n",
      "Average test loss: 0.0024782330863591696\n",
      "Epoch 215/300\n",
      "Average training loss: 0.054322273502747216\n",
      "Average test loss: 84.66759644583199\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05434356325202518\n",
      "Average test loss: 0.002424749965262082\n",
      "Epoch 217/300\n",
      "Average training loss: 0.054941853370931416\n",
      "Average test loss: 0.002484442762306167\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05417623175183932\n",
      "Average test loss: 0.0027858841973874304\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05459405791428354\n",
      "Average test loss: 0.0024698749616121254\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05380744616521729\n",
      "Average test loss: 0.002489358802429504\n",
      "Epoch 221/300\n",
      "Average training loss: 0.054394075360563066\n",
      "Average test loss: 0.0028420937766010564\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05397185946504275\n",
      "Average test loss: 0.022171421783665816\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05436507480012046\n",
      "Average test loss: 0.002410136019397113\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05371821481320593\n",
      "Average test loss: 0.002337433483865526\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05433788645598624\n",
      "Average test loss: 0.002486542664261328\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0538297247423066\n",
      "Average test loss: 0.002569492320012715\n",
      "Epoch 227/300\n",
      "Average training loss: 0.053525322682327695\n",
      "Average test loss: 0.0024039968229416344\n",
      "Epoch 228/300\n",
      "Average training loss: 0.053656027889913985\n",
      "Average test loss: 0.0033155981407811245\n",
      "Epoch 229/300\n",
      "Average training loss: 0.055151214785046046\n",
      "Average test loss: 0.0023466010373085736\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0533817736407121\n",
      "Average test loss: 0.002386224888265133\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05338911971780989\n",
      "Average test loss: 0.0024440665048443607\n",
      "Epoch 232/300\n",
      "Average training loss: 0.053430892172786924\n",
      "Average test loss: 0.002512380739260051\n",
      "Epoch 233/300\n",
      "Average training loss: 0.054589865148067476\n",
      "Average test loss: 0.002382021519045035\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0532941907412476\n",
      "Average test loss: 0.017342922649449772\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0539436824950907\n",
      "Average test loss: 0.006029281933688455\n",
      "Epoch 236/300\n",
      "Average training loss: 0.053097517589728034\n",
      "Average test loss: 0.002388024319584171\n",
      "Epoch 237/300\n",
      "Average training loss: 0.053055434925688635\n",
      "Average test loss: 0.0023815131150186062\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05332357348004977\n",
      "Average test loss: 0.0025055815336397953\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05305486501587762\n",
      "Average test loss: 0.028849283662107254\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05338373214006424\n",
      "Average test loss: 0.07748199042015605\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05311221875415908\n",
      "Average test loss: 0.002472581981163886\n",
      "Epoch 242/300\n",
      "Average training loss: 0.052908023566007614\n",
      "Average test loss: 0.0025077410098165273\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05315310427215364\n",
      "Average test loss: 0.002626549381348822\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05333349320292473\n",
      "Average test loss: 0.002401041731962727\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05289834767248895\n",
      "Average test loss: 0.0032646914906799794\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05360265839099884\n",
      "Average test loss: 0.0031102263029250832\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05292846984333462\n",
      "Average test loss: 0.0023632903041111097\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05359928324818611\n",
      "Average test loss: 0.00259361749721898\n",
      "Epoch 249/300\n",
      "Average training loss: 0.052644726223415796\n",
      "Average test loss: 0.0023415722402019635\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05288211517532666\n",
      "Average test loss: 0.010185216097368135\n",
      "Epoch 251/300\n",
      "Average training loss: 0.052731285164753595\n",
      "Average test loss: 0.002419811688363552\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05260057815247112\n",
      "Average test loss: 0.002600367926682035\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05271165262328254\n",
      "Average test loss: 0.002524003470109569\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05281442432933384\n",
      "Average test loss: 0.0024978451002389194\n",
      "Epoch 255/300\n",
      "Average training loss: 0.052713246014383104\n",
      "Average test loss: 0.002398935754576491\n",
      "Epoch 256/300\n",
      "Average training loss: 0.052689606133434506\n",
      "Average test loss: 0.00319478902945088\n",
      "Epoch 257/300\n",
      "Average training loss: 0.053273330632183286\n",
      "Average test loss: 0.00239368983813458\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05269538178708818\n",
      "Average test loss: 0.0023488042917516497\n",
      "Epoch 259/300\n",
      "Average training loss: 0.052678346637222505\n",
      "Average test loss: 0.0027207209672778843\n",
      "Epoch 260/300\n",
      "Average training loss: 0.052586935722165634\n",
      "Average test loss: 0.05768986653143333\n",
      "Epoch 261/300\n",
      "Average training loss: 0.052567479633622696\n",
      "Average test loss: 0.002435826448102792\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05207802257604069\n",
      "Average test loss: 0.0025444471972684065\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05252105521162351\n",
      "Average test loss: 0.0054096218448960115\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05241972262495094\n",
      "Average test loss: 0.002460848103794787\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05269835803243849\n",
      "Average test loss: 0.0024506210865866806\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05201561957597733\n",
      "Average test loss: 0.002420231926565369\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05207745471265581\n",
      "Average test loss: 0.0024221597022066515\n",
      "Epoch 268/300\n",
      "Average training loss: 0.053006145205762654\n",
      "Average test loss: 0.0030184624752857617\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05178525869051615\n",
      "Average test loss: 0.002486629291644527\n",
      "Epoch 270/300\n",
      "Average training loss: 0.052111576795578006\n",
      "Average test loss: 0.002546368268214994\n",
      "Epoch 271/300\n",
      "Average training loss: 0.052104497939348224\n",
      "Average test loss: 0.0024740282429589165\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05213221163882149\n",
      "Average test loss: 0.0029761770382109617\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05236293067865901\n",
      "Average test loss: 207030027350.4711\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05599446512262026\n",
      "Average test loss: 0.0025676755112492376\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05123326752252049\n",
      "Average test loss: 0.0023632593613324895\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05169185015559197\n",
      "Average test loss: 0.0023509173962391085\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05168117853336864\n",
      "Average test loss: 0.005789311647518641\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05218190224965413\n",
      "Average test loss: 0.003822677830648091\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05164589281876882\n",
      "Average test loss: 0.00400758641709884\n",
      "Epoch 280/300\n",
      "Average training loss: 0.051677959240145156\n",
      "Average test loss: 0.002367916751239035\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05192273971769545\n",
      "Average test loss: 4.094732732375463\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05161506069368786\n",
      "Average test loss: 0.0024130127924597926\n",
      "Epoch 283/300\n",
      "Average training loss: 0.051851048072179155\n",
      "Average test loss: 0.0024859051861696774\n",
      "Epoch 284/300\n",
      "Average training loss: 0.051503939885232185\n",
      "Average test loss: 0.0024638190012839104\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05203075584769249\n",
      "Average test loss: 0.0024574405903824504\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05244567105505202\n",
      "Average test loss: 0.002915778196002874\n",
      "Epoch 287/300\n",
      "Average training loss: 0.051502447373337215\n",
      "Average test loss: 0.0024679194071019687\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05146474912762642\n",
      "Average test loss: 0.0025676321972989374\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0513520373834504\n",
      "Average test loss: 0.0027604452211202847\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05154187102781402\n",
      "Average test loss: 0.0023855185329707132\n",
      "Epoch 291/300\n",
      "Average training loss: 0.051528624832630154\n",
      "Average test loss: 0.002459707891775502\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0514178752568033\n",
      "Average test loss: 0.002787778076922728\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05139463756812943\n",
      "Average test loss: 291330.04548611114\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05157440584566858\n",
      "Average test loss: 0.002708127970289853\n",
      "Epoch 295/300\n",
      "Average training loss: 0.050903165916601815\n",
      "Average test loss: 3890.301232204861\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05150550878047943\n",
      "Average test loss: 0.004007446651036541\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05144949551423391\n",
      "Average test loss: 0.0033454941685001054\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05109538441565302\n",
      "Average test loss: 0.07667157198488712\n",
      "Epoch 299/300\n",
      "Average training loss: 0.051277263879776\n",
      "Average test loss: 0.002472031814356645\n",
      "Epoch 300/300\n",
      "Average training loss: 0.051476793941524296\n",
      "Average test loss: 0.0036962632791449628\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.246690906736585\n",
      "Average test loss: 0.011657972073389424\n",
      "Epoch 2/300\n",
      "Average training loss: 2.2143574195437963\n",
      "Average test loss: 0.006280739477939076\n",
      "Epoch 3/300\n",
      "Average training loss: 1.3046419472164579\n",
      "Average test loss: 0.005063499983814028\n",
      "Epoch 4/300\n",
      "Average training loss: 0.9116783736546834\n",
      "Average test loss: 0.005034949814693795\n",
      "Epoch 5/300\n",
      "Average training loss: 0.6779023281733195\n",
      "Average test loss: 0.0041598811876028775\n",
      "Epoch 6/300\n",
      "Average training loss: 0.5273655512862735\n",
      "Average test loss: 0.004075597550099095\n",
      "Epoch 7/300\n",
      "Average training loss: 0.42204164388444687\n",
      "Average test loss: 0.003795059790627824\n",
      "Epoch 8/300\n",
      "Average training loss: 0.3454893140263028\n",
      "Average test loss: 0.0036319466970033115\n",
      "Epoch 9/300\n",
      "Average training loss: 0.28904659793112014\n",
      "Average test loss: 0.003511879939999845\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2480195139116711\n",
      "Average test loss: 0.003359257445981105\n",
      "Epoch 11/300\n",
      "Average training loss: 0.216960186375512\n",
      "Average test loss: 0.003937893690955307\n",
      "Epoch 12/300\n",
      "Average training loss: 0.19341796987586551\n",
      "Average test loss: 0.003060110744295849\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1750599833726883\n",
      "Average test loss: 0.002972379142832425\n",
      "Epoch 14/300\n",
      "Average training loss: 0.16145544424321917\n",
      "Average test loss: 0.003765521397607194\n",
      "Epoch 15/300\n",
      "Average training loss: 0.15010817574792437\n",
      "Average test loss: 0.002821959636691544\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1401931846804089\n",
      "Average test loss: 0.0037258625181598795\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13331617963314057\n",
      "Average test loss: 0.0027881046098967395\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12785968947410584\n",
      "Average test loss: 0.0026808927331326738\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12186748081445695\n",
      "Average test loss: 0.00266523123304877\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11784451415803697\n",
      "Average test loss: 0.002552287230475081\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11296373495790693\n",
      "Average test loss: 0.005338175509952837\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1093094540172153\n",
      "Average test loss: 0.0034053145926445723\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10599751085042954\n",
      "Average test loss: 0.003277982087805867\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1033710452583101\n",
      "Average test loss: 0.0022452476978715922\n",
      "Epoch 25/300\n",
      "Average training loss: 0.09837482166290283\n",
      "Average test loss: 0.0024084402428319056\n",
      "Epoch 26/300\n",
      "Average training loss: 0.09439527030785878\n",
      "Average test loss: 0.002776794202418791\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09262044637070761\n",
      "Average test loss: 0.0023452090500957435\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08891496229834027\n",
      "Average test loss: 0.002190566872867445\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08530397774113549\n",
      "Average test loss: 0.002656469103983707\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08317441821098327\n",
      "Average test loss: 0.0018925187193478148\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08164560338523653\n",
      "Average test loss: 0.0019956046735040015\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0785369630323516\n",
      "Average test loss: 0.0020726899937209154\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07625994691583846\n",
      "Average test loss: 0.001954103328494562\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07417640718486573\n",
      "Average test loss: 0.002348925059247348\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07220026781161626\n",
      "Average test loss: 0.0024562056841742662\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07255625571807225\n",
      "Average test loss: 0.002215238895267248\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06961665221055349\n",
      "Average test loss: 0.001737359630771809\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06839728499452273\n",
      "Average test loss: 0.0016692128657466835\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06765047231316566\n",
      "Average test loss: 0.001919453318334288\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06636866944697169\n",
      "Average test loss: 0.00173028511636787\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06618210558758841\n",
      "Average test loss: 0.0018406174747894208\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06450455894900693\n",
      "Average test loss: 0.0017014850630528396\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06417738762166765\n",
      "Average test loss: 0.0016287765570191873\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06272547039058474\n",
      "Average test loss: 0.0018280186045707928\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06250548377301958\n",
      "Average test loss: 0.00180901571477039\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06127092366748386\n",
      "Average test loss: 0.0017219971977174282\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06233211032218403\n",
      "Average test loss: 0.0016148184560653236\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06008668419718742\n",
      "Average test loss: 0.00160654435130871\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06022904469569524\n",
      "Average test loss: 0.0015751974196690653\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05889745423197746\n",
      "Average test loss: 0.0016084737501417597\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05893293577101495\n",
      "Average test loss: 0.0017158400455696715\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05718667565451728\n",
      "Average test loss: 0.0016112543335184454\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0566028109325303\n",
      "Average test loss: 0.0015581031218171119\n",
      "Epoch 54/300\n",
      "Average training loss: 0.056494950205087664\n",
      "Average test loss: 0.001533464790166666\n",
      "Epoch 55/300\n",
      "Average training loss: 0.058969849596420924\n",
      "Average test loss: 0.0016089432003080018\n",
      "Epoch 56/300\n",
      "Average training loss: 0.055618369615740244\n",
      "Average test loss: 0.001621990209341877\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0551924097471767\n",
      "Average test loss: 0.0016580994996345706\n",
      "Epoch 58/300\n",
      "Average training loss: 0.055240732577111984\n",
      "Average test loss: 0.002078850114511119\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05519745528201262\n",
      "Average test loss: 0.0015368342074669069\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0542499483525753\n",
      "Average test loss: 0.0015981226004660128\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0540130635201931\n",
      "Average test loss: 0.0015056553443169429\n",
      "Epoch 62/300\n",
      "Average training loss: 0.053955569254027475\n",
      "Average test loss: 0.001582363369046814\n",
      "Epoch 63/300\n",
      "Average training loss: 0.053337065524525114\n",
      "Average test loss: 0.0016975450773412983\n",
      "Epoch 64/300\n",
      "Average training loss: 0.054441645049386554\n",
      "Average test loss: 0.001870712065241403\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05228520012564129\n",
      "Average test loss: 0.0014848795786189537\n",
      "Epoch 66/300\n",
      "Average training loss: 0.052267236610253655\n",
      "Average test loss: 0.0026754170541341106\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05244312254256672\n",
      "Average test loss: 0.00148815389836414\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05204389199614525\n",
      "Average test loss: 0.0014775570782108438\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05207608266009225\n",
      "Average test loss: 0.008772046700119973\n",
      "Epoch 70/300\n",
      "Average training loss: 0.052839680009418064\n",
      "Average test loss: 0.0015790714422861734\n",
      "Epoch 71/300\n",
      "Average training loss: 0.050951009084781014\n",
      "Average test loss: 0.001486280829180032\n",
      "Epoch 72/300\n",
      "Average training loss: 0.050777701520257526\n",
      "Average test loss: 0.0015371765717864037\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05081919226050377\n",
      "Average test loss: 0.001534292506881886\n",
      "Epoch 74/300\n",
      "Average training loss: 0.050685623877578315\n",
      "Average test loss: 0.0015886012414056394\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05461292961902089\n",
      "Average test loss: 0.0017710557878017426\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05170263242059284\n",
      "Average test loss: 0.0015179763412516978\n",
      "Epoch 77/300\n",
      "Average training loss: 0.049876521286037236\n",
      "Average test loss: 0.0016134119182825088\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0499277057548364\n",
      "Average test loss: 0.0015756798380364974\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04934573502673043\n",
      "Average test loss: 0.001535946184148391\n",
      "Epoch 80/300\n",
      "Average training loss: 0.049775969985458586\n",
      "Average test loss: 0.0015560762419675788\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04901892459723685\n",
      "Average test loss: 0.0014828768117974202\n",
      "Epoch 82/300\n",
      "Average training loss: 0.048897093921899795\n",
      "Average test loss: 0.0014742409546549121\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05304444569349289\n",
      "Average test loss: 0.0014702281825658347\n",
      "Epoch 84/300\n",
      "Average training loss: 0.048485378199153474\n",
      "Average test loss: 0.0014781533912238147\n",
      "Epoch 85/300\n",
      "Average training loss: 0.048343458937274084\n",
      "Average test loss: 0.0017169140684935782\n",
      "Epoch 86/300\n",
      "Average training loss: 0.048393591791391376\n",
      "Average test loss: 0.001524491560852362\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04851002686222394\n",
      "Average test loss: 0.003538024886821707\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04822030741141902\n",
      "Average test loss: 0.0017121668183762166\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04761195654670398\n",
      "Average test loss: 0.0015137952342629433\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04814777777261204\n",
      "Average test loss: 0.0014975634348682232\n",
      "Epoch 91/300\n",
      "Average training loss: 0.047755272562305134\n",
      "Average test loss: 0.001921676202159789\n",
      "Epoch 92/300\n",
      "Average training loss: 0.048561229195859694\n",
      "Average test loss: 0.0014981301029523214\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04727579593492879\n",
      "Average test loss: 0.0016249918583780528\n",
      "Epoch 94/300\n",
      "Average training loss: 0.047354341215557524\n",
      "Average test loss: 0.0015773656786315972\n",
      "Epoch 95/300\n",
      "Average training loss: 0.047241589817735886\n",
      "Average test loss: 0.001503798683691356\n",
      "Epoch 96/300\n",
      "Average training loss: 0.047442773716317284\n",
      "Average test loss: 0.0028729196251887413\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04912353837158945\n",
      "Average test loss: 0.005862340848065085\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04976651012235218\n",
      "Average test loss: 0.001745782156371408\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04637194679180781\n",
      "Average test loss: 0.001499694293985764\n",
      "Epoch 100/300\n",
      "Average training loss: 0.046078052719434104\n",
      "Average test loss: 0.001671340582271417\n",
      "Epoch 101/300\n",
      "Average training loss: 0.045927909245093664\n",
      "Average test loss: 0.0016621734737418592\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04587458691000938\n",
      "Average test loss: 0.0018787419371720817\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04610282578070959\n",
      "Average test loss: 0.001616107010593017\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04558743717273076\n",
      "Average test loss: 0.001561347506629924\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04606596400671535\n",
      "Average test loss: 0.0037412515667577585\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04553093168470595\n",
      "Average test loss: 0.0015102182096905178\n",
      "Epoch 107/300\n",
      "Average training loss: 0.045197284118996726\n",
      "Average test loss: 0.0016294624602629078\n",
      "Epoch 108/300\n",
      "Average training loss: 0.045417475316259594\n",
      "Average test loss: 0.0016104457517051034\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04621727083126704\n",
      "Average test loss: 0.0016258710229562388\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04487909201780955\n",
      "Average test loss: 0.0017022220246079895\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04473872802986039\n",
      "Average test loss: 0.0018819767888635396\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04509854204787148\n",
      "Average test loss: 0.0015755876072992882\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0462356926103433\n",
      "Average test loss: 0.001569132755148328\n",
      "Epoch 114/300\n",
      "Average training loss: 0.044237376858790714\n",
      "Average test loss: 0.0019461767378573616\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04415115799837642\n",
      "Average test loss: 0.0020890286390980086\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04506505625446638\n",
      "Average test loss: 0.0015959223197359177\n",
      "Epoch 117/300\n",
      "Average training loss: 0.043981833073827956\n",
      "Average test loss: 0.001492937101982534\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04397487779458364\n",
      "Average test loss: 0.0015649062345425289\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04446007167630726\n",
      "Average test loss: 0.0016119866655725573\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04405435651871893\n",
      "Average test loss: 0.003351079248305824\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04395826804306772\n",
      "Average test loss: 0.001659235606694387\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04367415834466616\n",
      "Average test loss: 0.001604955968240069\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0437772557106283\n",
      "Average test loss: 0.0019941613184702067\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04368425671590699\n",
      "Average test loss: 0.0015134631198727422\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04431614249282413\n",
      "Average test loss: 0.0014789203683742219\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04316583079099655\n",
      "Average test loss: 0.0015335875454669198\n",
      "Epoch 127/300\n",
      "Average training loss: 0.043128470414214665\n",
      "Average test loss: 0.0016055172262713314\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04316908618476656\n",
      "Average test loss: 0.0015404902042614088\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04327518401874436\n",
      "Average test loss: 0.001921167048625648\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04279746969209777\n",
      "Average test loss: 0.007932347164385848\n",
      "Epoch 131/300\n",
      "Average training loss: 0.042771691279278864\n",
      "Average test loss: 0.003716411131227182\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04304503663380941\n",
      "Average test loss: 0.011309673051039379\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04273454427056842\n",
      "Average test loss: 0.0015444816957331366\n",
      "Epoch 134/300\n",
      "Average training loss: 0.044876128057638805\n",
      "Average test loss: 0.0015287303291778597\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04280870089762741\n",
      "Average test loss: 0.0015218940321356058\n",
      "Epoch 136/300\n",
      "Average training loss: 0.042496631973319585\n",
      "Average test loss: 0.001536345237141682\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04212044069502089\n",
      "Average test loss: 0.0015378985094527404\n",
      "Epoch 138/300\n",
      "Average training loss: 0.043036821438206564\n",
      "Average test loss: 0.0037666161060333253\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04243259229593807\n",
      "Average test loss: 0.006947707291278574\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04203340439663993\n",
      "Average test loss: 0.00680899210812317\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04268833691212866\n",
      "Average test loss: 0.0015228333127581411\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04182413493262397\n",
      "Average test loss: 0.0016529561060791215\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04169524597459369\n",
      "Average test loss: 0.0021496194255434804\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04197162738773558\n",
      "Average test loss: 0.0018442552294582128\n",
      "Epoch 145/300\n",
      "Average training loss: 0.042113525519768394\n",
      "Average test loss: 0.0016256407424807549\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04150116585360633\n",
      "Average test loss: 0.0015711414084459345\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04148485785391596\n",
      "Average test loss: 0.001841854421628846\n",
      "Epoch 148/300\n",
      "Average training loss: 0.042616618633270265\n",
      "Average test loss: 0.0020115378161685332\n",
      "Epoch 149/300\n",
      "Average training loss: 0.041683374189668235\n",
      "Average test loss: 0.0015424846707739763\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04130144347747167\n",
      "Average test loss: 0.0015548091680846281\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04908694177865982\n",
      "Average test loss: 0.0031312858648598193\n",
      "Epoch 152/300\n",
      "Average training loss: 0.042214135580592685\n",
      "Average test loss: 50.97251253085666\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04136221337649557\n",
      "Average test loss: 0.0024286093925022416\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0411317239801089\n",
      "Average test loss: 0.0015668017077777121\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04122502929303381\n",
      "Average test loss: 0.0015488118216809299\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0421164628714323\n",
      "Average test loss: 0.0016327769981904162\n",
      "Epoch 157/300\n",
      "Average training loss: 0.040862482269605004\n",
      "Average test loss: 0.0016208877235444055\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04124338036775589\n",
      "Average test loss: 488823343174.9973\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04121958871351348\n",
      "Average test loss: 0.0015876845925942892\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04067842740482754\n",
      "Average test loss: 0.0015657410761858853\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04114508757657475\n",
      "Average test loss: 0.00439737804647949\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04125352235635121\n",
      "Average test loss: 0.07534883151368962\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04425301622682148\n",
      "Average test loss: 0.0015956302916424142\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04055904219216771\n",
      "Average test loss: 0.0021688090132342443\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04030909197860294\n",
      "Average test loss: 0.0015444495033265816\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04053188751803504\n",
      "Average test loss: 0.0015673036422166559\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04032076668077045\n",
      "Average test loss: 0.10278793390923076\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04138233856360118\n",
      "Average test loss: 0.0017629820060812765\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0410797084139453\n",
      "Average test loss: 0.0015942463296362095\n",
      "Epoch 170/300\n",
      "Average training loss: 0.040530994456675316\n",
      "Average test loss: 0.001798281069844961\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04034407051404317\n",
      "Average test loss: 0.001644477818782131\n",
      "Epoch 172/300\n",
      "Average training loss: 0.039894130766391755\n",
      "Average test loss: 0.001609257630796896\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04229243859648704\n",
      "Average test loss: 0.0016145095015979475\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03999609641565217\n",
      "Average test loss: 0.004487414827156398\n",
      "Epoch 175/300\n",
      "Average training loss: 0.040198335458834965\n",
      "Average test loss: 0.001656262925101651\n",
      "Epoch 176/300\n",
      "Average training loss: 0.040362244096067215\n",
      "Average test loss: 233.2788277452257\n",
      "Epoch 177/300\n",
      "Average training loss: 0.040130619906716875\n",
      "Average test loss: 0.0015905801775451336\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040590885914034314\n",
      "Average test loss: 0.0016140446151710218\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04042256754802333\n",
      "Average test loss: 0.00206004892455207\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04005730809105767\n",
      "Average test loss: 0.025795777909457685\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04095118685728974\n",
      "Average test loss: 0.0016330646029673517\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04010789636936453\n",
      "Average test loss: 0.0019470829916083151\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03945130314926307\n",
      "Average test loss: 0.0016812496566627588\n",
      "Epoch 184/300\n",
      "Average training loss: 0.039762440770864486\n",
      "Average test loss: 0.04456472240471178\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03963711144526799\n",
      "Average test loss: 0.0016701608877629042\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040715365015798144\n",
      "Average test loss: 0.001556505753348271\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03946620376904805\n",
      "Average test loss: 0.0016262253092394934\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03952230192224185\n",
      "Average test loss: 0.0021906194135339722\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03942017122109731\n",
      "Average test loss: 0.001559340656424562\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03957331116000811\n",
      "Average test loss: 0.0016715468102031284\n",
      "Epoch 191/300\n",
      "Average training loss: 0.039318964888652164\n",
      "Average test loss: 0.0018429321157746017\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04078454409042994\n",
      "Average test loss: 0.0025002869408991602\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03930167112416691\n",
      "Average test loss: 0.005104267041716311\n",
      "Epoch 194/300\n",
      "Average training loss: 0.039094962947898444\n",
      "Average test loss: 0.0016769701583931843\n",
      "Epoch 195/300\n",
      "Average training loss: 0.040125031885173586\n",
      "Average test loss: 0.15078373846742843\n",
      "Epoch 196/300\n",
      "Average training loss: 0.039162554661432904\n",
      "Average test loss: 0.001805089388974011\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03912709356016583\n",
      "Average test loss: 0.0021321351726849872\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03926668701569239\n",
      "Average test loss: 0.0016374214434375365\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03964029461145401\n",
      "Average test loss: 0.0016258332234703832\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07226313878099123\n",
      "Average test loss: 0.0015933215819920103\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04805698582198885\n",
      "Average test loss: 0.004577703262368838\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04192016617457072\n",
      "Average test loss: 0.0016416552386557062\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04041318043073018\n",
      "Average test loss: 0.0016422994138879908\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03903311366505093\n",
      "Average test loss: 0.0018196621634480026\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03860530577434434\n",
      "Average test loss: 0.0015511805389283432\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03861636975738737\n",
      "Average test loss: 0.0016300593365190757\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0388329158657127\n",
      "Average test loss: 0.0015899259510139625\n",
      "Epoch 208/300\n",
      "Average training loss: 0.038593824840254254\n",
      "Average test loss: 0.001678851361076037\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03930737278858821\n",
      "Average test loss: 0.0016283045414214332\n",
      "Epoch 210/300\n",
      "Average training loss: 0.038550974696874615\n",
      "Average test loss: 0.0022009969922817414\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03864983673890432\n",
      "Average test loss: 0.001840123446037372\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03861931283109718\n",
      "Average test loss: 0.0017142485168038144\n",
      "Epoch 213/300\n",
      "Average training loss: 0.040353166782193715\n",
      "Average test loss: 0.0016477909065369103\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0398407170606984\n",
      "Average test loss: 0.0015886011374079518\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03854059422678418\n",
      "Average test loss: 0.001685230018571019\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0387227754758464\n",
      "Average test loss: 261.2410213860406\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03997864153153367\n",
      "Average test loss: 0.0017561910392509567\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03843841954072316\n",
      "Average test loss: 0.00220119164739218\n",
      "Epoch 219/300\n",
      "Average training loss: 0.038422118037939074\n",
      "Average test loss: 0.0020321942998820708\n",
      "Epoch 220/300\n",
      "Average training loss: 0.038530691216389336\n",
      "Average test loss: 22150.871860351563\n",
      "Epoch 221/300\n",
      "Average training loss: 0.039789851127399335\n",
      "Average test loss: 0.0017796694110665057\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0381894899573591\n",
      "Average test loss: 0.0016640175803461009\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03830566245317459\n",
      "Average test loss: 0.0018939959334416522\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03813457733392715\n",
      "Average test loss: 0.0018672886474264993\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03834603937466939\n",
      "Average test loss: 0.0020875744958304696\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03890485397643513\n",
      "Average test loss: 0.0020305805694725777\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03949700654215283\n",
      "Average test loss: 0.0016784550493790043\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03808928019801776\n",
      "Average test loss: 0.001612306263194316\n",
      "Epoch 229/300\n",
      "Average training loss: 0.038442154058151774\n",
      "Average test loss: 0.0016562769737922484\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03839551392197609\n",
      "Average test loss: 0.0016717568220984604\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03815850982401106\n",
      "Average test loss: 0.0015804575079431137\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04071093451976776\n",
      "Average test loss: 0.0016306758147871328\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03783304620451397\n",
      "Average test loss: 0.0016356022474873398\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03799424081047376\n",
      "Average test loss: 0.0016380309341475368\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03845547883543703\n",
      "Average test loss: 0.003810798423157798\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03793749176793628\n",
      "Average test loss: 0.0017125636568913858\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03804251598815123\n",
      "Average test loss: 0.0017514338347439965\n",
      "Epoch 238/300\n",
      "Average training loss: 0.038393278726273114\n",
      "Average test loss: 0.0023914199434220793\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03775334428085221\n",
      "Average test loss: 0.0021080657713011736\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03925109853016005\n",
      "Average test loss: 0.0018683973820880055\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03784654438495636\n",
      "Average test loss: 0.001708890099906259\n",
      "Epoch 242/300\n",
      "Average training loss: 0.037634290867381626\n",
      "Average test loss: 0.001760646507350935\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03795524223976665\n",
      "Average test loss: 0.001789358588349488\n",
      "Epoch 244/300\n",
      "Average training loss: 0.041228904919491875\n",
      "Average test loss: 0.0023977038134924238\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03770219186776214\n",
      "Average test loss: 0.001684182585010098\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03754782559805446\n",
      "Average test loss: 0.0019427757029318146\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04078516270716985\n",
      "Average test loss: 0.522782675123877\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03776991377605332\n",
      "Average test loss: 0.002757796851090259\n",
      "Epoch 249/300\n",
      "Average training loss: 0.037399068153566785\n",
      "Average test loss: 0.00411005062258078\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03776921010348532\n",
      "Average test loss: 0.001605163720332914\n",
      "Epoch 251/300\n",
      "Average training loss: 0.037622290680805845\n",
      "Average test loss: 0.0020204340156374707\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03755840779344241\n",
      "Average test loss: 0.0017552203773003486\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03944313442541493\n",
      "Average test loss: 0.0016716277252675758\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03741577639844682\n",
      "Average test loss: 0.03572785231967767\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03749396738409996\n",
      "Average test loss: 0.0018845146116283206\n",
      "Epoch 256/300\n",
      "Average training loss: 0.037840249215563136\n",
      "Average test loss: 0.0016717793984959524\n",
      "Epoch 257/300\n",
      "Average training loss: 0.037468952000141144\n",
      "Average test loss: 0.0016390749395100606\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03749394736687342\n",
      "Average test loss: 0.0016660948230160608\n",
      "Epoch 259/300\n",
      "Average training loss: 0.037778565608792836\n",
      "Average test loss: 0.0016935106220965584\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03740004904733764\n",
      "Average test loss: 0.0028774263461430866\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03816586199733946\n",
      "Average test loss: 0.0016892775237146351\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03700082136856185\n",
      "Average test loss: 0.0016502632041358286\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03738563800851504\n",
      "Average test loss: 0.001640902781031198\n",
      "Epoch 264/300\n",
      "Average training loss: 0.037623841260870296\n",
      "Average test loss: 0.001824777429509494\n",
      "Epoch 265/300\n",
      "Average training loss: 0.038685390022065905\n",
      "Average test loss: 0.001626611849396593\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03692956646614604\n",
      "Average test loss: 0.0018396396218902534\n",
      "Epoch 267/300\n",
      "Average training loss: 0.037314744684431286\n",
      "Average test loss: 0.0016918048664617041\n",
      "Epoch 268/300\n",
      "Average training loss: 0.038000725669993296\n",
      "Average test loss: 0.0018109430888046821\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03724744900729921\n",
      "Average test loss: 0.006495817815057105\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03726969661646419\n",
      "Average test loss: 0.0016080805922134055\n",
      "Epoch 271/300\n",
      "Average training loss: 0.039808037385344504\n",
      "Average test loss: 0.001733191135339439\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03675674237807592\n",
      "Average test loss: 0.0016662234341104825\n",
      "Epoch 273/300\n",
      "Average training loss: 0.038340310139788523\n",
      "Average test loss: 0.001736070791673329\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03699702882435587\n",
      "Average test loss: 0.0016416188293240136\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03687707518206702\n",
      "Average test loss: 0.0015986609442366494\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03717375093036228\n",
      "Average test loss: 0.0016814520343517263\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03804177519844638\n",
      "Average test loss: 0.002036086462852028\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03665358521209823\n",
      "Average test loss: 0.0018597136160565747\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03700229965150356\n",
      "Average test loss: 0.0016993140812135404\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03699641720453898\n",
      "Average test loss: 0.001629129006423884\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0368513966302077\n",
      "Average test loss: 0.001785085354724692\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03761696575416459\n",
      "Average test loss: 0.0016411902626148529\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03661088423596488\n",
      "Average test loss: 0.001763411444094446\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03760191495385435\n",
      "Average test loss: 0.06995045025149982\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03665336409873433\n",
      "Average test loss: 0.001755219267681241\n",
      "Epoch 286/300\n",
      "Average training loss: 0.037335745581322244\n",
      "Average test loss: 0.0025695719049415654\n",
      "Epoch 287/300\n",
      "Average training loss: 0.036590231557687126\n",
      "Average test loss: 0.0016394921131949458\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0367660102016396\n",
      "Average test loss: 0.002269135781460338\n",
      "Epoch 289/300\n",
      "Average training loss: 0.037018031461371316\n",
      "Average test loss: 0.001702356580644846\n",
      "Epoch 290/300\n",
      "Average training loss: 0.036550413058863744\n",
      "Average test loss: 0.001625453866707782\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03700531372096803\n",
      "Average test loss: 0.0016538636663721666\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03691524070666896\n",
      "Average test loss: 0.0019068275403438344\n",
      "Epoch 293/300\n",
      "Average training loss: 0.036714430789152784\n",
      "Average test loss: 0.014696659548415078\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03678505988253487\n",
      "Average test loss: 0.0016566428410717182\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03635001520315806\n",
      "Average test loss: 0.001682249018198086\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03721638930506176\n",
      "Average test loss: 0.0019373808935698536\n",
      "Epoch 297/300\n",
      "Average training loss: 0.036316838062471817\n",
      "Average test loss: 0.00167833313304517\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038110725512107216\n",
      "Average test loss: 0.0016618766552872128\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03630312829547458\n",
      "Average test loss: 0.0017076147424264086\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03654030478331778\n",
      "Average test loss: 0.001970110716815624\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Gauss_32_Depth3/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.01\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.73\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.01\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.33\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.44\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.68\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.72\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.02\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.01\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 25.85\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 25.95\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.07\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.32\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.39\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.63\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.86\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.86\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.31\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.91\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.76\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.29\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.05\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.59\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.41\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.63\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.21\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.35\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.56\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.81\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.98\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.26\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.74\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.38\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.45\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 33.12\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 33.08\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.99\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.79\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 33.02\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 33.13\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 33.07\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 33.27\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 33.32\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 33.47\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 33.35\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 33.45\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 33.53\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.64\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f0b6e-7868-4274-9824-5909b1afda2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
