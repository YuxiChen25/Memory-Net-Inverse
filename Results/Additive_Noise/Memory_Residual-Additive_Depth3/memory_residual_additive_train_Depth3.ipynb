{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.05)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.05)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.05)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.05)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.24540969877110588\n",
      "Average test loss: 0.01192673532002502\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0681650084455808\n",
      "Average test loss: 0.010338141134215726\n",
      "Epoch 3/300\n",
      "Average training loss: 0.061690603719817265\n",
      "Average test loss: 0.009675252297686206\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05821551741494073\n",
      "Average test loss: 0.009545150240676271\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05659246618217892\n",
      "Average test loss: 0.010357521711124314\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05482836539215512\n",
      "Average test loss: 0.00931094189484914\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05312696397304535\n",
      "Average test loss: 0.009195075285931428\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05205142753654056\n",
      "Average test loss: 0.009323555344508754\n",
      "Epoch 9/300\n",
      "Average training loss: 0.051076821323898104\n",
      "Average test loss: 0.008766685999102063\n",
      "Epoch 10/300\n",
      "Average training loss: 0.050441909074783325\n",
      "Average test loss: 0.009033389079074064\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04974671028720008\n",
      "Average test loss: 0.008576941953765021\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04916455766558647\n",
      "Average test loss: 0.009551750759283701\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04847737986511654\n",
      "Average test loss: 0.008393837232556607\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04803440580434269\n",
      "Average test loss: 0.008411244104305902\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0474051982694202\n",
      "Average test loss: 0.008460398408273857\n",
      "Epoch 16/300\n",
      "Average training loss: 0.046954607311222286\n",
      "Average test loss: 0.008336995490723186\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0464864963889122\n",
      "Average test loss: 0.008034756259785758\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04611064498954349\n",
      "Average test loss: 0.008212464947667386\n",
      "Epoch 19/300\n",
      "Average training loss: 0.045839590089188684\n",
      "Average test loss: 0.00787052588413159\n",
      "Epoch 20/300\n",
      "Average training loss: 0.045435899446407954\n",
      "Average test loss: 0.00793286298091213\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04508325795001454\n",
      "Average test loss: 0.007828529235389497\n",
      "Epoch 22/300\n",
      "Average training loss: 0.044802371346288256\n",
      "Average test loss: 0.007832392612679137\n",
      "Epoch 23/300\n",
      "Average training loss: 0.044455136232905916\n",
      "Average test loss: 0.007702224991387791\n",
      "Epoch 24/300\n",
      "Average training loss: 0.044260380301210614\n",
      "Average test loss: 0.008238989680177635\n",
      "Epoch 25/300\n",
      "Average training loss: 0.043924953990512425\n",
      "Average test loss: 0.007846547919842932\n",
      "Epoch 26/300\n",
      "Average training loss: 0.043773337761561074\n",
      "Average test loss: 0.007604798107511467\n",
      "Epoch 27/300\n",
      "Average training loss: 0.043509262210792966\n",
      "Average test loss: 0.007640020487209161\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04328399223420355\n",
      "Average test loss: 0.007478350051575237\n",
      "Epoch 29/300\n",
      "Average training loss: 0.043089204851124024\n",
      "Average test loss: 0.007643983396391074\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04292120646105872\n",
      "Average test loss: 0.007750119765185647\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04275799745652411\n",
      "Average test loss: 0.00745277814525697\n",
      "Epoch 32/300\n",
      "Average training loss: 0.042553226467635895\n",
      "Average test loss: 0.007368550687200493\n",
      "Epoch 33/300\n",
      "Average training loss: 0.042508481913142736\n",
      "Average test loss: 0.007339519571926858\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04221345891224013\n",
      "Average test loss: 0.007337060034688976\n",
      "Epoch 35/300\n",
      "Average training loss: 0.042086230036285185\n",
      "Average test loss: 0.00780010432501634\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04197806941138373\n",
      "Average test loss: 0.007333249338799053\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04179403656721115\n",
      "Average test loss: 0.007344271758778228\n",
      "Epoch 38/300\n",
      "Average training loss: 0.041700589249531426\n",
      "Average test loss: 0.0075402225226991705\n",
      "Epoch 39/300\n",
      "Average training loss: 0.041661861403120885\n",
      "Average test loss: 0.007233566157519817\n",
      "Epoch 40/300\n",
      "Average training loss: 0.041495232585403656\n",
      "Average test loss: 0.007193901406808032\n",
      "Epoch 41/300\n",
      "Average training loss: 0.041478625516096754\n",
      "Average test loss: 0.007199304145243433\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04123187784685029\n",
      "Average test loss: 0.007254251761568917\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04117259510689312\n",
      "Average test loss: 0.00713126511995991\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04108984317382177\n",
      "Average test loss: 0.007136681508686808\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04104958684576882\n",
      "Average test loss: 0.007506748854286141\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04082590834961997\n",
      "Average test loss: 0.0070922521588703\n",
      "Epoch 47/300\n",
      "Average training loss: 0.040842523409260645\n",
      "Average test loss: 0.007061255072967874\n",
      "Epoch 48/300\n",
      "Average training loss: 0.040745522283845474\n",
      "Average test loss: 0.007121991638094187\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04067351946896977\n",
      "Average test loss: 0.007054284411999914\n",
      "Epoch 50/300\n",
      "Average training loss: 0.040553215887811446\n",
      "Average test loss: 0.007068284478866392\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04048141682810254\n",
      "Average test loss: 0.007102941318518585\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04044176731838121\n",
      "Average test loss: 0.0070681294240057466\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04037021450532807\n",
      "Average test loss: 0.0070716308914124965\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04028753219379319\n",
      "Average test loss: 0.006980540143946806\n",
      "Epoch 55/300\n",
      "Average training loss: 0.040205044077502354\n",
      "Average test loss: 0.007072787230213483\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04019148355060154\n",
      "Average test loss: 0.006971261209083928\n",
      "Epoch 57/300\n",
      "Average training loss: 0.040118369109100764\n",
      "Average test loss: 0.00698227583989501\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04004720722966724\n",
      "Average test loss: 0.006991912300801939\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04000817192263073\n",
      "Average test loss: 0.006996190348019202\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03990794974896643\n",
      "Average test loss: 0.007045684648884667\n",
      "Epoch 61/300\n",
      "Average training loss: 0.039887107216649585\n",
      "Average test loss: 0.007064804972459872\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03985155453946855\n",
      "Average test loss: 0.006973896362715297\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03981890838344892\n",
      "Average test loss: 0.007002758069584767\n",
      "Epoch 64/300\n",
      "Average training loss: 0.039722785002655456\n",
      "Average test loss: 0.007030166020409928\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03968405561976963\n",
      "Average test loss: 0.006971432086494234\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03968814237581359\n",
      "Average test loss: 0.006989117881076204\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03963783436351352\n",
      "Average test loss: 0.006987523291260004\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03959973821706242\n",
      "Average test loss: 0.007072837627182404\n",
      "Epoch 69/300\n",
      "Average training loss: 0.039430490197406874\n",
      "Average test loss: 0.007339533612132072\n",
      "Epoch 72/300\n",
      "Average training loss: 0.039419848885801106\n",
      "Average test loss: 0.007033597110046281\n",
      "Epoch 73/300\n",
      "Average training loss: 0.039421127799484465\n",
      "Average test loss: 0.00702106498223212\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0393740567598078\n",
      "Average test loss: 0.006991538162032763\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03934140369958348\n",
      "Average test loss: 0.006970957311491171\n",
      "Epoch 76/300\n",
      "Average training loss: 0.039296043104595606\n",
      "Average test loss: 0.006906712642560402\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03925417437487178\n",
      "Average test loss: 0.007058576494869259\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039086142006847596\n",
      "Average test loss: 0.006989826619625092\n",
      "Epoch 84/300\n",
      "Average training loss: 0.039082658059067195\n",
      "Average test loss: 0.007205950424902969\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03907938001553218\n",
      "Average test loss: 0.006957918561167187\n",
      "Epoch 86/300\n",
      "Average training loss: 0.038994561539755924\n",
      "Average test loss: 0.006925470385700464\n",
      "Epoch 87/300\n",
      "Average training loss: 0.038986091481314766\n",
      "Average test loss: 0.007004996728565958\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03891874556740125\n",
      "Average test loss: 0.007066916898720794\n",
      "Epoch 89/300\n",
      "Average training loss: 0.038915778123670156\n",
      "Average test loss: 0.006999600320226616\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03893286493089464\n",
      "Average test loss: 0.006876043979078531\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03885154446297222\n",
      "Average test loss: 0.006996090199384425\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03882758988936742\n",
      "Average test loss: 0.006856086045917537\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03887685589989026\n",
      "Average test loss: 0.006888199930389722\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03885011814369096\n",
      "Average test loss: 0.007158426712370581\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0387792229950428\n",
      "Average test loss: 0.007098923919929398\n",
      "Epoch 96/300\n",
      "Average training loss: 0.038746974259614944\n",
      "Average test loss: 0.006865618968589438\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03873379066255358\n",
      "Average test loss: 0.006990561188922988\n",
      "Epoch 98/300\n",
      "Average training loss: 0.038773098107841276\n",
      "Average test loss: 0.0072440844592120916\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03876868462893698\n",
      "Average test loss: 0.006865114777038495\n",
      "Epoch 100/300\n",
      "Average training loss: 0.038661584195163516\n",
      "Average test loss: 0.007036795742809772\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0386460927642054\n",
      "Average test loss: 0.007112569977011945\n",
      "Epoch 102/300\n",
      "Average training loss: 0.038656330502695506\n",
      "Average test loss: 0.007051373613377412\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03857121799720658\n",
      "Average test loss: 0.006978075890491406\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03851176812913683\n",
      "Average test loss: 0.0069424407693247\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03851700630452898\n",
      "Average test loss: 0.006863995851741897\n",
      "Epoch 111/300\n",
      "Average training loss: 0.038479563085569275\n",
      "Average test loss: 0.0078100022243128885\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03841738560795784\n",
      "Average test loss: 0.0070042541772127155\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03846502966682116\n",
      "Average test loss: 0.006871740149954955\n",
      "Epoch 114/300\n",
      "Average training loss: 0.038419074012173544\n",
      "Average test loss: 0.007631968475878239\n",
      "Epoch 115/300\n",
      "Average training loss: 0.038393692639138964\n",
      "Average test loss: 0.0071355221975180835\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03841380240188705\n",
      "Average test loss: 0.00691113202398022\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03834708612494998\n",
      "Average test loss: 0.0069070329591631886\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03837269237968657\n",
      "Average test loss: 0.007646738193101353\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03833199943767653\n",
      "Average test loss: 0.006845343983007802\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03827409246563911\n",
      "Average test loss: 0.006865372761256165\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03832015266683367\n",
      "Average test loss: 0.007101723071601656\n",
      "Epoch 122/300\n",
      "Average training loss: 0.038298172877894505\n",
      "Average test loss: 0.006877965577567617\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03825173355142276\n",
      "Average test loss: 0.006869356592496236\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0382259963022338\n",
      "Average test loss: 0.007118825594998069\n",
      "Epoch 125/300\n",
      "Average training loss: 0.038241560346550414\n",
      "Average test loss: 0.007027804457479053\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03825056174397468\n",
      "Average test loss: 0.00702800867996282\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0382026952256759\n",
      "Average test loss: 0.006944613159530693\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03819220867090755\n",
      "Average test loss: 0.006840452451258898\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0382106717924277\n",
      "Average test loss: 0.006898444425728586\n",
      "Epoch 130/300\n",
      "Average training loss: 0.038143524295753906\n",
      "Average test loss: 0.006830183866123358\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03815497353010707\n",
      "Average test loss: 0.006960163228213787\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03813172048661444\n",
      "Average test loss: 0.006843337767653995\n",
      "Epoch 133/300\n",
      "Average training loss: 0.038130925218264264\n",
      "Average test loss: 0.006887207894689507\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03813327642612987\n",
      "Average test loss: 0.006972573297719161\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03815158371792899\n",
      "Average test loss: 0.007273263677954674\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03805077698495653\n",
      "Average test loss: 0.007147478450917536\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0381297704577446\n",
      "Average test loss: 0.0068244990681608515\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03799185452858607\n",
      "Average test loss: 0.006842725460314088\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03806063014931149\n",
      "Average test loss: 0.0070878361645672055\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03804298101034429\n",
      "Average test loss: 0.006986819913817777\n",
      "Epoch 141/300\n",
      "Average training loss: 0.038012396153476506\n",
      "Average test loss: 0.007529833271271652\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03794693322314156\n",
      "Average test loss: 0.006829733435064554\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03801430918441878\n",
      "Average test loss: 0.007250119334707658\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03798610294858615\n",
      "Average test loss: 0.006842075058155589\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03792088376813465\n",
      "Average test loss: 0.006880164970954259\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03792399615049362\n",
      "Average test loss: 0.006876038882467482\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03796296666065852\n",
      "Average test loss: 0.007055386802388562\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03794296111994319\n",
      "Average test loss: 0.0068480471207035916\n",
      "Epoch 149/300\n",
      "Average training loss: 0.037863404721021655\n",
      "Average test loss: 0.006895465635177162\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03787787474857436\n",
      "Average test loss: 0.0068702875061167614\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03787587243980831\n",
      "Average test loss: 0.006945009508066707\n",
      "Epoch 152/300\n",
      "Average training loss: 0.037852091881963944\n",
      "Average test loss: 0.006806789524853229\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03782400793830554\n",
      "Average test loss: 0.006912083080245389\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03783213580979241\n",
      "Average test loss: 0.00681182801309559\n",
      "Epoch 155/300\n",
      "Average training loss: 0.037848410202397244\n",
      "Average test loss: 0.00709521215616001\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03783796795540386\n",
      "Average test loss: 0.007061488244682551\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03783499421013726\n",
      "Average test loss: 0.0074828800728751555\n",
      "Epoch 158/300\n",
      "Average training loss: 0.037800527741511665\n",
      "Average test loss: 0.0068399471839269005\n",
      "Epoch 159/300\n",
      "Average training loss: 0.037830425335301295\n",
      "Average test loss: 0.006824223664071825\n",
      "Epoch 160/300\n",
      "Average training loss: 0.038219120469358235\n",
      "Average test loss: 0.006803415888299544\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03766013029548857\n",
      "Average test loss: 0.0068510081275469726\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03764539898104138\n",
      "Average test loss: 0.006793751589126057\n",
      "Epoch 169/300\n",
      "Average training loss: 0.037626594013637965\n",
      "Average test loss: 0.006819755633672079\n",
      "Epoch 170/300\n",
      "Average training loss: 0.037625041478210025\n",
      "Average test loss: 0.006828499173124631\n",
      "Epoch 171/300\n",
      "Average training loss: 0.037607394417126974\n",
      "Average test loss: 0.007023454069263405\n",
      "Epoch 172/300\n",
      "Average training loss: 0.037656441721651286\n",
      "Average test loss: 0.006945146292448044\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03763091087341309\n",
      "Average test loss: 0.007128175536791483\n",
      "Epoch 174/300\n",
      "Average training loss: 0.037594161401192344\n",
      "Average test loss: 0.006978109961996476\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03762798987494575\n",
      "Average test loss: 0.006831065982994106\n",
      "Epoch 176/300\n",
      "Average training loss: 0.037613923817873\n",
      "Average test loss: 0.006859270741542181\n",
      "Epoch 177/300\n",
      "Average training loss: 0.037538862350914214\n",
      "Average test loss: 0.0069745224279661976\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03759214876095454\n",
      "Average test loss: 0.006919945654769739\n",
      "Epoch 179/300\n",
      "Average training loss: 0.037535294420189325\n",
      "Average test loss: 0.006852571163740423\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03751647192902035\n",
      "Average test loss: 0.006927301950338814\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03751372139321433\n",
      "Average test loss: 0.006837297424674034\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03755634941988521\n",
      "Average test loss: 0.006871077790856361\n",
      "Epoch 183/300\n",
      "Average training loss: 0.037533502022425336\n",
      "Average test loss: 0.006876641476319896\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03754574109448327\n",
      "Average test loss: 0.006923934750258922\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0375010526643859\n",
      "Average test loss: 0.007493215368853675\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03746212818887499\n",
      "Average test loss: 0.006838224826587571\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03751028349995613\n",
      "Average test loss: 0.006845600670410527\n",
      "Epoch 188/300\n",
      "Average training loss: 0.037423788881964154\n",
      "Average test loss: 0.006824877663205067\n",
      "Epoch 189/300\n",
      "Average training loss: 0.037441193362077074\n",
      "Average test loss: 0.006817752055823803\n",
      "Epoch 190/300\n",
      "Average training loss: 0.037460889402363036\n",
      "Average test loss: 0.0068803378186292116\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03745626713501082\n",
      "Average test loss: 0.0068451709333393305\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03740215571721395\n",
      "Average test loss: 0.006829823525829447\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03747227452860938\n",
      "Average test loss: 0.007469531401164002\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03738132956458463\n",
      "Average test loss: 0.007041867624554369\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03740905227263769\n",
      "Average test loss: 0.006933595991382996\n",
      "Epoch 196/300\n",
      "Average training loss: 0.037389207621415455\n",
      "Average test loss: 0.007543358446823226\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03739887631932894\n",
      "Average test loss: 0.006831137749056021\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03739785382151604\n",
      "Average test loss: 0.006909242517418332\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03731825036803881\n",
      "Average test loss: 0.0068931946100460155\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03737774933874607\n",
      "Average test loss: 0.00691848924342129\n",
      "Epoch 201/300\n",
      "Average training loss: 0.037337604214747744\n",
      "Average test loss: 0.006904973687397109\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03740450792842441\n",
      "Average test loss: 0.006873645895057254\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0373504150973426\n",
      "Average test loss: 0.006847874643074142\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03734538206126955\n",
      "Average test loss: 0.006896155837923288\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03730714652604527\n",
      "Average test loss: 0.006872183074553807\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03729816355639034\n",
      "Average test loss: 0.0073743838601642185\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03734711589415868\n",
      "Average test loss: 0.006901708282944229\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03726260237230195\n",
      "Average test loss: 0.006978625638617409\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03731116699510151\n",
      "Average test loss: 0.007205000542104244\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03727030432555411\n",
      "Average test loss: 0.006849754998667372\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0372437447309494\n",
      "Average test loss: 0.006987228639423847\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03729024680124389\n",
      "Average test loss: 0.006823987374703089\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03723348135749499\n",
      "Average test loss: 0.006910857761485709\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03721812161803246\n",
      "Average test loss: 0.006964298761139314\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0372325840956635\n",
      "Average test loss: 0.006929467051393456\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03724634557631281\n",
      "Average test loss: 0.006959038767549727\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03728573325276375\n",
      "Average test loss: 0.006922929148293204\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0371475259926584\n",
      "Average test loss: 0.006849006529897452\n",
      "Epoch 219/300\n",
      "Average training loss: 0.037225231882598665\n",
      "Average test loss: 0.006850328280280034\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03715609241525332\n",
      "Average test loss: 0.00736558641658889\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03715787906448046\n",
      "Average test loss: 0.007073420518802272\n",
      "Epoch 222/300\n",
      "Average training loss: 0.037156528251038656\n",
      "Average test loss: 0.006975376637445556\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03715341337190734\n",
      "Average test loss: 0.006904278388453855\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0371264694167508\n",
      "Average test loss: 0.006912909011873934\n",
      "Epoch 225/300\n",
      "Average training loss: 0.037202562093734744\n",
      "Average test loss: 0.006988236728641722\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0371277284555965\n",
      "Average test loss: 0.006843277976330784\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03707969997326533\n",
      "Average test loss: 0.006973899619860781\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03714613155192799\n",
      "Average test loss: 0.006896762804024749\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0371012106206682\n",
      "Average test loss: 0.006951764503700866\n",
      "Epoch 230/300\n",
      "Average training loss: 0.037107647481891846\n",
      "Average test loss: 0.006807550757709477\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03714598516623179\n",
      "Average test loss: 0.007069631994598442\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0370607739786307\n",
      "Average test loss: 0.006975258343749576\n",
      "Epoch 233/300\n",
      "Average training loss: 0.037083330220646325\n",
      "Average test loss: 0.0068275578262077434\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03705689468317562\n",
      "Average test loss: 0.006885581433773041\n",
      "Epoch 235/300\n",
      "Average training loss: 0.037090106788608766\n",
      "Average test loss: 0.006861004702332947\n",
      "Epoch 236/300\n",
      "Average training loss: 0.037056027697192295\n",
      "Average test loss: 0.697817904525333\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03707605551679929\n",
      "Average test loss: 0.006836295320755905\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03705131700966093\n",
      "Average test loss: 0.006917398603426086\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03704707662595643\n",
      "Average test loss: 0.0069340855023927155\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03703079800804456\n",
      "Average test loss: 0.006933622940133015\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03703270917799738\n",
      "Average test loss: 0.0068893648990326455\n",
      "Epoch 242/300\n",
      "Average training loss: 0.036989575001928544\n",
      "Average test loss: 0.006939408615645435\n",
      "Epoch 243/300\n",
      "Average training loss: 0.037050109717581006\n",
      "Average test loss: 0.007029290780838993\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03705250396993425\n",
      "Average test loss: 0.006881709273904562\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03696098687913683\n",
      "Average test loss: 0.006979217756539583\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03698820790979597\n",
      "Average test loss: 0.006860389384958479\n",
      "Epoch 247/300\n",
      "Average training loss: 0.037007310764657124\n",
      "Average test loss: 0.006901295918143458\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03696217486924595\n",
      "Average test loss: 0.006867439092033439\n",
      "Epoch 249/300\n",
      "Average training loss: 0.036998398466242685\n",
      "Average test loss: 0.007150458236121469\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03695630241268211\n",
      "Average test loss: 0.006866648725751373\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03696028071310785\n",
      "Average test loss: 0.006842191338125202\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03694075368510352\n",
      "Average test loss: 0.006837986641046074\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03692219220929675\n",
      "Average test loss: 0.006900116973453098\n",
      "Epoch 254/300\n",
      "Average training loss: 0.036938071207867726\n",
      "Average test loss: 0.006948270151184665\n",
      "Epoch 255/300\n",
      "Average training loss: 0.036902250124348536\n",
      "Average test loss: 0.006857164354374012\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03687231413192219\n",
      "Average test loss: 0.006887437231838703\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03701422860225042\n",
      "Average test loss: 0.006936979766521189\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0369212668273184\n",
      "Average test loss: 0.006878644468883673\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0368935689760579\n",
      "Average test loss: 0.006912883366975519\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03690679173668226\n",
      "Average test loss: 0.007119214960063497\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03687050355143017\n",
      "Average test loss: 0.007179742742743757\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03685110569993655\n",
      "Average test loss: 0.006928610143562158\n",
      "Epoch 263/300\n",
      "Average training loss: 0.036852211740281846\n",
      "Average test loss: 0.007039471359716522\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03684373895658387\n",
      "Average test loss: 0.006878637794819143\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03683684253030353\n",
      "Average test loss: 0.0069046542309224605\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03682821023133066\n",
      "Average test loss: 0.011260166859461202\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03684821037782563\n",
      "Average test loss: 0.00693582270708349\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03685756330688794\n",
      "Average test loss: 0.006952638948957125\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03679043694999483\n",
      "Average test loss: 0.006942932395471467\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03687154013249609\n",
      "Average test loss: 0.0069192034713923935\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0367503439138333\n",
      "Average test loss: 0.006897527564730909\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03677746464477645\n",
      "Average test loss: 0.006872286652525266\n",
      "Epoch 273/300\n",
      "Average training loss: 0.036803570217556425\n",
      "Average test loss: 0.0070189756713807585\n",
      "Epoch 274/300\n",
      "Average training loss: 0.036830977569023766\n",
      "Average test loss: 0.006938272387617165\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0367737086713314\n",
      "Average test loss: 0.006977297695974509\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03678757459587521\n",
      "Average test loss: 0.007006234182251824\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03676385435130861\n",
      "Average test loss: 0.006927138421891464\n",
      "Epoch 278/300\n",
      "Average training loss: 0.036765044473939475\n",
      "Average test loss: 0.007041303643335899\n",
      "Epoch 279/300\n",
      "Average training loss: 0.036784140162997775\n",
      "Average test loss: 0.0069724002443253994\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03677518253856235\n",
      "Average test loss: 0.006910437155928877\n",
      "Epoch 281/300\n",
      "Average training loss: 0.036757816241847145\n",
      "Average test loss: 0.014190929215815332\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03675383544299338\n",
      "Average test loss: 0.006918543962554799\n",
      "Epoch 283/300\n",
      "Average training loss: 0.036690196148223345\n",
      "Average test loss: 0.007023575573745701\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03671698471903801\n",
      "Average test loss: 0.007008881381402413\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03669786920481258\n",
      "Average test loss: 0.006856557975212733\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03670628768205643\n",
      "Average test loss: 0.006916935200492541\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03668820789125231\n",
      "Average test loss: 0.006921658487369617\n",
      "Epoch 288/300\n",
      "Average training loss: 0.036681157843934165\n",
      "Average test loss: 0.006942926218940152\n",
      "Epoch 289/300\n",
      "Average training loss: 0.036724066212773326\n",
      "Average test loss: 0.007059698200060261\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03673364217744933\n",
      "Average test loss: 0.006997772097587586\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03668034824066692\n",
      "Average test loss: 0.007009911137736506\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03669107920262549\n",
      "Average test loss: 0.00701926798083716\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03669311375916004\n",
      "Average test loss: 0.007087465382284588\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03663707771235042\n",
      "Average test loss: 0.0069362192323638334\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03665839954879549\n",
      "Average test loss: 0.007013803723785612\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03666153952148225\n",
      "Average test loss: 0.007075368592722548\n",
      "Epoch 297/300\n",
      "Average training loss: 0.036640639980634056\n",
      "Average test loss: 0.007055697972989745\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03663529891437954\n",
      "Average test loss: 0.007020478549102942\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03666331979797946\n",
      "Average test loss: 0.006963753593464693\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03659650413526429\n",
      "Average test loss: 0.0068796737384465\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2113426703678237\n",
      "Average test loss: 0.008211353309038612\n",
      "Epoch 2/300\n",
      "Average training loss: 0.055416465169853636\n",
      "Average test loss: 0.007434894109765689\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04893221098515722\n",
      "Average test loss: 0.007011401790711615\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04594195048676597\n",
      "Average test loss: 0.007351257278687424\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04297150188353326\n",
      "Average test loss: 0.006586241569370032\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04163671300146315\n",
      "Average test loss: 0.006344420961207814\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03990733742713928\n",
      "Average test loss: 0.006321862705879741\n",
      "Epoch 8/300\n",
      "Average training loss: 0.038813534984985985\n",
      "Average test loss: 0.00616145082604554\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03781798786421617\n",
      "Average test loss: 0.006220758953442177\n",
      "Epoch 10/300\n",
      "Average training loss: 0.036927026612891094\n",
      "Average test loss: 0.006051293971637885\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03618023318052292\n",
      "Average test loss: 0.00587060173932049\n",
      "Epoch 12/300\n",
      "Average training loss: 0.035575468589862186\n",
      "Average test loss: 0.005934627423269881\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03506167846586969\n",
      "Average test loss: 0.005789680601408084\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03442909040715959\n",
      "Average test loss: 0.005470757239394718\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0339536933335993\n",
      "Average test loss: 0.005499342131531901\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03356952079468303\n",
      "Average test loss: 0.005366849129812585\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03317598241567612\n",
      "Average test loss: 0.005348480263931883\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03281276751061281\n",
      "Average test loss: 0.00535020497151547\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03258110560973485\n",
      "Average test loss: 0.005412377466758093\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03220797448356946\n",
      "Average test loss: 0.005265251283844312\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03190033861663606\n",
      "Average test loss: 0.00509083925291068\n",
      "Epoch 22/300\n",
      "Average training loss: 0.031657642079724206\n",
      "Average test loss: 0.005068420759505696\n",
      "Epoch 23/300\n",
      "Average training loss: 0.031483284340964424\n",
      "Average test loss: 0.005050306207603879\n",
      "Epoch 24/300\n",
      "Average training loss: 0.031195068498452504\n",
      "Average test loss: 0.005137441810634401\n",
      "Epoch 25/300\n",
      "Average training loss: 0.030998443547222348\n",
      "Average test loss: 0.004922827329072688\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03078986852367719\n",
      "Average test loss: 0.0049054889898333285\n",
      "Epoch 27/300\n",
      "Average training loss: 0.030702536770039136\n",
      "Average test loss: 0.00496258489208089\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03044869546757804\n",
      "Average test loss: 0.004888412651502424\n",
      "Epoch 29/300\n",
      "Average training loss: 0.030275011216600735\n",
      "Average test loss: 0.004842039936946498\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03017984252505832\n",
      "Average test loss: 0.0051762164951198635\n",
      "Epoch 31/300\n",
      "Average training loss: 0.030023999551932017\n",
      "Average test loss: 0.004770571422245767\n",
      "Epoch 32/300\n",
      "Average training loss: 0.029919032567077213\n",
      "Average test loss: 0.004800219089620643\n",
      "Epoch 33/300\n",
      "Average training loss: 0.029839553341269493\n",
      "Average test loss: 0.004756741229030821\n",
      "Epoch 34/300\n",
      "Average training loss: 0.029706024186478722\n",
      "Average test loss: 0.004687401270286905\n",
      "Epoch 35/300\n",
      "Average training loss: 0.029570895792709457\n",
      "Average test loss: 0.004804995431254308\n",
      "Epoch 36/300\n",
      "Average training loss: 0.029486297246482638\n",
      "Average test loss: 0.004693064748826954\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02936864638990826\n",
      "Average test loss: 0.004702450278939473\n",
      "Epoch 38/300\n",
      "Average training loss: 0.029306001045637662\n",
      "Average test loss: 0.004648946411700713\n",
      "Epoch 39/300\n",
      "Average training loss: 0.029223997284968693\n",
      "Average test loss: 0.004632296868910392\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02915747039847904\n",
      "Average test loss: 0.0046290130008839895\n",
      "Epoch 41/300\n",
      "Average training loss: 0.029105415541264747\n",
      "Average test loss: 0.00507702970960074\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02901095978750123\n",
      "Average test loss: 0.004620123784161276\n",
      "Epoch 43/300\n",
      "Average training loss: 0.028952460694644185\n",
      "Average test loss: 0.004617299776110384\n",
      "Epoch 44/300\n",
      "Average training loss: 0.028913100502557226\n",
      "Average test loss: 0.004623787948654758\n",
      "Epoch 45/300\n",
      "Average training loss: 0.028805138443907103\n",
      "Average test loss: 0.0045749608754283855\n",
      "Epoch 46/300\n",
      "Average training loss: 0.028820309879051313\n",
      "Average test loss: 0.004740906741056177\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02875459063715405\n",
      "Average test loss: 0.004613230673803224\n",
      "Epoch 48/300\n",
      "Average training loss: 0.028741190582513808\n",
      "Average test loss: 0.004609130019115077\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02861114969021744\n",
      "Average test loss: 0.0046013352661910985\n",
      "Epoch 50/300\n",
      "Average training loss: 0.028643822252750396\n",
      "Average test loss: 0.004545037909928295\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02852313063542048\n",
      "Average test loss: 0.004633868508454826\n",
      "Epoch 52/300\n",
      "Average training loss: 0.028536986546383963\n",
      "Average test loss: 0.004679823714825842\n",
      "Epoch 53/300\n",
      "Average training loss: 0.028482865558730232\n",
      "Average test loss: 0.004542268803550137\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02843196628159947\n",
      "Average test loss: 0.004512017872598436\n",
      "Epoch 55/300\n",
      "Average training loss: 0.028401296084125838\n",
      "Average test loss: 0.004556331478887134\n",
      "Epoch 56/300\n",
      "Average training loss: 0.028349123006065687\n",
      "Average test loss: 0.004532768722623586\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02835836804740959\n",
      "Average test loss: 0.004668307798604171\n",
      "Epoch 58/300\n",
      "Average training loss: 0.028306844032473036\n",
      "Average test loss: 0.00459175328993135\n",
      "Epoch 59/300\n",
      "Average training loss: 0.028292882182531887\n",
      "Average test loss: 0.004509112806783782\n",
      "Epoch 60/300\n",
      "Average training loss: 0.028236354996760686\n",
      "Average test loss: 0.0046017529695398275\n",
      "Epoch 61/300\n",
      "Average training loss: 0.028213531576924855\n",
      "Average test loss: 0.004625278824733363\n",
      "Epoch 62/300\n",
      "Average training loss: 0.028168070061339274\n",
      "Average test loss: 0.0045360232003861\n",
      "Epoch 63/300\n",
      "Average training loss: 0.028141215208503934\n",
      "Average test loss: 0.004536252229577965\n",
      "Epoch 64/300\n",
      "Average training loss: 0.028142401845918763\n",
      "Average test loss: 0.004477881406744321\n",
      "Epoch 65/300\n",
      "Average training loss: 0.028086836291684045\n",
      "Average test loss: 0.004507257702036037\n",
      "Epoch 66/300\n",
      "Average training loss: 0.028138304067982567\n",
      "Average test loss: 0.09011672998799218\n",
      "Epoch 67/300\n",
      "Average training loss: 0.028433639627363946\n",
      "Average test loss: 0.0045429401608804865\n",
      "Epoch 68/300\n",
      "Average training loss: 0.028049809177716573\n",
      "Average test loss: 0.004533998638391495\n",
      "Epoch 69/300\n",
      "Average training loss: 0.027958740944663683\n",
      "Average test loss: 0.004498411579678456\n",
      "Epoch 70/300\n",
      "Average training loss: 0.027974944043490622\n",
      "Average test loss: 0.004476085984665487\n",
      "Epoch 71/300\n",
      "Average training loss: 0.027928760685854488\n",
      "Average test loss: 0.0044683656431734565\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02791216743323538\n",
      "Average test loss: 0.0044857334399388896\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02791676545639833\n",
      "Average test loss: 0.004473806863443719\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027913046144776875\n",
      "Average test loss: 0.004466983711553944\n",
      "Epoch 75/300\n",
      "Average training loss: 0.027883242727981674\n",
      "Average test loss: 0.004516119752907091\n",
      "Epoch 76/300\n",
      "Average training loss: 0.027843685993717775\n",
      "Average test loss: 0.004490381317834059\n",
      "Epoch 77/300\n",
      "Average training loss: 0.027831542611122133\n",
      "Average test loss: 0.004515628202921815\n",
      "Epoch 78/300\n",
      "Average training loss: 0.027781432850493327\n",
      "Average test loss: 0.004512822959985998\n",
      "Epoch 79/300\n",
      "Average training loss: 0.027807423344916767\n",
      "Average test loss: 0.004482631176089247\n",
      "Epoch 80/300\n",
      "Average training loss: 0.027748911576138603\n",
      "Average test loss: 0.004464246417913172\n",
      "Epoch 81/300\n",
      "Average training loss: 0.027756031847662396\n",
      "Average test loss: 0.004590385324011246\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0277563839736912\n",
      "Average test loss: 0.00450480244267318\n",
      "Epoch 83/300\n",
      "Average training loss: 0.027712328293257288\n",
      "Average test loss: 0.004505032627118958\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02769790019094944\n",
      "Average test loss: 0.004462271666361226\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02768263856569926\n",
      "Average test loss: 0.004442670686791341\n",
      "Epoch 86/300\n",
      "Average training loss: 0.027649085675676664\n",
      "Average test loss: 0.004495785165371166\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02772363635732068\n",
      "Average test loss: 0.0044514170189698535\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02763349194659127\n",
      "Average test loss: 0.004586190569731924\n",
      "Epoch 89/300\n",
      "Average training loss: 0.027602647653884357\n",
      "Average test loss: 0.004512914958306484\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02759606943362289\n",
      "Average test loss: 0.004456188537594345\n",
      "Epoch 91/300\n",
      "Average training loss: 0.027613153487443923\n",
      "Average test loss: 0.004440661277208063\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0275319569044643\n",
      "Average test loss: 0.004522640253106753\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02754865490562386\n",
      "Average test loss: 0.004621943978799714\n",
      "Epoch 94/300\n",
      "Average training loss: 0.027518745897544755\n",
      "Average test loss: 0.004644597630947828\n",
      "Epoch 95/300\n",
      "Average training loss: 0.027507019228405424\n",
      "Average test loss: 0.004572923031118181\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02750661980277962\n",
      "Average test loss: 0.004447352169081569\n",
      "Epoch 97/300\n",
      "Average training loss: 0.027488721479972205\n",
      "Average test loss: 0.004616150346067217\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0274661190311114\n",
      "Average test loss: 0.0045065340242452095\n",
      "Epoch 99/300\n",
      "Average training loss: 0.027440506670210096\n",
      "Average test loss: 0.004749410869346725\n",
      "Epoch 100/300\n",
      "Average training loss: 0.027433841927184\n",
      "Average test loss: 0.004458683372578687\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02744526654150751\n",
      "Average test loss: 0.004810147167079978\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02740727214349641\n",
      "Average test loss: 0.004422483777213428\n",
      "Epoch 103/300\n",
      "Average training loss: 0.027397224398122894\n",
      "Average test loss: 0.004521143453402651\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02739488887621297\n",
      "Average test loss: 0.006053958609286282\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02743261860807737\n",
      "Average test loss: 0.004722284416357676\n",
      "Epoch 106/300\n",
      "Average training loss: 0.027364736013942296\n",
      "Average test loss: 0.004508024463223087\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02734732384979725\n",
      "Average test loss: 0.004461817031933202\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02734343731403351\n",
      "Average test loss: 0.0044792486040128605\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02732648683918847\n",
      "Average test loss: 0.004428231524096595\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027308023750782014\n",
      "Average test loss: 0.004472792618390587\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02730130142966906\n",
      "Average test loss: 0.0045098661941786605\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027300972311033143\n",
      "Average test loss: 0.004435507080207269\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027276678957872922\n",
      "Average test loss: 0.0044902118038800025\n",
      "Epoch 114/300\n",
      "Average training loss: 0.027258530577023824\n",
      "Average test loss: 0.004478837661652101\n",
      "Epoch 115/300\n",
      "Average training loss: 0.027261180303162998\n",
      "Average test loss: 0.0045497968403829464\n",
      "Epoch 116/300\n",
      "Average training loss: 0.027236080105106034\n",
      "Average test loss: 0.004424774398406346\n",
      "Epoch 117/300\n",
      "Average training loss: 0.027256073825889165\n",
      "Average test loss: 0.00443111294756333\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027220847103330822\n",
      "Average test loss: 0.004456216477685505\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02723124365342988\n",
      "Average test loss: 0.0044457353916433125\n",
      "Epoch 120/300\n",
      "Average training loss: 0.027180305778980254\n",
      "Average test loss: 0.004514068298041821\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027178416836592886\n",
      "Average test loss: 0.004482458003693157\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027181565009885365\n",
      "Average test loss: 0.004501926097191042\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027159734941191144\n",
      "Average test loss: 0.004414325286944707\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02715088874267207\n",
      "Average test loss: 0.0044724830157226985\n",
      "Epoch 125/300\n",
      "Average training loss: 0.027144786584708425\n",
      "Average test loss: 0.00442354549943573\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02714301488134596\n",
      "Average test loss: 0.004415989483396212\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027139894248710737\n",
      "Average test loss: 0.004503847619104716\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027321726214554573\n",
      "Average test loss: 0.004415301438834932\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02708377924727069\n",
      "Average test loss: 0.0045347812333040765\n",
      "Epoch 130/300\n",
      "Average training loss: 0.027101876662837133\n",
      "Average test loss: 0.004436671640930904\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02704866803354687\n",
      "Average test loss: 0.004467572941134373\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02706672386162811\n",
      "Average test loss: 0.005003761885066827\n",
      "Epoch 133/300\n",
      "Average training loss: 0.027061910351117453\n",
      "Average test loss: 0.004447177657236656\n",
      "Epoch 134/300\n",
      "Average training loss: 0.027050451134641964\n",
      "Average test loss: 0.004452747138837973\n",
      "Epoch 135/300\n",
      "Average training loss: 0.027093262005183433\n",
      "Average test loss: 0.004460694916132424\n",
      "Epoch 136/300\n",
      "Average training loss: 0.027045746535062788\n",
      "Average test loss: 0.0044691183548420665\n",
      "Epoch 137/300\n",
      "Average training loss: 0.027054432049393652\n",
      "Average test loss: 0.00448452519501249\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0270200937655237\n",
      "Average test loss: 0.004500213572134574\n",
      "Epoch 139/300\n",
      "Average training loss: 0.027010532261596786\n",
      "Average test loss: 0.004519151519156165\n",
      "Epoch 140/300\n",
      "Average training loss: 0.027024199353324042\n",
      "Average test loss: 0.004479493477278286\n",
      "Epoch 141/300\n",
      "Average training loss: 0.026997572200165854\n",
      "Average test loss: 0.004448139595488707\n",
      "Epoch 142/300\n",
      "Average training loss: 0.026983616936537953\n",
      "Average test loss: 0.004406584625856744\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02697152162426048\n",
      "Average test loss: 0.004468517606043153\n",
      "Epoch 144/300\n",
      "Average training loss: 0.026980843984418443\n",
      "Average test loss: 0.004533397291683489\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02695043495297432\n",
      "Average test loss: 0.004449428707982103\n",
      "Epoch 146/300\n",
      "Average training loss: 0.026976685992545553\n",
      "Average test loss: 0.004428779258082311\n",
      "Epoch 147/300\n",
      "Average training loss: 0.026950146906905705\n",
      "Average test loss: 0.004426810461613867\n",
      "Epoch 148/300\n",
      "Average training loss: 0.026928318394554986\n",
      "Average test loss: 0.004724273543804884\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026939355490936172\n",
      "Average test loss: 0.004462594405230549\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02692033664054341\n",
      "Average test loss: 0.004463652914389968\n",
      "Epoch 151/300\n",
      "Average training loss: 0.026904590126540926\n",
      "Average test loss: 0.004492037649369902\n",
      "Epoch 152/300\n",
      "Average training loss: 0.026915644960270987\n",
      "Average test loss: 0.004437453166892131\n",
      "Epoch 153/300\n",
      "Average training loss: 0.026902630913588736\n",
      "Average test loss: 0.00447471551804079\n",
      "Epoch 154/300\n",
      "Average training loss: 0.026882628657751612\n",
      "Average test loss: 0.004553904100424714\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02688869685265753\n",
      "Average test loss: 0.004422024347715907\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02688168709145652\n",
      "Average test loss: 0.00441105511950122\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02688431465294626\n",
      "Average test loss: 0.004520872747111652\n",
      "Epoch 158/300\n",
      "Average training loss: 0.026863337092929415\n",
      "Average test loss: 0.004420444726530049\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02684549177189668\n",
      "Average test loss: 0.004636608088389039\n",
      "Epoch 160/300\n",
      "Average training loss: 0.026845583733585147\n",
      "Average test loss: 0.004426498159352276\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02683959130777253\n",
      "Average test loss: 0.004554317731410265\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02683039871520466\n",
      "Average test loss: 0.004443502767218483\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026841880099640954\n",
      "Average test loss: 0.004416353756354915\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026813350000315244\n",
      "Average test loss: 0.004439128745347262\n",
      "Epoch 165/300\n",
      "Average training loss: 0.026808493306239446\n",
      "Average test loss: 0.004426545046580335\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02679411252670818\n",
      "Average test loss: 0.004438968538617094\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026791618309087224\n",
      "Average test loss: 0.004413200028654602\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02681422685417864\n",
      "Average test loss: 0.004449861992150545\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026784629164470566\n",
      "Average test loss: 0.004463021715895997\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02677880561020639\n",
      "Average test loss: 0.004447927189369996\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026765676577885946\n",
      "Average test loss: 0.004447746068653133\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02676176491379738\n",
      "Average test loss: 0.040907247679101096\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02690753294361962\n",
      "Average test loss: 0.004423679459840059\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026720822556151283\n",
      "Average test loss: 0.004425693432903952\n",
      "Epoch 175/300\n",
      "Average training loss: 0.026745691281225947\n",
      "Average test loss: 0.004428619208642178\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026735737471116915\n",
      "Average test loss: 0.004461493875003523\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0267080397175418\n",
      "Average test loss: 0.004448348816898134\n",
      "Epoch 178/300\n",
      "Average training loss: 0.026728862017393114\n",
      "Average test loss: 0.004629946566704247\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02670771041346921\n",
      "Average test loss: 0.0046876699212524625\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026706429185138807\n",
      "Average test loss: 0.0044384314599964355\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026713424298498364\n",
      "Average test loss: 0.0044268556307587355\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026697830125689506\n",
      "Average test loss: 0.004436811968684197\n",
      "Epoch 183/300\n",
      "Average training loss: 0.026669988666971525\n",
      "Average test loss: 0.004422695390052266\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026677276545100742\n",
      "Average test loss: 0.004401325529441238\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026697880008154446\n",
      "Average test loss: 0.0044348975887729064\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026688081992997062\n",
      "Average test loss: 0.004415131624788046\n",
      "Epoch 187/300\n",
      "Average training loss: 0.026654478904273776\n",
      "Average test loss: 0.0044291412346065045\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02664535915520456\n",
      "Average test loss: 0.004497745011001825\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02666973458064927\n",
      "Average test loss: 0.004428991983541184\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02666180881857872\n",
      "Average test loss: 0.004417618843002452\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02665113880402512\n",
      "Average test loss: 0.004423189506348637\n",
      "Epoch 192/300\n",
      "Average training loss: 0.026635906181401676\n",
      "Average test loss: 0.00443719599623647\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026618526938888762\n",
      "Average test loss: 0.0043986242127915225\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02663100470768081\n",
      "Average test loss: 0.004669795222580433\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02665580393373966\n",
      "Average test loss: 0.004421805854058928\n",
      "Epoch 196/300\n",
      "Average training loss: 0.026607059659229385\n",
      "Average test loss: 0.004429902954647938\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026599790622790654\n",
      "Average test loss: 0.004464316243926684\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02660977231628365\n",
      "Average test loss: 0.004434090923104021\n",
      "Epoch 199/300\n",
      "Average training loss: 0.026596482482221392\n",
      "Average test loss: 0.00441034801180164\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026576743062999512\n",
      "Average test loss: 0.004414911558437679\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02655784284406238\n",
      "Average test loss: 0.004411061213662227\n",
      "Epoch 202/300\n",
      "Average training loss: 0.026597450074222352\n",
      "Average test loss: 0.00469483734584517\n",
      "Epoch 203/300\n",
      "Average training loss: 0.026584652993414136\n",
      "Average test loss: 0.00442640422988269\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02656604323618942\n",
      "Average test loss: 0.004416469772035877\n",
      "Epoch 205/300\n",
      "Average training loss: 0.026562670406368043\n",
      "Average test loss: 0.004452366758758823\n",
      "Epoch 206/300\n",
      "Average training loss: 0.026577045086357328\n",
      "Average test loss: 0.004412807288269202\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02655970136158996\n",
      "Average test loss: 0.0046230804199973745\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02655491742160585\n",
      "Average test loss: 0.004424116048547957\n",
      "Epoch 209/300\n",
      "Average training loss: 0.026528399086660808\n",
      "Average test loss: 0.004416609135352903\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026557536483638816\n",
      "Average test loss: 0.004437669898072878\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02651275148325496\n",
      "Average test loss: 0.004468191178722514\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0265188833143976\n",
      "Average test loss: 0.004435442446006669\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026511599792374507\n",
      "Average test loss: 0.004484183573474487\n",
      "Epoch 214/300\n",
      "Average training loss: 0.026522491064336566\n",
      "Average test loss: 0.004430055020584\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02650240405731731\n",
      "Average test loss: 0.00444209881623586\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026495043534371588\n",
      "Average test loss: 0.004425122113691436\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026498480627934137\n",
      "Average test loss: 0.004479353092610836\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026496494543221262\n",
      "Average test loss: 0.004427250930418571\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02651184292137623\n",
      "Average test loss: 0.004807399229456981\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02646841855843862\n",
      "Average test loss: 0.004422436224089729\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02649021237757471\n",
      "Average test loss: 0.004451325431466102\n",
      "Epoch 222/300\n",
      "Average training loss: 0.026463664511839548\n",
      "Average test loss: 0.004454334070078201\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026476973495549627\n",
      "Average test loss: 0.004456853556343251\n",
      "Epoch 224/300\n",
      "Average training loss: 0.026462810277938842\n",
      "Average test loss: 0.004501479163765907\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026445901619063482\n",
      "Average test loss: 0.0044634992807275715\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02645904197792212\n",
      "Average test loss: 0.0044609346932007205\n",
      "Epoch 227/300\n",
      "Average training loss: 0.026439540558391147\n",
      "Average test loss: 0.004455369491957956\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02644309332138962\n",
      "Average test loss: 0.004454495608392689\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026445280366473728\n",
      "Average test loss: 0.004447041935804817\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026643559447593158\n",
      "Average test loss: 0.0052871179038451776\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026420418312152225\n",
      "Average test loss: 0.004409011880556742\n",
      "Epoch 232/300\n",
      "Average training loss: 0.026401778115166558\n",
      "Average test loss: 0.0044133967012166975\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02642871950732337\n",
      "Average test loss: 0.004551302316288153\n",
      "Epoch 234/300\n",
      "Average training loss: 0.026414309874176978\n",
      "Average test loss: 0.004437918429159456\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02640787452790472\n",
      "Average test loss: 0.004587419193651941\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02642195373442438\n",
      "Average test loss: 0.0044656967541409865\n",
      "Epoch 237/300\n",
      "Average training loss: 0.026495434386862648\n",
      "Average test loss: 0.0044104164164099425\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02636888427204556\n",
      "Average test loss: 0.004433233808726072\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02637640862001313\n",
      "Average test loss: 0.004443677172892624\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026383479378289647\n",
      "Average test loss: 0.004661158860557609\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026407269751032192\n",
      "Average test loss: 0.00453999568356408\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026401165079739358\n",
      "Average test loss: 0.004427833203640249\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026376201161079936\n",
      "Average test loss: 0.0045301703434023595\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02641004672812091\n",
      "Average test loss: 0.004424907907429668\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02635013781984647\n",
      "Average test loss: 0.00444489602247874\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02637518738210201\n",
      "Average test loss: 0.004458098836657074\n",
      "Epoch 247/300\n",
      "Average training loss: 0.026340795434183543\n",
      "Average test loss: 0.004543660294678476\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026336608696315024\n",
      "Average test loss: 0.004571196551745136\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026356436423129504\n",
      "Average test loss: 0.004444747432445486\n",
      "Epoch 250/300\n",
      "Average training loss: 0.026338324204087257\n",
      "Average test loss: 0.0044197629226578605\n",
      "Epoch 251/300\n",
      "Average training loss: 0.026365997209317156\n",
      "Average test loss: 0.004471592736740907\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02644345237976975\n",
      "Average test loss: 0.004416618663196762\n",
      "Epoch 253/300\n",
      "Average training loss: 0.026309882264998224\n",
      "Average test loss: 0.004447622023729814\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026314560542503994\n",
      "Average test loss: 0.004528529946174887\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02634116404586368\n",
      "Average test loss: 0.011008944850001071\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026320836769209967\n",
      "Average test loss: 0.004456178705725405\n",
      "Epoch 257/300\n",
      "Average training loss: 0.026315995729631846\n",
      "Average test loss: 0.004428497224218316\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026311887880166373\n",
      "Average test loss: 0.006340407081569234\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026286242317822246\n",
      "Average test loss: 0.004423258914715714\n",
      "Epoch 260/300\n",
      "Average training loss: 0.026298335267437828\n",
      "Average test loss: 0.004430188534574376\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026320205464959143\n",
      "Average test loss: 0.004428954908831252\n",
      "Epoch 262/300\n",
      "Average training loss: 0.026284448479612667\n",
      "Average test loss: 0.00465092402158512\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02629652080270979\n",
      "Average test loss: 0.00441352373899685\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02628968430393272\n",
      "Average test loss: 0.004416293483227491\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026282472749551138\n",
      "Average test loss: 0.004457363090167443\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026268670249316426\n",
      "Average test loss: 0.0044318989622924064\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026269216780861217\n",
      "Average test loss: 0.004445176376236809\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02629735657076041\n",
      "Average test loss: 0.005246611314101352\n",
      "Epoch 269/300\n",
      "Average training loss: 0.026240540815724266\n",
      "Average test loss: 0.004488312625843618\n",
      "Epoch 270/300\n",
      "Average training loss: 0.026290293395519256\n",
      "Average test loss: 0.004486850887743963\n",
      "Epoch 271/300\n",
      "Average training loss: 0.026241823009318776\n",
      "Average test loss: 0.004458165453953875\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026364791669779352\n",
      "Average test loss: 0.004444059665832255\n",
      "Epoch 273/300\n",
      "Average training loss: 0.026238988175988198\n",
      "Average test loss: 0.004419481336449584\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02622572865254349\n",
      "Average test loss: 0.004434916213154793\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026231061346001096\n",
      "Average test loss: 0.004472896854289704\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02622958473695649\n",
      "Average test loss: 0.004439139169537359\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0262320415576299\n",
      "Average test loss: 0.0044506179785562885\n",
      "Epoch 278/300\n",
      "Average training loss: 0.026235182103183534\n",
      "Average test loss: 0.004433456929814485\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02621780261066225\n",
      "Average test loss: 0.004473886149624983\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026236522258983718\n",
      "Average test loss: 0.004499744441360235\n",
      "Epoch 281/300\n",
      "Average training loss: 0.026212655342287488\n",
      "Average test loss: 0.004460919044084019\n",
      "Epoch 282/300\n",
      "Average training loss: 0.026216779281695683\n",
      "Average test loss: 0.004445914700213406\n",
      "Epoch 283/300\n",
      "Average training loss: 0.026228608336713578\n",
      "Average test loss: 0.004468596965074539\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026215209760599666\n",
      "Average test loss: 0.0044266754072159525\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026218878302309248\n",
      "Average test loss: 0.004449756123539475\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02618455549577872\n",
      "Average test loss: 0.004438564029832681\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026220725191964042\n",
      "Average test loss: 0.00605871448541681\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026196428277426295\n",
      "Average test loss: 0.004475893029943109\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0262149483114481\n",
      "Average test loss: 0.004486145186341471\n",
      "Epoch 290/300\n",
      "Average training loss: 0.026177800989813274\n",
      "Average test loss: 0.00445029215183523\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026169219344854354\n",
      "Average test loss: 0.004445037714309163\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02618724697166019\n",
      "Average test loss: 0.004425334740016195\n",
      "Epoch 293/300\n",
      "Average training loss: 0.026197669375273916\n",
      "Average test loss: 0.004616546471499734\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02616048587858677\n",
      "Average test loss: 0.004477506085195475\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026184480460153686\n",
      "Average test loss: 0.004430025628043546\n",
      "Epoch 296/300\n",
      "Average training loss: 0.026166460379958154\n",
      "Average test loss: 0.004442111512232158\n",
      "Epoch 297/300\n",
      "Average training loss: 0.026202430985040135\n",
      "Average test loss: 0.004467838367654218\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0261312246306075\n",
      "Average test loss: 0.004444926953564087\n",
      "Epoch 299/300\n",
      "Average training loss: 0.026159430099858178\n",
      "Average test loss: 0.004620037308169736\n",
      "Epoch 300/300\n",
      "Average training loss: 0.026198933336469863\n",
      "Average test loss: 0.004675814068151845\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.19184472144643466\n",
      "Average test loss: 0.0064481688497795\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04666293050845464\n",
      "Average test loss: 0.006020586883028348\n",
      "Epoch 3/300\n",
      "Average training loss: 0.041292247470882205\n",
      "Average test loss: 0.005669107193748156\n",
      "Epoch 4/300\n",
      "Average training loss: 0.038830955740478304\n",
      "Average test loss: 0.005422115802764893\n",
      "Epoch 5/300\n",
      "Average training loss: 0.036592075715462365\n",
      "Average test loss: 0.005328322274817361\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03495161552892791\n",
      "Average test loss: 0.005490963940819105\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03369551489750544\n",
      "Average test loss: 0.00529583623384436\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03249005270997683\n",
      "Average test loss: 0.004857234651843707\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03160568431019783\n",
      "Average test loss: 0.004549675882690483\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0306896135525571\n",
      "Average test loss: 0.004429691496408648\n",
      "Epoch 11/300\n",
      "Average training loss: 0.029868885179360708\n",
      "Average test loss: 0.004419328659979833\n",
      "Epoch 12/300\n",
      "Average training loss: 0.029187296380599338\n",
      "Average test loss: 0.004613436173233721\n",
      "Epoch 13/300\n",
      "Average training loss: 0.028621959439582297\n",
      "Average test loss: 0.0043073581092887454\n",
      "Epoch 14/300\n",
      "Average training loss: 0.028118339066704114\n",
      "Average test loss: 0.004265840657055378\n",
      "Epoch 15/300\n",
      "Average training loss: 0.027612947470611996\n",
      "Average test loss: 0.004118844023595253\n",
      "Epoch 16/300\n",
      "Average training loss: 0.027258257317874166\n",
      "Average test loss: 0.004161617506502403\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02694006838897864\n",
      "Average test loss: 0.004331480553580655\n",
      "Epoch 18/300\n",
      "Average training loss: 0.026611774930523503\n",
      "Average test loss: 0.003924664727101723\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02625905473695861\n",
      "Average test loss: 0.003866980106052425\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02596956276231342\n",
      "Average test loss: 0.003870250571105215\n",
      "Epoch 21/300\n",
      "Average training loss: 0.025749319314956665\n",
      "Average test loss: 0.003987968812179234\n",
      "Epoch 22/300\n",
      "Average training loss: 0.025572447000278366\n",
      "Average test loss: 0.0037395134485430186\n",
      "Epoch 23/300\n",
      "Average training loss: 0.025321929042538008\n",
      "Average test loss: 0.003967650386401349\n",
      "Epoch 24/300\n",
      "Average training loss: 0.025156820939646826\n",
      "Average test loss: 0.0036862478562527234\n",
      "Epoch 25/300\n",
      "Average training loss: 0.025001971393823624\n",
      "Average test loss: 0.003709328847005963\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02487262178460757\n",
      "Average test loss: 0.0036314856020940676\n",
      "Epoch 27/300\n",
      "Average training loss: 0.024716654946406683\n",
      "Average test loss: 0.0036345167201426295\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02454441066417429\n",
      "Average test loss: 0.0035873499796208407\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02444025163849195\n",
      "Average test loss: 0.003623508360236883\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02435985525449117\n",
      "Average test loss: 0.0035487928351180423\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024198750981026227\n",
      "Average test loss: 0.003544039927009079\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024122482397490076\n",
      "Average test loss: 0.0035423960516022313\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02406616061594751\n",
      "Average test loss: 0.0035223469386498132\n",
      "Epoch 34/300\n",
      "Average training loss: 0.023952192342943615\n",
      "Average test loss: 0.0034986992447326583\n",
      "Epoch 35/300\n",
      "Average training loss: 0.023910678163170815\n",
      "Average test loss: 0.003771179743732015\n",
      "Epoch 36/300\n",
      "Average training loss: 0.023788148027327326\n",
      "Average test loss: 0.003525482005129258\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023758727216058307\n",
      "Average test loss: 0.0036474912265936532\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023665101571215524\n",
      "Average test loss: 0.0034816427315688798\n",
      "Epoch 39/300\n",
      "Average training loss: 0.023618109565642146\n",
      "Average test loss: 0.003463900790653295\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023563285367356405\n",
      "Average test loss: 0.0035544062654177346\n",
      "Epoch 41/300\n",
      "Average training loss: 0.023529758714967305\n",
      "Average test loss: 0.0035039902346001733\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023448436311549612\n",
      "Average test loss: 0.0034629178072015446\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02343251063591904\n",
      "Average test loss: 0.0034175360169675616\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0233764942371183\n",
      "Average test loss: 0.003489179018057055\n",
      "Epoch 45/300\n",
      "Average training loss: 0.023323210342062844\n",
      "Average test loss: 0.0034850324369553065\n",
      "Epoch 46/300\n",
      "Average training loss: 0.023310408756136894\n",
      "Average test loss: 0.003421435084607866\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02323257792327139\n",
      "Average test loss: 0.0034349447186622353\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023205537941720752\n",
      "Average test loss: 0.003424581353035238\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02316637111041281\n",
      "Average test loss: 0.003562046722198526\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02312032383349207\n",
      "Average test loss: 0.0034243928353405663\n",
      "Epoch 51/300\n",
      "Average training loss: 0.023081561835275755\n",
      "Average test loss: 0.0033998449579295187\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023078766082723936\n",
      "Average test loss: 0.003400198235693905\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02304441700047917\n",
      "Average test loss: 0.003633661934071117\n",
      "Epoch 54/300\n",
      "Average training loss: 0.022979147028591898\n",
      "Average test loss: 0.0033954227500491672\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02297512350148625\n",
      "Average test loss: 0.0033633178716732394\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02292732251021597\n",
      "Average test loss: 0.0033663528048329884\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02291437399884065\n",
      "Average test loss: 0.0033767470011694563\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02286820657054583\n",
      "Average test loss: 0.0033729215326408547\n",
      "Epoch 59/300\n",
      "Average training loss: 0.022845399661196603\n",
      "Average test loss: 0.0034401519416520994\n",
      "Epoch 60/300\n",
      "Average training loss: 0.022842979656325445\n",
      "Average test loss: 0.0033494557705190447\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022810259458091522\n",
      "Average test loss: 0.0033745403372579153\n",
      "Epoch 62/300\n",
      "Average training loss: 0.022777697346276708\n",
      "Average test loss: 0.003406457940116525\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02275349542664157\n",
      "Average test loss: 0.0034272763969169723\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02275515721903907\n",
      "Average test loss: 0.0033457602032770715\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02271117921339141\n",
      "Average test loss: 0.003471519328032931\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02270227239198155\n",
      "Average test loss: 0.0033552290302597816\n",
      "Epoch 67/300\n",
      "Average training loss: 0.022667560059163306\n",
      "Average test loss: 0.00333090525223977\n",
      "Epoch 68/300\n",
      "Average training loss: 0.022648058036135302\n",
      "Average test loss: 0.0033450372443637913\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022618540243970024\n",
      "Average test loss: 0.003349543346919947\n",
      "Epoch 70/300\n",
      "Average training loss: 0.022619111105799675\n",
      "Average test loss: 0.0033750590419189797\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02263601367175579\n",
      "Average test loss: 0.00350983437957863\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022551804333925247\n",
      "Average test loss: 0.003324152067510618\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02255141457915306\n",
      "Average test loss: 0.0033781122161696355\n",
      "Epoch 74/300\n",
      "Average training loss: 0.022524413728051716\n",
      "Average test loss: 0.0033593385223713184\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02250596966677242\n",
      "Average test loss: 0.003319904581954082\n",
      "Epoch 76/300\n",
      "Average training loss: 0.022475765955117015\n",
      "Average test loss: 0.0033268222094823916\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022481360480189325\n",
      "Average test loss: 0.0033736025885575346\n",
      "Epoch 78/300\n",
      "Average training loss: 0.022464691675371594\n",
      "Average test loss: 0.003341069331392646\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02244597020910846\n",
      "Average test loss: 0.003310031325245897\n",
      "Epoch 80/300\n",
      "Average training loss: 0.022443747743964196\n",
      "Average test loss: 0.003400531033467915\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022424832589096493\n",
      "Average test loss: 0.003535087492316961\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022400199684831833\n",
      "Average test loss: 0.003335432279441092\n",
      "Epoch 83/300\n",
      "Average training loss: 0.022381619564361042\n",
      "Average test loss: 0.008301977898511621\n",
      "Epoch 84/300\n",
      "Average training loss: 0.022380371062291994\n",
      "Average test loss: 0.003591534878230757\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02234333140982522\n",
      "Average test loss: 0.0036593634076416493\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02232940668364366\n",
      "Average test loss: 0.0033070308276348646\n",
      "Epoch 87/300\n",
      "Average training loss: 0.022370478204554983\n",
      "Average test loss: 0.003371625820174813\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022287973559565015\n",
      "Average test loss: 0.0032999896626505587\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022291445147660044\n",
      "Average test loss: 0.0032984022932748\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022269870100749864\n",
      "Average test loss: 0.0033008265188998647\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022264677393767567\n",
      "Average test loss: 0.0033116001261191234\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02225292417075899\n",
      "Average test loss: 0.003314151608488626\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02224478801091512\n",
      "Average test loss: 0.0033307763160102897\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022235158519612417\n",
      "Average test loss: 0.003348732763487432\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022215080857276918\n",
      "Average test loss: 0.0032996393617035612\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022205916906396547\n",
      "Average test loss: 0.00335717398693992\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022182031056947177\n",
      "Average test loss: 0.003292324263602495\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022169460967183113\n",
      "Average test loss: 0.003315410728669829\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022160106680459448\n",
      "Average test loss: 0.003349386551727851\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022153106341759363\n",
      "Average test loss: 0.003289229177352455\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022147979651888212\n",
      "Average test loss: 0.0032966597011933724\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02212125728196568\n",
      "Average test loss: 0.003340718789233102\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02211395849949784\n",
      "Average test loss: 0.003301882791850302\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022094759494066238\n",
      "Average test loss: 0.0034187308740284707\n",
      "Epoch 105/300\n",
      "Average training loss: 0.022111648860904904\n",
      "Average test loss: 0.003292383120291763\n",
      "Epoch 106/300\n",
      "Average training loss: 0.022079651046130393\n",
      "Average test loss: 0.003339798212051392\n",
      "Epoch 107/300\n",
      "Average training loss: 0.022084645734892952\n",
      "Average test loss: 0.003320913707216581\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022063282305995625\n",
      "Average test loss: 0.003318351448410087\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02204937641819318\n",
      "Average test loss: 0.0036268690795534186\n",
      "Epoch 110/300\n",
      "Average training loss: 0.022049554829796154\n",
      "Average test loss: 0.0033210117427839173\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02202421770244837\n",
      "Average test loss: 0.0033285080078575345\n",
      "Epoch 112/300\n",
      "Average training loss: 0.022006049849920804\n",
      "Average test loss: 0.003286654162324137\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02202037013404899\n",
      "Average test loss: 0.02947842428750462\n",
      "Epoch 114/300\n",
      "Average training loss: 0.022035997364256116\n",
      "Average test loss: 0.0033120466131303047\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02198244112895595\n",
      "Average test loss: 0.003289200444188383\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02199689787460698\n",
      "Average test loss: 0.0033000105942289034\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02197846172749996\n",
      "Average test loss: 0.003299568377642168\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021954277469052208\n",
      "Average test loss: 0.003305351273260183\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021945570584800508\n",
      "Average test loss: 0.00329347762838006\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021940036760436165\n",
      "Average test loss: 0.0033046009278752736\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021953818938798376\n",
      "Average test loss: 0.0033174677143494287\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02194096199174722\n",
      "Average test loss: 0.003513561963207192\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0219418024337954\n",
      "Average test loss: 0.0032897592975447577\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02192134866449568\n",
      "Average test loss: 0.003283452265792423\n",
      "Epoch 125/300\n",
      "Average training loss: 0.021889575961563323\n",
      "Average test loss: 0.003291219190797872\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02189721058226294\n",
      "Average test loss: 0.003274436772490541\n",
      "Epoch 127/300\n",
      "Average training loss: 0.021878846171829437\n",
      "Average test loss: 0.003299722842872143\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021888649756709736\n",
      "Average test loss: 0.0032802495604587925\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021882355486353238\n",
      "Average test loss: 0.003309642151825958\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02185036386218336\n",
      "Average test loss: 0.003286231199900309\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02186028075052632\n",
      "Average test loss: 0.0033143135170555778\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02184151362048255\n",
      "Average test loss: 0.0033230399385922485\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02183942981229888\n",
      "Average test loss: 0.003320462191891339\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021827357749144235\n",
      "Average test loss: 0.0033026770200166437\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021822201955649587\n",
      "Average test loss: 0.0032845239848312406\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0218060063554181\n",
      "Average test loss: 0.0032789302385515637\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02180595788359642\n",
      "Average test loss: 0.0032833487041708494\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021794851217004987\n",
      "Average test loss: 0.0033260516917539965\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021775268501705594\n",
      "Average test loss: 0.0032944904619620907\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02177946547832754\n",
      "Average training loss: 0.021737208142876625\n",
      "Average test loss: 0.00329732882976532\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02173972413606114\n",
      "Average test loss: 0.003277796381049686\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02172264758745829\n",
      "Average test loss: 0.003274716179197033\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021736218040188153\n",
      "Average test loss: 0.003334335955894656\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02170846416387293\n",
      "Average test loss: 0.003279384964870082\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02169568805396557\n",
      "Average test loss: 0.0035565610952261423\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021697249453928737\n",
      "Average test loss: 0.003315247582478656\n",
      "Epoch 153/300\n",
      "Average training loss: 0.021699452771080863\n",
      "Average test loss: 0.0035204306451810732\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021686801637212434\n",
      "Average test loss: 0.0032785780156652134\n",
      "Epoch 155/300\n",
      "Average training loss: 0.021682696933547657\n",
      "Average test loss: 0.003281863643063439\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02164183530708154\n",
      "Average test loss: 0.0032966731145150133\n",
      "Epoch 157/300\n",
      "Average training loss: 0.021688108616405063\n",
      "Average test loss: 0.0033152656861477428\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02165784006151888\n",
      "Average test loss: 0.003278948694984946\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021630363982584742\n",
      "Average test loss: 0.0033103778060111734\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021632844858699374\n",
      "Average test loss: 0.003315843853685591\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021640516746375297\n",
      "Average test loss: 0.0033018315347532433\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02162992182208432\n",
      "Average test loss: 0.003305854291137722\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021614882570174004\n",
      "Average test loss: 0.003294288208708167\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021628035770522223\n",
      "Average test loss: 0.0033002517092972994\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021606018983655505\n",
      "Average test loss: 0.003310920588672161\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021598419325219262\n",
      "Average test loss: 0.003273691114866071\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021608361664745543\n",
      "Average test loss: 0.003772750773363643\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021604129971729384\n",
      "Average test loss: 0.0032893210336979893\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021618128202027746\n",
      "Average test loss: 0.0033390821452356045\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021596055896745788\n",
      "Average test loss: 0.0032876397925946446\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021569740219248665\n",
      "Average test loss: 0.0033239891872637803\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02156712945136759\n",
      "Average test loss: 0.003302561172387666\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0215610032296843\n",
      "Average test loss: 0.0032765436079353095\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02155640947818756\n",
      "Average test loss: 0.0032803789801481697\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021555645421147348\n",
      "Average test loss: 0.0033001483941657674\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021564391841491063\n",
      "Average test loss: 0.0033940638216833274\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02154548800488313\n",
      "Average test loss: 0.0032955148118651574\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02155033512910207\n",
      "Average test loss: 0.0032799305710941554\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021532257477442425\n",
      "Average test loss: 0.016786734368238184\n",
      "Epoch 180/300\n",
      "Average training loss: 0.021571079747544394\n",
      "Average test loss: 0.0032806214450134172\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021523027966419857\n",
      "Average test loss: 0.003375073303365045\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021515383834640186\n",
      "Average test loss: 0.0032801141631272103\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02151708076066441\n",
      "Average test loss: 0.0032659643789132438\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021511306057373683\n",
      "Average test loss: 0.0032849378757592706\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021507336593336528\n",
      "Average test loss: 0.0032624214424027335\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021518296241760253\n",
      "Average test loss: 0.0032681615729298857\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02149789446592331\n",
      "Average test loss: 0.0032832238231268194\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02145315112504694\n",
      "Average test loss: 0.003312713411119249\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0214580407159196\n",
      "Average test loss: 0.0033305194007439746\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02144178117480543\n",
      "Average test loss: 0.003316766694602039\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021447811330358186\n",
      "Average test loss: 0.003629467498511076\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021432072440783184\n",
      "Average test loss: 0.003267493024054501\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02143988434639242\n",
      "Average test loss: 0.0032680790709952515\n",
      "Epoch 201/300\n",
      "Average training loss: 0.021430136755108832\n",
      "Average test loss: 0.003305195591929886\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02143159253233009\n",
      "Average test loss: 0.00328432159891559\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02140920208560096\n",
      "Average test loss: 0.003648147150874138\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02140362719198068\n",
      "Average test loss: 0.003270335409169396\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02140982284810808\n",
      "Average test loss: 0.0032760999239981174\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021398028531008296\n",
      "Average test loss: 0.003285144707808892\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02140394290122721\n",
      "Average test loss: 0.0033286599965973033\n",
      "Epoch 208/300\n",
      "Average training loss: 0.021403227319320043\n",
      "Average test loss: 0.003281978004301588\n",
      "Epoch 209/300\n",
      "Average training loss: 0.021393248900771142\n",
      "Average test loss: 0.0032706388214396105\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021393990733557276\n",
      "Average test loss: 0.0032759025649478036\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021365773239069515\n",
      "Average test loss: 0.0032853858582675456\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02138463502460056\n",
      "Average test loss: 0.0033041958438439502\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02136408448550436\n",
      "Average test loss: 0.0033028913084417583\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021375964613424406\n",
      "Average test loss: 0.0032743388060480355\n",
      "Epoch 215/300\n",
      "Average training loss: 0.021341250118282105\n",
      "Average test loss: 0.003292866445456942\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021331484728389315\n",
      "Average test loss: 0.003309732131039103\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021332500388224918\n",
      "Average test loss: 0.003390236944788032\n",
      "Epoch 222/300\n",
      "Average training loss: 0.021333010408613417\n",
      "Average test loss: 0.003331648079471456\n",
      "Epoch 223/300\n",
      "Average training loss: 0.021361943416297435\n",
      "Average test loss: 0.003337925222184923\n",
      "Epoch 224/300\n",
      "Average training loss: 0.021314607322216034\n",
      "Average test loss: 0.0032867495227191183\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021314853083756236\n",
      "Average test loss: 0.003309093630148305\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021331381938523717\n",
      "Average test loss: 0.0032987627738879787\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0213064613574081\n",
      "Average test loss: 0.0032688882580647867\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02131356010503239\n",
      "Average test loss: 0.0033167418580916192\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02129740485548973\n",
      "Average test loss: 0.0032929947587351004\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02129630842142635\n",
      "Average test loss: 0.00475081364148193\n",
      "Epoch 231/300\n",
      "Average training loss: 0.021294833714763324\n",
      "Average test loss: 0.003297710266378191\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02129060801698102\n",
      "Average test loss: 0.0034343637439111867\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02129389796157678\n",
      "Average test loss: 0.003267680915279521\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02128917333152559\n",
      "Average test loss: 0.003383193472193347\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02128404182361232\n",
      "Average test loss: 0.003340926446020603\n",
      "Epoch 236/300\n",
      "Average training loss: 0.021275614811314476\n",
      "Average test loss: 0.005574061520811584\n",
      "Epoch 237/300\n",
      "Average training loss: 0.021289617624547747\n",
      "Average test loss: 0.0033506603981885647\n",
      "Epoch 238/300\n",
      "Average training loss: 0.021270597239335377\n",
      "Average test loss: 0.003360613511461351\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02127146351751354\n",
      "Average test loss: 0.0033118028603494167\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02127616703344716\n",
      "Average test loss: 0.003336739926909407\n",
      "Epoch 241/300\n",
      "Average training loss: 0.021233968173464138\n",
      "Average test loss: 0.0032954364688032203\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021263985443446373\n",
      "Average test loss: 0.003273795697527627\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021246983584430484\n",
      "Average test loss: 0.0032752326861437823\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02125302797142002\n",
      "Average test loss: 0.0032894226542363562\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021246837321254944\n",
      "Average test loss: 0.0032896055976549786\n",
      "Epoch 246/300\n",
      "Average training loss: 0.021253898622261153\n",
      "Average test loss: 0.0032902540710444254\n",
      "Epoch 247/300\n",
      "Average training loss: 0.021234323922130798\n",
      "Average test loss: 0.0032831511044253904\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02122649875945515\n",
      "Average test loss: 0.0032995804763502547\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021227245446708468\n",
      "Average test loss: 0.004115534714112679\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02137337213423517\n",
      "Average test loss: 0.003323883334795634\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021211197939183978\n",
      "Average test loss: 0.003325498652127054\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02120683305296633\n",
      "Average test loss: 0.0032927949159509604\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021220787243710622\n",
      "Average test loss: 0.003283556687231693\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021207144412729476\n",
      "Average test loss: 0.0032962428209268382\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02120755776597394\n",
      "Average test loss: 0.003280689854381813\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021211377995709577\n",
      "Average test loss: 0.0033010506534741986\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021198836477266416\n",
      "Average test loss: 0.003302762465758456\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02119523756702741\n",
      "Average test loss: 0.003290324310875601\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021198740351531242\n",
      "Average test loss: 0.0032801978768159947\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02119841923813025\n",
      "Average test loss: 0.0032745933480974702\n",
      "Epoch 261/300\n",
      "Average training loss: 0.021190537133150632\n",
      "Average test loss: 0.003364990346133709\n",
      "Epoch 262/300\n",
      "Average training loss: 0.021183258381154802\n",
      "Average test loss: 0.003291816455622514\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021200113475322722\n",
      "Average test loss: 0.003322987504510416\n",
      "Epoch 264/300\n",
      "Average training loss: 0.021164292593797047\n",
      "Average test loss: 0.0032651813932591013\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0211778458820449\n",
      "Average test loss: 0.003278409569627709\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02116661217643155\n",
      "Average test loss: 0.003275003852115737\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02115138898127609\n",
      "Average test loss: 0.003275789581860105\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02114008267886109\n",
      "Average test loss: 0.003385744591140085\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021150080755352973\n",
      "Average test loss: 0.003276052402332425\n",
      "Epoch 275/300\n",
      "Average training loss: 0.021148892218867937\n",
      "Average test loss: 0.0033098009067277115\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021157475435899364\n",
      "Average test loss: 0.003350343690978156\n",
      "Epoch 277/300\n",
      "Average training loss: 0.021133008435368538\n",
      "Average test loss: 0.003338446610089805\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021127106936441528\n",
      "Average test loss: 0.003277786802086565\n",
      "Epoch 279/300\n",
      "Average training loss: 0.021131096311741406\n",
      "Average test loss: 0.0033416619166317914\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02112605434159438\n",
      "Average test loss: 0.0033616421779410705\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02113604901234309\n",
      "Average test loss: 0.003287503927325209\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02110805831518438\n",
      "Average test loss: 0.003358257592552238\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021125797009302512\n",
      "Average test loss: 0.0033263132230689127\n",
      "Epoch 284/300\n",
      "Average training loss: 0.021125440961784786\n",
      "Average test loss: 0.003278916014979283\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021105992514226173\n",
      "Average test loss: 0.0032891941567262015\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021110884605182543\n",
      "Average test loss: 0.0032920251155479086\n",
      "Epoch 287/300\n",
      "Average training loss: 0.021104093099633854\n",
      "Average test loss: 0.003317709977635079\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021101052584747475\n",
      "Average test loss: 0.003288244452741411\n",
      "Epoch 289/300\n",
      "Average training loss: 0.021112105253669952\n",
      "Average test loss: 0.0034670491483476426\n",
      "Epoch 290/300\n",
      "Average training loss: 0.021095591344767146\n",
      "Average test loss: 0.0032921226976646316\n",
      "Epoch 291/300\n",
      "Average training loss: 0.021076620840364033\n",
      "Average test loss: 0.0032836990935934916\n",
      "Epoch 292/300\n",
      "Average training loss: 0.021102698910567496\n",
      "Average test loss: 0.003290487475693226\n",
      "Epoch 293/300\n",
      "Average training loss: 0.021100001740786765\n",
      "Average test loss: 0.003307769810573922\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021085949978894656\n",
      "Average test loss: 0.0033150035101506443\n",
      "Epoch 295/300\n",
      "Average training loss: 0.021077471082409222\n",
      "Average test loss: 0.0033815587018099095\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021075550066100228\n",
      "Average test loss: 0.003273186608735058\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021074776778618496\n",
      "Average test loss: 0.0032776491737200155\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021081598670946226\n",
      "Average test loss: 0.0033545995087673267\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02106011453602049\n",
      "Average test loss: 0.003307277045523127\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021065622182355987\n",
      "Average test loss: 0.0032995068975206878\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1686972006658713\n",
      "Average test loss: 0.0056037954398327405\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04067858726448483\n",
      "Average test loss: 0.005762820693353812\n",
      "Epoch 3/300\n",
      "Average training loss: 0.036282526353995\n",
      "Average test loss: 0.005144704097261032\n",
      "Epoch 4/300\n",
      "Average training loss: 0.026980335755480662\n",
      "Average test loss: 0.0036505968098839124\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02616231669816706\n",
      "Average test loss: 0.0041500696544018055\n",
      "Epoch 11/300\n",
      "Average training loss: 0.025409317491783035\n",
      "Average test loss: 0.003524904734351569\n",
      "Epoch 12/300\n",
      "Average training loss: 0.024819420185354022\n",
      "Average test loss: 0.003405680774193671\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024262101565798125\n",
      "Average test loss: 0.003388261898317271\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023936443969607354\n",
      "Average test loss: 0.003310046990712484\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023445023982061282\n",
      "Average test loss: 0.003310659678032001\n",
      "Epoch 16/300\n",
      "Average training loss: 0.023093760839766928\n",
      "Average test loss: 0.0032835435991485913\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02278632950782776\n",
      "Average test loss: 0.0034235795934995017\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02253111902376016\n",
      "Average test loss: 0.0030961814117100505\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022266834676265716\n",
      "Average test loss: 0.0030712427850812674\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02202028083966838\n",
      "Average test loss: 0.0030932530334426295\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021815590845213995\n",
      "Average test loss: 0.003116392731045683\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021655641754468283\n",
      "Average test loss: 0.003000481039699581\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021437665187650258\n",
      "Average test loss: 0.0029549622059696252\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021343878510925503\n",
      "Average test loss: 0.003023533384833071\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021191103542844453\n",
      "Average test loss: 0.0028948239215339224\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021058044524656403\n",
      "Average test loss: 0.0028767131136523353\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020965972950889006\n",
      "Average test loss: 0.0029138371410469214\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02084987420340379\n",
      "Average test loss: 0.00284816534527474\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020763518393039703\n",
      "Average test loss: 0.0029389371662918063\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020628735156522856\n",
      "Average test loss: 0.0028702808846202163\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020592311337590218\n",
      "Average test loss: 0.002847808854861392\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02050808832794428\n",
      "Average test loss: 0.002871017070901063\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020405056532886292\n",
      "Average test loss: 0.002804384955101543\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020359113317396905\n",
      "Average test loss: 0.0027948433104902507\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020289648034506374\n",
      "Average test loss: 0.002774816691875458\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020223378782471022\n",
      "Average test loss: 0.0028159044883731338\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020174041176835696\n",
      "Average test loss: 0.0028241712527556553\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020095578425460392\n",
      "Average test loss: 0.0027878405352433524\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02007991830507914\n",
      "Average test loss: 0.002737054854631424\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020002635137902364\n",
      "Average test loss: 0.0027268645713726678\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019960972274343174\n",
      "Average test loss: 0.00273630764004257\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019930854246020317\n",
      "Average test loss: 0.0027317917137924166\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019901739274462066\n",
      "Average test loss: 0.003689787520923548\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01984639765073856\n",
      "Average test loss: 0.0027175074635694425\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019808672782447603\n",
      "Average test loss: 0.0027177508009804618\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019737272928158443\n",
      "Average test loss: 0.0027065434952576956\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019729657716221278\n",
      "Average test loss: 0.002739387836928169\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019690603610542087\n",
      "Average test loss: 0.0027059230560229886\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019662679985165597\n",
      "Average test loss: 0.0027373003262198634\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01962452209326956\n",
      "Average test loss: 0.0027179702023665112\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019618751987814903\n",
      "Average test loss: 0.0027135856724861593\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01961747840874725\n",
      "Average test loss: 0.0026764202925066154\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019525438245799807\n",
      "Average test loss: 0.0027167869431691037\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019498417985108162\n",
      "Average test loss: 0.002706934054692586\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019489034199052388\n",
      "Average test loss: 0.0030786403748724197\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019450268907679452\n",
      "Average test loss: 0.0027067793257948424\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019434448497162925\n",
      "Average test loss: 0.002706414140553938\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019407146202193366\n",
      "Average test loss: 0.0026996561056002975\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019273855664663844\n",
      "Average test loss: 0.002703157101654344\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019266515307956273\n",
      "Average test loss: 0.002688237759595116\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019226755652162763\n",
      "Average test loss: 0.0026650360429452524\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019235472915901077\n",
      "Average test loss: 0.002654963037619988\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019218622512287564\n",
      "Average test loss: 0.0026726956975956758\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01917553358276685\n",
      "Average test loss: 0.0026475664713523452\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019136678379442957\n",
      "Average test loss: 0.0026592832969294655\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019159866153366036\n",
      "Average test loss: 0.002669970182598465\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019134550783369276\n",
      "Average test loss: 0.0028328588578022187\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01912243719564544\n",
      "Average test loss: 0.0026763227739267877\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01909805810617076\n",
      "Average test loss: 0.0026698769568983052\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01907353931830989\n",
      "Average test loss: 0.0026450699433270427\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01907432450602452\n",
      "Average test loss: 0.002686604010562102\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019043874715765317\n",
      "Average test loss: 0.0026877998086727328\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019039990146954854\n",
      "Average test loss: 0.004176149003000723\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01902677818470531\n",
      "Average test loss: 0.002641171165431539\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01899837873213821\n",
      "Average test loss: 0.0026501812388499578\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01899095522529549\n",
      "Average test loss: 0.0026524057866384584\n",
      "Epoch 82/300\n",
      "Average training loss: 0.018984008558922343\n",
      "Average test loss: 0.002674511557031009\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018970547748936546\n",
      "Average test loss: 0.002633428524558743\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01893848814898067\n",
      "Average test loss: 0.0026492836407075327\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018943509929709966\n",
      "Average test loss: 0.0026450677230540247\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018917489098178017\n",
      "Average test loss: 0.002630357715404696\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018902021475964124\n",
      "Average test loss: 0.002652066811505291\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01889789037572013\n",
      "Average test loss: 0.002668009390226669\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01889233887195587\n",
      "Average test loss: 0.002643526757756869\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018866593667202527\n",
      "Average test loss: 0.0031777301941894823\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018870365525285402\n",
      "Average test loss: 0.0026516276705596184\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018848255793253582\n",
      "Average test loss: 0.0026254937919891544\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018843653309676384\n",
      "Average test loss: 0.00262881749537256\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01882014609211021\n",
      "Average test loss: 0.002645344748368694\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018809649737344846\n",
      "Average test loss: 0.002678960111613075\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018811358200179207\n",
      "Average test loss: 0.002655080970790651\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018813124236133363\n",
      "Average test loss: 0.0026496844295826223\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01880045210984018\n",
      "Average test loss: 0.002658778469595644\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01875934062567022\n",
      "Average test loss: 0.002744416703987453\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018761326622631816\n",
      "Average test loss: 0.002700470834142632\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018760442758599918\n",
      "Average test loss: 0.0026314973363445863\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018745924113525284\n",
      "Average test loss: 0.002677814836303393\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018737534656292864\n",
      "Average test loss: 0.00262765331649118\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018728508323431016\n",
      "Average test loss: 0.0026389795353429185\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018718259962068665\n",
      "Average test loss: 0.003530376149962346\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018705749995178645\n",
      "Average test loss: 0.002625631340055002\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018712948772642346\n",
      "Average test loss: 0.0026140987227360407\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018675513315531942\n",
      "Average test loss: 0.002638887093298965\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018684247127837606\n",
      "Average test loss: 0.0026200998632444276\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018673107917937967\n",
      "Average test loss: 0.0026309654770625963\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01866013274424606\n",
      "Average test loss: 0.0026358439445288645\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018635769544376266\n",
      "Average test loss: 0.0026529921216683254\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018651040174894863\n",
      "Average test loss: 0.00264978628905697\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018643533946739302\n",
      "Average test loss: 0.0026436317306425838\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018633687102132375\n",
      "Average test loss: 0.0026307888908518686\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0186157686892483\n",
      "Average test loss: 0.0026894503026786776\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01861606435643302\n",
      "Average test loss: 0.0026115008542935054\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018594801555905077\n",
      "Average test loss: 0.002622905547006263\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01859063276151816\n",
      "Average test loss: 0.0026290830394460093\n",
      "Epoch 120/300\n",
      "Average training loss: 0.018577989576591385\n",
      "Average test loss: 0.002610279452469614\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01857122153540452\n",
      "Average test loss: 0.0026253880626625485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018562064402633242\n",
      "Average test loss: 0.0026765249971714283\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018545475817388957\n",
      "Average test loss: 0.0026209694767991704\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018552845095594725\n",
      "Average test loss: 0.002615055771751536\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01854324904580911\n",
      "Average test loss: 0.01788575800259908\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018597131338384417\n",
      "Average test loss: 0.002629704113221831\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01851756189763546\n",
      "Average test loss: 0.0026657302278197473\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018518562780486213\n",
      "Average test loss: 0.0026348351306385463\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01851038629975584\n",
      "Average test loss: 0.002631225573726826\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01851126866870456\n",
      "Average test loss: 0.002616140509645144\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018482295345928934\n",
      "Average test loss: 0.002642634330317378\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018502606530984244\n",
      "Average test loss: 0.0026123919991983308\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018474607524772486\n",
      "Average test loss: 0.002653062977931566\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018482728694876036\n",
      "Average test loss: 0.002618914183229208\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018463253998094134\n",
      "Average test loss: 0.002620298342158397\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018455755619539153\n",
      "Average test loss: 0.0026370718161472014\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018446539238095284\n",
      "Average test loss: 0.0026215518733693492\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01844956562585301\n",
      "Average test loss: 0.002626853197813034\n",
      "Epoch 139/300\n",
      "Average training loss: 0.018449596958027947\n",
      "Average test loss: 0.002616624395052592\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01843354741566711\n",
      "Average test loss: 0.0026244137295418314\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018430333958731757\n",
      "Average test loss: 0.002809922720409102\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018408227294683457\n",
      "Average test loss: 0.0026243819981399507\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01842433801541726\n",
      "Average test loss: 0.002930301004399856\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018400823192463982\n",
      "Average test loss: 0.002632228977771269\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018401146408584382\n",
      "Average test loss: 0.002606758785744508\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018404848198095958\n",
      "Average test loss: 0.002625887736885084\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018424046122365527\n",
      "Average test loss: 0.0026317485492262575\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018372302409675387\n",
      "Average test loss: 0.002695275095498396\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018382998682558535\n",
      "Average test loss: 0.002609918055848943\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01838902759220865\n",
      "Average test loss: 0.002618130097579625\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01835863926096095\n",
      "Average test loss: 0.002658118248813682\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018362707123160363\n",
      "Average test loss: 0.0026235273215505814\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018353727829125192\n",
      "Average test loss: 0.002633289142822226\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018343632436460918\n",
      "Average test loss: 0.0026043097310596043\n",
      "Epoch 155/300\n",
      "Average training loss: 0.018336861767702634\n",
      "Average test loss: 0.002619437151071098\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018334881618618966\n",
      "Average test loss: 0.0027418823884800075\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018336469787690376\n",
      "Average test loss: 0.0026120176491224102\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01831658797297213\n",
      "Average test loss: 0.0026280911039147114\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01833428109023306\n",
      "Average test loss: 0.002604778709097041\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018318888677491083\n",
      "Average test loss: 0.002683783640257186\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01831878772046831\n",
      "Average test loss: 0.002630050356603331\n",
      "Epoch 162/300\n",
      "Average training loss: 0.018296227871543832\n",
      "Average test loss: 0.0026136768398185573\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01829854928950469\n",
      "Average test loss: 0.002607737082781063\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018288028195500374\n",
      "Average test loss: 0.0026007912637044986\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01828903372420205\n",
      "Average test loss: 0.0026197723630401824\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01828643481267823\n",
      "Average test loss: 0.0026107742186221813\n",
      "Epoch 167/300\n",
      "Average training loss: 0.018262300627099142\n",
      "Average test loss: 0.0026142622234506738\n",
      "Epoch 168/300\n",
      "Average training loss: 0.018286231483022372\n",
      "Average test loss: 0.0026510195725907883\n",
      "Epoch 169/300\n",
      "Average training loss: 0.018268764533102513\n",
      "Average test loss: 0.0026040694173425434\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0182638504770067\n",
      "Average test loss: 0.0026226240007413758\n",
      "Epoch 171/300\n",
      "Average training loss: 0.018238632180624537\n",
      "Average test loss: 0.0027877537683687276\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018257120301326115\n",
      "Average test loss: 0.002620369695747892\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018244513152374162\n",
      "Average test loss: 0.0026120893862098455\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01824524795015653\n",
      "Average test loss: 0.002603530512501796\n",
      "Epoch 175/300\n",
      "Average training loss: 0.018243794797195328\n",
      "Average test loss: 0.0026207601334899662\n",
      "Epoch 176/300\n",
      "Average training loss: 0.018232822850346567\n",
      "Average test loss: 0.002616405496166812\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018222343198127218\n",
      "Average test loss: 0.0026121136392984125\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018220187657409244\n",
      "Average test loss: 0.002600137648069196\n",
      "Epoch 179/300\n",
      "Average training loss: 0.018196158470378983\n",
      "Average test loss: 0.0026022143450876076\n",
      "Epoch 180/300\n",
      "Average training loss: 0.018215172903405297\n",
      "Average test loss: 0.002614453229432305\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018211436150802507\n",
      "Average test loss: 0.002605635881837871\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01821616138352288\n",
      "Average test loss: 0.002656948481996854\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01819627619617515\n",
      "Average test loss: 0.0026553464393234915\n",
      "Epoch 184/300\n",
      "Average training loss: 0.018193702719277807\n",
      "Average test loss: 0.002604097818541858\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01818619876437717\n",
      "Average test loss: 0.0026105847789181604\n",
      "Epoch 186/300\n",
      "Average training loss: 0.018199357289406987\n",
      "Average test loss: 0.0026118110494895114\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018191316508584552\n",
      "Average test loss: 0.0026215577425642146\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018175879060394234\n",
      "Average test loss: 0.002818854911459817\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018163449909951952\n",
      "Average test loss: 0.007544334611131086\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01818113095064958\n",
      "Average test loss: 0.0026214173753849332\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01816716592344973\n",
      "Average test loss: 0.002652345763829847\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01816024224128988\n",
      "Average test loss: 0.0026166809228145415\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018148427890406715\n",
      "Average test loss: 0.0026015821997490193\n",
      "Epoch 194/300\n",
      "Average training loss: 0.018149241063329907\n",
      "Average test loss: 0.0026067220856332116\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01816210422085391\n",
      "Average test loss: 0.002599286276847124\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018147883666886223\n",
      "Average test loss: 0.0026085512002723085\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01814971412387159\n",
      "Average test loss: 0.0026097511804352206\n",
      "Epoch 198/300\n",
      "Average training loss: 0.018130690215362445\n",
      "Average test loss: 0.0026069868958244723\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018120266315009858\n",
      "Average test loss: 0.002609243077122503\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018128699199193053\n",
      "Average test loss: 0.0026135831233114\n",
      "Epoch 201/300\n",
      "Average training loss: 0.018116956798566713\n",
      "Average test loss: 0.0026428717530022064\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018127641012271246\n",
      "Average test loss: 0.0026132965837087894\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018107497240106265\n",
      "Average test loss: 0.0026128527780787812\n",
      "Epoch 204/300\n",
      "Average training loss: 0.018097493469715117\n",
      "Average test loss: 0.002659292647201154\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01811862555808491\n",
      "Average test loss: 0.0026034513213154344\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01809656293027931\n",
      "Average test loss: 0.0026648653062681356\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01819511149989234\n",
      "Average test loss: 0.0026139554565565454\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018083277833958467\n",
      "Average test loss: 0.0026013729828927253\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018083002577225366\n",
      "Average test loss: 0.0041250457018613814\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018077606196204823\n",
      "Average test loss: 0.0026090848323785595\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01808530163268248\n",
      "Average test loss: 0.0026142449397593736\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01807244656317764\n",
      "Average test loss: 0.0026387793498320712\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01806271717780166\n",
      "Average test loss: 0.0026666108017994296\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01807140247275432\n",
      "Average test loss: 0.0026569068937872847\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018059889644384385\n",
      "Average test loss: 0.0026087102834135294\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01806521428293652\n",
      "Average test loss: 0.004647429540339443\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01806957310438156\n",
      "Average test loss: 0.002597007275041607\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018051129115952386\n",
      "Average test loss: 0.002618428300341798\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018072854277160433\n",
      "Average test loss: 0.0026072693481627436\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018044887653655476\n",
      "Average test loss: 0.0026269546175996463\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018050681137376363\n",
      "Average test loss: 0.0026125936249477995\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018034584037131735\n",
      "Average test loss: 0.00263102852842874\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01803001849187745\n",
      "Average test loss: 0.0026086658984422685\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018035618028706974\n",
      "Average test loss: 0.002616268318353428\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01802282008032004\n",
      "Average test loss: 0.0026090384184693297\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01801991004579597\n",
      "Average test loss: 0.0025979533806029294\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018022472189532387\n",
      "Average test loss: 0.0026373930441008675\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018024877281652557\n",
      "Average test loss: 0.0026116516992656723\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018009188547730447\n",
      "Average test loss: 0.0026136956413586932\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018015251161323652\n",
      "Average test loss: 0.002608571726621853\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018005738788180882\n",
      "Average test loss: 0.0026172380850960813\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018011372657285796\n",
      "Average test loss: 0.0028590554491513304\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01800506988581684\n",
      "Average test loss: 0.0026000179826385444\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017991234709819157\n",
      "Average test loss: 0.002627770091407001\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01799513899617725\n",
      "Average test loss: 0.0026116824104554124\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01800330273889833\n",
      "Average test loss: 0.0026044987531171904\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01798360876407888\n",
      "Average test loss: 0.0027088720142427417\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01798168456554413\n",
      "Average test loss: 0.0026208217858026423\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017987961322069167\n",
      "Average test loss: 0.002633221916026539\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017987930956814023\n",
      "Average test loss: 0.0026102873854753042\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01798028042415778\n",
      "Average test loss: 0.002628606997223364\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0179657526910305\n",
      "Average test loss: 0.002705134067684412\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0179719002428982\n",
      "Average test loss: 0.002614903152402904\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01797190749148528\n",
      "Average test loss: 0.002604120115749538\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017946626133388942\n",
      "Average test loss: 0.0026205737739801406\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017961752944522433\n",
      "Average test loss: 0.0026160268944998583\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01796319555822346\n",
      "Average test loss: 0.002611367377038631\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017959394390384357\n",
      "Average test loss: 0.0027807416261898146\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017946718555357722\n",
      "Average test loss: 0.0026568736425704427\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017951813365850184\n",
      "Average test loss: 0.0026060308288368915\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017942036271095276\n",
      "Average test loss: 0.002599025970324874\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017942167941066955\n",
      "Average test loss: 0.002655607717525628\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01794671058323648\n",
      "Average test loss: 0.0029985534962680607\n",
      "Epoch 254/300\n",
      "Average training loss: 0.017934687036607\n",
      "Average test loss: 0.002635412926061286\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017926568559474415\n",
      "Average test loss: 0.0026160453711118964\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01792995760507054\n",
      "Average test loss: 0.002616354748710162\n",
      "Epoch 257/300\n",
      "Average training loss: 0.017932764162619907\n",
      "Average test loss: 0.0026029112707409595\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01791266413529714\n",
      "Average test loss: 0.0026723077557981013\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017928907594747013\n",
      "Average test loss: 0.0026295349662088688\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017921031670437917\n",
      "Average test loss: 0.002615120900173982\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017918257741464508\n",
      "Average test loss: 0.0026016092019983464\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017900379003749953\n",
      "Average test loss: 0.002632948377273149\n",
      "Epoch 263/300\n",
      "Average training loss: 0.017905440146724384\n",
      "Average test loss: 0.0025929974869100584\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017935551434755326\n",
      "Average test loss: 0.002603891566188799\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017901758703920575\n",
      "Average test loss: 0.002605840563567148\n",
      "Epoch 266/300\n",
      "Average training loss: 0.017889809557133252\n",
      "Average test loss: 0.002600545307207439\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017899235018425518\n",
      "Average test loss: 0.0026116483209447728\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017893988450368246\n",
      "Average test loss: 0.002665705487339033\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017891706297794976\n",
      "Average test loss: 0.002662147065831555\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017879298023051687\n",
      "Average test loss: 0.002621751299955779\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01788936669793394\n",
      "Average test loss: 0.002619226501840684\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017886986209286585\n",
      "Average test loss: 0.0025974951634804406\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0178942162460751\n",
      "Average test loss: 0.0026330516756408744\n",
      "Epoch 274/300\n",
      "Average training loss: 0.017866114993890125\n",
      "Average test loss: 0.0026110638015800053\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017867883142497803\n",
      "Average test loss: 0.002611095004197624\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01786403379837672\n",
      "Average test loss: 0.002605483581415481\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017864960427085557\n",
      "Average test loss: 0.002607034821684162\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01785881368484762\n",
      "Average test loss: 0.002604814335082968\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017868671771552828\n",
      "Average test loss: 0.002655707023727397\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017863856034146416\n",
      "Average test loss: 0.0026003242718676727\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01784976006216473\n",
      "Average test loss: 0.002602699463152223\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017857925117015838\n",
      "Average test loss: 0.0026459665853116247\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017861488829056422\n",
      "Average test loss: 0.0026269411544005074\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017857100945379997\n",
      "Average test loss: 0.0026466852747317818\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017839127559628753\n",
      "Average test loss: 0.002608704192770852\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01784740260243416\n",
      "Average test loss: 0.0025943068015492625\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017849291477766302\n",
      "Average test loss: 0.002620501542670859\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017841622869173686\n",
      "Average test loss: 0.0026683203068872292\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017833220620950063\n",
      "Average test loss: 0.0026014710491937066\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017837042139636147\n",
      "Average test loss: 0.00261835123391615\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017836399369769627\n",
      "Average test loss: 0.002646242316812277\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01782873072557979\n",
      "Average test loss: 0.002639834802597761\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017834198446737396\n",
      "Average test loss: 0.002616005044016573\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01782408645004034\n",
      "Average test loss: 0.0027947562616318466\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017813759518994225\n",
      "Average test loss: 0.0028154797760976684\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017822640107737646\n",
      "Average test loss: 0.0026092423351688518\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01781806472937266\n",
      "Average test loss: 0.00260655960916645\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017818130392167304\n",
      "Average test loss: 0.0026151768914941284\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017805103422039083\n",
      "Average test loss: 0.0026631759197140734\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017816910475492477\n",
      "Average test loss: 0.00268227054282195\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth3/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.75\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.35\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.83\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.16\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.35\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.69\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.81\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.60\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.77\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.80\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.33\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.3459947907394834\n",
      "Average test loss: 0.0125288162910276\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2997870032389959\n",
      "Average test loss: 0.010775690217812856\n",
      "Epoch 3/300\n",
      "Average training loss: 0.23044689456621806\n",
      "Average test loss: 0.010410836333201992\n",
      "Epoch 4/300\n",
      "Average training loss: 0.20159962048795488\n",
      "Average test loss: 0.00921963002284368\n",
      "Epoch 5/300\n",
      "Average training loss: 0.18489862977133856\n",
      "Average test loss: 0.008933491490781308\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1736775727669398\n",
      "Average test loss: 0.009391500785946846\n",
      "Epoch 7/300\n",
      "Average training loss: 0.16556002485752105\n",
      "Average test loss: 0.008891558445162243\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15999583201275933\n",
      "Average test loss: 0.009749198113050726\n",
      "Epoch 9/300\n",
      "Average training loss: 0.154166055560112\n",
      "Average test loss: 0.008512370507336326\n",
      "Epoch 10/300\n",
      "Average training loss: 0.149682834704717\n",
      "Average test loss: 0.00795588823987378\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14609274543656242\n",
      "Average test loss: 0.007873504031035636\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14270707533094618\n",
      "Average test loss: 0.007917005248367786\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1393381843301985\n",
      "Average test loss: 0.007885715894401074\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1361905062728458\n",
      "Average test loss: 0.008623905383050441\n",
      "Epoch 15/300\n",
      "Average training loss: 0.13389913006623586\n",
      "Average test loss: 0.00831291372080644\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13226229764355554\n",
      "Average test loss: 0.007347111347234912\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1299769820239809\n",
      "Average test loss: 0.007587725122769674\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1278923353486591\n",
      "Average test loss: 0.007832355915672249\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12630353760719298\n",
      "Average test loss: 0.007122049307657613\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12503891670041614\n",
      "Average test loss: 0.007403254894332753\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12366576139132182\n",
      "Average test loss: 0.0073920305826597745\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12231797721650865\n",
      "Average test loss: 0.00730446246970031\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1211770929760403\n",
      "Average test loss: 0.007175924677815702\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12001383117834727\n",
      "Average test loss: 0.007098590551565091\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11922550792826546\n",
      "Average test loss: 0.007420598144332568\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11797410721911325\n",
      "Average test loss: 0.006908900543633434\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11747779811753167\n",
      "Average test loss: 0.0069098448132475215\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11668352011839549\n",
      "Average test loss: 0.00687952544954088\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11554897693792979\n",
      "Average test loss: 0.0072864273521635265\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11500265338023503\n",
      "Average test loss: 0.006885448244296842\n",
      "Epoch 31/300\n",
      "Average training loss: 0.11420673223336537\n",
      "Average test loss: 0.0068843754459586405\n",
      "Epoch 32/300\n",
      "Average training loss: 0.11356145303116905\n",
      "Average test loss: 0.006748868157466253\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11319919539822472\n",
      "Average test loss: 0.006801032897084952\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11264281840456856\n",
      "Average test loss: 0.36257415408558313\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11206063343418969\n",
      "Average test loss: 0.006792452346947458\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11178331167168087\n",
      "Average test loss: 0.007302555135968659\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11113120877080493\n",
      "Average test loss: 0.006660266058312522\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11074251630571154\n",
      "Average test loss: 0.00698716958405243\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11018277047077815\n",
      "Average test loss: 0.00658613451404704\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11013694018125535\n",
      "Average test loss: 0.007112074575904343\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1096149045560095\n",
      "Average test loss: 0.006705916601336665\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10946190411514706\n",
      "Average test loss: 0.008001197699043486\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10900203306145138\n",
      "Average test loss: 0.007180830742749903\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10848477274179459\n",
      "Average test loss: 0.0066628539512554805\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10845273156960805\n",
      "Average test loss: 0.006762101031839848\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10799879354900783\n",
      "Average test loss: 0.0065737129155960345\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10770076116588381\n",
      "Average test loss: 0.006658181010021104\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10739369393719567\n",
      "Average test loss: 0.0066147414886703094\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10704421340094672\n",
      "Average test loss: 0.0065250395954483086\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10780101494656669\n",
      "Average test loss: 0.007639904263532824\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10674869424435827\n",
      "Average test loss: 0.006590763170272112\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10647269439034991\n",
      "Average test loss: 0.009203810629745325\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10605386271079381\n",
      "Average test loss: 0.006543739270004961\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10607513503233591\n",
      "Average test loss: 0.0067344746208853196\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10578150180313323\n",
      "Average test loss: 0.0064833348381022615\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10562846596373453\n",
      "Average test loss: 0.006508514931632413\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10547363603777356\n",
      "Average test loss: 0.006663694620960289\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10543760119544135\n",
      "Average test loss: 0.006499783600370089\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10492648579676946\n",
      "Average test loss: 0.00683430071754588\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10474314237965478\n",
      "Average test loss: 0.006808493885315126\n",
      "Epoch 61/300\n",
      "Average training loss: 0.10459588746229807\n",
      "Average test loss: 0.0064450224277873835\n",
      "Epoch 62/300\n",
      "Average training loss: 0.1043681433002154\n",
      "Average test loss: 0.0065039294407599505\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1092168643143442\n",
      "Average test loss: 0.007088815642727746\n",
      "Epoch 64/300\n",
      "Average training loss: 0.10433132959074444\n",
      "Average test loss: 0.0065191416016055475\n",
      "Epoch 65/300\n",
      "Average training loss: 0.104121364083555\n",
      "Average test loss: 0.07057718831631872\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10418202913469739\n",
      "Average test loss: 0.006661884155538347\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10401732740799587\n",
      "Average test loss: 0.0064640175236596\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10361987298727035\n",
      "Average test loss: 0.0065039631782306566\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10321328510840734\n",
      "Average test loss: 0.006465153050091531\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10360933150847752\n",
      "Average test loss: 0.006802677976174487\n",
      "Epoch 71/300\n",
      "Average training loss: 0.10300803911685943\n",
      "Average test loss: 0.006696449392371707\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10294415354066425\n",
      "Average test loss: 0.006689610917121172\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10300313156843185\n",
      "Average test loss: 0.006471140944295459\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10299353337950176\n",
      "Average test loss: 0.006764378767874506\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1026390756699774\n",
      "Average test loss: 0.1568454144431485\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10241352441575792\n",
      "Average test loss: 0.006811927487452825\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1023765551050504\n",
      "Average test loss: 0.31317323353555465\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1023242067164845\n",
      "Average test loss: 0.0077738522734079095\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10236995470523834\n",
      "Average test loss: 0.006516750674280855\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10190460608402888\n",
      "Average test loss: 0.006500547875132825\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10165797418355942\n",
      "Average test loss: 0.006478809469276005\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10180206274986267\n",
      "Average test loss: 0.006413578742494186\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10155552740891774\n",
      "Average test loss: 0.006656989505721463\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1015532191991806\n",
      "Average test loss: 64.2001385260688\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10132256421115664\n",
      "Average test loss: 0.00662130917608738\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10122934135463503\n",
      "Average test loss: 0.0066325880512595175\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10150863029559454\n",
      "Average test loss: 0.0066041228150328\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10105617078807619\n",
      "Average test loss: 0.00659407777711749\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10087164645062553\n",
      "Average test loss: 0.006429685492896371\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10097354129950205\n",
      "Average test loss: 0.006454093221988943\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10154820053444968\n",
      "Average test loss: 0.0066212022718456056\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10081404678026835\n",
      "Average test loss: 0.0064017235429750545\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10042382601896922\n",
      "Average test loss: 0.006386394939488835\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10051810502343708\n",
      "Average test loss: 0.006519745665291945\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10040532754527198\n",
      "Average test loss: 0.006456487823691633\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10009876369105446\n",
      "Average test loss: 0.006553610009451707\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10151416209008958\n",
      "Average test loss: 0.006753091022786167\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10014199013180203\n",
      "Average test loss: 0.011659564737023578\n",
      "Epoch 99/300\n",
      "Average training loss: 0.09987510245376163\n",
      "Average test loss: 0.0064140146341588765\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0998430423869027\n",
      "Average test loss: 0.00652955974969599\n",
      "Epoch 101/300\n",
      "Average training loss: 0.09976410843266381\n",
      "Average test loss: 0.006428012020058102\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10027150227626165\n",
      "Average test loss: 0.006455051889436112\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09943525836865107\n",
      "Average test loss: 0.006478570515910784\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09954338675075107\n",
      "Average test loss: 0.006613082493758864\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0994065622554885\n",
      "Average test loss: 0.006431492934003472\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09952581949366464\n",
      "Average test loss: 0.00640428706838025\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09935974438322916\n",
      "Average test loss: 0.006720638709349765\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09950508626302083\n",
      "Average test loss: 0.006723275783575244\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09906828173663881\n",
      "Average test loss: 0.0064129407496915924\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09916323310799069\n",
      "Average test loss: 0.006416345663368702\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09891371746195687\n",
      "Average test loss: 0.006448414327783717\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09915037531322902\n",
      "Average test loss: 0.006507535292870468\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09871200996637344\n",
      "Average test loss: 0.009544143926766184\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09858192455768586\n",
      "Average test loss: 0.006570927598410182\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09871545087628894\n",
      "Average test loss: 0.0069465752347475955\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09861968212657504\n",
      "Average test loss: 0.006517792885916101\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09870073046949175\n",
      "Average test loss: 0.006390711793469058\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09878202573458354\n",
      "Average test loss: 0.006400620575166411\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09909530995951758\n",
      "Average test loss: 0.0067689076707594924\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09815204458766513\n",
      "Average test loss: 0.008262615785002708\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09808794120285245\n",
      "Average test loss: 0.0064965530654622445\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09787453226910697\n",
      "Average test loss: 0.00715659802324242\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09800684989823236\n",
      "Average test loss: 0.007269516064061059\n",
      "Epoch 124/300\n",
      "Average training loss: 0.098941773030493\n",
      "Average test loss: 0.006638744663033221\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09754921059476004\n",
      "Average test loss: 0.006565549708902836\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09783695195118586\n",
      "Average test loss: 0.0064804278272721505\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09767513781785965\n",
      "Average test loss: 0.006428156251708667\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0977618820004993\n",
      "Average test loss: 0.006938070060478316\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0975298685365253\n",
      "Average test loss: 0.0068585666736794846\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09725263304842843\n",
      "Average test loss: 0.006640135381784704\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09735278911060757\n",
      "Average test loss: 0.006434001889907652\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09748008524047004\n",
      "Average test loss: 0.006660138295756446\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0974138775004281\n",
      "Average test loss: 0.006556146846049362\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0972330496708552\n",
      "Average test loss: 0.006528281716008981\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09747140309545729\n",
      "Average test loss: 0.006427772158549892\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09705366432004504\n",
      "Average test loss: 0.00643392120839821\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09704509492052926\n",
      "Average test loss: 0.006511674626833863\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09705537284082837\n",
      "Average test loss: 0.006487650986760855\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09674854747454326\n",
      "Average test loss: 0.006397440486484104\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09751069476207097\n",
      "Average test loss: 0.007309393749468857\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09677723186545902\n",
      "Average test loss: 0.007809765430788199\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09655890996588601\n",
      "Average test loss: 0.006797710854974058\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09660308436552684\n",
      "Average test loss: 0.006617510192096233\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09661185622215271\n",
      "Average test loss: 0.0065673316826836935\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09645256904760996\n",
      "Average test loss: 0.007795343509564797\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09631865257687039\n",
      "Average test loss: 0.006542469290395578\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09635281467437744\n",
      "Average test loss: 0.006666888054046366\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09967460072702831\n",
      "Average test loss: 0.006455988941921128\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09622632859150569\n",
      "Average test loss: 0.006729124977356857\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09599164590570662\n",
      "Average test loss: 0.006427214914725887\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09617984588940938\n",
      "Average test loss: 0.006563301710618866\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09599386570188734\n",
      "Average test loss: 0.006674341021726529\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0959071387582355\n",
      "Average test loss: 0.008012984242704179\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09601651149988175\n",
      "Average test loss: 0.006544925161533885\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09585865472422705\n",
      "Average test loss: 0.007005954532159699\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09602589073446062\n",
      "Average test loss: 0.006480859870711962\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0962531080643336\n",
      "Average test loss: 0.00902446751958794\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0958899760974778\n",
      "Average test loss: 0.006488594433913628\n",
      "Epoch 159/300\n",
      "Average training loss: 1.178658274418778\n",
      "Average test loss: 0.008107716406385104\n",
      "Epoch 160/300\n",
      "Average training loss: 17.86351414542728\n",
      "Average test loss: 0.010751848999824789\n",
      "Epoch 161/300\n",
      "Average training loss: 2.436857269499037\n",
      "Average test loss: 0.008543639410287142\n",
      "Epoch 162/300\n",
      "Average training loss: 1.9604299825032552\n",
      "Average test loss: 0.008384477607491943\n",
      "Epoch 163/300\n",
      "Average training loss: 1.6099036713706123\n",
      "Average test loss: 0.00808548170617885\n",
      "Epoch 164/300\n",
      "Average training loss: 1.3105187798606024\n",
      "Average test loss: 0.013606692176726129\n",
      "Epoch 165/300\n",
      "Average training loss: 1.0445572979185316\n",
      "Average test loss: 0.007763445775128073\n",
      "Epoch 166/300\n",
      "Average training loss: 0.8119518180953131\n",
      "Average test loss: 0.007444766136507193\n",
      "Epoch 167/300\n",
      "Average training loss: 0.6474592708481682\n",
      "Average test loss: 0.0077184071354568\n",
      "Epoch 168/300\n",
      "Average training loss: 0.5217666438950432\n",
      "Average test loss: 0.014232348587777879\n",
      "Epoch 169/300\n",
      "Average training loss: 0.4251531314055125\n",
      "Average test loss: 0.00755204911240273\n",
      "Epoch 170/300\n",
      "Average training loss: 0.34586300751898025\n",
      "Average test loss: 0.0070283108842041754\n",
      "Epoch 171/300\n",
      "Average training loss: 0.2804939692550235\n",
      "Average test loss: 0.007473412061731021\n",
      "Epoch 172/300\n",
      "Average training loss: 0.2303168105284373\n",
      "Average test loss: 0.006813651234325435\n",
      "Epoch 173/300\n",
      "Average training loss: 0.19598338680797153\n",
      "Average test loss: 0.0068008509212070046\n",
      "Epoch 174/300\n",
      "Average training loss: 0.17423279967572955\n",
      "Average test loss: 0.007117310668031375\n",
      "Epoch 175/300\n",
      "Average training loss: 0.15944607871770858\n",
      "Average test loss: 0.006871838773290316\n",
      "Epoch 176/300\n",
      "Average training loss: 0.14874440211057663\n",
      "Average test loss: 0.006689983417176538\n",
      "Epoch 177/300\n",
      "Average training loss: 0.13995000490877363\n",
      "Average test loss: 0.006776978193057908\n",
      "Epoch 178/300\n",
      "Average training loss: 0.13366378235816956\n",
      "Average test loss: 0.006508719894621108\n",
      "Epoch 179/300\n",
      "Average training loss: 0.12851819764905506\n",
      "Average test loss: 0.006686679855402973\n",
      "Epoch 180/300\n",
      "Average training loss: 0.12494364914629194\n",
      "Average test loss: 0.006771621697064903\n",
      "Epoch 181/300\n",
      "Average training loss: 0.12102615160412258\n",
      "Average test loss: 0.006523819877041711\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11794173280398051\n",
      "Average test loss: 5949.362794704861\n",
      "Epoch 183/300\n",
      "Average training loss: 0.11562826798359553\n",
      "Average test loss: 0.006462217494017548\n",
      "Epoch 184/300\n",
      "Average training loss: 0.11331207095914417\n",
      "Average test loss: 0.006482716322359112\n",
      "Epoch 185/300\n",
      "Average training loss: 0.11138880525694952\n",
      "Average test loss: 0.006797061233884758\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10955051896969477\n",
      "Average test loss: 0.006427923272881243\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10812517036994299\n",
      "Average test loss: 0.006382098050167163\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10658115055163701\n",
      "Average test loss: 0.0065734119158652096\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10508885286913977\n",
      "Average test loss: 0.006436884098996718\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10361453593439526\n",
      "Average test loss: 0.006381943927043014\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10253053294287788\n",
      "Average test loss: 0.01062545682862401\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10116517862346437\n",
      "Average test loss: 0.0063677001798318495\n",
      "Epoch 193/300\n",
      "Average training loss: 0.10010074299573898\n",
      "Average test loss: 0.006404258272300164\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09924429276254441\n",
      "Average test loss: 0.007097045630216598\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09844818176825841\n",
      "Average test loss: 0.0064784044747551285\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0978481968111462\n",
      "Average test loss: 0.0064052327589856255\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09762888581222957\n",
      "Average test loss: 0.006507630894581477\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09738792785671022\n",
      "Average test loss: 0.006477598747031557\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09699879297282961\n",
      "Average test loss: 0.006396756377485063\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0966011484530237\n",
      "Average test loss: 0.006734662195874585\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09645021735297309\n",
      "Average test loss: 0.0069883128437731\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09623848281966316\n",
      "Average test loss: 0.006521646507912212\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09624866722689734\n",
      "Average test loss: 0.0065310772843658925\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09600469051467048\n",
      "Average test loss: 0.006707866292860773\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09633003968000411\n",
      "Average test loss: 0.006556416001998716\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09622126570012834\n",
      "Average test loss: 0.006900838728994131\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09606560603777567\n",
      "Average test loss: 0.0071528576910495755\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09558530670404435\n",
      "Average test loss: 0.006425038428770171\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09578204458289677\n",
      "Average test loss: 0.006432616848912504\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09555157723029455\n",
      "Average test loss: 0.006442100216531091\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09545911302169165\n",
      "Average test loss: 0.007026450050373872\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09529270678758621\n",
      "Average test loss: 0.0064288476631045345\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09530970829062992\n",
      "Average test loss: 0.006487145507915152\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09519576815101835\n",
      "Average test loss: 0.006464422894020876\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09511046425501506\n",
      "Average test loss: 0.007024170023699602\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0950241915318701\n",
      "Average test loss: 0.006510820629696051\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09522590969668494\n",
      "Average test loss: 0.0064680601490868465\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09506340467267567\n",
      "Average test loss: 0.0065048933223717745\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09481087157461378\n",
      "Average test loss: 0.0064136503102878726\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09487212557262845\n",
      "Average test loss: 0.0065389724920193355\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09469706028037601\n",
      "Average test loss: 0.006679382740623421\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0945358247227139\n",
      "Average test loss: 0.006515558241970009\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09494702874951892\n",
      "Average test loss: 0.0284017552178767\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09434679983721839\n",
      "Average test loss: 0.006647947510911359\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0944647794233428\n",
      "Average test loss: 0.00663803237511052\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09431764271524218\n",
      "Average test loss: 0.0065152134990526574\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09451713504393895\n",
      "Average test loss: 0.006453717090189457\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09449872513612112\n",
      "Average test loss: 0.006638292010873556\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09444197558694416\n",
      "Average test loss: 0.006514835837814543\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09412100642919541\n",
      "Average test loss: 0.006607246425416735\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09406024519602457\n",
      "Average test loss: 0.006527827522406975\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09407546312941445\n",
      "Average test loss: 0.006676809790233771\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09416088722811805\n",
      "Average test loss: 0.006511110566142533\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09446834845013088\n",
      "Average test loss: 0.006535320706251594\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09442228309313457\n",
      "Average test loss: 0.019136472855591113\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0938767384092013\n",
      "Average test loss: 0.006532671300073465\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09413737744092941\n",
      "Average test loss: 0.006522699903282855\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09381157790952259\n",
      "Average test loss: 0.00694717774954107\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09365839385324054\n",
      "Average test loss: 0.006485179912712839\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09381510382890701\n",
      "Average test loss: 0.006621080469754007\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09495953281720479\n",
      "Average test loss: 0.006723410653571288\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09341908569468392\n",
      "Average test loss: 0.006627384958581792\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09360150729285346\n",
      "Average test loss: 0.006601505081686708\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09354191061523226\n",
      "Average test loss: 0.0066698638184203045\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0933293749358919\n",
      "Average test loss: 0.006558499243524339\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09343322830730014\n",
      "Average test loss: 0.006966896225180891\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09355904602342181\n",
      "Average test loss: 0.006586280502792862\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09324973277250925\n",
      "Average test loss: 0.006664408851000998\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09321683088276121\n",
      "Average test loss: 0.006681080070220762\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09307482184304132\n",
      "Average test loss: 0.006553049492753214\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09337018034193251\n",
      "Average test loss: 0.006496555344925986\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09343980710373985\n",
      "Average test loss: 0.006607601813971996\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09314294715722402\n",
      "Average test loss: 0.006619607517702712\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09294927201668421\n",
      "Average test loss: 0.3109986079931259\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09298111312919193\n",
      "Average test loss: 0.0157148718279269\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09292002070612378\n",
      "Average test loss: 0.006674524664878845\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09310106356276406\n",
      "Average test loss: 0.0065403866333266095\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09289817382891973\n",
      "Average test loss: 0.0066082169152796265\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09295964486731423\n",
      "Average test loss: 0.007010297309193346\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09285584850443734\n",
      "Average test loss: 0.0075998666766617035\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09275797600878609\n",
      "Average test loss: 0.006683375852389468\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09283601352903578\n",
      "Average test loss: 0.006808595532758368\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09269292582405939\n",
      "Average test loss: 0.01140772619512346\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0926632597843806\n",
      "Average test loss: 0.006635586576743259\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09257550975349214\n",
      "Average test loss: 0.006628308492402236\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09262453344795439\n",
      "Average test loss: 0.006678958000408279\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0924675535692109\n",
      "Average test loss: 0.0066092138530479535\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09246997874975205\n",
      "Average test loss: 0.009881433234446579\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09258449444505903\n",
      "Average test loss: 0.006714564034922255\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09249474690357844\n",
      "Average test loss: 0.0066139688297278355\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09224956503841612\n",
      "Average test loss: 0.008667456188963519\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09263879441552691\n",
      "Average test loss: 0.007936467847062482\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09217289810710483\n",
      "Average test loss: 0.0066867237765755915\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09217618257469601\n",
      "Average test loss: 0.006535540089425113\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09212579243050681\n",
      "Average test loss: 0.006811207990679476\n",
      "Epoch 276/300\n",
      "Average training loss: 1.7208886799414953\n",
      "Average test loss: 0.007627979566239648\n",
      "Epoch 277/300\n",
      "Average training loss: 0.49313384787241615\n",
      "Average test loss: 0.007442850942826934\n",
      "Epoch 278/300\n",
      "Average training loss: 0.35768719289037915\n",
      "Average test loss: 0.006934471151067151\n",
      "Epoch 279/300\n",
      "Average training loss: 0.2760175180832545\n",
      "Average test loss: 0.006820976188199388\n",
      "Epoch 280/300\n",
      "Average training loss: 0.21723276519775392\n",
      "Average test loss: 0.006734539042744372\n",
      "Epoch 281/300\n",
      "Average training loss: 0.17960896889368694\n",
      "Average test loss: 0.006607529124038087\n",
      "Epoch 282/300\n",
      "Average training loss: 0.15721627892388237\n",
      "Average test loss: 0.006526911374181509\n",
      "Epoch 283/300\n",
      "Average training loss: 0.14403304753700893\n",
      "Average test loss: 0.006521401947985093\n",
      "Epoch 284/300\n",
      "Average training loss: 0.1351164485944642\n",
      "Average test loss: 0.006603001551495658\n",
      "Epoch 285/300\n",
      "Average training loss: 0.12855245465040208\n",
      "Average test loss: 0.006645389333367347\n",
      "Epoch 286/300\n",
      "Average training loss: 0.12311869966983795\n",
      "Average test loss: 0.006753386384083165\n",
      "Epoch 287/300\n",
      "Average training loss: 0.11873783871862624\n",
      "Average test loss: 0.006790455474207799\n",
      "Epoch 288/300\n",
      "Average training loss: 0.11545481797059377\n",
      "Average test loss: 0.006522512964076466\n",
      "Epoch 289/300\n",
      "Average training loss: 0.11284151851468616\n",
      "Average test loss: 0.007806000532375442\n",
      "Epoch 290/300\n",
      "Average training loss: 0.11123160354296366\n",
      "Average test loss: 0.006431286609421174\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10894227545791202\n",
      "Average test loss: 0.006476856043769253\n",
      "Epoch 292/300\n",
      "Average training loss: 0.107200565914313\n",
      "Average test loss: 0.0072448530826303694\n",
      "Epoch 293/300\n",
      "Average training loss: 0.1059395614531305\n",
      "Average test loss: 0.006461795507619778\n",
      "Epoch 294/300\n",
      "Average training loss: 0.10456908107466167\n",
      "Average test loss: 0.006816871928258075\n",
      "Epoch 295/300\n",
      "Average training loss: 0.10357789846923617\n",
      "Average test loss: 0.006434158930761946\n",
      "Epoch 296/300\n",
      "Average training loss: 0.10253267730606927\n",
      "Average test loss: 0.006995529140863154\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10149910027450985\n",
      "Average test loss: 0.006526105442601774\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10056902354955673\n",
      "Average test loss: 0.006739867805606789\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09962262723843257\n",
      "Average test loss: 0.0309567684729894\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09886801838212543\n",
      "Average test loss: 0.025500145951079\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.8596526738272774\n",
      "Average test loss: 0.008747785930004385\n",
      "Epoch 2/300\n",
      "Average training loss: 0.22503325724601744\n",
      "Average test loss: 0.007115498599906763\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1695571748945448\n",
      "Average test loss: 0.006664293816106187\n",
      "Epoch 4/300\n",
      "Average training loss: 0.14736546456813812\n",
      "Average test loss: 0.006475859199133184\n",
      "Epoch 5/300\n",
      "Average training loss: 0.13462362865606944\n",
      "Average test loss: 0.006188758246186706\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1265424265993966\n",
      "Average test loss: 0.006062825434737735\n",
      "Epoch 7/300\n",
      "Average training loss: 0.12040744149022632\n",
      "Average test loss: 0.0058362912444604766\n",
      "Epoch 8/300\n",
      "Average training loss: 0.11615636783838272\n",
      "Average test loss: 0.005734473076959451\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11239381671614117\n",
      "Average test loss: 0.005669773322012689\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10871002135674158\n",
      "Average test loss: 0.007315852571692732\n",
      "Epoch 11/300\n",
      "Average training loss: 0.10522467681434419\n",
      "Average test loss: 0.005524618553204669\n",
      "Epoch 12/300\n",
      "Average training loss: 0.10188605054219564\n",
      "Average test loss: 0.005555185226930512\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09933988461229536\n",
      "Average test loss: 0.006509176346576876\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09602919836176767\n",
      "Average test loss: 0.005213608311282264\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09423556358946694\n",
      "Average test loss: 0.005441485939340459\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09172104512982898\n",
      "Average test loss: 0.0051464895655711496\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08960904231336382\n",
      "Average test loss: 0.005164130321807332\n",
      "Epoch 18/300\n",
      "Average training loss: 0.08799087403217952\n",
      "Average test loss: 0.004991139949402875\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08624299589792887\n",
      "Average test loss: 0.005391067767308818\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08486872118711472\n",
      "Average test loss: 0.004743107754737139\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0839239299164878\n",
      "Average test loss: 0.0048710574180715616\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08274950810604625\n",
      "Average test loss: 0.004724581970936722\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08176848521497515\n",
      "Average test loss: 0.0045844154482086496\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08082127383682462\n",
      "Average test loss: 0.004892920261041986\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08009688188963467\n",
      "Average test loss: 0.0046545486698548\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07950703225864304\n",
      "Average test loss: 0.004948393060515324\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07858569146527185\n",
      "Average test loss: 0.004450058731353945\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07818930994802051\n",
      "Average test loss: 0.0045229338844203285\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07749057328701019\n",
      "Average test loss: 0.004683049637410376\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07700515482823055\n",
      "Average test loss: 0.004446319948881864\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07646878924634722\n",
      "Average test loss: 0.005182235317511691\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07601066745652092\n",
      "Average test loss: 0.00438524836798509\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07549744901061058\n",
      "Average test loss: 0.004358017135411501\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07527467197842068\n",
      "Average test loss: 0.004385374888156851\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07492887126074897\n",
      "Average test loss: 0.004327049422802196\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07441297962268194\n",
      "Average test loss: 0.004353604760848813\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07408525762293074\n",
      "Average test loss: 0.00436423996090889\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07380926679240332\n",
      "Average test loss: 0.004370392351307803\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07367024280958706\n",
      "Average test loss: 0.004458847758049766\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07329688283469941\n",
      "Average test loss: 0.0044243748589522305\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0730899018910196\n",
      "Average test loss: 0.004448672325246864\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07292455415593253\n",
      "Average test loss: 0.004298313598252005\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07248011415203412\n",
      "Average test loss: 0.004315895126925574\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07233389565679763\n",
      "Average test loss: 0.004335702658527427\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07207924397786458\n",
      "Average test loss: 0.004279072136928638\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07217971235513687\n",
      "Average test loss: 0.0042513375656886235\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07184820753998226\n",
      "Average test loss: 0.004236853705098232\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07163787647750643\n",
      "Average test loss: 0.005349068296452364\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07159356678856743\n",
      "Average test loss: 0.0049032269554833575\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0712494954864184\n",
      "Average test loss: 0.004304713424709108\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0713759316570229\n",
      "Average test loss: 0.004195718883640237\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07102125222484271\n",
      "Average test loss: 0.004240804947291811\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0710682951675521\n",
      "Average test loss: 0.0042590715446405945\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0707854385011726\n",
      "Average test loss: 0.0042092148293223644\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0705130433373981\n",
      "Average test loss: 0.004209941142342157\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0705859254234367\n",
      "Average test loss: 0.004223465632233355\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07041834406720267\n",
      "Average test loss: 0.0042862532836281594\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07023037928673956\n",
      "Average test loss: 0.004196849942621258\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0702346468700303\n",
      "Average test loss: 0.005729032601747248\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07006151019202338\n",
      "Average test loss: 0.004284623715612623\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06994921839899487\n",
      "Average test loss: 0.004187994269447194\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06983489665057924\n",
      "Average test loss: 0.004204726575977273\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07329166417651706\n",
      "Average test loss: 0.004543441650768121\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07016911351018482\n",
      "Average test loss: 0.004392043720930815\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06962995870245828\n",
      "Average test loss: 0.004246752057638433\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06944663635227415\n",
      "Average test loss: 0.004160141235010491\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06936946090724733\n",
      "Average test loss: 0.004302597396489647\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06929783692955971\n",
      "Average test loss: 0.004332663344840209\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06917059204644627\n",
      "Average test loss: 0.0041915892257044715\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0691240000989702\n",
      "Average test loss: 0.004628290151556333\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06906447052293353\n",
      "Average test loss: 0.0048942884976665175\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06891312580638462\n",
      "Average test loss: 0.004405962153855298\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06885544557703865\n",
      "Average test loss: 0.004183540945665704\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06893027824494574\n",
      "Average test loss: 0.0042346564858324\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06871436435646482\n",
      "Average test loss: 0.004321454032013813\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06879060221049521\n",
      "Average test loss: 0.0045367260850552055\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06847904677523507\n",
      "Average test loss: 0.00414032113345133\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06846526294284397\n",
      "Average test loss: 0.004184960779216555\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06868769447339905\n",
      "Average test loss: 0.004273865410851107\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06876748238007227\n",
      "Average test loss: 0.004378029236156079\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06820016647378603\n",
      "Average test loss: 0.004189272141704957\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06832864100734393\n",
      "Average test loss: 0.00416190945998662\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06798261063297589\n",
      "Average test loss: 0.004639313494165739\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06815706840488646\n",
      "Average test loss: 0.004174585596140888\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06804772703515159\n",
      "Average test loss: 0.0042103653256264\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06793141777647867\n",
      "Average test loss: 0.004385877899825573\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06790413888957765\n",
      "Average test loss: 0.004291678143872155\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0677999702360895\n",
      "Average test loss: 0.004274521845910284\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06831480098432965\n",
      "Average test loss: 0.004137257185661131\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06752217821280161\n",
      "Average test loss: 0.004150446949733628\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06751671246025297\n",
      "Average test loss: 0.004325117058638069\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06756010527743234\n",
      "Average test loss: 46.740845509847006\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0676351762480206\n",
      "Average test loss: 0.004393814515529408\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06740776527921359\n",
      "Average test loss: 0.004267308885024653\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06737591318951713\n",
      "Average test loss: 0.004191857163897819\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06743854131301244\n",
      "Average test loss: 0.004162991404947307\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06726288428240353\n",
      "Average test loss: 0.004274152330226369\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0672794720729192\n",
      "Average test loss: 0.004163537045733796\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06720756408572197\n",
      "Average test loss: 0.004174045340054565\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06700154667430454\n",
      "Average test loss: 0.004161819501055612\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06739989955557717\n",
      "Average test loss: 0.004208900850473179\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06694776623778873\n",
      "Average test loss: 0.004155106387411555\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06676562441057629\n",
      "Average test loss: 0.004245152973880371\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06683331605460908\n",
      "Average test loss: 0.004146813427822457\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06672340271870295\n",
      "Average test loss: 0.00430705669336021\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06681682752900653\n",
      "Average test loss: 0.00429330089315772\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06668294532431497\n",
      "Average test loss: 0.004539321594354179\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06679461916950014\n",
      "Average test loss: 0.004150861459266808\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06655150081713994\n",
      "Average test loss: 0.005576470701644818\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06654844195975197\n",
      "Average test loss: 0.00423361151996586\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06652922140227424\n",
      "Average test loss: 0.004370440250469579\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06656114553411802\n",
      "Average test loss: 0.005409867109109958\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0663626306851705\n",
      "Average test loss: 0.004318527233269479\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0664989210234748\n",
      "Average test loss: 0.004494970946676201\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06632665670580334\n",
      "Average test loss: 0.0043772756465607215\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06623695890439882\n",
      "Average test loss: 0.004180274628102779\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06619007653660244\n",
      "Average test loss: 0.00417312100281318\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06633458973964056\n",
      "Average test loss: 0.004150830054862632\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06607595442401039\n",
      "Average test loss: 0.004179550204839971\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06613114945093791\n",
      "Average test loss: 0.004306881902118524\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06598478455675973\n",
      "Average test loss: 0.004318251455823581\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06976415749059783\n",
      "Average test loss: 0.004215303503183855\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06650146940019395\n",
      "Average test loss: 0.004605492170486185\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06586964681744575\n",
      "Average test loss: 0.004298176178294751\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06563787133163876\n",
      "Average test loss: 0.005645254037032524\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0657911313176155\n",
      "Average test loss: 0.004163325572593345\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06568640214867062\n",
      "Average test loss: 0.004164486883829037\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06584113181961908\n",
      "Average test loss: 0.004181904597414864\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0657810346616639\n",
      "Average test loss: 0.004282400030849709\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06562461379501555\n",
      "Average test loss: 0.0050171413549946415\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06559948452976015\n",
      "Average test loss: 0.004308830310487085\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06593712441126505\n",
      "Average test loss: 0.004186560863835944\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06554281152288119\n",
      "Average test loss: 0.0042041837241914536\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06544186991453171\n",
      "Average test loss: 0.0042115075435075495\n",
      "Epoch 135/300\n",
      "Average training loss: 0.065513763361507\n",
      "Average test loss: 0.00433822462707758\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06539489191439417\n",
      "Average test loss: 0.00433032388612628\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06540960513883166\n",
      "Average test loss: 0.004177269354048703\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06544183863533867\n",
      "Average test loss: 0.004188654680012001\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06534296170539326\n",
      "Average test loss: 0.0047112562917172906\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06539166145854526\n",
      "Average test loss: 0.004391190872010257\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0652093474401368\n",
      "Average test loss: 0.004615607763330142\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06520260307192803\n",
      "Average test loss: 0.004901221661104097\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06509465914964675\n",
      "Average test loss: 0.006033924475312233\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06518522955973943\n",
      "Average test loss: 0.014264329965743753\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06539746190441979\n",
      "Average test loss: 0.004511624780793985\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06503305402729247\n",
      "Average test loss: 0.0042190710902214055\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0648884722855356\n",
      "Average test loss: 0.0046802599626696775\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06521305668354034\n",
      "Average test loss: 0.004322632347337074\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0649441893696785\n",
      "Average test loss: 0.004230763197979993\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06490833629171054\n",
      "Average test loss: 0.004469423773388068\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06480954040752517\n",
      "Average test loss: 0.00422217905935314\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06635902355445755\n",
      "Average test loss: 0.0045199798486298985\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06506128996610641\n",
      "Average test loss: 0.004685723371389839\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06457577677236663\n",
      "Average test loss: 0.004260272848523325\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06462405364380942\n",
      "Average test loss: 0.004197309596670999\n",
      "Epoch 156/300\n",
      "Average training loss: 0.064612069328626\n",
      "Average test loss: 0.004727787675956885\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06460244340697924\n",
      "Average test loss: 0.004181299442942771\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06461168321635988\n",
      "Average test loss: 0.0041722102922697865\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06461153462198045\n",
      "Average test loss: 0.004186066468763683\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06449894909726249\n",
      "Average test loss: 0.004190885451518827\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06467380719052421\n",
      "Average test loss: 0.0042639862582501435\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06453563198778364\n",
      "Average test loss: 0.004487001146914231\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06434419600831137\n",
      "Average test loss: 0.010903159906880722\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06461485352781084\n",
      "Average test loss: 0.004230623915998472\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06445450778802236\n",
      "Average test loss: 0.004566567404609588\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06444049963686202\n",
      "Average test loss: 0.0042997002891368334\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06428647574451235\n",
      "Average test loss: 0.004862433763841788\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06497326050864326\n",
      "Average test loss: 0.004387429831756486\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06429031244913737\n",
      "Average test loss: 0.004467001831779878\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06410620260238648\n",
      "Average test loss: 0.004268491557074918\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06431093931860395\n",
      "Average test loss: 0.004244560024597579\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06409669298595852\n",
      "Average test loss: 0.10804040768080288\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0642004409564866\n",
      "Average test loss: 0.0042212801509433325\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06405984198715951\n",
      "Average test loss: 0.004409418795671728\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06421249532699586\n",
      "Average test loss: 0.004250657142450412\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06401598932676845\n",
      "Average test loss: 0.004283321974799037\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06411527634329266\n",
      "Average test loss: 0.004387053769081831\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06466933347119225\n",
      "Average test loss: 0.004329509413490693\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06379328953557545\n",
      "Average test loss: 0.004288888958179289\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06391678212417497\n",
      "Average test loss: 0.004220296738048394\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06390277389354176\n",
      "Average test loss: 0.004276946677101983\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06388758081197739\n",
      "Average test loss: 0.004461009864177969\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06396537862883674\n",
      "Average test loss: 0.004268392422546943\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06396001081003083\n",
      "Average test loss: 0.004225269610683123\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06380557314223713\n",
      "Average test loss: 0.004291732775461342\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06710708705584208\n",
      "Average test loss: 0.008175548698339197\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0637070560157299\n",
      "Average test loss: 0.0042686508852574565\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06353741881582473\n",
      "Average test loss: 0.004223475858155224\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06349677868684132\n",
      "Average test loss: 0.004272650532010529\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0635459183719423\n",
      "Average test loss: 0.004320931466917197\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06348530140188005\n",
      "Average test loss: 0.004271793193701241\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06362580761644575\n",
      "Average test loss: 0.004402337798641787\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06366864562365744\n",
      "Average test loss: 0.004276585695437259\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0636489827103085\n",
      "Average test loss: 0.004262239124212\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06364728904101584\n",
      "Average test loss: 0.009464115836554104\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06379248979356554\n",
      "Average test loss: 0.004294452373352316\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06351174024409718\n",
      "Average test loss: 0.004259183925059106\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06344690195057127\n",
      "Average test loss: 0.0061816748662127385\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06356322595145968\n",
      "Average test loss: 0.004255580652919081\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0635386791891522\n",
      "Average test loss: 0.004233540230741103\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06358249936501185\n",
      "Average test loss: 0.004495065980901321\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06327024418777889\n",
      "Average test loss: 0.004366679756384757\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06352214547660616\n",
      "Average test loss: 0.004353222449206644\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06329348669449489\n",
      "Average test loss: 0.004407694163007869\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06347189996971024\n",
      "Average test loss: 0.004222730667847726\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06325568046503596\n",
      "Average test loss: 0.012309250259564983\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06331345854202906\n",
      "Average test loss: 0.004196819832134578\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06330938157108094\n",
      "Average test loss: 0.0042537985464764965\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06330785889758005\n",
      "Average test loss: 0.004404280564851231\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0632385365433163\n",
      "Average test loss: 0.004268054426544242\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06320927533838484\n",
      "Average test loss: 0.004298401440597243\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0631728117763996\n",
      "Average test loss: 0.004787060914975073\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06327478430668514\n",
      "Average test loss: 0.004318181501494513\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06305902036031087\n",
      "Average test loss: 0.004283377153178056\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06308949722184075\n",
      "Average test loss: 0.004545461852931314\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06329867808686362\n",
      "Average test loss: 3077442.6652222225\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06454462672273319\n",
      "Average test loss: 0.0044973359223869115\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06304726671510273\n",
      "Average test loss: 0.00425295834760699\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06298834197388754\n",
      "Average test loss: 0.004293011101997561\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06287926071219974\n",
      "Average test loss: 0.004264573479692141\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06288052876790365\n",
      "Average test loss: 0.004754540907632974\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06292493119504716\n",
      "Average test loss: 0.004328233047491975\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06302751288811366\n",
      "Average test loss: 0.004666502194893029\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0635210978521241\n",
      "Average test loss: 0.015432026201652157\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06278456744220522\n",
      "Average test loss: 0.0046281185148076874\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06278770038816664\n",
      "Average test loss: 0.004577646089924706\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06289424157804913\n",
      "Average test loss: 0.00451596888485882\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06275463275114695\n",
      "Average test loss: 0.004260145164819228\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0628407868511147\n",
      "Average test loss: 0.0043870730871955555\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06290985056095653\n",
      "Average test loss: 0.0043415298716475564\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06277989551093843\n",
      "Average test loss: 0.004231460922501154\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06276091247797012\n",
      "Average test loss: 0.004323334792835845\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06291724586817954\n",
      "Average test loss: 0.006133967067632411\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06273129474454456\n",
      "Average test loss: 0.004408331332107385\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06266789417465528\n",
      "Average test loss: 0.005185877122812801\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06261206875907051\n",
      "Average test loss: 0.00464853402848045\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06287903700272242\n",
      "Average test loss: 0.004218185659911898\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06254256980948977\n",
      "Average test loss: 0.004376625473921498\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06268908899360233\n",
      "Average test loss: 0.0057013563927676945\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06261885323789385\n",
      "Average test loss: 0.004271167176879115\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06260617364777459\n",
      "Average test loss: 0.004301383799148931\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06269565139214198\n",
      "Average test loss: 0.00453243613946769\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06252844683991537\n",
      "Average test loss: 0.008228557698842552\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0627239174478584\n",
      "Average test loss: 0.004283209664540159\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06246285800139109\n",
      "Average test loss: 0.004816841393709183\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06247522276308801\n",
      "Average test loss: 0.0042924879553417365\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06246316891908646\n",
      "Average test loss: 0.0042690232085684935\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0625259598824713\n",
      "Average test loss: 0.004258887097032534\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06244538305534257\n",
      "Average test loss: 0.004401159861849414\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06251501515838835\n",
      "Average test loss: 0.0043567935236626205\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06244236867295371\n",
      "Average test loss: 0.00426520727119512\n",
      "Epoch 252/300\n",
      "Average training loss: 0.062357706407705944\n",
      "Average test loss: 0.004301448581533299\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0627029934922854\n",
      "Average test loss: 0.004342216222236554\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0624995044403606\n",
      "Average test loss: 0.004295885251628028\n",
      "Epoch 255/300\n",
      "Average training loss: 0.062183560252189636\n",
      "Average test loss: 0.004312996944205628\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06227890806727939\n",
      "Average test loss: 0.004680923889908525\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0625510433183776\n",
      "Average test loss: 0.00422792422481709\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0623707825673951\n",
      "Average test loss: 0.0042488668173965485\n",
      "Epoch 259/300\n",
      "Average training loss: 0.062258530689610375\n",
      "Average test loss: 0.00434632050494353\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06238642569382986\n",
      "Average test loss: 0.00438970082708531\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06230787860022651\n",
      "Average test loss: 0.004379455213538474\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06223201053672367\n",
      "Average test loss: 0.004259926199085182\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06225564072529475\n",
      "Average test loss: 0.004260250206001931\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06207680396901236\n",
      "Average test loss: 0.004420467886659834\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0623372729950481\n",
      "Average test loss: 0.0044007099337048\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06222916652758916\n",
      "Average test loss: 0.005324265506118536\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0621294496456782\n",
      "Average test loss: 0.0043799700323078365\n",
      "Epoch 268/300\n",
      "Average training loss: 0.062221222907304766\n",
      "Average test loss: 0.030277949710687\n",
      "Epoch 269/300\n",
      "Average training loss: 0.062289151807626085\n",
      "Average test loss: 0.0042674559810095365\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0621039041545656\n",
      "Average test loss: 0.004338803179148171\n",
      "Epoch 271/300\n",
      "Average training loss: 0.061935663044452666\n",
      "Average test loss: 0.004320989654709896\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06247228335009681\n",
      "Average test loss: 0.00427692598849535\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06190413220392333\n",
      "Average test loss: 0.0043289449007974734\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06200516760680411\n",
      "Average test loss: 0.005239305292566617\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0620044635269377\n",
      "Average test loss: 0.004381979967984889\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06200277563267284\n",
      "Average test loss: 0.004431970725042952\n",
      "Epoch 277/300\n",
      "Average training loss: 0.062127981232272256\n",
      "Average test loss: 0.004284529726331433\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06197129111157523\n",
      "Average test loss: 0.004298739432874653\n",
      "Epoch 279/300\n",
      "Average training loss: 0.062041223463084966\n",
      "Average test loss: 0.0043967195952104195\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06191838337977727\n",
      "Average test loss: 0.004362597378177775\n",
      "Epoch 281/300\n",
      "Average training loss: 0.062139030767811666\n",
      "Average test loss: 0.0047710988016592135\n",
      "Epoch 282/300\n",
      "Average training loss: 0.061879022065136166\n",
      "Average test loss: 0.004300766086412801\n",
      "Epoch 283/300\n",
      "Average training loss: 0.062061621056662665\n",
      "Average test loss: 0.004888004669298728\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06185451051923964\n",
      "Average test loss: 0.004294565515799655\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06445898832215204\n",
      "Average test loss: 0.004287494897428486\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06165073545773824\n",
      "Average test loss: 46.51545316314697\n",
      "Epoch 287/300\n",
      "Average training loss: 0.061710000180535844\n",
      "Average test loss: 0.004258601603822576\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06186772502793206\n",
      "Average test loss: 0.004707512192428112\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0617686058415307\n",
      "Average test loss: 0.0043149421417878735\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06178607674770885\n",
      "Average test loss: 0.0043507750984281305\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0616848306854566\n",
      "Average test loss: 0.004340123264739911\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06205540748106109\n",
      "Average test loss: 0.004339818813320663\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0616534210840861\n",
      "Average test loss: 0.0042551732750402554\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06175577037864261\n",
      "Average test loss: 0.0043473853547539975\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0616138682630327\n",
      "Average test loss: 0.004731849153008726\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06183408250411351\n",
      "Average test loss: 0.004362388072328435\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06172179052564833\n",
      "Average test loss: 0.004424713449138734\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06174782751003901\n",
      "Average test loss: 0.00428815971646044\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06164515310525894\n",
      "Average test loss: 0.004485694298313724\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06172638378540675\n",
      "Average test loss: 0.004367629247821039\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6056086925665538\n",
      "Average test loss: 0.0072243205217851535\n",
      "Epoch 2/300\n",
      "Average training loss: 0.181670171684689\n",
      "Average test loss: 0.005793532115717729\n",
      "Epoch 3/300\n",
      "Average training loss: 0.13631956492529976\n",
      "Average test loss: 0.005730989012453292\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11864371359348297\n",
      "Average test loss: 0.005046342794266012\n",
      "Epoch 5/300\n",
      "Average training loss: 0.10849814425574408\n",
      "Average test loss: 0.005032639142125845\n",
      "Epoch 6/300\n",
      "Average training loss: 0.10192501158184475\n",
      "Average test loss: 0.004850460934142271\n",
      "Epoch 7/300\n",
      "Average training loss: 0.09591627612378863\n",
      "Average test loss: 0.004731364174849457\n",
      "Epoch 8/300\n",
      "Average training loss: 0.09226969585816065\n",
      "Average test loss: 0.004571066717720694\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08863969435294469\n",
      "Average test loss: 0.00429241553776794\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08549738409452969\n",
      "Average test loss: 0.004718671345876323\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08238247613774406\n",
      "Average test loss: 0.004081195971618096\n",
      "Epoch 12/300\n",
      "Average training loss: 0.08009002157714631\n",
      "Average test loss: 0.005208599557065301\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07771654046575228\n",
      "Average test loss: 0.0039805716052651405\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07538298692968157\n",
      "Average test loss: 0.0040936779930359785\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07335666045215394\n",
      "Average test loss: 0.003986280493438244\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07156021422810024\n",
      "Average test loss: 0.0037577008025513753\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06992782222562366\n",
      "Average test loss: 0.003832444147310323\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0684646975464291\n",
      "Average test loss: 0.003747804634273052\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0671562270058526\n",
      "Average test loss: 0.003632227008955346\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06494295423560673\n",
      "Average test loss: 0.0036181299446357623\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06391026015414132\n",
      "Average test loss: 0.0034992530432840186\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06313320455286238\n",
      "Average test loss: 0.0035667557815710705\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06078964375125037\n",
      "Average test loss: 0.003452820710009999\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06026404615243276\n",
      "Average test loss: 0.0034576127342879773\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05964207610487938\n",
      "Average test loss: 0.003368546895475851\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05931214036875301\n",
      "Average test loss: 0.003385232558266984\n",
      "Epoch 31/300\n",
      "Average training loss: 0.058847071902619466\n",
      "Average test loss: 0.0084043448401822\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05865711564156744\n",
      "Average test loss: 0.0033008244662649103\n",
      "Epoch 33/300\n",
      "Average training loss: 0.058114934785498513\n",
      "Average test loss: 0.003431673170791732\n",
      "Epoch 34/300\n",
      "Average training loss: 0.057465477691756355\n",
      "Average test loss: 0.0032668003915912575\n",
      "Epoch 37/300\n",
      "Average training loss: 0.057224763224522274\n",
      "Average test loss: 0.00323108061713477\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05691675451397896\n",
      "Average test loss: 0.003282921691528625\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0567108608616723\n",
      "Average test loss: 0.003276954780022303\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05660105801290936\n",
      "Average test loss: 0.0033137311583591833\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05646800274981393\n",
      "Average test loss: 0.0032350444067269562\n",
      "Epoch 42/300\n",
      "Average training loss: 0.056101721704006194\n",
      "Average test loss: 0.003247719213573469\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0560265031059583\n",
      "Average test loss: 0.003217608544561598\n",
      "Epoch 44/300\n",
      "Average training loss: 0.055791659623384476\n",
      "Average test loss: 0.04085574879580074\n",
      "Epoch 45/300\n",
      "Average training loss: 0.055769957416587405\n",
      "Average test loss: 0.003202108777852522\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05555978300836351\n",
      "Average test loss: 0.0031822523021449644\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05536980182594723\n",
      "Average test loss: 0.003157594273280766\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05543670567207866\n",
      "Average test loss: 0.00398615611013439\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05511361962225702\n",
      "Average test loss: 0.0032449454687949685\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05501473730471399\n",
      "Average test loss: 0.003188098375582033\n",
      "Epoch 51/300\n",
      "Average training loss: 0.054946889566050636\n",
      "Average test loss: 0.003161008317851358\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05485509612825182\n",
      "Average test loss: 0.003383440503436658\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05470640078849263\n",
      "Average test loss: 0.003192325211130083\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05452232826418347\n",
      "Average test loss: 0.0031398967256148656\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0544817397693793\n",
      "Average test loss: 0.0031767680247624715\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0544392830034097\n",
      "Average test loss: 0.003180147871789005\n",
      "Epoch 57/300\n",
      "Average training loss: 0.054372807929913204\n",
      "Average test loss: 0.0034703223471426303\n",
      "Epoch 58/300\n",
      "Average training loss: 0.054188161694341236\n",
      "Average test loss: 0.003142772663384676\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05420799057351219\n",
      "Average test loss: 0.0036483106422755453\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05384894895884726\n",
      "Average test loss: 0.0031458334459198846\n",
      "Epoch 63/300\n",
      "Average training loss: 0.053913685003916426\n",
      "Average test loss: 0.003168257084364692\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05373327578107516\n",
      "Average test loss: 0.0031709348168224094\n",
      "Epoch 65/300\n",
      "Average training loss: 0.053748822725481454\n",
      "Average test loss: 0.003239008747248186\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0536613887084855\n",
      "Average test loss: 0.003187282815989521\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0536017358733548\n",
      "Average test loss: 0.003547141516374217\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05351406309008598\n",
      "Average test loss: 0.0033028524737391206\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05340975066357189\n",
      "Average test loss: 0.003206522832107213\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05332678055432108\n",
      "Average test loss: 0.0031238341159704657\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05327116364571783\n",
      "Average test loss: 0.0031217041586836178\n",
      "Epoch 72/300\n",
      "Average training loss: 0.053295796132749984\n",
      "Average test loss: 0.003097690319021543\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05375787822736634\n",
      "Average test loss: 0.003113605191310247\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05304149982002047\n",
      "Average test loss: 0.003253628454481562\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0529446781012747\n",
      "Average test loss: 0.003102955445026358\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05295872060457865\n",
      "Average test loss: 0.003109484149970942\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05287509753637844\n",
      "Average test loss: 0.0033791986192680066\n",
      "Epoch 78/300\n",
      "Average training loss: 0.052821884098980164\n",
      "Average test loss: 0.0031428278918481534\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05284945991304186\n",
      "Average test loss: 0.003520803964800305\n",
      "Epoch 80/300\n",
      "Average training loss: 0.052761150909794705\n",
      "Average test loss: 0.003311167961607377\n",
      "Epoch 81/300\n",
      "Average training loss: 0.052603706174426605\n",
      "Average test loss: 0.003107728485845857\n",
      "Epoch 82/300\n",
      "Average training loss: 0.052731674803627865\n",
      "Average test loss: 0.003902465682476759\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05269466610418426\n",
      "Average test loss: 0.0030963155556884077\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05260555682579676\n",
      "Average test loss: 0.003079836361317171\n",
      "Epoch 85/300\n",
      "Average training loss: 0.052474463767475554\n",
      "Average test loss: 0.003842647567805317\n",
      "Epoch 86/300\n",
      "Average training loss: 0.052968307349416946\n",
      "Average test loss: 0.003151232618217667\n",
      "Epoch 87/300\n",
      "Average training loss: 0.052314412083890705\n",
      "Average test loss: 0.0030921683452195593\n",
      "Epoch 88/300\n",
      "Average training loss: 0.052243657976388934\n",
      "Average test loss: 0.0031058998921265203\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05217725275291337\n",
      "Average test loss: 0.0030930783260199757\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05215432280964322\n",
      "Average test loss: 0.0031451143661720884\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05223159552945031\n",
      "Average test loss: 0.003081689956080582\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05208141486843427\n",
      "Average test loss: 0.003137089865282178\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05209925306505627\n",
      "Average test loss: 0.003631528444588184\n",
      "Epoch 94/300\n",
      "Average training loss: 0.051935132993592155\n",
      "Average test loss: 0.003263236464311679\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05198034722275204\n",
      "Average test loss: 0.0030904070927451053\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05197370233800676\n",
      "Average test loss: 0.0030987368567536276\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05189694207575586\n",
      "Average test loss: 0.003087819455501934\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05184936962193913\n",
      "Average test loss: 0.008414641083114677\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05179837150375048\n",
      "Average test loss: 0.003083956159444319\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05181293344166544\n",
      "Average test loss: 0.0031259147779395184\n",
      "Epoch 101/300\n",
      "Average training loss: 0.051755174219608305\n",
      "Average test loss: 0.0032337105110701587\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05164828881621361\n",
      "Average test loss: 0.0031928376379526326\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05195151847932074\n",
      "Average test loss: 0.0033941198190053303\n",
      "Epoch 104/300\n",
      "Average training loss: 0.051524761885404585\n",
      "Average test loss: 0.0030943762802829347\n",
      "Epoch 105/300\n",
      "Average training loss: 0.051640514896975624\n",
      "Average test loss: 0.003137190683020486\n",
      "Epoch 106/300\n",
      "Average training loss: 0.051479501161310406\n",
      "Average test loss: 0.012300065676371257\n",
      "Epoch 107/300\n",
      "Average training loss: 0.051520173913902706\n",
      "Average test loss: 0.003116207434174915\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05142636991209454\n",
      "Average test loss: 0.005622395262329115\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05148436746663517\n",
      "Average test loss: 0.003111767953882615\n",
      "Epoch 110/300\n",
      "Average training loss: 0.051365780962838066\n",
      "Average test loss: 0.003153059869383772\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05139471422301398\n",
      "Average test loss: 0.003746261427178979\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05132917677031623\n",
      "Average test loss: 0.0030913457833230493\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05172373641199536\n",
      "Average test loss: 0.003194266946365436\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05116571931375397\n",
      "Average test loss: 0.003091737566722764\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05117948415544298\n",
      "Average test loss: 0.003126421359264188\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05116918442315525\n",
      "Average test loss: 0.003118314182592763\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05108764859371715\n",
      "Average test loss: 0.003249054961320427\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05108370998170641\n",
      "Average test loss: 0.0030945675366868576\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05109912208053801\n",
      "Average test loss: 0.0032032193885081343\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0509716917110814\n",
      "Average test loss: 0.00316533594371544\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05099741613533762\n",
      "Average test loss: 0.0033201895650062296\n",
      "Epoch 122/300\n",
      "Average training loss: 0.050938605434364743\n",
      "Average test loss: 0.003209277474010984\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05091776515046755\n",
      "Average test loss: 0.0036046818213330374\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05086431524157524\n",
      "Average test loss: 110.25815443589953\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05089170274800724\n",
      "Average test loss: 0.003251506241452363\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05069326817327076\n",
      "Average test loss: 0.003383288148790598\n",
      "Epoch 127/300\n",
      "Average training loss: 0.050775283028682075\n",
      "Average test loss: 0.003145078163800968\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05071468704938888\n",
      "Average test loss: 0.0033946279332869585\n",
      "Epoch 129/300\n",
      "Average training loss: 0.050649783306651645\n",
      "Average test loss: 0.01827685458958149\n",
      "Epoch 130/300\n",
      "Average training loss: 0.050879086265961326\n",
      "Average test loss: 0.00490891874457399\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05068215471506119\n",
      "Average test loss: 0.0032902979739010334\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05067122716705005\n",
      "Average test loss: 0.004060203681389491\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05063766803344091\n",
      "Average test loss: 0.02281988496747282\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05065782080756293\n",
      "Average test loss: 0.0031228810979260337\n",
      "Epoch 135/300\n",
      "Average training loss: 0.050573375549581315\n",
      "Average test loss: 46.9259724867079\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05114224752452638\n",
      "Average test loss: 0.0031089837617344325\n",
      "Epoch 137/300\n",
      "Average training loss: 0.050343495508035026\n",
      "Average test loss: 0.0044034959338605404\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05040391519334581\n",
      "Average test loss: 0.003117957975094517\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05037358518772655\n",
      "Average test loss: 0.003109948350323571\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05040200558635924\n",
      "Average test loss: 0.003877945857329501\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05055030391613642\n",
      "Average test loss: 0.0031308122544238966\n",
      "Epoch 142/300\n",
      "Average training loss: 0.050388908939229114\n",
      "Average test loss: 0.0031182698516382112\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05023906503121058\n",
      "Average test loss: 0.003346140028701888\n",
      "Epoch 146/300\n",
      "Average training loss: 0.050271670235527886\n",
      "Average test loss: 0.0031840783682548337\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05011592464645703\n",
      "Average test loss: 0.003173840177555879\n",
      "Epoch 148/300\n",
      "Average training loss: 0.050243462562561035\n",
      "Average test loss: 0.0031678765058103537\n",
      "Epoch 149/300\n",
      "Average training loss: 0.050098039355542925\n",
      "Average test loss: 0.0031503591671999957\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0513338415854507\n",
      "Average test loss: 0.0031284187936948408\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04989674316512214\n",
      "Average test loss: 0.003139916288562947\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04987330026096768\n",
      "Average test loss: 0.0031333424794591134\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05006138104862637\n",
      "Average test loss: 0.0031938379568358264\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04991702740722232\n",
      "Average test loss: 0.003224134757493933\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04995273927847544\n",
      "Average test loss: 0.0032291687480691406\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05006608696778615\n",
      "Average test loss: 0.0031822481469975577\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04989656596713596\n",
      "Average test loss: 0.0031613304428756237\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05111981241239442\n",
      "Average test loss: 0.0031176044831259385\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04985293945007854\n",
      "Average test loss: 0.0031558332505325476\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04976641839411523\n",
      "Average test loss: 0.003169648767138521\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0498287806113561\n",
      "Average test loss: 0.0031473687127646474\n",
      "Epoch 162/300\n",
      "Average training loss: 0.049835394110944536\n",
      "Average test loss: 0.003219120711916023\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04976917988061905\n",
      "Average test loss: 0.0044840410802927284\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04972809188233482\n",
      "Average test loss: 0.0033327895665748253\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04977192061808374\n",
      "Average test loss: 0.0031676582764420245\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04990024173922009\n",
      "Average test loss: 0.0032197757853815955\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04983488460050689\n",
      "Average test loss: 0.003122591823546423\n",
      "Epoch 168/300\n",
      "Average training loss: 0.049578902893596226\n",
      "Average test loss: 0.0031010190289881496\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05193561441368527\n",
      "Average test loss: 0.0031202892834941544\n",
      "Epoch 170/300\n",
      "Average training loss: 0.049552545309066776\n",
      "Average test loss: 0.0031698528035647338\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0495176074968444\n",
      "Average test loss: 0.0031606815269009936\n",
      "Epoch 172/300\n",
      "Average training loss: 0.049606702463494405\n",
      "Average test loss: 0.0032063780267619422\n",
      "Epoch 173/300\n",
      "Average training loss: 0.049520432617929246\n",
      "Average test loss: 0.0031202204066018264\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05001322076386876\n",
      "Average test loss: 0.0031342117968532774\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04940524006883303\n",
      "Average test loss: 0.0031767290745758348\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04943996173143387\n",
      "Average test loss: 0.00338306108986338\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04957623025112682\n",
      "Average test loss: 0.0033009494816263517\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04939740917086601\n",
      "Average test loss: 0.003540803583131896\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04939740149180094\n",
      "Average test loss: 0.0031549115582472746\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04969673420654403\n",
      "Average test loss: 0.0032070061730013954\n",
      "Epoch 184/300\n",
      "Average training loss: 0.049486465245485306\n",
      "Average test loss: 0.0031553243750499353\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04957264289922184\n",
      "Average test loss: 0.003185808667913079\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04924170164598359\n",
      "Average test loss: 0.0032245405943443378\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0492860130932596\n",
      "Average test loss: 0.003384853244241741\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04932042205002573\n",
      "Average test loss: 0.003131483859175609\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04930485450890329\n",
      "Average test loss: 0.003326990279265576\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04925463714202245\n",
      "Average test loss: 0.003165848130567206\n",
      "Epoch 191/300\n",
      "Average training loss: 0.049343856281704374\n",
      "Average test loss: 0.0034688907460206087\n",
      "Epoch 192/300\n",
      "Average training loss: 0.049326692771580485\n",
      "Average test loss: 0.003340263490461641\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04934953812426991\n",
      "Average test loss: 0.0032052196661631264\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04917186004254553\n",
      "Average test loss: 0.00315308341383934\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04911274773213598\n",
      "Average test loss: 0.0031657560266968275\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04914157670405176\n",
      "Average test loss: 0.003211517043825653\n",
      "Epoch 197/300\n",
      "Average training loss: 0.049072441541486315\n",
      "Average test loss: 0.003161441578840216\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04918241983652115\n",
      "Average test loss: 0.003221020021993253\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04904084201984935\n",
      "Average test loss: 0.003235278302182754\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04964600375294685\n",
      "Average test loss: 0.003189024351330267\n",
      "Epoch 203/300\n",
      "Average training loss: 0.049062397645579445\n",
      "Average test loss: 0.003226126378402114\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04893974739975399\n",
      "Average test loss: 0.003202220279723406\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04898123225238588\n",
      "Average test loss: 0.006158610610912244\n",
      "Epoch 206/300\n",
      "Average training loss: 0.048897088289260864\n",
      "Average test loss: 0.0032197847360124193\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04900896426041921\n",
      "Average test loss: 0.0031939380665620166\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0489471735490693\n",
      "Average test loss: 0.0031509340335097577\n",
      "Epoch 209/300\n",
      "Average training loss: 0.048954013029734296\n",
      "Average test loss: 0.003228980459893743\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04902134959896406\n",
      "Average test loss: 0.003169685326723589\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04881356574429406\n",
      "Average test loss: 0.0032862279535167747\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04882287316189872\n",
      "Average test loss: 0.0032796480510797767\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04876936892337269\n",
      "Average test loss: 0.0037897583776050144\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04882388362288475\n",
      "Average test loss: 0.003218586814486318\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04892748276392619\n",
      "Average test loss: 0.0031481999908056523\n",
      "Epoch 218/300\n",
      "Average training loss: 0.048800616544153956\n",
      "Average test loss: 0.003302114808311065\n",
      "Epoch 221/300\n",
      "Average training loss: 0.048779752453168235\n",
      "Average test loss: 0.003165772757389479\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04933827913138601\n",
      "Average test loss: 0.003189829122689035\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04867199931542079\n",
      "Average test loss: 0.003229749178720845\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04875011635488934\n",
      "Average test loss: 0.003202482795756724\n",
      "Epoch 225/300\n",
      "Average training loss: 0.048618973026673\n",
      "Average test loss: 0.003924939599716001\n",
      "Epoch 226/300\n",
      "Average training loss: 0.049382356961568195\n",
      "Average test loss: 0.003261271512756745\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04860611785782708\n",
      "Average test loss: 0.003327034638573726\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04854338320096334\n",
      "Average test loss: 0.003404844069439504\n",
      "Epoch 229/300\n",
      "Average training loss: 0.048633630835347705\n",
      "Average test loss: 0.0032641510305305324\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04859496118625005\n",
      "Average test loss: 0.0035875914173407685\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048594688716861935\n",
      "Average test loss: 0.0031397830645243326\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04898187535007795\n",
      "Average test loss: 0.003282428068212337\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04848767441842291\n",
      "Average test loss: 0.003274544102864133\n",
      "Epoch 234/300\n",
      "Average training loss: 0.048518030686510935\n",
      "Average test loss: 0.0031783647570345136\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04871100340783596\n",
      "Average test loss: 0.0032413850016891955\n",
      "Epoch 236/300\n",
      "Average training loss: 0.048684536072942944\n",
      "Average test loss: 0.0034225231860246924\n",
      "Epoch 237/300\n",
      "Average training loss: 0.048454271333085164\n",
      "Average test loss: 0.003208012982789013\n",
      "Epoch 238/300\n",
      "Average training loss: 0.048496432145436606\n",
      "Average test loss: 0.0033824000331676668\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04847638969951206\n",
      "Average test loss: 0.003267348876429929\n",
      "Epoch 240/300\n",
      "Average training loss: 0.048502165877156785\n",
      "Average test loss: 0.0032709506697331868\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04850112622645166\n",
      "Average test loss: 0.014191114434765445\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04843868420521418\n",
      "Average test loss: 0.003204147084719605\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04848251655366686\n",
      "Average test loss: 0.0031463866426299017\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04850132189525498\n",
      "Average test loss: 0.0032122488129470083\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04839578068918652\n",
      "Average test loss: 0.0032207918463067874\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04854019903474384\n",
      "Average test loss: 0.0034153892687625357\n",
      "Epoch 247/300\n",
      "Average training loss: 0.048372692243920434\n",
      "Average test loss: 0.0044140893535481555\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04847286633650462\n",
      "Average test loss: 0.003176331223299106\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04832714710964097\n",
      "Average training loss: 0.04846794009539816\n",
      "Average test loss: 0.003513562208041549\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04827017200655408\n",
      "Average test loss: 0.0032095076764623322\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0484725290271971\n",
      "Average test loss: 0.003368238656057252\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04824510091212061\n",
      "Average test loss: 0.03275609200530582\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04828063486350907\n",
      "Average test loss: 0.00325780257044567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04828782744540108\n",
      "Average test loss: 0.003214007624528474\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04813450397054354\n",
      "Average test loss: 0.0031458780243992807\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04840626061293814\n",
      "Average test loss: 0.007005734456910028\n",
      "Epoch 260/300\n",
      "Average training loss: 0.048089591777986954\n",
      "Average test loss: 0.0033821156347791353\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04826124636663331\n",
      "Average test loss: 0.00454620185866952\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04830210771494441\n",
      "Average test loss: 0.0032359243351966142\n",
      "Epoch 263/300\n",
      "Average training loss: 0.048304737322860294\n",
      "Average test loss: 0.0031761742329431903\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04819287574953503\n",
      "Average test loss: 0.0033910614125844504\n",
      "Epoch 265/300\n",
      "Average training loss: 0.048152698596318565\n",
      "Average test loss: 0.0038324473305708833\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0482207576168908\n",
      "Average test loss: 0.0031598236956116225\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04816180605689684\n",
      "Average test loss: 0.003688961171855529\n",
      "Epoch 268/300\n",
      "Average training loss: 0.048243323660559126\n",
      "Average test loss: 0.0031725179416437943\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04816790518992477\n",
      "Average test loss: 0.003244748738076952\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04815880144966973\n",
      "Average test loss: 0.003179862937165631\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04820974508921305\n",
      "Average test loss: 0.0033239627679189044\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04808408610688315\n",
      "Average test loss: 0.003179541779268119\n",
      "Epoch 273/300\n",
      "Average training loss: 0.048021974484125776\n",
      "Average test loss: 0.003331662272827493\n",
      "Epoch 274/300\n",
      "Average training loss: 0.048252259098821214\n",
      "Average test loss: 0.0032252685663600763\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04801904143889745\n",
      "Average test loss: 0.003195090810250905\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0480050605237484\n",
      "Average test loss: 0.006386382021630803\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04832372915082508\n",
      "Average test loss: 0.0032509714373283915\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04801921058032248\n",
      "Average test loss: 0.005755394134256575\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04802785052193536\n",
      "Average test loss: 0.003284181881282065\n",
      "Epoch 280/300\n",
      "Average training loss: 0.047963953180445565\n",
      "Average test loss: 0.004824275015128983\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04990187674760818\n",
      "Average test loss: 0.0032216445836755966\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04785805853539043\n",
      "Average test loss: 0.003201131364123689\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04792832776904106\n",
      "Average test loss: 0.0038778253777159584\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0478471326066388\n",
      "Average test loss: 0.0032205539672738974\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04795831549167633\n",
      "Average test loss: 0.0031845527024318776\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04801327911019325\n",
      "Average test loss: 0.003412711771618989\n",
      "Epoch 290/300\n",
      "Average training loss: 0.047920128266016646\n",
      "Average test loss: 0.0038084837552160027\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04782198421822654\n",
      "Average test loss: 0.003309227673957745\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0484246581726604\n",
      "Average test loss: 0.0031899488328231704\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04785515743162897\n",
      "Average test loss: 0.003263922992265887\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04783052733871672\n",
      "Average test loss: 0.0034570903848442766\n",
      "Epoch 295/300\n",
      "Average training loss: 0.047915109972159066\n",
      "Average test loss: 0.0032123538154280847\n",
      "Epoch 296/300\n",
      "Average training loss: 0.049877846866846084\n",
      "Average test loss: 0.0031742854056259\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0477906447549661\n",
      "Average test loss: 0.003206783111517628\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04770131065779262\n",
      "Average test loss: 0.0031729688685801292\n",
      "Epoch 299/300\n",
      "Average training loss: 0.047707801471153895\n",
      "Average test loss: 0.0032877042080379196\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04769640231463644\n",
      "Average test loss: 0.0031987345814704893\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5426575660308202\n",
      "Average test loss: 0.006470959765629636\n",
      "Epoch 2/300\n",
      "Average training loss: 0.17619248036543528\n",
      "Average test loss: 0.005454870034009218\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12758156051238378\n",
      "Average test loss: 0.004617935962147183\n",
      "Epoch 4/300\n",
      "Average training loss: 0.10872044056653976\n",
      "Average test loss: 0.004834783427417278\n",
      "Epoch 5/300\n",
      "Average training loss: 0.097942800005277\n",
      "Average test loss: 0.004287024456179804\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09086412435107762\n",
      "Average test loss: 0.0040427430704649955\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0855622907280922\n",
      "Average test loss: 0.003925334127826823\n",
      "Epoch 8/300\n",
      "Average training loss: 0.08152369946241379\n",
      "Average test loss: 0.003823433160367939\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07835851475265292\n",
      "Average test loss: 0.004018398312230905\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07516708768076368\n",
      "Average test loss: 0.0035164040333280962\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07239700934622023\n",
      "Average test loss: 0.003613530543115404\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07094711265961329\n",
      "Average test loss: 0.003687852993607521\n",
      "Epoch 13/300\n",
      "Average test loss: 0.003406953563706742\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06346618183453878\n",
      "Average test loss: 0.003391030877828598\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06214779745207893\n",
      "Average test loss: 0.0033234796716521183\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06069871926969952\n",
      "Average test loss: 0.003246050730554594\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05877677196926541\n",
      "Average test loss: 0.0031008864229338036\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05785541252626313\n",
      "Average test loss: 0.0031713858236455256\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05651249122619629\n",
      "Average test loss: 0.0030741479674147236\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05561229819059372\n",
      "Average test loss: 0.002865336024098926\n",
      "Epoch 23/300\n",
      "Average training loss: 0.054824202954769134\n",
      "Average test loss: 0.0030727980576662553\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05382153838541773\n",
      "Average test loss: 0.002838164383131597\n",
      "Epoch 25/300\n",
      "Average training loss: 0.053061057223214046\n",
      "Average test loss: 0.0028024310976680782\n",
      "Epoch 26/300\n",
      "Average training loss: 0.052394016116857525\n",
      "Average test loss: 0.002753680282582839\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05175809943013721\n",
      "Average test loss: 0.0028021911467529005\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0512414706548055\n",
      "Average test loss: 0.0027673870302322837\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05074361560079786\n",
      "Average test loss: 0.002813795531479021\n",
      "Epoch 30/300\n",
      "Average training loss: 0.050178163200616833\n",
      "Average test loss: 0.002740783386553327\n",
      "Epoch 31/300\n",
      "Average training loss: 0.049921445551845765\n",
      "Average test loss: 0.002731325424586733\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04935511658589045\n",
      "Average test loss: 0.0026532380071779093\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04896447782715162\n",
      "Average test loss: 0.0026293908370037876\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04860995408727063\n",
      "Average test loss: 0.0026886941708831325\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04826855549878544\n",
      "Average test loss: 0.00264728954496483\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04800587366686927\n",
      "Average test loss: 0.0026970109982002114\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04775209292603864\n",
      "Average test loss: 0.0025991743045548597\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04750846149192916\n",
      "Average test loss: 0.002597019283307923\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04719047970904244\n",
      "Average test loss: 0.002669428361372815\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04706360400716464\n",
      "Average test loss: 0.0026115489720056452\n",
      "Epoch 41/300\n",
      "Average training loss: 0.046270263264576596\n",
      "Average test loss: 0.0027526456196275023\n",
      "Epoch 45/300\n",
      "Average training loss: 0.046188752306832205\n",
      "Average test loss: 0.0025397020439720816\n",
      "Epoch 46/300\n",
      "Average training loss: 0.046010305242406\n",
      "Average test loss: 0.0025424672975722287\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04577768882777956\n",
      "Average test loss: 0.005351868170830939\n",
      "Epoch 48/300\n",
      "Average training loss: 0.045814337809880576\n",
      "Average test loss: 0.002590036225194732\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04570505584610833\n",
      "Average test loss: 0.002649538256434931\n",
      "Epoch 50/300\n",
      "Average training loss: 0.045419965744018555\n",
      "Average test loss: 0.002517460906671153\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04543675230609046\n",
      "Average test loss: 0.002588649036362767\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04523331774605645\n",
      "Average test loss: 0.002832299389772945\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04524196413821644\n",
      "Average test loss: 0.0025197788727366263\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04515203262699975\n",
      "Average test loss: 0.002534984276526504\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04497926170296139\n",
      "Average test loss: 0.0025036822391880883\n",
      "Epoch 56/300\n",
      "Average training loss: 0.044956574774450725\n",
      "Average test loss: 0.002516169167537656\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0448351635005739\n",
      "Average test loss: 0.002560162385097808\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04474835378925006\n",
      "Average test loss: 0.0025338347748749784\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04459461883041594\n",
      "Average test loss: 0.003400196355664068\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04461578388346566\n",
      "Average test loss: 0.028432499256812863\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04452521722184287\n",
      "Average test loss: 0.0025210384020788804\n",
      "Epoch 62/300\n",
      "Average training loss: 0.044439306394921406\n",
      "Average test loss: 0.002719419104978442\n",
      "Epoch 63/300\n",
      "Average training loss: 0.044485562331146665\n",
      "Average test loss: 0.002515996382261316\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04433582442998886\n",
      "Average test loss: 0.0025264638680964706\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04423520603775978\n",
      "Average test loss: 0.002501943876966834\n",
      "Epoch 66/300\n",
      "Average training loss: 0.044157830036348766\n",
      "Average test loss: 0.0025293690057264433\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04387005089388953\n",
      "Average test loss: 0.0027006245309280025\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04386413108640247\n",
      "Average test loss: 0.0025657156021851634\n",
      "Epoch 71/300\n",
      "Average training loss: 0.043844330731365413\n",
      "Average test loss: 0.0024812189738990533\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04376574100719558\n",
      "Average test loss: 0.0025339821595698597\n",
      "Epoch 73/300\n",
      "Average training loss: 0.044611070305109024\n",
      "Average test loss: 0.0026590543338615032\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04364474062787162\n",
      "Average test loss: 0.002475268591919707\n",
      "Epoch 75/300\n",
      "Average training loss: 0.043602709316545064\n",
      "Average test loss: 0.0025239624863283502\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04362969406114684\n",
      "Average test loss: 0.002500885962612099\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04354533445835113\n",
      "Average test loss: 0.0025451843875149887\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04361768741077847\n",
      "Average test loss: 0.0024688434046175743\n",
      "Epoch 79/300\n",
      "Average training loss: 0.043390357206265134\n",
      "Average test loss: 0.002659787650530537\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04363472983903355\n",
      "Average test loss: 0.0025039933531855545\n",
      "Epoch 81/300\n",
      "Average training loss: 0.043353197471963034\n",
      "Average test loss: 0.002616072370774216\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04341603750652737\n",
      "Average test loss: 0.0024906713879770705\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04322714267836677\n",
      "Average test loss: 0.0028218003834287325\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04344035233060519\n",
      "Average test loss: 0.00270362441179653\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04321472701099184\n",
      "Average test loss: 0.002579843986779451\n",
      "Epoch 86/300\n",
      "Average training loss: 0.043167478531599045\n",
      "Average test loss: 0.002482018550030059\n",
      "Epoch 87/300\n",
      "Average training loss: 0.043094824983014\n",
      "Average test loss: 0.002545952839983834\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04306410767303573\n",
      "Average test loss: 0.0026367414467450647\n",
      "Epoch 89/300\n",
      "Average training loss: 0.043029888268974095\n",
      "Average test loss: 0.0030352676730189057\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0430488218234645\n",
      "Average test loss: 0.0025885058918760882\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04300460727678405\n",
      "Average test loss: 0.0030609762171904245\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04296640991502338\n",
      "Average test loss: 0.0024839105032798316\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04287810680601332\n",
      "Average test loss: 0.002489294865892993\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04285865815314982\n",
      "Average test loss: 0.0025963800214231013\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04286316888199912\n",
      "Average test loss: 0.004067787631932232\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04277555659910043\n",
      "Average test loss: 0.0024751017679356868\n",
      "Epoch 97/300\n",
      "Average training loss: 0.042739675144354505\n",
      "Average test loss: 0.002586795023952921\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04254789332217641\n",
      "Average test loss: 0.002501410038313932\n",
      "Epoch 101/300\n",
      "Average training loss: 0.042631945679585136\n",
      "Average test loss: 0.0025050762502683535\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04260410712162654\n",
      "Average test loss: 0.0024749308302998542\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04249793853031265\n",
      "Average test loss: 0.002718191160923905\n",
      "Epoch 104/300\n",
      "Average training loss: 0.042499962429205576\n",
      "Average test loss: 0.002549722960425748\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04244885359538926\n",
      "Average test loss: 0.17680154129200512\n",
      "Epoch 109/300\n",
      "Average training loss: 0.042496526953246856\n",
      "Average test loss: 0.0025690752872162396\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04235294695032967\n",
      "Average test loss: 0.0025126240843286116\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04245666106541952\n",
      "Average test loss: 0.0028302215155627994\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04225652643375927\n",
      "Average test loss: 0.0026377939041703938\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04225442745950487\n",
      "Average test loss: 0.004815836790121264\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04223927267723614\n",
      "Average test loss: 0.002526609625460373\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04208261780606376\n",
      "Average test loss: 0.002492125567359229\n",
      "Epoch 116/300\n",
      "Average training loss: 0.042269515487882825\n",
      "Average test loss: 0.002539736967533827\n",
      "Epoch 117/300\n",
      "Average training loss: 0.042087566326061886\n",
      "Average test loss: 0.003469487271996008\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04217805097500483\n",
      "Average test loss: 0.0025168355864783128\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04206138236655129\n",
      "Average test loss: 0.002532313571828935\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04197980663511488\n",
      "Average test loss: 0.0038376949425372813\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04210665597683853\n",
      "Average test loss: 0.002700376069587138\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04193392089671559\n",
      "Average test loss: 0.0024606438076330557\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04198335419429673\n",
      "Average test loss: 0.002485403493253721\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04192444940076934\n",
      "Average test loss: 0.0027023036353704\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04197420499059889\n",
      "Average test loss: 0.002510821354885896\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04205634059177504\n",
      "Average test loss: 0.0025126850720908907\n",
      "Epoch 127/300\n",
      "Average training loss: 0.041849838621086544\n",
      "Average test loss: 0.002516815865619315\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04179154047701094\n",
      "Average test loss: 0.004847782669175002\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04184279634555181\n",
      "Average test loss: 0.002934668927763899\n",
      "Epoch 130/300\n",
      "Average training loss: 0.041801467955112456\n",
      "Average test loss: 0.002512970053901275\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04179649240771929\n",
      "Average test loss: 0.0025541926473379136\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0417381639248795\n",
      "Average test loss: 0.002474893702297575\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04185804051823086\n",
      "Average test loss: 0.0025036610439419745\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04166628957457012\n",
      "Average test loss: 0.002481833019811246\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04168568951719337\n",
      "Average test loss: 0.0025176041018631725\n",
      "Epoch 136/300\n",
      "Average training loss: 0.041658487058348126\n",
      "Average test loss: 0.0025804739739331934\n",
      "Epoch 137/300\n",
      "Average training loss: 0.041560043301847246\n",
      "Average test loss: 0.0025358113934182457\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04163654471105999\n",
      "Average test loss: 0.0025088655340174833\n",
      "Epoch 139/300\n",
      "Average training loss: 0.041569666922092435\n",
      "Average test loss: 0.0026511959232803847\n",
      "Epoch 140/300\n",
      "Average training loss: 0.041620548079411186\n",
      "Average test loss: 0.0028649408813152047\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04140639925665326\n",
      "Average test loss: 0.0025028819385915996\n",
      "Epoch 142/300\n",
      "Average training loss: 0.041579180230697\n",
      "Average test loss: 0.002532810761489802\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04150463165177239\n",
      "Average test loss: 0.002628525335755613\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04156395804385344\n",
      "Average test loss: 0.0024832457858655187\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04143530565169123\n",
      "Average test loss: 0.002983108593771855\n",
      "Epoch 146/300\n",
      "Average training loss: 0.041427359342575076\n",
      "Average test loss: 0.0025071627375566298\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04142773687839508\n",
      "Average test loss: 0.0025319828511112265\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04139269788397683\n",
      "Average test loss: 0.0024679996153960625\n",
      "Epoch 149/300\n",
      "Average training loss: 0.041375043276283476\n",
      "Average test loss: 0.0024836183122048773\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04136076626181603\n",
      "Average test loss: 4.268393676254484\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04137605703539318\n",
      "Average test loss: 0.0029744455805048348\n",
      "Epoch 152/300\n",
      "Average training loss: 0.041323903418249555\n",
      "Average test loss: 0.0024700086638331413\n",
      "Epoch 153/300\n",
      "Average training loss: 0.041254789974954395\n",
      "Average test loss: 0.0025028962081091273\n",
      "Epoch 154/300\n",
      "Average training loss: 0.041351042648156484\n",
      "Average test loss: 0.0024855084617932638\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04117311117053032\n",
      "Average test loss: 0.002489426334078113\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04129121414489216\n",
      "Average test loss: 0.0032612953076346053\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04112725990348392\n",
      "Average test loss: 0.0025177125771426493\n",
      "Epoch 158/300\n",
      "Average training loss: 0.041240252905421784\n",
      "Average test loss: 0.0025373207906054128\n",
      "Epoch 159/300\n",
      "Average training loss: 0.041169651488463084\n",
      "Average test loss: 0.0024846746404137876\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0410807665222221\n",
      "Average test loss: 0.0025188642922374938\n",
      "Epoch 164/300\n",
      "Average training loss: 0.041286858244074714\n",
      "Average test loss: 0.0025126924289183483\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04097799793216917\n",
      "Average test loss: 0.0027611961743483943\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04102364679508739\n",
      "Average test loss: 0.00249032530002296\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04146232377158271\n",
      "Average test loss: 0.002494204670604732\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04093557657135857\n",
      "Average test loss: 0.002516388856805861\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0409101834959454\n",
      "Average test loss: 0.0024922708144618404\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04097445703546206\n",
      "Average test loss: 0.002584133987418479\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04095281402269999\n",
      "Average test loss: 0.0024901494251357184\n",
      "Epoch 172/300\n",
      "Average training loss: 0.040990145941575365\n",
      "Average test loss: 0.002526247213698096\n",
      "Epoch 173/300\n",
      "Average training loss: 0.040906739006439846\n",
      "Average test loss: 0.0026260885275486445\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04091788254512681\n",
      "Average test loss: 0.0026897362454069984\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04088563521703084\n",
      "Average test loss: 0.0025004673002080786\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04092619059814347\n",
      "Average test loss: 0.002605526943380634\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04084939124186834\n",
      "Average test loss: 0.002531740346302589\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040884575542476445\n",
      "Average test loss: 0.0027543569078875914\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04082518914673063\n",
      "Average test loss: 0.002479574099803964\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04214583574732145\n",
      "Average test loss: 0.0025438742509318723\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04069326554404365\n",
      "Average test loss: 0.009625858180638817\n",
      "Epoch 182/300\n",
      "Average training loss: 0.040704499979813895\n",
      "Average test loss: 0.7504246487087674\n",
      "Epoch 183/300\n",
      "Average training loss: 0.040792577074633705\n",
      "Average test loss: 0.002645006864021222\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04074130127165053\n",
      "Average test loss: 0.0025453153560972878\n",
      "Epoch 185/300\n",
      "Average training loss: 0.040780885717935034\n",
      "Average test loss: 0.013135935223764843\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040705868797169795\n",
      "Average test loss: 0.002508553594143854\n",
      "Epoch 187/300\n",
      "Average training loss: 0.040639700141217976\n",
      "Average test loss: 0.002498340731383198\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04062331459257338\n",
      "Average test loss: 0.002882080227136612\n",
      "Epoch 189/300\n",
      "Average training loss: 0.041387577801942826\n",
      "Average test loss: 0.0614938246972031\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04078834466470613\n",
      "Average test loss: 0.0024957216191622946\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04054067592157258\n",
      "Average test loss: 0.002547991128431426\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04059454170531697\n",
      "Average test loss: 0.0025311127772761717\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04056387328108152\n",
      "Average test loss: 0.002632589221207632\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04052846735715866\n",
      "Average test loss: 0.0024975655677003995\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04076739914384153\n",
      "Average test loss: 0.002584068845750557\n",
      "Epoch 196/300\n",
      "Average training loss: 0.040523946175972624\n",
      "Average test loss: 0.0025392023170780803\n",
      "Epoch 197/300\n",
      "Average training loss: 0.040563064283794824\n",
      "Average test loss: 0.03108791706379917\n",
      "Epoch 198/300\n",
      "Average training loss: 0.040639018868406616\n",
      "Average test loss: 0.002657617349177599\n",
      "Epoch 199/300\n",
      "Average training loss: 0.040427118400732674\n",
      "Average test loss: 0.0025403834258516628\n",
      "Epoch 203/300\n",
      "Average training loss: 0.040652332464853926\n",
      "Average test loss: 0.002529412470654481\n",
      "Epoch 204/300\n",
      "Average training loss: 0.040485639062192706\n",
      "Average test loss: 0.08879140313797527\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04042497638199064\n",
      "Average test loss: 0.0025117411439617475\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04040559311542246\n",
      "Average test loss: 0.002638491284309162\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0403740490410063\n",
      "Average test loss: 0.0028050376625938546\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04040675517254406\n",
      "Average test loss: 0.002578730298413171\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04036007616255018\n",
      "Average test loss: 0.002594493518273036\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04048079593314065\n",
      "Average test loss: 0.0024907091756661733\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04037926592429479\n",
      "Average test loss: 0.002538770618951983\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04039616337418556\n",
      "Average test loss: 0.013930315395196279\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04032476665576299\n",
      "Average test loss: 0.0025533786103543307\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04034437328908178\n",
      "Average test loss: 0.0036380357676082187\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04045987838506698\n",
      "Average test loss: 0.0025414585162782007\n",
      "Epoch 216/300\n",
      "Average training loss: 0.040242604283822904\n",
      "Average test loss: 0.002587711500003934\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04022255429294374\n",
      "Average test loss: 0.0028091066506587795\n",
      "Epoch 218/300\n",
      "Average training loss: 0.040285552614265016\n",
      "Average test loss: 0.0026260659661557937\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04027977693080902\n",
      "Average test loss: 0.00251809128270381\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04020535626676348\n",
      "Average test loss: 0.002600421191503604\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04029338569442431\n",
      "Average test loss: 0.002503682519826624\n",
      "Epoch 222/300\n",
      "Average training loss: 0.040176171600818635\n",
      "Average test loss: 0.0025381000431047547\n",
      "Epoch 223/300\n",
      "Average training loss: 0.040159111344152024\n",
      "Average test loss: 0.002528239155188203\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04083614890442954\n",
      "Average test loss: 0.0025647619979249107\n",
      "Epoch 228/300\n",
      "Average training loss: 0.039986344632175236\n",
      "Average test loss: 0.002521440031627814\n",
      "Epoch 229/300\n",
      "Average training loss: 0.040051783323287966\n",
      "Average test loss: 0.0027304984629154204\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04007265736328231\n",
      "Average test loss: 0.0025301119731739163\n",
      "Epoch 231/300\n",
      "Average training loss: 0.040150990910000274\n",
      "Average test loss: 0.002650683202677303\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04004554203318225\n",
      "Average test loss: 0.0026387462529043355\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04013052905268139\n",
      "Average test loss: 0.0027105530875010625\n",
      "Epoch 234/300\n",
      "Average training loss: 0.040073532028330694\n",
      "Average test loss: 0.0025760199611799582\n",
      "Epoch 235/300\n",
      "Average training loss: 0.040128590517573885\n",
      "Average test loss: 0.0025670358940131136\n",
      "Epoch 236/300\n",
      "Average training loss: 0.040072243127557965\n",
      "Average test loss: 0.0026106714519361653\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04011313501331541\n",
      "Average test loss: 0.002571935721983512\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04014028766089016\n",
      "Average test loss: 0.00255142982945674\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04009177141388257\n",
      "Average test loss: 0.0032506549428734516\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0400052171183957\n",
      "Average test loss: 0.002550908860853977\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04002006307244301\n",
      "Average test loss: 0.0027334485948085786\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03997470584511757\n",
      "Average test loss: 0.0032632613809158405\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03996328503555722\n",
      "Average test loss: 0.002527009872512685\n",
      "Epoch 244/300\n",
      "Average training loss: 0.039914009590943655\n",
      "Average test loss: 0.0039015878306494817\n",
      "Epoch 245/300\n",
      "Average training loss: 0.040011000523964564\n",
      "Average test loss: 0.002686344552371237\n",
      "Epoch 246/300\n",
      "Average training loss: 0.040199674838119086\n",
      "Average test loss: 0.0025855271814184057\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03986495063371129\n",
      "Average test loss: 0.002536474629201823\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03994166805346807\n",
      "Average test loss: 0.0026327766974767047\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03986275373564826\n",
      "Average test loss: 0.005733079520157642\n",
      "Epoch 250/300\n",
      "Average training loss: 0.040076694283220506\n",
      "Average test loss: 0.0025885783415287734\n",
      "Epoch 251/300\n",
      "Average training loss: 0.039886203424798115\n",
      "Average test loss: 0.0025358397463957467\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03985459927055571\n",
      "Average test loss: 0.002539945887815621\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03990393669075436\n",
      "Average test loss: 0.002807569197896454\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03993291962809033\n",
      "Average test loss: 0.00254465944889105\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03995061653190189\n",
      "Average test loss: 0.002775792268001371\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03990788737932841\n",
      "Average test loss: 0.002672046459797356\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03993983973066012\n",
      "Average test loss: 0.0026457413143167892\n",
      "Epoch 258/300\n",
      "Average training loss: 0.039858267592059245\n",
      "Average test loss: 0.0025389668587595224\n",
      "Epoch 259/300\n",
      "Average training loss: 0.039839615556928845\n",
      "Average test loss: 0.0025415473396165505\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03981048419078191\n",
      "Average test loss: 0.00285906009727882\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04018933365742366\n",
      "Average test loss: 0.00250377247399754\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0397388519346714\n",
      "Average test loss: 0.0025390391767852838\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03975739395287302\n",
      "Average test loss: 0.0025622052101211415\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0398440088795291\n",
      "Average test loss: 0.0025439676414761277\n",
      "Epoch 265/300\n",
      "Average training loss: 0.039745952434009975\n",
      "Average test loss: 0.003228572203260329\n",
      "Epoch 266/300\n",
      "Average training loss: 0.039681465082698396\n",
      "Average test loss: 0.002912696100667947\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03974145538277096\n",
      "Average test loss: 0.0026216625332211455\n",
      "Epoch 268/300\n",
      "Average training loss: 0.039763777633508045\n",
      "Average test loss: 0.002556732653743691\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03975944670041402\n",
      "Average test loss: 0.002647373762395647\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04006811239653164\n",
      "Average test loss: 0.0026411320138722658\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0396659420993593\n",
      "Average test loss: 0.0025887002918041414\n",
      "Epoch 272/300\n",
      "Average training loss: 0.039629616929425134\n",
      "Average test loss: 0.0025507264714688064\n",
      "Epoch 273/300\n",
      "Average training loss: 0.039688320580455995\n",
      "Average test loss: 0.002554470791377955\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03970922914478514\n",
      "Average test loss: 0.002591357318891419\n",
      "Epoch 275/300\n",
      "Average training loss: 0.039778414941496316\n",
      "Average test loss: 0.0025290625718318756\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039709641642040674\n",
      "Average test loss: 0.009636912199358146\n",
      "Epoch 277/300\n",
      "Average training loss: 0.039599611974424784\n",
      "Average test loss: 0.03334305639399422\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03978294416268666\n",
      "Average test loss: 0.0025835463752349216\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03963049322035578\n",
      "Average test loss: 0.002583400735631585\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03962524069348971\n",
      "Average test loss: 0.002562514981254935\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03966066911816597\n",
      "Average test loss: 0.002573537080238263\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03981561266713672\n",
      "Average test loss: 0.10908124465412564\n",
      "Epoch 283/300\n",
      "Average training loss: 0.039592625190814336\n",
      "Average test loss: 0.0027066982916245856\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03963336624039544\n",
      "Average test loss: 0.00259743808168504\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03960374614927504\n",
      "Average test loss: 0.0033805470623903804\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0396651338868671\n",
      "Average test loss: 0.002582616534498003\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03958050956825415\n",
      "Average test loss: 0.0025570720500416224\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03965484697620074\n",
      "Average test loss: 0.0025315365160091057\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0399033359752761\n",
      "Average test loss: 0.0028064584374013876\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03947265420357386\n",
      "Average test loss: 0.002838869798100657\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03950963546170129\n",
      "Average test loss: 0.0026559988581058053\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03952592107984755\n",
      "Average test loss: 0.0025902414472980632\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03947821111811532\n",
      "Average test loss: 0.0025697173283745845\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03957391600145234\n",
      "Average test loss: 0.0030523089749945535\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03989809489912457\n",
      "Average test loss: 0.0026389183528307413\n",
      "Epoch 296/300\n",
      "Average training loss: 0.039458723614613216\n",
      "Average test loss: 0.0026231590550806786\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03946279891663128\n",
      "Average test loss: 0.008516224693506956\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03943739196989272\n",
      "Average test loss: 0.0025562863685190678\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03974412287274996\n",
      "Average test loss: 0.002523257758261429\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03944362465871705\n",
      "Average test loss: 0.0029730237285710043\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth3/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.12\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.32\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.72\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.19\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.35\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.47\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.60\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.66\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.08\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.19\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.13\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.48\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.81\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.02\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.31\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.37\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.41\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.47\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.55\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.56\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.55\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.00\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.55\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.55\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.55\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.57\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.119542094124688\n",
      "Average test loss: 0.015864242278039457\n",
      "Epoch 2/300\n",
      "Average training loss: 2.8118924999237063\n",
      "Average test loss: 0.011438821845584446\n",
      "Epoch 3/300\n",
      "Average training loss: 1.7733702626758152\n",
      "Average test loss: 0.010352713471485508\n",
      "Epoch 4/300\n",
      "Average training loss: 1.2113547609647115\n",
      "Average test loss: 0.010094747164183192\n",
      "Epoch 5/300\n",
      "Average training loss: 0.8821588178740607\n",
      "Average test loss: 0.009168474615448051\n",
      "Epoch 6/300\n",
      "Average training loss: 0.6764970213572185\n",
      "Average test loss: 0.011411783911287785\n",
      "Epoch 7/300\n",
      "Average training loss: 0.5528867750697666\n",
      "Average test loss: 0.008606575486560663\n",
      "Epoch 8/300\n",
      "Average training loss: 0.47884594525231255\n",
      "Average test loss: 0.008614603896107938\n",
      "Epoch 9/300\n",
      "Average training loss: 0.4299452804989285\n",
      "Average test loss: 0.00845354657413231\n",
      "Epoch 10/300\n",
      "Average training loss: 0.39847463904486763\n",
      "Average test loss: 0.010674206876092487\n",
      "Epoch 11/300\n",
      "Average training loss: 0.375467923561732\n",
      "Average test loss: 0.01021959968490733\n",
      "Epoch 12/300\n",
      "Average training loss: 0.354983480029636\n",
      "Average test loss: 0.015257249802350999\n",
      "Epoch 13/300\n",
      "Average training loss: 0.33975391070048017\n",
      "Average test loss: 0.010222553893095917\n",
      "Epoch 14/300\n",
      "Average training loss: 0.32777271906534833\n",
      "Average test loss: 0.008548945585058795\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3207144393920898\n",
      "Average test loss: 0.007734035533749395\n",
      "Epoch 16/300\n",
      "Average training loss: 0.30896263281504316\n",
      "Average test loss: 0.009752028489278423\n",
      "Epoch 17/300\n",
      "Average training loss: 0.30232963281207614\n",
      "Average test loss: 0.007803088174511989\n",
      "Epoch 18/300\n",
      "Average training loss: 0.29486640469233194\n",
      "Average test loss: 0.007953533738023705\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2868549847602844\n",
      "Average test loss: 0.00874390722024772\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2795736973285675\n",
      "Average test loss: 0.007230055481609371\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2698876604239146\n",
      "Average test loss: 0.007362923913117912\n",
      "Epoch 22/300\n",
      "Average training loss: 0.2669923391474618\n",
      "Average test loss: 0.00738692317571905\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2603749921056959\n",
      "Average test loss: 0.008484160020119614\n",
      "Epoch 24/300\n",
      "Average training loss: 0.25646774135695566\n",
      "Average test loss: 0.0071883554889096155\n",
      "Epoch 25/300\n",
      "Average training loss: 0.250980897810724\n",
      "Average test loss: 0.006963394540879461\n",
      "Epoch 26/300\n",
      "Average training loss: 0.24846994894080693\n",
      "Average test loss: 0.007159689140402608\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2441022957166036\n",
      "Average test loss: 0.007750169360803233\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2407028996679518\n",
      "Average test loss: 0.006943177996410264\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2377365953789817\n",
      "Average test loss: 0.006838205761379666\n",
      "Epoch 30/300\n",
      "Average training loss: 0.23514389930831062\n",
      "Average test loss: 0.007016283506320583\n",
      "Epoch 31/300\n",
      "Average training loss: 0.23222549443774754\n",
      "Average test loss: 0.006738578842745887\n",
      "Epoch 32/300\n",
      "Average training loss: 0.23039411175251007\n",
      "Average test loss: 0.006765923616786798\n",
      "Epoch 33/300\n",
      "Average training loss: 0.2276623202694787\n",
      "Average test loss: 0.00802533015732964\n",
      "Epoch 34/300\n",
      "Average training loss: 0.22688241244686974\n",
      "Average test loss: 0.006727529220696952\n",
      "Epoch 35/300\n",
      "Average training loss: 0.224768523812294\n",
      "Average test loss: 0.006874595451686117\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2232486745119095\n",
      "Average test loss: 0.006677118855218093\n",
      "Epoch 37/300\n",
      "Average training loss: 0.22107428505685595\n",
      "Average test loss: 0.006746930284632577\n",
      "Epoch 38/300\n",
      "Average training loss: 0.2205149852567249\n",
      "Average test loss: 0.006582167994644907\n",
      "Epoch 39/300\n",
      "Average training loss: 0.21960398557451036\n",
      "Average test loss: 0.007005016510685285\n",
      "Epoch 40/300\n",
      "Average training loss: 0.21747304877969953\n",
      "Average test loss: 0.007329835370182991\n",
      "Epoch 41/300\n",
      "Average training loss: 0.21683318894439274\n",
      "Average test loss: 0.006704351325829824\n",
      "Epoch 42/300\n",
      "Average training loss: 0.2151402718623479\n",
      "Average test loss: 0.014051487735162179\n",
      "Epoch 43/300\n",
      "Average training loss: 0.21434424551328024\n",
      "Average test loss: 0.006916820772820049\n",
      "Epoch 44/300\n",
      "Average training loss: 0.21371826881832548\n",
      "Average test loss: 0.006814701589445273\n",
      "Epoch 45/300\n",
      "Average training loss: 0.21260448763105605\n",
      "Average test loss: 0.007294320736494329\n",
      "Epoch 46/300\n",
      "Average training loss: 0.21160298381911383\n",
      "Average test loss: 0.006729884101698796\n",
      "Epoch 47/300\n",
      "Average training loss: 0.21118459465768602\n",
      "Average test loss: 0.006509923940317498\n",
      "Epoch 48/300\n",
      "Average training loss: 0.21018703953425089\n",
      "Average test loss: 0.00657050265206231\n",
      "Epoch 49/300\n",
      "Average training loss: 0.20923991110589768\n",
      "Average test loss: 0.006618439498047034\n",
      "Epoch 50/300\n",
      "Average training loss: 0.2089766904115677\n",
      "Average test loss: 0.0066301312413480544\n",
      "Epoch 51/300\n",
      "Average training loss: 0.2087520096566942\n",
      "Average test loss: 0.006411102906283405\n",
      "Epoch 52/300\n",
      "Average training loss: 0.20864753258228302\n",
      "Average test loss: 0.007837822568913301\n",
      "Epoch 53/300\n",
      "Average training loss: 0.20681187370088366\n",
      "Average test loss: 0.006555700042181545\n",
      "Epoch 54/300\n",
      "Average training loss: 0.2088412370549308\n",
      "Average test loss: 0.006568348308404286\n",
      "Epoch 55/300\n",
      "Average training loss: 0.20666396945052676\n",
      "Average test loss: 0.006723586200426022\n",
      "Epoch 56/300\n",
      "Average training loss: 0.20555842490990955\n",
      "Average test loss: 0.006479045305815008\n",
      "Epoch 57/300\n",
      "Average training loss: 0.20388167226314544\n",
      "Average test loss: 0.006618524670600891\n",
      "Epoch 58/300\n",
      "Average training loss: 0.20399765209356943\n",
      "Average test loss: 0.00661235163691971\n",
      "Epoch 59/300\n",
      "Average training loss: 0.20280265935262043\n",
      "Average test loss: 0.00655255357755555\n",
      "Epoch 60/300\n",
      "Average training loss: 0.20259890372223324\n",
      "Average test loss: 0.00659897447253267\n",
      "Epoch 61/300\n",
      "Average training loss: 0.20239283861054314\n",
      "Average test loss: 0.006353368163936668\n",
      "Epoch 62/300\n",
      "Average training loss: 0.20236227565341525\n",
      "Average test loss: 0.006749889735546377\n",
      "Epoch 63/300\n",
      "Average training loss: 0.20366417043738894\n",
      "Average test loss: 0.007525751453720861\n",
      "Epoch 64/300\n",
      "Average training loss: 0.20054169211122724\n",
      "Average test loss: 0.006366257705208328\n",
      "Epoch 65/300\n",
      "Average training loss: 0.20043169309033287\n",
      "Average test loss: 0.006660323231584496\n",
      "Epoch 66/300\n",
      "Average training loss: 0.19963574112786187\n",
      "Average test loss: 0.006380066829009189\n",
      "Epoch 67/300\n",
      "Average training loss: 0.200728223045667\n",
      "Average test loss: 0.006479037940088246\n",
      "Epoch 68/300\n",
      "Average training loss: 0.20004026356008317\n",
      "Average test loss: 0.006712795823812485\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1985052525467343\n",
      "Average test loss: 0.057142427563667296\n",
      "Epoch 70/300\n",
      "Average training loss: 0.19799032362302144\n",
      "Average test loss: 12.440316402223376\n",
      "Epoch 71/300\n",
      "Average training loss: 0.19756438442071278\n",
      "Average test loss: 0.006480280443198151\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1982084801197052\n",
      "Average test loss: 0.006278459163175689\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1979870969719357\n",
      "Average test loss: 0.007363866484827466\n",
      "Epoch 74/300\n",
      "Average training loss: 0.19614333673318227\n",
      "Average test loss: 0.0063532977278033895\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1965689628124237\n",
      "Average test loss: 0.006441824530561765\n",
      "Epoch 76/300\n",
      "Average training loss: 0.19630792966153887\n",
      "Average test loss: 0.006446881983843114\n",
      "Epoch 77/300\n",
      "Average training loss: 0.195925038907263\n",
      "Average test loss: 0.0065014233791993725\n",
      "Epoch 78/300\n",
      "Average training loss: 0.19500744067298043\n",
      "Average test loss: 0.0063844404564135605\n",
      "Epoch 79/300\n",
      "Average training loss: 0.19445296108722687\n",
      "Average test loss: 0.006706974874354071\n",
      "Epoch 80/300\n",
      "Average training loss: 0.19902089868651496\n",
      "Average test loss: 0.006290174294677046\n",
      "Epoch 81/300\n",
      "Average training loss: 0.19373407092359332\n",
      "Average test loss: 0.00646996233281162\n",
      "Epoch 82/300\n",
      "Average training loss: 0.1936182661851247\n",
      "Average test loss: 0.0064936197722951575\n",
      "Epoch 83/300\n",
      "Average training loss: 0.19393013450834487\n",
      "Average test loss: 0.006479414036704434\n",
      "Epoch 84/300\n",
      "Average training loss: 0.19377149544821845\n",
      "Average test loss: 0.006461526424520546\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1931009913418028\n",
      "Average test loss: 0.006303049060205619\n",
      "Epoch 86/300\n",
      "Average training loss: 0.19270195946428512\n",
      "Average test loss: 0.017255823011199635\n",
      "Epoch 87/300\n",
      "Average training loss: 0.19384487238195208\n",
      "Average test loss: 0.007123246666871839\n",
      "Epoch 90/300\n",
      "Average training loss: 0.19173547579182518\n",
      "Average test loss: 0.006673849850893021\n",
      "Epoch 91/300\n",
      "Average training loss: 0.19103644491566552\n",
      "Average test loss: 162179.2830625\n",
      "Epoch 92/300\n",
      "Average training loss: 0.19259218789471522\n",
      "Average test loss: 0.006288955384658443\n",
      "Epoch 93/300\n",
      "Average training loss: 0.19255385927359264\n",
      "Average test loss: 0.006884730048063729\n",
      "Epoch 94/300\n",
      "Average training loss: 0.19067392557197146\n",
      "Average test loss: 0.006484460901882913\n",
      "Epoch 95/300\n",
      "Average training loss: 0.18937079971366458\n",
      "Average test loss: 0.006600690968334675\n",
      "Epoch 96/300\n",
      "Average training loss: 0.18970363422234854\n",
      "Average test loss: 0.006416369096272522\n",
      "Epoch 97/300\n",
      "Average training loss: 0.19118733194139267\n",
      "Average test loss: 0.007522011610368888\n",
      "Epoch 98/300\n",
      "Average training loss: 0.18966102519300249\n",
      "Average test loss: 0.006542160231620073\n",
      "Epoch 99/300\n",
      "Average training loss: 0.18896913700633577\n",
      "Average test loss: 0.006360279783192608\n",
      "Epoch 101/300\n",
      "Average training loss: 0.18857103594144187\n",
      "Average test loss: 0.006328542189051708\n",
      "Epoch 102/300\n",
      "Average training loss: 0.19036419661177528\n",
      "Average test loss: 0.0063131904697252645\n",
      "Epoch 103/300\n",
      "Average training loss: 0.1880534744527605\n",
      "Average test loss: 0.052881100846661463\n",
      "Epoch 104/300\n",
      "Average training loss: 0.18764171805646684\n",
      "Average test loss: 0.006646086243291696\n",
      "Epoch 105/300\n",
      "Average training loss: 0.1867234060102039\n",
      "Average test loss: 0.006271668418414063\n",
      "Epoch 108/300\n",
      "Average training loss: 0.1901883834997813\n",
      "Average test loss: 0.19555444376998476\n",
      "Epoch 109/300\n",
      "Average training loss: 0.18674971217579311\n",
      "Average test loss: 0.006465850039902661\n",
      "Epoch 110/300\n",
      "Average training loss: 0.18734812468952602\n",
      "Average test loss: 0.00717953981914454\n",
      "Epoch 111/300\n",
      "Average training loss: 0.1859202830725246\n",
      "Average test loss: 0.006228013686421845\n",
      "Epoch 114/300\n",
      "Average training loss: 0.18691614271534815\n",
      "Average test loss: 333938167957.2764\n",
      "Epoch 115/300\n",
      "Average training loss: 0.18608904655774433\n",
      "Average test loss: 0.03214505025868614\n",
      "Epoch 116/300\n",
      "Average training loss: 0.18572998246881697\n",
      "Average test loss: 0.006526674004478587\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1841858052942488\n",
      "Average test loss: 0.006774913053959608\n",
      "Epoch 118/300\n",
      "Average training loss: 0.18440132480197483\n",
      "Average test loss: 0.0068133138095339135\n",
      "Epoch 119/300\n",
      "Average training loss: 0.18470615741941665\n",
      "Average test loss: 0.006398881404143241\n",
      "Epoch 120/300\n",
      "Average training loss: 0.18501161687903933\n",
      "Average test loss: 0.006364335552685791\n",
      "Epoch 121/300\n",
      "Average training loss: 0.18353610032134585\n",
      "Average test loss: 0.0065239188944300015\n",
      "Epoch 122/300\n",
      "Average training loss: 0.18338134360313416\n",
      "Average test loss: 0.006465880500773589\n",
      "Epoch 123/300\n",
      "Average training loss: 0.18383545965618558\n",
      "Average test loss: 0.006306547350767586\n",
      "Epoch 124/300\n",
      "Average training loss: 0.18948611504501767\n",
      "Average test loss: 0.007036850006216102\n",
      "Epoch 125/300\n",
      "Average training loss: 0.18316187942028045\n",
      "Average test loss: 0.006302440151572227\n",
      "Epoch 126/300\n",
      "Average training loss: 0.18267605533864764\n",
      "Average test loss: 0.008632886422177155\n",
      "Epoch 129/300\n",
      "Average training loss: 0.18239957728650835\n",
      "Average test loss: 0.053433559584948755\n",
      "Epoch 130/300\n",
      "Average training loss: 0.18218390182654062\n",
      "Average test loss: 36273041862062.53\n",
      "Epoch 131/300\n",
      "Average training loss: 0.186577003147867\n",
      "Average test loss: 0.006714750475767586\n",
      "Epoch 132/300\n",
      "Average training loss: 0.18099911287095813\n",
      "Average test loss: 0.00634983519381947\n",
      "Epoch 133/300\n",
      "Average training loss: 0.1816357979774475\n",
      "Average test loss: 0.007452126442558236\n",
      "Epoch 134/300\n",
      "Average training loss: 0.18191657617357043\n",
      "Average test loss: 0.006415457950698005\n",
      "Epoch 135/300\n",
      "Average training loss: 0.18126004338264465\n",
      "Average test loss: 0.006331874592436684\n",
      "Epoch 136/300\n",
      "Average training loss: 0.1806043427652783\n",
      "Average test loss: 0.0072985441899961895\n",
      "Epoch 137/300\n",
      "Average training loss: 0.1812785053120719\n",
      "Average test loss: 0.0063900757618248465\n",
      "Epoch 138/300\n",
      "Average training loss: 0.18191672248310514\n",
      "Average test loss: 0.007640636335230536\n",
      "Epoch 139/300\n",
      "Average training loss: 0.18031440330876244\n",
      "Average test loss: 0.022723667913840877\n",
      "Epoch 140/300\n",
      "Average training loss: 0.18038648642434013\n",
      "Average test loss: 6315718.451222222\n",
      "Epoch 141/300\n",
      "Average training loss: 0.18164867492516837\n",
      "Average test loss: 0.006449667957921823\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1796910395754708\n",
      "Average test loss: 0.0074781998900903595\n",
      "Epoch 143/300\n",
      "Average training loss: 0.17960870316293504\n",
      "Average test loss: 0.013451449252665042\n",
      "Epoch 144/300\n",
      "Average training loss: 0.1853235785431332\n",
      "Average test loss: 0.006571835489736663\n",
      "Epoch 145/300\n",
      "Average training loss: 0.17989478260940975\n",
      "Average test loss: 0.0064982828837301995\n",
      "Epoch 146/300\n",
      "Average training loss: 0.17826663789484237\n",
      "Average test loss: 0.00644200348150399\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1788715976079305\n",
      "Average test loss: 0.006572067318691147\n",
      "Epoch 148/300\n",
      "Average training loss: 0.17840976079305013\n",
      "Average test loss: 0.006656153957876894\n",
      "Epoch 149/300\n",
      "Average training loss: 0.1792360943026013\n",
      "Average test loss: 0.013165651922424634\n",
      "Epoch 150/300\n",
      "Average training loss: 0.17850877377722\n",
      "Average test loss: 0.006631323461731275\n",
      "Epoch 151/300\n",
      "Average training loss: 0.18218035683366987\n",
      "Average test loss: 0.007224635606838597\n",
      "Epoch 152/300\n",
      "Average training loss: 0.17971261235078176\n",
      "Average test loss: 0.006564218970636527\n",
      "Epoch 153/300\n",
      "Average training loss: 0.17754320539368523\n",
      "Average test loss: 0.006609571755760246\n",
      "Epoch 154/300\n",
      "Average training loss: 0.17750797180334726\n",
      "Average test loss: 0.006485557649284601\n",
      "Epoch 155/300\n",
      "Average training loss: 0.17752569927109613\n",
      "Average test loss: 0.006519563964671559\n",
      "Epoch 156/300\n",
      "Average training loss: 0.17862188552485572\n",
      "Average test loss: 0.006607606910169124\n",
      "Epoch 157/300\n",
      "Average training loss: 0.1772722840309143\n",
      "Average test loss: 0.006456922368870841\n",
      "Epoch 158/300\n",
      "Average training loss: 0.17743413358264498\n",
      "Average test loss: 0.006835224624723196\n",
      "Epoch 159/300\n",
      "Average training loss: 0.17738766151004368\n",
      "Average test loss: 0.010942929369293981\n",
      "Epoch 160/300\n",
      "Average training loss: 0.17839008042547438\n",
      "Average test loss: 0.006421109117567539\n",
      "Epoch 161/300\n",
      "Average training loss: 0.17664999735355377\n",
      "Average test loss: 0.00690715038528045\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1762783492008845\n",
      "Average test loss: 0.04527422586000628\n",
      "Epoch 165/300\n",
      "Average training loss: 0.17557173432244194\n",
      "Average test loss: 0.00650317885643906\n",
      "Epoch 166/300\n",
      "Average training loss: 0.1768637774652905\n",
      "Average test loss: 0.006529770026604335\n",
      "Epoch 167/300\n",
      "Average training loss: 0.17568691254986657\n",
      "Average test loss: 0.006609827092538277\n",
      "Epoch 168/300\n",
      "Average training loss: 0.1791545613474316\n",
      "Average test loss: 0.0063828107350402405\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1763199420902464\n",
      "Average test loss: 0.006434095607449611\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1751100407573912\n",
      "Average test loss: 0.0066545223775837156\n",
      "Epoch 171/300\n",
      "Average training loss: 0.1752530434926351\n",
      "Average test loss: 0.006418168136643039\n",
      "Epoch 172/300\n",
      "Average training loss: 0.17587313286463419\n",
      "Average test loss: 0.00688745444185204\n",
      "Epoch 173/300\n",
      "Average training loss: 0.1777258782916599\n",
      "Average test loss: 0.0069520970707138375\n",
      "Epoch 174/300\n",
      "Average training loss: 0.1780397965643141\n",
      "Average test loss: 0.007968134083681636\n",
      "Epoch 175/300\n",
      "Average training loss: 0.17462343984180026\n",
      "Average test loss: 0.006382216158840391\n",
      "Epoch 176/300\n",
      "Average training loss: 0.1742754405869378\n",
      "Average test loss: 0.006420922949082321\n",
      "Epoch 177/300\n",
      "Average training loss: 0.17476512942049238\n",
      "Average test loss: 0.006488971530977223\n",
      "Epoch 178/300\n",
      "Average training loss: 0.17440283184581332\n",
      "Average test loss: 0.008044405159850915\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1749151670998997\n",
      "Average test loss: 0.007742710231078995\n",
      "Epoch 180/300\n",
      "Average training loss: 0.17768255983458625\n",
      "Average test loss: 0.0066739823607107\n",
      "Epoch 181/300\n",
      "Average training loss: 0.17380796076191796\n",
      "Average training loss: 0.17323045282893712\n",
      "Average test loss: 0.018723291910356946\n",
      "Epoch 185/300\n",
      "Average training loss: 0.17364461105399662\n",
      "Average test loss: 0.006430068710611926\n",
      "Epoch 186/300\n",
      "Average training loss: 0.17475510892603133\n",
      "Average test loss: 0.006941980796555678\n",
      "Epoch 187/300\n",
      "Average training loss: 0.17416919506920708\n",
      "Average test loss: 0.006741004053089354\n",
      "Epoch 188/300\n",
      "Average training loss: 0.17334218090110354\n",
      "Average test loss: 0.006570141126712163\n",
      "Epoch 189/300\n",
      "Average training loss: 0.17243459655178917\n",
      "Average test loss: 0.006551157208780448\n",
      "Epoch 190/300\n",
      "Average training loss: 0.1735836234887441\n",
      "Average test loss: 0.006521746818390158\n",
      "Epoch 191/300\n",
      "Average training loss: 0.17466130128171709\n",
      "Average test loss: 0.006843834093461434\n",
      "Epoch 192/300\n",
      "Average training loss: 0.17208243107133442\n",
      "Average test loss: 0.006527447426070769\n",
      "Epoch 193/300\n",
      "Average training loss: 0.17258896715111202\n",
      "Average test loss: 0.08504701580603917\n",
      "Epoch 194/300\n",
      "Average training loss: 0.17354507742987738\n",
      "Average test loss: 0.006752488502611716\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1720208499034246\n",
      "Average test loss: 0.0066426548788117035\n",
      "Epoch 196/300\n",
      "Average training loss: 0.17237507814831204\n",
      "Average test loss: 0.007495971623394224\n",
      "Epoch 197/300\n",
      "Average training loss: 0.17214370969931284\n",
      "Average test loss: 0.006618003108435207\n",
      "Epoch 198/300\n",
      "Average training loss: 0.17213153750366636\n",
      "Average test loss: 0.006953575087090333\n",
      "Epoch 199/300\n",
      "Average training loss: 0.1720851163731681\n",
      "Average test loss: 0.009285388506121105\n",
      "Epoch 202/300\n",
      "Average training loss: 0.2182529092364841\n",
      "Average test loss: 0.006365153543651104\n",
      "Epoch 203/300\n",
      "Average training loss: 0.1759806788497501\n",
      "Average test loss: 0.006427508537554079\n",
      "Epoch 204/300\n",
      "Average training loss: 0.1718468763563368\n",
      "Average test loss: 0.007496934295528465\n",
      "Epoch 205/300\n",
      "Average training loss: 0.17173412112394967\n",
      "Average test loss: 0.006519720945507288\n",
      "Epoch 206/300\n",
      "Average training loss: 0.17005969006485408\n",
      "Average test loss: 0.007045837319559521\n",
      "Epoch 207/300\n",
      "Average training loss: 0.17025187272495693\n",
      "Average test loss: 0.0065204070214596056\n",
      "Epoch 208/300\n",
      "Average training loss: 0.17057997442616357\n",
      "Average test loss: 0.006902445762521691\n",
      "Epoch 209/300\n",
      "Average training loss: 0.17077762525611453\n",
      "Average test loss: 0.006626485572920905\n",
      "Epoch 210/300\n",
      "Average training loss: 0.1703579144610299\n",
      "Average test loss: 0.006650076852490505\n",
      "Epoch 211/300\n",
      "Average training loss: 0.17108813162644704\n",
      "Average test loss: 0.008194617898927795\n",
      "Epoch 212/300\n",
      "Average training loss: 0.1713839801814821\n",
      "Average test loss: 0.006640234921541479\n",
      "Epoch 213/300\n",
      "Average training loss: 0.17223021666208904\n",
      "Average test loss: 0.006617207993649774\n",
      "Epoch 214/300\n",
      "Average training loss: 0.17364390116267733\n",
      "Average test loss: 0.006612086210813787\n",
      "Epoch 215/300\n",
      "Average training loss: 0.16993442255920835\n",
      "Average test loss: 0.006683389990280072\n",
      "Epoch 216/300\n",
      "Average training loss: 0.1722420239183638\n",
      "Average test loss: 0.006511949807819393\n",
      "Epoch 219/300\n",
      "Average training loss: 0.1704330898920695\n",
      "Average test loss: 0.006436665739864111\n",
      "Epoch 220/300\n",
      "Average training loss: 0.16972285205788082\n",
      "Average test loss: 0.006491658216549291\n",
      "Epoch 221/300\n",
      "Average training loss: 0.1695669222805235\n",
      "Average test loss: 0.006735376227233145\n",
      "Epoch 222/300\n",
      "Average training loss: 0.17137810046143\n",
      "Average test loss: 0.0066059131456746\n",
      "Epoch 223/300\n",
      "Average training loss: 0.1700681210094028\n",
      "Average test loss: 0.006554810218926933\n",
      "Epoch 224/300\n",
      "Average training loss: 0.17024679851531982\n",
      "Average test loss: 0.008988843110700449\n",
      "Epoch 225/300\n",
      "Average training loss: 0.16903457935651142\n",
      "Average test loss: 0.00650182341121965\n",
      "Epoch 226/300\n",
      "Average training loss: 0.17000760956605276\n",
      "Average test loss: 0.13005693811178207\n",
      "Epoch 227/300\n",
      "Average training loss: 0.17056790986326006\n",
      "Average test loss: 0.00788651030510664\n",
      "Epoch 228/300\n",
      "Average training loss: 0.16961601073212093\n",
      "Average test loss: 0.006494530018419028\n",
      "Epoch 229/300\n",
      "Average training loss: 0.16866071206993527\n",
      "Average test loss: 0.00662048395557536\n",
      "Epoch 230/300\n",
      "Average training loss: 0.16989845632844502\n",
      "Average test loss: 0.006641472477465868\n",
      "Epoch 231/300\n",
      "Average training loss: 0.17482599209414587\n",
      "Average test loss: 0.006502952923377355\n",
      "Epoch 232/300\n",
      "Average training loss: 0.16833496006329854\n",
      "Average test loss: 0.006566961241265138\n",
      "Epoch 233/300\n",
      "Average training loss: 0.16852948358986114\n",
      "Average test loss: 0.0071512447661823695\n",
      "Epoch 234/300\n",
      "Average training loss: 0.1698617884185579\n",
      "Average test loss: 0.009376610526608096\n",
      "Epoch 235/300\n",
      "Average training loss: 0.1679511204428143\n",
      "Average test loss: 0.006543008277813594\n",
      "Epoch 237/300\n",
      "Average training loss: 0.17015195804172092\n",
      "Average test loss: 0.006493268030385176\n",
      "Epoch 238/300\n",
      "Average training loss: 0.16857632496621874\n",
      "Average test loss: 0.006498182182510694\n",
      "Epoch 239/300\n",
      "Average training loss: 0.16780318311850229\n",
      "Average test loss: 0.006540654757784472\n",
      "Epoch 240/300\n",
      "Average training loss: 0.16805785029464299\n",
      "Average test loss: 0.42241049840715195\n",
      "Epoch 241/300\n",
      "Average training loss: 0.17028113253911337\n",
      "Average test loss: 0.010925999672876463\n",
      "Epoch 242/300\n",
      "Average training loss: 0.17057684396372902\n",
      "Average test loss: 0.006659100780056583\n",
      "Epoch 243/300\n",
      "Average training loss: 0.16741528533564673\n",
      "Average test loss: 0.006621650505810976\n",
      "Epoch 244/300\n",
      "Average training loss: 0.16841718011432225\n",
      "Average test loss: 0.006580489031142659\n",
      "Epoch 245/300\n",
      "Average training loss: 0.16828427280320063\n",
      "Average test loss: 0.00713244073631035\n",
      "Epoch 246/300\n",
      "Average training loss: 0.17000489670700497\n",
      "Average test loss: 0.006885526812324921\n",
      "Epoch 247/300\n",
      "Average training loss: 0.16735095148616366\n",
      "Average test loss: 0.006734981346461508\n",
      "Epoch 248/300\n",
      "Average training loss: 0.167886276072926\n",
      "Average test loss: 0.006738595465819041\n",
      "Epoch 249/300\n",
      "Average training loss: 0.16754880367385017\n",
      "Average test loss: 0.009960729121747945\n",
      "Epoch 250/300\n",
      "Average training loss: 0.16695622771978377\n",
      "Average test loss: 0.006618994034826756\n",
      "Epoch 251/300\n",
      "Average training loss: 0.1679603170156479\n",
      "Average test loss: 0.006560677616546552\n",
      "Epoch 252/300\n",
      "Average training loss: 0.16784682675202686\n",
      "Average test loss: 0.01333813419027461\n",
      "Epoch 253/300\n",
      "Average training loss: 0.16806673871146308\n",
      "Average test loss: 0.006513119774146213\n",
      "Epoch 254/300\n",
      "Average training loss: 0.16713404675324758\n",
      "Average test loss: 0.04853150095542272\n",
      "Epoch 257/300\n",
      "Average training loss: 0.16760836707221138\n",
      "Average test loss: 0.007374270715233352\n",
      "Epoch 258/300\n",
      "Average training loss: 0.16650419392850663\n",
      "Average test loss: 0.006667241044756439\n",
      "Epoch 259/300\n",
      "Average training loss: 0.1719391981628206\n",
      "Average test loss: 0.006647785772465997\n",
      "Epoch 260/300\n",
      "Average training loss: 0.16607056256135305\n",
      "Average test loss: 0.006692956178138653\n",
      "Epoch 261/300\n",
      "Average training loss: 0.16642459317048391\n",
      "Average test loss: 0.006695263880822393\n",
      "Epoch 262/300\n",
      "Average training loss: 0.16699747690227296\n",
      "Average test loss: 0.00686205277674728\n",
      "Epoch 263/300\n",
      "Average training loss: 0.1678267588218053\n",
      "Average test loss: 0.006619264261176189\n",
      "Epoch 264/300\n",
      "Average training loss: 0.16679970547888015\n",
      "Average test loss: 0.006580903634428978\n",
      "Epoch 265/300\n",
      "Average training loss: 0.16662179263432822\n",
      "Average test loss: 0.006745182706664006\n",
      "Epoch 266/300\n",
      "Average training loss: 0.16647349361578623\n",
      "Average test loss: 0.3672601562076145\n",
      "Epoch 267/300\n",
      "Average training loss: 0.16618676411443287\n",
      "Average test loss: 0.006896668852203422\n",
      "Epoch 268/300\n",
      "Average training loss: 0.16618910385502708\n",
      "Average test loss: 0.006993772685527802\n",
      "Epoch 269/300\n",
      "Average training loss: 0.1669763104915619\n",
      "Average test loss: 0.007007704990605513\n",
      "Epoch 270/300\n",
      "Average training loss: 0.17105566586388482\n",
      "Average test loss: 0.006619864351219601\n",
      "Epoch 271/300\n",
      "Average training loss: 0.16544454469945696\n",
      "Average test loss: 0.017787456647389465\n",
      "Epoch 272/300\n",
      "Average training loss: 0.16575257070859273\n",
      "Average test loss: 0.006694346353411674\n",
      "Epoch 273/300\n",
      "Average training loss: 0.1655259688032998\n",
      "Average test loss: 0.006879550509568718\n",
      "Epoch 274/300\n",
      "Average training loss: 0.16841667518350814\n",
      "Average test loss: 0.006924871736102634\n",
      "Epoch 275/300\n",
      "Average training loss: 0.16544640587435827\n",
      "Average test loss: 0.006756499124897851\n",
      "Epoch 276/300\n",
      "Average training loss: 0.16536244246694776\n",
      "Average test loss: 0.010625880235185226\n",
      "Epoch 277/300\n",
      "Average training loss: 0.16664871213171217\n",
      "Average test loss: 0.00655805007658071\n",
      "Epoch 278/300\n",
      "Average training loss: 0.16557215746243795\n",
      "Average test loss: 0.006557688584758176\n",
      "Epoch 279/300\n",
      "Average training loss: 0.16565407119194667\n",
      "Average test loss: 0.007304438634051217\n",
      "Epoch 280/300\n",
      "Average training loss: 0.16560116781128778\n",
      "Average test loss: 0.007670209071702428\n",
      "Epoch 281/300\n",
      "Average training loss: 0.16557301019297704\n",
      "Average test loss: 0.006641521313952075\n",
      "Epoch 282/300\n",
      "Average training loss: 0.16587642719348272\n",
      "Average test loss: 0.006675827194419172\n",
      "Epoch 283/300\n",
      "Average training loss: 0.16630243424574534\n",
      "Average test loss: 0.006828365409953727\n",
      "Epoch 284/300\n",
      "Average training loss: 0.1648910012377633\n",
      "Average test loss: 0.006582729072620471\n",
      "Epoch 285/300\n",
      "Average training loss: 0.16643592552344005\n",
      "Average test loss: 0.006613878702537881\n",
      "Epoch 286/300\n",
      "Average training loss: 0.16653017587131924\n",
      "Average test loss: 0.006658207948837015\n",
      "Epoch 287/300\n",
      "Average training loss: 0.16487632960743373\n",
      "Average test loss: 0.006637481478767263\n",
      "Epoch 288/300\n",
      "Average training loss: 0.1653567672835456\n",
      "Average test loss: 24.39968845452203\n",
      "Epoch 289/300\n",
      "Average training loss: 0.17442553867234123\n",
      "Average test loss: 0.010669086786607902\n",
      "Epoch 290/300\n",
      "Average training loss: 0.16463820683956146\n",
      "Average test loss: 0.006698438774380419\n",
      "Epoch 291/300\n",
      "Average training loss: 0.16451069610648686\n",
      "Average test loss: 0.006508743207901716\n",
      "Epoch 292/300\n",
      "Average training loss: 0.16444091453817156\n",
      "Average test loss: 0.006779362640033166\n",
      "Epoch 293/300\n",
      "Average training loss: 0.1645021616882748\n",
      "Average test loss: 0.006535510356227557\n",
      "Epoch 294/300\n",
      "Average training loss: 0.1642397603061464\n",
      "Average test loss: 0.006485842073129283\n",
      "Epoch 297/300\n",
      "Average training loss: 0.16488373047775692\n",
      "Average test loss: 0.007074470030764739\n",
      "Epoch 298/300\n",
      "Average training loss: 0.16524172541830276\n",
      "Average test loss: 0.006652451451039976\n",
      "Epoch 299/300\n",
      "Average training loss: 0.16385966520839268\n",
      "Average test loss: 0.006841246731165382\n",
      "Epoch 300/300\n",
      "Average training loss: 0.1650308917760849\n",
      "Average test loss: 0.00689750403083033\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.506097933663263\n",
      "Average test loss: 0.011372890109817187\n",
      "Epoch 2/300\n",
      "Average training loss: 2.0797269044452245\n",
      "Average test loss: 0.0080095697177781\n",
      "Epoch 3/300\n",
      "Average training loss: 1.174865775850084\n",
      "Average test loss: 0.007002690966758463\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7631603824827407\n",
      "Average test loss: 0.006492899399250746\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5504340417650011\n",
      "Average test loss: 0.006741391109923522\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4354958055814107\n",
      "Average test loss: 0.006305702142417431\n",
      "Epoch 7/300\n",
      "Average training loss: 0.36771999168395997\n",
      "Average test loss: 0.006246220021198194\n",
      "Epoch 8/300\n",
      "Average training loss: 0.32469984345965913\n",
      "Average test loss: 0.005753693678726753\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2934533080259959\n",
      "Average test loss: 0.005877206498550044\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2713683758311802\n",
      "Average test loss: 0.006348807904041476\n",
      "Epoch 11/300\n",
      "Average training loss: 0.2535892393456565\n",
      "Average test loss: 0.005729694553961356\n",
      "Epoch 12/300\n",
      "Average training loss: 0.24054870887597402\n",
      "Average test loss: 0.005534833534724183\n",
      "Epoch 13/300\n",
      "Average training loss: 0.2306083472834693\n",
      "Average test loss: 0.006140012585868438\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2216828061607149\n",
      "Average test loss: 0.007855195594330629\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20299956013096704\n",
      "Average test loss: 0.005172268295453654\n",
      "Epoch 18/300\n",
      "Average training loss: 0.19842089058293236\n",
      "Average test loss: 0.00517304499157601\n",
      "Epoch 19/300\n",
      "Average training loss: 0.19380214361349743\n",
      "Average test loss: 0.005668520758135451\n",
      "Epoch 20/300\n",
      "Average training loss: 0.18909682067235312\n",
      "Average test loss: 0.0051120701887541345\n",
      "Epoch 21/300\n",
      "Average training loss: 0.17657662415504455\n",
      "Average test loss: 0.005290396213531494\n",
      "Epoch 24/300\n",
      "Average training loss: 0.17351481233702765\n",
      "Average test loss: 0.004651286977446742\n",
      "Epoch 25/300\n",
      "Average training loss: 0.17003056445386674\n",
      "Average test loss: 0.004761262653188573\n",
      "Epoch 26/300\n",
      "Average training loss: 0.16611052718427446\n",
      "Average test loss: 0.005463201938403977\n",
      "Epoch 27/300\n",
      "Average training loss: 0.16372786422570546\n",
      "Average test loss: 0.005049415118992329\n",
      "Epoch 28/300\n",
      "Average training loss: 0.16103714497884114\n",
      "Average test loss: 0.0047453790600928995\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1582485431432724\n",
      "Average test loss: 0.0046000262945890425\n",
      "Epoch 30/300\n",
      "Average training loss: 0.15672940180036757\n",
      "Average test loss: 0.004467599946384629\n",
      "Epoch 31/300\n",
      "Average training loss: 0.15420084816879695\n",
      "Average test loss: 0.004603369777815209\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15223788596524132\n",
      "Average test loss: 0.004490220612535874\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1511343692673577\n",
      "Average test loss: 0.004469154210347269\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14964163745774164\n",
      "Average test loss: 0.00439235639344487\n",
      "Epoch 35/300\n",
      "Average training loss: 0.14804138839244843\n",
      "Average test loss: 0.0043125060647726055\n",
      "Epoch 36/300\n",
      "Average training loss: 0.14692731886439853\n",
      "Average test loss: 0.004712514856209358\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1459417195253902\n",
      "Average test loss: 0.00435668429856499\n",
      "Epoch 38/300\n",
      "Average training loss: 0.14517248017258114\n",
      "Average test loss: 0.004894802803380622\n",
      "Epoch 39/300\n",
      "Average training loss: 0.14363097529941135\n",
      "Average test loss: 0.004278810003979339\n",
      "Epoch 40/300\n",
      "Average training loss: 0.14350249608357749\n",
      "Average test loss: 0.004307975448564523\n",
      "Epoch 41/300\n",
      "Average training loss: 0.14195706386036344\n",
      "Average test loss: 0.004351978454738855\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1409691679345237\n",
      "Average test loss: 0.004463324449749457\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14142804781595866\n",
      "Average test loss: 0.004283300900003976\n",
      "Epoch 44/300\n",
      "Average training loss: 0.13882976840602027\n",
      "Average test loss: 0.004458341852037443\n",
      "Epoch 47/300\n",
      "Average training loss: 0.14055758812692432\n",
      "Average test loss: 0.7391316052542792\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1380769449207518\n",
      "Average test loss: 0.004191630479776197\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1369657592177391\n",
      "Average test loss: 0.004204430000649558\n",
      "Epoch 50/300\n",
      "Average training loss: 0.13626697259479098\n",
      "Average test loss: 0.02415847795870569\n",
      "Epoch 51/300\n",
      "Average training loss: 0.136198204656442\n",
      "Average test loss: 0.004314631288664208\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1355692563984129\n",
      "Average test loss: 0.004302596191565196\n",
      "Epoch 53/300\n",
      "Average training loss: 0.13547342587841882\n",
      "Average test loss: 0.004201474996904532\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1350662229458491\n",
      "Average test loss: 0.0044697308105727035\n",
      "Epoch 55/300\n",
      "Average training loss: 0.13441127501593697\n",
      "Average test loss: 0.004400994149347147\n",
      "Epoch 56/300\n",
      "Average training loss: 0.13448300041092767\n",
      "Average test loss: 0.004523573071178463\n",
      "Epoch 57/300\n",
      "Average training loss: 0.13364399727185566\n",
      "Average test loss: 0.004584813200351265\n",
      "Epoch 58/300\n",
      "Average training loss: 0.13363001486327913\n",
      "Average test loss: 0.0041636899883548415\n",
      "Epoch 59/300\n",
      "Average training loss: 0.13282272229592004\n",
      "Average test loss: 0.004363305247078339\n",
      "Epoch 60/300\n",
      "Average training loss: 0.13321924669875038\n",
      "Average test loss: 0.08795244362288052\n",
      "Epoch 61/300\n",
      "Average training loss: 0.13249467990133498\n",
      "Average test loss: 0.005962388926496108\n",
      "Epoch 62/300\n",
      "Average training loss: 0.13193039847744836\n",
      "Average test loss: 0.004283426028986772\n",
      "Epoch 63/300\n",
      "Average training loss: 0.13139730485942627\n",
      "Average test loss: 0.004146190759415428\n",
      "Epoch 66/300\n",
      "Average training loss: 0.13221799313359792\n",
      "Average test loss: 0.004519493610494666\n",
      "Epoch 67/300\n",
      "Average training loss: 0.13073642134666444\n",
      "Average test loss: 0.00814023520424962\n",
      "Epoch 68/300\n",
      "Average training loss: 0.1302985164920489\n",
      "Average test loss: 0.0040747853728632134\n",
      "Epoch 69/300\n",
      "Average training loss: 0.13068278529908922\n",
      "Average test loss: 5.282580400678847\n",
      "Epoch 70/300\n",
      "Average training loss: 0.13045663547515868\n",
      "Average test loss: 0.004250529389000601\n",
      "Epoch 71/300\n",
      "Average training loss: 0.13031391635868284\n",
      "Average test loss: 0.0042442960602541765\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1299350432621108\n",
      "Average test loss: 0.004483010994477405\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12941762479808597\n",
      "Average test loss: 0.004130502166847388\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12900410206450355\n",
      "Average test loss: 0.0044170168247073885\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12920544365379547\n",
      "Average test loss: 0.004205769358202815\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1284734676082929\n",
      "Average test loss: 0.004432132721775108\n",
      "Epoch 77/300\n",
      "Average training loss: 0.12929716071155337\n",
      "Average test loss: 0.3889560023513105\n",
      "Epoch 78/300\n",
      "Average training loss: 0.12844795450899335\n",
      "Average test loss: 0.004179865594125456\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1276426561805937\n",
      "Average test loss: 0.004763234264113837\n",
      "Epoch 80/300\n",
      "Average training loss: 0.12745329288641613\n",
      "Average test loss: 0.004247776361803214\n",
      "Epoch 81/300\n",
      "Average training loss: 0.12825474294026692\n",
      "Average test loss: 0.004142674735022916\n",
      "Epoch 82/300\n",
      "Average training loss: 0.12717825298839144\n",
      "Average test loss: 0.00414637293956346\n",
      "Epoch 85/300\n",
      "Average training loss: 0.12681743763552772\n",
      "Average test loss: 0.004105008405115869\n",
      "Epoch 86/300\n",
      "Average training loss: 0.12800766213734946\n",
      "Average test loss: 0.00448742809270819\n",
      "Epoch 87/300\n",
      "Average training loss: 0.12665736031532288\n",
      "Average test loss: 0.004703038939585289\n",
      "Epoch 88/300\n",
      "Average training loss: 0.12752292913860744\n",
      "Average test loss: 0.016566860420836344\n",
      "Epoch 89/300\n",
      "Average training loss: 0.12775051999754375\n",
      "Average test loss: 0.0040827579129901195\n",
      "Epoch 90/300\n",
      "Average training loss: 0.1255514275630315\n",
      "Average test loss: 0.0042412152936061225\n",
      "Epoch 91/300\n",
      "Average training loss: 0.1251593594815996\n",
      "Average test loss: 0.004530880090677076\n",
      "Epoch 92/300\n",
      "Average training loss: 0.12532522008154126\n",
      "Average test loss: 0.004427402236809333\n",
      "Epoch 93/300\n",
      "Average training loss: 0.12571943338049782\n",
      "Average test loss: 0.004118300416299866\n",
      "Epoch 94/300\n",
      "Average training loss: 0.12546263138453165\n",
      "Average test loss: 0.0041950292359623645\n",
      "Epoch 95/300\n",
      "Average training loss: 0.1251043118238449\n",
      "Average test loss: 0.004455065816226933\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1247155452105734\n",
      "Average test loss: 0.004425473010788362\n",
      "Epoch 97/300\n",
      "Average training loss: 0.12568660929467942\n",
      "Average test loss: 0.00409907824649579\n",
      "Epoch 98/300\n",
      "Average training loss: 0.12448255274693171\n",
      "Average test loss: 0.004123760195448994\n",
      "Epoch 99/300\n",
      "Average training loss: 0.12426274825467004\n",
      "Average test loss: 0.092752572859327\n",
      "Epoch 100/300\n",
      "Average training loss: 0.12436238153113259\n",
      "Average test loss: 0.004282501540043288\n",
      "Epoch 101/300\n",
      "Average training loss: 0.12693734761741426\n",
      "Average test loss: 0.004983030659870969\n",
      "Epoch 102/300\n",
      "Average training loss: 0.12340737802452512\n",
      "Average test loss: 0.004081580664341648\n",
      "Epoch 103/300\n",
      "Average training loss: 0.1268501781490114\n",
      "Average test loss: 0.04088440221548081\n",
      "Epoch 106/300\n",
      "Average training loss: 0.12291345422135459\n",
      "Average test loss: 0.004151174887186951\n",
      "Epoch 107/300\n",
      "Average training loss: 0.12284512720505396\n",
      "Average test loss: 0.004164798601633973\n",
      "Epoch 108/300\n",
      "Average training loss: 0.12255019307136536\n",
      "Average test loss: 0.005237371259679397\n",
      "Epoch 109/300\n",
      "Average training loss: 0.12297849851184421\n",
      "Average test loss: 0.004136240364776717\n",
      "Epoch 110/300\n",
      "Average training loss: 0.12235830676555634\n",
      "Average test loss: 0.007462442671259244\n",
      "Epoch 111/300\n",
      "Average training loss: 0.12255044261614481\n",
      "Average test loss: 0.004104873983810345\n",
      "Epoch 112/300\n",
      "Average training loss: 0.12303696461518605\n",
      "Average test loss: 0.0041518227056496675\n",
      "Epoch 113/300\n",
      "Average training loss: 0.12161244231462479\n",
      "Average test loss: 0.0040965924492726725\n",
      "Epoch 114/300\n",
      "Average training loss: 0.12172454346550836\n",
      "Average test loss: 0.004441983794586526\n",
      "Epoch 115/300\n",
      "Average training loss: 0.12294195990429985\n",
      "Average test loss: 0.004217121031549241\n",
      "Epoch 116/300\n",
      "Average training loss: 0.12180750342210134\n",
      "Average test loss: 0.004117246984814604\n",
      "Epoch 117/300\n",
      "Average training loss: 0.1242137688530816\n",
      "Average test loss: 0.0041069299698703815\n",
      "Epoch 118/300\n",
      "Average training loss: 0.1210792148841752\n",
      "Average test loss: 1.000155991739697\n",
      "Epoch 119/300\n",
      "Average training loss: 0.12096938476959865\n",
      "Average test loss: 0.00423915055890878\n",
      "Epoch 120/300\n",
      "Average training loss: 0.12054077357716031\n",
      "Average test loss: 0.004088051316638788\n",
      "Epoch 121/300\n",
      "Average training loss: 0.12099502818451988\n",
      "Average test loss: 264.26250051540796\n",
      "Epoch 124/300\n",
      "Average training loss: 0.12043920050726996\n",
      "Average test loss: 0.004749642999842763\n",
      "Epoch 125/300\n",
      "Average training loss: 0.12088823858896891\n",
      "Average test loss: 0.00716402570447988\n",
      "Epoch 126/300\n",
      "Average training loss: 0.12042426370249854\n",
      "Average test loss: 0.007258987286024624\n",
      "Epoch 127/300\n",
      "Average training loss: 0.12031398234102461\n",
      "Average test loss: 0.004294361500690381\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11972254000107448\n",
      "Average test loss: 0.0041663183505750365\n",
      "Epoch 129/300\n",
      "Average training loss: 0.12013092954291238\n",
      "Average test loss: 0.004154587448885043\n",
      "Epoch 130/300\n",
      "Average training loss: 0.12005639733208551\n",
      "Average test loss: 0.027211574918693967\n",
      "Epoch 131/300\n",
      "Average training loss: 0.11978101214435366\n",
      "Average test loss: 0.004144395040969054\n",
      "Epoch 132/300\n",
      "Average training loss: 0.12022012584739261\n",
      "Average test loss: 0.004298014944212304\n",
      "Epoch 133/300\n",
      "Average training loss: 0.1197719441652298\n",
      "Average test loss: 0.0065050660065478745\n",
      "Epoch 134/300\n",
      "Average training loss: 0.11947975647449494\n",
      "Average test loss: 0.004261686991486284\n",
      "Epoch 135/300\n",
      "Average training loss: 0.11937920787599351\n",
      "Average test loss: 0.004308472799758116\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11992307113276587\n",
      "Average test loss: 0.004835180009818739\n",
      "Epoch 137/300\n",
      "Average training loss: 0.1187985356648763\n",
      "Average test loss: 0.02755894226829211\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11901427516672346\n",
      "Average test loss: 0.004227162230759859\n",
      "Epoch 139/300\n",
      "Average training loss: 0.11871079922384686\n",
      "Average test loss: 0.004318662307742569\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1190838107864062\n",
      "Average test loss: 0.006782493424912293\n",
      "Epoch 141/300\n",
      "Average training loss: 0.11816035387251113\n",
      "Average test loss: 0.004172085084021091\n",
      "Epoch 142/300\n",
      "Average training loss: 0.11884598971737756\n",
      "Average test loss: 0.007833599846810103\n",
      "Epoch 143/300\n",
      "Average training loss: 0.11788188698556688\n",
      "Average test loss: 0.006915109538162748\n",
      "Epoch 146/300\n",
      "Average training loss: 0.11772034927871493\n",
      "Average test loss: 0.004900358148126139\n",
      "Epoch 147/300\n",
      "Average training loss: 0.11994584261708789\n",
      "Average test loss: 0.0041623024452063775\n",
      "Epoch 148/300\n",
      "Average training loss: 0.117601821369595\n",
      "Average test loss: 0.004167340125060744\n",
      "Epoch 149/300\n",
      "Average training loss: 0.11760972588592106\n",
      "Average test loss: 0.32346818393634424\n",
      "Epoch 150/300\n",
      "Average training loss: 0.11726651587088903\n",
      "Average test loss: 0.004673018200943868\n",
      "Epoch 151/300\n",
      "Average training loss: 0.11743850670920478\n",
      "Average test loss: 0.004198185534112983\n",
      "Epoch 152/300\n",
      "Average training loss: 0.11690871041350895\n",
      "Average test loss: 0.09698025141821967\n",
      "Epoch 155/300\n",
      "Average training loss: 0.1174269968867302\n",
      "Average test loss: 0.04145590281362335\n",
      "Epoch 156/300\n",
      "Average training loss: 0.11676292139291763\n",
      "Average test loss: 0.018269442094696893\n",
      "Epoch 157/300\n",
      "Average training loss: 0.11708514144685533\n",
      "Average test loss: 0.004193882180998722\n",
      "Epoch 158/300\n",
      "Average training loss: 0.11725758583015866\n",
      "Average test loss: 0.004353527788900666\n",
      "Epoch 159/300\n",
      "Average training loss: 0.11685384521219465\n",
      "Average test loss: 0.004231755016785529\n",
      "Epoch 160/300\n",
      "Average training loss: 0.11654956271913317\n",
      "Average test loss: 0.004271416679024696\n",
      "Epoch 161/300\n",
      "Average training loss: 0.11656645115216573\n",
      "Average test loss: 0.008304913552270995\n",
      "Epoch 162/300\n",
      "Average training loss: 0.11683238827519947\n",
      "Average test loss: 0.18652537584801515\n",
      "Epoch 163/300\n",
      "Average training loss: 0.11598937324682872\n",
      "Average test loss: 0.004202780373187528\n",
      "Epoch 164/300\n",
      "Average training loss: 0.11693020751741197\n",
      "Average test loss: 0.004156880180040995\n",
      "Epoch 165/300\n",
      "Average training loss: 0.11626195802291234\n",
      "Average test loss: 0.00468854646715853\n",
      "Epoch 166/300\n",
      "Average training loss: 0.11607577127880521\n",
      "Average test loss: 0.008578964602202177\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1161805561184883\n",
      "Average test loss: 0.004146323527726863\n",
      "Epoch 168/300\n",
      "Average training loss: 0.11601922879616419\n",
      "Average test loss: 0.0043344725945757496\n",
      "Epoch 169/300\n",
      "Average training loss: 0.11616268609629737\n",
      "Average test loss: 0.004232248806291156\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1159244013097551\n",
      "Average test loss: 0.004542943525231546\n",
      "Epoch 171/300\n",
      "Average training loss: 0.11473678896824518\n",
      "Average test loss: 0.0045137313293914\n",
      "Epoch 174/300\n",
      "Average training loss: 0.11551458081271913\n",
      "Average test loss: 0.018344008225533698\n",
      "Epoch 175/300\n",
      "Average training loss: 0.11534786667426428\n",
      "Average test loss: 0.004685902922931645\n",
      "Epoch 176/300\n",
      "Average training loss: 0.11527157788806491\n",
      "Average test loss: 0.004202113454954492\n",
      "Epoch 177/300\n",
      "Average training loss: 0.11518784088558622\n",
      "Average test loss: 0.00435106604339348\n",
      "Epoch 178/300\n",
      "Average training loss: 0.11670118024614122\n",
      "Average test loss: 0.004332949816766712\n",
      "Epoch 179/300\n",
      "Average training loss: 0.115152881026268\n",
      "Average test loss: 0.004346308536413643\n",
      "Epoch 180/300\n",
      "Average training loss: 0.11502495907412635\n",
      "Average test loss: 0.00425111416934265\n",
      "Epoch 181/300\n",
      "Average training loss: 0.11494781523280674\n",
      "Average test loss: 0.004269701420432991\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11574955019023683\n",
      "Average test loss: 0.004309429301569859\n",
      "Epoch 183/300\n",
      "Average training loss: 0.11753776766194238\n",
      "Average test loss: 0.004176104665837354\n",
      "Epoch 184/300\n",
      "Average training loss: 0.11505534794595507\n",
      "Average test loss: 0.007352290774385134\n",
      "Epoch 185/300\n",
      "Average training loss: 0.11439935648441314\n",
      "Average test loss: 0.004161921273709999\n",
      "Epoch 186/300\n",
      "Average training loss: 0.11446713744269477\n",
      "Average test loss: 0.004362784902461701\n",
      "Epoch 187/300\n",
      "Average training loss: 0.11450135605865054\n",
      "Average test loss: 0.004145251379244858\n",
      "Epoch 188/300\n",
      "Average training loss: 0.11467992385228475\n",
      "Average test loss: 0.004586950199885501\n",
      "Epoch 189/300\n",
      "Average training loss: 0.11414056161377165\n",
      "Average test loss: 0.004373540916790565\n",
      "Epoch 190/300\n",
      "Average training loss: 0.11420055270857281\n",
      "Average test loss: 0.004255436408436961\n",
      "Epoch 193/300\n",
      "Average training loss: 0.11422737121582031\n",
      "Average test loss: 0.00510587503347132\n",
      "Epoch 194/300\n",
      "Average training loss: 0.11739033403661515\n",
      "Average test loss: 0.0042640459334684746\n",
      "Epoch 195/300\n",
      "Average training loss: 0.11390647553073036\n",
      "Average test loss: 0.004193207057192921\n",
      "Epoch 196/300\n",
      "Average training loss: 0.11341078856256273\n",
      "Average test loss: 0.004211596573806472\n",
      "Epoch 197/300\n",
      "Average training loss: 0.11425428259372711\n",
      "Average test loss: 0.004433160330686304\n",
      "Epoch 198/300\n",
      "Average training loss: 0.11376123772064844\n",
      "Average test loss: 0.004332096074604326\n",
      "Epoch 199/300\n",
      "Average training loss: 0.11386953491634792\n",
      "Average test loss: 0.043607518326905036\n",
      "Epoch 200/300\n",
      "Average training loss: 0.11485846345292197\n",
      "Average test loss: 0.0044078131131827835\n",
      "Epoch 201/300\n",
      "Average training loss: 0.11384120094776154\n",
      "Average test loss: 0.004408701895425717\n",
      "Epoch 202/300\n",
      "Average training loss: 0.1136058181921641\n",
      "Average test loss: 0.004221679131603903\n",
      "Epoch 203/300\n",
      "Average training loss: 0.11331330635150273\n",
      "Average test loss: 0.019218565319975216\n",
      "Epoch 204/300\n",
      "Average training loss: 0.1141631926563051\n",
      "Average test loss: 0.004343892759333054\n",
      "Epoch 205/300\n",
      "Average training loss: 0.11319890582561493\n",
      "Average test loss: 0.004183742493804958\n",
      "Epoch 206/300\n",
      "Average training loss: 0.11486866781446668\n",
      "Average test loss: 0.004576330812027057\n",
      "Epoch 207/300\n",
      "Average training loss: 0.11457630066076914\n",
      "Average test loss: 0.007888268786999914\n",
      "Epoch 208/300\n",
      "Average training loss: 0.11264471861388949\n",
      "Average test loss: 0.004274135065575441\n",
      "Epoch 209/300\n",
      "Average training loss: 0.1131272999909189\n",
      "Average test loss: 0.006578039739280939\n",
      "Epoch 210/300\n",
      "Average training loss: 0.11261428446902169\n",
      "Average test loss: 0.0042976254092322455\n",
      "Epoch 213/300\n",
      "Average training loss: 0.11319756900601917\n",
      "Average test loss: 0.004208650330288543\n",
      "Epoch 214/300\n",
      "Average training loss: 0.11660531138049232\n",
      "Average test loss: 0.004342946248335971\n",
      "Epoch 215/300\n",
      "Average training loss: 0.11243861393796073\n",
      "Average test loss: 0.004828919579999314\n",
      "Epoch 216/300\n",
      "Average training loss: 0.11206575430101819\n",
      "Average test loss: 0.004678088319798311\n",
      "Epoch 217/300\n",
      "Average training loss: 0.11264148179690044\n",
      "Average test loss: 0.004277423167808188\n",
      "Epoch 218/300\n",
      "Average training loss: 0.11242292825380962\n",
      "Average test loss: 0.004291140232649114\n",
      "Epoch 221/300\n",
      "Average training loss: 0.1129624080657959\n",
      "Average test loss: 0.0042788104953037365\n",
      "Epoch 222/300\n",
      "Average training loss: 0.11491872150368161\n",
      "Average test loss: 0.024834545467462804\n",
      "Epoch 223/300\n",
      "Average training loss: 0.11204762349526087\n",
      "Average test loss: 0.004611364276872741\n",
      "Epoch 224/300\n",
      "Average training loss: 0.11207977419429355\n",
      "Average test loss: 0.013532467746072346\n",
      "Epoch 225/300\n",
      "Average training loss: 0.11261440952618917\n",
      "Average test loss: 0.004254267173508803\n",
      "Epoch 226/300\n",
      "Average training loss: 0.11218418101469675\n",
      "Average test loss: 0.004433248474780056\n",
      "Epoch 227/300\n",
      "Average training loss: 0.11296557299296062\n",
      "Average test loss: 0.004222312603559759\n",
      "Epoch 228/300\n",
      "Average training loss: 0.11280265559090509\n",
      "Average test loss: 0.02083003836000959\n",
      "Epoch 229/300\n",
      "Average training loss: 0.1120753195087115\n",
      "Average test loss: 0.005627520342667898\n",
      "Epoch 230/300\n",
      "Average training loss: 0.112363966372278\n",
      "Average test loss: 0.004249636126889123\n",
      "Epoch 231/300\n",
      "Average training loss: 0.11228258809778426\n",
      "Average test loss: 0.004223629656765196\n",
      "Epoch 232/300\n",
      "Average training loss: 0.11234965009821786\n",
      "Average test loss: 0.004323596043305265\n",
      "Epoch 233/300\n",
      "Average training loss: 0.11274158337381152\n",
      "Average test loss: 0.020054386694398192\n",
      "Epoch 234/300\n",
      "Average training loss: 0.11407747414377001\n",
      "Average test loss: 0.004600556863471866\n",
      "Epoch 235/300\n",
      "Average training loss: 0.11163688535160489\n",
      "Average test loss: 0.0045154655205292835\n",
      "Epoch 236/300\n",
      "Average training loss: 0.1120082360903422\n",
      "Average test loss: 0.004843271172708935\n",
      "Epoch 237/300\n",
      "Average training loss: 0.11149288331137763\n",
      "Average test loss: 0.004218394276996454\n",
      "Epoch 238/300\n",
      "Average training loss: 0.11226333800951639\n",
      "Average test loss: 0.004340670496639278\n",
      "Epoch 240/300\n",
      "Average training loss: 0.11181272890832689\n",
      "Average test loss: 0.004649585420472754\n",
      "Epoch 241/300\n",
      "Average training loss: 0.11171429693698882\n",
      "Average test loss: 0.004260567436408665\n",
      "Epoch 242/300\n",
      "Average training loss: 0.11147210197316276\n",
      "Average test loss: 0.004295357886701822\n",
      "Epoch 243/300\n",
      "Average training loss: 0.11144296834203932\n",
      "Average test loss: 0.005407449831565221\n",
      "Epoch 244/300\n",
      "Average training loss: 0.11136759552028444\n",
      "Average test loss: 0.004731410892887248\n",
      "Epoch 245/300\n",
      "Average training loss: 0.1122581490278244\n",
      "Average test loss: 0.004231959291423361\n",
      "Epoch 246/300\n",
      "Average training loss: 0.11106429282824198\n",
      "Average test loss: 0.004615510587890943\n",
      "Epoch 247/300\n",
      "Average training loss: 0.11297173364957173\n",
      "Average test loss: 0.004245871207366387\n",
      "Epoch 248/300\n",
      "Average training loss: 0.11098890986045201\n",
      "Average test loss: 0.32790067618423036\n",
      "Epoch 249/300\n",
      "Average training loss: 0.11142550637986925\n",
      "Average test loss: 0.004403196156438854\n",
      "Epoch 250/300\n",
      "Average training loss: 0.11143128125535118\n",
      "Average test loss: 0.004477343016614517\n",
      "Epoch 251/300\n",
      "Average training loss: 0.11122132454978095\n",
      "Average test loss: 0.004381715178903606\n",
      "Epoch 252/300\n",
      "Average training loss: 0.11147915551397536\n",
      "Average test loss: 0.004456199512299564\n",
      "Epoch 253/300\n",
      "Average training loss: 0.11107029273112615\n",
      "Average test loss: 0.004413251879935463\n",
      "Epoch 254/300\n",
      "Average training loss: 0.11324826560417811\n",
      "Average test loss: 0.0043223966633280115\n",
      "Epoch 257/300\n",
      "Average training loss: 0.11035016094313728\n",
      "Average test loss: 0.0050439738411870265\n",
      "Epoch 258/300\n",
      "Average training loss: 0.1109024000035392\n",
      "Average test loss: 0.0042697069574561385\n",
      "Epoch 259/300\n",
      "Average training loss: 0.11078288696871863\n",
      "Average test loss: 0.004426475079523193\n",
      "Epoch 260/300\n",
      "Average training loss: 0.11115329446064101\n",
      "Average test loss: 0.0043199227843433614\n",
      "Epoch 261/300\n",
      "Average training loss: 0.11088421869940228\n",
      "Average test loss: 0.00424883862812486\n",
      "Epoch 262/300\n",
      "Average training loss: 0.11159536351760228\n",
      "Average test loss: 0.004295181959246596\n",
      "Epoch 263/300\n",
      "Average training loss: 0.11093704350789388\n",
      "Average test loss: 0.004267249620830019\n",
      "Epoch 264/300\n",
      "Average training loss: 0.11012835190693537\n",
      "Average test loss: 0.006334535531285736\n",
      "Epoch 265/300\n",
      "Average training loss: 0.11062014999654558\n",
      "Average test loss: 0.005013380639668968\n",
      "Epoch 266/300\n",
      "Average training loss: 0.11118863802485995\n",
      "Average test loss: 0.005575793196343714\n",
      "Epoch 267/300\n",
      "Average training loss: 0.11062045548359553\n",
      "Average test loss: 0.004702533429695501\n",
      "Epoch 268/300\n",
      "Average training loss: 0.11012053771813711\n",
      "Average test loss: 0.0897307468354702\n",
      "Epoch 269/300\n",
      "Average training loss: 0.11360523585478464\n",
      "Average test loss: 0.004270742697848214\n",
      "Epoch 270/300\n",
      "Average training loss: 0.11023599814044105\n",
      "Average test loss: 0.007028002016246319\n",
      "Epoch 271/300\n",
      "Average training loss: 0.11019870414998796\n",
      "Average test loss: 0.004252944132106171\n",
      "Epoch 272/300\n",
      "Average training loss: 0.11002829842434989\n",
      "Average test loss: 0.22886727780765959\n",
      "Epoch 273/300\n",
      "Average training loss: 0.1105330283443133\n",
      "Average test loss: 0.004447593726424707\n",
      "Epoch 274/300\n",
      "Average training loss: 0.11003887913624445\n",
      "Average test loss: 0.004275404921008481\n",
      "Epoch 277/300\n",
      "Average training loss: 0.11081324555476507\n",
      "Average test loss: 0.004325325918487377\n",
      "Epoch 278/300\n",
      "Average training loss: 0.11002118104034\n",
      "Average test loss: 0.004277293857187033\n",
      "Epoch 279/300\n",
      "Average training loss: 0.1098740888039271\n",
      "Average test loss: 0.004553796477201912\n",
      "Epoch 280/300\n",
      "Average training loss: 0.11004223597049713\n",
      "Average test loss: 0.004376817249175575\n",
      "Epoch 281/300\n",
      "Average training loss: 0.11154456318087048\n",
      "Average test loss: 0.0047055316397713294\n",
      "Epoch 282/300\n",
      "Average training loss: 0.1100005773835712\n",
      "Average test loss: 0.00437747701174683\n",
      "Epoch 283/300\n",
      "Average training loss: 0.11033222971359889\n",
      "Average test loss: 0.004394345586084657\n",
      "Epoch 284/300\n",
      "Average training loss: 0.10987332222196791\n",
      "Average test loss: 0.004468217244785693\n",
      "Epoch 285/300\n",
      "Average training loss: 0.1098231329255634\n",
      "Average test loss: 0.004307220802745885\n",
      "Epoch 286/300\n",
      "Average training loss: 0.10968801558017731\n",
      "Average test loss: 0.0042588217386768925\n",
      "Epoch 289/300\n",
      "Average training loss: 0.10967397479878531\n",
      "Average test loss: 0.004257087637980779\n",
      "Epoch 290/300\n",
      "Average training loss: 0.11026372632715437\n",
      "Average test loss: 0.004430967662069533\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10992613824870852\n",
      "Average test loss: 0.0045207679279976425\n",
      "Epoch 292/300\n",
      "Average training loss: 0.10998450220955742\n",
      "Average test loss: 0.00437691979855299\n",
      "Epoch 293/300\n",
      "Average training loss: 0.10988935414287779\n",
      "Average test loss: 0.009303321377477712\n",
      "Epoch 294/300\n",
      "Average training loss: 0.10967203331655927\n",
      "Average test loss: 0.0046409232703348005\n",
      "Epoch 295/300\n",
      "Average training loss: 0.10953922369745042\n",
      "Average test loss: 0.004416873950925138\n",
      "Epoch 296/300\n",
      "Average training loss: 0.1144889177216424\n",
      "Average test loss: 0.005496501311245892\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10925121358368133\n",
      "Average test loss: 0.004268543507903814\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10902427772018644\n",
      "Average test loss: 0.004435885404547055\n",
      "Epoch 299/300\n",
      "Average training loss: 0.10972987937927246\n",
      "Average test loss: 0.004508820266773303\n",
      "Epoch 300/300\n",
      "Average training loss: 0.10946683434645335\n",
      "Average test loss: 0.004823231160640717\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.566230128606161\n",
      "Average test loss: 0.02997681507302655\n",
      "Epoch 2/300\n",
      "Average training loss: 1.5123628399107192\n",
      "Average test loss: 0.006975593277977573\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8241901978386773\n",
      "Average test loss: 0.0061732471730146145\n",
      "Epoch 4/300\n",
      "Average training loss: 0.5570867240164015\n",
      "Average test loss: 0.005716220891310109\n",
      "Epoch 5/300\n",
      "Average training loss: 0.295550356970893\n",
      "Average test loss: 0.005427488060875072\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2610600714948442\n",
      "Average test loss: 0.004766101621712248\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2364594445493486\n",
      "Average test loss: 0.00516257923675908\n",
      "Epoch 10/300\n",
      "Average training loss: 0.21790593677096898\n",
      "Average test loss: 0.004491305376506514\n",
      "Epoch 11/300\n",
      "Average training loss: 0.20502360651228163\n",
      "Average test loss: 0.0047313934937119485\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1930664625432756\n",
      "Average test loss: 0.004233180890894598\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1842446453438865\n",
      "Average test loss: 0.004186923315127691\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1772042747868432\n",
      "Average test loss: 0.004774423933691449\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1708835156361262\n",
      "Average test loss: 0.003983598944213655\n",
      "Epoch 16/300\n",
      "Average training loss: 0.16575467616319656\n",
      "Average test loss: 0.004383491757429308\n",
      "Epoch 17/300\n",
      "Average training loss: 0.16004328027036455\n",
      "Average test loss: 0.0039006432166530025\n",
      "Epoch 18/300\n",
      "Average training loss: 0.15724324815803103\n",
      "Average test loss: 0.004315383771641387\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1517158340215683\n",
      "Average test loss: 0.004021981990378764\n",
      "Epoch 20/300\n",
      "Average training loss: 0.14725873951117197\n",
      "Average test loss: 0.004144735025448931\n",
      "Epoch 21/300\n",
      "Average training loss: 0.14376013776991103\n",
      "Average test loss: 0.0038002259853399463\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1397073243326611\n",
      "Average test loss: 0.003634188422312339\n",
      "Epoch 23/300\n",
      "Average training loss: 0.13683091043101417\n",
      "Average test loss: 0.0039720779651155076\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13494421524471706\n",
      "Average test loss: 0.0035099456765585475\n",
      "Epoch 25/300\n",
      "Average training loss: 0.13076847242646747\n",
      "Average test loss: 0.0035740047891934714\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12326729738050037\n",
      "Average test loss: 0.0033794210445549752\n",
      "Epoch 29/300\n",
      "Average training loss: 0.12143301296896404\n",
      "Average test loss: 0.004670331083859006\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11662676154242621\n",
      "Average test loss: 0.003317619010806084\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11493182119395998\n",
      "Average test loss: 0.0036202764130300945\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11412018062008751\n",
      "Average test loss: 0.0033971999337275823\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11310481431749132\n",
      "Average test loss: 0.003550463443828954\n",
      "Epoch 36/300\n",
      "Average training loss: 0.111906609416008\n",
      "Average test loss: 0.003258441106312805\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11147499868604872\n",
      "Average test loss: 0.003244186361423797\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11016816125975715\n",
      "Average test loss: 0.003462867523233096\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10969484216637082\n",
      "Average test loss: 0.003433543580273787\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10911325711011886\n",
      "Average test loss: 0.0032839092866828045\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10834204479720858\n",
      "Average test loss: 0.0032103357166051866\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10746234042776956\n",
      "Average test loss: 0.004321527861886555\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10751460038291083\n",
      "Average test loss: 0.003405252846578757\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10646578503317303\n",
      "Average test loss: 0.003231425875176986\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10607934030559328\n",
      "Average test loss: 0.003255288534073366\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10595024018155204\n",
      "Average test loss: 0.0032068659296880164\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1049246551129553\n",
      "Average test loss: 0.00325780113786459\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10559671955638461\n",
      "Average test loss: 0.003184243729751971\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10467492993010415\n",
      "Average test loss: 0.003406742248684168\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10450876051518652\n",
      "Average test loss: 0.0032854173017872706\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10373691810501946\n",
      "Average test loss: 0.0031577994517154166\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10357615539762709\n",
      "Average test loss: 0.0031403039931837055\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10438994293080436\n",
      "Average test loss: 0.0032563561441169846\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10266509601142672\n",
      "Average test loss: 0.003152305186001791\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10231935383213892\n",
      "Average test loss: 0.0031353756234877637\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10211109636889563\n",
      "Average test loss: 0.003140502051346832\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10218107425504261\n",
      "Average test loss: 0.0031593971747077175\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10178220797247357\n",
      "Average test loss: 0.03930715055887898\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10159289532899857\n",
      "Average test loss: 0.003115502229788237\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10142132614718544\n",
      "Average test loss: 0.003205730023483435\n",
      "Epoch 61/300\n",
      "Average training loss: 0.10148591273360782\n",
      "Average test loss: 0.0030825307538939846\n",
      "Epoch 62/300\n",
      "Average training loss: 0.10086655110120774\n",
      "Average test loss: 0.012537740556730164\n",
      "Epoch 63/300\n",
      "Average training loss: 0.10079308256838057\n",
      "Average test loss: 0.003599700841638777\n",
      "Epoch 64/300\n",
      "Average training loss: 0.10048448975880941\n",
      "Average test loss: 0.003197706120295657\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09986173233058718\n",
      "Average test loss: 0.0034349642280075286\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1000696297287941\n",
      "Average test loss: 0.0032519075210309693\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09984315719207128\n",
      "Average test loss: 0.0032393999989661906\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10005118732982211\n",
      "Average test loss: 0.00319348897660772\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09961787225140466\n",
      "Average test loss: 0.003122604489326477\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09982024822632472\n",
      "Average test loss: 0.003100938006407685\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09873247151242362\n",
      "Average test loss: 0.003099330777509345\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09936162485016717\n",
      "Average test loss: 0.0033683233329405387\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09838190801938375\n",
      "Average test loss: 0.01766359212083949\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09881685368551148\n",
      "Average test loss: 0.003559479968001445\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09821226963731977\n",
      "Average test loss: 0.0032590162580211956\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09802186408307817\n",
      "Average test loss: 0.0030591541717035903\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10341952230532964\n",
      "Average test loss: 0.00348945925426152\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09845203273826175\n",
      "Average test loss: 0.003988897510907716\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09778226408031251\n",
      "Average test loss: 0.0030519994936055606\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09724054105414284\n",
      "Average test loss: 0.0030488978309763802\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09714100292656157\n",
      "Average test loss: 0.04737563526630401\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09737465624676811\n",
      "Average test loss: 0.003458072814883457\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09701553169223998\n",
      "Average test loss: 0.007465297488702668\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0976077588001887\n",
      "Average test loss: 32.12025335099962\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09654515404171414\n",
      "Average test loss: 0.003122402264840073\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09653440523809857\n",
      "Average test loss: 0.003142041573714879\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09800689936346478\n",
      "Average test loss: 0.0032788263455861143\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0960516359143787\n",
      "Average test loss: 0.0031592749311692187\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0960639454987314\n",
      "Average test loss: 56.18239669756095\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09598967939284113\n",
      "Average test loss: 0.0031753402159859735\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09607388865947723\n",
      "Average test loss: 0.0031258765233473647\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09544550159904693\n",
      "Average test loss: 0.003175944632747107\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09623446383741167\n",
      "Average test loss: 0.003077400344941351\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09571131163173252\n",
      "Average test loss: 0.003352374591347244\n",
      "Epoch 95/300\n",
      "Average training loss: 0.09506244270006815\n",
      "Average test loss: 0.008195317902912697\n",
      "Epoch 96/300\n",
      "Average training loss: 0.09499909017483393\n",
      "Average test loss: 0.0030436935323394005\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09502473831839031\n",
      "Average test loss: 0.003179322929431995\n",
      "Epoch 98/300\n",
      "Average training loss: 0.09892988358603584\n",
      "Average test loss: 0.0030736451180030902\n",
      "Epoch 99/300\n",
      "Average training loss: 0.09460977227158017\n",
      "Average test loss: 0.0033451654105964633\n",
      "Epoch 100/300\n",
      "Average training loss: 0.09449200044737922\n",
      "Average test loss: 0.003044277142526375\n",
      "Epoch 101/300\n",
      "Average training loss: 0.09438041115469403\n",
      "Average test loss: 0.0030612940345373417\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09477162894275454\n",
      "Average test loss: 0.003246785891138845\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09434412294626236\n",
      "Average test loss: 0.0031223537000930972\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09568089228206211\n",
      "Average test loss: 0.0037346770928965677\n",
      "Epoch 105/300\n",
      "Average training loss: 0.09490655232800378\n",
      "Average test loss: 0.003222152589199444\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09380774037043253\n",
      "Average test loss: 0.0031165839650978645\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09386413436465793\n",
      "Average test loss: 0.003188843018685778\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09345714543263117\n",
      "Average test loss: 0.0030849963054060936\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09401828540033765\n",
      "Average test loss: 0.003128995339696606\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09363049864106708\n",
      "Average test loss: 0.0031006279738826884\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09372477793693543\n",
      "Average test loss: 0.0032739988497147957\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09376719890038172\n",
      "Average test loss: 0.0030692220261941353\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09285866317484114\n",
      "Average test loss: 0.0031784861307177278\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09787916595406003\n",
      "Average test loss: 0.0030930169326149753\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09376345705323749\n",
      "Average test loss: 0.0031585856419470574\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09252837173144023\n",
      "Average test loss: 0.0031190638511131206\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09238155807389153\n",
      "Average test loss: 0.011823929612835248\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09264344531959957\n",
      "Average test loss: 0.003094130248659187\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09220148706436157\n",
      "Average test loss: 0.0030894287671479914\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09263104922241634\n",
      "Average test loss: 0.0033647193486491837\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0922200619644589\n",
      "Average test loss: 0.0030416105426847935\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09260945323440764\n",
      "Average test loss: 0.0033830522116687564\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09234375907977423\n",
      "Average test loss: 0.003131836405230893\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11200784259372287\n",
      "Average test loss: 0.0032539781091941726\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09745643904474047\n",
      "Average test loss: 0.003064306575804949\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09323896385563744\n",
      "Average test loss: 0.0030542715473307505\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09229121846622891\n",
      "Average test loss: 0.0033571848287764525\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09202653650442759\n",
      "Average test loss: 0.003469528411825498\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09175262880987592\n",
      "Average test loss: 0.003167448944722613\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09175378933880064\n",
      "Average test loss: 0.003090653462128507\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09258590515454611\n",
      "Average test loss: 0.003717696783443292\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0916024263103803\n",
      "Average test loss: 0.0033978840592834686\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0914316179090076\n",
      "Average test loss: 0.0034335228958063653\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09117091462347243\n",
      "Average test loss: 0.009485052670455641\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09200353363487455\n",
      "Average test loss: 0.0034363917567663722\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09106957391897837\n",
      "Average test loss: 0.004826601941138506\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09108550814787547\n",
      "Average test loss: 0.0031279898265169728\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09808815884590148\n",
      "Average test loss: 0.0033117139120068815\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09241719965802299\n",
      "Average test loss: 0.0030880846853057545\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09081761592626572\n",
      "Average test loss: 0.004083023848426011\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09064101257589129\n",
      "Average test loss: 0.0032192548314730327\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09095246768660016\n",
      "Average test loss: 0.003079127508526047\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09113781495226754\n",
      "Average test loss: 0.003443580496021443\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09134323790338304\n",
      "Average test loss: 0.003175687675467796\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09032335731056002\n",
      "Average test loss: 0.0031214297566976813\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09173151438766056\n",
      "Average test loss: 0.0031612917899878487\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09005325594875548\n",
      "Average test loss: 0.0032263437068710725\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09007536313268874\n",
      "Average test loss: 0.008471584863546822\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09100874056418737\n",
      "Average test loss: 0.0031507775783538817\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09043322152561611\n",
      "Average test loss: 0.0031105697908335264\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09000022594134013\n",
      "Average test loss: 0.0031111923133333524\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08991468077898025\n",
      "Average test loss: 0.0032208519333766566\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09285273921489716\n",
      "Average test loss: 0.0031318252753052447\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09007971541749106\n",
      "Average test loss: 0.0031151248214559423\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08947895590464273\n",
      "Average test loss: 0.0032888708849334054\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08931320014927123\n",
      "Average test loss: 0.0034656853321939707\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08979496746593052\n",
      "Average test loss: 0.0031094742676036225\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08961757020817862\n",
      "Average test loss: 0.003102055081269807\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08971365559763378\n",
      "Average test loss: 0.010388071658710639\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08966236337688234\n",
      "Average test loss: 0.0031257664755814604\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10402679864565531\n",
      "Average test loss: 0.0031080827532956995\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09101107379462983\n",
      "Average test loss: 0.003245727775618434\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08897564342286851\n",
      "Average test loss: 0.004619114437450965\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08851706570386887\n",
      "Average test loss: 0.0031362398395107854\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08855529858006371\n",
      "Average test loss: 0.003126677450827426\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08897978634966744\n",
      "Average test loss: 0.0045963560406946475\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08882343979676564\n",
      "Average test loss: 0.005557095251563522\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08951212530665928\n",
      "Average test loss: 0.0032729980154997773\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0888354693253835\n",
      "Average test loss: 0.004150373414158821\n",
      "Epoch 171/300\n",
      "Average training loss: 0.089589447942045\n",
      "Average test loss: 0.0032378202927195363\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08900950438446469\n",
      "Average test loss: 0.003173846429420842\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08834148997730679\n",
      "Average test loss: 0.003643472204605738\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08867504082785713\n",
      "Average test loss: 0.011101209673616622\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09031824960311254\n",
      "Average test loss: 0.003162507046427992\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08817379404438866\n",
      "Average test loss: 0.003912600617855787\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08816348066594866\n",
      "Average test loss: 0.003096667357203033\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09059935593936179\n",
      "Average test loss: 0.0032844563803325097\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08811699272526635\n",
      "Average test loss: 0.0035603131629112694\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08786389492617713\n",
      "Average test loss: 0.0031581631436323125\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0882796844376458\n",
      "Average test loss: 0.0033075186724050177\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08815331904755698\n",
      "Average test loss: 0.00343430867128902\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08823260282145606\n",
      "Average test loss: 15.32942461025715\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0894908070895407\n",
      "Average test loss: 0.003161552937494384\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0881357314089934\n",
      "Average test loss: 0.0031607287613054117\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08770181571775013\n",
      "Average test loss: 0.0035753369614895847\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0888745664689276\n",
      "Average test loss: 0.0031779261310067443\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08764510377248129\n",
      "Average test loss: 0.003120230975250403\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08772177596886953\n",
      "Average test loss: 0.003241230646148324\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08802504391802682\n",
      "Average test loss: 0.0033102841435207263\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08753514171971215\n",
      "Average test loss: 0.003231117106353243\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08799359091122945\n",
      "Average test loss: 0.0031605718011657397\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08780941687689887\n",
      "Average test loss: 0.003211457090659274\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08752926205264197\n",
      "Average test loss: 0.0031393285344044368\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08764948244889577\n",
      "Average test loss: 0.036918568884332974\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08756171825859282\n",
      "Average test loss: 0.003261150490389102\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08781402960750792\n",
      "Average test loss: 0.0031551044802698826\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0869599058760537\n",
      "Average test loss: 0.00328047843505111\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08748619465695487\n",
      "Average test loss: 0.0032009958076394266\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08779941853549746\n",
      "Average test loss: 0.007249515911771191\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08694966173171997\n",
      "Average test loss: 0.003419232555768556\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08856076574987835\n",
      "Average test loss: 0.0037814790550619363\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08716245663166046\n",
      "Average test loss: 0.003610204224371248\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08714773331085841\n",
      "Average test loss: 0.003259382551122043\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08774417432149251\n",
      "Average test loss: 0.003739058212273651\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08659431807200114\n",
      "Average test loss: 0.0033408713380081788\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0867243309020996\n",
      "Average test loss: 0.0031330504974143372\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08737670191129049\n",
      "Average test loss: 0.0031596551491982405\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08749565580156114\n",
      "Average test loss: 0.003260513993187083\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08706571805477142\n",
      "Average test loss: 0.0031319958058496316\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08745985788769192\n",
      "Average test loss: 0.0033378977537569074\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08710351590977775\n",
      "Average test loss: 3.4475130279594\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08688537954621844\n",
      "Average test loss: 0.0032178708360426954\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08663556082381142\n",
      "Average test loss: 0.0031775646602941886\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0865377246008979\n",
      "Average test loss: 0.003419096498232749\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08640312580929863\n",
      "Average test loss: 0.0031779610688487687\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08703638563553492\n",
      "Average test loss: 0.02086945843199889\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08782299410634571\n",
      "Average test loss: 0.003989617430708475\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08607965638902453\n",
      "Average test loss: 0.0031722811634341875\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08660072763760884\n",
      "Average test loss: 0.003379370002903872\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08639386688669523\n",
      "Average test loss: 0.0043841782148099605\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08647958359453413\n",
      "Average test loss: 0.003512333695880241\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0862272728814019\n",
      "Average test loss: 0.003532214265109764\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08670235868957307\n",
      "Average test loss: 0.003294341972718636\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09448422798183229\n",
      "Average test loss: 0.0032008901515768635\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08615705043739742\n",
      "Average test loss: 0.003205167677046524\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0862549664179484\n",
      "Average test loss: 0.0033041390244745545\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08579063503609763\n",
      "Average test loss: 0.008307139318850305\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08614418694045808\n",
      "Average test loss: 0.003222981376366483\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08597520704401863\n",
      "Average test loss: 0.0031537808233665096\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08580586986409293\n",
      "Average test loss: 0.0034652736145589086\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08620589235093859\n",
      "Average test loss: 0.003165193212115102\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08574759260813396\n",
      "Average test loss: 0.0031846661679446695\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08608335769838757\n",
      "Average test loss: 0.0032190159041848446\n",
      "Epoch 235/300\n",
      "Average training loss: 0.11065494435363346\n",
      "Average test loss: 0.006017861364616288\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08865041759941313\n",
      "Average test loss: 0.003226199177404245\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08618761548068789\n",
      "Average test loss: 0.006730297310070859\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08523322679599127\n",
      "Average test loss: 0.0031561229876759978\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08503154420190387\n",
      "Average test loss: 0.0031865381764041053\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08515775194433\n",
      "Average test loss: 0.003191136917927199\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08560783851808972\n",
      "Average test loss: 0.003281240322523647\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08543713259034687\n",
      "Average test loss: 0.0032140117273148565\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08571299554904302\n",
      "Average test loss: 0.0031698042317810987\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08668565562036303\n",
      "Average test loss: 0.0038105800189077855\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08603328293561935\n",
      "Average test loss: 0.003400274378971921\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08566840311553743\n",
      "Average test loss: 0.003355685294502311\n",
      "Epoch 247/300\n",
      "Average training loss: 0.085196822239293\n",
      "Average test loss: 0.003236045167057051\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08558976926406224\n",
      "Average test loss: 0.0033037563554114764\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08575953920682271\n",
      "Average test loss: 0.003342828848502702\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08547326292594273\n",
      "Average test loss: 0.004236755725410249\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08626446055041419\n",
      "Average test loss: 0.0034791952398502165\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09466294687986374\n",
      "Average test loss: 0.0031336572253041796\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08533908375104268\n",
      "Average test loss: 0.0032305506954176558\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08480620973971155\n",
      "Average test loss: 0.003238523400285178\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08465820963515176\n",
      "Average test loss: 0.003187542826351192\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08864628880553775\n",
      "Average test loss: 0.0035198831024269264\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0866298648847474\n",
      "Average test loss: 0.0031740441222985584\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08459193323718177\n",
      "Average test loss: 0.003227962068385548\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08467768830723232\n",
      "Average test loss: 0.0033593045220606855\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08461261760526233\n",
      "Average test loss: 0.0031906337164756323\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08638683211803436\n",
      "Average test loss: 0.012872226720054945\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08508589041233063\n",
      "Average test loss: 0.0031899624735944802\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08540416667196486\n",
      "Average test loss: 0.0031940875666009055\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08485428133938047\n",
      "Average test loss: 0.689196592370669\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08518699289030499\n",
      "Average test loss: 0.003299076464648048\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08519454438156551\n",
      "Average test loss: 0.003630361328522364\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08466241765022278\n",
      "Average test loss: 0.00334531997413271\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08534385800361634\n",
      "Average test loss: 1223771.0456666667\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08991185496913062\n",
      "Average test loss: 0.003195224039670494\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08418841092454063\n",
      "Average test loss: 0.006924199242972665\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08498113177882301\n",
      "Average test loss: 0.0031869163865016568\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08811467420392566\n",
      "Average test loss: 0.003322482000208563\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08446741974684928\n",
      "Average test loss: 0.0034012462633351485\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08434345548682742\n",
      "Average test loss: 0.006656920633382268\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08448961804972754\n",
      "Average test loss: 0.0054154765705267584\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08498347887065676\n",
      "Average test loss: 0.0033002610059662\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0843404182460573\n",
      "Average test loss: 0.0032004287542982232\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09092822255690892\n",
      "Average test loss: 0.003338760356315308\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08432472236288918\n",
      "Average test loss: 0.0033336784972084893\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08392715635564592\n",
      "Average test loss: 0.0033495093966937725\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08421562596824433\n",
      "Average test loss: 0.0031768295756644674\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08431803892056147\n",
      "Average test loss: 0.003481579482348429\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08483175147242017\n",
      "Average test loss: 0.08407326821486155\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08547634612851673\n",
      "Average test loss: 0.003464440479874611\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08405187610122893\n",
      "Average test loss: 0.0038603569596178003\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08409139757023917\n",
      "Average test loss: 0.0033696045320894983\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09062713656160566\n",
      "Average test loss: 0.003199511907994747\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08379744213819504\n",
      "Average test loss: 0.0037777784584710995\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08399612953265508\n",
      "Average test loss: 0.003207022231279148\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08408723443084293\n",
      "Average test loss: 0.003204501485245095\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08425310099124908\n",
      "Average test loss: 0.0032906528365694815\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08433696117003758\n",
      "Average test loss: 0.003211835370502538\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08490040491686927\n",
      "Average test loss: 0.0032944295238703488\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08402253695991305\n",
      "Average test loss: 0.003647545419219467\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08426206589407391\n",
      "Average test loss: 0.0032594850084020034\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08450720789697436\n",
      "Average test loss: 0.0032412915312581593\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08413435428672367\n",
      "Average test loss: 0.0049608414566351304\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08414162001344892\n",
      "Average test loss: 0.0036460998608834213\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08466746963395012\n",
      "Average test loss: 0.0032575622596260575\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08398094273938073\n",
      "Average test loss: 0.0034853334927724467\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1224.6350324859618\n",
      "Average test loss: 0.055431040780411826\n",
      "Epoch 2/300\n",
      "Average training loss: 14.787121163262261\n",
      "Average test loss: 0.059583141449424955\n",
      "Epoch 3/300\n",
      "Average training loss: 10.718397380405003\n",
      "Average test loss: 0.0782500566177898\n",
      "Epoch 4/300\n",
      "Average training loss: 8.255821426815457\n",
      "Average test loss: 0.13835259966800612\n",
      "Epoch 5/300\n",
      "Average training loss: 6.582076874203152\n",
      "Average test loss: 0.05246287525445223\n",
      "Epoch 6/300\n",
      "Average training loss: 5.394597415076362\n",
      "Average test loss: 0.1427578867876695\n",
      "Epoch 7/300\n",
      "Average training loss: 4.547386056688096\n",
      "Average test loss: 0.0073023319219549495\n",
      "Epoch 8/300\n",
      "Average training loss: 3.8665412432352704\n",
      "Average test loss: 0.006608819170958466\n",
      "Epoch 9/300\n",
      "Average training loss: 3.2984484000735814\n",
      "Average test loss: 0.006097579656375779\n",
      "Epoch 10/300\n",
      "Average training loss: 2.7857107050153944\n",
      "Average test loss: 0.006190091168300973\n",
      "Epoch 11/300\n",
      "Average training loss: 2.406763861550225\n",
      "Average test loss: 0.0054293713538597025\n",
      "Epoch 12/300\n",
      "Average training loss: 2.064354708353678\n",
      "Average test loss: 0.005566832167406877\n",
      "Epoch 13/300\n",
      "Average training loss: 1.7813348520067003\n",
      "Average test loss: 0.004876985206786129\n",
      "Epoch 14/300\n",
      "Average training loss: 1.53948431830936\n",
      "Average test loss: 0.004808013031880061\n",
      "Epoch 15/300\n",
      "Average training loss: 1.3419156834284465\n",
      "Average test loss: 0.00465674879319138\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1739619375864665\n",
      "Average test loss: 0.004510525645481216\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0234087301890056\n",
      "Average test loss: 0.004415230276063085\n",
      "Epoch 18/300\n",
      "Average training loss: 0.8893065677218968\n",
      "Average test loss: 0.004399401808364524\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7699791718588935\n",
      "Average test loss: 0.00426712081912491\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6673757418526544\n",
      "Average test loss: 0.004279051716128985\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5794020824962192\n",
      "Average test loss: 0.003987732502321402\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5028943580521478\n",
      "Average test loss: 0.003802159183141258\n",
      "Epoch 23/300\n",
      "Average training loss: 0.43928518141640555\n",
      "Average test loss: 0.003768430912453267\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3855415283838908\n",
      "Average test loss: 0.0038094275349544156\n",
      "Epoch 25/300\n",
      "Average training loss: 0.3400253372722202\n",
      "Average test loss: 0.0035119457515991395\n",
      "Epoch 26/300\n",
      "Average training loss: 0.30252207658025954\n",
      "Average test loss: 0.003499662904689709\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2712339884969923\n",
      "Average test loss: 0.003369347573361463\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2458010520670149\n",
      "Average test loss: 0.003426667729806569\n",
      "Epoch 29/300\n",
      "Average training loss: 0.22440780709849464\n",
      "Average test loss: 0.0032240951071596806\n",
      "Epoch 30/300\n",
      "Average training loss: 0.20618810764948528\n",
      "Average test loss: 0.003128558221169644\n",
      "Epoch 31/300\n",
      "Average training loss: 0.19123027378982968\n",
      "Average test loss: 0.003091439991361565\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1791470051209132\n",
      "Average test loss: 0.0032971920741515027\n",
      "Epoch 33/300\n",
      "Average training loss: 0.168946278863483\n",
      "Average test loss: 0.0030942909471276736\n",
      "Epoch 34/300\n",
      "Average training loss: 0.16097783346970876\n",
      "Average test loss: 0.003056129186931584\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15247674666510688\n",
      "Average test loss: 0.0029103168965213828\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1464964978562461\n",
      "Average test loss: 0.0028756692097004917\n",
      "Epoch 37/300\n",
      "Average training loss: 0.14091544710265266\n",
      "Average test loss: 0.0030230085955311853\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1361899137099584\n",
      "Average test loss: 0.002875297425314784\n",
      "Epoch 39/300\n",
      "Average training loss: 0.13148556239075132\n",
      "Average test loss: 0.002848606827151444\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1274623398118549\n",
      "Average test loss: 0.002782068197511964\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1250325656467014\n",
      "Average test loss: 0.0029829745052589313\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1213209717935986\n",
      "Average test loss: 0.0027298792768269777\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11809793483548695\n",
      "Average test loss: 0.002882252376112673\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1163810819387436\n",
      "Average test loss: 0.0029395759132587245\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11254533026615779\n",
      "Average test loss: 0.002730952041430606\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11048010356558693\n",
      "Average test loss: 0.002691659280202455\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10912525000837114\n",
      "Average test loss: 0.0026819417282111114\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10690677812364366\n",
      "Average test loss: 0.0026666022058990267\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10474864717986848\n",
      "Average test loss: 0.0027010627752169967\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11642746314075259\n",
      "Average test loss: 0.002669858279741473\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10580770497189627\n",
      "Average test loss: 0.0026522599837432307\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10200551691982482\n",
      "Average test loss: 0.002760371493175626\n",
      "Epoch 53/300\n",
      "Average training loss: 0.1003146997888883\n",
      "Average test loss: 0.00262319567364951\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0985316342247857\n",
      "Average test loss: 0.0027004335580600634\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09781184197134442\n",
      "Average test loss: 0.0025971428079323636\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0955692400932312\n",
      "Average test loss: 0.0026502546473509735\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09487863630718656\n",
      "Average test loss: 0.002630250746384263\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09382271967331568\n",
      "Average test loss: 0.002543062351644039\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09252627080016666\n",
      "Average test loss: 0.0025750404470082787\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09194450259870952\n",
      "Average test loss: 0.002581846662072672\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09096973928478029\n",
      "Average test loss: 0.046881265666749744\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09303522484170065\n",
      "Average test loss: 0.0025331903306974306\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09071579268574714\n",
      "Average test loss: 0.0025663019633955426\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0892659624947442\n",
      "Average test loss: 0.0026148722674697636\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08855848501788245\n",
      "Average test loss: 0.002550800761207938\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0888342662817902\n",
      "Average test loss: 0.002511144912491242\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08774099194341235\n",
      "Average test loss: 0.002507928202342656\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08755619682206048\n",
      "Average test loss: 0.0025308497339073155\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08684510519769456\n",
      "Average test loss: 0.0025064396967904435\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08647431737184524\n",
      "Average test loss: 0.002494440406974819\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08583124889267815\n",
      "Average test loss: 0.002478912736185723\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08575307722886404\n",
      "Average test loss: 0.002757397196979986\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08522281953361299\n",
      "Average test loss: 0.0027462001502927807\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08466259229183197\n",
      "Average test loss: 0.002595356348901987\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08456625727812449\n",
      "Average test loss: 0.0025788732460803455\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0844671544763777\n",
      "Average test loss: 0.0025063901914076673\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08398802296320597\n",
      "Average test loss: 0.0025128081780340936\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08357737990220387\n",
      "Average test loss: 0.002567477308627632\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08381966331601143\n",
      "Average test loss: 0.0024815679972784387\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08378845005565219\n",
      "Average test loss: 0.0024486225704765984\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08308847824732463\n",
      "Average test loss: 0.027864779725670814\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08282275668780009\n",
      "Average test loss: 0.00246382839894957\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0824879331058926\n",
      "Average test loss: 0.0029155085035082367\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08259756797552109\n",
      "Average test loss: 0.0025035618401856885\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08214453716410532\n",
      "Average test loss: 0.2740290354291598\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08230705754624473\n",
      "Average test loss: 0.002805401723831892\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08193127496043841\n",
      "Average test loss: 0.0025473209584338796\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08256443297863006\n",
      "Average test loss: 0.0025953637195958034\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08142728167772294\n",
      "Average test loss: 0.002548638176379932\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08122555031379064\n",
      "Average test loss: 0.002468742465807332\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08093774682945676\n",
      "Average test loss: 0.00242674980022841\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08078528288337919\n",
      "Average test loss: 0.002449995861492223\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08119004917144776\n",
      "Average test loss: 0.002445799810397956\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08132717078261906\n",
      "Average test loss: 0.0026994685294727486\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08029176997476153\n",
      "Average test loss: 0.002608956011426118\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08045928771297137\n",
      "Average test loss: 0.006304690000911554\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08029537646306886\n",
      "Average test loss: 0.004623041105767091\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08014961685074701\n",
      "Average test loss: 0.036566840945018664\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07988543520371119\n",
      "Average test loss: 0.0024511497505009176\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08001272837983238\n",
      "Average test loss: 0.0024700636309054162\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08021705069144566\n",
      "Average test loss: 0.0024346014606869883\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07943333825800154\n",
      "Average test loss: 0.0025117703181587988\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07958587908744812\n",
      "Average test loss: 0.0028053990983300738\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0792064618534512\n",
      "Average test loss: 0.002462479136677252\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07901660743686888\n",
      "Average test loss: 0.006775161820153396\n",
      "Epoch 106/300\n",
      "Average training loss: 0.078969812810421\n",
      "Average test loss: 6.098430636405944\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0790842151178254\n",
      "Average test loss: 0.4331530841779378\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08526297281185787\n",
      "Average test loss: 0.0025032619875338343\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0786943444079823\n",
      "Average test loss: 0.0032266146689653398\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07841436777512233\n",
      "Average test loss: 0.002435251260176301\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0785003157456716\n",
      "Average test loss: 0.0024205368171549505\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07872207505835427\n",
      "Average test loss: 0.0026135211709058946\n",
      "Epoch 113/300\n",
      "Average training loss: 0.078091585305002\n",
      "Average test loss: 0.0024256032247924144\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07819169055753283\n",
      "Average test loss: 0.0024654156311104693\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07820571578873528\n",
      "Average test loss: 0.002546539861087998\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07803890622986688\n",
      "Average test loss: 3.4299762914445666\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07779941056172053\n",
      "Average test loss: 0.0024638280866460666\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07794538308183352\n",
      "Average test loss: 0.00246606759292384\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07877150254117118\n",
      "Average test loss: 0.0024806784509370725\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07812191151910358\n",
      "Average test loss: 0.0032092297271721893\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07719889583190283\n",
      "Average test loss: 0.009212272733449937\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07737434336211946\n",
      "Average test loss: 0.0029821467246446344\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07745292322503196\n",
      "Average test loss: 0.025623734772205354\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07781962622536553\n",
      "Average test loss: 0.0025405203714552855\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07681553006834455\n",
      "Average test loss: 0.0025056911197801433\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07675498167011473\n",
      "Average test loss: 0.002457697657868266\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07709963928990894\n",
      "Average test loss: 0.0033732667966849274\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07671416371398503\n",
      "Average test loss: 0.0024872498555729788\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0777202497323354\n",
      "Average test loss: 0.0024298396079490583\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07693538151846992\n",
      "Average test loss: 0.0035794290649808114\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07623987228340573\n",
      "Average test loss: 0.0026041416130546066\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07607664931151602\n",
      "Average test loss: 0.004707048373710778\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07629124254650539\n",
      "Average test loss: 0.0024681455542643867\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07635604478253259\n",
      "Average test loss: 0.003356982592907217\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07609386163287693\n",
      "Average test loss: 0.0025738964234996173\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07631320644087261\n",
      "Average test loss: 0.0026698629146234857\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07569903832674027\n",
      "Average test loss: 0.003304929858487513\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07873508669932683\n",
      "Average test loss: 0.05165392270684242\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07860885389645894\n",
      "Average test loss: 0.005108834164010154\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0755717802312639\n",
      "Average test loss: 0.0031648032483127383\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07530580161677466\n",
      "Average test loss: 0.003169354242169195\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0760649497906367\n",
      "Average test loss: 0.002469176366718279\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07552627056505945\n",
      "Average test loss: 0.0028800375585754713\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07508517271280289\n",
      "Average test loss: 0.00250955575135433\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07503589155607754\n",
      "Average test loss: 0.0024617480540441144\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07528182027075025\n",
      "Average test loss: 0.002631405607693725\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07859948898686303\n",
      "Average test loss: 0.0024466528551032145\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07471475678682327\n",
      "Average test loss: 0.002845920702856448\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07465880130065812\n",
      "Average test loss: 0.002691330720567041\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07457906119028727\n",
      "Average test loss: 0.0025410245636271105\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07505447985066308\n",
      "Average test loss: 0.00837906397961908\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08687464016013675\n",
      "Average test loss: 0.0025444285025199255\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07773961477809482\n",
      "Average test loss: 0.0024946523195960454\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07518949100044038\n",
      "Average test loss: 0.0025248152646753525\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07444029812018077\n",
      "Average test loss: 0.0371575442229708\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0742676844464408\n",
      "Average test loss: 2222186.874333333\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0749809028936757\n",
      "Average test loss: 0.0024541267892345788\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07438575641976462\n",
      "Average test loss: 0.002528566566813323\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07436670116583506\n",
      "Average test loss: 0.002467338124083148\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07470340730084313\n",
      "Average test loss: 0.002518161464275585\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07430720814069113\n",
      "Average test loss: 0.0025099075250327586\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07446062300933733\n",
      "Average test loss: 0.002961772604741984\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07392823644479116\n",
      "Average test loss: 0.00277904953310887\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0742597416970465\n",
      "Average test loss: 0.0039262316601557865\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07522656790084309\n",
      "Average test loss: 0.004390592231725653\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07384556675950686\n",
      "Average test loss: 0.0038849374229709306\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07351890875233544\n",
      "Average test loss: 0.005190684880233473\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0768174697458744\n",
      "Average test loss: 0.0030813800357282163\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07322846679223909\n",
      "Average test loss: 0.002580881625827816\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07342579444249471\n",
      "Average test loss: 0.002529868926645981\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07359635969003042\n",
      "Average test loss: 0.0026378486889104048\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07371534863445493\n",
      "Average test loss: 0.0025697547656794387\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07355338781078656\n",
      "Average test loss: 0.003055749446567562\n",
      "Epoch 174/300\n",
      "Average training loss: 0.073338384853469\n",
      "Average test loss: 0.002628989138536983\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07335393541389042\n",
      "Average test loss: 0.002474149451073673\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0732535572383139\n",
      "Average test loss: 0.002467291640738646\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0732191607190503\n",
      "Average test loss: 0.002605564289829797\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07345462592442831\n",
      "Average test loss: 0.0025053123264676997\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07338897308376101\n",
      "Average test loss: 0.003099691252741549\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07331349725524584\n",
      "Average test loss: 0.0024814694124377437\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0731396860215399\n",
      "Average test loss: 0.02021256645851665\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07288200387689803\n",
      "Average test loss: 0.00266785755691429\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07315487978855768\n",
      "Average test loss: 0.002514304099397527\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07304170874092314\n",
      "Average test loss: 0.07669875588102473\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07331647872924804\n",
      "Average test loss: 0.002463968211164077\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07272529086139468\n",
      "Average test loss: 0.002498406903197368\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07286967323554887\n",
      "Average test loss: 0.013754716768032974\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07284207511279318\n",
      "Average test loss: 0.0025038774626122582\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07223676729864545\n",
      "Average test loss: 0.0027365132212224933\n",
      "Epoch 190/300\n",
      "Average training loss: 0.075329964266883\n",
      "Average test loss: 0.0025565214198496606\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07200790051619212\n",
      "Average test loss: 0.0025434291780822805\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0722547111246321\n",
      "Average test loss: 0.0026590158250182866\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07245733913448121\n",
      "Average test loss: 0.0025328590874042775\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07245378159152137\n",
      "Average test loss: 0.002472092107559244\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07224499535560608\n",
      "Average test loss: 0.002599648168310523\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07356992531485028\n",
      "Average test loss: 0.0025166981449971596\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07242640219794379\n",
      "Average test loss: 0.0024749948769393895\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07201414287090302\n",
      "Average test loss: 0.004325611629005935\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07210219776630401\n",
      "Average test loss: 0.0033074210584163664\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07275653133127424\n",
      "Average test loss: 0.002524709219319953\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07181685416566001\n",
      "Average test loss: 0.0025515950819891362\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07184905004501342\n",
      "Average test loss: 0.002550780527707603\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07391766037543615\n",
      "Average test loss: 0.0025569110105021133\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07474456637104353\n",
      "Average test loss: 0.0024979401791675225\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07129394800133175\n",
      "Average test loss: 0.0025060917985522085\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07139827907747692\n",
      "Average test loss: 0.5698191325134702\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07152976201640235\n",
      "Average test loss: 0.002647214551973674\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07158442088630464\n",
      "Average test loss: 0.0027593427463952036\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07165167830387752\n",
      "Average test loss: 0.002559013548410601\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07185284666220347\n",
      "Average test loss: 0.0025036855060607197\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07228997547096676\n",
      "Average test loss: 0.002519346330521835\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07271276953485277\n",
      "Average test loss: 0.0028455414803077775\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07135715357131428\n",
      "Average test loss: 0.002557824740600255\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0711029353539149\n",
      "Average test loss: 0.003290981510654092\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07095221935047044\n",
      "Average test loss: 0.0026063663816700377\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07540699958138995\n",
      "Average test loss: 0.0025119807498736515\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07123038204511006\n",
      "Average test loss: 0.0027083889697160986\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07104942452907562\n",
      "Average test loss: 0.002621585245968567\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07093778336379263\n",
      "Average test loss: 0.0025519648012187744\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07653958559698529\n",
      "Average test loss: 0.002636166749108169\n",
      "Epoch 221/300\n",
      "Average training loss: 0.070553814318445\n",
      "Average test loss: 0.0024907697999022073\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07088074782159594\n",
      "Average test loss: 0.0025371782394746937\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0710892825027307\n",
      "Average test loss: 0.0027378232988218465\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07462470159928004\n",
      "Average test loss: 0.0024799007462958495\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07070851662423876\n",
      "Average test loss: 0.0025016110851946802\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07072899789942635\n",
      "Average test loss: 0.0026216313141501613\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07089944590793716\n",
      "Average test loss: 0.0027746524394800264\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0711711438132657\n",
      "Average test loss: 0.003195757678916885\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0707269412080447\n",
      "Average test loss: 0.006354934870161944\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07164291076196565\n",
      "Average test loss: 0.0027535592661135726\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07118227549393971\n",
      "Average test loss: 0.0025828277396245133\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07065749049186706\n",
      "Average test loss: 0.0024882963717811637\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07057819297578599\n",
      "Average test loss: 0.0025901827197521926\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07188254874944687\n",
      "Average test loss: 0.003045633426763945\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07057155074344741\n",
      "Average test loss: 0.002496727095089025\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0706624405781428\n",
      "Average test loss: 0.002545718938836621\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07028150973717372\n",
      "Average test loss: 0.010611197173595429\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07129770738548703\n",
      "Average test loss: 0.01728446439239714\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0707271427181032\n",
      "Average test loss: 0.002534600715877281\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07039987274010977\n",
      "Average test loss: 0.002616889444283313\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07027350526716974\n",
      "Average test loss: 0.002573604964961608\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07046283704704709\n",
      "Average test loss: 0.0025513843616677655\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0816694738401307\n",
      "Average test loss: 0.0025207544362379447\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07029944356282553\n",
      "Average test loss: 0.002511517827088634\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06993919017579821\n",
      "Average test loss: 0.0026705950879388385\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07138635521464878\n",
      "Average test loss: 0.0025426767420851522\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07013393300771713\n",
      "Average test loss: 0.002569415875296626\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07006065853436788\n",
      "Average test loss: 0.0056482760368122\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07005670583248139\n",
      "Average test loss: 0.0028100749775767327\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07014947636259927\n",
      "Average test loss: 0.0031003925220833882\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07021334850788116\n",
      "Average test loss: 0.0025909126847982406\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07024414248598947\n",
      "Average test loss: 0.002705825069712268\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07011254148350822\n",
      "Average test loss: 0.0031762546584424046\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07048196334309048\n",
      "Average test loss: 0.004569324683191048\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07247785885466469\n",
      "Average test loss: 0.002509874989175134\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06965901230441199\n",
      "Average test loss: 0.00257110774392883\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06954634490278032\n",
      "Average test loss: 0.002623306776707371\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06974726933903164\n",
      "Average test loss: 0.007192459423508909\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07099940682119793\n",
      "Average test loss: 0.002556568483511607\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06959838503599167\n",
      "Average test loss: 0.002703750352892611\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07089582212103737\n",
      "Average test loss: 0.0029037592858076095\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06970842507812712\n",
      "Average test loss: 0.002519053752844532\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06979479055934482\n",
      "Average test loss: 0.004512261821991868\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07253346074952019\n",
      "Average test loss: 0.002711987547369467\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07006097697549396\n",
      "Average test loss: 0.00251283211985396\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06965295888317956\n",
      "Average test loss: 1741787.175251736\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06957918380035294\n",
      "Average test loss: 0.0027305359604458015\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0695686643520991\n",
      "Average test loss: 0.003925391372293234\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07101447440518273\n",
      "Average test loss: 0.002526566141181522\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06947826472918192\n",
      "Average test loss: 0.002548942693612642\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0695386527578036\n",
      "Average test loss: 0.0025840774805595476\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07016132422950533\n",
      "Average test loss: 0.002556433563741545\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06911359081003401\n",
      "Average test loss: 0.0033233010170774326\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06955827269951502\n",
      "Average test loss: 51.24681577764617\n",
      "Epoch 275/300\n",
      "Average training loss: 0.20990865529908073\n",
      "Average test loss: 0.006708617275787724\n",
      "Epoch 276/300\n",
      "Average training loss: 0.15105703404214646\n",
      "Average test loss: 0.002723676438960764\n",
      "Epoch 277/300\n",
      "Average training loss: 0.10120373586151335\n",
      "Average test loss: 0.002639484411933356\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09249307805962033\n",
      "Average test loss: 0.004553892001095745\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08768776031997469\n",
      "Average test loss: 0.002846687164778511\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08447490768300163\n",
      "Average test loss: 0.002659877763440212\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08176626800166235\n",
      "Average test loss: 0.0028874622281226847\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07968947378794353\n",
      "Average test loss: 0.002473771722573373\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07769691732856962\n",
      "Average test loss: 0.002496940087940958\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07613701461421118\n",
      "Average test loss: 0.002503396723833349\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07457388826211293\n",
      "Average test loss: 0.0025085501757760842\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07338724035024644\n",
      "Average test loss: 0.0024873625352564784\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0722311362226804\n",
      "Average test loss: 0.0027152096173829504\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07146847175227271\n",
      "Average test loss: 0.0025062136482447385\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07131339526838726\n",
      "Average test loss: 0.0026455739295730986\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07074976534313626\n",
      "Average test loss: 0.0025141917623372543\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0702979298233986\n",
      "Average test loss: 1.0134512059324317\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07042316885789235\n",
      "Average test loss: 0.0027124749641451572\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07010316775242488\n",
      "Average test loss: 0.0029637387707415553\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07084015668763055\n",
      "Average test loss: 0.003934696925183137\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07765805364317364\n",
      "Average test loss: 0.0036873744219127627\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08381091757615407\n",
      "Average test loss: 0.0024754684915145237\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07189782610866759\n",
      "Average test loss: 0.0026673945747315883\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07021414974000718\n",
      "Average test loss: 0.0025373835826499594\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07836386654774348\n",
      "Average test loss: 0.002482367386006647\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07092705980936687\n",
      "Average test loss: 0.002530647648912337\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth3/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 20.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.76\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.23\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.42\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.67\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.75\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.90\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.29\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.37\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.35\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 24.37\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 24.50\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 24.46\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 24.63\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 24.64\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 24.61\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 24.66\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 24.71\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 24.73\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 24.74\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 24.79\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 24.80\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.50\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.98\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.40\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.60\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.63\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.72\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.08\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.19\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.26\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.32\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.39\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.46\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.45\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.54\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.51\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.53\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.51\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.96\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.09\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.57\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.53\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.93\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.86\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.40\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.49\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
