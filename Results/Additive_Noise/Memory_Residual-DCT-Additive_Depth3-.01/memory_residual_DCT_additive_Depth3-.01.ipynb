{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.01)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.01)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.01)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.01)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.060384024833639464\n",
      "Average test loss: 0.005168065807885594\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02371134506000413\n",
      "Average test loss: 0.004640367968628804\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022627102457814748\n",
      "Average test loss: 0.0044687315614687076\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02214720558292336\n",
      "Average test loss: 0.0044698291255368125\n",
      "Epoch 5/300\n",
      "Average training loss: 0.021872589990496634\n",
      "Average test loss: 0.00449177739272515\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02166958801779482\n",
      "Average test loss: 0.004340135991987255\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02152130233993133\n",
      "Average test loss: 0.004321808335267835\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02140096071859201\n",
      "Average test loss: 0.004306664714796676\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021299885011381574\n",
      "Average test loss: 0.004270886572698752\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021212717754973305\n",
      "Average test loss: 0.004289746989599532\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021146548766228887\n",
      "Average test loss: 0.004257258834938208\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02106773762901624\n",
      "Average test loss: 0.004221125351472034\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021012530484133297\n",
      "Average test loss: 0.004207636717292997\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020950821083452968\n",
      "Average test loss: 0.004219484886775414\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020893444473544756\n",
      "Average test loss: 0.004187231748468346\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020851813713709512\n",
      "Average test loss: 0.00417362122837868\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0207888105817967\n",
      "Average test loss: 0.004162067732049359\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02075325648817751\n",
      "Average test loss: 0.004148610213564502\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0206878050631947\n",
      "Average test loss: 0.004172207313693232\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02066121815972858\n",
      "Average test loss: 0.004151004910055134\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02062172976632913\n",
      "Average test loss: 0.004151131867948505\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0205864451477925\n",
      "Average test loss: 0.004124848328530788\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02053239065905412\n",
      "Average test loss: 0.004133581148874428\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020510969604055088\n",
      "Average test loss: 0.004144021620353063\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020481422307590643\n",
      "Average test loss: 0.00410261504062348\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020451066970825195\n",
      "Average test loss: 0.004098229153702657\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020422925349738864\n",
      "Average test loss: 0.004088961511850357\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020385849899715848\n",
      "Average test loss: 0.004085769260095225\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020358996394607757\n",
      "Average test loss: 0.004078998755249712\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02034022830509477\n",
      "Average test loss: 0.004150524255716138\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020319963567786748\n",
      "Average test loss: 0.004082622249714203\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02028772135078907\n",
      "Average test loss: 0.004068683427655035\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020277494466967052\n",
      "Average test loss: 0.004067804678032795\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020241698523362477\n",
      "Average test loss: 0.004057045539220174\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020228286133872137\n",
      "Average test loss: 0.004057588414599498\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02020743024845918\n",
      "Average test loss: 0.004053640640444226\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020208568720353974\n",
      "Average test loss: 0.004071183012177547\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020178852796554566\n",
      "Average test loss: 0.00404713885154989\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020160848767393165\n",
      "Average test loss: 0.004081418927758932\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020138390347361566\n",
      "Average test loss: 0.004040099107970794\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02012733414106899\n",
      "Average test loss: 0.004052472586433093\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02011640911963251\n",
      "Average test loss: 0.004041064208166467\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02010367835727003\n",
      "Average test loss: 0.004028204181128078\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020089082520869044\n",
      "Average test loss: 0.004030054432650407\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02007721628745397\n",
      "Average test loss: 0.004035293618424071\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02006096697350343\n",
      "Average test loss: 0.0040222554509010576\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02004909430940946\n",
      "Average test loss: 0.004031136100697849\n",
      "Epoch 48/300\n",
      "Average training loss: 0.020034414045512677\n",
      "Average test loss: 0.0040163857665740784\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020018071451120906\n",
      "Average test loss: 0.004015601878778802\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02000397444433636\n",
      "Average test loss: 0.004019243574390809\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02000214478870233\n",
      "Average test loss: 0.00401705121414529\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01999767906963825\n",
      "Average test loss: 0.004015417913388874\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01997892690036032\n",
      "Average test loss: 0.004013952231034636\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01996189270582464\n",
      "Average test loss: 0.004008749016871055\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019957035998503368\n",
      "Average test loss: 0.004015167684190803\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019943713350428476\n",
      "Average test loss: 0.00401522856288486\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019935583480530317\n",
      "Average test loss: 0.0040202887507362495\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019930946558713913\n",
      "Average test loss: 0.004008870168071654\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019916654255655076\n",
      "Average test loss: 0.004008150923583242\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019907552535335223\n",
      "Average test loss: 0.004013544540645348\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019899246733221743\n",
      "Average test loss: 0.004008030617816581\n",
      "Epoch 62/300\n",
      "Average training loss: 0.019888555333018303\n",
      "Average test loss: 0.004007839685098992\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0198772158374389\n",
      "Average test loss: 0.004000677272677422\n",
      "Epoch 64/300\n",
      "Average training loss: 0.019876617716418372\n",
      "Average test loss: 0.004028652405159341\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01986338521540165\n",
      "Average test loss: 0.004010839348038038\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019854122776124212\n",
      "Average test loss: 0.00400999699657162\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019843958111272916\n",
      "Average test loss: 0.003998099371997846\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0198412882288297\n",
      "Average test loss: 0.003995649711125427\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019835470823778047\n",
      "Average test loss: 0.004020222581716047\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019823924514982435\n",
      "Average test loss: 0.004003025172485245\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019815115274654495\n",
      "Average test loss: 0.004006887934274144\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019798586865266164\n",
      "Average test loss: 0.003996382883646422\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019799036883645586\n",
      "Average test loss: 0.0039897902396818005\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019789930240975487\n",
      "Average test loss: 0.003991959834678305\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019780676167872217\n",
      "Average test loss: 0.0039921324720813165\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019776834515233835\n",
      "Average test loss: 0.0040000535756763485\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019764321843783062\n",
      "Average test loss: 0.003994010553591781\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019757151751054657\n",
      "Average test loss: 0.003993051658487982\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019748242361678017\n",
      "Average test loss: 0.0039965154754204885\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019743423751658863\n",
      "Average test loss: 0.003991214911556906\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019737451644407378\n",
      "Average test loss: 0.004002085902831621\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019735497930811512\n",
      "Average test loss: 0.003985110316425562\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019722980001734364\n",
      "Average test loss: 0.003991214920456211\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01971799794005023\n",
      "Average test loss: 0.004008944334669246\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019707162191470465\n",
      "Average test loss: 0.003988023852722513\n",
      "Epoch 86/300\n",
      "Average training loss: 0.019703174688749842\n",
      "Average test loss: 0.0039888003721005386\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019690757389697763\n",
      "Average test loss: 0.003989346623420716\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019689315049184693\n",
      "Average test loss: 0.003988520463307698\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01969403352836768\n",
      "Average test loss: 0.003999226878707607\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01967543027136061\n",
      "Average test loss: 0.003989491888425416\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019662822767264315\n",
      "Average test loss: 0.0039907009138001335\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01966451575855414\n",
      "Average test loss: 0.00399384400765929\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01965373702843984\n",
      "Average test loss: 0.003979756235662434\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019650452686680687\n",
      "Average test loss: 0.004017041711343659\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019649712569183773\n",
      "Average test loss: 0.003987297195527289\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019634893164038658\n",
      "Average test loss: 0.0039939072332862355\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019632032366262542\n",
      "Average test loss: 0.003988408991239137\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01962400119834476\n",
      "Average test loss: 0.004002522438557612\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01961901158425543\n",
      "Average test loss: 0.003998185721536478\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01960986338224676\n",
      "Average test loss: 0.003987321610665983\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019601887739366955\n",
      "Average test loss: 0.003983835668199592\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01958932579557101\n",
      "Average test loss: 0.003985828957623906\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01958669888890452\n",
      "Average test loss: 0.004011087137791846\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019581539329555298\n",
      "Average test loss: 0.00398610251231326\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019578742106755576\n",
      "Average test loss: 0.003981716289288468\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019567920494410727\n",
      "Average test loss: 0.003987359986537033\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01956028692589866\n",
      "Average test loss: 0.004000610147292415\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01955571922328737\n",
      "Average test loss: 0.0040025463004906975\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019556124759217103\n",
      "Average test loss: 0.003990972328931093\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019547311019566325\n",
      "Average test loss: 0.003979091894709402\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01954593225651317\n",
      "Average test loss: 0.003996259660356574\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01953463551733229\n",
      "Average test loss: 0.0039752315334561795\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019529788355032603\n",
      "Average test loss: 0.0040065598154647485\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019518421192963917\n",
      "Average test loss: 0.004013556404246224\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01951751045882702\n",
      "Average test loss: 0.0039964527760942775\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01951655443923341\n",
      "Average test loss: 0.003993975341113077\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019507315405540997\n",
      "Average test loss: 0.003984900772571564\n",
      "Epoch 118/300\n",
      "Average training loss: 0.019503652504748767\n",
      "Average test loss: 0.004013041257444355\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019491534794370335\n",
      "Average test loss: 0.003988150893814034\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01949103194806311\n",
      "Average test loss: 0.003986350915912125\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019484463481439486\n",
      "Average test loss: 0.003991763381908337\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01947420350379414\n",
      "Average test loss: 0.003998078188031084\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019465526666906145\n",
      "Average test loss: 0.003992620662268665\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01946205163664288\n",
      "Average test loss: 0.004033798686332172\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01946347835328844\n",
      "Average test loss: 0.00403123725950718\n",
      "Epoch 126/300\n",
      "Average training loss: 0.019454585257503723\n",
      "Average test loss: 0.004002693093071381\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019445582018130354\n",
      "Average test loss: 0.00398921336275008\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019439741479025947\n",
      "Average test loss: 0.003993408084950513\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01942866092340814\n",
      "Average test loss: 0.004016355167660448\n",
      "Epoch 130/300\n",
      "Average training loss: 0.019431770453850427\n",
      "Average test loss: 0.003987844224191375\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019421249772111574\n",
      "Average test loss: 0.00398094691356851\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01941985793908437\n",
      "Average test loss: 0.003987705309771829\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01941320990357134\n",
      "Average test loss: 0.003987368362231387\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01940801830920908\n",
      "Average test loss: 0.003991588762236966\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019408767201834255\n",
      "Average test loss: 0.004011181589629915\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01939867841700713\n",
      "Average test loss: 0.003994249524962571\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01938434205783738\n",
      "Average test loss: 0.003976619164976809\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01938519543988837\n",
      "Average test loss: 0.004007126839624511\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019371720785895982\n",
      "Average test loss: 0.004024582894725932\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01937810606509447\n",
      "Average test loss: 0.004011832493667801\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01936742279595799\n",
      "Average test loss: 0.0039992135249906115\n",
      "Epoch 142/300\n",
      "Average training loss: 0.019363224712510903\n",
      "Average test loss: 0.0040062872531513374\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019355582056774033\n",
      "Average test loss: 0.004007370930578974\n",
      "Epoch 144/300\n",
      "Average training loss: 0.019355756102336776\n",
      "Average test loss: 0.003991558784825934\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01934144669274489\n",
      "Average test loss: 0.0040043902264701\n",
      "Epoch 146/300\n",
      "Average training loss: 0.019351447459724214\n",
      "Average test loss: 0.003990795173578792\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01933502641816934\n",
      "Average test loss: 0.003990446312146055\n",
      "Epoch 148/300\n",
      "Average training loss: 0.019333266909751626\n",
      "Average test loss: 0.003981083222561413\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01932297586235735\n",
      "Average test loss: 0.004004576766242583\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019316584237747722\n",
      "Average test loss: 0.004009213076283534\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01931479925579495\n",
      "Average test loss: 0.003997399542273746\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019305308748450545\n",
      "Average test loss: 0.0039858488510880205\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019300927423768575\n",
      "Average test loss: 0.003997831380201711\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019299004905753665\n",
      "Average test loss: 0.004010179298205508\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01929495810303423\n",
      "Average test loss: 0.004012917715849148\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01928261577255196\n",
      "Average test loss: 0.0039917614027443855\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01928826438304451\n",
      "Average test loss: 0.004043419912457466\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01927696109149191\n",
      "Average test loss: 0.004002401985641983\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019275699529382916\n",
      "Average test loss: 0.004020934735735258\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019260771916972265\n",
      "Average test loss: 0.003989482279453013\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019258419055905605\n",
      "Average test loss: 0.004068430768739846\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019258353297909103\n",
      "Average test loss: 0.003992423206567764\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01925312722557121\n",
      "Average test loss: 0.004031616028812197\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01924371696346336\n",
      "Average test loss: 0.003996889278292656\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01924114500482877\n",
      "Average test loss: 0.004094159256252977\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019232142133845224\n",
      "Average test loss: 0.004000690956082609\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019228458671106233\n",
      "Average test loss: 0.004017845764549242\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019219046332769923\n",
      "Average test loss: 0.003993540594561232\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019220185379187266\n",
      "Average test loss: 0.004001383040307296\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01921105074054665\n",
      "Average test loss: 0.004002857852727175\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019203374933865335\n",
      "Average test loss: 0.004051520384848118\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019201662379834387\n",
      "Average test loss: 0.004010770911557807\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019199773515264192\n",
      "Average test loss: 0.004003844125403298\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019191598329279157\n",
      "Average test loss: 0.004010912504047156\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01918414546880457\n",
      "Average test loss: 0.0039953457431660755\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01918232073966\n",
      "Average test loss: 0.004018351877522137\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019175736640062595\n",
      "Average test loss: 0.004022530915008651\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01918161845869488\n",
      "Average test loss: 0.004019128607792986\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019164418453971543\n",
      "Average test loss: 0.004004486001614067\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019169224047826398\n",
      "Average test loss: 0.004025748855330878\n",
      "Epoch 181/300\n",
      "Average training loss: 0.019156207265953223\n",
      "Average test loss: 0.004006060945284036\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01915564699636565\n",
      "Average test loss: 0.004016845425797833\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019153087341123157\n",
      "Average test loss: 0.004046841524748338\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01913822307354874\n",
      "Average test loss: 0.004077093307342794\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01912798869030343\n",
      "Average test loss: 0.004045319015367164\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019136712301108572\n",
      "Average test loss: 0.00403194849131008\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019129992463522486\n",
      "Average test loss: 0.003995697663062149\n",
      "Epoch 188/300\n",
      "Average training loss: 0.019126077656944594\n",
      "Average test loss: 0.0040395036910971\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019119529618157282\n",
      "Average test loss: 0.004028855849057436\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019116665992471908\n",
      "Average test loss: 0.004024030533101824\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019106769513752727\n",
      "Average test loss: 0.0040418523287193645\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019105561786227757\n",
      "Average test loss: 0.004032355875397722\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019100940545399982\n",
      "Average test loss: 0.004045470820946826\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019085802164342667\n",
      "Average test loss: 0.004051869488838646\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019081066681279076\n",
      "Average test loss: 0.00401515941383938\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019084135247601403\n",
      "Average test loss: 0.004016465962140096\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01908492472436693\n",
      "Average test loss: 0.0040203186143189665\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019072983952032194\n",
      "Average test loss: 0.004024274166466462\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01906828533609708\n",
      "Average test loss: 0.004027807251860698\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019065749559965397\n",
      "Average test loss: 0.0040533703027500045\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019060264630450144\n",
      "Average test loss: 0.004037809246944056\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01906473500861062\n",
      "Average test loss: 0.004036417521950272\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019042179728547732\n",
      "Average test loss: 0.0040273825203379\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019045851117206945\n",
      "Average test loss: 0.004046827228118976\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019039783684743777\n",
      "Average test loss: 0.0040633442621264195\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019035859987139702\n",
      "Average test loss: 0.004034792721685436\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019030105360680158\n",
      "Average test loss: 0.004037729170173407\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019033137386043866\n",
      "Average test loss: 0.00406148116911451\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019020662805272474\n",
      "Average test loss: 0.00408693377715018\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0190189578541451\n",
      "Average test loss: 0.004035404501275884\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019014284430278673\n",
      "Average test loss: 0.004065370038565662\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019014464086956447\n",
      "Average test loss: 0.004019956733617517\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01900452560434739\n",
      "Average test loss: 0.0040518041393823095\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01900680579741796\n",
      "Average test loss: 0.0040350438745485415\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018995813412798775\n",
      "Average test loss: 0.004057784789552292\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01899554343438811\n",
      "Average test loss: 0.0040509626432839365\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01897728225340446\n",
      "Average test loss: 0.0041510333029760255\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018983788102865218\n",
      "Average test loss: 0.004087945546540949\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018971259204049906\n",
      "Average test loss: 0.004057076133787632\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01897298733724488\n",
      "Average test loss: 0.0040889237394763365\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018973834154506524\n",
      "Average test loss: 0.004068688764340348\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018956067903174296\n",
      "Average test loss: 0.0040836646395425\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01895897343920337\n",
      "Average test loss: 0.004085442542822825\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018948694558607208\n",
      "Average test loss: 0.00406069627424909\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01895017382171419\n",
      "Average test loss: 0.004069289270788431\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01894596777525213\n",
      "Average test loss: 0.004072843151580957\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018936348123682868\n",
      "Average test loss: 0.00410458199866116\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01893715405795309\n",
      "Average test loss: 0.004025079525179333\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018937750107712217\n",
      "Average test loss: 0.0040818891872962315\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018928484237856334\n",
      "Average test loss: 0.004037877260810799\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01891792378988531\n",
      "Average test loss: 0.0040462676381899255\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018914119412501654\n",
      "Average test loss: 0.0040605206783447005\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01891191329393122\n",
      "Average test loss: 0.00406035238587194\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018908908202416367\n",
      "Average test loss: 0.0040707491158197325\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018907685363458264\n",
      "Average test loss: 0.004033963155001402\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018905262666030064\n",
      "Average test loss: 0.004128792674177223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01888776485456361\n",
      "Average test loss: 0.004048877713580926\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01889943078160286\n",
      "Average test loss: 0.00404410055735045\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018886138442489837\n",
      "Average test loss: 0.004056554971676733\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018887590636809667\n",
      "Average test loss: 0.00406973100370831\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01888761826356252\n",
      "Average test loss: 0.004059803640262948\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018876114937994216\n",
      "Average test loss: 0.004083856167064773\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01887484988735782\n",
      "Average test loss: 0.004020418592625194\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01887141471273369\n",
      "Average test loss: 0.0040285190985434585\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01887010950015651\n",
      "Average test loss: 0.004114406312919325\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018863318471444977\n",
      "Average test loss: 0.004059012854264842\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018854588807457023\n",
      "Average test loss: 0.004049897382035851\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018862735129064983\n",
      "Average test loss: 0.004075258623808622\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01885231368409263\n",
      "Average test loss: 0.0040607689420382185\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018851109790305298\n",
      "Average test loss: 0.004084016703897052\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01884193982018365\n",
      "Average test loss: 0.004085640684184101\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018836412017544112\n",
      "Average test loss: 0.004025122464944919\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01883146377735668\n",
      "Average test loss: 0.0040856135922173655\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018834625588523016\n",
      "Average test loss: 0.004037403751992517\n",
      "Epoch 255/300\n",
      "Average training loss: 0.018816054599980512\n",
      "Average test loss: 0.004072535771048731\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018821092613869245\n",
      "Average test loss: 0.004060845219012764\n",
      "Epoch 257/300\n",
      "Average training loss: 0.018818369769387774\n",
      "Average test loss: 0.004070726640108559\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01881257136993938\n",
      "Average test loss: 0.004099612673123677\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018806585386395454\n",
      "Average test loss: 0.004087030493550831\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01880215727455086\n",
      "Average test loss: 0.004108842656637231\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018799402134286033\n",
      "Average test loss: 0.0041098151314589715\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018798756563001208\n",
      "Average test loss: 0.004095526628196239\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01878852139578925\n",
      "Average test loss: 0.0041723236416776974\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018789762078060043\n",
      "Average test loss: 0.004099136566950215\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018784417630897627\n",
      "Average test loss: 0.00408814045559201\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018773880061176088\n",
      "Average test loss: 0.004126189415653547\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018776148019565477\n",
      "Average test loss: 0.004056278563621971\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0187706212947766\n",
      "Average test loss: 0.004155069359681672\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01876726064251529\n",
      "Average test loss: 0.004058282513378395\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018773451389537916\n",
      "Average test loss: 0.004068054363959365\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018762709230184554\n",
      "Average test loss: 0.004102660472194354\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018754402256674237\n",
      "Average test loss: 0.004070321364742186\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018751958255966504\n",
      "Average test loss: 0.004101341537717316\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01875225817412138\n",
      "Average test loss: 0.004085551159249412\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018746768700579803\n",
      "Average test loss: 0.004148874030345016\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01874919391506248\n",
      "Average test loss: 0.004086973324004147\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01873715820742978\n",
      "Average test loss: 0.004139763404097822\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018737951070070268\n",
      "Average test loss: 0.004149099128113853\n",
      "Epoch 279/300\n",
      "Average test loss: 0.004078954394492838\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018727143845624395\n",
      "Average test loss: 0.00410485705650515\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018727220975690417\n",
      "Average test loss: 0.004244564281569587\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01873703315191799\n",
      "Average test loss: 0.004066643291670415\n",
      "Epoch 283/300\n",
      "Average training loss: 0.018718354418045945\n",
      "Average test loss: 0.004110297383947505\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018713154307670062\n",
      "Average test loss: 0.00413563741909133\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018713378931085267\n",
      "Average test loss: 0.004083239156545865\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018707659713923932\n",
      "Average test loss: 0.0041126035203536354\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018699630982346005\n",
      "Average test loss: 0.0041281335275206305\n",
      "Epoch 288/300\n",
      "Average training loss: 0.018696179194582833\n",
      "Average test loss: 0.004129465496788423\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01870840378022856\n",
      "Average test loss: 0.004102618378069666\n",
      "Epoch 290/300\n",
      "Average training loss: 0.018693679083138705\n",
      "Average test loss: 0.004129939612829023\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018694798214568032\n",
      "Average test loss: 0.004114506471488211\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01869063102785084\n",
      "Average test loss: 0.004155114842371808\n",
      "Epoch 293/300\n",
      "Average training loss: 0.018682460187209978\n",
      "Average test loss: 0.004125170756752292\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018678104172150295\n",
      "Average test loss: 0.004091606205122339\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018677651594082516\n",
      "Average test loss: 0.00409166956692934\n",
      "Epoch 296/300\n",
      "Average training loss: 0.018679013995660677\n",
      "Average test loss: 0.004077348115957446\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018668927664558094\n",
      "Average test loss: 0.0042355910945269795\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01867224336001608\n",
      "Average test loss: 0.004108678221909536\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018664369119538202\n",
      "Average test loss: 0.004250597106913725\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018654581828249826\n",
      "Average test loss: 0.004103246277819077\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.059262028503749106\n",
      "Average test loss: 0.004506816518182556\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02124314435157511\n",
      "Average test loss: 0.004112712503307396\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01990945485730966\n",
      "Average test loss: 0.003882614745034112\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019266323768430287\n",
      "Average test loss: 0.0038074396076715655\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01884854711757766\n",
      "Average test loss: 0.0037227566656139162\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01854113570931885\n",
      "Average test loss: 0.0036702325671083397\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01828258290555742\n",
      "Average test loss: 0.0036541885828806292\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018073027600844702\n",
      "Average test loss: 0.0035802689637574885\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01788471878816684\n",
      "Average test loss: 0.0035321814057727653\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017733305835061604\n",
      "Average test loss: 0.003533413260140353\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017596431652704875\n",
      "Average test loss: 0.0035032189809199835\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017479928808079824\n",
      "Average test loss: 0.0034848542370729974\n",
      "Epoch 13/300\n",
      "Average training loss: 0.017363464570707744\n",
      "Average test loss: 0.0034326490757779944\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01726523111015558\n",
      "Average test loss: 0.003428517963530289\n",
      "Epoch 15/300\n",
      "Average training loss: 0.017173440353737936\n",
      "Average test loss: 0.0034173906832519504\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017075790097316107\n",
      "Average test loss: 0.0033659591894182896\n",
      "Epoch 17/300\n",
      "Average training loss: 0.016984503467877707\n",
      "Average test loss: 0.0033339247583515116\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01689537545459138\n",
      "Average test loss: 0.003323873428006967\n",
      "Epoch 19/300\n",
      "Average training loss: 0.016827778461906646\n",
      "Average test loss: 0.0033187498001174794\n",
      "Epoch 20/300\n",
      "Average training loss: 0.016740634583764607\n",
      "Average test loss: 0.003310610609336032\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016676897792352572\n",
      "Average test loss: 0.0032896926131927304\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016612905533777342\n",
      "Average test loss: 0.0032780512364374266\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016544788434273667\n",
      "Average test loss: 0.003286023841963874\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01648235068966945\n",
      "Average test loss: 0.003261900746987926\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016421751015716128\n",
      "Average test loss: 0.0032290595326986576\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016370964932772847\n",
      "Average test loss: 0.003220103020883269\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016324162403742473\n",
      "Average test loss: 0.00318709712020225\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016281260659297307\n",
      "Average test loss: 0.003201181673341327\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01622481602595912\n",
      "Average test loss: 0.003182339276704523\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01619024818556176\n",
      "Average test loss: 0.0031856680044697393\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016142793364822865\n",
      "Average test loss: 0.0031748856022540068\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01611439898941252\n",
      "Average test loss: 0.0031894505781431995\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01607788836210966\n",
      "Average test loss: 0.0031463607686261336\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016034107927646903\n",
      "Average test loss: 0.003142260516062379\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016005554741455447\n",
      "Average test loss: 0.003130760053586629\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015976561648150284\n",
      "Average test loss: 0.0031195673280292086\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015950779470304648\n",
      "Average test loss: 0.003124887965619564\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015910936899483204\n",
      "Average test loss: 0.0031172356626225843\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015884647961292003\n",
      "Average test loss: 0.0031110421462605398\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01586708364304569\n",
      "Average test loss: 0.0031027113104032146\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015851292625897462\n",
      "Average test loss: 0.0030955688930633996\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0158142630201247\n",
      "Average test loss: 0.003096950255541338\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01579191879596975\n",
      "Average test loss: 0.003096600870291392\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015773600007924767\n",
      "Average test loss: 0.0030954007024152413\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01574548373536931\n",
      "Average test loss: 0.0030938390681727063\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01573886689874861\n",
      "Average test loss: 0.00307592242045535\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015711095818214945\n",
      "Average test loss: 0.0030845353717191353\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015688818524281185\n",
      "Average test loss: 0.0030683836955577135\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015665836475789546\n",
      "Average test loss: 0.0030916960982398855\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015657803398039606\n",
      "Average test loss: 0.0030820233169943094\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015636371093491714\n",
      "Average test loss: 0.003174297310825851\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015616145109964742\n",
      "Average test loss: 0.0030744938461316957\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015598218768007226\n",
      "Average test loss: 0.0030631334407048095\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015583206148611174\n",
      "Average test loss: 0.003085581128795942\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01556997577349345\n",
      "Average test loss: 0.003072542662628823\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015548140479458702\n",
      "Average test loss: 0.003051054272800684\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015531851276755333\n",
      "Average test loss: 0.003075986748768224\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015515279396540588\n",
      "Average test loss: 0.0030919270354012648\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015501016341149807\n",
      "Average test loss: 0.0030520677932848535\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015496039878163073\n",
      "Average test loss: 0.00306543869111273\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015470308384961553\n",
      "Average test loss: 0.0030706021697777842\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01546440325677395\n",
      "Average test loss: 0.0030446762937224574\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015448808021843433\n",
      "Average test loss: 0.0030594694378475347\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015431245457794932\n",
      "Average test loss: 0.0030395208658236597\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015416517824762397\n",
      "Average test loss: 0.0030274621264802085\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015408101358347468\n",
      "Average test loss: 0.06310292913185225\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015485613718628883\n",
      "Average test loss: 0.0031048914988835653\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015376094914972781\n",
      "Average test loss: 0.0030827340227034357\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015367193282478386\n",
      "Average test loss: 0.003043851593302356\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01535297420538134\n",
      "Average test loss: 0.0030349031223191156\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015333678859803412\n",
      "Average test loss: 0.00303473398130801\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015333681552774377\n",
      "Average test loss: 0.003038201545468635\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01531552004151874\n",
      "Average test loss: 0.003027651355498367\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015304622512724665\n",
      "Average test loss: 0.0030244475230574606\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015292162992060184\n",
      "Average test loss: 0.0030408650963670678\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015283742472529411\n",
      "Average test loss: 0.0030603981734149987\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015266825866368083\n",
      "Average test loss: 0.00302952808721198\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015263482820656565\n",
      "Average test loss: 0.003045308358553383\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015243587562607394\n",
      "Average test loss: 0.0030262894510394997\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015236003504031235\n",
      "Average test loss: 0.0030259107367859948\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015222348087363773\n",
      "Average test loss: 0.003027708761394024\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015223767526447773\n",
      "Average test loss: 0.003058642563720544\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015201333207388718\n",
      "Average test loss: 0.0030314928140077325\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015192515129844347\n",
      "Average test loss: 0.0030182476323097943\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015185269656280677\n",
      "Average test loss: 0.0030166709509988625\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015173212730222279\n",
      "Average test loss: 0.003016960561172002\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015168110264672173\n",
      "Average test loss: 0.0030209378567006852\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015160041214691267\n",
      "Average test loss: 0.0031923332230912316\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015145936222540008\n",
      "Average test loss: 0.003032449951602353\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015133591047591634\n",
      "Average test loss: 0.0030257365179972517\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015129346510602368\n",
      "Average test loss: 0.0030240214189721478\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015108504222499\n",
      "Average test loss: 0.003037638803737031\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015101046292318239\n",
      "Average test loss: 0.003044874672053589\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015098272207710478\n",
      "Average test loss: 0.003022326978958315\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01508938629180193\n",
      "Average test loss: 0.0030211674813181164\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015082339071565204\n",
      "Average test loss: 0.0030337888435978027\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015064430512487889\n",
      "Average test loss: 0.0030520320464339523\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015052122342089812\n",
      "Average test loss: 0.003033654897577233\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015048192910850048\n",
      "Average test loss: 0.0031097567764421306\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015049139321678215\n",
      "Average test loss: 0.0030403537197659413\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015040101788110204\n",
      "Average test loss: 0.0030693411942985323\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015027092500693269\n",
      "Average test loss: 0.003040301151573658\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015014198871950308\n",
      "Average test loss: 0.0030428153090178967\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015011715199384424\n",
      "Average test loss: 0.00318460550552441\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01500837954464886\n",
      "Average test loss: 0.003027117231446836\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01499419073926078\n",
      "Average test loss: 0.0030547512926989134\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014981016408238146\n",
      "Average test loss: 0.003038513842970133\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014969888466099897\n",
      "Average test loss: 0.0030463153873052863\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014967504921058814\n",
      "Average test loss: 0.003047802885580394\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0149542760261231\n",
      "Average test loss: 0.0031180674768984317\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014947534386482504\n",
      "Average test loss: 0.0030393026243481372\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014936347542537583\n",
      "Average test loss: 0.003041381970461872\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014931011494663027\n",
      "Average test loss: 0.003024094669976168\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014932683946357833\n",
      "Average test loss: 0.003070015346320967\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014826795830494828\n",
      "Average test loss: 0.003031812021819254\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014828142203390599\n",
      "Average test loss: 0.0030860342842837173\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014825409491856893\n",
      "Average test loss: 0.0030594488605856897\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014813408461709818\n",
      "Average test loss: 0.00302865186230176\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014816954130927722\n",
      "Average test loss: 0.003050674070086744\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014802059960862\n",
      "Average test loss: 0.003060719787246651\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01478784704208374\n",
      "Average test loss: 0.0030514688303487168\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014784892487029234\n",
      "Average test loss: 0.0030575898293819692\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014781560382909245\n",
      "Average test loss: 0.0030285334396693443\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014775278114610248\n",
      "Average test loss: 0.003103731178989013\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014759785691897074\n",
      "Average test loss: 0.003048063387473424\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01476492757929696\n",
      "Average test loss: 0.0030542430080887344\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014748834825224346\n",
      "Average test loss: 0.003094894989911053\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014743589378893375\n",
      "Average test loss: 0.0031317404521008334\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014737803494764699\n",
      "Average test loss: 0.0030619075710160864\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014734680583079656\n",
      "Average test loss: 0.0031397216448353395\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014722446772787307\n",
      "Average test loss: 0.0030159198039521776\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01471710410962502\n",
      "Average test loss: 0.0030565809201863075\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014715398846401108\n",
      "Average test loss: 0.0030575556074165637\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014706188733379047\n",
      "Average test loss: 0.0030395623068842624\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014710933750702275\n",
      "Average test loss: 0.003031721151744326\n",
      "Epoch 147/300\n",
      "Average training loss: 0.014696109119388793\n",
      "Average test loss: 0.0030688226483762265\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014670661291314497\n",
      "Average test loss: 0.0030745426528155802\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014665651872754096\n",
      "Average test loss: 0.0030380899206631715\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01465702165166537\n",
      "Average test loss: 0.0030517158729748595\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014655808865196174\n",
      "Average test loss: 0.0033559702154662874\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014650612019002437\n",
      "Average test loss: 0.0030162828183836407\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014643104929063054\n",
      "Average test loss: 0.0030440058395680455\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014638281909955872\n",
      "Average test loss: 0.0030999600091535183\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014638196484082275\n",
      "Average test loss: 0.0030408427226874565\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01462371868143479\n",
      "Average test loss: 0.0030757615758726996\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014623249938090643\n",
      "Average test loss: 0.003083576456540161\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014618885410328707\n",
      "Average test loss: 0.0031673042726599507\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014554598641892274\n",
      "Average test loss: 0.00350005024464594\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014556204561558034\n",
      "Average test loss: 0.0030781182539131907\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014544738973180453\n",
      "Average test loss: 0.0030804516372995245\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0145374216520124\n",
      "Average test loss: 0.0031179733330176937\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014539238745139705\n",
      "Average test loss: 0.0031109618414193393\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014535832775963677\n",
      "Average test loss: 0.0030905695996350712\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014523943414290747\n",
      "Average test loss: 0.0030742554838458697\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014523925071789158\n",
      "Average test loss: 0.003065281029169758\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014519084717664454\n",
      "Average test loss: 0.003054021569175853\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014514329468210537\n",
      "Average test loss: 0.0030584950397411982\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014501255082587402\n",
      "Average test loss: 0.003072015101297034\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014499091698891587\n",
      "Average test loss: 0.003064429665191306\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014493004402352704\n",
      "Average test loss: 0.0030741649367329146\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014496870246198442\n",
      "Average test loss: 0.0030706073207159837\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014482536235617266\n",
      "Average test loss: 0.003084055493896206\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014483715198934079\n",
      "Average test loss: 0.0030602928035789066\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014477938976552751\n",
      "Average test loss: 0.0030630579023725457\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014473901441527737\n",
      "Average test loss: 0.0031151242603858313\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014475297882325119\n",
      "Average test loss: 0.00309976777413653\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014470720693469047\n",
      "Average test loss: 0.003122488535940647\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014458475806646876\n",
      "Average test loss: 0.003105514797071616\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014452977635794216\n",
      "Average test loss: 0.0030759663859175313\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014443040202889177\n",
      "Average test loss: 0.0031501514601210753\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014449110500514508\n",
      "Average test loss: 0.0031185100436624553\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014449623254438241\n",
      "Average test loss: 0.0031367119718343018\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014445939636892743\n",
      "Average test loss: 0.003071630906727579\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014441377340919442\n",
      "Average test loss: 0.0030707787612660065\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014438217420544889\n",
      "Average test loss: 0.0030758760112027327\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014424818991786903\n",
      "Average test loss: 0.0030758726474725538\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014429096918139192\n",
      "Average test loss: 0.003103627011593845\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014415145019690196\n",
      "Average test loss: 0.0030863019668807587\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014412625834345818\n",
      "Average test loss: 0.003065330935228202\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014401896862520112\n",
      "Average test loss: 0.0030823933742940425\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014409710995025105\n",
      "Average test loss: 0.003076675251747171\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014409629299408859\n",
      "Average test loss: 0.0030836579280181064\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014395000049637424\n",
      "Average test loss: 0.003084043018726839\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014396710491014852\n",
      "Average test loss: 0.003068100072029564\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014389028877847725\n",
      "Average test loss: 0.003072284511393971\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014384773548278544\n",
      "Average test loss: 0.0030875737277997863\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014384220969345834\n",
      "Average test loss: 0.0030644573571367395\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014378154169354174\n",
      "Average test loss: 0.003096091885326637\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01437384340332614\n",
      "Average test loss: 0.0031496176717595924\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014356937347186936\n",
      "Average test loss: 0.0031023496834354266\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014361504967841837\n",
      "Average test loss: 0.003108293582374851\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014353848172558678\n",
      "Average test loss: 0.0030780825199973254\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014360142153170373\n",
      "Average test loss: 0.0030817010154326756\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014359413431750404\n",
      "Average test loss: 0.0031144483747581643\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014353087609840763\n",
      "Average test loss: 0.0030835946769350105\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014346947434047857\n",
      "Average test loss: 0.003037285339501169\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014337831173506049\n",
      "Average test loss: 0.0031514494546378653\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014347662040342888\n",
      "Average test loss: 0.0031263928833521076\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014327916619678338\n",
      "Average test loss: 0.0031916797699199782\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014334763191640378\n",
      "Average test loss: 0.003078938748687506\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014328484552601974\n",
      "Average test loss: 0.003222699577609698\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014327594839864307\n",
      "Average test loss: 0.0031214421447366475\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01431861857490407\n",
      "Average test loss: 0.0031206304277810785\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014322517700493335\n",
      "Average test loss: 0.0031340714448855984\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014314645821849506\n",
      "Average test loss: 0.003096855553901858\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01430771670738856\n",
      "Average test loss: 0.0030956496625310845\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014313310197658009\n",
      "Average test loss: 0.003077513488009572\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014297545882562795\n",
      "Average test loss: 0.0031166864465922116\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014299386010401779\n",
      "Average test loss: 0.0031810604522211684\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014311981447868877\n",
      "Average test loss: 0.0031356138644946948\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014289314641720719\n",
      "Average test loss: 0.0031399688755886423\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014291016059617202\n",
      "Average test loss: 0.0031558480540083513\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01428945441212919\n",
      "Average test loss: 0.0030607295268111757\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014278214322196113\n",
      "Average test loss: 0.003099531097544564\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014273180425167084\n",
      "Average test loss: 0.0030784628238115044\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014272297472589545\n",
      "Average test loss: 0.003111705651713742\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014264138013124466\n",
      "Average test loss: 0.0030694257819818124\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014269543789327144\n",
      "Average test loss: 0.0030997687002850905\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014275677510019806\n",
      "Average test loss: 0.003210731387966209\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014260075240499444\n",
      "Average test loss: 0.0031104677754143872\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014254383112821314\n",
      "Average test loss: 0.003093284309738212\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014253125794231892\n",
      "Average test loss: 0.003152827471701635\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014242868966526455\n",
      "Average test loss: 0.0031520854903178083\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014252945572137833\n",
      "Average test loss: 0.0031811032067570423\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014246249979568852\n",
      "Average test loss: 0.0031522286106935807\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014241145119071007\n",
      "Average test loss: 0.003092291603485743\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014241266529179281\n",
      "Average test loss: 0.003109070879096786\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014235620379447938\n",
      "Average test loss: 0.003090838206724988\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014229957135187255\n",
      "Average test loss: 0.0030860590028266113\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01422731311370929\n",
      "Average test loss: 0.0032188735442856948\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014232766504089037\n",
      "Average test loss: 0.003202585427297486\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0142228862469395\n",
      "Average test loss: 0.0030644591042978896\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014224657216833697\n",
      "Average test loss: 0.003135917988088396\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014216758566598098\n",
      "Average test loss: 0.0032481817738670444\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014227689065039158\n",
      "Average test loss: 0.003114744638403257\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014177954152226448\n",
      "Average test loss: 0.0031409190992514294\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014182083210183515\n",
      "Average test loss: 0.003121558221264018\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01417175670961539\n",
      "Average test loss: 0.0031020176506911714\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014171584603687127\n",
      "Average test loss: 0.0030602337283392746\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014162075531979401\n",
      "Average test loss: 0.003189329254337483\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014174005951318475\n",
      "Average test loss: 0.0031476484909653663\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014157685900727909\n",
      "Average test loss: 0.003154479754674766\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014154960671232807\n",
      "Average test loss: 0.003111652026987738\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014156356164150768\n",
      "Average test loss: 0.003140023076078958\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014155548079146279\n",
      "Average test loss: 0.003184023283835914\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014149168418513405\n",
      "Average test loss: 0.0031083547940684688\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014139515608549117\n",
      "Average test loss: 0.0031051100759456555\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01414285703168975\n",
      "Average test loss: 0.0031827356604238353\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014139553411967225\n",
      "Average test loss: 0.0031012704275134536\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014142366986307832\n",
      "Average test loss: 0.0031629825604872572\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01413647081454595\n",
      "Average test loss: 0.003123150838125083\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014134800649351544\n",
      "Average test loss: 0.00310385731400715\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01414316481600205\n",
      "Average test loss: 0.0030714689025448426\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014121183185941643\n",
      "Average test loss: 0.0031225282665756013\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014128068355222543\n",
      "Average test loss: 0.0030945463159845936\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014117773483196894\n",
      "Average test loss: 0.003088523456826806\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01412381121267875\n",
      "Average test loss: 0.0030950922009845574\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014127666537132528\n",
      "Average test loss: 0.0031372009406073226\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01411938882784711\n",
      "Average test loss: 0.003110105287283659\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014106330138113764\n",
      "Average test loss: 0.0031317467232131295\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01411665373047193\n",
      "Average test loss: 0.003163852612281011\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014117095289131005\n",
      "Average test loss: 0.00329366231088837\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014098720897403029\n",
      "Average test loss: 0.003117572754414545\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014105308141145441\n",
      "Average test loss: 0.003099505589861009\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014103808745741844\n",
      "Average test loss: 0.0031327559147030113\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05559275959100988\n",
      "Average test loss: 0.003997272023310264\n",
      "Epoch 2/300\n",
      "Average training loss: 0.018407904046277204\n",
      "Average test loss: 0.0036112262619038423\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017014199775126245\n",
      "Average test loss: 0.0033685983535316254\n",
      "Epoch 4/300\n",
      "Average training loss: 0.016309429172840385\n",
      "Average test loss: 0.0031979355261557633\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015826162389583058\n",
      "Average test loss: 0.0031397200693479843\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01544816950460275\n",
      "Average test loss: 0.003024206656962633\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01512973229173157\n",
      "Average test loss: 0.0029375393903917735\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014881014645927481\n",
      "Average test loss: 0.002880594123982721\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014658792081806395\n",
      "Average test loss: 0.002829470910752813\n",
      "Epoch 10/300\n",
      "Average training loss: 0.014487668986949656\n",
      "Average test loss: 0.0027925094533711673\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014317304058207405\n",
      "Average test loss: 0.00275363603213595\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01415976377411021\n",
      "Average test loss: 0.002729326818138361\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014023339595231745\n",
      "Average test loss: 0.0026867700388862026\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013902031851311525\n",
      "Average test loss: 0.002664644641594754\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013777269293864568\n",
      "Average test loss: 0.0026253694852607116\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013661786461869876\n",
      "Average test loss: 0.002620198799918095\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013566285694638887\n",
      "Average test loss: 0.002590996232090725\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01346717460743255\n",
      "Average test loss: 0.0025721675948136382\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01335514615310563\n",
      "Average test loss: 0.0025263651251378988\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013278693217370244\n",
      "Average test loss: 0.002518720423181852\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013196288901070753\n",
      "Average test loss: 0.002518886311393645\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013115138029886616\n",
      "Average test loss: 0.002471115566169222\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013043004465599854\n",
      "Average test loss: 0.002454118974506855\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012993557274341584\n",
      "Average test loss: 0.002445868746067087\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01293160971874992\n",
      "Average test loss: 0.0024354548980999326\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012864614200260903\n",
      "Average test loss: 0.002427396827480859\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012816206575267844\n",
      "Average test loss: 0.0024293815214186908\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012776226886444622\n",
      "Average test loss: 0.0024147008467051718\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012727030258211825\n",
      "Average test loss: 0.0023767311595794226\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012678235385153029\n",
      "Average test loss: 0.002402371166066991\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01262632189111577\n",
      "Average test loss: 0.002383120282863577\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012601116500794887\n",
      "Average test loss: 0.0023671691521174377\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012568200907773441\n",
      "Average test loss: 0.0023536978378478025\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012530961663358741\n",
      "Average test loss: 0.00235259715674652\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01250691013617648\n",
      "Average test loss: 0.002362752308965557\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012465777696006828\n",
      "Average test loss: 0.0023356115241638486\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01244241616461012\n",
      "Average test loss: 0.002350945561710331\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012414159199429883\n",
      "Average test loss: 0.002314126763906744\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012383279112478097\n",
      "Average test loss: 0.0023308571442547774\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012362163674500253\n",
      "Average test loss: 0.0023157828255660003\n",
      "Epoch 41/300\n",
      "Average training loss: 0.012333133117192321\n",
      "Average test loss: 0.002306472065962023\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012306633349921969\n",
      "Average test loss: 0.0023093392798263167\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012295164361596108\n",
      "Average test loss: 0.0022926943492558266\n",
      "Epoch 44/300\n",
      "Average training loss: 0.012268571722838614\n",
      "Average test loss: 0.0023039904989095197\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012243607542581029\n",
      "Average test loss: 0.0023004260957241057\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012227062517570124\n",
      "Average test loss: 0.002304429009142849\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012203466852506001\n",
      "Average test loss: 0.0022990944033695593\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01218276617758804\n",
      "Average test loss: 0.002281063608204325\n",
      "Epoch 49/300\n",
      "Average training loss: 0.012169829935663276\n",
      "Average test loss: 0.002287200372873081\n",
      "Epoch 50/300\n",
      "Average training loss: 0.012157395005226135\n",
      "Average test loss: 0.002275985289365053\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012131750036444929\n",
      "Average test loss: 0.0022730163977377946\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012119264455305206\n",
      "Average test loss: 0.0022798487413674593\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012099954998327627\n",
      "Average test loss: 0.002280689824062089\n",
      "Epoch 54/300\n",
      "Average training loss: 0.012090999404589336\n",
      "Average test loss: 0.002276332425264021\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012071246914565563\n",
      "Average test loss: 0.002262200719366471\n",
      "Epoch 56/300\n",
      "Average training loss: 0.012045008341471354\n",
      "Average test loss: 0.0022648803463412655\n",
      "Epoch 57/300\n",
      "Average training loss: 0.012036794382664893\n",
      "Average test loss: 0.0022613781268397966\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0120213311885794\n",
      "Average test loss: 0.002260883540742927\n",
      "Epoch 59/300\n",
      "Average training loss: 0.012004111979570653\n",
      "Average test loss: 0.002255900874837405\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011995183169013924\n",
      "Average test loss: 0.002260396639092101\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011981846120622423\n",
      "Average test loss: 0.0022703392979585464\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011959909486687846\n",
      "Average test loss: 0.0022867850272191895\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011956734600994322\n",
      "Average test loss: 0.0022906939844704335\n",
      "Epoch 64/300\n",
      "Average training loss: 0.011937952415810691\n",
      "Average test loss: 0.002246636488371425\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01193605773233705\n",
      "Average test loss: 0.002247958702345689\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011911436757279768\n",
      "Average test loss: 0.002248175841135283\n",
      "Epoch 67/300\n",
      "Average training loss: 0.011905619897776179\n",
      "Average test loss: 0.002256840138592654\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011898090918031004\n",
      "Average test loss: 0.002250474143359396\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01187759385506312\n",
      "Average test loss: 0.002241005641511745\n",
      "Epoch 70/300\n",
      "Average training loss: 0.011874451067712571\n",
      "Average test loss: 0.0022360145851141877\n",
      "Epoch 71/300\n",
      "Average training loss: 0.011864543591108587\n",
      "Average test loss: 0.0023071950890330806\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01183816907223728\n",
      "Average test loss: 0.0022385355020976727\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011833094163901277\n",
      "Average test loss: 0.0022412641593772503\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011819538409511249\n",
      "Average test loss: 0.0022335274120171863\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011810672261648707\n",
      "Average test loss: 0.002259439668721623\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011806687044600646\n",
      "Average test loss: 0.0022370640178107555\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01179938479926851\n",
      "Average test loss: 0.002228753000187377\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01177390034414\n",
      "Average test loss: 0.0022284920644015075\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011776320079962413\n",
      "Average test loss: 0.002229464021925297\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011763754386868741\n",
      "Average test loss: 0.0022444357404278383\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011752840204371347\n",
      "Average test loss: 0.002250084123056796\n",
      "Epoch 82/300\n",
      "Average training loss: 0.011741403560671541\n",
      "Average test loss: 0.0022420918097098667\n",
      "Epoch 83/300\n",
      "Average training loss: 0.011728381384578017\n",
      "Average test loss: 0.0026169940042826863\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01172570428914494\n",
      "Average test loss: 0.002241123062454992\n",
      "Epoch 85/300\n",
      "Average training loss: 0.011712912771436903\n",
      "Average test loss: 0.002264051290642884\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011702737388511499\n",
      "Average test loss: 0.0022524664228161175\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011699240369101365\n",
      "Average test loss: 0.0022501021079305147\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011678737291859255\n",
      "Average test loss: 0.002223992478930288\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011679869381917847\n",
      "Average test loss: 0.0022236763805978826\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01166374105711778\n",
      "Average test loss: 0.0022217469740038118\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011659005271063911\n",
      "Average test loss: 0.002220094861669673\n",
      "Epoch 92/300\n",
      "Average training loss: 0.011650867662496038\n",
      "Average test loss: 0.0022445617051174245\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011644561683966053\n",
      "Average test loss: 0.0022365894663251107\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011630908733854691\n",
      "Average test loss: 0.0022454995570911304\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011632159944209788\n",
      "Average test loss: 0.0022124474459431238\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011616969267527262\n",
      "Average test loss: 0.00224171853893333\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011607989065349102\n",
      "Average test loss: 0.002219509500389298\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011601082876738575\n",
      "Average test loss: 0.0022388730409244697\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011595564485424094\n",
      "Average test loss: 0.0022813615979005893\n",
      "Epoch 100/300\n",
      "Average training loss: 0.011579704364968671\n",
      "Average test loss: 0.0022241889205243852\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01157832963930236\n",
      "Average test loss: 0.0022238543577906156\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011567500737806161\n",
      "Average test loss: 0.0022552247734533414\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011563385231627359\n",
      "Average test loss: 0.0022344306918481984\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011550308673746056\n",
      "Average test loss: 0.0022472481671720742\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0115603491374188\n",
      "Average test loss: 0.002222611215586464\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011540606824060282\n",
      "Average test loss: 0.0022428979960580665\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011540972426533699\n",
      "Average test loss: 0.0022293887173549997\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011528024516171879\n",
      "Average test loss: 0.002224944570619199\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011518831774592399\n",
      "Average test loss: 0.002233795727396177\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011514984743048748\n",
      "Average test loss: 0.002251470964608921\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011503843512800005\n",
      "Average test loss: 0.002241859333589673\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011495158927308189\n",
      "Average test loss: 0.002222173215407464\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011491885327630572\n",
      "Average test loss: 0.0025000863768574262\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011491293834315406\n",
      "Average test loss: 0.0022725736144930124\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011471989285614756\n",
      "Average test loss: 0.0022188987156583203\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011472751101685896\n",
      "Average test loss: 0.0022378365364339615\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011473307949801287\n",
      "Average test loss: 0.0022382104250912864\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011460502247843478\n",
      "Average test loss: 0.0022212199572887686\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011449929606169463\n",
      "Average test loss: 0.0022225433983322647\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011444652530882093\n",
      "Average test loss: 0.0022457209987979797\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011446179347733657\n",
      "Average test loss: 0.0022117001302540303\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011433859929442406\n",
      "Average test loss: 0.0022523251066191327\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011432160981827312\n",
      "Average test loss: 0.0022217499268137745\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011428085843722026\n",
      "Average test loss: 0.002212955310837262\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011420580224030547\n",
      "Average test loss: 0.0022306146954910624\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01140760603133175\n",
      "Average test loss: 0.0022330001110417975\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01140393074394928\n",
      "Average test loss: 0.0022459410804634293\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011395951672146718\n",
      "Average test loss: 0.002238090731203556\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011388122383919027\n",
      "Average test loss: 0.002211284492578771\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01138917554418246\n",
      "Average test loss: 0.00224701925025632\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011386720295167631\n",
      "Average test loss: 0.0022242276836186648\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011379216697480944\n",
      "Average test loss: 0.0022304731510165666\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01137449915210406\n",
      "Average test loss: 0.002229594108338157\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011372245384587181\n",
      "Average test loss: 0.002208538668023215\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011357506564921804\n",
      "Average test loss: 0.002245809896124734\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011357210797568162\n",
      "Average test loss: 0.0022294722027662727\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011340736819638146\n",
      "Average test loss: 0.0022282110347102086\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01134005457493994\n",
      "Average test loss: 0.0022493200403534705\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011335244990057416\n",
      "Average test loss: 0.0022252154580007\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011332569532924228\n",
      "Average test loss: 0.002241035940746466\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011326806628455718\n",
      "Average test loss: 0.0022547163814306257\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011321782625383801\n",
      "Average test loss: 0.002250880557215876\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011317192243205177\n",
      "Average test loss: 0.0022125994362350966\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011315396641691526\n",
      "Average test loss: 0.0022365085232175057\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011312868418792884\n",
      "Average test loss: 0.0022260786497758493\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011297082031766574\n",
      "Average test loss: 0.00222890034938852\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011300013207727009\n",
      "Average test loss: 0.0022184509184314974\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011297151441375414\n",
      "Average test loss: 0.0022155561600294377\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011282810576260089\n",
      "Average test loss: 0.002219874480739236\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01127718051771323\n",
      "Average test loss: 0.0022309078237869673\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011275266751646995\n",
      "Average test loss: 0.002240377280033297\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011266111185153325\n",
      "Average test loss: 0.0022468759045004847\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011268997824026478\n",
      "Average test loss: 0.0022254727572823562\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011262195771767033\n",
      "Average test loss: 0.0022306023864075543\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011251433018181059\n",
      "Average test loss: 0.002232077506267362\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011244187135663298\n",
      "Average test loss: 0.0022494004557116164\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011253353719909986\n",
      "Average test loss: 0.002259380533463425\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011240469247102737\n",
      "Average test loss: 0.0022150105878503785\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011240469018618266\n",
      "Average test loss: 0.0022350743951068982\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011235633383194606\n",
      "Average test loss: 0.0022641169181507495\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011221383715669313\n",
      "Average test loss: 0.0022098268235309255\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011225533273484972\n",
      "Average test loss: 0.0022101544052776363\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01121733002530204\n",
      "Average test loss: 0.0022332423039608533\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011216521564457151\n",
      "Average test loss: 0.002268096188083291\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011215783538089857\n",
      "Average test loss: 0.0022282018709099956\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011212847593757841\n",
      "Average test loss: 0.002223722234575285\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011198698170483112\n",
      "Average test loss: 0.0022815638228009145\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011193380635645655\n",
      "Average test loss: 0.002252056137037774\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011201056086354785\n",
      "Average test loss: 0.0022444247611694865\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011182187145782842\n",
      "Average test loss: 0.0022359177797204918\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011179039349158605\n",
      "Average test loss: 0.0022187690533076723\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011185268103248543\n",
      "Average test loss: 0.002233879675364329\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011180431257519457\n",
      "Average test loss: 0.0022228590675319234\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011167661057578193\n",
      "Average test loss: 0.002247778486460447\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01116774697187874\n",
      "Average test loss: 0.0022408745870408087\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011162473289502992\n",
      "Average test loss: 0.002236528695250551\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011160167392757204\n",
      "Average test loss: 0.0022457350250333547\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011159906519783868\n",
      "Average test loss: 0.0022269591801903315\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011152346579564942\n",
      "Average test loss: 0.0023004631422874\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011140924447112613\n",
      "Average test loss: 0.002231969053339627\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011145101902385553\n",
      "Average test loss: 0.0023497261044879755\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011145777465568649\n",
      "Average test loss: 0.0022521254767974216\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011137505495713816\n",
      "Average test loss: 0.002230977719442712\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011133053440186712\n",
      "Average test loss: 0.0022425149710228044\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011130857473446264\n",
      "Average test loss: 0.0022506542342404525\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011134543269872666\n",
      "Average test loss: 0.0022297339253127576\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011124084140691493\n",
      "Average test loss: 0.0022431463582648172\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011121036923593945\n",
      "Average test loss: 0.0022191993352025747\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011130883188711272\n",
      "Average test loss: 0.002338315015348295\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011109283431536622\n",
      "Average test loss: 0.002226309856606854\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01111143258627918\n",
      "Average test loss: 0.0022911498294108444\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011101433448493481\n",
      "Average test loss: 0.0022409745478588674\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011099073072274526\n",
      "Average test loss: 0.002244328821491864\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011098357424139976\n",
      "Average test loss: 0.0022435016949764555\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011091578272067838\n",
      "Average test loss: 0.0022390206831817824\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0110937873961197\n",
      "Average test loss: 0.002275797701130311\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011082651089462969\n",
      "Average test loss: 0.0022407083161589173\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011093456546465557\n",
      "Average test loss: 0.0022912367863787545\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011078854975601037\n",
      "Average test loss: 0.002252160369108121\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011071162496176032\n",
      "Average test loss: 0.002227814563653535\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011075176487366358\n",
      "Average test loss: 0.0022369777402944037\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011078227437204784\n",
      "Average test loss: 0.002257094475337201\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011066340105401145\n",
      "Average test loss: 0.0022562446540428534\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011057082662979762\n",
      "Average test loss: 0.002244894774423705\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011057366834746467\n",
      "Average test loss: 0.0022295822111061876\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011058357333143552\n",
      "Average test loss: 0.0022529860019890797\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011061470123214854\n",
      "Average test loss: 0.0023334733813794124\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011055662373701732\n",
      "Average test loss: 0.002244843000339137\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011052466296487384\n",
      "Average test loss: 0.0022409413850141896\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011037750956912836\n",
      "Average test loss: 0.0022448363753242626\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011033904373645782\n",
      "Average test loss: 0.002278495378378365\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011038108561601903\n",
      "Average test loss: 0.002278769549810224\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011032963653405507\n",
      "Average test loss: 0.002230307978681392\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011029759003884263\n",
      "Average test loss: 0.002243499323208299\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01102554201417499\n",
      "Average test loss: 0.002266398321216305\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011029290840029717\n",
      "Average test loss: 0.0022605369980964398\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011023573169277774\n",
      "Average test loss: 0.0022934758224421076\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011018783963388868\n",
      "Average test loss: 0.002270194644936257\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01100979235685534\n",
      "Average test loss: 0.0022453344725072385\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011014209662046697\n",
      "Average test loss: 0.002380700803258353\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011020545453661018\n",
      "Average test loss: 0.0023352732439008025\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011007368000845114\n",
      "Average test loss: 0.002313229308670594\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010996077932417393\n",
      "Average test loss: 0.0022708756696018908\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011005185056891706\n",
      "Average test loss: 0.002272483664668269\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011011802923348215\n",
      "Average test loss: 0.002255985002964735\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010990896498163542\n",
      "Average test loss: 0.0022791077498760487\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01098545874406894\n",
      "Average test loss: 0.0022453016564249993\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0109954329588347\n",
      "Average test loss: 0.002270865133860045\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01099014001008537\n",
      "Average test loss: 0.0022619785049723252\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010972629046274556\n",
      "Average test loss: 0.0023773674459920987\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010978629786107275\n",
      "Average test loss: 0.0022585674600882665\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010978128872811794\n",
      "Average test loss: 0.0023448462403482862\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010973828494548797\n",
      "Average test loss: 0.0022316457103523945\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010977344053486982\n",
      "Average test loss: 0.0023900464636584123\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010958761034740342\n",
      "Average test loss: 0.0022896714271563624\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010982522175543838\n",
      "Average test loss: 0.002802570877596736\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010967145709527863\n",
      "Average test loss: 0.0022892207416395347\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01095787541485495\n",
      "Average test loss: 0.0022868742318823934\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010958552226424218\n",
      "Average test loss: 0.0023014013212588097\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010959087122645643\n",
      "Average test loss: 0.0023248911793861122\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0109502821067969\n",
      "Average test loss: 0.0022671613511112\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010951387817660968\n",
      "Average test loss: 0.0022652304519174826\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010941509965393278\n",
      "Average test loss: 0.002239414853768216\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010954279580877887\n",
      "Average test loss: 0.0022970406950140994\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010949715400735537\n",
      "Average test loss: 0.0022555448240083125\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010949221495125028\n",
      "Average test loss: 0.002279855480314129\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010939958311617374\n",
      "Average test loss: 0.0022728817781640424\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010938537425465054\n",
      "Average test loss: 0.0022696947707898088\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010929697900182672\n",
      "Average test loss: 0.0022823938253439136\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01092740166021718\n",
      "Average test loss: 0.0023039675917890337\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0109247396695945\n",
      "Average test loss: 0.002264613940897915\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010929862697091368\n",
      "Average test loss: 0.0022724227556544874\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010924153429766496\n",
      "Average test loss: 0.002276332537540131\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010920764652391274\n",
      "Average test loss: 0.0022633048482239245\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010922375271717707\n",
      "Average test loss: 0.0022520316080190243\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010921072707408004\n",
      "Average test loss: 0.0022593150530010463\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010912559783293141\n",
      "Average test loss: 0.0022679416414143313\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010913658722407288\n",
      "Average test loss: 0.002265400374515189\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010912325006806188\n",
      "Average test loss: 0.002293527671032482\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010910871082709895\n",
      "Average test loss: 0.0022678492735657427\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010905348260369565\n",
      "Average test loss: 0.0022430750432734688\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010896667401823733\n",
      "Average test loss: 0.0022586919070324966\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010903404355049133\n",
      "Average test loss: 0.002326643274579611\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010899049416184425\n",
      "Average test loss: 0.0022670720271352265\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010897516451776027\n",
      "Average test loss: 0.002279768537523018\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010912269968125556\n",
      "Average test loss: 0.002245172465013133\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0108975482740336\n",
      "Average test loss: 0.0022591186055085726\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010885498027834627\n",
      "Average test loss: 0.0022788795817436443\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010878549728956487\n",
      "Average test loss: 0.002319486616386308\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010892785236032473\n",
      "Average test loss: 0.0023029415906510417\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01088260957184765\n",
      "Average test loss: 0.002265778198838234\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010884767174720764\n",
      "Average test loss: 0.0022496101746542585\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010887219717933072\n",
      "Average test loss: 0.002324846442271438\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01087105226682292\n",
      "Average test loss: 0.0022795069441199304\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010879109279149108\n",
      "Average test loss: 0.002257282213618358\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010878610496305757\n",
      "Average test loss: 0.0023588269808226163\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01086636057909992\n",
      "Average test loss: 0.0023323726018683777\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010875326346192095\n",
      "Average test loss: 0.002270621042802102\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010862255997127956\n",
      "Average test loss: 0.0022528011372519863\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01086465709656477\n",
      "Average test loss: 0.002275842071821292\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010868209431568781\n",
      "Average test loss: 0.002279724284592602\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010856467160085837\n",
      "Average test loss: 0.002281378014634053\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010860331852402951\n",
      "Average test loss: 0.002324338879850176\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010854194344745742\n",
      "Average test loss: 0.002294947449221379\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010846316408779887\n",
      "Average test loss: 0.002272702622537812\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010860170966221227\n",
      "Average test loss: 0.0022987771773089966\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010851616789897283\n",
      "Average test loss: 0.00227707326432897\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010852440449512666\n",
      "Average test loss: 0.002265040007730325\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010845230417119132\n",
      "Average test loss: 0.0022891467932818666\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010838784538209438\n",
      "Average test loss: 0.0022744115344766115\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010839664821823438\n",
      "Average test loss: 0.0022769133570707508\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010841585225115219\n",
      "Average test loss: 0.002284498088889652\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010843292257024182\n",
      "Average test loss: 0.0023431030810914105\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010843787868817647\n",
      "Average test loss: 0.0023057585244791374\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010825021307501528\n",
      "Average test loss: 0.002308264857985907\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010831470111178028\n",
      "Average test loss: 0.0022734778453078535\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010827012301319175\n",
      "Average test loss: 0.0022597904540598394\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010826332843138112\n",
      "Average test loss: 0.0023407503677946\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01081806299297346\n",
      "Average test loss: 0.002386876860840453\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010817043667866124\n",
      "Average test loss: 0.002312124154426985\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05084100628064738\n",
      "Average test loss: 0.003312101833936241\n",
      "Epoch 2/300\n",
      "Average training loss: 0.015327441283398205\n",
      "Average test loss: 0.002964634855588277\n",
      "Epoch 3/300\n",
      "Average training loss: 0.013916570163435405\n",
      "Average test loss: 0.002669556675892737\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013179802873068386\n",
      "Average test loss: 0.0026054775690038997\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01264757046269046\n",
      "Average test loss: 0.002419527116955982\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012225957847303815\n",
      "Average test loss: 0.0023466521679527228\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011905061238341862\n",
      "Average test loss: 0.0022445809859782456\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011651692922744487\n",
      "Average test loss: 0.002257527897341384\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011423244890239504\n",
      "Average test loss: 0.002133031221210129\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01123747381195426\n",
      "Average test loss: 0.002105541406199336\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011073451556265355\n",
      "Average test loss: 0.0020835161914841997\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010929814077085919\n",
      "Average test loss: 0.002023097712546587\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010797846249408192\n",
      "Average test loss: 0.0019983347007590862\n",
      "Epoch 14/300\n",
      "Average training loss: 0.010692721689740817\n",
      "Average test loss: 0.001974755329804288\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010562813320921526\n",
      "Average test loss: 0.0020146650603661935\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010479163038233916\n",
      "Average test loss: 0.0019177679494023323\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010368556509415308\n",
      "Average test loss: 0.002009833121775753\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0102766354125407\n",
      "Average test loss: 0.0019033761326637533\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010184688121080398\n",
      "Average test loss: 0.00190444619467275\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010102474045422341\n",
      "Average test loss: 0.0018753164919714134\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010024742162062061\n",
      "Average test loss: 0.0018408722986156742\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009963830241726505\n",
      "Average test loss: 0.001847530537388391\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009899917740788725\n",
      "Average test loss: 0.0018029832618518008\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0098490636870265\n",
      "Average test loss: 0.0018058151159849431\n",
      "Epoch 25/300\n",
      "Average training loss: 0.009780068012575308\n",
      "Average test loss: 0.0017738036214270526\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009740284372121096\n",
      "Average test loss: 0.001760323834295074\n",
      "Epoch 27/300\n",
      "Average training loss: 0.00969359214645293\n",
      "Average test loss: 0.0017449058543683755\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009643576045003202\n",
      "Average test loss: 0.0017502305165140164\n",
      "Epoch 29/300\n",
      "Average training loss: 0.00962460369327002\n",
      "Average test loss: 0.001738162209176355\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009570840907593569\n",
      "Average test loss: 0.0017582311502968271\n",
      "Epoch 31/300\n",
      "Average training loss: 0.009546830193036133\n",
      "Average test loss: 0.0017614919292844003\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009517168450272746\n",
      "Average test loss: 0.0017619790716303719\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009477239617456992\n",
      "Average test loss: 0.001712178905804952\n",
      "Epoch 34/300\n",
      "Average training loss: 0.00945222990297609\n",
      "Average test loss: 0.0017052872257514132\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009417858388274908\n",
      "Average test loss: 0.0017081474792212247\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00939882967993617\n",
      "Average test loss: 0.001680646505413784\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009385459203273059\n",
      "Average test loss: 0.0016734175201919344\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009341503528257212\n",
      "Average test loss: 0.0016989175012956062\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009328503186504047\n",
      "Average test loss: 0.0016675979964849022\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009307000684241454\n",
      "Average test loss: 0.001664432504731748\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00929060647305515\n",
      "Average test loss: 0.0016663462254736159\n",
      "Epoch 42/300\n",
      "Average training loss: 0.00926829619291756\n",
      "Average test loss: 0.0016724137601753077\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009256878979504108\n",
      "Average test loss: 0.0017642331755099198\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009221390480796496\n",
      "Average test loss: 0.0016631580682264435\n",
      "Epoch 45/300\n",
      "Average training loss: 0.009211108398934206\n",
      "Average test loss: 0.0016463682039951286\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009198481639226277\n",
      "Average test loss: 0.0016530759464949368\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009182739878694217\n",
      "Average test loss: 0.0016494020314680206\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009161434128466579\n",
      "Average test loss: 0.0016473469029491146\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009148218923972712\n",
      "Average test loss: 0.0016390356924384833\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009134035877469512\n",
      "Average test loss: 0.001644020556161801\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009114213231537077\n",
      "Average test loss: 0.0016407308706806766\n",
      "Epoch 52/300\n",
      "Average training loss: 0.00909462768625882\n",
      "Average test loss: 0.0016355281398735112\n",
      "Epoch 53/300\n",
      "Average training loss: 0.009084827850262324\n",
      "Average test loss: 0.0016533265016041695\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009070565645893414\n",
      "Average test loss: 0.0016221315000827113\n",
      "Epoch 55/300\n",
      "Average training loss: 0.009061541295713849\n",
      "Average test loss: 0.0016383756379493408\n",
      "Epoch 56/300\n",
      "Average training loss: 0.00904744954738352\n",
      "Average test loss: 0.001625913800464736\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009030617287175522\n",
      "Average test loss: 0.0016267094663861726\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009024167783558369\n",
      "Average test loss: 0.0016221027178689837\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009016391756219997\n",
      "Average test loss: 0.0016197751636306446\n",
      "Epoch 60/300\n",
      "Average training loss: 0.009001397464010451\n",
      "Average test loss: 0.0016125103337690234\n",
      "Epoch 61/300\n",
      "Average training loss: 0.008986057757503456\n",
      "Average test loss: 0.0016212469087913631\n",
      "Epoch 62/300\n",
      "Average training loss: 0.008977344623042477\n",
      "Average test loss: 0.0016360549319328532\n",
      "Epoch 63/300\n",
      "Average training loss: 0.008970393913901514\n",
      "Average test loss: 0.0016164474127193292\n",
      "Epoch 64/300\n",
      "Average training loss: 0.008953108327670229\n",
      "Average test loss: 0.0016113371204377876\n",
      "Epoch 65/300\n",
      "Average training loss: 0.008946237908883227\n",
      "Average test loss: 0.0016272310831894478\n",
      "Epoch 66/300\n",
      "Average training loss: 0.008932773266401555\n",
      "Average test loss: 0.0016038358577837547\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008924572354389562\n",
      "Average test loss: 0.0016031365333538917\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008912975533968873\n",
      "Average test loss: 0.001611392281121678\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008901718022922675\n",
      "Average test loss: 0.0016041848002415564\n",
      "Epoch 70/300\n",
      "Average training loss: 0.008891384738186995\n",
      "Average test loss: 0.001617437993383242\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008889052665068044\n",
      "Average test loss: 0.0016093673185031447\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008882914832068814\n",
      "Average test loss: 0.0015950789043886794\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008862405954963631\n",
      "Average test loss: 0.001593005611664719\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008854837096399731\n",
      "Average test loss: 0.001590956442678968\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008848636320067776\n",
      "Average test loss: 0.0015917050371774368\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008841229355583588\n",
      "Average test loss: 0.0015984413673480352\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008823959602249994\n",
      "Average test loss: 0.0015929264275150166\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008828480902645323\n",
      "Average test loss: 0.0017095716742074323\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008810811259680324\n",
      "Average test loss: 0.0015917290267017153\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008801690312309398\n",
      "Average test loss: 0.0016081442919870217\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008799961093399259\n",
      "Average test loss: 0.0015917881451961067\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00878787303219239\n",
      "Average test loss: 0.0015917607077086966\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008780619700749715\n",
      "Average test loss: 0.0015888914218586353\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008775205575757556\n",
      "Average test loss: 0.0015888814556722839\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008763434197339747\n",
      "Average test loss: 0.0015912428092625405\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00876183060887787\n",
      "Average test loss: 0.0015824181338151297\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008750589270972543\n",
      "Average test loss: 0.0016114056950642002\n",
      "Epoch 88/300\n",
      "Average training loss: 0.008745007306337357\n",
      "Average test loss: 0.0016231447226471372\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008739138121406238\n",
      "Average test loss: 0.0015871254053587715\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008735559194452232\n",
      "Average test loss: 0.00158508497931891\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008724146911667454\n",
      "Average test loss: 0.0015970123124619325\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008714113151861562\n",
      "Average test loss: 0.0015945825068694021\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00871100269133846\n",
      "Average test loss: 0.0015789549518376589\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008705417586697472\n",
      "Average test loss: 0.0015908763880530994\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008697676474435461\n",
      "Average test loss: 0.0015945207100982468\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008688579983595344\n",
      "Average test loss: 0.0015892683968672322\n",
      "Epoch 97/300\n",
      "Average training loss: 0.008690030577696031\n",
      "Average test loss: 0.0015889604658716255\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008686689746876558\n",
      "Average test loss: 0.001576408011942274\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008672228125234445\n",
      "Average test loss: 0.0015884812261081403\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008666888612012069\n",
      "Average test loss: 0.0016085397829819056\n",
      "Epoch 101/300\n",
      "Average training loss: 0.008660807509803109\n",
      "Average test loss: 0.0015860738163400027\n",
      "Epoch 102/300\n",
      "Average training loss: 0.008660985481821828\n",
      "Average test loss: 0.0015972932043174903\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00865163303166628\n",
      "Average test loss: 0.0016029772637411951\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008650337054911587\n",
      "Average test loss: 0.0015759423743519519\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008643754870941241\n",
      "Average test loss: 0.0017666851561516524\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008629681992034118\n",
      "Average test loss: 0.0015802258467301727\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008637255055622923\n",
      "Average test loss: 0.001584723660722375\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008617404962165488\n",
      "Average test loss: 0.0015710352780297398\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008610877525889212\n",
      "Average test loss: 0.0015815690747565693\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008609401315036748\n",
      "Average test loss: 0.0015830987833647264\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008606363337073061\n",
      "Average test loss: 0.001569407908245921\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008600131265819072\n",
      "Average test loss: 0.0015784251833748486\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008595860878212585\n",
      "Average test loss: 0.0015700002155887584\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008585217193596893\n",
      "Average test loss: 0.0015833606724109915\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00858131339525183\n",
      "Average test loss: 0.0015810132755173577\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008590307940211562\n",
      "Average test loss: 0.0016175315645005968\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008578697008805142\n",
      "Average test loss: 0.0015761970872473386\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008568971460892095\n",
      "Average test loss: 0.0015860575679689645\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008564834634462993\n",
      "Average test loss: 0.0015813364894646737\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008561155114736822\n",
      "Average test loss: 0.0015819317407699096\n",
      "Epoch 121/300\n",
      "Average training loss: 0.008548239080856244\n",
      "Average test loss: 0.001610326328728762\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008548419342272812\n",
      "Average test loss: 0.0016198065288157927\n",
      "Epoch 123/300\n",
      "Average training loss: 0.00854623397356934\n",
      "Average test loss: 0.0015666988845914602\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008541977453562949\n",
      "Average test loss: 0.0015743933741210235\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008538975696596835\n",
      "Average test loss: 0.0017827964717936184\n",
      "Epoch 126/300\n",
      "Average training loss: 0.00852733407707678\n",
      "Average test loss: 0.0015768296459896696\n",
      "Epoch 127/300\n",
      "Average training loss: 0.008529923039591974\n",
      "Average test loss: 0.001585773650349842\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008525598970966207\n",
      "Average test loss: 0.0016083185780379507\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008517086442973878\n",
      "Average test loss: 0.0015744275989838772\n",
      "Epoch 130/300\n",
      "Average training loss: 0.008510625773006016\n",
      "Average test loss: 0.0015702968887570832\n",
      "Epoch 131/300\n",
      "Average training loss: 0.00851131929581364\n",
      "Average test loss: 0.0016028555726839437\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008504453938868312\n",
      "Average test loss: 0.0015639730555315813\n",
      "Epoch 133/300\n",
      "Average training loss: 0.00850144019888507\n",
      "Average test loss: 0.0016042407518770132\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008494144378436937\n",
      "Average test loss: 0.0015736567088299328\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008488061825020446\n",
      "Average test loss: 0.0015901954960491922\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008491286526951525\n",
      "Average test loss: 0.0015768351252708171\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008476597058276336\n",
      "Average test loss: 0.0015767675574041074\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008480856486906607\n",
      "Average test loss: 0.0015990380321939787\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00848099355896314\n",
      "Average test loss: 0.0015713580699844494\n",
      "Epoch 140/300\n",
      "Average training loss: 0.00847014761219422\n",
      "Average test loss: 0.0015972303216242128\n",
      "Epoch 141/300\n",
      "Average training loss: 0.008466920038478242\n",
      "Average test loss: 0.0015968667032818\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008460288683573404\n",
      "Average test loss: 0.0015941364785863293\n",
      "Epoch 143/300\n",
      "Average training loss: 0.00846682724605004\n",
      "Average test loss: 0.0015897118545447786\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008451714871658219\n",
      "Average test loss: 0.0015953486522452699\n",
      "Epoch 145/300\n",
      "Average training loss: 0.00845252736326721\n",
      "Average test loss: 0.0015787790056525006\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008451400460468399\n",
      "Average test loss: 0.00157059919120123\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008449326689458556\n",
      "Average test loss: 0.0015792143049960334\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00844160300741593\n",
      "Average test loss: 0.0016009337131658362\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008440339158806535\n",
      "Average test loss: 0.0015756684721757968\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008437078160130316\n",
      "Average test loss: 0.0015769035348461734\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008429823820375734\n",
      "Average test loss: 0.0016004356968527039\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008427996546030044\n",
      "Average test loss: 0.001581937433530887\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008430496083779467\n",
      "Average test loss: 0.0015812363766340746\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008421886271900601\n",
      "Average test loss: 0.0015705387230134673\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008419758524331782\n",
      "Average test loss: 0.0015907399172170294\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00841902657929394\n",
      "Average test loss: 0.0015927646530585157\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008405484154820442\n",
      "Average test loss: 0.0015781414459149043\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008408907973517975\n",
      "Average test loss: 0.0016013593919989135\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008402512931989299\n",
      "Average test loss: 0.0015720562456796566\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008401668296919929\n",
      "Average test loss: 0.0015947805857285857\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008388011704716418\n",
      "Average test loss: 0.001583554523376127\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008390333178970548\n",
      "Average test loss: 0.001583645777673357\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008388632917983665\n",
      "Average test loss: 0.0015822990442522697\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008390918934510813\n",
      "Average test loss: 0.0015700431985573636\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008386831565449636\n",
      "Average test loss: 0.001597640136567255\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008383556120097637\n",
      "Average test loss: 0.0015731220663421685\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008373653906501001\n",
      "Average test loss: 0.0015880466249460976\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008394722923222516\n",
      "Average test loss: 0.0015808383335048953\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008368768475535844\n",
      "Average test loss: 0.001588981026938806\n",
      "Epoch 170/300\n",
      "Average training loss: 0.00836713662578\n",
      "Average test loss: 0.001627809823387199\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008368158747752508\n",
      "Average test loss: 0.001633175468330996\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008364627400206196\n",
      "Average test loss: 0.0015840221100176374\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008361952229506439\n",
      "Average test loss: 0.001593667815025482\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008352830522590213\n",
      "Average test loss: 0.0015789948080976804\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00835963331701027\n",
      "Average test loss: 0.0015867275148630142\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008349797486017147\n",
      "Average test loss: 0.0015842648264434603\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008345245041781002\n",
      "Average test loss: 0.0015853836199061738\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008341401648190286\n",
      "Average test loss: 0.0015723795841137568\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008338327928549714\n",
      "Average test loss: 0.001574112504720688\n",
      "Epoch 180/300\n",
      "Average training loss: 0.008344348249750005\n",
      "Average test loss: 0.0015716767279017304\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008332949771235387\n",
      "Average test loss: 0.001580507158405251\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008337110239598487\n",
      "Average test loss: 0.0015770350631533398\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008339814293715688\n",
      "Average test loss: 0.0015733348925908407\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008323967983325322\n",
      "Average test loss: 0.001570454757143226\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008318976532253955\n",
      "Average test loss: 0.001588658867817786\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00832369433881508\n",
      "Average test loss: 0.0015712556814153989\n",
      "Epoch 187/300\n",
      "Average training loss: 0.00832525236159563\n",
      "Average test loss: 0.0015798584669828415\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008318855027357738\n",
      "Average test loss: 0.0016065453842489257\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008318249706592824\n",
      "Average test loss: 0.0016174621919376982\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008309429296188884\n",
      "Average test loss: 0.0016002756454464463\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008312029785994027\n",
      "Average test loss: 0.0015712599240553875\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008304851456648773\n",
      "Average test loss: 0.0015836930287381013\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008302136570215226\n",
      "Average test loss: 0.0015940449060872198\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00830284421145916\n",
      "Average test loss: 0.0016040189055105051\n",
      "Epoch 195/300\n",
      "Average training loss: 0.00830427241573731\n",
      "Average test loss: 0.0015768079362395737\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008301970401157936\n",
      "Average test loss: 0.0015982517579363452\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00829460369878345\n",
      "Average test loss: 0.001594267083487163\n",
      "Epoch 198/300\n",
      "Average training loss: 0.00829534567064709\n",
      "Average test loss: 0.0016107811677373118\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008282271479980813\n",
      "Average test loss: 0.0015914141363981696\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00828974096601208\n",
      "Average test loss: 0.001580911108189159\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008281700670719146\n",
      "Average test loss: 0.0015740186862854494\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008290934642983807\n",
      "Average test loss: 0.0015807012663119368\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008278304711812073\n",
      "Average test loss: 0.0015803968072351483\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008273283099962605\n",
      "Average test loss: 0.0015799526239020957\n",
      "Epoch 205/300\n",
      "Average training loss: 0.00827894419638647\n",
      "Average test loss: 0.001581522496106724\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008273105814639066\n",
      "Average test loss: 0.0016050947366489305\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008271341270870633\n",
      "Average test loss: 0.0015918283871271543\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008264782269381814\n",
      "Average test loss: 0.0015854129824373457\n",
      "Epoch 209/300\n",
      "Average training loss: 0.00826500779307551\n",
      "Average test loss: 0.0017588402841033207\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008262863251898024\n",
      "Average test loss: 0.001583006674837735\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008260839215997193\n",
      "Average test loss: 0.0015941796300725805\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008259050762487782\n",
      "Average test loss: 0.0015900758100259634\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008255898841967185\n",
      "Average test loss: 0.0016151055696730812\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008254826058530146\n",
      "Average test loss: 0.0016480374618226454\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008251465116110112\n",
      "Average test loss: 0.0015898604573061068\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008246783887760506\n",
      "Average test loss: 0.0017302970294323233\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008246214513149526\n",
      "Average test loss: 0.001591540895609392\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008245420684417089\n",
      "Average test loss: 0.0015945471919452151\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008239358365535736\n",
      "Average test loss: 0.001596441639173362\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00824400460968415\n",
      "Average test loss: 0.0015940952582491768\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008239306575722164\n",
      "Average test loss: 0.0015930163699926603\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008243047167443568\n",
      "Average test loss: 0.001625820882514947\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008229119778507286\n",
      "Average test loss: 0.001580575152248558\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008230036028971275\n",
      "Average test loss: 0.0015903877351019118\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008227867572257916\n",
      "Average test loss: 0.0015874188936108517\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008225175438241826\n",
      "Average test loss: 0.0015802312137352095\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008223435602254338\n",
      "Average test loss: 0.0016459150972465673\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008223055734402603\n",
      "Average test loss: 0.0015795851996065014\n",
      "Epoch 229/300\n",
      "Average training loss: 0.00822099761871828\n",
      "Average test loss: 0.0016220303216121263\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00822279099871715\n",
      "Average test loss: 0.001585913789458573\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008223210689922173\n",
      "Average test loss: 0.0015952787413779231\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008217167171753114\n",
      "Average test loss: 0.001681624114099476\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008214249187459548\n",
      "Average test loss: 0.0015922947001333037\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0082062180965311\n",
      "Average test loss: 0.001587321858594401\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008207675880855985\n",
      "Average test loss: 0.0016149424568025603\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008214962537503905\n",
      "Average test loss: 0.001596303252917197\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008207872953679826\n",
      "Average test loss: 0.0016172644135852654\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00820157165866759\n",
      "Average test loss: 0.0015921110287308692\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008210845660418272\n",
      "Average test loss: 0.0016255618705310756\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008207747523155477\n",
      "Average test loss: 0.0015937604057706064\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008203457101351685\n",
      "Average test loss: 0.0016022849466858639\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008192939682139291\n",
      "Average test loss: 0.0015929455679530898\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00819288591792186\n",
      "Average test loss: 0.001578709224342472\n",
      "Epoch 244/300\n",
      "Average training loss: 0.00819448704438077\n",
      "Average test loss: 0.0015919612221833733\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008191115596228175\n",
      "Average test loss: 0.0016057589766052035\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008192624198065863\n",
      "Average test loss: 0.0015848135247619616\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008192097222225534\n",
      "Average test loss: 0.001591744408270137\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008197042947014174\n",
      "Average test loss: 0.0016622535965094963\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008183240679403146\n",
      "Average test loss: 0.0015961537888894478\n",
      "Epoch 250/300\n",
      "Average training loss: 0.00817052296631866\n",
      "Average test loss: 0.0016041432456630799\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008182301585459047\n",
      "Average test loss: 0.0015947677279926009\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008183475772125854\n",
      "Average test loss: 0.0015973363188612792\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008176862871481312\n",
      "Average test loss: 0.0016079954145890142\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008177515513367123\n",
      "Average test loss: 0.0016295195369877748\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008176698443790277\n",
      "Average test loss: 0.0015871222306870752\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008168652589536375\n",
      "Average test loss: 0.001600386684656971\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008173512534134918\n",
      "Average test loss: 0.0015901693219525946\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008166461295965644\n",
      "Average test loss: 0.0015973673463902539\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008168786219424672\n",
      "Average test loss: 0.0016039412483159039\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008164806725664272\n",
      "Average test loss: 0.001634944273572829\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008161212859468327\n",
      "Average test loss: 0.0015877862051646742\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008160709051208364\n",
      "Average test loss: 0.001605542521095938\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008157267888387044\n",
      "Average test loss: 0.0015942021243067251\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0081558332476351\n",
      "Average test loss: 0.0015826821997761726\n",
      "Epoch 265/300\n",
      "Average training loss: 0.00815424654343062\n",
      "Average test loss: 0.0016162142904682293\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008148609641111559\n",
      "Average test loss: 0.001598201052389211\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008161348921143346\n",
      "Average test loss: 0.0015794975692406297\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00814500059808294\n",
      "Average test loss: 0.001639906252010001\n",
      "Epoch 269/300\n",
      "Average training loss: 0.00815090448409319\n",
      "Average test loss: 0.0016074301499045558\n",
      "Epoch 270/300\n",
      "Average training loss: 0.00814747464739614\n",
      "Average test loss: 0.0015843365108594298\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008144460492249992\n",
      "Average test loss: 0.0016299101657544573\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008151293519470427\n",
      "Average test loss: 0.0015893075749691989\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008140941865742206\n",
      "Average test loss: 0.0016254165194307764\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008139005516552262\n",
      "Average test loss: 0.0015876986120517056\n",
      "Epoch 275/300\n",
      "Average training loss: 0.00813883942613999\n",
      "Average test loss: 0.0016124283921801381\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008137762753499879\n",
      "Average test loss: 0.0015984625894990233\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008130912207894855\n",
      "Average test loss: 0.0015839344091299508\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008133980933162902\n",
      "Average test loss: 0.0016026490864654382\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008135743533571561\n",
      "Average test loss: 0.0016096953288652002\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008137632466852665\n",
      "Average test loss: 0.00159648716615306\n",
      "Epoch 281/300\n",
      "Average training loss: 0.00812867925233311\n",
      "Average test loss: 0.0015971916997805239\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008131195448338985\n",
      "Average test loss: 0.001624137530931168\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00813096884265542\n",
      "Average test loss: 0.0015891832812792725\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008123619753453467\n",
      "Average test loss: 0.00159211068890161\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008122655413217014\n",
      "Average test loss: 0.0015999424885958433\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008116453852918412\n",
      "Average test loss: 0.0015946605924723877\n",
      "Epoch 287/300\n",
      "Average training loss: 0.00812421701144841\n",
      "Average test loss: 0.001616640294281145\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008125369361291329\n",
      "Average test loss: 0.0016089528716272778\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008121588757468594\n",
      "Average test loss: 0.0016116083091021413\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008112707696027226\n",
      "Average test loss: 0.0015944238713838988\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008124979888399442\n",
      "Average test loss: 0.0016326712649315596\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008111945981366766\n",
      "Average test loss: 0.001608234592454715\n",
      "Epoch 293/300\n",
      "Average training loss: 0.008111416397409307\n",
      "Average test loss: 0.0016099379625585345\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008111043775247203\n",
      "Average test loss: 0.0016238664356577727\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008109144607351886\n",
      "Average test loss: 0.0016074394314653344\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008111767341279321\n",
      "Average test loss: 0.0015892921527847648\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008106664022637738\n",
      "Average test loss: 0.0015926520827536782\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0081054360345006\n",
      "Average test loss: 0.0016097231621129645\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008107824961758322\n",
      "Average test loss: 0.0016139314223287834\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008100903678271506\n",
      "Average test loss: 0.0016106770163815882\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth3-.01/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.78\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.74\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.49\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7528415042691761\n",
      "Average test loss: 0.0053060034956369136\n",
      "Epoch 2/300\n",
      "Average training loss: 0.11119143628411823\n",
      "Average test loss: 0.005061962634738949\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08272468745708465\n",
      "Average test loss: 0.004715821463200781\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0758079924583435\n",
      "Average test loss: 0.004594141910473506\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0724109559059143\n",
      "Average test loss: 0.004518310106876824\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0702678271399604\n",
      "Average test loss: 0.004503238973518213\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06875890318221516\n",
      "Average test loss: 0.0044075109888282085\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06761713846524557\n",
      "Average test loss: 0.004395750842988491\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06666735280222363\n",
      "Average test loss: 0.004436897427257564\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06593333857589298\n",
      "Average test loss: 0.004310242485668924\n",
      "Epoch 11/300\n",
      "Average training loss: 0.065354350288709\n",
      "Average test loss: 0.004300358399334881\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06482918480038644\n",
      "Average test loss: 0.0042812212080591255\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06436439296272066\n",
      "Average test loss: 0.004258275259907047\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06399056183960702\n",
      "Average test loss: 0.004266754647509919\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06364763938056098\n",
      "Average test loss: 0.004236373666053017\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06337935365570917\n",
      "Average test loss: 0.004200642702894078\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06308142947488361\n",
      "Average test loss: 0.004201554677552647\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0628197327223089\n",
      "Average test loss: 0.004228845188187228\n",
      "Epoch 19/300\n",
      "Average training loss: 0.062563521027565\n",
      "Average test loss: 0.004171132194913096\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06231458647052447\n",
      "Average test loss: 0.0041514170714136624\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06221429453955756\n",
      "Average test loss: 0.0041469258318344755\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0619885767367151\n",
      "Average test loss: 0.004223771449799339\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06180077377623982\n",
      "Average test loss: 0.004126965645700693\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0615920136405362\n",
      "Average test loss: 0.004107075124565098\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06143018577827348\n",
      "Average test loss: 0.004146993633773592\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06128562738498052\n",
      "Average test loss: 0.004105686931560437\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0611297292013963\n",
      "Average test loss: 0.004085701499548224\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06097201812929577\n",
      "Average test loss: 0.004085183286832439\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06083509046170447\n",
      "Average test loss: 0.004072806489550405\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06071084308293131\n",
      "Average test loss: 0.004070953144174483\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06059900912311342\n",
      "Average test loss: 0.004079589609470633\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06047766362627347\n",
      "Average test loss: 0.0040495315848125355\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06037035290731324\n",
      "Average test loss: 0.0040504163727164266\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06029360784755813\n",
      "Average test loss: 0.004091472634838687\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06015209164884355\n",
      "Average test loss: 0.004028973402662409\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06009859875175688\n",
      "Average test loss: 0.004017577229688565\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05997361783848869\n",
      "Average test loss: 0.004021063499566582\n",
      "Epoch 38/300\n",
      "Average training loss: 0.059909186922841605\n",
      "Average test loss: 0.004014862229840623\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0598210652006997\n",
      "Average test loss: 0.004014777446372642\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05978653310736021\n",
      "Average test loss: 0.004014999206695292\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05966455213891135\n",
      "Average test loss: 0.004015551533343063\n",
      "Epoch 42/300\n",
      "Average training loss: 0.059622761580679154\n",
      "Average test loss: 0.004023485632406341\n",
      "Epoch 43/300\n",
      "Average training loss: 0.059556977176003986\n",
      "Average test loss: 0.003999369331532054\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05948607277539041\n",
      "Average test loss: 0.003989426602919896\n",
      "Epoch 45/300\n",
      "Average training loss: 0.059439807252751456\n",
      "Average test loss: 0.003996465936303139\n",
      "Epoch 46/300\n",
      "Average training loss: 0.059386974980433785\n",
      "Average test loss: 0.004011251316923235\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05930867964691586\n",
      "Average test loss: 0.003988587561787831\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05928779599070549\n",
      "Average test loss: 0.003985102757294145\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05918923724028799\n",
      "Average test loss: 0.003979950528591871\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05913699360688528\n",
      "Average test loss: 0.0039946730381084814\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05910138038959768\n",
      "Average test loss: 0.003974281591880653\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05903153645330005\n",
      "Average test loss: 0.00400759960586826\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05898920678098996\n",
      "Average test loss: 0.0039739344918893445\n",
      "Epoch 54/300\n",
      "Average training loss: 0.058941705455382666\n",
      "Average test loss: 0.003990802778965897\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05888407829403877\n",
      "Average test loss: 0.0039742626847906245\n",
      "Epoch 56/300\n",
      "Average training loss: 0.058858805345164404\n",
      "Average test loss: 0.003953914568656021\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05879960358142853\n",
      "Average test loss: 0.003963166135466761\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05877878240413136\n",
      "Average test loss: 0.003971292474617561\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05871080570750766\n",
      "Average test loss: 0.0039729239032086396\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05866856832967864\n",
      "Average test loss: 0.003972098565143016\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05864820860491859\n",
      "Average test loss: 0.0039688392761680815\n",
      "Epoch 62/300\n",
      "Average training loss: 0.058569080551465355\n",
      "Average test loss: 0.00396563947511216\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05853308470050494\n",
      "Average test loss: 0.003959553483252724\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0585578036374516\n",
      "Average test loss: 0.003950712799405058\n",
      "Epoch 65/300\n",
      "Average training loss: 0.058450149551033975\n",
      "Average test loss: 0.0042108761680622895\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05844836871160401\n",
      "Average test loss: 0.0039597048461437225\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05839029888974296\n",
      "Average test loss: 0.003953690084732241\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05833224869105551\n",
      "Average test loss: 0.003969901018672519\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05830520762999852\n",
      "Average test loss: 0.003946551815503173\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0582916260725922\n",
      "Average test loss: 0.003945745603595343\n",
      "Epoch 71/300\n",
      "Average training loss: 0.058260147631168366\n",
      "Average test loss: 0.0039473984293225736\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05820732354786661\n",
      "Average test loss: 0.003957025589835313\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05817387210660511\n",
      "Average test loss: 0.003959046563754479\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05811436632937855\n",
      "Average test loss: 0.003959317812075217\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05807818775044547\n",
      "Average test loss: 0.003977528827057944\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05807571732004484\n",
      "Average test loss: 0.003953541072292461\n",
      "Epoch 77/300\n",
      "Average training loss: 0.058017286820544134\n",
      "Average test loss: 0.003966795642756754\n",
      "Epoch 78/300\n",
      "Average training loss: 0.057951214892996684\n",
      "Average test loss: 0.003944607766552104\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0579485405517949\n",
      "Average test loss: 0.003948472778002421\n",
      "Epoch 80/300\n",
      "Average training loss: 0.057922965427239737\n",
      "Average test loss: 0.00394813393553098\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05785348103443781\n",
      "Average test loss: 0.003939904226611058\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05784653303689427\n",
      "Average test loss: 0.003958907557858361\n",
      "Epoch 83/300\n",
      "Average training loss: 0.057812500920560624\n",
      "Average test loss: 0.00395809809760087\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0577634115450912\n",
      "Average test loss: 0.00403932088944647\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05771851333644655\n",
      "Average test loss: 0.003969791623039378\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05771505582994885\n",
      "Average test loss: 0.003939807980838749\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05763180864519543\n",
      "Average test loss: 0.0039731675386428834\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05760170012381342\n",
      "Average test loss: 0.003986185452797346\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05759942839874162\n",
      "Average test loss: 0.003960899184561438\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0575236546529664\n",
      "Average test loss: 0.003965110509345929\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05751158867941962\n",
      "Average test loss: 0.003950096424255106\n",
      "Epoch 92/300\n",
      "Average training loss: 0.057467381139596305\n",
      "Average test loss: 0.003944556659294499\n",
      "Epoch 93/300\n",
      "Average training loss: 0.057423038575384354\n",
      "Average test loss: 0.003948475675450431\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05740502729018529\n",
      "Average test loss: 0.003943081303603119\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05733799105220371\n",
      "Average test loss: 0.003947720228176978\n",
      "Epoch 96/300\n",
      "Average training loss: 0.057333763529857\n",
      "Average test loss: 0.003931563117644853\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05730481359693739\n",
      "Average test loss: 0.003959717114352518\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0572749906198846\n",
      "Average test loss: 0.0039619165766570305\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05722097375326687\n",
      "Average test loss: 0.003948573874516619\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05718855464458466\n",
      "Average test loss: 0.003943911445016662\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05718731878863441\n",
      "Average test loss: 0.0039438491430547505\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0571324824127886\n",
      "Average test loss: 0.003944771661112706\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05708746335903803\n",
      "Average test loss: 0.00395446011900074\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05706579292151663\n",
      "Average test loss: 0.003980315956183606\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05698917139901055\n",
      "Average test loss: 0.003964287564572361\n",
      "Epoch 106/300\n",
      "Average training loss: 0.056965585029787484\n",
      "Average test loss: 0.003945967114633984\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05691306788722674\n",
      "Average test loss: 0.00397043949779537\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05693549485007922\n",
      "Average test loss: 0.00395668317998449\n",
      "Epoch 109/300\n",
      "Average training loss: 0.056871605846616956\n",
      "Average test loss: 0.003961094184054268\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05682260811991162\n",
      "Average test loss: 0.003941672853918539\n",
      "Epoch 111/300\n",
      "Average training loss: 0.056798515886068345\n",
      "Average test loss: 0.003967133845306105\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05676616980963283\n",
      "Average test loss: 0.003956375436650382\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05671989755829175\n",
      "Average test loss: 0.003934412361019188\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05671541971961657\n",
      "Average test loss: 0.003943063721267713\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05666166646613015\n",
      "Average test loss: 0.003967577358500825\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05662625435325835\n",
      "Average test loss: 0.003970612294351061\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05659821141097281\n",
      "Average test loss: 0.003931537673705154\n",
      "Epoch 118/300\n",
      "Average training loss: 0.056519196308321425\n",
      "Average test loss: 0.003944940736724271\n",
      "Epoch 119/300\n",
      "Average training loss: 0.056506306032339734\n",
      "Average test loss: 0.004017786307467354\n",
      "Epoch 120/300\n",
      "Average training loss: 0.056475231462054784\n",
      "Average test loss: 0.003975707883222236\n",
      "Epoch 121/300\n",
      "Average training loss: 0.056412423584196306\n",
      "Average test loss: 0.003978812023169465\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05642134512795342\n",
      "Average test loss: 0.003978266163832612\n",
      "Epoch 123/300\n",
      "Average training loss: 0.056384659234020446\n",
      "Average test loss: 0.0039727685174180405\n",
      "Epoch 124/300\n",
      "Average training loss: 0.056310692969295716\n",
      "Average test loss: 0.003976559798750613\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05628240388631821\n",
      "Average test loss: 0.003947524775233534\n",
      "Epoch 126/300\n",
      "Average training loss: 0.056330192314253916\n",
      "Average test loss: 0.003970694038603041\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0562317108478811\n",
      "Average test loss: 0.003970079097896814\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05616245834363831\n",
      "Average test loss: 0.003971130799295174\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05613923931452963\n",
      "Average training loss: 0.05595472670594851\n",
      "Average test loss: 0.003982268646566404\n",
      "Epoch 134/300\n",
      "Average training loss: 0.055983587834570143\n",
      "Average test loss: 0.0039762804365406435\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0559007179207272\n",
      "Average test loss: 0.0040435529073907265\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05587054813901583\n",
      "Average test loss: 0.004002215484985047\n",
      "Epoch 137/300\n",
      "Average training loss: 0.055876990626255674\n",
      "Average test loss: 0.004013028112964498\n",
      "Epoch 138/300\n",
      "Average training loss: 0.055758189519246416\n",
      "Average test loss: 0.0040115739119549595\n",
      "Epoch 139/300\n",
      "Average training loss: 0.055762695988019306\n",
      "Average test loss: 0.003994089334375329\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05581509244773123\n",
      "Average test loss: 0.004044601961970329\n",
      "Epoch 141/300\n",
      "Average training loss: 0.055710149043136176\n",
      "Average test loss: 0.003966868967024816\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05566051841775576\n",
      "Average test loss: 0.003991726163567768\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05566678956813283\n",
      "Average test loss: 0.004050572018035584\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05560372629761696\n",
      "Average test loss: 0.00397320926334295\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05555128814114465\n",
      "Average test loss: 0.004009914361354378\n",
      "Epoch 146/300\n",
      "Average training loss: 0.055532244165738424\n",
      "Average test loss: 0.004008666224777698\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05544656325711144\n",
      "Average test loss: 0.003981974979241689\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05543175255921152\n",
      "Average test loss: 0.0040056513150533045\n",
      "Epoch 149/300\n",
      "Average training loss: 0.055400538120004866\n",
      "Average test loss: 0.00400424839845962\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05541260030865669\n",
      "Average test loss: 0.004032569242020448\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05533059999015596\n",
      "Average test loss: 0.0040079547557979826\n",
      "Epoch 152/300\n",
      "Average training loss: 0.055322428319189286\n",
      "Average test loss: 0.004022128896994723\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05529404067993164\n",
      "Average test loss: 0.004052304461184475\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05515470450454288\n",
      "Average test loss: 0.0040139069778637754\n",
      "Epoch 155/300\n",
      "Average training loss: 0.055159794131914776\n",
      "Average test loss: 0.003990452401753929\n",
      "Epoch 156/300\n",
      "Average training loss: 0.055178885049290124\n",
      "Average test loss: 0.004068979156514009\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05515835663345125\n",
      "Average test loss: 0.004067346056302388\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05508108589384291\n",
      "Average test loss: 0.004057796380172173\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05493560921152433\n",
      "Average test loss: 0.004016016646184855\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0549131687382857\n",
      "Average test loss: 0.004022900550419258\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05488530816965633\n",
      "Average test loss: 0.004031839482486248\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05483776540557543\n",
      "Average test loss: 0.004053524315771129\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05477435641156302\n",
      "Average test loss: 0.003991806206603845\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05476119197739495\n",
      "Average test loss: 0.004131701985994975\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05474242850144704\n",
      "Average test loss: 0.004735754710518652\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05472010041938888\n",
      "Average test loss: 0.004045657912890116\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05467627492547035\n",
      "Average test loss: 0.004084221947524283\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05465053953395949\n",
      "Average test loss: 0.0040444517417086495\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05459837547606892\n",
      "Average test loss: 0.004071436281005541\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05454681052764257\n",
      "Average test loss: 0.0040665040508740475\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05456368509597249\n",
      "Average test loss: 0.004098201332820787\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05448847856952085\n",
      "Average test loss: 0.004062667792869939\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05445303589436743\n",
      "Average test loss: 0.004128768012341526\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05440512587957912\n",
      "Average test loss: 0.004083211269643572\n",
      "Epoch 178/300\n",
      "Average training loss: 0.054398256700899865\n",
      "Average test loss: 0.0040410464213540154\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05438225146796968\n",
      "Average test loss: 0.004058451499789954\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05435036809908019\n",
      "Average test loss: 0.004059792818708552\n",
      "Epoch 181/300\n",
      "Average training loss: 0.054380580657058294\n",
      "Average test loss: 0.0041733239454527695\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0542340265876717\n",
      "Average test loss: 0.004029684570721454\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05424721794989374\n",
      "Average test loss: 0.004075310539868143\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05425434242023362\n",
      "Average test loss: 0.0040809586433072885\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05416818310817083\n",
      "Average test loss: 0.004152038158228\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05417015379998419\n",
      "Average test loss: 0.004047858218352\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05416337788436148\n",
      "Average test loss: 0.004036506773697005\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05415857122341792\n",
      "Average test loss: 0.004025104513598813\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05403887226846483\n",
      "Average test loss: 0.004055240456221832\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05402138993806309\n",
      "Average test loss: 0.004059945881780651\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0539928829173247\n",
      "Average test loss: 0.004084906271348397\n",
      "Epoch 192/300\n",
      "Average training loss: 0.054017950786484616\n",
      "Average test loss: 0.004036448076781299\n",
      "Epoch 193/300\n",
      "Average training loss: 0.053936939530902436\n",
      "Average test loss: 0.004058912302470869\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05389782669146856\n",
      "Average test loss: 0.004068351573414273\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0538606288002597\n",
      "Average test loss: 0.004075199930825168\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05393542731801669\n",
      "Average test loss: 0.0041148672228058175\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05387199771073129\n",
      "Average test loss: 0.004076812600716948\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05383311645852195\n",
      "Average test loss: 0.004139786837001642\n",
      "Epoch 199/300\n",
      "Average training loss: 0.053742682725191115\n",
      "Average test loss: 0.004033995359722111\n",
      "Epoch 200/300\n",
      "Average training loss: 0.053682015627622606\n",
      "Average test loss: 0.004095985234197643\n",
      "Epoch 201/300\n",
      "Average training loss: 0.053763525125053196\n",
      "Average test loss: 0.004113386864463488\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05371120896273189\n",
      "Average test loss: 0.004057778327001466\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05368846176730262\n",
      "Average test loss: 0.004073889805624883\n",
      "Epoch 204/300\n",
      "Average training loss: 0.053655736658308244\n",
      "Average test loss: 0.004124731093231175\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05354543773333231\n",
      "Average test loss: 0.004076520811766386\n",
      "Epoch 206/300\n",
      "Average training loss: 0.053592756705151665\n",
      "Average test loss: 0.004100990631928047\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05351568954520755\n",
      "Average test loss: 0.0041564631277902254\n",
      "Epoch 208/300\n",
      "Average training loss: 0.053558608661095304\n",
      "Average test loss: 0.00408626106629769\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05342447387841013\n",
      "Average test loss: 0.004104444713228279\n",
      "Epoch 213/300\n",
      "Average training loss: 0.053422681500514345\n",
      "Average test loss: 0.004119975876890951\n",
      "Epoch 214/300\n",
      "Average training loss: 0.053347441716326605\n",
      "Average test loss: 0.004102097042318847\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05335471683740616\n",
      "Average test loss: 0.00412999045331445\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05330606295333968\n",
      "Average test loss: 0.004146657937723729\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05328958782222536\n",
      "Average test loss: 0.004066505016345117\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05328925637072987\n",
      "Average test loss: 0.004122780846224891\n",
      "Epoch 219/300\n",
      "Average training loss: 0.053244945489697985\n",
      "Average test loss: 0.004060554259146253\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05321993043356472\n",
      "Average test loss: 0.004120322898858123\n",
      "Epoch 221/300\n",
      "Average training loss: 0.053229143301645916\n",
      "Average test loss: 0.004110160018627842\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05314551952812407\n",
      "Average test loss: 0.004045320855660571\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05319538191623158\n",
      "Average test loss: 0.004178679661204418\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05314005592796538\n",
      "Average test loss: 0.004080589664686057\n",
      "Epoch 225/300\n",
      "Average training loss: 0.053030992140372595\n",
      "Average test loss: 0.00409485762193799\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05306906074947781\n",
      "Average test loss: 0.004094515329019891\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05303813401526875\n",
      "Average test loss: 0.004094120612574948\n",
      "Epoch 228/300\n",
      "Average training loss: 0.053091730021768146\n",
      "Average test loss: 0.004184463293602069\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0530100892384847\n",
      "Average test loss: 0.004135712479137712\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05298186659812927\n",
      "Average test loss: 0.00411930916706721\n",
      "Epoch 231/300\n",
      "Average training loss: 0.052998860402239695\n",
      "Average test loss: 0.00411187256914046\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05294863665766186\n",
      "Average test loss: 0.004270601096873482\n",
      "Epoch 233/300\n",
      "Average training loss: 0.052926635387871\n",
      "Average test loss: 0.004077170559515556\n",
      "Epoch 234/300\n",
      "Average test loss: 0.004086938492953778\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05285766408509678\n",
      "Average test loss: 0.004152764152321551\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05282804447412491\n",
      "Average test loss: 0.004102051557766067\n",
      "Epoch 239/300\n",
      "Average training loss: 0.052772613065110316\n",
      "Average test loss: 0.0041384309956596955\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05275840888420741\n",
      "Average test loss: 0.00415360467425651\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05275981903076172\n",
      "Average test loss: 0.004143593359738588\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05276187296377288\n",
      "Average test loss: 0.004162040705482165\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05271985508004824\n",
      "Average test loss: 0.004156013651440541\n",
      "Epoch 244/300\n",
      "Average training loss: 0.052663096586863196\n",
      "Average test loss: 0.004338926215552622\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05261289491587215\n",
      "Average test loss: 0.004168490685936477\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05261442785461744\n",
      "Average test loss: 0.0042660838618046705\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05268864728344811\n",
      "Average test loss: 0.004138469946053293\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05254543787572119\n",
      "Average test loss: 0.004181347724464205\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05257285554210345\n",
      "Average test loss: 0.004233390255313781\n",
      "Epoch 250/300\n",
      "Average training loss: 0.052584082752466205\n",
      "Average test loss: 0.00408849247213867\n",
      "Epoch 251/300\n",
      "Average training loss: 0.052575831012593374\n",
      "Average test loss: 0.004090906902733777\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05249318621887101\n",
      "Average test loss: 0.004132512488712867\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05248834019568231\n",
      "Average test loss: 0.0042442747412456405\n",
      "Epoch 254/300\n",
      "Average training loss: 0.052472670283582476\n",
      "Average test loss: 0.009789563219166464\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0524778760737843\n",
      "Average test loss: 0.004120544154403938\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05245148672329055\n",
      "Average test loss: 0.004182939892634749\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05235679965880182\n",
      "Average test loss: 0.004127262212956945\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05242801060941484\n",
      "Average test loss: 0.004235908962786198\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05234376931852765\n",
      "Average test loss: 0.004289119291222758\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05235768658916155\n",
      "Average test loss: 0.0041819388816754025\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05232811651627223\n",
      "Average test loss: 0.004151770932806863\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0522877874341276\n",
      "Average test loss: 0.004100154772400856\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05231009868118498\n",
      "Average test loss: 0.004144813778085841\n",
      "Epoch 264/300\n",
      "Average training loss: 0.052267045872079004\n",
      "Average test loss: 0.004152733874403768\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05242474450336562\n",
      "Average test loss: 0.004168817452672455\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05225351214408874\n",
      "Average test loss: 0.004340105234334866\n",
      "Epoch 270/300\n",
      "Average training loss: 0.052166192849477135\n",
      "Average test loss: 0.004235641022109323\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05212918679581748\n",
      "Average test loss: 0.0040766528265343775\n",
      "Epoch 272/300\n",
      "Average training loss: 0.052154669291443294\n",
      "Average test loss: 0.004186825020859639\n",
      "Epoch 273/300\n",
      "Average training loss: 0.052126064426369134\n",
      "Average test loss: 0.004207450045479668\n",
      "Epoch 274/300\n",
      "Average training loss: 0.052061535292201574\n",
      "Average test loss: 0.004141985111766391\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05204190631376372\n",
      "Average test loss: 0.004203860993186633\n",
      "Epoch 276/300\n",
      "Average training loss: 0.052116919706265134\n",
      "Average test loss: 0.0041539622412787545\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05203621322247717\n",
      "Average test loss: 0.004150313480860657\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05199442351857821\n",
      "Average test loss: 0.0041598555561569005\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05197789991895358\n",
      "Average test loss: 0.00418093325321873\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05198774679832988\n",
      "Average test loss: 0.004142500407579872\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05200206926465035\n",
      "Average test loss: 0.0041977937389165165\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05194999073611366\n",
      "Average test loss: 0.004163619657357534\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05196699351734585\n",
      "Average test loss: 0.004272988750288884\n",
      "Epoch 284/300\n",
      "Average training loss: 0.051943915536006295\n",
      "Average test loss: 0.0041947267295585735\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05191993897491031\n",
      "Average test loss: 0.00417372388827304\n",
      "Epoch 286/300\n",
      "Average training loss: 0.051891836166381836\n",
      "Average test loss: 0.004168273075173298\n",
      "Epoch 287/300\n",
      "Average training loss: 0.051839178419775436\n",
      "Average test loss: 0.004212618855966462\n",
      "Epoch 288/300\n",
      "Average training loss: 0.051849449882904686\n",
      "Average test loss: 0.004111650478301777\n",
      "Epoch 289/300\n",
      "Average training loss: 0.051790019459194606\n",
      "Average test loss: 0.004167762666526768\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05182710429363781\n",
      "Average test loss: 0.004111878681100077\n",
      "Epoch 291/300\n",
      "Average training loss: 0.051806123240126505\n",
      "Average test loss: 0.00416864482478963\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05181289973523882\n",
      "Average test loss: 0.004199680832111173\n",
      "Epoch 293/300\n",
      "Average training loss: 0.051797103265921275\n",
      "Average test loss: 0.004203219018773072\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05176417667998208\n",
      "Average test loss: 0.004220570122616158\n",
      "Epoch 295/300\n",
      "Average training loss: 0.051731243358718024\n",
      "Average test loss: 0.004163263796932167\n",
      "Epoch 296/300\n",
      "Average training loss: 0.051718794968393114\n",
      "Average test loss: 0.004178579283671247\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05170530638760991\n",
      "Average test loss: 0.004264746639567117\n",
      "Epoch 298/300\n",
      "Average training loss: 0.051725654559002986\n",
      "Average test loss: 0.004214871029473014\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05164324710766474\n",
      "Average test loss: 0.004225728022141589\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05168863387902578\n",
      "Average test loss: 0.004189749622303579\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7552336714002821\n",
      "Average test loss: 0.005424671334938871\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09829653745889663\n",
      "Average test loss: 0.004631292878339688\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07816111457016733\n",
      "Average test loss: 0.00443773703980777\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07086150514417225\n",
      "Average test loss: 0.004209396314496796\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06658000536428557\n",
      "Average test loss: 0.004100039884448051\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06373245708147685\n",
      "Average test loss: 0.004024424758222368\n",
      "Epoch 7/300\n",
      "Average training loss: 0.061605742146571475\n",
      "Average test loss: 0.004003205217834976\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05997931650612089\n",
      "Average test loss: 0.003815589265276988\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05862953995664914\n",
      "Average test loss: 0.0037861833235041963\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05746095551053683\n",
      "Average test loss: 0.003705274403716127\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05642352577381664\n",
      "Average test loss: 0.0036564151383936404\n",
      "Epoch 12/300\n",
      "Average training loss: 0.055487696889373994\n",
      "Average test loss: 0.003589749966851539\n",
      "Epoch 13/300\n",
      "Average training loss: 0.054683581557538774\n",
      "Average test loss: 0.003559554654277033\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05392812589638763\n",
      "Average test loss: 0.00351725669287973\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05326465240783162\n",
      "Average test loss: 0.003529532899873124\n",
      "Epoch 16/300\n",
      "Average training loss: 0.052598316695955064\n",
      "Average test loss: 0.0034784184787422416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05206057405471802\n",
      "Average test loss: 0.0033793909906720122\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05160249535242716\n",
      "Average test loss: 0.0033419114467170504\n",
      "Epoch 19/300\n",
      "Average training loss: 0.051083719902568395\n",
      "Average test loss: 0.003364846979578336\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05063042146298621\n",
      "Average test loss: 0.0032825613025989795\n",
      "Epoch 21/300\n",
      "Average training loss: 0.050274312853813174\n",
      "Average test loss: 0.003297736045387056\n",
      "Epoch 22/300\n",
      "Average training loss: 0.049869384876555865\n",
      "Average test loss: 0.0033109809677633975\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04945837728182475\n",
      "Average test loss: 0.003244535546335909\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04913809636897511\n",
      "Average test loss: 0.003229360990019308\n",
      "Epoch 25/300\n",
      "Average training loss: 0.048848794577850234\n",
      "Average test loss: 0.003183184383230077\n",
      "Epoch 26/300\n",
      "Average training loss: 0.048544973336988026\n",
      "Average test loss: 0.003213265351744162\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04819974451594883\n",
      "Average test loss: 0.003141356567127837\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04796155056357384\n",
      "Average test loss: 0.003139511814340949\n",
      "Epoch 29/300\n",
      "Average training loss: 0.047722934663295746\n",
      "Average test loss: 0.0031573547989957864\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04753652177916633\n",
      "Average test loss: 0.003117025220559703\n",
      "Epoch 31/300\n",
      "Average training loss: 0.047294417844878305\n",
      "Average test loss: 0.0031632278493295115\n",
      "Epoch 32/300\n",
      "Average training loss: 0.047109337511989806\n",
      "Average test loss: 0.0030738105562825996\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04691702943543593\n",
      "Average test loss: 0.0030809198593099913\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04672486479414834\n",
      "Average test loss: 0.0030600311213897333\n",
      "Epoch 35/300\n",
      "Average training loss: 0.046550517347123885\n",
      "Average test loss: 0.003064495989224977\n",
      "Epoch 36/300\n",
      "Average training loss: 0.046374561058150396\n",
      "Average test loss: 0.0030721528213471174\n",
      "Epoch 37/300\n",
      "Average training loss: 0.046229884687397214\n",
      "Average test loss: 0.0030313278113802272\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0460823266837332\n",
      "Average test loss: 0.00304632981100844\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04597167571220133\n",
      "Average test loss: 0.003031656115626295\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0458186938199732\n",
      "Average test loss: 0.0030347331774731477\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04568088678187794\n",
      "Average test loss: 0.0030280293934047222\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04561151578691271\n",
      "Average test loss: 0.0030019699330959054\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04546542368001408\n",
      "Average test loss: 0.002994673709695538\n",
      "Epoch 44/300\n",
      "Average training loss: 0.045340450829929775\n",
      "Average test loss: 0.0030314637016918924\n",
      "Epoch 45/300\n",
      "Average training loss: 0.045233886573049754\n",
      "Average test loss: 0.002995158716208405\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04510835698578093\n",
      "Average test loss: 0.002988100609431664\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04503637768493758\n",
      "Average test loss: 0.0030079797719501787\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04490335233343972\n",
      "Average test loss: 0.0029799987816562256\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04484793626930979\n",
      "Average test loss: 0.002977171007543802\n",
      "Epoch 50/300\n",
      "Average training loss: 0.044716065347194675\n",
      "Average test loss: 0.002959366898983717\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04465268505944146\n",
      "Average test loss: 0.0029618948366906908\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04456569237013658\n",
      "Average test loss: 0.00298145443904731\n",
      "Epoch 53/300\n",
      "Average training loss: 0.044473745945427155\n",
      "Average test loss: 0.002969610725219051\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04440858722395367\n",
      "Average test loss: 0.0029535401513179145\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04429273981849353\n",
      "Average test loss: 0.003014436161145568\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04424458394944668\n",
      "Average test loss: 0.0029678502027980155\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04412403793964121\n",
      "Average test loss: 0.0029829287636611197\n",
      "Epoch 58/300\n",
      "Average training loss: 0.044093415973914994\n",
      "Average test loss: 0.002963378035566873\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0439861784113778\n",
      "Average test loss: 0.0029536510482430457\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04391644466254446\n",
      "Average test loss: 0.0029808764699846505\n",
      "Epoch 61/300\n",
      "Average training loss: 0.043833432366450625\n",
      "Average test loss: 0.0029479947723448276\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04376578541265594\n",
      "Average test loss: 0.002943215595972207\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04367921103702651\n",
      "Average test loss: 0.002989159311271376\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04363953658276134\n",
      "Average test loss: 0.0029392492576605744\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04352174024780591\n",
      "Average test loss: 0.0029339384575270944\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04343364081117842\n",
      "Average test loss: 0.002929456447147661\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04340465118487676\n",
      "Average test loss: 0.002947858874582582\n",
      "Epoch 68/300\n",
      "Average training loss: 0.043324081311623255\n",
      "Average test loss: 0.0029725406140916877\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04326653506689601\n",
      "Average test loss: 0.002959467522903449\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04324965994556745\n",
      "Average test loss: 0.0029557110615488556\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0431109084851212\n",
      "Average test loss: 0.002972852872063716\n",
      "Epoch 72/300\n",
      "Average training loss: 0.043062355948819055\n",
      "Average test loss: 0.002940843471429414\n",
      "Epoch 73/300\n",
      "Average training loss: 0.043018807768821714\n",
      "Average test loss: 0.0029300274470200143\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0429011772920688\n",
      "Average test loss: 0.002933731548074219\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04283738713463148\n",
      "Average test loss: 0.0029373733864890205\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0428076132585605\n",
      "Average test loss: 0.0029986501679652266\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0427348111834791\n",
      "Average test loss: 0.002937640963950091\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04267168253329065\n",
      "Average test loss: 0.0029582956389834483\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04260395259824064\n",
      "Average test loss: 0.0029883626451094945\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0425770785941018\n",
      "Average test loss: 0.0029606201601111225\n",
      "Epoch 81/300\n",
      "Average training loss: 0.042458334346612295\n",
      "Average test loss: 0.002934058803651068\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04249756178922123\n",
      "Average test loss: 0.002942882263825999\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04234522117508782\n",
      "Average test loss: 0.0029564631819311116\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04230363718337483\n",
      "Average test loss: 0.0029149554843703905\n",
      "Epoch 85/300\n",
      "Average training loss: 0.042226354873842666\n",
      "Average test loss: 0.002936219782878955\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04214148324396875\n",
      "Average test loss: 0.0029422131296661163\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04217070032656193\n",
      "Average test loss: 0.002925216343667772\n",
      "Epoch 88/300\n",
      "Average training loss: 0.042059928718540404\n",
      "Average test loss: 0.002932547628879547\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04202585612071885\n",
      "Average test loss: 0.0029485983738882673\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04196483569675022\n",
      "Average test loss: 0.0029316331248523462\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04194129655427403\n",
      "Average test loss: 0.002944029836398032\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04186268110076587\n",
      "Average test loss: 0.002981011593507396\n",
      "Epoch 93/300\n",
      "Average training loss: 0.041814978526698215\n",
      "Average test loss: 0.002924940768422352\n",
      "Epoch 94/300\n",
      "Average training loss: 0.041730314082569546\n",
      "Average test loss: 0.0030248409431013795\n",
      "Epoch 95/300\n",
      "Average training loss: 0.041687248975038525\n",
      "Average test loss: 0.0029556174907419417\n",
      "Epoch 96/300\n",
      "Average training loss: 0.041629472159677085\n",
      "Average test loss: 0.002922990812195672\n",
      "Epoch 97/300\n",
      "Average training loss: 0.041580063492059706\n",
      "Average test loss: 0.0029689325886882016\n",
      "Epoch 98/300\n",
      "Average training loss: 0.041514928013086316\n",
      "Average test loss: 0.002939495198842552\n",
      "Epoch 99/300\n",
      "Average training loss: 0.041458622886074915\n",
      "Average test loss: 0.0029371838766253655\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04140302742852105\n",
      "Average test loss: 0.0029366624866508776\n",
      "Epoch 101/300\n",
      "Average training loss: 0.041352360109488166\n",
      "Average test loss: 0.002944373254974683\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04132689957486259\n",
      "Average test loss: 0.002941181362917026\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04120731963713964\n",
      "Average test loss: 0.002920563178964787\n",
      "Epoch 104/300\n",
      "Average training loss: 0.041185964590973324\n",
      "Average test loss: 0.002952049161824915\n",
      "Epoch 105/300\n",
      "Average training loss: 0.041164037307103475\n",
      "Average test loss: 0.0029677592085467445\n",
      "Epoch 106/300\n",
      "Average training loss: 0.041115362799829906\n",
      "Average test loss: 0.002970421168125338\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04103135147028499\n",
      "Average test loss: 0.002948846065128843\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04095855078101158\n",
      "Average test loss: 0.0029599799466215903\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04098002531793382\n",
      "Average test loss: 0.0029674186611341104\n",
      "Epoch 110/300\n",
      "Average training loss: 0.040918678194284436\n",
      "Average test loss: 0.0029460337911215093\n",
      "Epoch 111/300\n",
      "Average training loss: 0.040842939012580445\n",
      "Average test loss: 0.0031389854666259553\n",
      "Epoch 112/300\n",
      "Average training loss: 0.040802640871869196\n",
      "Average test loss: 0.002962723366709219\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04078544204930464\n",
      "Average test loss: 0.0029599272741211787\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04071656693187025\n",
      "Average test loss: 0.0029774262646420136\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0406825266679128\n",
      "Average test loss: 0.0029455530409597686\n",
      "Epoch 116/300\n",
      "Average training loss: 0.040616005001796615\n",
      "Average test loss: 0.0029638115469780234\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04057674725353718\n",
      "Average test loss: 0.0030083899088203907\n",
      "Epoch 118/300\n",
      "Average training loss: 0.040521151194969816\n",
      "Average test loss: 0.0029705589132176507\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04049316086702877\n",
      "Average test loss: 0.0029604162250955897\n",
      "Epoch 120/300\n",
      "Average training loss: 0.040475650459527966\n",
      "Average test loss: 0.0029739170329024394\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04039491878449917\n",
      "Average test loss: 0.0030150665669805474\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0403463920156161\n",
      "Average test loss: 0.002977024241661032\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04029544966750675\n",
      "Average test loss: 0.0030168169401586058\n",
      "Epoch 124/300\n",
      "Average training loss: 0.040279771188894906\n",
      "Average test loss: 0.0029702471757514608\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04023916368020906\n",
      "Average test loss: 0.003048117488415705\n",
      "Epoch 126/300\n",
      "Average training loss: 0.040175880468553964\n",
      "Average test loss: 0.0029738730281177496\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04014999021424188\n",
      "Average test loss: 0.0029720554546349576\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0400865864339802\n",
      "Average test loss: 0.003001008276724153\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04006538133819898\n",
      "Average test loss: 0.002963930405469404\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04002624206079377\n",
      "Average test loss: 0.0029583134899536768\n",
      "Epoch 131/300\n",
      "Average training loss: 0.039979213429821865\n",
      "Average test loss: 0.002975163130917483\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03994911813404825\n",
      "Average test loss: 0.002985768838889069\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03990281629231241\n",
      "Average test loss: 0.003070510980983575\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03985069023734993\n",
      "Average test loss: 0.0029940281367550293\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03980444953507847\n",
      "Average test loss: 0.0029786723296468458\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03975283520917098\n",
      "Average test loss: 0.002996362751142846\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03977180221345689\n",
      "Average test loss: 0.003000092039919562\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0396898924642139\n",
      "Average test loss: 0.0030132797157598865\n",
      "Epoch 139/300\n",
      "Average training loss: 0.039658745792176986\n",
      "Average test loss: 0.003036262981283168\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03965826145807902\n",
      "Average test loss: 0.002996134608777033\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03957322538892428\n",
      "Average test loss: 0.0029684437103569506\n",
      "Epoch 142/300\n",
      "Average training loss: 0.039567022615008884\n",
      "Average test loss: 0.003036712118114034\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03953093882401784\n",
      "Average test loss: 0.003026020715220107\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03946537226769659\n",
      "Average test loss: 0.0031659956090152263\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03946557649307781\n",
      "Average test loss: 0.0030317271711925668\n",
      "Epoch 146/300\n",
      "Average training loss: 0.039398703628116184\n",
      "Average test loss: 0.0030098503122313156\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03936106935640176\n",
      "Average test loss: 0.0030528202950954437\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03934115198585722\n",
      "Average test loss: 0.00299395287844042\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0393181388411257\n",
      "Average test loss: 0.0029861156776961354\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03924459221131272\n",
      "Average test loss: 0.0030543738123443393\n",
      "Epoch 151/300\n",
      "Average training loss: 0.039239046668012936\n",
      "Average test loss: 0.0030570999833030832\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03922002817028099\n",
      "Average test loss: 0.003009903157957726\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03913273410002391\n",
      "Average test loss: 0.0030411064895904728\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03915106714102957\n",
      "Average test loss: 0.0030227604189680683\n",
      "Epoch 155/300\n",
      "Average training loss: 0.039109833851456645\n",
      "Average test loss: 0.0030044388017720644\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03913362774252892\n",
      "Average test loss: 0.0030790972415771748\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03904300563865238\n",
      "Average test loss: 0.003088773076215552\n",
      "Epoch 158/300\n",
      "Average training loss: 0.039014335681994756\n",
      "Average test loss: 0.003014447923335764\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03903952449891302\n",
      "Average test loss: 0.003025850770270659\n",
      "Epoch 160/300\n",
      "Average training loss: 0.038931731462478636\n",
      "Average test loss: 0.0030170343714869684\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03896677868564924\n",
      "Average test loss: 0.003038702153083351\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03892298493782679\n",
      "Average test loss: 0.00308921803554727\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03884963370694054\n",
      "Average test loss: 0.003066883571859863\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03888011551896731\n",
      "Average test loss: 0.0030376850565274555\n",
      "Epoch 165/300\n",
      "Average training loss: 0.038791929961906536\n",
      "Average test loss: 0.003056551008174817\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03878333828846613\n",
      "Average test loss: 0.0030355205428269174\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03874718208114306\n",
      "Average test loss: 0.003025519057073527\n",
      "Epoch 168/300\n",
      "Average training loss: 0.038725850413242974\n",
      "Average test loss: 0.003319413368486696\n",
      "Epoch 169/300\n",
      "Average training loss: 0.038732525817222065\n",
      "Average test loss: 0.0030466491302682296\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03868665523992645\n",
      "Average test loss: 0.0031281751489473713\n",
      "Epoch 171/300\n",
      "Average training loss: 0.038569614506430094\n",
      "Average test loss: 0.003011020950559113\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0385815020352602\n",
      "Average test loss: 0.0030577868794401486\n",
      "Epoch 173/300\n",
      "Average training loss: 0.038591718686951534\n",
      "Average test loss: 0.0030652491321994197\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03853427865107854\n",
      "Average test loss: 0.0030692810975015165\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03852557001180119\n",
      "Average test loss: 0.003037902137057649\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03854552586873372\n",
      "Average test loss: 0.0030662684508909782\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03849319733844863\n",
      "Average test loss: 0.003027581235393882\n",
      "Epoch 178/300\n",
      "Average training loss: 0.038453212956587476\n",
      "Average test loss: 0.0030181199630929362\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03843071895423863\n",
      "Average test loss: 0.0030863454987605414\n",
      "Epoch 180/300\n",
      "Average training loss: 0.038416381455130044\n",
      "Average test loss: 0.0030737818187723557\n",
      "Epoch 181/300\n",
      "Average training loss: 0.038378799882200026\n",
      "Average test loss: 0.0031067640371620657\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03833439828952154\n",
      "Average test loss: 0.003023369503724906\n",
      "Epoch 183/300\n",
      "Average training loss: 0.038299263447523116\n",
      "Average test loss: 0.003077093340870407\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03833052411841022\n",
      "Average test loss: 0.0030535364324847858\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03829213596880436\n",
      "Average test loss: 0.003050724741899305\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03823532829019759\n",
      "Average test loss: 0.0030740035886151922\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03821626502937741\n",
      "Average test loss: 0.003178030404365725\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03822853161560164\n",
      "Average test loss: 0.003023805923346016\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03822523264752494\n",
      "Average test loss: 0.0030700274941821894\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03813444974852933\n",
      "Average test loss: 0.003038183457321591\n",
      "Epoch 191/300\n",
      "Average training loss: 0.038093035244279434\n",
      "Average test loss: 0.0030674500800669195\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03815636110305786\n",
      "Average test loss: 0.003119834800147348\n",
      "Epoch 193/300\n",
      "Average training loss: 0.038063632759783\n",
      "Average test loss: 0.0031113071909381284\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03808603040046162\n",
      "Average test loss: 0.0030649221378068127\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0380750086804231\n",
      "Average test loss: 0.003036086136682166\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03801480384998852\n",
      "Average test loss: 0.0030699627188344797\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03800646283063624\n",
      "Average test loss: 0.0031112359311017725\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03799237980279658\n",
      "Average test loss: 0.003047911465167999\n",
      "Epoch 199/300\n",
      "Average training loss: 0.037955652124351924\n",
      "Average test loss: 0.0031097205740710098\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03789927369025019\n",
      "Average test loss: 0.00306608500600689\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0379756500158045\n",
      "Average test loss: 0.003077625742389096\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03789532306790352\n",
      "Average test loss: 0.003040858726017177\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03784657292895847\n",
      "Average test loss: 0.0030970544947518243\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03783486245406999\n",
      "Average test loss: 0.0030992948048644596\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0378386112915145\n",
      "Average test loss: 0.0031221180645128093\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03781131009260814\n",
      "Average test loss: 0.0030910079677899677\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03779779855741395\n",
      "Average test loss: 0.0030904153510928154\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03771629731522666\n",
      "Average test loss: 0.0030426479685637686\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03775901475217607\n",
      "Average test loss: 0.0030627821932236354\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03778713235590193\n",
      "Average test loss: 0.003094147536696659\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03769057885143492\n",
      "Average test loss: 0.003136332353783978\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03783538717528184\n",
      "Average test loss: 0.0030636780915988815\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03771123017205132\n",
      "Average test loss: 0.00302744188801282\n",
      "Epoch 214/300\n",
      "Average training loss: 0.037632113426923755\n",
      "Average test loss: 0.0030957095900343524\n",
      "Epoch 215/300\n",
      "Average training loss: 0.037666087713506484\n",
      "Average test loss: 0.003118664091039035\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03762842701044348\n",
      "Average test loss: 0.003267941392751204\n",
      "Epoch 217/300\n",
      "Average training loss: 0.037615673783752655\n",
      "Average test loss: 0.003185768083979686\n",
      "Epoch 218/300\n",
      "Average training loss: 0.037597530980904896\n",
      "Average test loss: 0.0030573573247012166\n",
      "Epoch 219/300\n",
      "Average training loss: 0.037541861838764616\n",
      "Average test loss: 0.0030652397784094016\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03753860808743371\n",
      "Average test loss: 0.0030421366455654305\n",
      "Epoch 221/300\n",
      "Average training loss: 0.037532719045877454\n",
      "Average test loss: 0.003120609771874216\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0375248148838679\n",
      "Average test loss: 0.003085187480267551\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03749936780664656\n",
      "Average test loss: 0.0031394400348265968\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03748788907130559\n",
      "Average test loss: 0.003238354509489404\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03752949476242065\n",
      "Average test loss: 0.00313008820803629\n",
      "Epoch 226/300\n",
      "Average training loss: 0.037474845442507\n",
      "Average test loss: 0.0032357179924017852\n",
      "Epoch 227/300\n",
      "Average training loss: 0.037476562052965165\n",
      "Average test loss: 0.0030932059989621243\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03734151357412338\n",
      "Average test loss: 0.0030479926167883806\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03740668790870243\n",
      "Average test loss: 0.0033311562416040233\n",
      "Epoch 230/300\n",
      "Average training loss: 0.037385034412145614\n",
      "Average test loss: 0.0030698383990675213\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03739157672723134\n",
      "Average test loss: 0.0030817017580072087\n",
      "Epoch 232/300\n",
      "Average training loss: 0.037328418529695935\n",
      "Average test loss: 0.0030718046105984186\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03731857421994209\n",
      "Average test loss: 0.003149131800979376\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03737702380120754\n",
      "Average test loss: 0.003187399570312765\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03731018639935388\n",
      "Average test loss: 0.0031377829561630884\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0372864181333118\n",
      "Average test loss: 0.0031811959749708575\n",
      "Epoch 237/300\n",
      "Average training loss: 0.037252372903956305\n",
      "Average test loss: 0.0030506637829045453\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03727976717882686\n",
      "Average test loss: 0.0030783632470087873\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03724486477838622\n",
      "Average test loss: 0.0031083372425701884\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0371999421649509\n",
      "Average test loss: 0.003112282348796725\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03720963877108362\n",
      "Average test loss: 0.003135542806237936\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03722784075803227\n",
      "Average test loss: 0.0031146241076704527\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03720947229199939\n",
      "Average test loss: 0.0030899760054631364\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03710546378791332\n",
      "Average test loss: 0.0031826209504571224\n",
      "Epoch 245/300\n",
      "Average training loss: 0.037146413505077365\n",
      "Average test loss: 0.0030532788704666827\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03712381162246068\n",
      "Average test loss: 0.003158427821058366\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03707386597328716\n",
      "Average test loss: 0.003084434243539969\n",
      "Epoch 248/300\n",
      "Average training loss: 0.037119825876421396\n",
      "Average test loss: 0.003077967064869073\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03703635872734917\n",
      "Average test loss: 0.003204682775048746\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03710289255115721\n",
      "Average test loss: 0.003105634714787205\n",
      "Epoch 251/300\n",
      "Average training loss: 0.037039091100295386\n",
      "Average test loss: 0.003107077950818671\n",
      "Epoch 252/300\n",
      "Average training loss: 0.037022315684292054\n",
      "Average test loss: 0.003082750417292118\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03703748100333744\n",
      "Average test loss: 0.0032306442372500898\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03701495090126991\n",
      "Average test loss: 0.00305855689673788\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03698522977696525\n",
      "Average test loss: 0.0030773170282029443\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0369583723164267\n",
      "Average test loss: 0.003064676012636887\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03695227653119299\n",
      "Average test loss: 0.0030782967218094402\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03699767127301958\n",
      "Average test loss: 0.0030977024665723245\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03692739582724042\n",
      "Average test loss: 0.0031529092109865613\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03694839419921239\n",
      "Average test loss: 0.0031240861099213362\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03694776720801989\n",
      "Average test loss: 0.0031120226790921557\n",
      "Epoch 262/300\n",
      "Average training loss: 0.036901722836825586\n",
      "Average test loss: 0.003190660696062777\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03696068671676848\n",
      "Average test loss: 0.003121202010454403\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03690025389194489\n",
      "Average test loss: 0.003166653013477723\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03686007328993744\n",
      "Average test loss: 0.003149714190719856\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03691064022481441\n",
      "Average test loss: 0.0032737248805868955\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03682027648886045\n",
      "Average test loss: 0.003117699216844307\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03681889988481998\n",
      "Average test loss: 0.0031258489491624965\n",
      "Epoch 269/300\n",
      "Average training loss: 0.036810624592834046\n",
      "Average test loss: 0.0031274567871458\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03681839583648576\n",
      "Average test loss: 0.0031151647569818627\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03678583769665824\n",
      "Average test loss: 0.0031056763811243907\n",
      "Epoch 272/300\n",
      "Average training loss: 0.036769889957375\n",
      "Average test loss: 0.003114101212264763\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03673459939161936\n",
      "Average test loss: 0.0031395596981876428\n",
      "Epoch 274/300\n",
      "Average training loss: 0.036752426458729635\n",
      "Average test loss: 0.003174092436209321\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03671515247888035\n",
      "Average test loss: 0.00316686938661668\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03669825482699606\n",
      "Average test loss: 0.0031930791516270904\n",
      "Epoch 277/300\n",
      "Average training loss: 0.036776843865712484\n",
      "Average test loss: 0.0031114846147182913\n",
      "Epoch 278/300\n",
      "Average training loss: 0.036736941284603544\n",
      "Average test loss: 0.0031152832694351673\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03668405399719874\n",
      "Average test loss: 0.003139390534410874\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0366741928656896\n",
      "Average test loss: 0.003128505465057161\n",
      "Epoch 281/300\n",
      "Average training loss: 0.036641526301701866\n",
      "Average test loss: 0.0031637292626417344\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03672436929742495\n",
      "Average test loss: 0.003123047624197271\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03662107171946102\n",
      "Average test loss: 0.0031279797525041635\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03665515471167034\n",
      "Average test loss: 0.0031195566283745897\n",
      "Epoch 285/300\n",
      "Average training loss: 0.036634529638621544\n",
      "Average test loss: 0.003089762338747581\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03665526125166151\n",
      "Average test loss: 0.0033040459238820605\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03667069823543231\n",
      "Average test loss: 0.0031789359994646577\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03658666550450855\n",
      "Average test loss: 0.0031271407200644413\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03652809614605374\n",
      "Average test loss: 0.003159786452021864\n",
      "Epoch 290/300\n",
      "Average training loss: 0.036570232994026605\n",
      "Average test loss: 0.0031438864877240524\n",
      "Epoch 291/300\n",
      "Average training loss: 0.036531583838992646\n",
      "Average test loss: 0.003165022670291364\n",
      "Epoch 292/300\n",
      "Average training loss: 0.036507984101772306\n",
      "Average test loss: 0.0032307977109319636\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03652456667853726\n",
      "Average test loss: 0.0031411561293320524\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03654028160042233\n",
      "Average test loss: 0.003110683280146784\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03646414787736204\n",
      "Average test loss: 0.0031556065808981655\n",
      "Epoch 296/300\n",
      "Average training loss: 0.036572576969861985\n",
      "Average test loss: 0.0031175332297053602\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03650854754779074\n",
      "Average test loss: 0.003221341015977992\n",
      "Epoch 298/300\n",
      "Average training loss: 0.036553565172685515\n",
      "Average test loss: 0.003149676510029369\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03645586280028025\n",
      "Average test loss: 0.0031872165478352044\n",
      "Epoch 300/300\n",
      "Average training loss: 0.036452131826016636\n",
      "Average test loss: 0.0031242515521330964\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7195078024996652\n",
      "Average test loss: 0.004903418793032566\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09053001676665412\n",
      "Average test loss: 0.00414638083986938\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07120533981257014\n",
      "Average test loss: 0.004022954731972681\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06332203810744816\n",
      "Average test loss: 0.003633830019583305\n",
      "Epoch 5/300\n",
      "Average training loss: 0.058609654035833146\n",
      "Average test loss: 0.003646730114188459\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05539853611919615\n",
      "Average test loss: 0.0033884239296118417\n",
      "Epoch 7/300\n",
      "Average training loss: 0.052978380772802564\n",
      "Average test loss: 0.003413744259418713\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05102605797847112\n",
      "Average test loss: 0.0032584001852406395\n",
      "Epoch 9/300\n",
      "Average training loss: 0.049442105329698983\n",
      "Average test loss: 0.0031166225179202027\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04799548847311073\n",
      "Average test loss: 0.0030352408604489432\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0467428799006674\n",
      "Average test loss: 0.0029573493788225785\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04562636307875315\n",
      "Average test loss: 0.0030227581999368136\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04471421930856175\n",
      "Average test loss: 0.0028955732223888237\n",
      "Epoch 14/300\n",
      "Average training loss: 0.043763714141315885\n",
      "Average test loss: 0.0027649566961659325\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04289911841021644\n",
      "Average test loss: 0.002715247686538431\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04210364119874106\n",
      "Average test loss: 0.002689401105253233\n",
      "Epoch 17/300\n",
      "Average training loss: 0.041380608696076605\n",
      "Average test loss: 0.0026848335516535572\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0407650618188911\n",
      "Average test loss: 0.0026637592210123936\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04017064892252286\n",
      "Average test loss: 0.002551620228629973\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03971449850002925\n",
      "Average test loss: 0.002488609404199653\n",
      "Epoch 21/300\n",
      "Average training loss: 0.039181097898218366\n",
      "Average test loss: 0.002488612883620792\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0387145464056068\n",
      "Average test loss: 0.0024527675376998055\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03836380916502741\n",
      "Average test loss: 0.0024533552637634177\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03801515193035205\n",
      "Average test loss: 0.0024174654005716246\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03761493917306264\n",
      "Average test loss: 0.002391236113384366\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03734086876776483\n",
      "Average test loss: 0.002375063935294747\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03699610341257519\n",
      "Average test loss: 0.0023475943284316197\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03669420289331012\n",
      "Average test loss: 0.0023384746139248212\n",
      "Epoch 29/300\n",
      "Average training loss: 0.036463876510659854\n",
      "Average test loss: 0.002316933382716444\n",
      "Epoch 30/300\n",
      "Average training loss: 0.036222361584504446\n",
      "Average test loss: 0.0023610621198183962\n",
      "Epoch 31/300\n",
      "Average training loss: 0.036077435423930486\n",
      "Average test loss: 0.002362813280481431\n",
      "Epoch 32/300\n",
      "Average training loss: 0.035845800136526426\n",
      "Average test loss: 0.002288690936389483\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03562719908853372\n",
      "Average test loss: 0.0022919284658403033\n",
      "Epoch 34/300\n",
      "Average training loss: 0.035430146747165256\n",
      "Average test loss: 0.002264287788213955\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03528935787743992\n",
      "Average test loss: 0.0022703360478497215\n",
      "Epoch 36/300\n",
      "Average training loss: 0.035132449310686854\n",
      "Average test loss: 0.0022429673495805926\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03494487993750307\n",
      "Average test loss: 0.002228381924331188\n",
      "Epoch 38/300\n",
      "Average training loss: 0.034828236230545574\n",
      "Average test loss: 0.0022263188128256134\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03468315836952792\n",
      "Average test loss: 0.0022323197544448906\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03456523569590515\n",
      "Average test loss: 0.002247217498305771\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03446330513060093\n",
      "Average test loss: 0.0021989619767086373\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03430308597617679\n",
      "Average test loss: 0.0022188616548147466\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03420697849657801\n",
      "Average test loss: 0.002244429536681208\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03415154112544325\n",
      "Average test loss: 0.002212689892699321\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03401083107789357\n",
      "Average test loss: 0.0022022143835201857\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03388387973606587\n",
      "Average test loss: 0.0021723373177357846\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03380260783930619\n",
      "Average test loss: 0.002196052267630067\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03368675866557492\n",
      "Average test loss: 0.0021812599485533106\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03358676856425073\n",
      "Average test loss: 0.002174401048881312\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03355148085123963\n",
      "Average test loss: 0.0021705512965304982\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03348506312403414\n",
      "Average test loss: 0.002172647214391165\n",
      "Epoch 52/300\n",
      "Average training loss: 0.033320637840363716\n",
      "Average test loss: 0.0022122743810630507\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03326607933971617\n",
      "Average test loss: 0.0021692763009212085\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03317676352792316\n",
      "Average test loss: 0.002154025975614786\n",
      "Epoch 55/300\n",
      "Average training loss: 0.033119795127047436\n",
      "Average test loss: 0.002157001426960859\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03304951680534416\n",
      "Average test loss: 0.0021695988608731163\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03298207565976514\n",
      "Average test loss: 0.00215515928549899\n",
      "Epoch 58/300\n",
      "Average training loss: 0.032862743685642876\n",
      "Average test loss: 0.002145818919978208\n",
      "Epoch 59/300\n",
      "Average training loss: 0.032778843498892256\n",
      "Average test loss: 0.002157312072813511\n",
      "Epoch 60/300\n",
      "Average training loss: 0.032758719945947326\n",
      "Average test loss: 0.0021551907190846074\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03265448250704341\n",
      "Average test loss: 0.0021432397752586337\n",
      "Epoch 62/300\n",
      "Average training loss: 0.032613107409742145\n",
      "Average test loss: 0.002142085499026709\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03250892588165071\n",
      "Average test loss: 0.002154340859399074\n",
      "Epoch 64/300\n",
      "Average training loss: 0.032454248044225906\n",
      "Average test loss: 0.00215288827713165\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03241845239036613\n",
      "Average test loss: 0.002144545623825656\n",
      "Epoch 66/300\n",
      "Average training loss: 0.032315877781973944\n",
      "Average test loss: 0.0021424695998430252\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03231094925933414\n",
      "Average test loss: 0.0021317000803020266\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03221078530119525\n",
      "Average test loss: 0.002150685271869103\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0321226556085878\n",
      "Average test loss: 0.0021408875078583758\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03206852754122681\n",
      "Average test loss: 0.002121906972800692\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0320295720514324\n",
      "Average test loss: 0.002137369150088893\n",
      "Epoch 72/300\n",
      "Average training loss: 0.031971665619148146\n",
      "Average test loss: 0.0021320487674739625\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03192575010491742\n",
      "Average test loss: 0.0021448108673923546\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03183357904354731\n",
      "Average test loss: 0.0021754512952433694\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03176645635233985\n",
      "Average test loss: 0.0021299679438687032\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03173740310139126\n",
      "Average test loss: 0.002136005785316229\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03172485686838627\n",
      "Average test loss: 0.0021757245446658796\n",
      "Epoch 78/300\n",
      "Average training loss: 0.031627751795781985\n",
      "Average test loss: 0.002139772856608033\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03152783011727863\n",
      "Average test loss: 0.002199519938064946\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03151979148056772\n",
      "Average test loss: 0.002130837407273551\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03146761136915949\n",
      "Average test loss: 0.002143889114881555\n",
      "Epoch 82/300\n",
      "Average training loss: 0.031402436449295947\n",
      "Average test loss: 0.0021713281840913825\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03134067887730069\n",
      "Average test loss: 0.0021269489319788084\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03132207609713077\n",
      "Average test loss: 0.002143105331187447\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03130828149947855\n",
      "Average test loss: 0.0021719586838864617\n",
      "Epoch 86/300\n",
      "Average training loss: 0.031168609869148996\n",
      "Average test loss: 0.002158636337146163\n",
      "Epoch 87/300\n",
      "Average training loss: 0.031121823984715673\n",
      "Average test loss: 0.002147721208476772\n",
      "Epoch 88/300\n",
      "Average training loss: 0.031107249170541765\n",
      "Average test loss: 0.0021294545876897044\n",
      "Epoch 89/300\n",
      "Average training loss: 0.031009853233893714\n",
      "Average test loss: 0.002127313710542189\n",
      "Epoch 90/300\n",
      "Average training loss: 0.031002078051368395\n",
      "Average test loss: 0.00214303051152577\n",
      "Epoch 91/300\n",
      "Average training loss: 0.030975852626893254\n",
      "Average test loss: 0.0021331624491140247\n",
      "Epoch 92/300\n",
      "Average training loss: 0.030893023586935468\n",
      "Average test loss: 0.0021431799911790423\n",
      "Epoch 93/300\n",
      "Average training loss: 0.030832267277770574\n",
      "Average test loss: 0.0021385814318847325\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03080806626048353\n",
      "Average test loss: 0.002131887040204472\n",
      "Epoch 95/300\n",
      "Average training loss: 0.030769265421562726\n",
      "Average test loss: 0.002141634052101937\n",
      "Epoch 96/300\n",
      "Average training loss: 0.030722214660710758\n",
      "Average test loss: 0.0021610327278160386\n",
      "Epoch 97/300\n",
      "Average training loss: 0.030665285194913548\n",
      "Average test loss: 0.0021405756974903246\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0306133507821295\n",
      "Average test loss: 0.0021702056391578583\n",
      "Epoch 99/300\n",
      "Average training loss: 0.030584113811453182\n",
      "Average test loss: 0.0021418757072339456\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03058170242773162\n",
      "Average test loss: 0.0021561435950506066\n",
      "Epoch 101/300\n",
      "Average training loss: 0.030486575331952835\n",
      "Average test loss: 0.0021471282442410786\n",
      "Epoch 102/300\n",
      "Average training loss: 0.030454214034809007\n",
      "Average test loss: 0.002176905729704433\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03045093605750137\n",
      "Average test loss: 0.0021555983734627563\n",
      "Epoch 104/300\n",
      "Average training loss: 0.030351300466391774\n",
      "Average test loss: 0.0021468160700880818\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030315327846341663\n",
      "Average test loss: 0.00218144307544248\n",
      "Epoch 106/300\n",
      "Average training loss: 0.030322173260980183\n",
      "Average test loss: 0.002416568333283067\n",
      "Epoch 107/300\n",
      "Average training loss: 0.030238611976305643\n",
      "Average test loss: 0.002173116581204037\n",
      "Epoch 108/300\n",
      "Average training loss: 0.030208865255117415\n",
      "Average test loss: 0.002167949707350797\n",
      "Epoch 109/300\n",
      "Average training loss: 0.030163522738549445\n",
      "Average test loss: 0.002141134331209792\n",
      "Epoch 110/300\n",
      "Average training loss: 0.030123207388652696\n",
      "Average test loss: 0.002154357595783141\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030063432266314825\n",
      "Average test loss: 0.0021528692876713143\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03005830868913068\n",
      "Average test loss: 0.0021512387715693975\n",
      "Epoch 113/300\n",
      "Average training loss: 0.030000307894415327\n",
      "Average test loss: 0.0021592915207147598\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02996379782093896\n",
      "Average test loss: 0.0021373305215189853\n",
      "Epoch 115/300\n",
      "Average training loss: 0.029940405739678277\n",
      "Average test loss: 0.0021444557004918654\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02992102166844739\n",
      "Average test loss: 0.0021660228286766343\n",
      "Epoch 117/300\n",
      "Average training loss: 0.029871009490556186\n",
      "Average test loss: 0.0021713757229348024\n",
      "Epoch 118/300\n",
      "Average training loss: 0.029802452385425567\n",
      "Average test loss: 0.002147850407080518\n",
      "Epoch 119/300\n",
      "Average training loss: 0.029768365018897586\n",
      "Average test loss: 0.0021596621268739303\n",
      "Epoch 120/300\n",
      "Average training loss: 0.029746568626827664\n",
      "Average test loss: 0.002186550098160903\n",
      "Epoch 121/300\n",
      "Average training loss: 0.029702370117108027\n",
      "Average test loss: 0.0021583986587615477\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02968188459508949\n",
      "Average test loss: 0.0022085708950956664\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02964419680999385\n",
      "Average test loss: 0.0021560576061407724\n",
      "Epoch 124/300\n",
      "Average training loss: 0.029575311977002357\n",
      "Average test loss: 0.0022225252717940343\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02960270486937629\n",
      "Average test loss: 0.002170031020210849\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02952752419643932\n",
      "Average test loss: 0.0021761647988524702\n",
      "Epoch 127/300\n",
      "Average training loss: 0.029495324143105083\n",
      "Average test loss: 0.002172353163154589\n",
      "Epoch 128/300\n",
      "Average training loss: 0.029482401253448594\n",
      "Average test loss: 0.0021690818348692525\n",
      "Epoch 129/300\n",
      "Average training loss: 0.029405641651815837\n",
      "Average test loss: 0.0022061388745076124\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02938349030415217\n",
      "Average test loss: 0.00230687483607067\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02938309230075942\n",
      "Average test loss: 0.002213839022235738\n",
      "Epoch 132/300\n",
      "Average training loss: 0.029333140182826255\n",
      "Average test loss: 0.002196535473482476\n",
      "Epoch 133/300\n",
      "Average training loss: 0.029320111953549916\n",
      "Average test loss: 0.0021860870735512837\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02929085693591171\n",
      "Average test loss: 0.0021608237179203166\n",
      "Epoch 135/300\n",
      "Average training loss: 0.029245911222365167\n",
      "Average test loss: 0.0023137387389110193\n",
      "Epoch 136/300\n",
      "Average training loss: 0.029208953503105376\n",
      "Average test loss: 0.002187845583591196\n",
      "Epoch 137/300\n",
      "Average training loss: 0.029170213426152863\n",
      "Average test loss: 0.002169196836546891\n",
      "Epoch 138/300\n",
      "Average training loss: 0.029131944969296455\n",
      "Average test loss: 0.002167729747895565\n",
      "Epoch 139/300\n",
      "Average training loss: 0.029105083917578062\n",
      "Average test loss: 0.0022052212957706717\n",
      "Epoch 140/300\n",
      "Average training loss: 0.029101249876949523\n",
      "Average test loss: 0.0021662627214358915\n",
      "Epoch 141/300\n",
      "Average training loss: 0.029070269046558274\n",
      "Average test loss: 0.002242936857044697\n",
      "Epoch 142/300\n",
      "Average training loss: 0.029036089423629972\n",
      "Average test loss: 0.002154213165760868\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02899429161681069\n",
      "Average test loss: 0.0021668162384173937\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02898703898820612\n",
      "Average test loss: 0.002166677562830349\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028948007219367557\n",
      "Average test loss: 0.002191536655442582\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02890911806623141\n",
      "Average test loss: 0.002192911331438356\n",
      "Epoch 147/300\n",
      "Average training loss: 0.028841580975386832\n",
      "Average test loss: 0.0021720565319475202\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02882411960595184\n",
      "Average test loss: 0.0021675095483660698\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02883617542352941\n",
      "Average test loss: 0.002177438636206918\n",
      "Epoch 150/300\n",
      "Average training loss: 0.028825018149283198\n",
      "Average test loss: 0.002203510468204816\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02874744468761815\n",
      "Average test loss: 0.0022122955732047558\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02872985699772835\n",
      "Average test loss: 0.0021534653455019\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02872388268344932\n",
      "Average test loss: 0.0021966070234775543\n",
      "Epoch 154/300\n",
      "Average training loss: 0.028735632965962092\n",
      "Average test loss: 0.0022030185622473556\n",
      "Epoch 155/300\n",
      "Average training loss: 0.028686110532946056\n",
      "Average test loss: 0.0022083660006109213\n",
      "Epoch 156/300\n",
      "Average training loss: 0.028626358497473928\n",
      "Average test loss: 0.002172039831471112\n",
      "Epoch 157/300\n",
      "Average training loss: 0.028662452098396088\n",
      "Average test loss: 0.0022024082909855576\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028576972120338016\n",
      "Average test loss: 0.002211726050823927\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02856927405628893\n",
      "Average test loss: 0.0022030819797267515\n",
      "Epoch 160/300\n",
      "Average training loss: 0.028504869712723625\n",
      "Average test loss: 0.002183873898660143\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02851568693998787\n",
      "Average test loss: 0.002220958236604929\n",
      "Epoch 162/300\n",
      "Average training loss: 0.028529667933781942\n",
      "Average test loss: 0.0022008375943534902\n",
      "Epoch 163/300\n",
      "Average training loss: 0.028461296930909156\n",
      "Average test loss: 0.002176555659311513\n",
      "Epoch 164/300\n",
      "Average training loss: 0.028439355125029882\n",
      "Average test loss: 0.0022107777196086116\n",
      "Epoch 165/300\n",
      "Average training loss: 0.028432522416114808\n",
      "Average test loss: 0.002193588705940379\n",
      "Epoch 166/300\n",
      "Average training loss: 0.028436002595557106\n",
      "Average test loss: 0.002250857199438744\n",
      "Epoch 167/300\n",
      "Average training loss: 0.028371168583631517\n",
      "Average test loss: 0.002223868131844534\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02830598273542192\n",
      "Average test loss: 0.002165729316261907\n",
      "Epoch 169/300\n",
      "Average training loss: 0.028346572279930114\n",
      "Average test loss: 0.0021788844538645612\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02829747886293464\n",
      "Average test loss: 0.00223619431650473\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02832574054929945\n",
      "Average test loss: 0.0022865585351569784\n",
      "Epoch 172/300\n",
      "Average training loss: 0.028298651804526648\n",
      "Average test loss: 0.0022292919756016797\n",
      "Epoch 173/300\n",
      "Average training loss: 0.028203520874182382\n",
      "Average test loss: 0.0022159518679190013\n",
      "Epoch 174/300\n",
      "Average training loss: 0.028233547169301244\n",
      "Average test loss: 0.0022159792446634838\n",
      "Epoch 175/300\n",
      "Average training loss: 0.028197567113571697\n",
      "Average test loss: 0.002215466081785659\n",
      "Epoch 176/300\n",
      "Average training loss: 0.028173596347371738\n",
      "Average test loss: 0.0022472806034816635\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02817273158662849\n",
      "Average test loss: 0.0022366103037363954\n",
      "Epoch 178/300\n",
      "Average training loss: 0.028138753554887243\n",
      "Average test loss: 0.0022324541016585297\n",
      "Epoch 179/300\n",
      "Average training loss: 0.028134888586070803\n",
      "Average test loss: 0.0023318294478166432\n",
      "Epoch 180/300\n",
      "Average training loss: 0.028102500254909197\n",
      "Average test loss: 0.002254025489712755\n",
      "Epoch 181/300\n",
      "Average training loss: 0.028079913155900107\n",
      "Average test loss: 0.00223681038349039\n",
      "Epoch 182/300\n",
      "Average training loss: 0.028042065269417233\n",
      "Average test loss: 0.0022218686220132644\n",
      "Epoch 183/300\n",
      "Average training loss: 0.028129672472675643\n",
      "Average test loss: 0.0022114026644784544\n",
      "Epoch 184/300\n",
      "Average training loss: 0.028012570506996577\n",
      "Average test loss: 0.002193117723696762\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02801822482711739\n",
      "Average test loss: 0.0022165670589440398\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02795488627751668\n",
      "Average test loss: 0.002205451362559365\n",
      "Epoch 187/300\n",
      "Average training loss: 0.027949788280659252\n",
      "Average test loss: 0.0022668490749266413\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02799312876827187\n",
      "Average test loss: 0.0022330923491261073\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02793124940660265\n",
      "Average test loss: 0.002242075217473838\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02791297173831198\n",
      "Average test loss: 0.00219658371185263\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027852748536401326\n",
      "Average test loss: 0.002264931874556674\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02788085846685701\n",
      "Average test loss: 0.0022648809634976917\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02783776247004668\n",
      "Average test loss: 0.0022272666369875273\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02786480021973451\n",
      "Average test loss: 0.002239993653363652\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02785784971879588\n",
      "Average test loss: 0.002230810081793202\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02776654504901833\n",
      "Average test loss: 0.0022482325453311203\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02777530277106497\n",
      "Average test loss: 0.0022223380414976013\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02772664450109005\n",
      "Average test loss: 0.0022333024468065964\n",
      "Epoch 199/300\n",
      "Average training loss: 0.027740799352526664\n",
      "Average test loss: 0.0021976211859534183\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0277511641714308\n",
      "Average test loss: 0.002219046572430266\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02776870647072792\n",
      "Average test loss: 0.00228650906888975\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02771026348074277\n",
      "Average test loss: 0.0022064950773492457\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027695732875002755\n",
      "Average test loss: 0.0022736269214914903\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02768685825003518\n",
      "Average test loss: 0.0022640007831570175\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02763192493882444\n",
      "Average test loss: 0.0022232334992537896\n",
      "Epoch 206/300\n",
      "Average training loss: 0.027606552542911635\n",
      "Average test loss: 0.0022178076927860577\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02761657583547963\n",
      "Average test loss: 0.0022777868335445723\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0276115758187241\n",
      "Average test loss: 0.0022400903137814667\n",
      "Epoch 209/300\n",
      "Average training loss: 0.027598345465130278\n",
      "Average test loss: 0.0022175494982964463\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02762833408680227\n",
      "Average test loss: 0.002243335036560893\n",
      "Epoch 211/300\n",
      "Average training loss: 0.027566838742958174\n",
      "Average test loss: 0.002272945171325571\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02751441342797544\n",
      "Average test loss: 0.002217028814057509\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02751827402247323\n",
      "Average test loss: 0.0023251453642215993\n",
      "Epoch 214/300\n",
      "Average training loss: 0.027484589490625592\n",
      "Average test loss: 0.0022385478822721377\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02751150458554427\n",
      "Average test loss: 0.002265781678673294\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02750276327629884\n",
      "Average test loss: 0.002236689186965426\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02748049621615145\n",
      "Average test loss: 0.002242247002820174\n",
      "Epoch 218/300\n",
      "Average training loss: 0.027470313565598595\n",
      "Average test loss: 0.002266266459185216\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02742187015215556\n",
      "Average test loss: 0.0022171280899395546\n",
      "Epoch 220/300\n",
      "Average training loss: 0.027447158412800893\n",
      "Average test loss: 0.0022686338445378675\n",
      "Epoch 221/300\n",
      "Average training loss: 0.027471821922394963\n",
      "Average test loss: 0.0022738401247188447\n",
      "Epoch 222/300\n",
      "Average training loss: 0.027415490926967725\n",
      "Average test loss: 0.0022431746199727057\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0273988620142142\n",
      "Average test loss: 0.002252240294797553\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02740325593782796\n",
      "Average test loss: 0.00226189152089258\n",
      "Epoch 225/300\n",
      "Average training loss: 0.027323808804154397\n",
      "Average test loss: 0.002386977362756928\n",
      "Epoch 226/300\n",
      "Average training loss: 0.027360144522455002\n",
      "Average test loss: 0.0022740413459638753\n",
      "Epoch 227/300\n",
      "Average training loss: 0.027350884874661762\n",
      "Average test loss: 0.002256228479039338\n",
      "Epoch 228/300\n",
      "Average training loss: 0.027352009859349994\n",
      "Average test loss: 0.0022635059748879736\n",
      "Epoch 229/300\n",
      "Average training loss: 0.027253081338273154\n",
      "Average test loss: 0.0023149125865764087\n",
      "Epoch 230/300\n",
      "Average training loss: 0.027273950485719574\n",
      "Average test loss: 0.002283902314077649\n",
      "Epoch 231/300\n",
      "Average training loss: 0.027293522708945805\n",
      "Average test loss: 0.0022323225867003204\n",
      "Epoch 232/300\n",
      "Average training loss: 0.027250496240125762\n",
      "Average test loss: 0.0022729426690687735\n",
      "Epoch 233/300\n",
      "Average training loss: 0.027239251183138954\n",
      "Average test loss: 0.002264392876687149\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027262709864311747\n",
      "Average test loss: 0.002296065096019043\n",
      "Epoch 235/300\n",
      "Average training loss: 0.027203298260768255\n",
      "Average test loss: 0.0022933042529556485\n",
      "Epoch 236/300\n",
      "Average training loss: 0.027222089113460645\n",
      "Average test loss: 0.002321064718688528\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02720349467959669\n",
      "Average test loss: 0.0022385578368686967\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02718585533565945\n",
      "Average test loss: 0.002294070275707377\n",
      "Epoch 239/300\n",
      "Average training loss: 0.027173906756771934\n",
      "Average test loss: 0.002362926244735718\n",
      "Epoch 240/300\n",
      "Average training loss: 0.027174099138213528\n",
      "Average test loss: 0.0022778663103882636\n",
      "Epoch 241/300\n",
      "Average training loss: 0.027136697999305195\n",
      "Average test loss: 0.0022374571829827297\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02714128226869636\n",
      "Average test loss: 0.0022715558615616625\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02715428918765651\n",
      "Average test loss: 0.0022158724636667303\n",
      "Epoch 244/300\n",
      "Average training loss: 0.027095839863022168\n",
      "Average test loss: 0.002246170628608929\n",
      "Epoch 245/300\n",
      "Average training loss: 0.027105518847703933\n",
      "Average test loss: 0.002264555946509871\n",
      "Epoch 246/300\n",
      "Average training loss: 0.027076633383830387\n",
      "Average test loss: 0.0022365281554973786\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02702989612519741\n",
      "Average test loss: 0.002260756484957205\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02703441288570563\n",
      "Average test loss: 0.00222912817945083\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027063105935851733\n",
      "Average test loss: 0.002285927097209626\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027023424044251444\n",
      "Average test loss: 0.0022702665627002717\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027057703566220073\n",
      "Average test loss: 0.002263552453368902\n",
      "Epoch 252/300\n",
      "Average training loss: 0.027019201005498567\n",
      "Average test loss: 0.002393495806803306\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027011114968193903\n",
      "Average test loss: 0.0022820838033739063\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026974599184261427\n",
      "Average test loss: 0.0023141609652795724\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02699594932132297\n",
      "Average test loss: 0.002253498471652468\n",
      "Epoch 256/300\n",
      "Average training loss: 0.026976940570606126\n",
      "Average test loss: 0.002284498765133321\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02694698067009449\n",
      "Average test loss: 0.002264315323179795\n",
      "Epoch 258/300\n",
      "Average training loss: 0.026924355384376313\n",
      "Average test loss: 0.002246217363824447\n",
      "Epoch 259/300\n",
      "Average training loss: 0.026959538776013585\n",
      "Average test loss: 0.0022416048103736505\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02691227848496702\n",
      "Average test loss: 0.002277982142443458\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02694000699619452\n",
      "Average test loss: 0.0022718059101866353\n",
      "Epoch 262/300\n",
      "Average training loss: 0.026900512349274423\n",
      "Average test loss: 0.002240764135494828\n",
      "Epoch 263/300\n",
      "Average training loss: 0.026875644370913505\n",
      "Average test loss: 0.002244577070698142\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02689729159573714\n",
      "Average test loss: 0.002279641072783205\n",
      "Epoch 265/300\n",
      "Average training loss: 0.026900722581479283\n",
      "Average test loss: 0.0022789077538376053\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026909132187565167\n",
      "Average test loss: 0.002212740135896537\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026817395185430845\n",
      "Average test loss: 0.0022740150831846726\n",
      "Epoch 268/300\n",
      "Average training loss: 0.026922495254211955\n",
      "Average test loss: 0.002240126135862536\n",
      "Epoch 269/300\n",
      "Average training loss: 0.026845693033602503\n",
      "Average test loss: 0.002355898641463783\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02681849439938863\n",
      "Average test loss: 0.0022555237643213737\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02680869375831551\n",
      "Average test loss: 0.002323338944878843\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026798411038186816\n",
      "Average test loss: 0.0022590350884323317\n",
      "Epoch 273/300\n",
      "Average training loss: 0.026787585804859796\n",
      "Average test loss: 0.0023104561037487455\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02686702304085096\n",
      "Average test loss: 0.00228615801802112\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026793906875782543\n",
      "Average test loss: 0.0022692320249560805\n",
      "Epoch 276/300\n",
      "Average training loss: 0.026889968709813222\n",
      "Average test loss: 0.002474569410396119\n",
      "Epoch 277/300\n",
      "Average training loss: 0.026777091047830052\n",
      "Average test loss: 0.002316890271173583\n",
      "Epoch 278/300\n",
      "Average training loss: 0.026760653904742665\n",
      "Average test loss: 0.00236103845656746\n",
      "Epoch 279/300\n",
      "Average training loss: 0.026768972198168435\n",
      "Average test loss: 0.0023129748315032987\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026707456169856918\n",
      "Average test loss: 0.0022942909623185795\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02672766229675876\n",
      "Average test loss: 0.0022784455015013617\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02672007031407621\n",
      "Average test loss: 0.002275153240395917\n",
      "Epoch 283/300\n",
      "Average training loss: 0.026705925848748948\n",
      "Average test loss: 0.002269860323311554\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026706955765684447\n",
      "Average test loss: 0.002285369236022234\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026712012636992667\n",
      "Average test loss: 0.002280538014239735\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02670402064753903\n",
      "Average test loss: 0.0023635814483794902\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026674150904019672\n",
      "Average test loss: 0.002270102237454719\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026685980257060794\n",
      "Average test loss: 0.002271703745962845\n",
      "Epoch 289/300\n",
      "Average training loss: 0.026634676045841642\n",
      "Average test loss: 0.0022796409879293706\n",
      "Epoch 290/300\n",
      "Average training loss: 0.026673835282524427\n",
      "Average test loss: 0.002322753865478767\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026603986307978632\n",
      "Average test loss: 0.0023301106748274633\n",
      "Epoch 292/300\n",
      "Average training loss: 0.026617580690317683\n",
      "Average test loss: 0.002308865003287792\n",
      "Epoch 293/300\n",
      "Average training loss: 0.026618902322318817\n",
      "Average test loss: 0.002323114212188456\n",
      "Epoch 294/300\n",
      "Average training loss: 0.026637970430983437\n",
      "Average test loss: 0.0023940170247935587\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026572228357195853\n",
      "Average test loss: 0.002258601482750641\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02660268414682812\n",
      "Average test loss: 0.0022678650406499704\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02654380747344759\n",
      "Average test loss: 0.0022823592968699005\n",
      "Epoch 298/300\n",
      "Average training loss: 0.026567625340488222\n",
      "Average test loss: 0.002270890537235472\n",
      "Epoch 299/300\n",
      "Average training loss: 0.026549775953094164\n",
      "Average test loss: 0.0022638497058716086\n",
      "Epoch 300/300\n",
      "Average training loss: 0.026562402347723644\n",
      "Average test loss: 0.0023021128295610347\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.6607706205844879\n",
      "Average test loss: 0.004445316912813319\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08616926580667496\n",
      "Average test loss: 0.0036233178799351057\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06264482100142373\n",
      "Average test loss: 0.0033039240340391796\n",
      "Epoch 4/300\n",
      "Average training loss: 0.053951983723375535\n",
      "Average test loss: 0.0034464535141984624\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04888816799720128\n",
      "Average test loss: 0.002958597752162152\n",
      "Epoch 6/300\n",
      "Average training loss: 0.045576828453275896\n",
      "Average test loss: 0.0027635638606217173\n",
      "Epoch 7/300\n",
      "Average training loss: 0.043090242038170495\n",
      "Average test loss: 0.002684097676848372\n",
      "Epoch 8/300\n",
      "Average training loss: 0.041108170327213076\n",
      "Average test loss: 0.002635633212617702\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03946603078974618\n",
      "Average test loss: 0.0024896475095301865\n",
      "Epoch 10/300\n",
      "Average training loss: 0.037935470514827305\n",
      "Average test loss: 0.002380246061210831\n",
      "Epoch 11/300\n",
      "Average training loss: 0.036608605341778865\n",
      "Average test loss: 0.002410729195094771\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03543254933423466\n",
      "Average test loss: 0.0022300654324806397\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03428955431116952\n",
      "Average test loss: 0.0021612831714252633\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03327817936407195\n",
      "Average test loss: 0.0020547876798858245\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03239177229338222\n",
      "Average test loss: 0.002014692693741785\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03155288151403268\n",
      "Average test loss: 0.001960777487191889\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030888173076841566\n",
      "Average test loss: 0.0019588868094401225\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03030199455552631\n",
      "Average test loss: 0.0018852777193403907\n",
      "Epoch 19/300\n",
      "Average training loss: 0.029735248868664105\n",
      "Average test loss: 0.0018553588355167045\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02931382341434558\n",
      "Average test loss: 0.0018180632395669819\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02894259173505836\n",
      "Average test loss: 0.0018059278115009267\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0285826352785031\n",
      "Average test loss: 0.0017925549221949446\n",
      "Epoch 23/300\n",
      "Average training loss: 0.028249972270594705\n",
      "Average test loss: 0.0017471194535804292\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02792438830435276\n",
      "Average test loss: 0.0017459861984890368\n",
      "Epoch 25/300\n",
      "Average training loss: 0.027672952820029524\n",
      "Average test loss: 0.0017765580935196743\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02738156129254235\n",
      "Average test loss: 0.0017126487048549784\n",
      "Epoch 27/300\n",
      "Average training loss: 0.027172876200742192\n",
      "Average test loss: 0.0017262277812179591\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026970919307735233\n",
      "Average test loss: 0.001689263228327036\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026752361378735965\n",
      "Average test loss: 0.0016696710329399342\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02655088260769844\n",
      "Average test loss: 0.001659894278479947\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02638605434530311\n",
      "Average test loss: 0.0016477006406833729\n",
      "Epoch 32/300\n",
      "Average training loss: 0.026212241613202625\n",
      "Average test loss: 0.001660231875669625\n",
      "Epoch 33/300\n",
      "Average training loss: 0.026051778662535878\n",
      "Average test loss: 0.0016439377435793479\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025913711599177783\n",
      "Average test loss: 0.0016345163494245047\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025760204490688113\n",
      "Average test loss: 0.001624077906832099\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025673467179139455\n",
      "Average test loss: 0.0016240227626015743\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02552103271087011\n",
      "Average test loss: 0.0016020656863434447\n",
      "Epoch 38/300\n",
      "Average training loss: 0.025396508945359123\n",
      "Average test loss: 0.001597594423426522\n",
      "Epoch 39/300\n",
      "Average training loss: 0.025261504381895065\n",
      "Average test loss: 0.0016046831522964768\n",
      "Epoch 40/300\n",
      "Average training loss: 0.025248815312981607\n",
      "Average test loss: 0.0016032367543213897\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02505079441434807\n",
      "Average test loss: 0.0015906082429509197\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02497739189614852\n",
      "Average test loss: 0.0015779356499099069\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02489845588306586\n",
      "Average test loss: 0.0015825351889555653\n",
      "Epoch 44/300\n",
      "Average training loss: 0.024860443206297028\n",
      "Average test loss: 0.0015658022828607095\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024716410987906987\n",
      "Average test loss: 0.0015746871122262545\n",
      "Epoch 46/300\n",
      "Average training loss: 0.024616637936896748\n",
      "Average test loss: 0.001557540641994112\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02455497211880154\n",
      "Average test loss: 0.0015733862860749165\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0244697559128205\n",
      "Average test loss: 0.0015567381730717089\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02441219360133012\n",
      "Average test loss: 0.0015715446357304852\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024364362984895707\n",
      "Average test loss: 0.0015590341314673424\n",
      "Epoch 51/300\n",
      "Average training loss: 0.024265056482619708\n",
      "Average test loss: 0.0015469677895307541\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02422478874690003\n",
      "Average test loss: 0.0015651444608552588\n",
      "Epoch 53/300\n",
      "Average training loss: 0.024151215109560226\n",
      "Average test loss: 0.0015532635040581225\n",
      "Epoch 54/300\n",
      "Average training loss: 0.024118467456764645\n",
      "Average test loss: 0.0015441708146697946\n",
      "Epoch 55/300\n",
      "Average training loss: 0.024033173471689224\n",
      "Average test loss: 0.0015331236898071237\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02396614229016834\n",
      "Average test loss: 0.0015367162412860326\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023884478368692928\n",
      "Average test loss: 0.0015445296607083745\n",
      "Epoch 58/300\n",
      "Average training loss: 0.023826185176769894\n",
      "Average test loss: 0.0015355271728088459\n",
      "Epoch 59/300\n",
      "Average training loss: 0.023792482417490747\n",
      "Average test loss: 0.001576625320232577\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02376091769006517\n",
      "Average test loss: 0.0015237255439990098\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02368323415517807\n",
      "Average test loss: 0.0015416124645206664\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02362390955289205\n",
      "Average test loss: 0.0015311276296981508\n",
      "Epoch 63/300\n",
      "Average training loss: 0.023586140816410382\n",
      "Average test loss: 0.0015387222194630239\n",
      "Epoch 64/300\n",
      "Average training loss: 0.023553811323311594\n",
      "Average test loss: 0.0015239020495468544\n",
      "Epoch 65/300\n",
      "Average training loss: 0.023470188217030633\n",
      "Average test loss: 0.0015306760790861314\n",
      "Epoch 66/300\n",
      "Average training loss: 0.023421593114733696\n",
      "Average test loss: 0.0015467826023490892\n",
      "Epoch 67/300\n",
      "Average training loss: 0.023387373745441437\n",
      "Average test loss: 0.001534626657764117\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02333137442254358\n",
      "Average test loss: 0.0015163646491451396\n",
      "Epoch 69/300\n",
      "Average training loss: 0.023281892524825203\n",
      "Average test loss: 0.0015203755044688782\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023250968368517026\n",
      "Average test loss: 0.0015212412473435202\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02318079039785597\n",
      "Average test loss: 0.0015135044420862363\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02316849949128098\n",
      "Average test loss: 0.0015191623185657793\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0230925649272071\n",
      "Average test loss: 0.0015081849973131384\n",
      "Epoch 74/300\n",
      "Average training loss: 0.023050219876898658\n",
      "Average test loss: 0.0015258480987201135\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023037795098291503\n",
      "Average test loss: 0.0015163200209434662\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02296533532771799\n",
      "Average test loss: 0.001571046353628238\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02291429089837604\n",
      "Average test loss: 0.0015139478247405754\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0228844962567091\n",
      "Average test loss: 0.001524215560613407\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022842537119984626\n",
      "Average test loss: 0.001522694493436979\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02282765692969163\n",
      "Average test loss: 0.001531390534920825\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022747122791078357\n",
      "Average test loss: 0.0015278023928403854\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022743082072999743\n",
      "Average test loss: 0.0015375018507863085\n",
      "Epoch 83/300\n",
      "Average training loss: 0.022687643695208762\n",
      "Average test loss: 0.001510694899285833\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0226550479547845\n",
      "Average test loss: 0.001522571603457133\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02260260925690333\n",
      "Average test loss: 0.0015118281932340728\n",
      "Epoch 86/300\n",
      "Average training loss: 0.022584493642052016\n",
      "Average test loss: 0.0015043242642035087\n",
      "Epoch 87/300\n",
      "Average training loss: 0.022579749633868534\n",
      "Average test loss: 0.0015063245099865728\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022520503426591554\n",
      "Average test loss: 0.0015161364412763052\n",
      "Epoch 89/300\n",
      "Average training loss: 0.022486714560124608\n",
      "Average test loss: 0.001543003168474469\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022431104444795184\n",
      "Average test loss: 0.0015126439541992213\n",
      "Epoch 91/300\n",
      "Average training loss: 0.022384541395637725\n",
      "Average test loss: 0.0015012712151639992\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02235299415886402\n",
      "Average test loss: 0.0015076768652846416\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02232390985555119\n",
      "Average test loss: 0.001512660428467724\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02229149260537492\n",
      "Average test loss: 0.0015065265653861893\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022247308308879533\n",
      "Average test loss: 0.0015221072292576233\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02226229452755716\n",
      "Average test loss: 0.0015146186402481463\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022181507046024004\n",
      "Average test loss: 0.0015477003402387102\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022158501093586285\n",
      "Average test loss: 0.0015124902673479585\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022159674967328707\n",
      "Average test loss: 0.0015139781143516302\n",
      "Epoch 100/300\n",
      "Average training loss: 0.022154241599970394\n",
      "Average test loss: 0.0015213850997388362\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022057307947013112\n",
      "Average test loss: 0.0015178133185125059\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022027394929693803\n",
      "Average test loss: 0.0015101435844682984\n",
      "Epoch 103/300\n",
      "Average training loss: 0.021983616658382946\n",
      "Average test loss: 0.001519668798479769\n",
      "Epoch 104/300\n",
      "Average training loss: 0.021979624910487067\n",
      "Average test loss: 0.0015077065719912449\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02192352493935161\n",
      "Average test loss: 0.001526905272156\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021884506144457392\n",
      "Average test loss: 0.0015260771051463153\n",
      "Epoch 107/300\n",
      "Average training loss: 0.021858909106916853\n",
      "Average test loss: 0.001547955709613032\n",
      "Epoch 108/300\n",
      "Average training loss: 0.021856338979469404\n",
      "Average test loss: 0.0015610976704499787\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02181904906117254\n",
      "Average test loss: 0.0015254289694130421\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021800072775946724\n",
      "Average test loss: 0.0015402277261018754\n",
      "Epoch 111/300\n",
      "Average training loss: 0.021761825639340614\n",
      "Average test loss: 0.0015354148973193433\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02173927022020022\n",
      "Average test loss: 0.0015123667356868585\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021704148675004642\n",
      "Average test loss: 0.0015154689758395156\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02166429376602173\n",
      "Average test loss: 0.00153506206928028\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0216541355566846\n",
      "Average test loss: 0.0015155320829815335\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0216119513909022\n",
      "Average test loss: 0.0015208913133376175\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02157411360575093\n",
      "Average test loss: 0.001536047471480237\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021557027427686587\n",
      "Average test loss: 0.0015453924205568101\n",
      "Epoch 119/300\n",
      "Average training loss: 0.021541179658638106\n",
      "Average test loss: 0.0015332212263925208\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021514576279454762\n",
      "Average test loss: 0.0015178753957152367\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02146108452644613\n",
      "Average test loss: 0.0015310695521119568\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021449464632405175\n",
      "Average test loss: 0.0015158173679891559\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0214229740301768\n",
      "Average test loss: 0.0015243010064586997\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02138251272175047\n",
      "Average test loss: 0.001530937887314293\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02140016082425912\n",
      "Average test loss: 0.0015353114646342065\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02131464281347063\n",
      "Average test loss: 0.0015308155218760173\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02133203054798974\n",
      "Average test loss: 0.0015288067841902374\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021297849918405214\n",
      "Average test loss: 0.0015934148011729121\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021303329560491774\n",
      "Average test loss: 0.0015234842091384861\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02139089344938596\n",
      "Average test loss: 0.0015615206342190504\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021222207259800698\n",
      "Average test loss: 0.0015182821554028327\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021199278692404427\n",
      "Average test loss: 0.0015345892559530006\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021204529462589158\n",
      "Average test loss: 0.0015529636294684477\n",
      "Epoch 134/300\n",
      "Average training loss: 0.021158088909255135\n",
      "Average test loss: 0.0015211336387114392\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021122813946670958\n",
      "Average test loss: 0.0015240102627625068\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021106717400252818\n",
      "Average test loss: 0.0015361267299287848\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02108287770880593\n",
      "Average test loss: 0.001528844937785632\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02104734622935454\n",
      "Average test loss: 0.0015332782914241154\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021062307480308743\n",
      "Average test loss: 0.0015453504691314366\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021009935615791214\n",
      "Average test loss: 0.001545130200063189\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021002252532376184\n",
      "Average test loss: 0.001543943911169966\n",
      "Epoch 142/300\n",
      "Average training loss: 0.020977638295127287\n",
      "Average test loss: 0.001542592691257596\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0209536232219802\n",
      "Average test loss: 0.0015821267053898838\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020958182068334684\n",
      "Average test loss: 0.0015682763553534944\n",
      "Epoch 145/300\n",
      "Average training loss: 0.020938159614801406\n",
      "Average test loss: 0.001547887431561119\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02090716552651591\n",
      "Average test loss: 0.0015424391133710743\n",
      "Epoch 147/300\n",
      "Average training loss: 0.020869064686199028\n",
      "Average test loss: 0.0015377255744404262\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02087099247591363\n",
      "Average test loss: 0.0015439262035199337\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0208230928497182\n",
      "Average test loss: 0.0015537983863097098\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020814576670527458\n",
      "Average test loss: 0.001976703705266118\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02081203073925442\n",
      "Average test loss: 0.00165755333373737\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02077035540011194\n",
      "Average test loss: 0.001532832125822703\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02073626363111867\n",
      "Average test loss: 0.0015971731450408696\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02075939748932918\n",
      "Average test loss: 0.0015516594669057263\n",
      "Epoch 155/300\n",
      "Average training loss: 0.020702756466137038\n",
      "Average test loss: 0.0015425070625626378\n",
      "Epoch 156/300\n",
      "Average training loss: 0.020713525543610254\n",
      "Average test loss: 0.0015593591052004033\n",
      "Epoch 157/300\n",
      "Average training loss: 0.020665469287170304\n",
      "Average test loss: 0.0015806935001164675\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02064277517961131\n",
      "Average test loss: 0.0015595014015626577\n",
      "Epoch 159/300\n",
      "Average training loss: 0.020629484300812087\n",
      "Average test loss: 0.0015515816004739867\n",
      "Epoch 160/300\n",
      "Average training loss: 0.020613091298275522\n",
      "Average test loss: 0.0015410441065000164\n",
      "Epoch 161/300\n",
      "Average training loss: 0.020627749545706644\n",
      "Average test loss: 0.0015456440680556827\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02059509899881151\n",
      "Average test loss: 0.001561013583973464\n",
      "Epoch 163/300\n",
      "Average training loss: 0.020591958045959473\n",
      "Average test loss: 0.001570622195593185\n",
      "Epoch 164/300\n",
      "Average training loss: 0.020557431237565146\n",
      "Average test loss: 0.0015565099946947562\n",
      "Epoch 165/300\n",
      "Average training loss: 0.020534678194257947\n",
      "Average test loss: 0.001570338880746729\n",
      "Epoch 166/300\n",
      "Average training loss: 0.020528110028968916\n",
      "Average test loss: 0.0015320228247178926\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02049603302280108\n",
      "Average test loss: 0.0015830106053294408\n",
      "Epoch 168/300\n",
      "Average training loss: 0.020490860264334415\n",
      "Average test loss: 0.0015660388484183285\n",
      "Epoch 169/300\n",
      "Average training loss: 0.020466643317706056\n",
      "Average test loss: 0.0015721007202648455\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02044571607808272\n",
      "Average test loss: 0.0015703839405129354\n",
      "Epoch 171/300\n",
      "Average training loss: 0.020458800309234196\n",
      "Average test loss: 0.001550189384776685\n",
      "Epoch 172/300\n",
      "Average training loss: 0.020386933357351356\n",
      "Average test loss: 0.0015420667783667644\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02039829958975315\n",
      "Average test loss: 0.0015657607039643658\n",
      "Epoch 174/300\n",
      "Average training loss: 0.020391147282388476\n",
      "Average test loss: 0.0015499376986796657\n",
      "Epoch 175/300\n",
      "Average training loss: 0.020367954787280824\n",
      "Average test loss: 0.0015503277420066297\n",
      "Epoch 176/300\n",
      "Average training loss: 0.020335803305109344\n",
      "Average test loss: 0.0015760725980831518\n",
      "Epoch 177/300\n",
      "Average training loss: 0.020337390946017372\n",
      "Average test loss: 0.0015762467341911461\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02034458694358667\n",
      "Average test loss: 0.0015515328930276963\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02030536312692695\n",
      "Average test loss: 0.0015617714806770286\n",
      "Epoch 180/300\n",
      "Average training loss: 0.020305759113695886\n",
      "Average test loss: 0.0015610724493033356\n",
      "Epoch 181/300\n",
      "Average training loss: 0.020265554539031453\n",
      "Average test loss: 0.0015830338299274444\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020274787813425062\n",
      "Average test loss: 0.001753030435130414\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020328128927283817\n",
      "Average test loss: 0.0015538239777088164\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0202785271157821\n",
      "Average test loss: 0.0015766551080677244\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020207843110793166\n",
      "Average test loss: 0.0015786885430829393\n",
      "Epoch 186/300\n",
      "Average training loss: 0.020214164078235626\n",
      "Average test loss: 0.001594041186177896\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02019021431936158\n",
      "Average test loss: 0.0015614371061221594\n",
      "Epoch 188/300\n",
      "Average training loss: 0.020162841237253613\n",
      "Average test loss: 0.0016298776154095927\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020120260877741707\n",
      "Average test loss: 0.001585797317015628\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020156853410932752\n",
      "Average test loss: 0.0015627949635705187\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020130884662270546\n",
      "Average test loss: 0.0015705126708166466\n",
      "Epoch 192/300\n",
      "Average training loss: 0.020139929155508676\n",
      "Average test loss: 0.001618341353825397\n",
      "Epoch 193/300\n",
      "Average training loss: 0.020113608966271082\n",
      "Average test loss: 0.0016013034312054514\n",
      "Epoch 194/300\n",
      "Average training loss: 0.020095268367065322\n",
      "Average test loss: 0.0015597154482578238\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02008767843163676\n",
      "Average test loss: 0.0016567277209833264\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020054884455270237\n",
      "Average test loss: 0.0015666266727882127\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02005995866821872\n",
      "Average test loss: 0.001575500648882654\n",
      "Epoch 198/300\n",
      "Average training loss: 0.020026915264626344\n",
      "Average test loss: 0.0015914278525031276\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020021751471691662\n",
      "Average test loss: 0.0015853854231536388\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02000643803841538\n",
      "Average test loss: 0.0015636698448409636\n",
      "Epoch 201/300\n",
      "Average training loss: 0.020036312960916094\n",
      "Average test loss: 0.0015670000784513023\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02000824806508091\n",
      "Average test loss: 0.0016146809780556294\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019991145064433415\n",
      "Average test loss: 0.0015751440778581632\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019962255756060282\n",
      "Average test loss: 0.0016442586338768403\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019939027022156452\n",
      "Average test loss: 0.0015940662767324183\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019943965007861456\n",
      "Average test loss: 0.0015823075947248274\n",
      "Epoch 207/300\n",
      "Average training loss: 0.019927934457858402\n",
      "Average test loss: 0.0015584992677387265\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01990779682331615\n",
      "Average test loss: 0.0015715820068079565\n",
      "Epoch 209/300\n",
      "Average training loss: 0.019897782050900988\n",
      "Average test loss: 0.0015906275225182375\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01988673350877232\n",
      "Average test loss: 0.001591507848041753\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01987572398947345\n",
      "Average test loss: 0.0016022339305943913\n",
      "Epoch 212/300\n",
      "Average training loss: 0.019861730923255286\n",
      "Average test loss: 0.0016921023350829878\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019831105689207713\n",
      "Average test loss: 0.001620487148873508\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01985827726456854\n",
      "Average test loss: 0.0015912235734156436\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019848618916339343\n",
      "Average test loss: 0.0015895188309045301\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01986596296230952\n",
      "Average test loss: 0.0016286659271766743\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01978841186149253\n",
      "Average test loss: 0.001588701672748559\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019801285556621022\n",
      "Average test loss: 0.0015980071937665343\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01980011535021994\n",
      "Average test loss: 0.0015817752888219224\n",
      "Epoch 220/300\n",
      "Average training loss: 0.019776188608672883\n",
      "Average test loss: 0.0015804719593789842\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019749151258005037\n",
      "Average test loss: 0.0015924697520935702\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019762477828396692\n",
      "Average test loss: 0.0015917485042785606\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019782259479165076\n",
      "Average test loss: 0.0016034114762312837\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019732251607709462\n",
      "Average test loss: 0.0016007202693985568\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019724124145176675\n",
      "Average test loss: 0.0015722494088113309\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01975189125869009\n",
      "Average test loss: 0.0015726076679097281\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019718133497569297\n",
      "Average test loss: 0.001593297169647283\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01967296376493242\n",
      "Average test loss: 0.0016116318258767328\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01969157913658354\n",
      "Average test loss: 0.0016022367978261577\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019655155269636047\n",
      "Average test loss: 0.0015840480767397417\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01966603739725219\n",
      "Average test loss: 0.0015965607147663831\n",
      "Epoch 232/300\n",
      "Average training loss: 0.019655257837639915\n",
      "Average test loss: 0.0016132563498492043\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019628879802094566\n",
      "Average test loss: 0.0015853187503914038\n",
      "Epoch 234/300\n",
      "Average training loss: 0.019674380297462147\n",
      "Average test loss: 0.001610301218719946\n",
      "Epoch 235/300\n",
      "Average training loss: 0.019629257698853812\n",
      "Average test loss: 0.0016220516708886459\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019608727440237998\n",
      "Average test loss: 0.0015755099815626939\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019618762597441674\n",
      "Average test loss: 0.0015744233161417973\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019621409197648366\n",
      "Average test loss: 0.0016414988512794177\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01960272585352262\n",
      "Average test loss: 0.001588598570579456\n",
      "Epoch 240/300\n",
      "Average training loss: 0.019590494578083355\n",
      "Average test loss: 0.0015942076010008652\n",
      "Epoch 241/300\n",
      "Average training loss: 0.019565064748128254\n",
      "Average test loss: 0.0015784529792144894\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019556021789709728\n",
      "Average test loss: 0.001609854607635902\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0195423550489876\n",
      "Average test loss: 0.0016008859836599892\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019571422855059306\n",
      "Average test loss: 0.0015746724576585823\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01952579447130362\n",
      "Average test loss: 0.0016453212740727596\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01951394500500626\n",
      "Average test loss: 0.0015991749813159307\n",
      "Epoch 247/300\n",
      "Average training loss: 0.019508253706826104\n",
      "Average test loss: 0.001635087045116557\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01950583789580398\n",
      "Average test loss: 0.0015799874757520026\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01950108503467507\n",
      "Average test loss: 0.0015894036394440466\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019490937910146184\n",
      "Average test loss: 0.0016311951574559014\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01950300342672401\n",
      "Average test loss: 0.0016248963288962841\n",
      "Epoch 252/300\n",
      "Average training loss: 0.019501580235030916\n",
      "Average test loss: 0.001571985232281602\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01948040269398027\n",
      "Average test loss: 0.0016394814608825577\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019442247663935024\n",
      "Average test loss: 0.001607546313251886\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019430829162398973\n",
      "Average test loss: 0.0016407151967287063\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019442413248949582\n",
      "Average test loss: 0.0016126187167440852\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019422381766968304\n",
      "Average test loss: 0.0016067003232116501\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01942130247089598\n",
      "Average test loss: 0.001619968718331721\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01940012457801236\n",
      "Average test loss: 0.0016203801669180393\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019394234513243038\n",
      "Average test loss: 0.001708428833530181\n",
      "Epoch 261/300\n",
      "Average training loss: 0.019412797963453663\n",
      "Average test loss: 0.0015764144429316123\n",
      "Epoch 262/300\n",
      "Average training loss: 0.019406250321202807\n",
      "Average test loss: 0.001598703409338163\n",
      "Epoch 263/300\n",
      "Average training loss: 0.019384914838605456\n",
      "Average test loss: 0.0015944339462245505\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0193713987916708\n",
      "Average test loss: 0.0015893810405913325\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01936048441959752\n",
      "Average test loss: 0.001647147616992394\n",
      "Epoch 266/300\n",
      "Average training loss: 0.019335426790846717\n",
      "Average test loss: 0.0016638738398647142\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01937401508457131\n",
      "Average test loss: 0.0016432816341726316\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0193254813419448\n",
      "Average test loss: 0.001623936424859696\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01934632099668185\n",
      "Average test loss: 0.001683810468763113\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019321092915203838\n",
      "Average test loss: 0.0015984889545167485\n",
      "Epoch 271/300\n",
      "Average training loss: 0.019324680690964064\n",
      "Average test loss: 0.001606210941862729\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01930636236568292\n",
      "Average test loss: 0.0016148146020455493\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01930554334157043\n",
      "Average test loss: 0.0016422014410297076\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019321603677339025\n",
      "Average test loss: 0.0016357885563953055\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019268133613798352\n",
      "Average test loss: 0.0016215578253484435\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01932863384236892\n",
      "Average test loss: 0.001630960492675917\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01923537068400118\n",
      "Average test loss: 0.0016976202833983633\n",
      "Epoch 278/300\n",
      "Average training loss: 0.019270339654551612\n",
      "Average test loss: 0.0016124102680219543\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01925112882918782\n",
      "Average test loss: 0.0016308556563324399\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019268116606606378\n",
      "Average test loss: 0.001592213295503623\n",
      "Epoch 281/300\n",
      "Average training loss: 0.019239968481991027\n",
      "Average test loss: 0.0016137434721200002\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01923496365878317\n",
      "Average test loss: 0.0032702459953725337\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01926184569630358\n",
      "Average test loss: 0.0016228213606195318\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019233789617816608\n",
      "Average test loss: 0.0015974871183021201\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019220855721169047\n",
      "Average test loss: 0.0016157648153603077\n",
      "Epoch 286/300\n",
      "Average training loss: 0.019206977152162127\n",
      "Average test loss: 0.0015935147404670716\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019218210089951754\n",
      "Average test loss: 0.0016565952218241162\n",
      "Epoch 288/300\n",
      "Average training loss: 0.019222518737117448\n",
      "Average test loss: 0.0016138608024145165\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01920426737765471\n",
      "Average test loss: 0.0016415546233248379\n",
      "Epoch 290/300\n",
      "Average training loss: 0.019181332247124778\n",
      "Average test loss: 0.0016721432835070623\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01916987931728363\n",
      "Average test loss: 0.0016608124509867694\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01917594587140613\n",
      "Average test loss: 0.0016237907734596066\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01919739640255769\n",
      "Average test loss: 0.0016024747043848037\n",
      "Epoch 294/300\n",
      "Average training loss: 0.019153762255277897\n",
      "Average test loss: 0.001596570012677047\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01915586967435148\n",
      "Average test loss: 0.001611523746823271\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019156094068454373\n",
      "Average test loss: 0.001621915871070491\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019150977694325978\n",
      "Average test loss: 0.0016576355314917035\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019124045402639442\n",
      "Average test loss: 0.0016067764663861858\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01911929983728462\n",
      "Average test loss: 0.0016096745998495155\n",
      "Epoch 300/300\n",
      "Average training loss: 0.019112693205475807\n",
      "Average test loss: 0.0016114031856672632\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth3-.01/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.74\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.98\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.15\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.56\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.72\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.10\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.23\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.44\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.49\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.63\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.60\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.67\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.247368184195625\n",
      "Average test loss: 0.00870816859851281\n",
      "Epoch 2/300\n",
      "Average training loss: 1.1503986932436625\n",
      "Average test loss: 0.005020906614760558\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5272008467780219\n",
      "Average test loss: 0.004860191978100273\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3551432976722717\n",
      "Average test loss: 0.004887593905131022\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2707812512980567\n",
      "Average test loss: 0.004692467278904385\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17103867714934878\n",
      "Average test loss: 0.004461202621873882\n",
      "Epoch 9/300\n",
      "Average training loss: 0.16078011702166664\n",
      "Average test loss: 0.004428372134764989\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15369965783754985\n",
      "Average test loss: 0.004415758913175927\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14839806539482542\n",
      "Average test loss: 0.0043920603278610445\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14436287976635828\n",
      "Average test loss: 0.004328922284973992\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1385558310680919\n",
      "Average test loss: 0.00431901650958591\n",
      "Epoch 15/300\n",
      "Average training loss: 0.13628749587138494\n",
      "Average test loss: 0.00430302038623227\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13447215868367088\n",
      "Average test loss: 0.004235853457202514\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13284409549501208\n",
      "Average test loss: 0.0042177329750524625\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13161946019199158\n",
      "Average test loss: 0.004193225672675504\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1303347806069586\n",
      "Average test loss: 0.0042018555932574805\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1294679388999939\n",
      "Average test loss: 0.00422675132089191\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12853361908594768\n",
      "Average test loss: 0.004172080495705207\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12774603480100633\n",
      "Average test loss: 0.0041695031358136075\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12709401248561011\n",
      "Average test loss: 0.004177520807832479\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12652443300353156\n",
      "Average test loss: 0.004133637018501758\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12586431099308862\n",
      "Average test loss: 0.004133656532400184\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12535706969764498\n",
      "Average test loss: 0.004150243974394268\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12488649692138036\n",
      "Average test loss: 0.0041107460127936465\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1243382140662935\n",
      "Average test loss: 0.004111216749168104\n",
      "Epoch 29/300\n",
      "Average training loss: 0.12408820636404885\n",
      "Average test loss: 0.00408952565656768\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12320393087466558\n",
      "Average test loss: 0.004067660596428646\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12283612938721974\n",
      "Average test loss: 0.004045351965973775\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12253505390220218\n",
      "Average test loss: 0.004074421470777856\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12216629945569568\n",
      "Average test loss: 0.0041234018738485045\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12175726432932747\n",
      "Average test loss: 0.004043863056848447\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12152187327543894\n",
      "Average test loss: 0.004029613136003415\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12117471741305458\n",
      "Average test loss: 0.0040463153349442615\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12103345763021045\n",
      "Average test loss: 0.004024741498132547\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1206934072110388\n",
      "Average test loss: 0.004025704190755884\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12051901177565257\n",
      "Average test loss: 0.004001927688717842\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12024552434682846\n",
      "Average test loss: 0.004015688844025135\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12003025568193859\n",
      "Average test loss: 0.003992945954203605\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11975274749596913\n",
      "Average test loss: 0.004031946608175834\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11959053902493583\n",
      "Average test loss: 0.004020148616077171\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11938568629158867\n",
      "Average test loss: 0.004012143160319991\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11917042287853029\n",
      "Average test loss: 0.00400142843172782\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11894896811246872\n",
      "Average test loss: 0.003988957546237442\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11880497027105755\n",
      "Average test loss: 0.003971420760369963\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11860825580358506\n",
      "Average test loss: 0.003972223951791724\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1183570870425966\n",
      "Average test loss: 0.003971533878395955\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1182290263970693\n",
      "Average test loss: 0.003966097394211425\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11804234978225496\n",
      "Average test loss: 0.003960825495835808\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11794179440869225\n",
      "Average test loss: 0.0040032346590111656\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11782310939497417\n",
      "Average test loss: 0.003972877867519855\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11773900738689634\n",
      "Average test loss: 0.003962311323939098\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11756855156686571\n",
      "Average test loss: 0.004051047552790907\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11747164644135369\n",
      "Average test loss: 0.00398067099104325\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1173353597190645\n",
      "Average test loss: 0.003997520332742068\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11722915218936072\n",
      "Average test loss: 0.003942680846071906\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11706652592950396\n",
      "Average test loss: 0.003949898292207056\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11697455750571358\n",
      "Average test loss: 0.003949611450028088\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11696104028489855\n",
      "Average test loss: 0.003946524049258895\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11667422748274274\n",
      "Average test loss: 0.003941077930231889\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1166934964325693\n",
      "Average test loss: 0.003949102664987246\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11656953552034166\n",
      "Average test loss: 0.003936390579367678\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11639762554566066\n",
      "Average test loss: 0.00392700248832504\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11635663017961714\n",
      "Average test loss: 0.003945343838383754\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11622406087319057\n",
      "Average test loss: 0.003926193826521436\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11616856049166785\n",
      "Average test loss: 0.003930774172354076\n",
      "Epoch 70/300\n",
      "Average training loss: 0.1160043745968077\n",
      "Average test loss: 0.003958698850125075\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11597136884265476\n",
      "Average test loss: 0.003928374347587427\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1158028102517128\n",
      "Average test loss: 0.003940123770799902\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11579926043086582\n",
      "Average test loss: 0.003946631604598628\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11566031369235781\n",
      "Average test loss: 0.003923510138359335\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11559711305300395\n",
      "Average test loss: 0.003925171251098315\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11544035093651878\n",
      "Average test loss: 0.003930476716823048\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11539627890785535\n",
      "Average test loss: 0.003925512755910555\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11523455675443013\n",
      "Average test loss: 0.003925875984546211\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1151341230471929\n",
      "Average test loss: 0.003930370204564598\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11499225907855563\n",
      "Average test loss: 0.003929709269354741\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11475051358011033\n",
      "Average test loss: 0.00391554809641093\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11468760628832711\n",
      "Average test loss: 0.003921378210186959\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11463754945993423\n",
      "Average test loss: 0.003941279475473695\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11457292233573066\n",
      "Average test loss: 0.003928398578945134\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11443399560451507\n",
      "Average test loss: 0.003919971575339635\n",
      "Epoch 88/300\n",
      "Average training loss: 0.114355686267217\n",
      "Average test loss: 0.003913312878045771\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11415700787305832\n",
      "Average test loss: 0.003929282267888387\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11407273974021276\n",
      "Average test loss: 0.003963938905547063\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11405553834305869\n",
      "Average test loss: 0.003940291742483774\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11390589641200172\n",
      "Average test loss: 0.003959359456267622\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11393146883779103\n",
      "Average test loss: 0.003915457673370838\n",
      "Epoch 94/300\n",
      "Average training loss: 0.1138362765510877\n",
      "Average test loss: 0.003943891854956746\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11369135236740112\n",
      "Average test loss: 0.00394156320248213\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1135808637407091\n",
      "Average test loss: 0.003930140308207936\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11343368467357423\n",
      "Average test loss: 0.003913321442488167\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1133797656695048\n",
      "Average test loss: 0.003952858816832304\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11330552670690748\n",
      "Average test loss: 0.0039297669902443886\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1131697858704461\n",
      "Average test loss: 0.003922479372678532\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11314957125319375\n",
      "Average test loss: 0.0039252496498326465\n",
      "Epoch 102/300\n",
      "Average training loss: 0.11278886785109837\n",
      "Average test loss: 0.0039500171409712894\n",
      "Epoch 105/300\n",
      "Average training loss: 0.1126219385266304\n",
      "Average test loss: 0.00392743243649602\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11266045668390062\n",
      "Average test loss: 0.003985638039600518\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11248551213741302\n",
      "Average test loss: 0.003954476806024711\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11251355366574393\n",
      "Average test loss: 0.00391777707884709\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11234146382411321\n",
      "Average test loss: 0.003990844120995866\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11221296799845165\n",
      "Average test loss: 0.003933869427070021\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11213285982608795\n",
      "Average test loss: 0.003949406828317377\n",
      "Epoch 112/300\n",
      "Average training loss: 0.11193496290180419\n",
      "Average test loss: 0.004408555091048281\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11192128443717957\n",
      "Average test loss: 0.003931199998077419\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11184683212969038\n",
      "Average test loss: 0.003922120532227887\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11164564037985272\n",
      "Average test loss: 0.003934367503143019\n",
      "Epoch 116/300\n",
      "Average training loss: 0.1115916285581059\n",
      "Average test loss: 0.004143370910030272\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11156424907843272\n",
      "Average test loss: 0.003943007883926233\n",
      "Epoch 118/300\n",
      "Average training loss: 0.11127543440792295\n",
      "Average test loss: 0.004005836166441441\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11113917672634124\n",
      "Average test loss: 0.00394963573747211\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11123065770334668\n",
      "Average test loss: 0.0039249420588215195\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11099520107110342\n",
      "Average test loss: 0.003927911137955056\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11091948619153764\n",
      "Average test loss: 0.003998713956111007\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11079635929399066\n",
      "Average test loss: 0.006108581217626731\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11070802864763472\n",
      "Average training loss: 0.11038845390081406\n",
      "Average test loss: 0.00396498209817542\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11047043854660459\n",
      "Average test loss: 0.003948819929526912\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11024815369314618\n",
      "Average test loss: 0.004118294794940286\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10994166928529739\n",
      "Average test loss: 0.004005254101836019\n",
      "Epoch 133/300\n",
      "Average training loss: 0.11001697408821848\n",
      "Average test loss: 0.003983798037800524\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10972094425227907\n",
      "Average test loss: 0.003989920576620433\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10967365960280101\n",
      "Average test loss: 0.003967160746869113\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10980404665072759\n",
      "Average test loss: 0.003956732883842455\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10942480338282055\n",
      "Average test loss: 0.004020399147437678\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10937655293279223\n",
      "Average test loss: 0.003998732440587547\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10913008819023769\n",
      "Average test loss: 0.004047597038042214\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10914268976449966\n",
      "Average test loss: 0.003994937663690912\n",
      "Epoch 141/300\n",
      "Average training loss: 0.10896549112266965\n",
      "Average test loss: 0.00407132955474986\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10890614250633451\n",
      "Average test loss: 0.003954292128483455\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10884848664866553\n",
      "Average test loss: 0.003974705340133773\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10872183400392532\n",
      "Average test loss: 0.0039644329572717345\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10865073925919003\n",
      "Average test loss: 0.004084577539107866\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10858756120999655\n",
      "Average test loss: 0.0039892197756303685\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10831968167093065\n",
      "Average test loss: 0.004011500522908237\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10833308327860303\n",
      "Average test loss: 0.004010029018753105\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10824082465304269\n",
      "Average test loss: 0.003998279108355443\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10785496880610784\n",
      "Average test loss: 0.004018279913812876\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10777786551581489\n",
      "Average test loss: 0.0039764361120760445\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1079381619559394\n",
      "Average test loss: 0.004096595187981923\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10765318635437224\n",
      "Average test loss: 0.004192150119278166\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10766508262024986\n",
      "Average test loss: 0.004002909752229849\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10735837857590781\n",
      "Average test loss: 0.004038795633448495\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10736542193094889\n",
      "Average test loss: 0.004010670709320241\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10707690285974078\n",
      "Average test loss: 0.003993935775218739\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10709650889370176\n",
      "Average test loss: 0.004126202383802997\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10699820005231434\n",
      "Average test loss: 0.004076434778049589\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10698367647661103\n",
      "Average test loss: 0.004089443494048384\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10684130071931415\n",
      "Average test loss: 0.004059464593314462\n",
      "Epoch 164/300\n",
      "Average training loss: 0.1069341265294287\n",
      "Average test loss: 0.004070004445604152\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10679268082645205\n",
      "Average test loss: 0.004076322893715567\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10662401722537147\n",
      "Average test loss: 0.004009393309139543\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1063286483420266\n",
      "Average test loss: 0.004147037391447359\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10637736813889609\n",
      "Average test loss: 0.004126581695137753\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1065900863872634\n",
      "Average test loss: 0.004027732991096046\n",
      "Epoch 170/300\n",
      "Average test loss: 0.0040453715287148955\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10591298959652583\n",
      "Average test loss: 0.004062870577805572\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10632863011625078\n",
      "Average test loss: 0.004061603721852104\n",
      "Epoch 174/300\n",
      "Average training loss: 0.1058479395839903\n",
      "Average test loss: 0.004098679125722911\n",
      "Epoch 175/300\n",
      "Average training loss: 0.1056337391336759\n",
      "Average test loss: 0.004080545083102253\n",
      "Epoch 176/300\n",
      "Average training loss: 0.1057384010222223\n",
      "Average test loss: 0.004030543488346868\n",
      "Epoch 177/300\n",
      "Average training loss: 0.105615424560176\n",
      "Average test loss: 0.004090585783951812\n",
      "Epoch 178/300\n",
      "Average training loss: 0.1055459353129069\n",
      "Average test loss: 0.004117097503609128\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10534708825084899\n",
      "Average test loss: 0.004099257570587926\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10532461190223694\n",
      "Average test loss: 0.0040306519013312125\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10518111545509762\n",
      "Average test loss: 0.004084978454228905\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10518370694584317\n",
      "Average test loss: 0.004063034415245056\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10518727098570929\n",
      "Average test loss: 0.004066344409353204\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10501977904637655\n",
      "Average test loss: 0.004075867098031772\n",
      "Epoch 185/300\n",
      "Average training loss: 0.1049246240456899\n",
      "Average test loss: 0.004076975852664974\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10475741120841768\n",
      "Average test loss: 0.004097111649811268\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10474153244495392\n",
      "Average test loss: 0.004090588507139021\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10482046798865001\n",
      "Average test loss: 0.004122753287355105\n",
      "Epoch 189/300\n",
      "Average training loss: 0.10460989779233933\n",
      "Average test loss: 0.004017377245757315\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10452038193411298\n",
      "Average test loss: 0.0040674003503388826\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10432409179872937\n",
      "Average test loss: 0.00412894155167871\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10424344773424997\n",
      "Average test loss: 0.004080353806830115\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1042353614502483\n",
      "Average test loss: 0.0041651679803099894\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10416881396373114\n",
      "Average test loss: 0.004090279777016904\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10425941085153156\n",
      "Average test loss: 0.004178316176765495\n",
      "Epoch 198/300\n",
      "Average training loss: 0.1039060572915607\n",
      "Average test loss: 0.004094491885147161\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10373589491181903\n",
      "Average test loss: 0.004131645468374094\n",
      "Epoch 200/300\n",
      "Average training loss: 0.10369701609346602\n",
      "Average test loss: 0.004097685907036066\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10373519991503821\n",
      "Average test loss: 0.0041016148765467934\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10375287675857545\n",
      "Average test loss: 0.00413692976327406\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10359326185782751\n",
      "Average test loss: 0.004078026198264625\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10367523439062966\n",
      "Average test loss: 0.0041017558816820385\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10357813527186711\n",
      "Average test loss: 0.0041614115857001805\n",
      "Epoch 206/300\n",
      "Average training loss: 0.103236757642693\n",
      "Average test loss: 0.004108251484731833\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10348862917555703\n",
      "Average test loss: 0.11782914296123717\n",
      "Epoch 208/300\n",
      "Average training loss: 0.10302060324615903\n",
      "Average test loss: 0.004143159127483765\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10306999199257956\n",
      "Average test loss: 0.004133985761553049\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10281580774651633\n",
      "Average test loss: 0.004117069015072452\n",
      "Epoch 213/300\n",
      "Average training loss: 0.10279138266377978\n",
      "Average test loss: 0.004126381432016691\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10297316376368204\n",
      "Average test loss: 0.004088930712805854\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10274424118465847\n",
      "Average test loss: 0.004084446385502815\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10303812887271245\n",
      "Average test loss: 0.004142158399853441\n",
      "Epoch 217/300\n",
      "Average training loss: 0.10251159299082226\n",
      "Average test loss: 0.004186094186372227\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10263515105512407\n",
      "Average test loss: 0.0041744284776763785\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10247498989767498\n",
      "Average test loss: 0.004167902503162622\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10237817203336291\n",
      "Average test loss: 0.004185521007825931\n",
      "Epoch 221/300\n",
      "Average training loss: 0.10225951257679197\n",
      "Average test loss: 0.004137745819364985\n",
      "Epoch 222/300\n",
      "Average training loss: 0.10233464197980033\n",
      "Average test loss: 0.004124348828776016\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10221451296408972\n",
      "Average test loss: 0.004080984311799208\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10215507174200482\n",
      "Average test loss: 0.004094673136042224\n",
      "Epoch 225/300\n",
      "Average training loss: 0.10196651072634591\n",
      "Average test loss: 0.0041378794742955105\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10214964012304942\n",
      "Average test loss: 0.004111661274400023\n",
      "Epoch 229/300\n",
      "Average training loss: 0.1018928365442488\n",
      "Average test loss: 0.004196069892495871\n",
      "Epoch 230/300\n",
      "Average training loss: 0.10169597172074847\n",
      "Average test loss: 0.004291044009642468\n",
      "Epoch 231/300\n",
      "Average training loss: 0.10173365184995863\n",
      "Average test loss: 0.0041375139707492455\n",
      "Epoch 232/300\n",
      "Average training loss: 0.10173520797491073\n",
      "Average test loss: 0.004111134280761083\n",
      "Epoch 233/300\n",
      "Average training loss: 0.10165158322784636\n",
      "Average test loss: 0.004060003666414155\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10151543937789069\n",
      "Average test loss: 0.004186021954649025\n",
      "Epoch 235/300\n",
      "Average training loss: 0.10165075859758589\n",
      "Average test loss: 0.0041165105452140175\n",
      "Epoch 236/300\n",
      "Average training loss: 0.10136990443865458\n",
      "Average test loss: 0.004170229451523887\n",
      "Epoch 237/300\n",
      "Average training loss: 0.10156434022055733\n",
      "Average test loss: 0.004167440535707606\n",
      "Epoch 238/300\n",
      "Average training loss: 0.101339088247882\n",
      "Average test loss: 0.0041700252395951085\n",
      "Epoch 239/300\n",
      "Average training loss: 0.10122835600376129\n",
      "Average test loss: 0.004161391037619777\n",
      "Epoch 240/300\n",
      "Average training loss: 0.10127397177616755\n",
      "Average test loss: 0.004141475905974706\n",
      "Epoch 241/300\n",
      "Average training loss: 0.10121548311577903\n",
      "Average test loss: 0.0043813528132935365\n",
      "Epoch 242/300\n",
      "Average training loss: 0.10123071520196067\n",
      "Average test loss: 0.004170500680597292\n",
      "Epoch 243/300\n",
      "Average training loss: 0.10091327599022124\n",
      "Average test loss: 0.004134857455061542\n",
      "Epoch 244/300\n",
      "Average training loss: 0.10097507210241424\n",
      "Average test loss: 0.004171684623178509\n",
      "Epoch 245/300\n",
      "Average training loss: 0.1010075177748998\n",
      "Average test loss: 0.004255760708202918\n",
      "Epoch 246/300\n",
      "Average training loss: 0.10109741608301799\n",
      "Average test loss: 0.004136437298109134\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10078238326973385\n",
      "Average test loss: 0.00428825480863452\n",
      "Epoch 248/300\n",
      "Average training loss: 0.10078455100456873\n",
      "Average test loss: 0.004122287142607901\n",
      "Epoch 251/300\n",
      "Average training loss: 0.10045075526502398\n",
      "Average test loss: 0.0041714149767325985\n",
      "Epoch 252/300\n",
      "Average training loss: 0.10063604854875141\n",
      "Average test loss: 0.0041861736482630175\n",
      "Epoch 253/300\n",
      "Average training loss: 0.10052038562960094\n",
      "Average test loss: 0.004177466151201062\n",
      "Epoch 254/300\n",
      "Average training loss: 0.10053240389956368\n",
      "Average test loss: 0.0041616151196261246\n",
      "Epoch 255/300\n",
      "Average training loss: 0.10053470781114367\n",
      "Average test loss: 0.004216434507527285\n",
      "Epoch 256/300\n",
      "Average training loss: 0.10042252796226078\n",
      "Average test loss: 0.004099418793494502\n",
      "Epoch 257/300\n",
      "Average training loss: 0.10030349213547177\n",
      "Average test loss: 15164.138035590278\n",
      "Epoch 258/300\n",
      "Average training loss: 0.10193325913614697\n",
      "Average test loss: 0.004179882392287254\n",
      "Epoch 259/300\n",
      "Average training loss: 0.10022790901528464\n",
      "Average test loss: 0.004219105679955747\n",
      "Epoch 260/300\n",
      "Average training loss: 0.10010947467221154\n",
      "Average test loss: 0.00424282331392169\n",
      "Epoch 261/300\n",
      "Average training loss: 0.10005744791693158\n",
      "Average test loss: 0.004199781901306576\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09995951910151375\n",
      "Average test loss: 0.004260566583110226\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09999869930081898\n",
      "Average test loss: 0.004221543458600839\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09988822385337617\n",
      "Average test loss: 0.004176446622444523\n",
      "Epoch 265/300\n",
      "Average test loss: 0.004367075153523021\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09976577660772536\n",
      "Average test loss: 0.004123745544917054\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09975665749443902\n",
      "Average test loss: 0.004207364598703053\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09972019463115268\n",
      "Average test loss: 0.004261504311528471\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09964666956000857\n",
      "Average test loss: 0.00421146368359526\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09996087615688642\n",
      "Average test loss: 0.004213302064687014\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0995775101184845\n",
      "Average test loss: 0.00418890969786379\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0995616944498486\n",
      "Average test loss: 0.004236989550913374\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09957029726770189\n",
      "Average test loss: 0.004305286078817315\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09966135031647153\n",
      "Average test loss: 0.004164096515211794\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09949148057566749\n",
      "Average test loss: 0.004201786768519216\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09938184360331959\n",
      "Average test loss: 0.004208847475134664\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09928671173254648\n",
      "Average test loss: 0.0042259202835460506\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09943147144052718\n",
      "Average test loss: 0.004307588407562839\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0992018502884441\n",
      "Average test loss: 0.004240519687533378\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09929576066467496\n",
      "Average test loss: 0.0042927885976516535\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09926821666293674\n",
      "Average test loss: 0.004118369602908691\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09897671272026168\n",
      "Average test loss: 0.00422184210187859\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09912292709615496\n",
      "Average test loss: 0.004197195504688555\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09904934637082947\n",
      "Average test loss: 0.00423783609457314\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09898404535982344\n",
      "Average test loss: 0.00422464866273933\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09882605081134373\n",
      "Average test loss: 0.0046911505626307595\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09879775829116504\n",
      "Average test loss: 0.004230247802618477\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09873563075065613\n",
      "Average test loss: 0.004238765681369437\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09894782584905625\n",
      "Average test loss: 0.004238849496675862\n",
      "Epoch 294/300\n",
      "Average training loss: 0.09868855237298542\n",
      "Average test loss: 0.004236930566736394\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09853230435318416\n",
      "Average test loss: 0.004230873081626164\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09863768917322159\n",
      "Average test loss: 0.00425063976769646\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09860650509595871\n",
      "Average test loss: 0.004203318601060245\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09846877131859462\n",
      "Average test loss: 0.004272021221410897\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09850236659579807\n",
      "Average test loss: 0.004124818235635757\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09858062709702385\n",
      "Average test loss: 0.0043064487928317656\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.55298415406545\n",
      "Average test loss: 0.00620522818259067\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2422651416990491\n",
      "Average test loss: 0.004960028048604727\n",
      "Epoch 3/300\n",
      "Average training loss: 0.6112718136575487\n",
      "Average test loss: 0.004631521970654528\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3969217106236352\n",
      "Average test loss: 0.004419314277254873\n",
      "Epoch 5/300\n",
      "Average training loss: 0.29009454153643716\n",
      "Average test loss: 0.0043355740027295215\n",
      "Epoch 6/300\n",
      "Average training loss: 0.22619087541103364\n",
      "Average test loss: 0.004385401416156027\n",
      "Epoch 7/300\n",
      "Average training loss: 0.18983597718344794\n",
      "Average test loss: 0.004220622868794534\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1689069023132324\n",
      "Average test loss: 0.004256891382651197\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1387716145184305\n",
      "Average test loss: 0.0037868911089996497\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1332676202389929\n",
      "Average test loss: 0.003737390004719297\n",
      "Epoch 13/300\n",
      "Average training loss: 0.12860482991404004\n",
      "Average test loss: 0.003706820828633176\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12475475412607193\n",
      "Average test loss: 0.0036120363424221673\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12141859934065077\n",
      "Average test loss: 0.003573283380932278\n",
      "Epoch 16/300\n",
      "Average training loss: 0.11865749460458755\n",
      "Average test loss: 0.0035788606482868393\n",
      "Epoch 17/300\n",
      "Average training loss: 0.11611918107668559\n",
      "Average test loss: 0.0034841723086105453\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11399182305733363\n",
      "Average test loss: 0.003451517049637106\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1120781882736418\n",
      "Average test loss: 0.0034852292055471074\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11035316342777676\n",
      "Average test loss: 0.003371893892478612\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10860485607385635\n",
      "Average test loss: 0.0034158976620270146\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10722355428669188\n",
      "Average test loss: 0.0032870187144726517\n",
      "Epoch 23/300\n",
      "Average training loss: 0.10597459438112047\n",
      "Average test loss: 0.0032773224684513277\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10489782587687174\n",
      "Average test loss: 0.003244300216022465\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1018204593790902\n",
      "Average test loss: 0.0031641423518045083\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1006909162402153\n",
      "Average test loss: 0.0031564088656256596\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09990135919385486\n",
      "Average test loss: 0.0031586202467895215\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09914892494678497\n",
      "Average test loss: 0.0031292153379569453\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09822814603646596\n",
      "Average test loss: 0.003081291676291989\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09758821274174584\n",
      "Average test loss: 0.0030963246735433735\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09683062416315079\n",
      "Average test loss: 0.003083747001985709\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09626787081029681\n",
      "Average test loss: 0.0030621963286151486\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09567533651987711\n",
      "Average test loss: 0.003091882970184088\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09487891960806316\n",
      "Average test loss: 0.0030332800400339893\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09444190946552489\n",
      "Average test loss: 0.0030385753334396417\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09389774870210224\n",
      "Average test loss: 0.003138085367778937\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09335598352882597\n",
      "Average test loss: 0.003055135367127756\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09297827311356863\n",
      "Average test loss: 0.002993256261572242\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0924893746442265\n",
      "Average test loss: 0.003003073007075323\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09204390919208527\n",
      "Average test loss: 0.0030780839667552047\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09161482818259133\n",
      "Average test loss: 0.0029823267083201144\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09125427693128586\n",
      "Average test loss: 0.0030274680041604573\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0909392493168513\n",
      "Average test loss: 0.0029676489217413797\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09055219966835445\n",
      "Average test loss: 0.0029554438789685567\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0902132670349545\n",
      "Average test loss: 0.0029457374099228118\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08983181569973628\n",
      "Average test loss: 0.003013853521603677\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08961437505814764\n",
      "Average test loss: 0.0029654896143409943\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08926903426647187\n",
      "Average test loss: 0.002944276559477051\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08899503971470726\n",
      "Average test loss: 0.0029507790281333855\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08875283192925983\n",
      "Average test loss: 0.0029320057773341737\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08844886551962959\n",
      "Average test loss: 0.002955067752963967\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08818929509984122\n",
      "Average test loss: 0.0029357026272142928\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08803879897461997\n",
      "Average test loss: 0.0029125304518060552\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08778088453743194\n",
      "Average test loss: 0.002935143827978108\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08753418347570631\n",
      "Average test loss: 0.002931107673794031\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08731552492247688\n",
      "Average test loss: 0.0029448331768314046\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08707065450482898\n",
      "Average test loss: 0.0029002659055921767\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08682522053188747\n",
      "Average test loss: 0.0029265145818806355\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08663639384508133\n",
      "Average test loss: 0.002900480955011315\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08646742216746012\n",
      "Average test loss: 0.0029071478091387284\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08610809049341414\n",
      "Average test loss: 0.0028914077900764014\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08601530561182234\n",
      "Average test loss: 0.002885472746152017\n",
      "Epoch 65/300\n",
      "Average training loss: 0.085697731104162\n",
      "Average test loss: 0.0029114689893192714\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08549667197465896\n",
      "Average test loss: 0.0028931210988925563\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08541611991326015\n",
      "Average test loss: 0.002924302671725551\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0851799465086725\n",
      "Average test loss: 0.002899101494294074\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08490520978636212\n",
      "Average test loss: 0.002907114748325613\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08477420638667213\n",
      "Average test loss: 0.0028834527155591383\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08454780902465185\n",
      "Average test loss: 0.002929152888349361\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08440840731726752\n",
      "Average test loss: 0.002914772167180975\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0842114871111181\n",
      "Average test loss: 0.002906734011446436\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08399649188584751\n",
      "Average test loss: 0.0028866223293460076\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08376281765434478\n",
      "Average test loss: 0.0028984875565187798\n",
      "Epoch 76/300\n",
      "Average training loss: 0.083635523253017\n",
      "Average test loss: 0.0028815638901044925\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08333784496453073\n",
      "Average test loss: 0.002893636339654525\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08321169666449228\n",
      "Average test loss: 0.0029351865253928635\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08311007322536575\n",
      "Average test loss: 0.002900395764865809\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08288407361507416\n",
      "Average test loss: 0.0028872499658415714\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0825763335691558\n",
      "Average test loss: 0.0028996715186577703\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08252972861131032\n",
      "Average test loss: 0.0028963117019997703\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08230330031779078\n",
      "Average test loss: 0.0029169166725542812\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0821137504643864\n",
      "Average test loss: 0.002903080675750971\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08194973066780302\n",
      "Average test loss: 0.0029139025762884153\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0818062426480982\n",
      "Average test loss: 0.0029352707008624243\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08153420028421614\n",
      "Average test loss: 0.0029000739326907527\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0815442528757784\n",
      "Average test loss: 0.0029552711698537073\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08135982772045665\n",
      "Average test loss: 0.0029306051898747684\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08114777626925045\n",
      "Average test loss: 0.0029658638731473023\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08095078221294615\n",
      "Average test loss: 0.002892960220678813\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08079269279374017\n",
      "Average test loss: 0.0028982378391342032\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0805577662454711\n",
      "Average test loss: 0.002909465990960598\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08029439361890157\n",
      "Average test loss: 0.002984224916746219\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0802408185005188\n",
      "Average test loss: 0.002898850142541859\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08006036856439379\n",
      "Average test loss: 0.0029048306999935043\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0799346563021342\n",
      "Average test loss: 0.0029306811884873443\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07986448656519254\n",
      "Average test loss: 0.0029038334252933663\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07956739225652483\n",
      "Average test loss: 0.0029144963706947036\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07944827641381158\n",
      "Average test loss: 0.0029209993278814686\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07930151648985015\n",
      "Average test loss: 0.002926263705516855\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07913285744190215\n",
      "Average test loss: 0.0029963079140418107\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07906269796358215\n",
      "Average test loss: 0.002922973558720615\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07898639083902041\n",
      "Average test loss: 0.0029596170058680906\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0787698714600669\n",
      "Average test loss: 0.0030904247293041813\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07851114685336749\n",
      "Average test loss: 0.002895373204102119\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0783691879577107\n",
      "Average test loss: 0.002901624758210447\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07820131325390604\n",
      "Average test loss: 0.002994521515443921\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07812172866529889\n",
      "Average test loss: 0.0029547379410101307\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0779537611371941\n",
      "Average test loss: 0.002973470391705632\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07777170493867662\n",
      "Average test loss: 0.002912032291189664\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07762499711248609\n",
      "Average test loss: 0.002912966280347771\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07746590438816282\n",
      "Average test loss: 0.002991784548179971\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07739469819598728\n",
      "Average test loss: 0.002933399093233877\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0772749318877856\n",
      "Average test loss: 0.0029601444775859513\n",
      "Epoch 116/300\n",
      "Average training loss: 0.077710295425521\n",
      "Average test loss: 0.0029509893651637766\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07707392079962624\n",
      "Average test loss: 0.0029788173298454946\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07681790767775641\n",
      "Average test loss: 0.0029497559503134756\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0766145305302408\n",
      "Average test loss: 0.002932427177619603\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07671624371740553\n",
      "Average test loss: 0.0030135294989579254\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0766066235171424\n",
      "Average test loss: 0.002944850449139873\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07638089784648683\n",
      "Average test loss: 0.0029678929220471116\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07620788908667035\n",
      "Average test loss: 0.002953042031576236\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07594432204299503\n",
      "Average test loss: 0.002966601507117351\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07609070287810432\n",
      "Average test loss: 0.002972273173638516\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0758884781897068\n",
      "Average test loss: 0.0030289625188128816\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0757648506462574\n",
      "Average test loss: 0.0029769154466274712\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07559474570883645\n",
      "Average test loss: 0.002999717437869145\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07548900138669544\n",
      "Average test loss: 0.0030511929616332052\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0756288325852818\n",
      "Average test loss: 0.002974434232339263\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07520194555653466\n",
      "Average test loss: 0.0029674347380383146\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07515808475348684\n",
      "Average test loss: 0.0030025240251173578\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07516350577937232\n",
      "Average test loss: 0.0030962037514481277\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07505209509531657\n",
      "Average test loss: 0.0030171852275315257\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07496565673086378\n",
      "Average test loss: 0.0030056026318214008\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07479660113652548\n",
      "Average test loss: 0.0029509375436852377\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07464490785532528\n",
      "Average test loss: 0.0030174078105224505\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07464305310779147\n",
      "Average test loss: 0.003001006271897091\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07460679091678725\n",
      "Average test loss: 0.003074796025123861\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07442200308375889\n",
      "Average test loss: 0.003082662410620186\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07410464085804092\n",
      "Average test loss: 0.003010085040496455\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07417681047651503\n",
      "Average test loss: 0.003060080835285286\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0740426877339681\n",
      "Average test loss: 0.0030376032185223368\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07395829706721836\n",
      "Average test loss: 0.0030243486201183664\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07384279343154695\n",
      "Average test loss: 0.003025811348317398\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07376502209239535\n",
      "Average test loss: 0.003018411306043466\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07369834464126163\n",
      "Average test loss: 0.003030243772599432\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0736305432650778\n",
      "Average test loss: 0.0030285678278240895\n",
      "Epoch 149/300\n",
      "Average training loss: 0.073556469268269\n",
      "Average test loss: 0.0030292270800305735\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07354578934775459\n",
      "Average test loss: 0.0030030590960135064\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0731750233537621\n",
      "Average test loss: 0.003095292633606328\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07329698505997657\n",
      "Average test loss: 0.0029962934909595383\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07317767661147648\n",
      "Average test loss: 0.0030629306878480647\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07297140663199955\n",
      "Average test loss: 0.003050009839857618\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07300705016652743\n",
      "Average test loss: 0.00300578949559066\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07292331554823452\n",
      "Average test loss: 0.0030467840416563883\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07281726998090744\n",
      "Average test loss: 0.0030209247022867204\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07276005387306213\n",
      "Average test loss: 0.002993138519840108\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07268389875690143\n",
      "Average test loss: 0.0030920627680089737\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07258038046624925\n",
      "Average test loss: 0.0029835534219940503\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07234307553370793\n",
      "Average test loss: 0.0030219632074650793\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07264399856991238\n",
      "Average test loss: 0.0030489830382996134\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07234476121928957\n",
      "Average test loss: 0.0030050246603786947\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07219537401199341\n",
      "Average test loss: 0.0030125005220373474\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07213205514351527\n",
      "Average test loss: 0.0030032332527140776\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07205464827683238\n",
      "Average test loss: 0.0030580075888170135\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07218789280785455\n",
      "Average test loss: 0.002991543942441543\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07207013487153584\n",
      "Average test loss: 0.0030848079493476285\n",
      "Epoch 169/300\n",
      "Average training loss: 0.071852360771762\n",
      "Average test loss: 0.002985402080333895\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0718913098110093\n",
      "Average test loss: 0.0030473539930664833\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0716599587963687\n",
      "Average test loss: 0.0030103489797976284\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07172639834218555\n",
      "Average test loss: 0.003103076471015811\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07156868226660623\n",
      "Average test loss: 0.0031576497027029594\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07172066327929497\n",
      "Average test loss: 0.0031134014524933367\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07145054915878508\n",
      "Average test loss: 0.0030889748788128298\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07154538689719306\n",
      "Average test loss: 0.0030861346024192043\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07126866732703314\n",
      "Average test loss: 0.003072956862548987\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07127539638015959\n",
      "Average test loss: 0.0030434673616869584\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07133086921109094\n",
      "Average test loss: 0.0030482146060094237\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07120834799607595\n",
      "Average test loss: 0.0030790393075181377\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07098575467533536\n",
      "Average test loss: 0.0030889948966602485\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07101726763115988\n",
      "Average test loss: 0.0030613163351598713\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07096497901942996\n",
      "Average test loss: 0.0030437152528514466\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07097081862886746\n",
      "Average test loss: 0.0030288833054817384\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07081878295871946\n",
      "Average test loss: 0.003076935511910253\n",
      "Epoch 186/300\n",
      "Average training loss: 0.070861038810677\n",
      "Average test loss: 0.003050464497672187\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0707282026277648\n",
      "Average test loss: 0.0030082844355040126\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07068250737587611\n",
      "Average test loss: 0.003017213130162822\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07069916529125637\n",
      "Average test loss: 0.0030605621633844242\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07049245475067033\n",
      "Average test loss: 0.00309179090646406\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07045485503143735\n",
      "Average test loss: 0.003040071827876899\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07051193039947086\n",
      "Average test loss: 0.003060203186339802\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07026581876145468\n",
      "Average test loss: 0.0030841297530051735\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07039403438568115\n",
      "Average test loss: 0.0030669914103216597\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07039703730742136\n",
      "Average test loss: 0.0030953268957220844\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07033067450920741\n",
      "Average test loss: 0.003144383327000671\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0702232066459126\n",
      "Average test loss: 0.003098766590985987\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07004159770740404\n",
      "Average test loss: 0.00305015025805268\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06995350169804361\n",
      "Average test loss: 0.003036433156372772\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06993514921267828\n",
      "Average test loss: 0.003064529788369934\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06979932399921947\n",
      "Average test loss: 0.0030521725463784405\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06992641527454058\n",
      "Average test loss: 0.003075825877073738\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06977769134110874\n",
      "Average test loss: 0.003057638528032435\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06973767657081287\n",
      "Average test loss: 0.0030903917495161293\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0698001639313168\n",
      "Average test loss: 0.0030725508100456662\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06964932458930545\n",
      "Average test loss: 0.0030469718200878963\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0697127866215176\n",
      "Average test loss: 0.0030585096896522577\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06980248478386138\n",
      "Average test loss: 0.0030676670894026754\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0696152518524064\n",
      "Average test loss: 0.0030790007648368676\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06948264775011274\n",
      "Average test loss: 0.0031137938871979715\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06933464241690106\n",
      "Average test loss: 0.0031165904396524033\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06936007489760716\n",
      "Average test loss: 0.0030854599961183137\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06926251975695293\n",
      "Average test loss: 0.0031390801632983816\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0693194979959064\n",
      "Average test loss: 0.0031156982976115414\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06924736863705847\n",
      "Average test loss: 0.0031509732908258834\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06916176842980915\n",
      "Average test loss: 0.0031229104494882956\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06921970316105419\n",
      "Average test loss: 0.0031253298392726315\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06907556343740887\n",
      "Average test loss: 0.003112207675559653\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06898933411969078\n",
      "Average test loss: 0.003073716315958235\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06885917601651616\n",
      "Average test loss: 0.0031409807287984424\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06888928771350118\n",
      "Average test loss: 0.0031268927570846346\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06890988354881604\n",
      "Average test loss: 0.003115067257856329\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06899836093187332\n",
      "Average test loss: 0.003091537393733031\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06881983345415857\n",
      "Average test loss: 0.0031053154178791575\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06877833015057776\n",
      "Average test loss: 0.0031032404872692295\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06864287003543641\n",
      "Average test loss: 0.003068843076419499\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06876801423231761\n",
      "Average test loss: 0.0030744375261581606\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0686110413107607\n",
      "Average test loss: 0.003079083300919996\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06862100227673848\n",
      "Average test loss: 0.003038320842302508\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0684580540060997\n",
      "Average test loss: 0.0031469846988717715\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06855619921949174\n",
      "Average test loss: 0.003084316525608301\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06837492702404659\n",
      "Average test loss: 0.0031634319365645446\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06839216871394052\n",
      "Average test loss: 0.003101128306446804\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06852790022889774\n",
      "Average test loss: 0.003230627968079514\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06830047910412153\n",
      "Average test loss: 0.00309588535502553\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06834599157505565\n",
      "Average test loss: 0.0030829677668710548\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06823905624283684\n",
      "Average test loss: 0.0031389132171041434\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06808161140812768\n",
      "Average test loss: 0.0030839986987411974\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06804754545953538\n",
      "Average test loss: 0.003130422219220135\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06804767017894321\n",
      "Average test loss: 0.0031379629228678015\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0681047506497966\n",
      "Average test loss: 0.0031225582746167977\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06805085748434067\n",
      "Average test loss: 0.003091364717938834\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06801403979460398\n",
      "Average test loss: 0.0031088182715078194\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06783728165096707\n",
      "Average test loss: 0.003117552918485469\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06786622150739034\n",
      "Average test loss: 0.0031292965314868425\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06803837227821351\n",
      "Average test loss: 0.003091981497179303\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06771451390451855\n",
      "Average test loss: 0.003123256136766738\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06780448407597012\n",
      "Average test loss: 0.0032157229349638024\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0677381126417054\n",
      "Average test loss: 0.003082291204482317\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06758643366230858\n",
      "Average test loss: 0.0031430658331761756\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06768729485405817\n",
      "Average test loss: 0.0031242206510570312\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06769971873693996\n",
      "Average test loss: 0.0031710446319646304\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06760928148362372\n",
      "Average test loss: 0.003175765306999286\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06750183896554841\n",
      "Average test loss: 0.00309857142282029\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06751923514405886\n",
      "Average test loss: 0.0031196514434284634\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06752550137374136\n",
      "Average test loss: 0.003330816440698173\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0675019765926732\n",
      "Average test loss: 0.003174178378449546\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06740732167164484\n",
      "Average test loss: 0.0031602055929187273\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06736471542384889\n",
      "Average test loss: 0.003191628351393673\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06721981996960111\n",
      "Average test loss: 0.0032295372751024033\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06731749689247873\n",
      "Average test loss: 0.00314050744763679\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06723510849475861\n",
      "Average test loss: 0.0031284420719991128\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06731750412450896\n",
      "Average test loss: 0.003128806848803328\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06720392393403583\n",
      "Average test loss: 0.003217388956911034\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06721774965524674\n",
      "Average test loss: 0.003151412968006399\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0673148108555211\n",
      "Average test loss: 0.0031658129157084557\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06720334499412113\n",
      "Average test loss: 0.003072533413767815\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06714263929592239\n",
      "Average test loss: 0.0031585533598230946\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06712315378917588\n",
      "Average test loss: 0.0031567639112472534\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06706625193688605\n",
      "Average test loss: 0.003125523190531466\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06707132800088988\n",
      "Average test loss: 0.0031383716844850117\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06715474135014746\n",
      "Average test loss: 0.0031137555841770437\n",
      "Epoch 275/300\n",
      "Average training loss: 0.066886353817251\n",
      "Average test loss: 0.003168712611206704\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06689253368642595\n",
      "Average test loss: 0.0031742261855138673\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0669537339674102\n",
      "Average test loss: 0.0031747433692216873\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06689968083964454\n",
      "Average test loss: 0.003236347672632999\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06674280305041207\n",
      "Average test loss: 0.003118383027613163\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06676870483491155\n",
      "Average test loss: 0.0031044145048492486\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06692712099353472\n",
      "Average test loss: 0.0031621035223619804\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0667549949089686\n",
      "Average test loss: 0.0031412904721995196\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06672211145030128\n",
      "Average test loss: 0.0030858539135919676\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06657730984025531\n",
      "Average test loss: 0.003171979136765003\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06667236843705178\n",
      "Average test loss: 0.0031376971790774\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0667183911535475\n",
      "Average test loss: 0.0031496193731824556\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06663047350115246\n",
      "Average test loss: 0.0031507355843981106\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06653669785128699\n",
      "Average test loss: 0.0031406875832213294\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06646916935178969\n",
      "Average test loss: 0.0031566629165576563\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06642085040277905\n",
      "Average test loss: 0.0031034326151841217\n",
      "Epoch 291/300\n",
      "Average training loss: 0.066413429978821\n",
      "Average test loss: 0.003173286918964651\n",
      "Epoch 292/300\n",
      "Average training loss: 0.066546940356493\n",
      "Average test loss: 0.003095711842179298\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06653835446967019\n",
      "Average test loss: 0.0031445281378303966\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06636779192421172\n",
      "Average test loss: 0.0031414871385527982\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06634004027313656\n",
      "Average test loss: 0.0030990613320221504\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06635386012328996\n",
      "Average test loss: 0.0031900234191368023\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06633063280582428\n",
      "Average test loss: 0.0031405798186444575\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06628310446606742\n",
      "Average test loss: 0.003140834257627527\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06620987164643076\n",
      "Average test loss: 0.0031704611103567813\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06610536276631886\n",
      "Average test loss: 0.0031964360003670055\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.588768948554993\n",
      "Average test loss: 0.006142705999728706\n",
      "Epoch 2/300\n",
      "Average training loss: 0.894493481265174\n",
      "Average test loss: 0.004879074056943258\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4433784510029687\n",
      "Average test loss: 0.004363410330480999\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2896296180089315\n",
      "Average test loss: 0.003993844385362334\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2171710136599011\n",
      "Average test loss: 0.0037798053849902417\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17935448887613084\n",
      "Average test loss: 0.0036540060631102987\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1569909575647778\n",
      "Average test loss: 0.004108418349383606\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14261028683185578\n",
      "Average test loss: 0.003444421437051561\n",
      "Epoch 9/300\n",
      "Average training loss: 0.13217879417207506\n",
      "Average test loss: 0.0033705645565771394\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12423964553409153\n",
      "Average test loss: 0.003288076442769832\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11791335629754596\n",
      "Average test loss: 0.0031409486302485068\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11268029814296299\n",
      "Average test loss: 0.0031177450385358597\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10805051014158461\n",
      "Average test loss: 0.0030968606625166204\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10422380319568846\n",
      "Average test loss: 0.00291708561219275\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1007520534992218\n",
      "Average test loss: 0.0028673070304923586\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09772059892283545\n",
      "Average test loss: 0.002843746757755677\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09505936156378852\n",
      "Average test loss: 0.0027399521955392427\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09267030755678812\n",
      "Average test loss: 0.002677696593105793\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09043278574943542\n",
      "Average test loss: 0.002648504961695936\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08847314412726297\n",
      "Average test loss: 0.0025532433479610417\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08656234525309668\n",
      "Average test loss: 0.0025367626843767034\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0848948039677408\n",
      "Average test loss: 0.002473988665681746\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08341731457577811\n",
      "Average test loss: 0.0024474785142681666\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0820196886294418\n",
      "Average test loss: 0.002420138130378392\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08062259468767378\n",
      "Average test loss: 0.0024081690965427295\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07950944827331437\n",
      "Average test loss: 0.0023817530162632467\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07845873577064938\n",
      "Average test loss: 0.0023491797575520145\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07729361483785842\n",
      "Average test loss: 0.0023475905978638267\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07645578104919858\n",
      "Average test loss: 0.0023650832436978818\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07561431484089957\n",
      "Average test loss: 0.002407502255299025\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07472990741332372\n",
      "Average test loss: 0.0022994302024857867\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07411015387045013\n",
      "Average test loss: 0.002283631042473846\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07331244249476326\n",
      "Average test loss: 0.0022995277730127176\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07293676275014878\n",
      "Average test loss: 0.002273028907676538\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07202385533849398\n",
      "Average test loss: 0.0022652409130500422\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07155417113833958\n",
      "Average test loss: 0.002300308490378989\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07096996136175261\n",
      "Average test loss: 0.002233818900254038\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07048079971472422\n",
      "Average test loss: 0.0022174141487727563\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06989914593431684\n",
      "Average test loss: 0.0022117848083790807\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06955766940116882\n",
      "Average test loss: 0.002201195197386874\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06904585809840097\n",
      "Average test loss: 0.0022276073886702457\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06868985051578945\n",
      "Average test loss: 0.002197276044843925\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06841308075851864\n",
      "Average test loss: 0.002195700541138649\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06808952374590768\n",
      "Average test loss: 0.0021657057779116765\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06753468937344022\n",
      "Average test loss: 0.0021623584661218855\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06728604788250393\n",
      "Average test loss: 0.0021883511085891063\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06694178913699256\n",
      "Average test loss: 0.0021705675092008377\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06681640907128653\n",
      "Average test loss: 0.0021425504897617633\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06635912049810092\n",
      "Average test loss: 0.00214756796695292\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06613482112354703\n",
      "Average test loss: 0.0021454039425071745\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06587404485543569\n",
      "Average test loss: 0.002140784889045689\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06567004566391309\n",
      "Average test loss: 0.0021220898119111856\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06531739900840654\n",
      "Average test loss: 0.002125622129171259\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06512538644009166\n",
      "Average test loss: 0.0021226361100044515\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06481535618503888\n",
      "Average test loss: 0.002128857724885974\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06461040512389607\n",
      "Average test loss: 0.002114139125165012\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06440977323717541\n",
      "Average test loss: 0.0021114316512313154\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06415918951564364\n",
      "Average test loss: 0.0021033499992142123\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06404598340060975\n",
      "Average test loss: 0.002130684042452938\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0637762009203434\n",
      "Average test loss: 0.0021199088796145385\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0636104373368952\n",
      "Average test loss: 0.0021038530086063675\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06340448990795347\n",
      "Average test loss: 0.0021218823784341415\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06319374116592937\n",
      "Average test loss: 0.0020978478007018567\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06297882599963082\n",
      "Average test loss: 0.002101288616553777\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06272055121262868\n",
      "Average test loss: 0.002110608860850334\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06261869565314716\n",
      "Average test loss: 0.0021004083034478955\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06244817686412069\n",
      "Average test loss: 0.002143691213387582\n",
      "Epoch 68/300\n",
      "Average training loss: 0.062236887719896106\n",
      "Average test loss: 0.002095087990164757\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06208300280902121\n",
      "Average test loss: 0.002114114925679233\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06200632908278041\n",
      "Average test loss: 0.002148769994990693\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06173288525806533\n",
      "Average test loss: 0.0021292937205483514\n",
      "Epoch 72/300\n",
      "Average training loss: 0.061504985809326174\n",
      "Average test loss: 0.0020964736732550795\n",
      "Epoch 73/300\n",
      "Average training loss: 0.061391773566603663\n",
      "Average test loss: 0.002097904770945509\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06122121508253945\n",
      "Average test loss: 0.002092866738223367\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06108303137951427\n",
      "Average test loss: 0.0021005889880988332\n",
      "Epoch 76/300\n",
      "Average training loss: 0.060846071273088456\n",
      "Average test loss: 0.0021071020297499165\n",
      "Epoch 77/300\n",
      "Average training loss: 0.060853490322828295\n",
      "Average test loss: 0.002100262817305823\n",
      "Epoch 78/300\n",
      "Average training loss: 0.060440433161126246\n",
      "Average test loss: 0.0020892337018416987\n",
      "Epoch 79/300\n",
      "Average training loss: 0.060300452394617926\n",
      "Average test loss: 0.002090407204710775\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06022747148076693\n",
      "Average test loss: 0.0021002387369258536\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06007650999559296\n",
      "Average test loss: 0.002090693120741182\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05980646252632141\n",
      "Average test loss: 0.0020921282635794746\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05967329819997152\n",
      "Average test loss: 0.002093577248768674\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05959908367196719\n",
      "Average test loss: 0.0020972134256735443\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05938506075077587\n",
      "Average test loss: 0.0021038599953883226\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05924849900603294\n",
      "Average test loss: 0.002104507571293248\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05915144972668754\n",
      "Average test loss: 0.0021394707050381434\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05888843241333962\n",
      "Average test loss: 0.00208385349282374\n",
      "Epoch 89/300\n",
      "Average training loss: 0.058700577328602475\n",
      "Average test loss: 0.0021408464442938565\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05864688350094689\n",
      "Average test loss: 0.0021058076274477772\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05840449884533882\n",
      "Average test loss: 0.002095270121159653\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05842146718502045\n",
      "Average test loss: 0.002135814560370313\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05819215082791117\n",
      "Average test loss: 0.002106568895487322\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05799546361962954\n",
      "Average test loss: 0.002100861048946778\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05788689000076718\n",
      "Average test loss: 0.0021077332142740487\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0578474985924032\n",
      "Average test loss: 0.002135008453288012\n",
      "Epoch 97/300\n",
      "Average training loss: 0.057671668380498885\n",
      "Average test loss: 0.0021165803110020026\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05740011505285899\n",
      "Average test loss: 0.0021158266798075704\n",
      "Epoch 99/300\n",
      "Average training loss: 0.057308444907267886\n",
      "Average test loss: 0.002118070690996117\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05710098207990329\n",
      "Average test loss: 0.002119856778014865\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05712624684969584\n",
      "Average test loss: 0.0020968080123679503\n",
      "Epoch 102/300\n",
      "Average training loss: 0.056884795222017503\n",
      "Average test loss: 0.002121274687970678\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05682644083102544\n",
      "Average test loss: 0.0021171894643662705\n",
      "Epoch 104/300\n",
      "Average training loss: 0.056746044016546676\n",
      "Average test loss: 0.0021492137039701145\n",
      "Epoch 105/300\n",
      "Average training loss: 0.056516964515050255\n",
      "Average test loss: 0.002131357105448842\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05640812826487753\n",
      "Average test loss: 0.002128385393983788\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05624126736323039\n",
      "Average test loss: 0.0021361074718750186\n",
      "Epoch 108/300\n",
      "Average training loss: 0.056248778104782106\n",
      "Average test loss: 0.0021228997664940025\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05607718767060174\n",
      "Average test loss: 0.0021091568952219353\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05610683025916417\n",
      "Average test loss: 0.002150565708780454\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0557913896938165\n",
      "Average test loss: 0.0021562336325231524\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05567564348710908\n",
      "Average test loss: 0.0021185040500842863\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05563557853632503\n",
      "Average test loss: 0.0021444497337150905\n",
      "Epoch 114/300\n",
      "Average training loss: 0.055505838880936305\n",
      "Average test loss: 0.0021277814807577266\n",
      "Epoch 115/300\n",
      "Average training loss: 0.055328095485766726\n",
      "Average test loss: 0.0021273955137779315\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05518498925699128\n",
      "Average test loss: 0.0021564061132570106\n",
      "Epoch 117/300\n",
      "Average training loss: 0.055134000404013526\n",
      "Average test loss: 0.0021335867125954894\n",
      "Epoch 118/300\n",
      "Average training loss: 0.054948761810859045\n",
      "Average test loss: 0.0021477507673617864\n",
      "Epoch 119/300\n",
      "Average training loss: 0.054967987494336235\n",
      "Average test loss: 0.0021223251519517767\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05473297797640165\n",
      "Average test loss: 0.002133033920907312\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0546058910422855\n",
      "Average test loss: 0.0021567618917259905\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05459103567732705\n",
      "Average test loss: 0.0021569856208645635\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05465765223900477\n",
      "Average test loss: 0.00218946108325488\n",
      "Epoch 124/300\n",
      "Average training loss: 0.054428574436240724\n",
      "Average test loss: 0.0021891145987643136\n",
      "Epoch 125/300\n",
      "Average training loss: 0.054279219213459226\n",
      "Average test loss: 0.0021929291393607856\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05427056599987878\n",
      "Average test loss: 0.0021852077390584683\n",
      "Epoch 127/300\n",
      "Average training loss: 0.054178944670491745\n",
      "Average test loss: 0.002142829940964778\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05392343079712656\n",
      "Average test loss: 0.002184230358960728\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05386198703116841\n",
      "Average test loss: 0.002206854734155867\n",
      "Epoch 130/300\n",
      "Average training loss: 0.054016213456789655\n",
      "Average test loss: 0.0022014171443879606\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05363667462600602\n",
      "Average test loss: 0.0022329644332122474\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05364801112148497\n",
      "Average test loss: 0.0021812279048479266\n",
      "Epoch 133/300\n",
      "Average training loss: 0.053566587769322924\n",
      "Average test loss: 0.002152060845038957\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05333294453554683\n",
      "Average test loss: 0.002156799319717619\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05322889707816972\n",
      "Average test loss: 0.0021581629991738334\n",
      "Epoch 136/300\n",
      "Average training loss: 0.053277342938714556\n",
      "Average test loss: 0.002159392450315257\n",
      "Epoch 137/300\n",
      "Average training loss: 0.053160762829913034\n",
      "Average test loss: 0.0021529956575897006\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05311659806635645\n",
      "Average test loss: 0.002200960565979282\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05300740409559674\n",
      "Average test loss: 0.002251937047888835\n",
      "Epoch 140/300\n",
      "Average training loss: 0.052840541183948514\n",
      "Average test loss: 0.002144450725780593\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05281746555036969\n",
      "Average test loss: 0.002197926494603356\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0529768195980125\n",
      "Average test loss: 0.0021560452222410174\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05268004052506553\n",
      "Average test loss: 0.0022132372606752646\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05269482928183344\n",
      "Average test loss: 0.0021714255086456736\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05245319948262638\n",
      "Average test loss: 0.002174792639911175\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05245099041859309\n",
      "Average test loss: 0.002198662659360303\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05236910312043296\n",
      "Average test loss: 0.0021890736904202237\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05217874526315265\n",
      "Average test loss: 0.0021633835087219873\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05226553446716732\n",
      "Average test loss: 0.0021816234245068495\n",
      "Epoch 150/300\n",
      "Average training loss: 0.052083817021714315\n",
      "Average test loss: 0.002243211122436656\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05204332553346952\n",
      "Average test loss: 0.002195840810942981\n",
      "Epoch 152/300\n",
      "Average training loss: 0.052042263040939965\n",
      "Average test loss: 0.0022428914529995787\n",
      "Epoch 153/300\n",
      "Average training loss: 0.051920750512017146\n",
      "Average test loss: 0.0021876586614590554\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05178330151240031\n",
      "Average test loss: 0.002198795501349701\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05177993252873421\n",
      "Average test loss: 0.002174276740186744\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05167433618174659\n",
      "Average test loss: 0.002350425773817632\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05160156347850958\n",
      "Average test loss: 0.0022440898327363863\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05158829341994391\n",
      "Average test loss: 0.002215380078699026\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05150100975897577\n",
      "Average test loss: 0.002181569042822553\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0514662506878376\n",
      "Average test loss: 0.002236861685083972\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05146896530522241\n",
      "Average test loss: 0.0022571525163948536\n",
      "Epoch 162/300\n",
      "Average training loss: 0.051366603076457976\n",
      "Average test loss: 0.0022133726091641518\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05125374743011263\n",
      "Average test loss: 0.0021673545579736432\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05134492606255743\n",
      "Average test loss: 0.0022099237892155847\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05117332238290045\n",
      "Average test loss: 0.0022251240726974277\n",
      "Epoch 166/300\n",
      "Average training loss: 0.051019716060823866\n",
      "Average test loss: 0.002185752478117744\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05105549830198288\n",
      "Average test loss: 0.0022258519931799836\n",
      "Epoch 168/300\n",
      "Average training loss: 0.050904409488042195\n",
      "Average test loss: 0.00221496540080342\n",
      "Epoch 169/300\n",
      "Average training loss: 0.050934214519129856\n",
      "Average test loss: 0.0022097732747594517\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05095618256264263\n",
      "Average test loss: 0.0022320958490793905\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05086096320384079\n",
      "Average test loss: 0.0023735431811461845\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0507929986582862\n",
      "Average test loss: 0.002223388678601219\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05063915459977256\n",
      "Average test loss: 0.0022519821574290594\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05059893602132797\n",
      "Average test loss: 0.002220267277624872\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05060612804690997\n",
      "Average test loss: 0.0022375726271420715\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05036817535426882\n",
      "Average test loss: 0.0021946322810318733\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05044303575820393\n",
      "Average test loss: 0.002257081231723229\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0504455691576004\n",
      "Average test loss: 0.0022108470181831054\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05043919671244092\n",
      "Average test loss: 0.0022302135722711684\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05032314532995224\n",
      "Average test loss: 0.002257920089074307\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0502168647216426\n",
      "Average test loss: 0.0022777149096752207\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05025583539572027\n",
      "Average test loss: 0.002231025607428617\n",
      "Epoch 183/300\n",
      "Average training loss: 0.050084960334830814\n",
      "Average test loss: 0.0023361314666560952\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0500998847981294\n",
      "Average test loss: 0.00222141778220733\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05010039512316386\n",
      "Average test loss: 0.002229398954142299\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0499918709827794\n",
      "Average test loss: 0.002184261112577385\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05001512979136573\n",
      "Average test loss: 0.0022099857955343192\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04986581167909834\n",
      "Average test loss: 0.0023217106062091057\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04996390151977539\n",
      "Average test loss: 0.002270770457883676\n",
      "Epoch 190/300\n",
      "Average training loss: 0.049779681033558315\n",
      "Average test loss: 0.0022189486438615453\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04982199110256301\n",
      "Average test loss: 0.0023874802401082384\n",
      "Epoch 192/300\n",
      "Average training loss: 0.049810587479008565\n",
      "Average test loss: 0.0022055830522957776\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04970802837610245\n",
      "Average test loss: 0.0022696837302711276\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04976708690325419\n",
      "Average test loss: 0.0022111155836739473\n",
      "Epoch 195/300\n",
      "Average training loss: 0.049614146398173437\n",
      "Average test loss: 0.0022093693901681236\n",
      "Epoch 196/300\n",
      "Average training loss: 0.049447610898150335\n",
      "Average test loss: 0.00223430916832553\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04943948472208447\n",
      "Average test loss: 0.002217492730046312\n",
      "Epoch 198/300\n",
      "Average training loss: 0.049458704127205745\n",
      "Average test loss: 0.002284801608986325\n",
      "Epoch 199/300\n",
      "Average training loss: 0.049469396928946176\n",
      "Average test loss: 0.0022801489625126122\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0493428722123305\n",
      "Average test loss: 0.002232776941317651\n",
      "Epoch 201/300\n",
      "Average training loss: 0.049264658394787046\n",
      "Average test loss: 0.002296520713923706\n",
      "Epoch 202/300\n",
      "Average training loss: 0.049413666950331794\n",
      "Average test loss: 0.0022896279459819198\n",
      "Epoch 203/300\n",
      "Average training loss: 0.049469252910878926\n",
      "Average test loss: 0.0022515813394760093\n",
      "Epoch 204/300\n",
      "Average training loss: 0.049259399072991476\n",
      "Average test loss: 0.0022213782036883964\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04908359874288241\n",
      "Average test loss: 0.002263819704970552\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0491962018708388\n",
      "Average test loss: 0.002274403778008289\n",
      "Epoch 207/300\n",
      "Average training loss: 0.049143311030334896\n",
      "Average test loss: 0.002258254991016454\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04904632081919246\n",
      "Average test loss: 0.0022588676284584735\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04899685046904617\n",
      "Average test loss: 0.0023381138392206697\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04897870425714387\n",
      "Average test loss: 0.002246456286766463\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04895803578032388\n",
      "Average test loss: 0.00224505898832447\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04903004017472267\n",
      "Average test loss: 0.002244044070235557\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04899441425336732\n",
      "Average test loss: 0.0022521863917095795\n",
      "Epoch 214/300\n",
      "Average training loss: 0.048798274798525706\n",
      "Average test loss: 0.002233026917816864\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04886094088024563\n",
      "Average test loss: 0.002272799677008556\n",
      "Epoch 216/300\n",
      "Average training loss: 0.048756418387095136\n",
      "Average test loss: 0.002253173722575108\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04876473402645853\n",
      "Average test loss: 0.002280818517630299\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04870913614498244\n",
      "Average test loss: 0.0023223538539475863\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04863102841046121\n",
      "Average test loss: 0.0023120258804410695\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0485945170753532\n",
      "Average test loss: 0.002280801087617874\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04861672271622552\n",
      "Average test loss: 0.0023313143379572365\n",
      "Epoch 222/300\n",
      "Average training loss: 0.048602769871552785\n",
      "Average test loss: 0.002267660757061094\n",
      "Epoch 223/300\n",
      "Average training loss: 0.048440140687757066\n",
      "Average test loss: 0.00227164432240857\n",
      "Epoch 224/300\n",
      "Average training loss: 0.048473071187734605\n",
      "Average test loss: 0.0022255145434497132\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04850457482702202\n",
      "Average test loss: 0.0022924688110748928\n",
      "Epoch 226/300\n",
      "Average training loss: 0.048493431988689634\n",
      "Average test loss: 0.0022829684273650247\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04847368143333329\n",
      "Average test loss: 0.002275044810440805\n",
      "Epoch 228/300\n",
      "Average training loss: 0.048335901922649804\n",
      "Average test loss: 0.0023699577690826524\n",
      "Epoch 229/300\n",
      "Average training loss: 0.048403894275426865\n",
      "Average test loss: 0.0022555861410995326\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0483286227285862\n",
      "Average test loss: 0.00230158385882775\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048288646726144684\n",
      "Average test loss: 0.002345202531458603\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04828466342555152\n",
      "Average test loss: 0.002273547662422061\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04809125158853001\n",
      "Average test loss: 0.0022545629456225368\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0480979068643517\n",
      "Average test loss: 0.0022894627627813155\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04821686351299286\n",
      "Average test loss: 0.0022906556009418435\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04807283572687043\n",
      "Average test loss: 0.002287770572428902\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04807916729648908\n",
      "Average test loss: 0.0022609325195145275\n",
      "Epoch 238/300\n",
      "Average training loss: 0.048099514156579974\n",
      "Average test loss: 0.002290040011569444\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04804664617776871\n",
      "Average test loss: 0.002312068390970429\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0479636731048425\n",
      "Average test loss: 0.00235890362970531\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04808480549520917\n",
      "Average test loss: 0.002299195264362627\n",
      "Epoch 242/300\n",
      "Average training loss: 0.047841210577223035\n",
      "Average test loss: 0.002269686175717248\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04803663414054447\n",
      "Average test loss: 0.0022682192718817126\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04793643177217907\n",
      "Average test loss: 0.0022721459127755628\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04778792345523834\n",
      "Average test loss: 0.002265664262904061\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04781600065363778\n",
      "Average test loss: 0.00230153872176177\n",
      "Epoch 247/300\n",
      "Average training loss: 0.047763761854834025\n",
      "Average test loss: 0.002249880577954981\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04796200278070238\n",
      "Average test loss: 0.002274841831686596\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0476904292802016\n",
      "Average test loss: 0.002256443531976806\n",
      "Epoch 250/300\n",
      "Average training loss: 0.047722269035047955\n",
      "Average test loss: 0.0022836908468355736\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04769497372706731\n",
      "Average test loss: 0.0022819501844545207\n",
      "Epoch 252/300\n",
      "Average training loss: 0.047598149014843834\n",
      "Average test loss: 0.002314591485593054\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04760362394650777\n",
      "Average test loss: 0.002315056288200948\n",
      "Epoch 254/300\n",
      "Average training loss: 0.047668693247768616\n",
      "Average test loss: 0.002267209262897571\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04751580340332455\n",
      "Average test loss: 0.00227051558614605\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04763905599713326\n",
      "Average test loss: 0.0023039455484184955\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04753029786878162\n",
      "Average test loss: 0.002333199390727613\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04767654111650255\n",
      "Average test loss: 0.00230536879102389\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04743059680528111\n",
      "Average test loss: 0.0023078398541029957\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04744798172844781\n",
      "Average test loss: 0.0022477103531774547\n",
      "Epoch 261/300\n",
      "Average training loss: 0.047483186685376697\n",
      "Average test loss: 0.0023124228950796855\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04731864027678966\n",
      "Average test loss: 0.002421537761059072\n",
      "Epoch 263/300\n",
      "Average training loss: 0.047415198190344704\n",
      "Average test loss: 0.002261832633987069\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0473028734922409\n",
      "Average test loss: 0.002291522569954395\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04729897632201513\n",
      "Average test loss: 0.0023401150663072863\n",
      "Epoch 266/300\n",
      "Average training loss: 0.047334610018465256\n",
      "Average test loss: 0.002290304065268073\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04725363578067886\n",
      "Average test loss: 0.002247315553534362\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04726536427603827\n",
      "Average test loss: 0.0022853960677360493\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04714029049873352\n",
      "Average test loss: 0.0022957626613270905\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04713814590043492\n",
      "Average test loss: 0.0023123426135215493\n",
      "Epoch 271/300\n",
      "Average training loss: 0.047201453061567415\n",
      "Average test loss: 0.002325508661568165\n",
      "Epoch 272/300\n",
      "Average training loss: 0.047157784640789034\n",
      "Average test loss: 0.0022950818141301474\n",
      "Epoch 273/300\n",
      "Average training loss: 0.047164907144175636\n",
      "Average test loss: 0.0023807593745489917\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04701000615126557\n",
      "Average test loss: 0.002343718589387006\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04705580493807793\n",
      "Average test loss: 0.0022960825570755533\n",
      "Epoch 276/300\n",
      "Average training loss: 0.047034137202633754\n",
      "Average test loss: 0.002273433457232184\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04700462094280455\n",
      "Average test loss: 0.0022846971253554027\n",
      "Epoch 278/300\n",
      "Average training loss: 0.046979617589049866\n",
      "Average test loss: 0.002262056841618485\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04695174017548561\n",
      "Average test loss: 0.002317587971687317\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04700755183895429\n",
      "Average test loss: 0.002323085251160794\n",
      "Epoch 281/300\n",
      "Average training loss: 0.046950607177284026\n",
      "Average test loss: 0.002298714250533117\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04686709389752812\n",
      "Average test loss: 0.0024205526691964932\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04690986863606506\n",
      "Average test loss: 0.0022904868657804196\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04688859694037172\n",
      "Average test loss: 0.0023278917039020195\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04677406793501642\n",
      "Average test loss: 0.0022786625743740135\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04691499716374609\n",
      "Average test loss: 0.002419141113654607\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04678061626023716\n",
      "Average test loss: 0.002305050217650003\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04680379422505697\n",
      "Average test loss: 0.00229904421687954\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04684606725970904\n",
      "Average test loss: 0.002301538060315781\n",
      "Epoch 290/300\n",
      "Average training loss: 0.046775998516215216\n",
      "Average test loss: 0.002270786641062134\n",
      "Epoch 291/300\n",
      "Average training loss: 0.046745236525932946\n",
      "Average test loss: 0.002353251584702068\n",
      "Epoch 292/300\n",
      "Average training loss: 0.046731378810273276\n",
      "Average test loss: 0.0023162820460274815\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04673467739754253\n",
      "Average test loss: 0.0023600844204839733\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04663611220651203\n",
      "Average test loss: 0.002398777645495203\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04654741925001144\n",
      "Average test loss: 0.0023340535747508207\n",
      "Epoch 296/300\n",
      "Average training loss: 0.046747956577274534\n",
      "Average test loss: 0.0023262821218619743\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04663955710993873\n",
      "Average test loss: 0.00255956745851371\n",
      "Epoch 298/300\n",
      "Average training loss: 0.046556935638189315\n",
      "Average test loss: 0.0022832846517364183\n",
      "Epoch 299/300\n",
      "Average training loss: 0.046586943450901246\n",
      "Average test loss: 0.002277607513591647\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04677699425816536\n",
      "Average test loss: 0.0023146216968695323\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.122100155512491\n",
      "Average test loss: 0.005623004634347227\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7181663830015395\n",
      "Average test loss: 0.00417569498759177\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3423974446323183\n",
      "Average test loss: 0.003678474133213361\n",
      "Epoch 4/300\n",
      "Average training loss: 0.22850659453868866\n",
      "Average test loss: 0.0035234929958565366\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1767236794895596\n",
      "Average test loss: 0.003183266851533618\n",
      "Epoch 6/300\n",
      "Average training loss: 0.14882258303960164\n",
      "Average test loss: 0.003063719865762525\n",
      "Epoch 7/300\n",
      "Average training loss: 0.13092355471187167\n",
      "Average test loss: 0.0030013565632204214\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1187889514234331\n",
      "Average test loss: 0.0028179218367569976\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10921695029735565\n",
      "Average test loss: 0.002742295331011216\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10203367786275017\n",
      "Average test loss: 0.0027211526417069967\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09603219193220139\n",
      "Average test loss: 0.0027265078562001385\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09105449555979835\n",
      "Average test loss: 0.002436480488628149\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08660036502944099\n",
      "Average test loss: 0.002325523080304265\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0828237857553694\n",
      "Average test loss: 0.002262445804921703\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07930286937620905\n",
      "Average test loss: 0.0021958334187252653\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07632923183176253\n",
      "Average test loss: 0.002088862726257907\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07360154575109482\n",
      "Average test loss: 0.0020414963050021068\n",
      "Epoch 18/300\n",
      "Average training loss: 0.071198509130213\n",
      "Average test loss: 0.0019528931882232428\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06896915936470031\n",
      "Average test loss: 0.001922818125018643\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06704882893959681\n",
      "Average test loss: 0.0018688428782754475\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0651518991722001\n",
      "Average test loss: 0.001880404571692149\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06356927961773343\n",
      "Average test loss: 0.0018096565862910615\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06195440434416135\n",
      "Average test loss: 0.001775958486315277\n",
      "Epoch 24/300\n",
      "Average training loss: 0.060718013604482014\n",
      "Average test loss: 0.0017855227152920432\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05955373571316401\n",
      "Average test loss: 0.001790706995460722\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05842350965903865\n",
      "Average test loss: 0.0017033584515253703\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05747416867812474\n",
      "Average test loss: 0.0016994874841637082\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05663271034095022\n",
      "Average test loss: 0.0016906378434764014\n",
      "Epoch 29/300\n",
      "Average training loss: 0.055592697223027544\n",
      "Average test loss: 0.0016994638277424707\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05514256524377399\n",
      "Average test loss: 0.0016679292263256178\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05447092124819756\n",
      "Average test loss: 0.0016488730169625745\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05359914256135623\n",
      "Average test loss: 0.0016422540551672379\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05295584853821331\n",
      "Average test loss: 0.0016228193297154374\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05253912004828453\n",
      "Average test loss: 0.0016305669078396426\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05201240236229367\n",
      "Average test loss: 0.00162506150547415\n",
      "Epoch 36/300\n",
      "Average training loss: 0.051697871857219274\n",
      "Average test loss: 0.0016245712898671626\n",
      "Epoch 37/300\n",
      "Average training loss: 0.051180156813727486\n",
      "Average test loss: 0.0016002772188641959\n",
      "Epoch 38/300\n",
      "Average training loss: 0.050802079409360885\n",
      "Average test loss: 0.0015745678767561913\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05030669362843037\n",
      "Average test loss: 0.0015811700274546941\n",
      "Epoch 40/300\n",
      "Average training loss: 0.050009193720089065\n",
      "Average test loss: 0.0015567771424021986\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04970779180526733\n",
      "Average test loss: 0.001567082898898257\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04954293257329199\n",
      "Average test loss: 0.0015640189711832338\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04912469114197625\n",
      "Average test loss: 0.0015580071260531743\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04881205078297191\n",
      "Average test loss: 0.001563381398924523\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0487378539873494\n",
      "Average test loss: 0.0015527696506016785\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04828995556301541\n",
      "Average test loss: 0.0015583042522064514\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0480520391398006\n",
      "Average test loss: 0.0015448487218883303\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04792076907224126\n",
      "Average test loss: 0.0015224229940730665\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04767401429017385\n",
      "Average test loss: 0.001515632819901738\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0474126629796293\n",
      "Average test loss: 0.0015186457137266794\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04722926173276371\n",
      "Average test loss: 0.0015138293994176719\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04698566200666957\n",
      "Average test loss: 0.0015361221842467784\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04686635404494074\n",
      "Average test loss: 0.0015180914543776048\n",
      "Epoch 54/300\n",
      "Average training loss: 0.046610032578309375\n",
      "Average test loss: 0.0015123534281738103\n",
      "Epoch 55/300\n",
      "Average training loss: 0.046611916306946014\n",
      "Average test loss: 0.0015031434231334263\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04632850065165096\n",
      "Average test loss: 0.0015132296587237054\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04611821044484774\n",
      "Average test loss: 0.001516576808773809\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04598929588331117\n",
      "Average test loss: 0.0015098384995427397\n",
      "Epoch 59/300\n",
      "Average training loss: 0.045777597376041944\n",
      "Average test loss: 0.0014935779817816285\n",
      "Epoch 60/300\n",
      "Average training loss: 0.045565861655606166\n",
      "Average test loss: 0.001484941232846015\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04553891112075912\n",
      "Average test loss: 0.0014933129923625126\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04537172327107853\n",
      "Average test loss: 0.0015037467003696495\n",
      "Epoch 63/300\n",
      "Average training loss: 0.045240547100702924\n",
      "Average test loss: 0.0014934872455584507\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04503356989887026\n",
      "Average test loss: 0.0015071236652632555\n",
      "Epoch 65/300\n",
      "Average training loss: 0.044863605502578945\n",
      "Average test loss: 0.0014855369608849287\n",
      "Epoch 66/300\n",
      "Average training loss: 0.044713558422194585\n",
      "Average test loss: 0.0014945931241123213\n",
      "Epoch 67/300\n",
      "Average training loss: 0.044575051526228586\n",
      "Average test loss: 0.0014978684327668615\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04445206058687634\n",
      "Average test loss: 0.001501399114727974\n",
      "Epoch 69/300\n",
      "Average training loss: 0.044506763759586544\n",
      "Average test loss: 0.0015566649214468068\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04416398231519593\n",
      "Average test loss: 0.001498077638964686\n",
      "Epoch 71/300\n",
      "Average training loss: 0.044011151721080145\n",
      "Average test loss: 0.0014828635382259057\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04385290715429518\n",
      "Average test loss: 0.0014781531736047732\n",
      "Epoch 73/300\n",
      "Average training loss: 0.043862381551000806\n",
      "Average test loss: 0.0014923214099059501\n",
      "Epoch 74/300\n",
      "Average training loss: 0.043642107639047835\n",
      "Average test loss: 0.001477788262690107\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04352178480890062\n",
      "Average test loss: 0.0014810107381393511\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04368708953261376\n",
      "Average test loss: 0.001476708302895228\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04337762063741684\n",
      "Average test loss: 0.0014842332770737508\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04313340525163545\n",
      "Average test loss: 0.001492823467641655\n",
      "Epoch 79/300\n",
      "Average training loss: 0.043035335673226253\n",
      "Average test loss: 0.0014761049469105073\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04287228182951609\n",
      "Average test loss: 0.0014737729368110499\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0428018000341124\n",
      "Average test loss: 0.0014835892986092302\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0426190158708228\n",
      "Average test loss: 0.001474083859473467\n",
      "Epoch 83/300\n",
      "Average training loss: 0.042604167567359075\n",
      "Average training loss: 0.042326933960119884\n",
      "Average test loss: 0.001480636322249969\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04220519585907459\n",
      "Average test loss: 0.0014853230581308404\n",
      "Epoch 87/300\n",
      "Average training loss: 0.042146211852629976\n",
      "Average test loss: 0.0014903314711732997\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04202686720093091\n",
      "Average test loss: 0.0014833759210175939\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04188010537458791\n",
      "Average test loss: 0.0014963322854600848\n",
      "Epoch 90/300\n",
      "Average training loss: 0.041755099687311385\n",
      "Average test loss: 0.0014833947736769915\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04160618211163415\n",
      "Average test loss: 0.0014906192351546552\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0415530091855261\n",
      "Average test loss: 0.0014850139911803935\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04144656493100855\n",
      "Average test loss: 0.0014917169075666203\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04129815693365203\n",
      "Average test loss: 0.001493663535039458\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04126446376575364\n",
      "Average test loss: 0.0014933517477992509\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0411659578151173\n",
      "Average test loss: 0.0014918725178059604\n",
      "Epoch 97/300\n",
      "Average training loss: 0.041045932044585544\n",
      "Average test loss: 0.0014918313620405064\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04088391788966126\n",
      "Average test loss: 0.0015012031668383214\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04087311453289456\n",
      "Average test loss: 0.0014856358968342344\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04076875107818179\n",
      "Average test loss: 0.0014821731406781408\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04054736696018113\n",
      "Average test loss: 0.0014882590251250399\n",
      "Epoch 103/300\n",
      "Average training loss: 0.040464677568939\n",
      "Average test loss: 0.0015315037419398627\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04023952869243092\n",
      "Average test loss: 0.0014809106416586372\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04024327628811201\n",
      "Average test loss: 0.0015155468646229969\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04010486663381259\n",
      "Average test loss: 0.0015140940842943058\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04007882492740949\n",
      "Average test loss: 0.00151000244413606\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03988401853707101\n",
      "Average test loss: 0.0015340952147833175\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04005703062481351\n",
      "Average test loss: 0.0015139824826684264\n",
      "Epoch 110/300\n",
      "Average training loss: 0.039788942337036136\n",
      "Average test loss: 0.0014979601388590204\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03975444038709005\n",
      "Average test loss: 0.0015218104874301288\n",
      "Epoch 112/300\n",
      "Average training loss: 0.039660945920480625\n",
      "Average test loss: 0.0014955520197335217\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03950594898727205\n",
      "Average test loss: 0.0014926215808114243\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03937761620680491\n",
      "Average test loss: 0.001531976479726533\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03933100132478608\n",
      "Average test loss: 0.0015078036977599065\n",
      "Epoch 116/300\n",
      "Average training loss: 0.039280436043938\n",
      "Average test loss: 0.0015143233810861905\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03904552747143639\n",
      "Average test loss: 0.0015205035645307766\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03903602444463306\n",
      "Average test loss: 0.001497917326581147\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03886652723285887\n",
      "Average test loss: 0.0015397402978916134\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03885527785784668\n",
      "Average test loss: 0.0015113873338947694\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03884677972727352\n",
      "Average test loss: 0.0015525367006452547\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03866821914580133\n",
      "Average test loss: 0.0015088240081030462\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03866880993710624\n",
      "Average test loss: 0.0015383169756581386\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03855833777288596\n",
      "Average test loss: 0.0014858429263242417\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0384536927110619\n",
      "Average test loss: 0.0015184400638358461\n",
      "Epoch 127/300\n",
      "Average training loss: 0.038374649650520745\n",
      "Average test loss: 0.0015058341628561417\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03829024586412642\n",
      "Average test loss: 0.0015207141611932053\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03824933916661474\n",
      "Average test loss: 0.001625044824141595\n",
      "Epoch 130/300\n",
      "Average training loss: 0.038131588992145325\n",
      "Average test loss: 0.0015222283633839753\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03803320900930299\n",
      "Average test loss: 0.0015586550895952515\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03796447073419889\n",
      "Average test loss: 0.0015180469766880075\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03791881547040409\n",
      "Average test loss: 0.0015171633631818823\n",
      "Epoch 135/300\n",
      "Average training loss: 0.037849370280901594\n",
      "Average test loss: 0.0015090242127577464\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03779997994833522\n",
      "Average test loss: 0.001538964677705533\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03771011681689156\n",
      "Average test loss: 0.0015550576029345394\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03772494026025136\n",
      "Average test loss: 0.0015904114360196723\n",
      "Epoch 139/300\n",
      "Average training loss: 0.037597939535975455\n",
      "Average test loss: 0.0015741184849499001\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03750148118204541\n",
      "Average test loss: 0.0015530680711898538\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03740520063704914\n",
      "Average test loss: 0.0015553595756904947\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03736642127235731\n",
      "Average test loss: 0.001567943539780875\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037327243112855485\n",
      "Average test loss: 0.0015766516753161948\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03727741909358236\n",
      "Average test loss: 0.0016051713082318505\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03730135665668382\n",
      "Average test loss: 0.0015373325016763476\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03712858013312022\n",
      "Average test loss: 0.00151737081963155\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03707260371910201\n",
      "Average test loss: 0.0015437039544598924\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03710352408223682\n",
      "Average test loss: 0.00197858253825042\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03683986379040612\n",
      "Average test loss: 0.001547204250573284\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03684613708655039\n",
      "Average test loss: 0.001535622868169513\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0367615185379982\n",
      "Average test loss: 0.0015692204613652495\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03671397759848171\n",
      "Average test loss: 0.001529502693356739\n",
      "Epoch 155/300\n",
      "Average training loss: 0.036599729840954144\n",
      "Average test loss: 0.0015623586910466353\n",
      "Epoch 156/300\n",
      "Average training loss: 0.036606909270087876\n",
      "Average test loss: 0.0015599204806817902\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03654793287979232\n",
      "Average test loss: 0.0015626909166781438\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0364826449321376\n",
      "Average test loss: 0.0016988043421879411\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03646747657987807\n",
      "Average test loss: 0.0015442329025309946\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036446892768144605\n",
      "Average test loss: 0.0015430231359269883\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03637775217824512\n",
      "Average test loss: 0.001718798319498698\n",
      "Epoch 162/300\n",
      "Average training loss: 0.036233928249941934\n",
      "Average test loss: 0.0016024812931815784\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036259698107838634\n",
      "Average test loss: 0.0015550805462731256\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03620535497532951\n",
      "Average test loss: 0.0015638523958623409\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03621828655401866\n",
      "Average test loss: 0.0015265285793381433\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03616341907448239\n",
      "Average test loss: 0.0015404401457765036\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03612820293505987\n",
      "Average test loss: 0.0015865792766627338\n",
      "Epoch 170/300\n",
      "Average training loss: 0.035976761865946984\n",
      "Average test loss: 0.001565117633384135\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03600170382857323\n",
      "Average test loss: 0.0015510828364640474\n",
      "Epoch 172/300\n",
      "Average training loss: 0.035785397268003886\n",
      "Average test loss: 0.0015723162533508408\n",
      "Epoch 173/300\n",
      "Average training loss: 0.035759300468696485\n",
      "Average test loss: 0.0017177698603934712\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03577844162782033\n",
      "Average test loss: 0.0015706981860825586\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03578109356264273\n",
      "Average test loss: 0.001587573142722249\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03566091342104806\n",
      "Average test loss: 0.0015509930671089224\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03560902883277999\n",
      "Average test loss: 0.0015960865038861004\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03557331738207075\n",
      "Average test loss: 0.0015905382275167438\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03554010065562195\n",
      "Average test loss: 0.0015902426718837684\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03553545941246881\n",
      "Average test loss: 0.0015665692226029932\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0356205350673861\n",
      "Average test loss: 0.0016043567668336132\n",
      "Epoch 182/300\n",
      "Average training loss: 0.035511025273137625\n",
      "Average test loss: 0.0015968921872683698\n",
      "Epoch 183/300\n",
      "Average training loss: 0.035370609510276055\n",
      "Average test loss: 0.0015755588508004116\n",
      "Epoch 184/300\n",
      "Average training loss: 0.035245579077137844\n",
      "Average test loss: 0.001591844392940402\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03528559677302837\n",
      "Average test loss: 0.0015731096212855643\n",
      "Epoch 187/300\n",
      "Average training loss: 0.035245360458890596\n",
      "Average test loss: 0.0015776924319151374\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03547987912429704\n",
      "Average test loss: 0.0016048905718036824\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03519836018482844\n",
      "Average test loss: 0.0015912385854042238\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03518846628069878\n",
      "Average test loss: 0.0015477102630668217\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03511846552292506\n",
      "Average test loss: 0.0015651973820705381\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03504885759618547\n",
      "Average test loss: 0.0016040348476833767\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03504512893491321\n",
      "Average test loss: 0.0015820958411528003\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03507438246409098\n",
      "Average test loss: 0.0015876727522247368\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03504112634062767\n",
      "Average test loss: 0.0016114599158366522\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03495995784799258\n",
      "Average test loss: 0.0015914442243261469\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03491879893011517\n",
      "Average test loss: 0.0016209799898788332\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03484602432780796\n",
      "Average test loss: 0.001605711670488947\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03488144115275807\n",
      "Average test loss: 0.0015921282999010549\n",
      "Epoch 200/300\n",
      "Average training loss: 0.034770580444071025\n",
      "Average test loss: 0.0016997904787874885\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03471879791882303\n",
      "Average test loss: 0.0015819032579246495\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03470672696497705\n",
      "Average test loss: 0.0016062570071468751\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03475123460756408\n",
      "Average test loss: 0.0016216366570442915\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03460372786389457\n",
      "Average test loss: 0.0015689458508665363\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03464467597007752\n",
      "Average test loss: 0.0016012128992523584\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03472480922109551\n",
      "Average test loss: 0.001581924863366617\n",
      "Epoch 208/300\n",
      "Average training loss: 0.034560184333059525\n",
      "Average test loss: 0.0015815597895739808\n",
      "Epoch 209/300\n",
      "Average training loss: 0.034493798789050845\n",
      "Average test loss: 0.0016014373174144162\n",
      "Epoch 210/300\n",
      "Average training loss: 0.034516074657440186\n",
      "Average test loss: 0.001642820066668921\n",
      "Epoch 211/300\n",
      "Average training loss: 0.034498568971951804\n",
      "Average test loss: 0.0015865054799036847\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03441918592817254\n",
      "Average test loss: 0.001612143798213866\n",
      "Epoch 213/300\n",
      "Average training loss: 0.034507012461622556\n",
      "Average test loss: 0.0016301365986259447\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03443242553869883\n",
      "Average test loss: 0.0015845194053318765\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03431137499378787\n",
      "Average test loss: 0.0016226601698953245\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03437882429030206\n",
      "Average test loss: 0.0016787754784648617\n",
      "Epoch 217/300\n",
      "Average training loss: 0.034282964213026894\n",
      "Average test loss: 0.0015963380867615343\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03424366483092308\n",
      "Average test loss: 0.0016253414913598033\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03426854046516948\n",
      "Average test loss: 0.0015989719692410694\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03421997496816847\n",
      "Average test loss: 0.00849481407718526\n",
      "Epoch 221/300\n",
      "Average training loss: 0.034180241415898004\n",
      "Average test loss: 0.0016229989637310306\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03423291444447305\n",
      "Average test loss: 0.001606521730725136\n",
      "Epoch 223/300\n",
      "Average training loss: 0.034095396359761554\n",
      "Average test loss: 0.0016130188515202867\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03411386046061913\n",
      "Average test loss: 0.0016078247397931086\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03404531220595042\n",
      "Average test loss: 0.001570326131147643\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03406038197875023\n",
      "Average test loss: 0.0016067364011994665\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03407725476225217\n",
      "Average test loss: 0.0016890093640734751\n",
      "Epoch 228/300\n",
      "Average training loss: 0.033942450652519864\n",
      "Average test loss: 0.001600432948105865\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03389489933186107\n",
      "Average test loss: 0.0015744480128503508\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03391739258501265\n",
      "Average test loss: 0.0016024775249987014\n",
      "Epoch 233/300\n",
      "Average training loss: 0.033932476381460824\n",
      "Average test loss: 0.0015992152551189064\n",
      "Epoch 234/300\n",
      "Average training loss: 0.033840173406733405\n",
      "Average test loss: 0.0016793436407008106\n",
      "Epoch 235/300\n",
      "Average training loss: 0.033840499131215944\n",
      "Average test loss: 0.0016200213104796907\n",
      "Epoch 236/300\n",
      "Average training loss: 0.033917694113320775\n",
      "Average test loss: 0.001613442391778032\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03381599603096644\n",
      "Average test loss: 0.001637057837098837\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03383538852466477\n",
      "Average test loss: 0.001614208394351105\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03374013084007634\n",
      "Average test loss: 0.001592674262356013\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03364500944316387\n",
      "Average test loss: 0.0016684239254229599\n",
      "Epoch 241/300\n",
      "Average training loss: 0.033770486129654775\n",
      "Average test loss: 0.0015998930217077335\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03372157916592227\n",
      "Average test loss: 0.0016140056682957544\n",
      "Epoch 243/300\n",
      "Average training loss: 0.033595974620845584\n",
      "Average test loss: 0.001627724121014277\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03355781541930305\n",
      "Average test loss: 0.0016278134229489498\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03359494347539213\n",
      "Average test loss: 0.0016670791137342652\n",
      "Epoch 246/300\n",
      "Average training loss: 0.033551501161522335\n",
      "Average test loss: 0.001646293836335341\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03360863525337643\n",
      "Average test loss: 0.0017234134298956228\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03353500496182177\n",
      "Average test loss: 0.0016066392047537697\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03344329407480028\n",
      "Average test loss: 0.0016366948665430148\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0335221510761314\n",
      "Average test loss: 0.0016767309164214465\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03341985149516\n",
      "Average test loss: 0.0016214019082900551\n",
      "Epoch 254/300\n",
      "Average training loss: 0.033420712699492774\n",
      "Average test loss: 0.001598837232010232\n",
      "Epoch 255/300\n",
      "Average training loss: 0.033451910780535804\n",
      "Average test loss: 0.001610122813946671\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03336275330599811\n",
      "Average test loss: 0.0016336513361376193\n",
      "Epoch 257/300\n",
      "Average training loss: 0.033397349584433765\n",
      "Average test loss: 0.001618921016756859\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0334421120915148\n",
      "Average test loss: 0.0016200714393829307\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03325882873104678\n",
      "Average test loss: 0.0016111640985020334\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03332648942205641\n",
      "Average test loss: 0.0015826531459784343\n",
      "Epoch 261/300\n",
      "Average training loss: 0.033231581270694734\n",
      "Average test loss: 0.0016452037373350726\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0332691028283702\n",
      "Average test loss: 0.0016824346464127302\n",
      "Epoch 265/300\n",
      "Average training loss: 0.033161335898770225\n",
      "Average test loss: 0.0016181070607983405\n",
      "Epoch 266/300\n",
      "Average training loss: 0.033196125984191895\n",
      "Average test loss: 0.0016350291373415126\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03329889532261424\n",
      "Average test loss: 0.0016753394820003046\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03313503700825903\n",
      "Average test loss: 0.0016220447787394126\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03308448643816842\n",
      "Average test loss: 0.0016022377003812127\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03311942805846532\n",
      "Average test loss: 0.0016106493488575022\n",
      "Epoch 271/300\n",
      "Average training loss: 0.033126609947946334\n",
      "Average test loss: 0.0016687391520374352\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03307398342920674\n",
      "Average test loss: 0.0016313647472610076\n",
      "Epoch 273/300\n",
      "Average training loss: 0.033013595047924255\n",
      "Average test loss: 0.0016319356674535407\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0330338231275479\n",
      "Average test loss: 0.001604847955620951\n",
      "Epoch 275/300\n",
      "Average training loss: 0.033109216294354864\n",
      "Average test loss: 0.0015989334488080607\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03314136395189497\n",
      "Average test loss: 0.0016094939672491617\n",
      "Epoch 277/300\n",
      "Average training loss: 0.033014114931225774\n",
      "Average test loss: 0.0016732586696743966\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03290633539689912\n",
      "Average test loss: 0.0016420388090320759\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0329589838915401\n",
      "Average test loss: 0.0016324123204168345\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03301130737529861\n",
      "Average test loss: 0.0016002678099191852\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0330162603110075\n",
      "Average test loss: 0.0016185012134826845\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03293462527129385\n",
      "Average test loss: 0.001627416984592047\n",
      "Epoch 284/300\n",
      "Average training loss: 0.032839396542972986\n",
      "Average test loss: 0.0016693796274355715\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03294141653676828\n",
      "Average test loss: 0.0016554096241792042\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03279854473306073\n",
      "Average test loss: 0.0017408617693516943\n",
      "Epoch 287/300\n",
      "Average training loss: 0.032837350848648285\n",
      "Average test loss: 0.001618352860212326\n",
      "Epoch 288/300\n",
      "Average training loss: 0.032922180579768284\n",
      "Average test loss: 0.0017910482217040327\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03273271715309885\n",
      "Average test loss: 0.001679891454676787\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03281691614290078\n",
      "Average test loss: 0.0016287034810003307\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03282814857529269\n",
      "Average test loss: 0.0016548566580232647\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03282787532276577\n",
      "Average test loss: 0.0017916976626341542\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03275651600460212\n",
      "Average test loss: 0.0016643468665166034\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03266128593517675\n",
      "Average test loss: 0.0016264465159426132\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0327039249787728\n",
      "Average test loss: 0.0016642786928762992\n",
      "Epoch 296/300\n",
      "Average training loss: 0.032678445109062725\n",
      "Average test loss: 0.0016314086229023007\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03269885785215431\n",
      "Average test loss: 0.0016285231798473332\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03264259259237184\n",
      "Average test loss: 0.0016756976829427812\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth3-.01/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.74\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.78\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.08\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.05\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.14\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.38\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.43\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.41\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.63\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.43\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.80\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.14\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.03\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.37\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.40\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.47\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.54\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.55\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.64\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.66\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.69\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.67\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.73\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.74\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.73\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.78\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
