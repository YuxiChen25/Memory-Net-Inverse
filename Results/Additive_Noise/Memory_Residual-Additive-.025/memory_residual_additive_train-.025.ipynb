{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.025)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.025)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.025)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.025)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15800885087251662\n",
      "Average test loss: 0.01096521337164773\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06364845725562837\n",
      "Average test loss: 0.009768919814378023\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05730780975023905\n",
      "Average test loss: 0.010604006215102144\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05392448871003257\n",
      "Average test loss: 0.009271428898804718\n",
      "Epoch 5/300\n",
      "Average training loss: 0.051685002939568625\n",
      "Average test loss: 0.00830350274592638\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04915047177672386\n",
      "Average test loss: 0.008877529405885272\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04773576867580414\n",
      "Average test loss: 0.008568395098050435\n",
      "Epoch 8/300\n",
      "Average training loss: 0.046410978509320154\n",
      "Average test loss: 0.008301617874039545\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04556328484084871\n",
      "Average test loss: 0.00795124609851175\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04468259781267908\n",
      "Average test loss: 0.008290607819126711\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04404013295968374\n",
      "Average test loss: 0.007596309651931127\n",
      "Epoch 12/300\n",
      "Average training loss: 0.043283528516689936\n",
      "Average test loss: 0.00749594500909249\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0425209653907352\n",
      "Average test loss: 0.007916500038156906\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04197826849089729\n",
      "Average test loss: 0.007132168365849389\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0414815875755416\n",
      "Average test loss: 0.007494760097728835\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04112729372911983\n",
      "Average test loss: 0.007441748686134815\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04079100664787822\n",
      "Average test loss: 0.0072135358519024315\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04020576057169172\n",
      "Average test loss: 0.006892473879787657\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03987468272447586\n",
      "Average test loss: 0.006937630425724719\n",
      "Epoch 20/300\n",
      "Average training loss: 0.039561158825953804\n",
      "Average test loss: 0.0069512437110145885\n",
      "Epoch 21/300\n",
      "Average training loss: 0.039220768332481386\n",
      "Average test loss: 0.007051363294737207\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03891295968492826\n",
      "Average test loss: 0.006697147500597769\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03874937903218799\n",
      "Average test loss: 0.00744260902951161\n",
      "Epoch 24/300\n",
      "Average training loss: 0.038420847131146324\n",
      "Average test loss: 0.006665481855058008\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03812524127297931\n",
      "Average test loss: 0.006795012487305535\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03807047005825573\n",
      "Average test loss: 0.006713825899693701\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03772493033276664\n",
      "Average test loss: 0.006578633116351234\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03745305583543248\n",
      "Average test loss: 0.006684762159983317\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03737044460574786\n",
      "Average test loss: 0.006532696860945887\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03737079928649796\n",
      "Average test loss: 0.00787550539109442\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03704760673642159\n",
      "Average test loss: 0.0065452369290093584\n",
      "Epoch 32/300\n",
      "Average training loss: 0.036835086521175175\n",
      "Average test loss: 0.006575779543982612\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03670594361921151\n",
      "Average test loss: 0.00641425297740433\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03661827001803451\n",
      "Average test loss: 0.006609550786928998\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03642049110598034\n",
      "Average test loss: 0.0064949872750375\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03630274630586306\n",
      "Average test loss: 0.00792064449025525\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03621322145395809\n",
      "Average test loss: 0.0063354554180469775\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03611181403199832\n",
      "Average test loss: 0.006275216716445154\n",
      "Epoch 39/300\n",
      "Average training loss: 0.035878925909598666\n",
      "Average test loss: 0.006318072120348612\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03581389305657811\n",
      "Average test loss: 0.006539758219487137\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03582763251165549\n",
      "Average test loss: 0.006253023183180226\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03565034242636628\n",
      "Average test loss: 0.0064200214586324165\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0356234508951505\n",
      "Average test loss: 0.006377962192313538\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03540407091710303\n",
      "Average test loss: 0.006581979576084349\n",
      "Epoch 45/300\n",
      "Average training loss: 0.035325627194510566\n",
      "Average test loss: 0.006430024489760399\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03528678214881155\n",
      "Average test loss: 0.00624753460743361\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03534472451110681\n",
      "Average test loss: 0.006288612729973263\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03520874197284381\n",
      "Average test loss: 0.006700134522798989\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03494545870025953\n",
      "Average test loss: 0.006258860599663523\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03499047716127501\n",
      "Average test loss: 0.006492797610246473\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03492115969790353\n",
      "Average test loss: 0.006958603281941679\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03482592498924997\n",
      "Average training loss: 0.034782391887572074\n",
      "Average test loss: 0.006225921747999059\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03461370268464088\n",
      "Average test loss: 0.006197290973944797\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03432826212379667\n",
      "Average test loss: 0.006147369703484906\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03428734151522318\n",
      "Average test loss: 0.006220705728563997\n",
      "Epoch 61/300\n",
      "Average training loss: 0.034231951321164765\n",
      "Average test loss: 0.0063835364071031415\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03417570784356859\n",
      "Average test loss: 0.006557097713152568\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03415249872042073\n",
      "Average test loss: 0.006289258112923967\n",
      "Epoch 64/300\n",
      "Average training loss: 0.034084677570396\n",
      "Average test loss: 0.006512223975939883\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03411301331056489\n",
      "Average test loss: 0.09673980791038937\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1770809543861283\n",
      "Average test loss: 0.008204141938851939\n",
      "Epoch 67/300\n",
      "Average training loss: 0.052228624721368154\n",
      "Average test loss: 0.00764737452318271\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04598648798796866\n",
      "Average test loss: 0.006986149288713932\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04312382893760999\n",
      "Average test loss: 0.006660814962867233\n",
      "Epoch 70/300\n",
      "Average training loss: 0.040908420390552945\n",
      "Average test loss: 0.006602547496557236\n",
      "Epoch 71/300\n",
      "Average training loss: 0.039400022102726834\n",
      "Average test loss: 0.006490616557912694\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0381198517481486\n",
      "Average test loss: 0.006642809744510386\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0371742484735118\n",
      "Average test loss: 0.006501340407464239\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03640099717511071\n",
      "Average test loss: 0.0063743335666755834\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03575873449775908\n",
      "Average test loss: 0.006376674519230922\n",
      "Epoch 76/300\n",
      "Average training loss: 0.035299518201086254\n",
      "Average test loss: 0.006218814778245158\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03489095012346904\n",
      "Average test loss: 0.006229502743317021\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03462579833136664\n",
      "Average test loss: 0.006171196031694611\n",
      "Epoch 79/300\n",
      "Average training loss: 0.034361060506767696\n",
      "Average test loss: 0.006252159870747063\n",
      "Epoch 80/300\n",
      "Average training loss: 0.034227430565489665\n",
      "Average test loss: 0.006217616130494409\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03417535446087519\n",
      "Average test loss: 0.006150348947693904\n",
      "Epoch 82/300\n",
      "Average training loss: 0.034073518122235934\n",
      "Average test loss: 0.006484444049497445\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03402131664421823\n",
      "Average test loss: 0.006205202846063508\n",
      "Epoch 84/300\n",
      "Average training loss: 0.034088025526867974\n",
      "Average test loss: 0.0061772823859420085\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0340258301032914\n",
      "Average test loss: 0.006141233290235202\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03389117018381754\n",
      "Average test loss: 0.00626255856288804\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03389601876338323\n",
      "Average test loss: 0.006208208030710618\n",
      "Epoch 88/300\n",
      "Average training loss: 0.033834275570180684\n",
      "Average test loss: 0.006166094337486558\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03385655926002396\n",
      "Average test loss: 0.006282043472760254\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03385086178779602\n",
      "Average test loss: 0.006218184758805566\n",
      "Epoch 91/300\n",
      "Average training loss: 0.033781289625498985\n",
      "Average test loss: 0.006617777407169342\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0336676406065623\n",
      "Average test loss: 0.006116208704809348\n",
      "Epoch 93/300\n",
      "Average training loss: 0.033671879061394266\n",
      "Average test loss: 0.006117739556564225\n",
      "Epoch 94/300\n",
      "Average training loss: 0.033632321374283895\n",
      "Average test loss: 0.0061703413737316925\n",
      "Epoch 95/300\n",
      "Average training loss: 0.033570234121547804\n",
      "Average test loss: 0.006286313959707817\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03352051481770144\n",
      "Average test loss: 0.00615370154256622\n",
      "Epoch 97/300\n",
      "Average training loss: 0.033471620317962435\n",
      "Average test loss: 0.006164926598055495\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03340618656741248\n",
      "Average test loss: 0.00612207216810849\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03338306227326393\n",
      "Average test loss: 0.006335095000763734\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03334127970039844\n",
      "Average test loss: 0.006264929558253951\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0331472688847118\n",
      "Average test loss: 0.0062287178507281675\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03306477354632484\n",
      "Average test loss: 0.006162325719164478\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03304434214366807\n",
      "Average test loss: 0.006163968281199535\n",
      "Epoch 108/300\n",
      "Average training loss: 0.033024240788486266\n",
      "Average test loss: 0.006327203871061404\n",
      "Epoch 109/300\n",
      "Average training loss: 0.033026709543334115\n",
      "Average test loss: 0.006196721033917533\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03292011055350304\n",
      "Average test loss: 0.006184575573437744\n",
      "Epoch 111/300\n",
      "Average training loss: 0.032871242307954365\n",
      "Average test loss: 0.0060787106880711185\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03284939236111111\n",
      "Average test loss: 0.0063644961093862855\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0328345932877726\n",
      "Average test loss: 0.006299412693414423\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03276772246592575\n",
      "Average test loss: 0.006829243977864583\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03283939980798298\n",
      "Average test loss: 0.006138918771098057\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03272060227725241\n",
      "Average test loss: 0.0061229555706183115\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03266195742620362\n",
      "Average test loss: 0.006103710666298866\n",
      "Epoch 118/300\n",
      "Average training loss: 0.032687489800983\n",
      "Average test loss: 0.006142909241633283\n",
      "Epoch 119/300\n",
      "Average training loss: 0.032611757823162606\n",
      "Average test loss: 0.006188530021243625\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032613302742441494\n",
      "Average test loss: 0.006154016902049382\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03260340371562375\n",
      "Average test loss: 0.006119633343898588\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03255368101927969\n",
      "Average test loss: 0.0060987648376160195\n",
      "Epoch 123/300\n",
      "Average training loss: 0.032488883879449634\n",
      "Average test loss: 0.006390233005500502\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03247561213043001\n",
      "Average test loss: 0.006476790375179715\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03246994147532516\n",
      "Average test loss: 0.0063121498156752855\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03242240944339169\n",
      "Average test loss: 0.006112359846631686\n",
      "Epoch 127/300\n",
      "Average training loss: 0.032405283528897495\n",
      "Average test loss: 0.00620410957849688\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03239428819715977\n",
      "Average test loss: 0.006173398441738553\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0323301401635011\n",
      "Average test loss: 0.006154464973757664\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03235267990496424\n",
      "Average test loss: 0.00614890295970771\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03231111201312807\n",
      "Average test loss: 0.0060969069711863996\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03223190789090263\n",
      "Average test loss: 0.006128159523010254\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03223217035002179\n",
      "Average test loss: 0.006141172065503068\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03222590818835629\n",
      "Average test loss: 0.0062038809274219805\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03216258361935616\n",
      "Average test loss: 0.006143433509601487\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03219502065082391\n",
      "Average test loss: 0.006222074083156056\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03214036626948251\n",
      "Average test loss: 0.006189279474731949\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03211694308121999\n",
      "Average test loss: 0.006256363601734241\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03210199812385771\n",
      "Average test loss: 0.006131389843093024\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03203247126440207\n",
      "Average test loss: 0.00614894875470135\n",
      "Epoch 141/300\n",
      "Average training loss: 0.032102347291178175\n",
      "Average test loss: 0.006511688572665056\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03206907297670841\n",
      "Average test loss: 0.006268773020141655\n",
      "Epoch 143/300\n",
      "Average training loss: 0.032004028535551496\n",
      "Average test loss: 0.006232417509787613\n",
      "Epoch 144/300\n",
      "Average training loss: 0.031958296514219706\n",
      "Average test loss: 0.006303624064144161\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03196269523104032\n",
      "Average test loss: 0.006321608146031697\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03185402528113789\n",
      "Average test loss: 0.006149525972290171\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03194865087336964\n",
      "Average test loss: 0.006115168291247554\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03191026811632845\n",
      "Average test loss: 0.006118836284097698\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03184936178889539\n",
      "Average test loss: 0.006140786704503827\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03186956745717261\n",
      "Average test loss: 0.006160571242372195\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03186743497517374\n",
      "Average test loss: 0.006892752355999417\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03181905662682322\n",
      "Average test loss: 0.0062471659518778325\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031731374664439094\n",
      "Average test loss: 0.006221353663752476\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03176121943526798\n",
      "Average test loss: 0.006116937315712372\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03171095202697648\n",
      "Average test loss: 0.006234660282317135\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03168854297366407\n",
      "Average test loss: 0.006156328238132927\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0317378371655941\n",
      "Average test loss: 0.0061102885674271315\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031657970858944784\n",
      "Average test loss: 0.006250646548138725\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03160428681307369\n",
      "Average test loss: 0.006135022927903467\n",
      "Epoch 160/300\n",
      "Average training loss: 0.031626086077756355\n",
      "Average test loss: 0.006484055263714658\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03163403329087628\n",
      "Average test loss: 0.006170280848112371\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03155316682987743\n",
      "Average test loss: 0.006171924439569315\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031597853200303186\n",
      "Average test loss: 0.006376620461543401\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03153433067268795\n",
      "Average test loss: 0.006282722279843357\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031557753286427924\n",
      "Average test loss: 0.006203951542990075\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03143717873427603\n",
      "Average test loss: 0.00616586555292209\n",
      "Epoch 167/300\n",
      "Average training loss: 0.031498853269550535\n",
      "Average test loss: 0.006268825057480071\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0314770806249645\n",
      "Average test loss: 0.006270189589510361\n",
      "Epoch 169/300\n",
      "Average training loss: 0.031414182129833434\n",
      "Average test loss: 0.006314524760676755\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03144677472611268\n",
      "Average test loss: 0.006236016272670693\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0314053496254815\n",
      "Average test loss: 0.006644525289535522\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03146381037102805\n",
      "Average test loss: 0.006151419916914569\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03140686987009313\n",
      "Average test loss: 0.006282797777818309\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03134673971765571\n",
      "Average test loss: 0.006313537249341607\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03136741387844086\n",
      "Average test loss: 0.006154345730112659\n",
      "Epoch 176/300\n",
      "Average training loss: 0.031312618555294144\n",
      "Average test loss: 0.006336555177138911\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031312148494852914\n",
      "Average test loss: 0.006233532461027304\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03127812719841798\n",
      "Average test loss: 0.006941822758151426\n",
      "Epoch 179/300\n",
      "Average training loss: 0.031275958485073516\n",
      "Average test loss: 0.006279856463273366\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03132901281118393\n",
      "Average test loss: 0.006303591976977057\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03127284293870131\n",
      "Average test loss: 0.0062587575320568346\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03122163975569937\n",
      "Average test loss: 0.006171373359238108\n",
      "Epoch 183/300\n",
      "Average training loss: 0.031197643379370373\n",
      "Average test loss: 0.006182378288772371\n",
      "Epoch 184/300\n",
      "Average training loss: 0.031240661525064044\n",
      "Average test loss: 0.007349758424278763\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03126578763458464\n",
      "Average test loss: 0.006451219664679634\n",
      "Epoch 186/300\n",
      "Average training loss: 0.031169984963205125\n",
      "Average test loss: 0.0062627540226611825\n",
      "Epoch 187/300\n",
      "Average training loss: 0.031120363424221673\n",
      "Average test loss: 0.0064828700717124675\n",
      "Epoch 188/300\n",
      "Average training loss: 0.031139711336957084\n",
      "Average test loss: 0.0066053818617429995\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03109309381743272\n",
      "Average test loss: 0.006248494897451665\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03117029211918513\n",
      "Average test loss: 0.00614182861852977\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03109711391064856\n",
      "Average test loss: 0.0063090840495295\n",
      "Epoch 192/300\n",
      "Average training loss: 0.031083053396807777\n",
      "Average test loss: 0.006265656085477935\n",
      "Epoch 193/300\n",
      "Average training loss: 0.031042469688587718\n",
      "Average test loss: 0.006238137187229263\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03111345608201292\n",
      "Average test loss: 0.006296887971046898\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03097927181257142\n",
      "Average test loss: 0.0061883789309197\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03097922921842999\n",
      "Average test loss: 0.0062322075362834664\n",
      "Epoch 197/300\n",
      "Average training loss: 0.031062167776955498\n",
      "Average test loss: 0.006247235330856508\n",
      "Epoch 198/300\n",
      "Average training loss: 0.031018602048357328\n",
      "Average test loss: 0.006376117743551731\n",
      "Epoch 199/300\n",
      "Average training loss: 0.031012234793768988\n",
      "Average test loss: 0.006386041178885434\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03100672260754638\n",
      "Average test loss: 0.006460183115883006\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030964421255720985\n",
      "Average test loss: 0.0062452538472910725\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030918053582310676\n",
      "Average test loss: 0.006652350405024158\n",
      "Epoch 203/300\n",
      "Average training loss: 0.030912368771102695\n",
      "Average test loss: 0.006261764933665593\n",
      "Epoch 204/300\n",
      "Average training loss: 0.030893844020035532\n",
      "Average test loss: 0.006323374483734369\n",
      "Epoch 205/300\n",
      "Average training loss: 0.030906025676263705\n",
      "Average test loss: 0.006323127158814007\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03084614242778884\n",
      "Average test loss: 0.006166790264348189\n",
      "Epoch 207/300\n",
      "Average training loss: 0.030888561584883265\n",
      "Average test loss: 0.006736176589296924\n",
      "Epoch 208/300\n",
      "Average training loss: 0.030860330359803307\n",
      "Average test loss: 0.0062565396341184775\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03081859544912974\n",
      "Average test loss: 0.006367353372275829\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03080733477075895\n",
      "Average test loss: 0.006378905554198557\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03080126972662078\n",
      "Average test loss: 0.007636485024458832\n",
      "Epoch 212/300\n",
      "Average training loss: 0.030823696492446793\n",
      "Average test loss: 0.006305927988555696\n",
      "Epoch 213/300\n",
      "Average training loss: 0.030752414130502278\n",
      "Average test loss: 0.006540509310033586\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03075524053639836\n",
      "Average test loss: 0.006263368650029103\n",
      "Epoch 215/300\n",
      "Average training loss: 0.030794768401318125\n",
      "Average test loss: 0.006296999330321948\n",
      "Epoch 216/300\n",
      "Average training loss: 0.030777742077906928\n",
      "Average test loss: 0.006375727350927061\n",
      "Epoch 217/300\n",
      "Average training loss: 0.030709619378050168\n",
      "Average test loss: 0.006456303953296608\n",
      "Epoch 218/300\n",
      "Average training loss: 0.030749156514803568\n",
      "Average test loss: 0.006319855264905426\n",
      "Epoch 219/300\n",
      "Average training loss: 0.030773758752478494\n",
      "Average test loss: 0.00638300465626849\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03073075000445048\n",
      "Average test loss: 0.006310234144330025\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03066015831960572\n",
      "Average test loss: 0.006609275084402826\n",
      "Epoch 222/300\n",
      "Average training loss: 0.030668460248245134\n",
      "Average test loss: 0.006580502566777997\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03070637277430958\n",
      "Average test loss: 0.006449627326594458\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03065932861963908\n",
      "Average test loss: 0.006396766067379051\n",
      "Epoch 225/300\n",
      "Average training loss: 0.030627575371000502\n",
      "Average test loss: 0.0063033687066700725\n",
      "Epoch 226/300\n",
      "Average training loss: 0.030703240619765388\n",
      "Average test loss: 0.006225923886729611\n",
      "Epoch 227/300\n",
      "Average training loss: 0.030654493711060948\n",
      "Average test loss: 0.006480881359842088\n",
      "Epoch 228/300\n",
      "Average training loss: 0.030581553806861242\n",
      "Average test loss: 0.006308066255930397\n",
      "Epoch 229/300\n",
      "Average training loss: 0.030570687039030923\n",
      "Average test loss: 0.006280345136920611\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03058155345916748\n",
      "Average test loss: 0.006276935379952192\n",
      "Epoch 231/300\n",
      "Average training loss: 0.030588157552811836\n",
      "Average test loss: 0.006444553654640913\n",
      "Epoch 232/300\n",
      "Average training loss: 0.030543562069535256\n",
      "Average test loss: 0.006303865297800965\n",
      "Epoch 233/300\n",
      "Average training loss: 0.030600638035270904\n",
      "Average test loss: 0.006321245558973816\n",
      "Epoch 234/300\n",
      "Average training loss: 0.030530545327398514\n",
      "Average test loss: 0.006308001197046704\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03052244184911251\n",
      "Average test loss: 0.006228856610755126\n",
      "Epoch 236/300\n",
      "Average training loss: 0.030542719814512464\n",
      "Average test loss: 0.00628975233518415\n",
      "Epoch 237/300\n",
      "Average training loss: 0.030494459327724245\n",
      "Average test loss: 0.0065342760421335695\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0305581244164043\n",
      "Average test loss: 0.006363385137584474\n",
      "Epoch 239/300\n",
      "Average training loss: 0.030528254916270574\n",
      "Average test loss: 0.006788277979112333\n",
      "Epoch 240/300\n",
      "Average training loss: 0.030475091411007776\n",
      "Average test loss: 0.006407384980056021\n",
      "Epoch 241/300\n",
      "Average training loss: 0.030497647674547303\n",
      "Average test loss: 0.0067970427398880325\n",
      "Epoch 242/300\n",
      "Average training loss: 0.030465740895933575\n",
      "Average test loss: 0.006265541384203567\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0304610964026716\n",
      "Average test loss: 0.006345044191512796\n",
      "Epoch 244/300\n",
      "Average training loss: 0.030416646096441482\n",
      "Average test loss: 0.006256093013203806\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030432814199063515\n",
      "Average test loss: 0.006333640486829811\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030383458915683958\n",
      "Average test loss: 0.006204099328567584\n",
      "Epoch 247/300\n",
      "Average training loss: 0.030521084467569987\n",
      "Average test loss: 0.006410383107761542\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03036066073510382\n",
      "Average test loss: 0.006562200017273426\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030469833188586766\n",
      "Average test loss: 0.006350644473814302\n",
      "Epoch 250/300\n",
      "Average training loss: 0.030331617007652918\n",
      "Average test loss: 0.006374146323237154\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03034082750313812\n",
      "Average test loss: 0.006319643896280064\n",
      "Epoch 252/300\n",
      "Average training loss: 0.030433068530427085\n",
      "Average test loss: 0.00633355463296175\n",
      "Epoch 253/300\n",
      "Average training loss: 0.030325514352983897\n",
      "Average test loss: 0.006305716192556752\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03030896564655834\n",
      "Average test loss: 0.006279168992406792\n",
      "Epoch 255/300\n",
      "Average training loss: 0.030304601907730103\n",
      "Average test loss: 0.006396680273115635\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03028217919336425\n",
      "Average test loss: 0.006307332662658559\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03035777192645603\n",
      "Average test loss: 0.006401914427677791\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03034132617877589\n",
      "Average test loss: 0.0064382534817689\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030248382116357487\n",
      "Average test loss: 0.006755070391628477\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03031682160993417\n",
      "Average test loss: 0.006335449718766742\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03028850382897589\n",
      "Average test loss: 0.00639170803874731\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030310024233327973\n",
      "Average test loss: 0.0062701146975159645\n",
      "Epoch 263/300\n",
      "Average training loss: 0.030207223902146023\n",
      "Average test loss: 0.007499664334787263\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03029856141242716\n",
      "Average test loss: 0.0065048234322004845\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03024038098255793\n",
      "Average test loss: 0.006581464571257432\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03025589855346415\n",
      "Average test loss: 0.006348615951422188\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030232759427693157\n",
      "Average test loss: 0.0063552602099047764\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030230564769771363\n",
      "Average test loss: 0.006290155008435249\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03019643517004119\n",
      "Average test loss: 0.0064123342409729955\n",
      "Epoch 270/300\n",
      "Average training loss: 0.030139077534278233\n",
      "Average test loss: 0.006601211743222342\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030224584698677063\n",
      "Average test loss: 0.006229013036936522\n",
      "Epoch 272/300\n",
      "Average training loss: 0.030164751672082477\n",
      "Average test loss: 0.006388601423344679\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03016247779627641\n",
      "Average test loss: 0.006424219505654441\n",
      "Epoch 274/300\n",
      "Average training loss: 0.030155617657634946\n",
      "Average test loss: 0.006399671237915754\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03016186248924997\n",
      "Average test loss: 0.00656051446745793\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03022635312212838\n",
      "Average test loss: 0.006447086022959815\n",
      "Epoch 277/300\n",
      "Average training loss: 0.030122306280665926\n",
      "Average test loss: 0.0073466236471301976\n",
      "Epoch 278/300\n",
      "Average training loss: 0.030106169654263392\n",
      "Average test loss: 0.006330972684340345\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03016730116804441\n",
      "Average test loss: 0.0064718141183257106\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030102673653099273\n",
      "Average test loss: 0.006355393177933163\n",
      "Epoch 281/300\n",
      "Average training loss: 0.030073462310764524\n",
      "Average test loss: 0.006358213908556435\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030051028609275818\n",
      "Average test loss: 0.006333874584900008\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03012736749317911\n",
      "Average test loss: 0.00643671152037051\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030086927253339026\n",
      "Average test loss: 0.0063756039916641185\n",
      "Epoch 285/300\n",
      "Average training loss: 0.030054486711819966\n",
      "Average test loss: 0.006287739652726386\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03007397170530425\n",
      "Average test loss: 0.006291826539569431\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03008796925842762\n",
      "Average test loss: 0.006484719117068582\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030072128569086393\n",
      "Average test loss: 0.006568410501711899\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03003975466887156\n",
      "Average test loss: 0.006479722773035368\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030050606530573634\n",
      "Average test loss: 0.006316531968613466\n",
      "Epoch 291/300\n",
      "Average training loss: 0.030006312867005665\n",
      "Average test loss: 0.006307837133606275\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030073546386427348\n",
      "Average test loss: 0.006346933900896046\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03007399870786402\n",
      "Average test loss: 0.006512556616216898\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030003628929456076\n",
      "Average test loss: 0.007578113657318883\n",
      "Epoch 295/300\n",
      "Average training loss: 0.030030706938770083\n",
      "Average test loss: 0.00642286026560598\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03001324707766374\n",
      "Average test loss: 0.006564389864189757\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0299738699644804\n",
      "Average test loss: 0.0063874427593416635\n",
      "Epoch 298/300\n",
      "Average training loss: 0.029936094333728156\n",
      "Average test loss: 0.006476732787158754\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029942035867108238\n",
      "Average test loss: 0.0063509480357170104\n",
      "Epoch 300/300\n",
      "Average training loss: 0.030003569894366793\n",
      "Average test loss: 0.006416061355421941\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13390633746650485\n",
      "Average test loss: 0.007969368746711149\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05023754267560111\n",
      "Average test loss: 0.013445708042217626\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04509503764576382\n",
      "Average test loss: 0.0068597758602764874\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04175226412879096\n",
      "Average test loss: 0.005896826742009984\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03945406600832939\n",
      "Average test loss: 0.005686162990414434\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03767683748404185\n",
      "Average test loss: 0.00674191472472416\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03584429302149349\n",
      "Average test loss: 0.005546004785845677\n",
      "Epoch 8/300\n",
      "Average training loss: 0.034440816928943\n",
      "Average test loss: 0.005458101674500439\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03364302533202701\n",
      "Average test loss: 0.005070828739139769\n",
      "Epoch 10/300\n",
      "Average training loss: 0.032688162854976124\n",
      "Average test loss: 0.0050044982878284325\n",
      "Epoch 11/300\n",
      "Average training loss: 0.031799370442827546\n",
      "Average test loss: 0.004790128367849522\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03116824715005027\n",
      "Average test loss: 0.0048550912174913615\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03048255518078804\n",
      "Average test loss: 0.004515532203639547\n",
      "Epoch 14/300\n",
      "Average training loss: 0.030127076195345985\n",
      "Average test loss: 0.004600010309782293\n",
      "Epoch 15/300\n",
      "Average training loss: 0.029598456501960753\n",
      "Average test loss: 0.004490284474359618\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02917423389189773\n",
      "Average test loss: 0.004377138555670778\n",
      "Epoch 17/300\n",
      "Average training loss: 0.028693563727868927\n",
      "Average test loss: 0.004440181245406469\n",
      "Epoch 18/300\n",
      "Average training loss: 0.028380205358068147\n",
      "Average test loss: 0.00425238942189349\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02816194989366664\n",
      "Average test loss: 0.0050251027254594694\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02777087741262383\n",
      "Average test loss: 0.004189774903779229\n",
      "Epoch 21/300\n",
      "Average training loss: 0.027549083984560435\n",
      "Average test loss: 0.004140191103228264\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027313668603698413\n",
      "Average test loss: 0.004071477370957533\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02706838232278824\n",
      "Average test loss: 0.00407669119661053\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026865902841091156\n",
      "Average test loss: 0.004087227939110663\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026701573961310916\n",
      "Average test loss: 0.004069352171487278\n",
      "Epoch 26/300\n",
      "Average training loss: 0.026468354956971276\n",
      "Average test loss: 0.004034016062815984\n",
      "Epoch 27/300\n",
      "Average training loss: 0.026321222705973518\n",
      "Average test loss: 0.004004396110359166\n",
      "Epoch 28/300\n",
      "Average training loss: 0.026250377198060355\n",
      "Average test loss: 0.0049730488608280814\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026020657016171348\n",
      "Average test loss: 0.003949653646598259\n",
      "Epoch 30/300\n",
      "Average training loss: 0.025968680140872797\n",
      "Average test loss: 0.00404351600073278\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02580289420319928\n",
      "Average test loss: 0.0039149578755928414\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02568767160839505\n",
      "Average test loss: 0.003956095559315549\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02552507241566976\n",
      "Average test loss: 0.003949442326815592\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025489635748995675\n",
      "Average test loss: 0.004068632167660528\n",
      "Epoch 35/300\n",
      "Average training loss: 0.025394476590885055\n",
      "Average test loss: 0.003904375285324123\n",
      "Epoch 36/300\n",
      "Average training loss: 0.025282955113384457\n",
      "Average test loss: 0.004236003049338857\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02518178471426169\n",
      "Average test loss: 0.0038266605004254315\n",
      "Epoch 38/300\n",
      "Average training loss: 0.025096126033200158\n",
      "Average test loss: 0.004240885144927435\n",
      "Epoch 39/300\n",
      "Average training loss: 0.025006748143169614\n",
      "Average test loss: 0.0038472316743185125\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024958446454670696\n",
      "Average test loss: 0.003922710897194015\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02490278949174616\n",
      "Average test loss: 0.0038240812917550403\n",
      "Epoch 42/300\n",
      "Average training loss: 0.024863512709736826\n",
      "Average test loss: 0.0037469334772063626\n",
      "Epoch 43/300\n",
      "Average training loss: 0.024744602354036437\n",
      "Average test loss: 0.0038253022972494365\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02470512543287542\n",
      "Average test loss: 0.004003161971146862\n",
      "Epoch 45/300\n",
      "Average training loss: 0.024654570796423487\n",
      "Average test loss: 0.0038849001100493804\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02455180457731088\n",
      "Average test loss: 0.0037261550579633976\n",
      "Epoch 47/300\n",
      "Average training loss: 0.024537705350253316\n",
      "Average test loss: 0.003782445646615492\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02446071447763178\n",
      "Average test loss: 0.003989703071200185\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024443595134549672\n",
      "Average test loss: 0.0037379465661942957\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024391333045230973\n",
      "Average test loss: 0.0037653050122575626\n",
      "Epoch 51/300\n",
      "Average training loss: 0.024300729364156722\n",
      "Average test loss: 0.0037528401969207656\n",
      "Epoch 52/300\n",
      "Average training loss: 0.024283484619524745\n",
      "Average test loss: 0.003728905389706294\n",
      "Epoch 53/300\n",
      "Average training loss: 0.024231661915779114\n",
      "Average test loss: 0.0037217931497014233\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02417975665297773\n",
      "Average test loss: 0.0037693509132497843\n",
      "Epoch 55/300\n",
      "Average training loss: 0.024166256138020092\n",
      "Average test loss: 0.0037934825310690537\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02411673112048043\n",
      "Average test loss: 0.004246026931537522\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02415028379029698\n",
      "Average test loss: 0.003801910842044486\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02404509156445662\n",
      "Average test loss: 0.0037417557136052186\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024001648323403464\n",
      "Average test loss: 0.0037292373668816356\n",
      "Epoch 60/300\n",
      "Average training loss: 0.023956998955872322\n",
      "Average test loss: 0.0037205113230480086\n",
      "Epoch 61/300\n",
      "Average training loss: 0.023901213073068195\n",
      "Average test loss: 0.003748667466971609\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09004999645385477\n",
      "Average test loss: 0.007042809930940469\n",
      "Epoch 63/300\n",
      "Average training loss: 0.049960522688097427\n",
      "Average test loss: 0.0049226133678522375\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03606992679172092\n",
      "Average test loss: 0.004351026028394699\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03184285342362192\n",
      "Average test loss: 0.004085207025210063\n",
      "Epoch 66/300\n",
      "Average training loss: 0.029794687585698233\n",
      "Average test loss: 0.003974184855818749\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02841059890223874\n",
      "Average test loss: 0.004078026852260034\n",
      "Epoch 68/300\n",
      "Average training loss: 0.027298831931418844\n",
      "Average test loss: 0.0038227976320518386\n",
      "Epoch 69/300\n",
      "Average training loss: 0.026515654671523305\n",
      "Average test loss: 0.0038834284748882055\n",
      "Epoch 70/300\n",
      "Average training loss: 0.025805419302648968\n",
      "Average test loss: 0.0038425678354170587\n",
      "Epoch 71/300\n",
      "Average training loss: 0.025295175087120796\n",
      "Average test loss: 0.003787442388633887\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024850546288821433\n",
      "Average test loss: 0.0037451025475230483\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024543029798401728\n",
      "Average test loss: 0.003730398918191592\n",
      "Epoch 74/300\n",
      "Average training loss: 0.024317824706435204\n",
      "Average test loss: 0.00376656984206703\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02416914145482911\n",
      "Average test loss: 0.0036988980720440547\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02408395158416695\n",
      "Average test loss: 0.0037545763483891885\n",
      "Epoch 77/300\n",
      "Average training loss: 0.024016927767131064\n",
      "Average test loss: 0.003701679994352162\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023961569262875453\n",
      "Average test loss: 0.0037110815168254906\n",
      "Epoch 79/300\n",
      "Average training loss: 0.023949668109416962\n",
      "Average test loss: 0.0036963534073697194\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023864055107037225\n",
      "Average test loss: 0.0037048097314933935\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02383301645020644\n",
      "Average test loss: 0.0037154502330554854\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023833255410194397\n",
      "Average test loss: 0.003671886407252815\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02386030289034049\n",
      "Average test loss: 0.0037164465913342103\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02373835029701392\n",
      "Average test loss: 0.0037115067386378846\n",
      "Epoch 85/300\n",
      "Average training loss: 0.023711951366729208\n",
      "Average test loss: 0.003677394864873754\n",
      "Epoch 86/300\n",
      "Average training loss: 0.023709513697359297\n",
      "Average test loss: 0.003666694988600082\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02365403180155489\n",
      "Average test loss: 0.0037083696108311413\n",
      "Epoch 88/300\n",
      "Average training loss: 0.023634079954690402\n",
      "Average test loss: 0.003754514779895544\n",
      "Epoch 89/300\n",
      "Average training loss: 0.023616764206025334\n",
      "Average test loss: 0.004042766600847244\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023544673008223375\n",
      "Average test loss: 0.004007518258773619\n",
      "Epoch 91/300\n",
      "Average training loss: 0.023539207781354585\n",
      "Average test loss: 0.0036834292478031584\n",
      "Epoch 92/300\n",
      "Average training loss: 0.023506397800313102\n",
      "Average test loss: 0.003750427556948529\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023495873684684435\n",
      "Average test loss: 0.0037078569510744677\n",
      "Epoch 94/300\n",
      "Average training loss: 0.023446785476472644\n",
      "Average test loss: 0.003721307953198751\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02340418921245469\n",
      "Average test loss: 0.003711341003990836\n",
      "Epoch 96/300\n",
      "Average training loss: 0.023404686697655254\n",
      "Average test loss: 0.003668254035628504\n",
      "Epoch 97/300\n",
      "Average training loss: 0.023366566040449673\n",
      "Average test loss: 0.00367079670023587\n",
      "Epoch 98/300\n",
      "Average training loss: 0.023331063855025504\n",
      "Average test loss: 0.003639301122062736\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023320054036047724\n",
      "Average test loss: 0.0036852693553600046\n",
      "Epoch 100/300\n",
      "Average training loss: 0.023270265067617098\n",
      "Average test loss: 0.003716266554676824\n",
      "Epoch 101/300\n",
      "Average training loss: 0.023314410496089195\n",
      "Average test loss: 0.0038703928421147996\n",
      "Epoch 102/300\n",
      "Average training loss: 0.023279538257254494\n",
      "Average test loss: 0.0036714391896708145\n",
      "Epoch 103/300\n",
      "Average training loss: 0.023201166888078054\n",
      "Average test loss: 0.0038518479814132053\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0231992031948434\n",
      "Average test loss: 0.0037121754692246516\n",
      "Epoch 105/300\n",
      "Average training loss: 0.023148153192467158\n",
      "Average test loss: 0.0037125690918829706\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023142019488745266\n",
      "Average test loss: 0.003678069757918517\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0231186244322194\n",
      "Average test loss: 0.0036870783526036476\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023122460535830922\n",
      "Average test loss: 0.003645607566667928\n",
      "Epoch 109/300\n",
      "Average training loss: 0.023095492364631758\n",
      "Average test loss: 0.003694490663707256\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02303248100976149\n",
      "Average test loss: 0.003759071712071697\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02304527032872041\n",
      "Average test loss: 0.003706933895022505\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02301534573899375\n",
      "Average test loss: 0.003717196708545089\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023024502696262467\n",
      "Average test loss: 0.0036898309286269877\n",
      "Epoch 114/300\n",
      "Average training loss: 0.022972606867551804\n",
      "Average test loss: 0.003669285980777608\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022963607135746213\n",
      "Average test loss: 0.0036467242936293284\n",
      "Epoch 116/300\n",
      "Average training loss: 0.022917069974872802\n",
      "Average test loss: 0.0036509446520358326\n",
      "Epoch 117/300\n",
      "Average training loss: 0.022935646588603656\n",
      "Average test loss: 0.003679280038923025\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02287702073322402\n",
      "Average test loss: 0.0037107297068254817\n",
      "Epoch 119/300\n",
      "Average training loss: 0.022895875588059424\n",
      "Average test loss: 0.0036423360374238756\n",
      "Epoch 120/300\n",
      "Average training loss: 0.022887340539031558\n",
      "Average test loss: 0.0036539965408543744\n",
      "Epoch 121/300\n",
      "Average training loss: 0.022832185477018357\n",
      "Average test loss: 0.0036702734333359534\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022803702291515137\n",
      "Average test loss: 0.003858878083113167\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022829234794610076\n",
      "Average test loss: 0.0037164103959997493\n",
      "Epoch 124/300\n",
      "Average training loss: 0.022795766103598805\n",
      "Average test loss: 0.003649757165461779\n",
      "Epoch 125/300\n",
      "Average training loss: 0.022784017475114927\n",
      "Average test loss: 0.003634680163943105\n",
      "Epoch 126/300\n",
      "Average training loss: 0.022775348010990355\n",
      "Average test loss: 0.003684724981172217\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02272488487760226\n",
      "Average test loss: 0.003682395564392209\n",
      "Epoch 128/300\n",
      "Average training loss: 0.022749836340546607\n",
      "Average test loss: 0.0036900495456324684\n",
      "Epoch 129/300\n",
      "Average training loss: 0.022699515432119368\n",
      "Average test loss: 0.0036656962753170066\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02270521328018771\n",
      "Average test loss: 0.003744964680737919\n",
      "Epoch 131/300\n",
      "Average training loss: 0.022687889357407887\n",
      "Average test loss: 0.003687034045242601\n",
      "Epoch 132/300\n",
      "Average training loss: 0.022687988625632392\n",
      "Average test loss: 0.003646839770178\n",
      "Epoch 133/300\n",
      "Average training loss: 0.022672886146439447\n",
      "Average test loss: 0.003714252627144257\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02264256217910184\n",
      "Average test loss: 0.003695308103536566\n",
      "Epoch 135/300\n",
      "Average training loss: 0.022601366264952554\n",
      "Average test loss: 0.0039013370527989336\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022601403032739956\n",
      "Average test loss: 0.004013153658145004\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02258485337595145\n",
      "Average test loss: 0.00364224865535895\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022584770042035313\n",
      "Average test loss: 0.003682358572880427\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022560663552747832\n",
      "Average test loss: 0.0038394798930320474\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022552585393190383\n",
      "Average test loss: 0.0037230523150000305\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02254329381386439\n",
      "Average test loss: 0.00374307590826518\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02253107639319367\n",
      "Average test loss: 0.003825781756805049\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022503067331181632\n",
      "Average test loss: 0.003789760356148084\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02248025654256344\n",
      "Average test loss: 0.003719185130049785\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022475087839696144\n",
      "Average test loss: 0.003678966066075696\n",
      "Epoch 146/300\n",
      "Average training loss: 0.022475254635016123\n",
      "Average test loss: 0.0036871328631209004\n",
      "Epoch 147/300\n",
      "Average training loss: 0.022462610438466073\n",
      "Average test loss: 0.003655588051925103\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022440981823537084\n",
      "Average test loss: 0.003704056742704577\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0224417703780863\n",
      "Average test loss: 0.003785354134109285\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02241407392670711\n",
      "Average test loss: 0.0037363597272584838\n",
      "Epoch 151/300\n",
      "Average training loss: 0.022437799162334866\n",
      "Average test loss: 0.0036735746264457705\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02237221312854025\n",
      "Average test loss: 0.00363108975523048\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02240550492538346\n",
      "Average test loss: 0.0036840499428411324\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022371691268351344\n",
      "Average test loss: 0.004225264446602928\n",
      "Epoch 155/300\n",
      "Average training loss: 0.022343868151307105\n",
      "Average test loss: 0.003673985368468695\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022359149113297463\n",
      "Average test loss: 0.003760983864052428\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022361431957946883\n",
      "Average test loss: 0.0036472947171164883\n",
      "Epoch 158/300\n",
      "Average training loss: 0.022306389186117385\n",
      "Average test loss: 0.0037710938908987576\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02233027328716384\n",
      "Average test loss: 0.003637687887996435\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02231335983177026\n",
      "Average test loss: 0.003818639009156161\n",
      "Epoch 161/300\n",
      "Average training loss: 0.022285325237446363\n",
      "Average test loss: 0.0036860901618169413\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02226850182645851\n",
      "Average test loss: 0.003651784477548467\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022277298889226384\n",
      "Average test loss: 0.0036967872521943518\n",
      "Epoch 164/300\n",
      "Average training loss: 0.022256275786293876\n",
      "Average test loss: 0.003722740335183011\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02224914397795995\n",
      "Average test loss: 0.003916765642869803\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02223118724922339\n",
      "Average test loss: 0.003700912181288004\n",
      "Epoch 167/300\n",
      "Average training loss: 0.022224015606774224\n",
      "Average test loss: 0.0036771752970914044\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022238579655687013\n",
      "Average test loss: 0.00369717264154719\n",
      "Epoch 169/300\n",
      "Average training loss: 0.022213763179050553\n",
      "Average test loss: 0.0036727863933063215\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022203886661264632\n",
      "Average test loss: 0.0036868632675872907\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022179796791738933\n",
      "Average test loss: 0.0036730071728428206\n",
      "Epoch 172/300\n",
      "Average training loss: 0.022175998194350136\n",
      "Average test loss: 0.0037374012958672313\n",
      "Epoch 173/300\n",
      "Average training loss: 0.022182225639621418\n",
      "Average test loss: 0.004080648471497827\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02214092070526547\n",
      "Average test loss: 0.004924844213657909\n",
      "Epoch 175/300\n",
      "Average training loss: 0.022171915168563525\n",
      "Average test loss: 0.0038382541358263954\n",
      "Epoch 176/300\n",
      "Average training loss: 0.022114545805586708\n",
      "Average test loss: 0.0037297869488182997\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022112628577484025\n",
      "Average test loss: 0.00369322002844678\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02210653690662649\n",
      "Average test loss: 0.0037819030974060295\n",
      "Epoch 179/300\n",
      "Average training loss: 0.022092085091604128\n",
      "Average test loss: 0.003687247366540962\n",
      "Epoch 180/300\n",
      "Average training loss: 0.022099928125739098\n",
      "Average test loss: 0.0037207737347731986\n",
      "Epoch 181/300\n",
      "Average training loss: 0.022107060796684688\n",
      "Average test loss: 0.0037227882105443214\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02208764618800746\n",
      "Average test loss: 0.003687942660310202\n",
      "Epoch 183/300\n",
      "Average training loss: 0.022074657062689462\n",
      "Average test loss: 0.0037249336106081805\n",
      "Epoch 184/300\n",
      "Average training loss: 0.022024451659785376\n",
      "Average test loss: 0.0036968635958102014\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02204608684281508\n",
      "Average test loss: 0.0037450380029363764\n",
      "Epoch 186/300\n",
      "Average training loss: 0.022070153411891724\n",
      "Average test loss: 0.00390688226185739\n",
      "Epoch 187/300\n",
      "Average training loss: 0.022023951596683924\n",
      "Average test loss: 0.003943400092216002\n",
      "Epoch 188/300\n",
      "Average training loss: 0.022021015278995037\n",
      "Average test loss: 0.0038336043117774858\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021996225963036218\n",
      "Average test loss: 0.0037943984735757112\n",
      "Epoch 190/300\n",
      "Average training loss: 0.022012728586792946\n",
      "Average test loss: 0.0037056834076841672\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02198556424677372\n",
      "Average test loss: 0.0037506309991909396\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021995033808880383\n",
      "Average test loss: 0.0036922515363742906\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021991643104288312\n",
      "Average test loss: 0.003742633558809757\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021985242749253908\n",
      "Average test loss: 0.0037112146568381126\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02196279163658619\n",
      "Average test loss: 0.003746908391929335\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021955600331226986\n",
      "Average test loss: 0.0037294944843484297\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02198016259736485\n",
      "Average test loss: 0.0038274390780263477\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021935550060537125\n",
      "Average test loss: 0.003843988881756862\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021942010488775043\n",
      "Average test loss: 0.0037302007120516567\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021901348614030414\n",
      "Average test loss: 0.00373983025819891\n",
      "Epoch 201/300\n",
      "Average training loss: 0.021923995695180363\n",
      "Average test loss: 0.0036594937683807477\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02188975900411606\n",
      "Average test loss: 0.003700133965454168\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021907832563751273\n",
      "Average test loss: 0.003727071809892853\n",
      "Epoch 204/300\n",
      "Average training loss: 0.021888070633014043\n",
      "Average test loss: 0.003717273840473758\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021861547988322048\n",
      "Average test loss: 0.0038333008202413718\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021922148043910662\n",
      "Average test loss: 0.0037200707269625533\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02182360564834542\n",
      "Average test loss: 0.003738934492899312\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02186839868956142\n",
      "Average test loss: 0.004347864600519339\n",
      "Epoch 209/300\n",
      "Average training loss: 0.021832069345646434\n",
      "Average test loss: 0.0037808435426818\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021824223970373473\n",
      "Average test loss: 0.0037025616166906222\n",
      "Epoch 211/300\n",
      "Average training loss: 0.021850070107314323\n",
      "Average test loss: 0.0037424709217415914\n",
      "Epoch 212/300\n",
      "Average training loss: 0.021804972623785338\n",
      "Average test loss: 0.004078758102945156\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02180837685200903\n",
      "Average test loss: 0.003693197980316149\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021816302789582145\n",
      "Average test loss: 0.0037017621451781854\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0217868669198619\n",
      "Average test loss: 0.004413662224801051\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02185456139097611\n",
      "Average test loss: 0.0037896080157823033\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02179372063279152\n",
      "Average test loss: 0.0037461738576077754\n",
      "Epoch 218/300\n",
      "Average training loss: 0.021778547171917226\n",
      "Average test loss: 0.003776908958537711\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02174114306105508\n",
      "Average test loss: 0.003786184915444917\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021751298838191564\n",
      "Average test loss: 0.003676483790907595\n",
      "Epoch 221/300\n",
      "Average training loss: 0.021764175156752268\n",
      "Average test loss: 0.003701018923272689\n",
      "Epoch 222/300\n",
      "Average training loss: 0.021756583945618736\n",
      "Average test loss: 0.003943345456487603\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02173573380874263\n",
      "Average test loss: 0.003854475904463066\n",
      "Epoch 224/300\n",
      "Average training loss: 0.021748020379907557\n",
      "Average test loss: 0.003740012427378032\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021734823629260065\n",
      "Average test loss: 0.0036987977100329267\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021718010129200088\n",
      "Average test loss: 0.003749171008666356\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021697877185212242\n",
      "Average test loss: 0.003882944994088676\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02170549369520611\n",
      "Average test loss: 0.003736965505613221\n",
      "Epoch 229/300\n",
      "Average training loss: 0.021721150062150425\n",
      "Average test loss: 0.003919348007895881\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02170269062452846\n",
      "Average test loss: 0.003834798685585459\n",
      "Epoch 231/300\n",
      "Average training loss: 0.021699137629734144\n",
      "Average test loss: 0.003902434035928713\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02168317454556624\n",
      "Average test loss: 0.0037439284566789864\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02169997885492113\n",
      "Average test loss: 0.0038391354551745786\n",
      "Epoch 234/300\n",
      "Average training loss: 0.021655322483844226\n",
      "Average test loss: 0.0037653516742090385\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0216648218350278\n",
      "Average test loss: 0.003810451070467631\n",
      "Epoch 236/300\n",
      "Average training loss: 0.021653121751215722\n",
      "Average test loss: 0.0039457284419073\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02167446732024352\n",
      "Average test loss: 0.003735035559783379\n",
      "Epoch 238/300\n",
      "Average training loss: 0.021623969591326182\n",
      "Average test loss: 0.0037704410298417014\n",
      "Epoch 239/300\n",
      "Average training loss: 0.021631003263923858\n",
      "Average test loss: 0.003877249831540717\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02164512131942643\n",
      "Average test loss: 0.003687784171559744\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02162493403752645\n",
      "Average test loss: 0.004038275047308869\n",
      "Epoch 242/300\n",
      "Average training loss: 0.021613576302925746\n",
      "Average test loss: 0.0037576740806301435\n",
      "Epoch 243/300\n",
      "Average training loss: 0.021625289224916033\n",
      "Average test loss: 0.0037324010700815254\n",
      "Epoch 244/300\n",
      "Average training loss: 0.021619506678647465\n",
      "Average test loss: 0.0037896647147006458\n",
      "Epoch 245/300\n",
      "Average training loss: 0.021603674597210354\n",
      "Average test loss: 0.0038621519121030965\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02162276328768995\n",
      "Average test loss: 0.003774607808639606\n",
      "Epoch 247/300\n",
      "Average training loss: 0.021572183868951267\n",
      "Average test loss: 0.0038356158174574374\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02155688070671426\n",
      "Average test loss: 0.0037433874329759016\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02157105193866624\n",
      "Average test loss: 0.003926048404847582\n",
      "Epoch 250/300\n",
      "Average training loss: 0.021601646413405735\n",
      "Average test loss: 0.003775465732026431\n",
      "Epoch 251/300\n",
      "Average training loss: 0.021557746931910513\n",
      "Average test loss: 0.003823471616953611\n",
      "Epoch 252/300\n",
      "Average training loss: 0.021532250313295257\n",
      "Average test loss: 0.0037381037421938446\n",
      "Epoch 253/300\n",
      "Average training loss: 0.021540462154481146\n",
      "Average test loss: 0.004136406638349096\n",
      "Epoch 254/300\n",
      "Average training loss: 0.021549686898787817\n",
      "Average test loss: 0.0039964399271541174\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02153692237370544\n",
      "Average test loss: 0.003802302285614941\n",
      "Epoch 256/300\n",
      "Average training loss: 0.021526777292291324\n",
      "Average test loss: 0.003802535890705056\n",
      "Epoch 257/300\n",
      "Average training loss: 0.021542781059940657\n",
      "Average test loss: 0.003771231062710285\n",
      "Epoch 258/300\n",
      "Average training loss: 0.021517778466145197\n",
      "Average test loss: 0.0038262443844642906\n",
      "Epoch 259/300\n",
      "Average training loss: 0.021516370923982727\n",
      "Average test loss: 0.0038155648621420067\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02149841599000825\n",
      "Average test loss: 0.0037307657537360987\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02152125846594572\n",
      "Average test loss: 0.003925164253761371\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02149299422899882\n",
      "Average test loss: 0.0038971038862235016\n",
      "Epoch 263/300\n",
      "Average training loss: 0.021488402383195028\n",
      "Average test loss: 0.003903549446413914\n",
      "Epoch 264/300\n",
      "Average training loss: 0.021489094426234562\n",
      "Average test loss: 0.0038020949156747923\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02148770358165105\n",
      "Average test loss: 0.0037859657595141066\n",
      "Epoch 266/300\n",
      "Average training loss: 0.021487247172329162\n",
      "Average test loss: 0.0037310766648087235\n",
      "Epoch 267/300\n",
      "Average training loss: 0.021469704083270497\n",
      "Average test loss: 0.003791016678429312\n",
      "Epoch 268/300\n",
      "Average training loss: 0.021473201649056542\n",
      "Average test loss: 0.0037294309441414145\n",
      "Epoch 269/300\n",
      "Average training loss: 0.021456326420108477\n",
      "Average test loss: 0.0037797652983831034\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02143687258164088\n",
      "Average test loss: 0.003873914895372258\n",
      "Epoch 271/300\n",
      "Average training loss: 0.021462230356203184\n",
      "Average test loss: 0.0038497361321416165\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02146745311551624\n",
      "Average test loss: 0.003786760021415022\n",
      "Epoch 273/300\n",
      "Average training loss: 0.021439657027522725\n",
      "Average test loss: 0.0038598367426958347\n",
      "Epoch 274/300\n",
      "Average training loss: 0.021418526997168858\n",
      "Average test loss: 0.0043705156137131985\n",
      "Epoch 275/300\n",
      "Average training loss: 0.021431570917367936\n",
      "Average test loss: 0.0037542009534728194\n",
      "Epoch 276/300\n",
      "Average training loss: 0.021428633777631653\n",
      "Average test loss: 0.003970687165442441\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02143584245112207\n",
      "Average test loss: 0.003970673337992695\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02139283825788233\n",
      "Average test loss: 0.003778306006350451\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02142721100482676\n",
      "Average test loss: 0.004129438126252757\n",
      "Epoch 280/300\n",
      "Average training loss: 0.021414889850550227\n",
      "Average test loss: 0.0038636520051707826\n",
      "Epoch 281/300\n",
      "Average training loss: 0.021400160051054426\n",
      "Average test loss: 0.003820695778148042\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02140224062320259\n",
      "Average test loss: 0.003878267029921214\n",
      "Epoch 283/300\n",
      "Average training loss: 0.021406146075990465\n",
      "Average test loss: 0.0040111341083215344\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02140727440185017\n",
      "Average test loss: 0.0038811852157943777\n",
      "Epoch 285/300\n",
      "Average training loss: 0.021380799131260977\n",
      "Average test loss: 0.003823975197556946\n",
      "Epoch 286/300\n",
      "Average training loss: 0.021372178537978067\n",
      "Average test loss: 0.003951625160045094\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02135421157711082\n",
      "Average test loss: 0.0038569090138706897\n",
      "Epoch 288/300\n",
      "Average training loss: 0.021376471295952798\n",
      "Average test loss: 0.004151608063735896\n",
      "Epoch 289/300\n",
      "Average training loss: 0.021372449666261673\n",
      "Average test loss: 0.0039148256809761126\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02137260974943638\n",
      "Average test loss: 0.0037854532649119694\n",
      "Epoch 291/300\n",
      "Average training loss: 0.021345528465178277\n",
      "Average test loss: 0.0038487726060880555\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02139241099688742\n",
      "Average test loss: 0.003871439337109526\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02134614994459682\n",
      "Average test loss: 0.0037461751773953437\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02135511843363444\n",
      "Average test loss: 0.0038856025810043017\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02132693457769023\n",
      "Average test loss: 0.003837625651102927\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02131410351395607\n",
      "Average test loss: 0.0039340852105783095\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021338644988007017\n",
      "Average test loss: 0.003812820648153623\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02131109312342273\n",
      "Average test loss: 0.003874576118257311\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02132872755991088\n",
      "Average test loss: 0.003784902054609524\n",
      "Epoch 300/300\n",
      "Average training loss: 0.021326628661817976\n",
      "Average test loss: 0.0038086246920542586\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12244741739498244\n",
      "Average test loss: 0.006180803872644901\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04242665491170353\n",
      "Average test loss: 0.005235610061635573\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03745235169265005\n",
      "Average test loss: 0.004627627171162102\n",
      "Epoch 4/300\n",
      "Average training loss: 0.034630277103847924\n",
      "Average test loss: 0.0044506580891708536\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03238488920198546\n",
      "Average test loss: 0.004326371728960011\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03067042791843414\n",
      "Average test loss: 0.004002262784375085\n",
      "Epoch 7/300\n",
      "Average training loss: 0.028951436407036252\n",
      "Average test loss: 0.004293092478066683\n",
      "Epoch 8/300\n",
      "Average training loss: 0.027763261632786855\n",
      "Average test loss: 0.003848146221496993\n",
      "Epoch 9/300\n",
      "Average training loss: 0.026784148544073103\n",
      "Average test loss: 0.004085812305203743\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02594791015982628\n",
      "Average test loss: 0.0037867335975170138\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02522901637438271\n",
      "Average test loss: 0.0037638854198157787\n",
      "Epoch 12/300\n",
      "Average training loss: 0.024605080985360676\n",
      "Average test loss: 0.0035422697905451057\n",
      "Epoch 13/300\n",
      "Average training loss: 0.024113253742456438\n",
      "Average test loss: 0.003376417580164141\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023667931526899336\n",
      "Average test loss: 0.0032437721884085073\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023203950638572376\n",
      "Average test loss: 0.004959296122607258\n",
      "Epoch 16/300\n",
      "Average training loss: 0.022936565313074322\n",
      "Average test loss: 0.0032256211472882165\n",
      "Epoch 17/300\n",
      "Average training loss: 0.022560374832815595\n",
      "Average test loss: 0.00307063745541705\n",
      "Epoch 18/300\n",
      "Average training loss: 0.022271395002802213\n",
      "Average test loss: 0.003152836752641532\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022003301272789638\n",
      "Average test loss: 0.0029787943495644463\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021783047133021885\n",
      "Average test loss: 0.002965854252792067\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021602513821588624\n",
      "Average test loss: 0.0028829698517090743\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021431389143069586\n",
      "Average test loss: 0.0029657721685038673\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021268449755178557\n",
      "Average test loss: 0.0028727289732131695\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02108045227577289\n",
      "Average test loss: 0.002827136459449927\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02092303556866116\n",
      "Average test loss: 0.0028472040343201824\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020834665232234532\n",
      "Average test loss: 0.0027999053878916633\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020739678367972372\n",
      "Average test loss: 0.0027571617402136327\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020609082346161208\n",
      "Average test loss: 0.002739615558543139\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020492621559235785\n",
      "Average test loss: 0.0028058133210159012\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020382004040810798\n",
      "Average test loss: 0.0028405055776238443\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02031400184167756\n",
      "Average test loss: 0.002708300014750825\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020207878893448248\n",
      "Average test loss: 0.002787117168617745\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02012190603216489\n",
      "Average test loss: 0.0026890909059180154\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020041234200199445\n",
      "Average test loss: 0.00269772548125022\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019959018076459566\n",
      "Average test loss: 0.0027184967783590156\n",
      "Epoch 36/300\n",
      "Average training loss: 0.019889621712267398\n",
      "Average test loss: 0.0027447670977562666\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01984997440709008\n",
      "Average test loss: 0.0027031815426631104\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019782468111978636\n",
      "Average test loss: 0.0026335525032546783\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01969726713664002\n",
      "Average test loss: 0.002634965205979016\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019645149202810394\n",
      "Average test loss: 0.0026412660164965526\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019626703035500313\n",
      "Average test loss: 0.0026293289711078007\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019573097320066558\n",
      "Average test loss: 0.0028374860547482968\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01950239589313666\n",
      "Average test loss: 0.002618547511183553\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019487505989770094\n",
      "Average test loss: 0.002655384864244196\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01941706020467811\n",
      "Average test loss: 0.002737966443515486\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019368694500790703\n",
      "Average test loss: 0.002613504703467091\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019331728958421283\n",
      "Average test loss: 0.002633851060229871\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019293500350581274\n",
      "Average test loss: 0.00262310525505907\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0192567050051358\n",
      "Average test loss: 0.0026060259256304964\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019245993955267802\n",
      "Average test loss: 0.002728365140242709\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019178536135289404\n",
      "Average test loss: 0.0025901945567586354\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019185604290829765\n",
      "Average test loss: 0.0026192817402382693\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0191116589738263\n",
      "Average test loss: 0.0025888382252305746\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019053860891196463\n",
      "Average test loss: 0.002611724288099342\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01905344307008717\n",
      "Average test loss: 0.0026168077939914334\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019011659439239235\n",
      "Average test loss: 0.002589789967259599\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019006349448528553\n",
      "Average test loss: 0.0027136819524069627\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01894747006230884\n",
      "Average test loss: 0.0025602731697468295\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018918062208427323\n",
      "Average test loss: 0.002609583761336075\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01888514252503713\n",
      "Average test loss: 0.002631079753136469\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01887873100489378\n",
      "Average test loss: 0.002562391040432784\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018830793557895555\n",
      "Average test loss: 0.002617908698403173\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018822509426209662\n",
      "Average test loss: 0.0025923511452145046\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018786586435304748\n",
      "Average test loss: 0.0026141587046699393\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018764568621913592\n",
      "Average test loss: 0.002566125742470225\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018741645504203108\n",
      "Average test loss: 0.0025756874181744123\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018738664868805145\n",
      "Average test loss: 0.0026846871198051504\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018687794418798552\n",
      "Average test loss: 0.0026013274000336725\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018680407353573375\n",
      "Average test loss: 0.0029201941858563157\n",
      "Epoch 70/300\n",
      "Average training loss: 0.018650696027610037\n",
      "Average test loss: 0.002555953986114926\n",
      "Epoch 71/300\n",
      "Average training loss: 0.018630480799410078\n",
      "Average test loss: 0.0025599058042797777\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01863124955031607\n",
      "Average test loss: 0.00262738413674136\n",
      "Epoch 73/300\n",
      "Average training loss: 0.018573232776588865\n",
      "Average test loss: 0.0029962666336860923\n",
      "Epoch 74/300\n",
      "Average training loss: 0.018594417522350946\n",
      "Average test loss: 0.0026046280945754715\n",
      "Epoch 75/300\n",
      "Average training loss: 0.018546998247504234\n",
      "Average test loss: 0.0025930436036239067\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01849519827630785\n",
      "Average test loss: 0.002591961463706361\n",
      "Epoch 77/300\n",
      "Average training loss: 0.018531528719597394\n",
      "Average test loss: 0.0025812290178404912\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018485048224528632\n",
      "Average test loss: 0.002563727668383055\n",
      "Epoch 79/300\n",
      "Average training loss: 0.018460855096578598\n",
      "Average test loss: 0.002581808142364025\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01844755413962735\n",
      "Average test loss: 0.0027218304141942\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018436497213939827\n",
      "Average test loss: 0.002850915281102061\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01844314087430636\n",
      "Average test loss: 0.002569065613672137\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01839134175164832\n",
      "Average test loss: 0.002647224986201359\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018394644453293748\n",
      "Average test loss: 0.002561165936705139\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01835906257894304\n",
      "Average test loss: 0.0027221227066798344\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0183443892829948\n",
      "Average test loss: 0.002726756702280707\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018319774488608043\n",
      "Average test loss: 0.0025780590062754022\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018322236811121304\n",
      "Average test loss: 0.0025657795030209754\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0183236320267121\n",
      "Average test loss: 0.002598145722722014\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01827285516096486\n",
      "Average test loss: 0.0028137750698046552\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018250927568309837\n",
      "Average test loss: 0.002626755537465215\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018249647272957695\n",
      "Average test loss: 0.0025447117154382996\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018233026727206177\n",
      "Average test loss: 0.0025804844448963802\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018224644533461995\n",
      "Average test loss: 0.002559920898328225\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018228528043462172\n",
      "Average test loss: 0.002646830750422345\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018172066499789558\n",
      "Average test loss: 0.002578011687638031\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018165989571147494\n",
      "Average test loss: 0.002632217081470622\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018181961498326727\n",
      "Average test loss: 0.002648282537650731\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018141464905606374\n",
      "Average test loss: 0.0026144676264375447\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01814159487022294\n",
      "Average test loss: 0.0025255569436897834\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018138580466310183\n",
      "Average test loss: 0.002748743399563763\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01813248232172595\n",
      "Average test loss: 0.0025503753788976204\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018072465264134936\n",
      "Average test loss: 0.0026887958322962123\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018076781411965687\n",
      "Average test loss: 0.0025712252310994598\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018101122276650533\n",
      "Average test loss: 0.0025596735769261914\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018071114604671795\n",
      "Average test loss: 0.0026726164859202173\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01805416157344977\n",
      "Average test loss: 0.002588345315307379\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018037502000729243\n",
      "Average test loss: 0.0025597328779598076\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0180428953535027\n",
      "Average test loss: 0.0029317327094160847\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018041929170489312\n",
      "Average test loss: 0.002542526927880115\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017964889004826547\n",
      "Average test loss: 0.002540875577678283\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017973978031012746\n",
      "Average test loss: 0.0029242388548122514\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01799139005608029\n",
      "Average test loss: 0.002607579023266832\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0179818384150664\n",
      "Average test loss: 0.006362368921438853\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0179572281125519\n",
      "Average test loss: 0.0025829127814827693\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017921591204073693\n",
      "Average test loss: 0.0025523800328373907\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017931588682863448\n",
      "Average test loss: 0.002612292386901875\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017929569368561108\n",
      "Average test loss: 0.002653536317249139\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0179485012822681\n",
      "Average test loss: 0.0026423343223416144\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01788451124727726\n",
      "Average test loss: 0.0025950058760742345\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01788762259648906\n",
      "Average test loss: 0.0026398464414394565\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01785627630684111\n",
      "Average test loss: 0.002850882365161346\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01788566753268242\n",
      "Average test loss: 0.0025750727790097394\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01785014268093639\n",
      "Average test loss: 0.0025344477341406875\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017831425176726447\n",
      "Average test loss: 0.002582214436390334\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017850944500830437\n",
      "Average test loss: 0.0025370731310298046\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017821824366847674\n",
      "Average test loss: 0.0025607559885829686\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017839517749845983\n",
      "Average test loss: 0.002596709759078092\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017827465739515092\n",
      "Average test loss: 0.0026179100188116235\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017805614616307947\n",
      "Average test loss: 0.0036939296734829746\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017784444663259718\n",
      "Average test loss: 0.0026647127959877255\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017799151296416918\n",
      "Average test loss: 0.00256225716881454\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017760334274835057\n",
      "Average test loss: 0.0026474262786408266\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01777629859166013\n",
      "Average test loss: 0.0025856306449406675\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01774543226013581\n",
      "Average test loss: 0.0026271031387150286\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017761164507932133\n",
      "Average test loss: 0.0026232367342131006\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017744335507353146\n",
      "Average test loss: 0.0025279820991886987\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017715210852523644\n",
      "Average test loss: 0.002612223262174262\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017716680733693972\n",
      "Average test loss: 0.002654342281528645\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017741239173544778\n",
      "Average test loss: 0.0025704839353760086\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017684440430667664\n",
      "Average test loss: 0.0026050932326664527\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017686145588755607\n",
      "Average test loss: 0.0025659454860207107\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01767505344748497\n",
      "Average test loss: 0.002610291737442215\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017665051627490255\n",
      "Average test loss: 0.00258352687727246\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017657579047812354\n",
      "Average test loss: 0.0027738274762199983\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017671111018293433\n",
      "Average test loss: 0.0026304659533003968\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01767373382548491\n",
      "Average test loss: 0.002597394114981095\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017648454195923275\n",
      "Average test loss: 0.002602136417395539\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017637684545583194\n",
      "Average test loss: 0.002635018620433079\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017614869796567494\n",
      "Average test loss: 0.0025739805209967826\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017616822961303923\n",
      "Average test loss: 0.002580799541125695\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01761305050055186\n",
      "Average test loss: 0.002640580868969361\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0176088934979505\n",
      "Average test loss: 0.0032927084085014133\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017606405776407985\n",
      "Average test loss: 0.0026066117874450154\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017586589011881086\n",
      "Average test loss: 0.002553667298828562\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017575164831346936\n",
      "Average test loss: 0.002554719066868226\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017557335684696834\n",
      "Average test loss: 0.0025491245506952206\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01754832383741935\n",
      "Average test loss: 0.0025907508215556544\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017589496069484286\n",
      "Average test loss: 0.002569833586613337\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01754774823784828\n",
      "Average test loss: 0.002618870723992586\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01755515569779608\n",
      "Average test loss: 0.002548579861306482\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01753690905372302\n",
      "Average test loss: 0.002577056023809645\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017512741641865837\n",
      "Average test loss: 0.008116643923438258\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017529768356018598\n",
      "Average test loss: 0.0025863797389384774\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017546265481246843\n",
      "Average test loss: 0.0025746963489800693\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01749478155374527\n",
      "Average test loss: 0.002592377608228061\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017474748777018653\n",
      "Average test loss: 0.002583754324147271\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01749453979730606\n",
      "Average test loss: 0.002612029540869925\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017475734669301246\n",
      "Average test loss: 0.0026488869370271764\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017488208325372803\n",
      "Average test loss: 0.002918154613011413\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017469153329730033\n",
      "Average test loss: 0.0025611300175595614\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017464039493766095\n",
      "Average test loss: 0.0025841584884458116\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017464043517907462\n",
      "Average test loss: 0.00256597686972883\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017473899383511807\n",
      "Average test loss: 0.002621682249009609\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017451467032233874\n",
      "Average test loss: 0.0026321677582131493\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017416866853833198\n",
      "Average test loss: 0.002559324032316605\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017427527093225054\n",
      "Average test loss: 0.0026287544502152337\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017420862283971574\n",
      "Average test loss: 0.0025932869385513996\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017419592723250388\n",
      "Average test loss: 0.0026042642609940634\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017400801612271203\n",
      "Average test loss: 0.0026683555723478396\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01742023177444935\n",
      "Average test loss: 0.0026112361225403015\n",
      "Epoch 182/300\n",
      "Average training loss: 0.017421177920367983\n",
      "Average test loss: 0.0026101690332094828\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01736300756699509\n",
      "Average test loss: 0.0025559324911899038\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017387761412395373\n",
      "Average test loss: 0.0025827630741728676\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017374453783863122\n",
      "Average test loss: 0.0030966722526484065\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01736422990428077\n",
      "Average test loss: 0.0026177528074218166\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017355224622620475\n",
      "Average test loss: 0.0025834091955588925\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017366724458005692\n",
      "Average test loss: 0.002692607955386241\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017337913188669417\n",
      "Average test loss: 0.002596086427362429\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017365268400973743\n",
      "Average test loss: 0.0025631343031095135\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017324379271931117\n",
      "Average test loss: 0.0026750355267690287\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017346697217888302\n",
      "Average test loss: 0.0026531177131045194\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01732916302813424\n",
      "Average test loss: 0.0025839918959471913\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01732191579209434\n",
      "Average test loss: 0.0025703061293396684\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01733829321960608\n",
      "Average test loss: 0.0026736300513148306\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017313306232293445\n",
      "Average test loss: 0.002626590023438136\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01730888514800204\n",
      "Average test loss: 0.002595416951303681\n",
      "Epoch 198/300\n",
      "Average training loss: 0.017292394216689797\n",
      "Average test loss: 0.0026584521604494918\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01731393525335524\n",
      "Average test loss: 0.005721111099753115\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017284595121939975\n",
      "Average test loss: 0.002614322742343777\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017288351762625907\n",
      "Average test loss: 0.0026999430269416835\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017275568056437703\n",
      "Average test loss: 0.002667087618054615\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01729002579715517\n",
      "Average test loss: 0.0026707368331650894\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01726945077213976\n",
      "Average test loss: 0.0027236007274025016\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01729829488280747\n",
      "Average test loss: 0.0025938735467692215\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017229872271418572\n",
      "Average test loss: 0.0025771064551340208\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01726035418940915\n",
      "Average test loss: 0.0028693614351666635\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01723973870442973\n",
      "Average test loss: 0.002596875667985943\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01724253479308552\n",
      "Average test loss: 0.0026930153630673886\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017224059209227562\n",
      "Average test loss: 0.0025915824288709295\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017222175121307373\n",
      "Average test loss: 0.0026296278244505324\n",
      "Epoch 212/300\n",
      "Average training loss: 0.017223329726192685\n",
      "Average test loss: 0.002568955858134561\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017246127045816847\n",
      "Average test loss: 0.0025823549702763556\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017215550949176154\n",
      "Average test loss: 0.0028470996516860195\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017190806100765864\n",
      "Average test loss: 0.0025633445932633347\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017202331357532078\n",
      "Average test loss: 0.0027905944823804828\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017226264965203072\n",
      "Average test loss: 0.0026384686719005306\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017181139157878028\n",
      "Average test loss: 0.0026192469551331466\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01716124900761578\n",
      "Average test loss: 0.002756835171745883\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017181988656520843\n",
      "Average test loss: 0.002609570761107736\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017175497853093677\n",
      "Average test loss: 0.0027725685658968155\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017200348171095052\n",
      "Average test loss: 0.0027205716744065286\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01715593664182557\n",
      "Average test loss: 0.0026874440295828715\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01716190540542205\n",
      "Average test loss: 0.0027018524350391495\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01716960649440686\n",
      "Average test loss: 0.002614151248501407\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01714874102671941\n",
      "Average test loss: 0.002932364060026076\n",
      "Epoch 227/300\n",
      "Average training loss: 0.017167565824257002\n",
      "Average test loss: 0.0025729623722533386\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01714811340967814\n",
      "Average test loss: 0.0026363117318186496\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01713457970983452\n",
      "Average test loss: 0.0027512711646656197\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01715029965672228\n",
      "Average test loss: 0.0026348033741944365\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01711551529665788\n",
      "Average test loss: 0.002583339317391316\n",
      "Epoch 232/300\n",
      "Average training loss: 0.017125691869192654\n",
      "Average test loss: 0.0025834784503612255\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01711805147760444\n",
      "Average test loss: 0.0026753772782782716\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017127401484383478\n",
      "Average test loss: 0.002635123788068692\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017102469093269772\n",
      "Average test loss: 0.0026369997519585823\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017103341673811275\n",
      "Average test loss: 0.0026143169140236244\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01710250747617748\n",
      "Average test loss: 0.0026631637770268652\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017098472936285868\n",
      "Average test loss: 0.00276207506035765\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01709487994843059\n",
      "Average test loss: 0.0028006378677156237\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017070861504309708\n",
      "Average test loss: 0.0027100254787753024\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01708822271724542\n",
      "Average test loss: 0.0025838425923138857\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017076383959915904\n",
      "Average test loss: 0.0027150838139156502\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017102515487207308\n",
      "Average test loss: 0.0026169660006546314\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017069396818677585\n",
      "Average test loss: 0.0026196274384856226\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01706417587896188\n",
      "Average test loss: 0.0026561234564416937\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017079388295610745\n",
      "Average test loss: 0.0026337760926948652\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017066348031163216\n",
      "Average test loss: 0.0032978868511401947\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017039516220490138\n",
      "Average test loss: 0.0026892227789180145\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017044811400274435\n",
      "Average test loss: 0.0026156082813524537\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01706755545321438\n",
      "Average test loss: 0.0027137024423314463\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01704642183581988\n",
      "Average test loss: 0.002876839196930329\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017035346285336546\n",
      "Average test loss: 0.0026091694293750656\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017028507424725427\n",
      "Average test loss: 0.0026352147239570817\n",
      "Epoch 254/300\n",
      "Average training loss: 0.017056670513417985\n",
      "Average test loss: 0.0026045076673229537\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017023041418857045\n",
      "Average test loss: 0.0025844586680953702\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017024906382792526\n",
      "Average test loss: 0.002602027493632502\n",
      "Epoch 257/300\n",
      "Average training loss: 0.017017875000834466\n",
      "Average test loss: 0.002613406262257033\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017033608230451743\n",
      "Average test loss: 0.0026961812367662786\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01701706047024992\n",
      "Average test loss: 0.0026471633478585216\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01699789319601324\n",
      "Average test loss: 0.0026211866833683517\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017012170745266807\n",
      "Average test loss: 0.002621216414289342\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016997627154820494\n",
      "Average test loss: 0.0026606297209444975\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016986307175623046\n",
      "Average test loss: 0.0026265512050853835\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016978459601600965\n",
      "Average test loss: 0.0026504887118935583\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01700159470902549\n",
      "Average test loss: 0.002629300321866241\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01700102975136704\n",
      "Average test loss: 0.002618986298640569\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01696506561835607\n",
      "Average test loss: 0.0026824391095174684\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016974752861592505\n",
      "Average test loss: 0.002699167331059774\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016969625703162617\n",
      "Average test loss: 0.002632488045013613\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016965044178068638\n",
      "Average test loss: 0.0026311254385444852\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01698723143339157\n",
      "Average test loss: 0.002670357021606631\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016954275010360613\n",
      "Average test loss: 0.0026662538264774615\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016968637257814407\n",
      "Average test loss: 0.0028430140438593095\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01696549799376064\n",
      "Average test loss: 0.0026743639655825166\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016942721740239197\n",
      "Average test loss: 0.0026681722794762914\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01692995268520382\n",
      "Average test loss: 0.0026806877363059257\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016941199786133235\n",
      "Average test loss: 0.002768516411590907\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01694788936773936\n",
      "Average test loss: 0.002703689043720563\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01692619819111294\n",
      "Average test loss: 0.0027234468102041217\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016915548973613314\n",
      "Average test loss: 0.002652163999983006\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01693911821146806\n",
      "Average test loss: 0.0026072883583191367\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016941049305929077\n",
      "Average test loss: 0.0027272686554739873\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016911166903045442\n",
      "Average test loss: 0.0027632399033755066\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016900913087858092\n",
      "Average test loss: 0.0026736547979008822\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016901245882941618\n",
      "Average test loss: 0.002657882649658455\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016909431379702355\n",
      "Average test loss: 0.002682853381252951\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016915694567892287\n",
      "Average test loss: 0.002677505489645733\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016900341203643216\n",
      "Average test loss: 0.002715864368196991\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016912910881969664\n",
      "Average test loss: 0.002653335021601783\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01689326845606168\n",
      "Average test loss: 0.0026612992317726213\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016874954925643074\n",
      "Average test loss: 0.002609410540718171\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01687076817618476\n",
      "Average test loss: 0.0026528667747560473\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016885100424289704\n",
      "Average test loss: 0.002634235789999366\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01687055462764369\n",
      "Average test loss: 0.002666085340703527\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016896248665120867\n",
      "Average test loss: 0.0026854591717322667\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016867141670650905\n",
      "Average test loss: 0.0026817322625882095\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016876214005880887\n",
      "Average test loss: 0.0026382799092680214\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01686033227543036\n",
      "Average test loss: 0.002648876244823138\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016876371974746385\n",
      "Average test loss: 0.002658820379525423\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016856113735172484\n",
      "Average test loss: 0.0026137765896403127\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11020900307099024\n",
      "Average test loss: 0.005236166911406649\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03669677265153991\n",
      "Average test loss: 0.0042075665473110145\n",
      "Epoch 3/300\n",
      "Average training loss: 0.031891343033976026\n",
      "Average test loss: 0.004361425518368682\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02932754310965538\n",
      "Average test loss: 0.003800308898298277\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02762384375764264\n",
      "Average test loss: 0.003250378020521667\n",
      "Epoch 6/300\n",
      "Average training loss: 0.025756382872660955\n",
      "Average test loss: 0.003288642549266418\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02465469890170627\n",
      "Average test loss: 0.003315710758169492\n",
      "Epoch 8/300\n",
      "Average training loss: 0.023435050149758656\n",
      "Average test loss: 0.0029663330687003005\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022544670487443606\n",
      "Average test loss: 0.002982845797100001\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0218079353686836\n",
      "Average test loss: 0.0028036824274394246\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021236519397960767\n",
      "Average test loss: 0.002722594307942523\n",
      "Epoch 12/300\n",
      "Average training loss: 0.020704654888974296\n",
      "Average test loss: 0.0026765175389332906\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02024769906534089\n",
      "Average test loss: 0.0024816816331197818\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019888116920987766\n",
      "Average test loss: 0.0026863509457972314\n",
      "Epoch 15/300\n",
      "Average training loss: 0.019519370289312468\n",
      "Average test loss: 0.002514510703790519\n",
      "Epoch 16/300\n",
      "Average training loss: 0.019171621960070398\n",
      "Average test loss: 0.0024088266869593\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01895698946714401\n",
      "Average test loss: 0.002340551588166919\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018625870535771052\n",
      "Average test loss: 0.002577495814818475\n",
      "Epoch 19/300\n",
      "Average training loss: 0.018464218561848003\n",
      "Average test loss: 0.0023532533071314297\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018271498499645128\n",
      "Average test loss: 0.0022186339910452565\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018083440588580236\n",
      "Average test loss: 0.002325633336479465\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01794003424462345\n",
      "Average test loss: 0.00220395141446756\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017780728351738717\n",
      "Average test loss: 0.0021645807644559275\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017668711407317055\n",
      "Average test loss: 0.0022578339872674808\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017552874353196884\n",
      "Average test loss: 0.0021440710483325853\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017435264004601374\n",
      "Average test loss: 0.0021224155156976646\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017292465021212896\n",
      "Average test loss: 0.002081426462986403\n",
      "Epoch 28/300\n",
      "Average training loss: 0.017259090335832703\n",
      "Average test loss: 0.002141434673840801\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0171167427715328\n",
      "Average test loss: 0.002067092340853479\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01704302784552177\n",
      "Average test loss: 0.0020801712038616338\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016974301907751295\n",
      "Average test loss: 0.0020114910161743564\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0168886938608355\n",
      "Average test loss: 0.002068175745300121\n",
      "Epoch 33/300\n",
      "Average training loss: 0.016826260531114207\n",
      "Average test loss: 0.002042457535552482\n",
      "Epoch 34/300\n",
      "Average training loss: 0.016826166490713754\n",
      "Average test loss: 0.002049479552751614\n",
      "Epoch 35/300\n",
      "Average training loss: 0.016720443724758095\n",
      "Average test loss: 0.0020329352017078137\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01662933578921689\n",
      "Average test loss: 0.002024429066106677\n",
      "Epoch 37/300\n",
      "Average training loss: 0.016593775258296067\n",
      "Average test loss: 0.0019914873713213536\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01652369076675839\n",
      "Average test loss: 0.0020441134819347\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016509937490026157\n",
      "Average test loss: 0.0020156227379209465\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01647245834271113\n",
      "Average test loss: 0.0020100335062791905\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01638723899672429\n",
      "Average test loss: 0.0019613196697706977\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01635927295933167\n",
      "Average test loss: 0.001970537771160404\n",
      "Epoch 43/300\n",
      "Average training loss: 0.016338241057263482\n",
      "Average test loss: 0.001978111667558551\n",
      "Epoch 44/300\n",
      "Average training loss: 0.016284870968924628\n",
      "Average test loss: 0.0019638128427581654\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016286961715254518\n",
      "Average test loss: 0.0020312572171290715\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016217863702939615\n",
      "Average test loss: 0.0020005309511389995\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01615051457368665\n",
      "Average test loss: 0.0019432943385715286\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016125815673006905\n",
      "Average test loss: 0.0019533664799398845\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01609498540808757\n",
      "Average test loss: 0.0019816221024634107\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01608309403558572\n",
      "Average test loss: 0.001957242955143253\n",
      "Epoch 51/300\n",
      "Average training loss: 0.016054250036676723\n",
      "Average test loss: 0.001953563728887174\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015980619000064002\n",
      "Average test loss: 0.0019891989270432128\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016004321535428367\n",
      "Average test loss: 0.0019300048409236802\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015950583087073433\n",
      "Average test loss: 0.0020919253004507885\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015942949417150683\n",
      "Average test loss: 0.0019891829145037465\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015885265552335315\n",
      "Average test loss: 0.0019552289210259914\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015865626585152413\n",
      "Average test loss: 0.0019397931747759383\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015830435114602248\n",
      "Average test loss: 0.0019491723506814905\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015839318060212666\n",
      "Average test loss: 0.0019556372679976953\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015792813293221925\n",
      "Average test loss: 0.0020117165493882365\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0157677888787455\n",
      "Average test loss: 0.001964228784458505\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015750910576846865\n",
      "Average test loss: 0.0020169700605587827\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01573090900480747\n",
      "Average test loss: 0.001951977695338428\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01571276942723327\n",
      "Average test loss: 0.002006143344566226\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015661488795446026\n",
      "Average test loss: 0.0019312598009904225\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01568247885670927\n",
      "Average test loss: 0.001908969039304389\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0156536023914814\n",
      "Average test loss: 0.002707360855821106\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015626343349615733\n",
      "Average test loss: 0.0019267191292925013\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015603328069051106\n",
      "Average test loss: 0.0019428637137429582\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015591075975033971\n",
      "Average test loss: 0.0019384308762641416\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015569908069239722\n",
      "Average test loss: 0.0019566367447583213\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015528183917204539\n",
      "Average test loss: 0.001942309152127968\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015534946001238293\n",
      "Average test loss: 0.0019213184271421697\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015551178651551405\n",
      "Average test loss: 0.0019746065821705594\n",
      "Epoch 75/300\n",
      "Average training loss: 0.015477777143319448\n",
      "Average test loss: 0.0019785298384312126\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015492687857813305\n",
      "Average test loss: 0.0019387501021847129\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015476786016590065\n",
      "Average test loss: 0.0019631021353933545\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015444878147708046\n",
      "Average test loss: 0.001998553142675923\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01543118000527223\n",
      "Average test loss: 0.0019380806206415098\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015412913137012058\n",
      "Average test loss: 0.0019413075463639365\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015391726874642901\n",
      "Average test loss: 0.0019238251180698474\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015382568490174081\n",
      "Average test loss: 0.0019034107153614362\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015349445768528515\n",
      "Average test loss: 0.001930781058760153\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015367843488852184\n",
      "Average test loss: 0.002044228615446223\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015344590826167001\n",
      "Average test loss: 0.0019121566739761168\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015308134022686217\n",
      "Average test loss: 0.0019185708425939084\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015306898859639963\n",
      "Average test loss: 0.002032868449576199\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015298104897141456\n",
      "Average test loss: 0.0019199572410434484\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01528454043964545\n",
      "Average test loss: 0.0019797040364808506\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01525463221139378\n",
      "Average test loss: 0.002863556379245387\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01528221825344695\n",
      "Average test loss: 0.0019257607913265625\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015230508585770925\n",
      "Average test loss: 0.0019519319591215915\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015228479728102684\n",
      "Average test loss: 0.0019159446669121582\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015201485075884395\n",
      "Average test loss: 0.001914068280098339\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015209519181814459\n",
      "Average test loss: 0.001973252057822214\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01519399351047145\n",
      "Average test loss: 0.001936212052177224\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015192874466379484\n",
      "Average test loss: 0.0019249297576025128\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01518291999730799\n",
      "Average test loss: 0.0019273464360998736\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015154577914211485\n",
      "Average test loss: 0.001962870114069018\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015126905198726389\n",
      "Average test loss: 0.0019311797596100304\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015128176152706146\n",
      "Average test loss: 0.0019092595185049707\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015142701173822085\n",
      "Average test loss: 0.0019555465895682573\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01510212039285236\n",
      "Average test loss: 0.0019059560577281648\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015096865729325348\n",
      "Average test loss: 0.0019644493997717895\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015095009139842457\n",
      "Average test loss: 0.0018979140209654966\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015085672785838444\n",
      "Average test loss: 0.001922380534104175\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01505567696524991\n",
      "Average test loss: 0.001892715643056565\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015048762672477298\n",
      "Average test loss: 0.0019044359046965838\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015042199777232276\n",
      "Average test loss: 0.0018973695718579823\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015016076260970698\n",
      "Average test loss: 0.0035236060372036363\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015048475094967418\n",
      "Average test loss: 0.002019233563500974\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015020064605606927\n",
      "Average test loss: 0.001976832172522942\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015011075302958489\n",
      "Average test loss: 0.0019106238302257325\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014999201698435678\n",
      "Average test loss: 0.0019209973317467503\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01498005034857326\n",
      "Average test loss: 0.0019320034386797083\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014986437564094861\n",
      "Average test loss: 0.0019989744215789767\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01497098387695021\n",
      "Average test loss: 0.00194210704560909\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014961411794026692\n",
      "Average test loss: 0.0019153488578481806\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014943967337409655\n",
      "Average test loss: 0.001932048115051455\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014927033108141688\n",
      "Average test loss: 0.001900657630422049\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014945290377570523\n",
      "Average test loss: 0.0019287808418480886\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014939290529323948\n",
      "Average test loss: 0.18797225577632587\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01597321469336748\n",
      "Average test loss: 0.001958635033418735\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014917446813649602\n",
      "Average test loss: 0.0019045643655376302\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014900140267279413\n",
      "Average test loss: 0.0019216511564122305\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014872348277933068\n",
      "Average test loss: 0.0019038605636192694\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014868188011149565\n",
      "Average test loss: 0.0019222374686764347\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014861982701553238\n",
      "Average test loss: 0.0019333512537802258\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01486743602156639\n",
      "Average test loss: 0.0019616048757193816\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014863417560855548\n",
      "Average test loss: 0.0019214571598503325\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01482820320294963\n",
      "Average test loss: 0.001922976989299059\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014850367740624482\n",
      "Average test loss: 0.0019482303347645535\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014817130610346793\n",
      "Average test loss: 0.0019091569011410077\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014827010996639729\n",
      "Average test loss: 0.001951146056358185\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014819743438313404\n",
      "Average test loss: 0.0020967916349569956\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01479493138194084\n",
      "Average test loss: 0.0019481791909784078\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014811727550294664\n",
      "Average test loss: 0.0019780501287637487\n",
      "Epoch 138/300\n",
      "Average training loss: 0.014785021189186308\n",
      "Average test loss: 0.0019252464885099066\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014780958895054128\n",
      "Average test loss: 0.002014790684502158\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014794033081167274\n",
      "Average test loss: 0.0019694097354594205\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014775189373228284\n",
      "Average test loss: 0.001936520574407445\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014766333392096891\n",
      "Average test loss: 0.001923882556044393\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014753501216570536\n",
      "Average test loss: 0.0019100736075391373\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014756605658266279\n",
      "Average test loss: 0.0019482380505651236\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01474210379521052\n",
      "Average test loss: 0.0019241400764634213\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014732408314943313\n",
      "Average test loss: 0.0019439682563145955\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0147254063155916\n",
      "Average test loss: 0.001916277752775285\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014721912555396557\n",
      "Average test loss: 0.0019015089944005013\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014709717081652748\n",
      "Average test loss: 0.0019338539491097133\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014703919670648045\n",
      "Average test loss: 0.0019476570160024695\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014686204140384991\n",
      "Average test loss: 0.0019633031311548418\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014708196215331554\n",
      "Average test loss: 0.0019154755069563786\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014672719707919491\n",
      "Average test loss: 0.0019277064847863382\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014667348692814509\n",
      "Average test loss: 0.0019165242166361875\n",
      "Epoch 155/300\n",
      "Average training loss: 0.014673148241308\n",
      "Average test loss: 0.0019582363785141045\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014652290793756644\n",
      "Average test loss: 0.0019279628263579474\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014659202325675223\n",
      "Average test loss: 0.0019481649827212096\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014644312231077088\n",
      "Average test loss: 0.001963480088652836\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014644738450646401\n",
      "Average test loss: 0.001974851740938094\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014612489092681143\n",
      "Average test loss: 0.0019511879450745053\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01464642757177353\n",
      "Average test loss: 0.0019142117969070873\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0146134531531069\n",
      "Average test loss: 0.001932766484717528\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01460283186369472\n",
      "Average test loss: 0.0019705829684519105\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014622048107286295\n",
      "Average test loss: 0.001989070122130215\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014587002077036434\n",
      "Average test loss: 0.0019703459940436812\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014599655287961165\n",
      "Average test loss: 0.001978504807170894\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014593666012088457\n",
      "Average test loss: 0.0019171444688820176\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014600603563090165\n",
      "Average test loss: 0.0019469392591466507\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01460162753197882\n",
      "Average test loss: 0.001973564115559889\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014565186149544187\n",
      "Average test loss: 0.001987319196160469\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014565828844077058\n",
      "Average test loss: 0.0019260457067026033\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014562828944789039\n",
      "Average test loss: 0.0019281278390230405\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014550271939072344\n",
      "Average test loss: 0.0019255283456295728\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014556654651959737\n",
      "Average test loss: 0.0019414853495028283\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014548604376614094\n",
      "Average test loss: 0.001998309978801343\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014541573637061649\n",
      "Average test loss: 0.0019467107786072626\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014526314452290535\n",
      "Average test loss: 0.0019703440701382027\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014520689523054494\n",
      "Average test loss: 0.0020350615847855805\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014523697281877199\n",
      "Average test loss: 0.0019409041106700897\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014527889016601774\n",
      "Average test loss: 0.0019235624042339622\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014514529641303751\n",
      "Average test loss: 0.0019330881046545174\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014498557726542155\n",
      "Average test loss: 0.0019515075275881423\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014484666058586703\n",
      "Average test loss: 0.0019255776206652322\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014493393602470556\n",
      "Average test loss: 0.001975405509894093\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014472394877837764\n",
      "Average test loss: 0.0019721747481574615\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014491839321951071\n",
      "Average test loss: 0.0019489064775407314\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014477710004482005\n",
      "Average test loss: 0.0019219085305101343\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01446098489397102\n",
      "Average test loss: 0.0019385514236572715\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014456436017321215\n",
      "Average test loss: 0.00197443074050049\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014444989471799797\n",
      "Average test loss: 0.001980894606250028\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01444110550151931\n",
      "Average test loss: 0.0019601774553043975\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014447762018276586\n",
      "Average test loss: 0.0020099306346641646\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014455823161535793\n",
      "Average test loss: 0.001981656174382402\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014441476095053885\n",
      "Average test loss: 0.0019642281809614764\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014423234050472578\n",
      "Average test loss: 0.0019357307929959563\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01440966463006205\n",
      "Average test loss: 0.0019605271912490328\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014428976332975758\n",
      "Average test loss: 0.0019223183506271906\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014414399722384083\n",
      "Average test loss: 0.001964911356361376\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014412700156370799\n",
      "Average test loss: 0.11554117233223385\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01730769903378354\n",
      "Average test loss: 0.0019976518783304427\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014533659715619353\n",
      "Average test loss: 0.00193845818284899\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014376769499646292\n",
      "Average test loss: 0.001911388337198231\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01434931777500444\n",
      "Average test loss: 0.0019524258724931214\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014342734243306848\n",
      "Average test loss: 0.0019354215330547758\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014343638707366255\n",
      "Average test loss: 0.0019384503941982986\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014363862290150589\n",
      "Average test loss: 0.0019579205904155968\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014390820364157359\n",
      "Average test loss: 0.0019611542696754136\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014350991690324413\n",
      "Average test loss: 0.0019936291727547842\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014391493087013563\n",
      "Average test loss: 0.00191972110254897\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014354019501143032\n",
      "Average test loss: 0.001948148198839691\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014355465587642459\n",
      "Average test loss: 0.001977741405988733\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014347405162122514\n",
      "Average test loss: 0.0019493452347815036\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014387091022398737\n",
      "Average test loss: 0.0019381325814045137\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01435778597659535\n",
      "Average test loss: 0.0019950340198766855\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014352658621966838\n",
      "Average test loss: 0.0019446429457101557\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014348142674399747\n",
      "Average test loss: 0.001993893245855967\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014341212916705343\n",
      "Average test loss: 0.00194728963687602\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01431948718842533\n",
      "Average test loss: 0.0019277829743093915\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014331257709198528\n",
      "Average test loss: 0.0020083681326359512\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014338984000186126\n",
      "Average test loss: 0.002017779322237604\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014328828697403272\n",
      "Average test loss: 0.0019865260902378295\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014326883481608496\n",
      "Average test loss: 0.001966197857633233\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014301482218007247\n",
      "Average test loss: 0.0019731297749612064\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014330599456197685\n",
      "Average test loss: 0.0020181561122751897\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014301275918881098\n",
      "Average test loss: 0.0019443054250958894\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014301113178332646\n",
      "Average test loss: 0.0020032878319422402\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014305023989743657\n",
      "Average test loss: 0.0020049110413011577\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014292541267143355\n",
      "Average test loss: 0.0019280317874832286\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014290051518215073\n",
      "Average test loss: 0.001938228063388831\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01429658566498094\n",
      "Average test loss: 0.001971371729289078\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014274550509949526\n",
      "Average test loss: 0.0019410821503649156\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014264163302878538\n",
      "Average test loss: 0.001940555731145044\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01427467614826229\n",
      "Average test loss: 0.0020213622589492137\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014269829163948694\n",
      "Average test loss: 0.0019315430972104272\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014270284227199025\n",
      "Average test loss: 0.001996147877847155\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014268511009713014\n",
      "Average test loss: 0.002000964517808623\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014248455570803748\n",
      "Average test loss: 0.0019367974878599246\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014253076014419396\n",
      "Average test loss: 0.0019530488991489014\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014235918059945106\n",
      "Average test loss: 0.00198477160734021\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01425714941488372\n",
      "Average test loss: 0.0019890764114550417\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014233924433588981\n",
      "Average test loss: 0.002009986688900325\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014225272566080093\n",
      "Average test loss: 0.003082263702733649\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014230505525237984\n",
      "Average test loss: 0.0019769165116465754\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014223886173218488\n",
      "Average test loss: 0.001999603329640296\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014210674131910007\n",
      "Average test loss: 0.0019535040615333453\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014221774723794725\n",
      "Average test loss: 0.0019852157112004026\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01420822347369459\n",
      "Average test loss: 0.001932238851984342\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014212809786200523\n",
      "Average test loss: 0.001995442476744453\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014221605128712124\n",
      "Average test loss: 0.0020173290956558452\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014203493988348378\n",
      "Average test loss: 0.002183788508280284\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014213322553369734\n",
      "Average test loss: 0.001963412463044127\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014208834360043207\n",
      "Average test loss: 0.00194957435131073\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014182578767339388\n",
      "Average test loss: 0.0020207962185765306\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014205391329195764\n",
      "Average test loss: 0.0019982671425160434\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014180021783543958\n",
      "Average test loss: 0.001931284363898966\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014168313914702997\n",
      "Average test loss: 0.0020048049786645507\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014173168768485386\n",
      "Average test loss: 0.0020139500852674247\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014173321814172798\n",
      "Average test loss: 0.0019914535586204795\n",
      "Epoch 259/300\n",
      "Average training loss: 0.014187873107691606\n",
      "Average test loss: 0.0019689500057655904\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014159752244750658\n",
      "Average test loss: 0.001963390654987759\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014157431733277109\n",
      "Average test loss: 0.001972939407452941\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01415539524787002\n",
      "Average test loss: 0.0019413305719693501\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014165511278642549\n",
      "Average test loss: 0.001943621126210524\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014154601386023893\n",
      "Average test loss: 0.001952021207453476\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014139355609814327\n",
      "Average test loss: 0.0019860031722734373\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014145738790432611\n",
      "Average test loss: 0.001940535970653097\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014155852581891748\n",
      "Average test loss: 0.0020302355780990587\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014140612159338262\n",
      "Average test loss: 0.0019820933864555424\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01412685515731573\n",
      "Average test loss: 0.0019587979695449274\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014130716443889671\n",
      "Average test loss: 0.0019570713313296437\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014125642898182075\n",
      "Average test loss: 0.0019557164316583013\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014107073313660092\n",
      "Average test loss: 0.0019514702583352725\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014156387036045392\n",
      "Average test loss: 0.0019785228439399764\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014111275316940414\n",
      "Average test loss: 0.002047680713960694\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014115244303312567\n",
      "Average test loss: 0.001982890164790054\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014115525724159347\n",
      "Average test loss: 0.003327976792636845\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014112162742349837\n",
      "Average test loss: 0.0019851636971450516\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014097360590265857\n",
      "Average test loss: 0.0019692177268055577\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014097028668555948\n",
      "Average test loss: 0.001954432806207074\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01408197305103143\n",
      "Average test loss: 0.0019666892080050374\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014090787783265115\n",
      "Average test loss: 0.0021187253093553913\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01408950395633777\n",
      "Average test loss: 0.0020851909316455326\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01409196221580108\n",
      "Average test loss: 0.0020889390692528753\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014084828258388572\n",
      "Average test loss: 0.0019567327815521924\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014075745407078\n",
      "Average test loss: 0.0019987235977831815\n",
      "Epoch 286/300\n",
      "Average training loss: 0.014071408503585391\n",
      "Average test loss: 0.0019435588637780811\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014061410925454564\n",
      "Average test loss: 0.002015873758950167\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014066051380501854\n",
      "Average test loss: 0.001996467909672194\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014061053935852316\n",
      "Average test loss: 0.0019634335686763127\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014067078191373083\n",
      "Average test loss: 0.0020039464746498398\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014061230779521995\n",
      "Average test loss: 0.0019376878545929988\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014061828295389812\n",
      "Average test loss: 0.0019895117392556535\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014056485898792744\n",
      "Average test loss: 0.002014574050489399\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014043545620308982\n",
      "Average test loss: 0.0019864933574572207\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01405968697865804\n",
      "Average test loss: 0.0019453431092616585\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014049606968959173\n",
      "Average test loss: 0.0019749839396940336\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014031323310401704\n",
      "Average test loss: 0.002043699678240551\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014049754052526422\n",
      "Average test loss: 0.001988814841541979\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014039321180019113\n",
      "Average test loss: 0.0020322940872154302\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014032495086391766\n",
      "Average test loss: 0.001954220989615553\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive-.025/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.16\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.78\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.48\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.97\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.24\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.34\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.883851372083028\n",
      "Average test loss: 0.012949718395868937\n",
      "Epoch 2/300\n",
      "Average training loss: 0.6788029842376709\n",
      "Average test loss: 0.012264204132888052\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4657020842499203\n",
      "Average test loss: 0.008794440970652634\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3670125378767649\n",
      "Average test loss: 0.008455127709855636\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3070465210808648\n",
      "Average test loss: 0.008775156863033772\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2672246738539802\n",
      "Average test loss: 0.007982154718703694\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2382504272196028\n",
      "Average test loss: 0.009370613079104159\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2141343701945411\n",
      "Average test loss: 0.007845525061918629\n",
      "Epoch 9/300\n",
      "Average training loss: 0.19977063177691565\n",
      "Average test loss: 0.007858407395995326\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18668859190411038\n",
      "Average test loss: 0.007233142123040226\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17426432745986514\n",
      "Average test loss: 0.0074540974373618766\n",
      "Epoch 12/300\n",
      "Average training loss: 0.16477865637673272\n",
      "Average test loss: 0.008014719198147455\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15794735248883565\n",
      "Average test loss: 0.00875952184283071\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1537068153884676\n",
      "Average test loss: 0.006758232055438889\n",
      "Epoch 15/300\n",
      "Average training loss: 0.14685936970180935\n",
      "Average test loss: 0.007460447792377737\n",
      "Epoch 16/300\n",
      "Average training loss: 0.14108303487300872\n",
      "Average test loss: 0.006939395536151197\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13778699011272855\n",
      "Average test loss: 0.007406088216851155\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13307746454079947\n",
      "Average test loss: 0.007662266877790292\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12884551839033762\n",
      "Average test loss: 0.0064242079862289955\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1256135955784056\n",
      "Average test loss: 0.006169002022180292\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12281006097131306\n",
      "Average test loss: 0.007149728734460142\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12030359488725663\n",
      "Average test loss: 0.006540937455164062\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11789884752697415\n",
      "Average test loss: 0.006059851711822881\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11555781404177348\n",
      "Average test loss: 0.006361415870901611\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1135587728354666\n",
      "Average test loss: 0.006006789354814424\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1118668610519833\n",
      "Average test loss: 0.005947313143561284\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11015362556113137\n",
      "Average test loss: 0.006231811825599935\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10868613297409481\n",
      "Average test loss: 0.006133585633503066\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10718863332933849\n",
      "Average test loss: 0.005867570701158709\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10604787834485371\n",
      "Average test loss: 0.006279819465966688\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10468012797170215\n",
      "Average test loss: 0.0058673031963407994\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10380709120962354\n",
      "Average test loss: 0.009074031229648324\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10230380164252387\n",
      "Average test loss: 0.005793209011356036\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10166895673010085\n",
      "Average test loss: 0.005845951108468903\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10062351220183903\n",
      "Average test loss: 0.005943404050750865\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10004153354300394\n",
      "Average test loss: 0.005704468192325698\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09916522379053963\n",
      "Average test loss: 0.005982551361123721\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09835449059142007\n",
      "Average test loss: 0.005752179673562448\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09777896755933761\n",
      "Average test loss: 0.00577275507317649\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09784876541958915\n",
      "Average test loss: 0.010002536750502056\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09641628777980804\n",
      "Average test loss: 0.005677433372371726\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09586137745777766\n",
      "Average test loss: 0.005659993076903952\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0952490332921346\n",
      "Average test loss: 0.005810102122525374\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09478616466787126\n",
      "Average test loss: 0.036300717092222635\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09398189908928341\n",
      "Average test loss: 0.005635479354610046\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09363935287793478\n",
      "Average test loss: 0.00560403594498833\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09314996623330646\n",
      "Average test loss: 0.00557962511272894\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09278531366586686\n",
      "Average test loss: 0.03419380785690414\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09228131728039847\n",
      "Average test loss: 0.005658050255642997\n",
      "Epoch 50/300\n",
      "Average training loss: 0.091747025138802\n",
      "Average test loss: 0.0059632771195222934\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09241796099477344\n",
      "Average test loss: 0.005634764357987377\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0910298145612081\n",
      "Average test loss: 0.0059129260261025695\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09066068034039604\n",
      "Average test loss: 0.0057989590594338045\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09027110724316703\n",
      "Average test loss: 0.005582426939159632\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09018485743469662\n",
      "Average test loss: 0.005660985457814402\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08979976544115278\n",
      "Average test loss: 0.05603645095974207\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08972633151213329\n",
      "Average test loss: 0.006846527257727252\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08984490562147564\n",
      "Average test loss: 0.005757136063857211\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08875595112641653\n",
      "Average test loss: 0.005575289813594685\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08835414948066075\n",
      "Average test loss: 0.006687854800787237\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08832306804921893\n",
      "Average test loss: 0.006073285521732436\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08796757318576177\n",
      "Average test loss: 0.005600920158955786\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0878955180313852\n",
      "Average test loss: 0.008969844327204757\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08744064464834002\n",
      "Average test loss: 0.0056708063251442375\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09092541684044732\n",
      "Average test loss: 0.005624374298999707\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08696162824498282\n",
      "Average test loss: 0.0060267427853412095\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08668164126078287\n",
      "Average test loss: 0.005715408308638467\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0866260499689314\n",
      "Average test loss: 0.006055967908352613\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08632243470350902\n",
      "Average test loss: 0.00571184456638164\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08601126633750068\n",
      "Average test loss: 0.0056186326413104936\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08601853679948383\n",
      "Average test loss: 0.005648709063314729\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08595400271813075\n",
      "Average test loss: 0.00567414772303568\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08576579025056627\n",
      "Average test loss: 0.005578973080962896\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0863396672407786\n",
      "Average test loss: 0.005910974430127276\n",
      "Epoch 75/300\n",
      "Average training loss: 0.25289808454116186\n",
      "Average test loss: 0.005909606355759833\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11625788881381353\n",
      "Average test loss: 0.005800548888742924\n",
      "Epoch 77/300\n",
      "Average training loss: 0.10817204068104426\n",
      "Average test loss: 0.011253936579657925\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10326435174544653\n",
      "Average test loss: 0.005656305532488558\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09969608400265376\n",
      "Average test loss: 0.00563907981912295\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09695445466703839\n",
      "Average test loss: 0.006058649540775352\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09442229520612293\n",
      "Average test loss: 0.005648060094151232\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09241573002603319\n",
      "Average test loss: 0.005718631047134598\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0910006263256073\n",
      "Average test loss: 0.006345503942006164\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08889616188075808\n",
      "Average test loss: 0.005798840756217638\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08783677662743462\n",
      "Average test loss: 0.00558845624824365\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08721353760030534\n",
      "Average test loss: 0.006069371041324404\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08655094510979122\n",
      "Average test loss: 0.006011405909640922\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08662302816576428\n",
      "Average test loss: 0.08852468481328753\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08667148250672552\n",
      "Average test loss: 0.005621894622428549\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08560328619347678\n",
      "Average test loss: 0.0057904878821637895\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08548655121193992\n",
      "Average test loss: 0.005561947234388855\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08508059313231045\n",
      "Average test loss: 0.005721335751314958\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08519946203298039\n",
      "Average test loss: 0.005751247927960422\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08485736289289263\n",
      "Average test loss: 0.00645512924508916\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08466157786051433\n",
      "Average test loss: 0.005709690573728747\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08437125262949202\n",
      "Average test loss: 0.006064061060133907\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08413850520716773\n",
      "Average test loss: 0.01869549078742663\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08410895579391056\n",
      "Average test loss: 0.005583456866029236\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08383673908313115\n",
      "Average test loss: 0.00562590582213468\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08354092367490133\n",
      "Average test loss: 0.005671595832953851\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08352752551105287\n",
      "Average test loss: 0.005683734880553352\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08340880164172915\n",
      "Average test loss: 0.005706844795495272\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08301065360837513\n",
      "Average test loss: 0.010999069593846798\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08319121422370275\n",
      "Average test loss: 0.006517111257546478\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08282349703709284\n",
      "Average test loss: 0.005647483893566662\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08272607211271922\n",
      "Average test loss: 0.005768487958030568\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0825871084663603\n",
      "Average test loss: 0.005689328584406111\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08242569807502959\n",
      "Average test loss: 0.005889304925998052\n",
      "Epoch 109/300\n",
      "Average training loss: 0.08530264968673389\n",
      "Average test loss: 0.006145229477849272\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08210412134726842\n",
      "Average test loss: 0.005743229907419946\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08174661562509007\n",
      "Average test loss: 0.006034452745483981\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08204078392187754\n",
      "Average test loss: 0.005824095215234491\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08137371500995425\n",
      "Average test loss: 0.005636207274678681\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08158453360862201\n",
      "Average test loss: 0.005717190585823523\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08150814445151223\n",
      "Average test loss: 0.005740732284469737\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08118037217855453\n",
      "Average test loss: 0.005793755335112413\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0810405470530192\n",
      "Average test loss: 0.00566720201406214\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08088079446554183\n",
      "Average test loss: 0.005696446248226695\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08082149900992712\n",
      "Average test loss: 0.006924385113848581\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08097680002450942\n",
      "Average test loss: 0.005926136920435561\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08062661792172326\n",
      "Average test loss: 0.006023580307347907\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08046789513693915\n",
      "Average test loss: 0.007157464911540349\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0806942678226365\n",
      "Average test loss: 0.0061241461564269335\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08014627539449268\n",
      "Average test loss: 0.0061665118208362\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08032120670212639\n",
      "Average test loss: 0.005928583272629314\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0799856635398335\n",
      "Average test loss: 0.005753367635524935\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07985289155112373\n",
      "Average test loss: 0.006117365854895777\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08015217829081747\n",
      "Average test loss: 0.005770789012105928\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07948352390858862\n",
      "Average test loss: 0.0060439083642429775\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07952201807498932\n",
      "Average test loss: 0.14200409836073716\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08109124414126079\n",
      "Average test loss: 0.006453254994418886\n",
      "Epoch 132/300\n",
      "Average training loss: 10.718370437377029\n",
      "Average test loss: 0.40135806859201856\n",
      "Epoch 133/300\n",
      "Average training loss: 2.4167950550715127\n",
      "Average test loss: 0.008679893093804519\n",
      "Epoch 134/300\n",
      "Average training loss: 1.643065657933553\n",
      "Average test loss: 0.0076735820579859945\n",
      "Epoch 135/300\n",
      "Average training loss: 1.22587140528361\n",
      "Average test loss: 0.008227330904867915\n",
      "Epoch 136/300\n",
      "Average training loss: 0.9517938129107157\n",
      "Average test loss: 0.007001805650691192\n",
      "Epoch 137/300\n",
      "Average training loss: 0.7207081231011284\n",
      "Average test loss: 0.007592333066380686\n",
      "Epoch 138/300\n",
      "Average training loss: 0.5519009860356648\n",
      "Average test loss: 0.006613647576007578\n",
      "Epoch 139/300\n",
      "Average training loss: 0.42423812855614557\n",
      "Average test loss: 0.006749160752114323\n",
      "Epoch 140/300\n",
      "Average training loss: 0.3317987931834327\n",
      "Average test loss: 0.008328119774659474\n",
      "Epoch 141/300\n",
      "Average training loss: 0.2740282281504737\n",
      "Average test loss: 0.013130018815398216\n",
      "Epoch 142/300\n",
      "Average training loss: 0.23751448120011223\n",
      "Average test loss: 0.009033268322547278\n",
      "Epoch 143/300\n",
      "Average training loss: 0.21105353844165803\n",
      "Average test loss: 0.0061609451534847415\n",
      "Epoch 144/300\n",
      "Average training loss: 0.1908057240512636\n",
      "Average test loss: 0.0122241402293245\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1744534032477273\n",
      "Average test loss: 0.0066402716504202946\n",
      "Epoch 146/300\n",
      "Average training loss: 0.1621410155561235\n",
      "Average test loss: 0.007775103715972768\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1511275657945209\n",
      "Average test loss: 0.0059515725183818076\n",
      "Epoch 148/300\n",
      "Average training loss: 0.14298683390352462\n",
      "Average test loss: 0.07995541366769209\n",
      "Epoch 149/300\n",
      "Average training loss: 0.1346183898581399\n",
      "Average test loss: 0.05034502273797989\n",
      "Epoch 150/300\n",
      "Average training loss: 0.12762152739365895\n",
      "Average test loss: 0.005895089130020804\n",
      "Epoch 151/300\n",
      "Average training loss: 0.12062756917211745\n",
      "Average test loss: 0.0056494246187309425\n",
      "Epoch 152/300\n",
      "Average training loss: 0.1158882151974572\n",
      "Average test loss: 0.005653435948408312\n",
      "Epoch 153/300\n",
      "Average training loss: 0.11188455335299174\n",
      "Average test loss: 0.006059954561293125\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10862063171466192\n",
      "Average test loss: 0.005726911517481009\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10597723811864852\n",
      "Average test loss: 0.005598484752078851\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10371668614943823\n",
      "Average test loss: 0.005904526898637414\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10123963819609748\n",
      "Average test loss: 0.005566391985863447\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09932343248526256\n",
      "Average test loss: 0.005656385157257318\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09747968491580751\n",
      "Average test loss: 0.005557394905222787\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0956968075633049\n",
      "Average test loss: 0.0061638854721354115\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09394274999698003\n",
      "Average test loss: 0.005551018704970678\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0923798898326026\n",
      "Average test loss: 0.01232894287010034\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09096725532081391\n",
      "Average test loss: 0.005581582516845729\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08897936718993717\n",
      "Average test loss: 0.006066350251436234\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08745206339491739\n",
      "Average test loss: 0.005816902488470077\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08577600560585658\n",
      "Average test loss: 0.005605485428124666\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08411972392267651\n",
      "Average test loss: 0.012386346800873678\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08285430657201343\n",
      "Average test loss: 0.005955697161042028\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08155806537469228\n",
      "Average test loss: 0.0063585307925111715\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08074993966685401\n",
      "Average test loss: 0.0058556286307672656\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08046561933226055\n",
      "Average test loss: 0.006826693052632941\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08005732221735848\n",
      "Average test loss: 0.0056966775519152486\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07958496148718729\n",
      "Average test loss: 0.007016306931980782\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07970628401968215\n",
      "Average test loss: 0.005732014985134204\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07972093840440114\n",
      "Average test loss: 0.037663081182373895\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07921354441510306\n",
      "Average test loss: 0.005673266159163581\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07907355368799633\n",
      "Average test loss: 0.005712256232897441\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07912445595529344\n",
      "Average test loss: 0.024093535211351182\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07889276427692837\n",
      "Average test loss: 0.005755953509360551\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0790925011502372\n",
      "Average test loss: 0.005909647859219048\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08231677145428128\n",
      "Average test loss: 0.0059702214300632475\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08092354850636588\n",
      "Average test loss: 0.0056984119916127784\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07835247465636995\n",
      "Average test loss: 0.005736607500248485\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07821226823661062\n",
      "Average test loss: 0.005856574723290073\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07808542642990748\n",
      "Average test loss: 0.006076938497109546\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07828326668341955\n",
      "Average test loss: 0.005722400861481826\n",
      "Epoch 187/300\n",
      "Average training loss: 0.078041870686743\n",
      "Average test loss: 0.09986575369371309\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07781239930788676\n",
      "Average test loss: 0.005812460738337702\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07785594862037234\n",
      "Average test loss: 0.005829993839893076\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07797896116309695\n",
      "Average test loss: 0.0058209410951369335\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07789241244064438\n",
      "Average test loss: 0.005795891852428516\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07748345378372404\n",
      "Average test loss: 0.0061184480653868785\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07794289676348368\n",
      "Average test loss: 0.00638571685180068\n",
      "Epoch 194/300\n",
      "Average training loss: 2.3114214635358916\n",
      "Average test loss: 2.4700983674857353\n",
      "Epoch 195/300\n",
      "Average training loss: 1.3740792286131116\n",
      "Average test loss: 0.007269620634615421\n",
      "Epoch 196/300\n",
      "Average training loss: 0.9058713054127163\n",
      "Average test loss: 0.006718924943771627\n",
      "Epoch 197/300\n",
      "Average training loss: 0.6860076874097188\n",
      "Average test loss: 0.006798394033478366\n",
      "Epoch 198/300\n",
      "Average training loss: 0.5299709971480899\n",
      "Average test loss: 0.006371165370775594\n",
      "Epoch 199/300\n",
      "Average training loss: 0.4120970849461026\n",
      "Average test loss: 0.0064685634353922475\n",
      "Epoch 200/300\n",
      "Average training loss: 0.3234960504108005\n",
      "Average test loss: 0.006117547103100353\n",
      "Epoch 201/300\n",
      "Average training loss: 0.2589614780743917\n",
      "Average test loss: 0.006410271552701791\n",
      "Epoch 202/300\n",
      "Average training loss: 0.2111366751856274\n",
      "Average test loss: 0.005925550476958355\n",
      "Epoch 203/300\n",
      "Average training loss: 0.176212575117747\n",
      "Average test loss: 0.005816261562622256\n",
      "Epoch 204/300\n",
      "Average training loss: 0.15144515403111775\n",
      "Average test loss: 0.005933492199828227\n",
      "Epoch 205/300\n",
      "Average training loss: 0.13525357943110997\n",
      "Average test loss: 0.010636795160671075\n",
      "Epoch 206/300\n",
      "Average training loss: 0.12514211723539564\n",
      "Average test loss: 0.005760255695631107\n",
      "Epoch 207/300\n",
      "Average training loss: 0.11798663612206776\n",
      "Average test loss: 0.005745549574908283\n",
      "Epoch 208/300\n",
      "Average training loss: 0.11306235077646043\n",
      "Average test loss: 0.005731070047865311\n",
      "Epoch 209/300\n",
      "Average training loss: 0.10950100006659826\n",
      "Average test loss: 0.005624119654297829\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10644132379690806\n",
      "Average test loss: 0.005671468767440981\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10398574954271317\n",
      "Average test loss: 0.005614852916035387\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10169313680463367\n",
      "Average test loss: 0.005650416058798631\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09970135798719194\n",
      "Average test loss: 0.005579112603432602\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09763122816880544\n",
      "Average test loss: 0.005542201747496923\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09594274942742453\n",
      "Average test loss: 0.005759234656476312\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09442860398027632\n",
      "Average test loss: 0.005587309767388635\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09284435356987847\n",
      "Average test loss: 0.005608427425225576\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09145743860138787\n",
      "Average test loss: 0.009457547944453028\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09016352438264423\n",
      "Average test loss: 0.005676663873096307\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08878126135137346\n",
      "Average test loss: 0.005629043348133564\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08744322129090627\n",
      "Average test loss: 0.005684493631952339\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08603507439957725\n",
      "Average test loss: 0.005774343952536583\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08488289422459072\n",
      "Average test loss: 0.005699910874168078\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08387710064649583\n",
      "Average test loss: 0.00570302865240309\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08253508200910356\n",
      "Average test loss: 0.005723685415254699\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08181728703445859\n",
      "Average test loss: 0.0059342016408012975\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08102952295541763\n",
      "Average test loss: 0.011902814235952165\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08013054207298491\n",
      "Average test loss: 0.005774050214224392\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07969146568245358\n",
      "Average test loss: 0.006678826329194837\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07957155841588974\n",
      "Average test loss: 0.005716739190949334\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07895342524184121\n",
      "Average test loss: 0.005902275722887781\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07860352818171183\n",
      "Average test loss: 0.005846258610486984\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07859098482794231\n",
      "Average test loss: 0.006456275879508919\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07801128142409855\n",
      "Average test loss: 0.005861575467718972\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07799304385317697\n",
      "Average test loss: 0.00580309871956706\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0780528081589275\n",
      "Average test loss: 0.005882831871509552\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07758343607849545\n",
      "Average test loss: 0.005900921512395144\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08559899516569243\n",
      "Average test loss: 0.005779934411247571\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07788712870412402\n",
      "Average test loss: 0.005826433730208211\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07716684736145868\n",
      "Average test loss: 0.005806196226014032\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07723353932301204\n",
      "Average test loss: 0.005812812034868531\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07730091967185339\n",
      "Average test loss: 0.00581383574836784\n",
      "Epoch 243/300\n",
      "Average training loss: 0.3434451863699489\n",
      "Average test loss: 0.006291240433024036\n",
      "Epoch 244/300\n",
      "Average training loss: 0.136189123166932\n",
      "Average test loss: 0.006012023877766397\n",
      "Epoch 245/300\n",
      "Average training loss: 0.11606173131201003\n",
      "Average test loss: 0.0058859718259837894\n",
      "Epoch 246/300\n",
      "Average training loss: 0.1073181379834811\n",
      "Average test loss: 0.005634002381728755\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10248742939366234\n",
      "Average test loss: 0.005756843909621239\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09886807951662276\n",
      "Average test loss: 0.005605105181121164\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09573423483636644\n",
      "Average test loss: 0.005848256771763166\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09279279557863872\n",
      "Average test loss: 0.005614662367850542\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09003872303167978\n",
      "Average test loss: 0.005642946036325561\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08729674210813311\n",
      "Average test loss: 0.005969540956119696\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08454043279091517\n",
      "Average test loss: 0.005764945179637935\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08230399746365018\n",
      "Average test loss: 0.0058907958029045\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08017046692636277\n",
      "Average test loss: 0.005726234925289949\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07898066698180305\n",
      "Average test loss: 0.005805749951137437\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07817218379179637\n",
      "Average test loss: 0.006348899715476566\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07764398005273607\n",
      "Average test loss: 0.0063060938579340775\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07743731007973353\n",
      "Average test loss: 0.0058214267732368576\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07715283383925756\n",
      "Average test loss: 0.005818711089177264\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07693348756101397\n",
      "Average test loss: 0.005829549307210578\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07717187871204483\n",
      "Average test loss: 0.005956805624481704\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07668109066618814\n",
      "Average test loss: 0.005829686221563154\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07647511843840281\n",
      "Average test loss: 0.005929517425596714\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07666828874084684\n",
      "Average test loss: 0.005930688004940748\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07634311105145349\n",
      "Average test loss: 0.005825707817243205\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0784466602868504\n",
      "Average test loss: 0.005856735468945569\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0760187061627706\n",
      "Average test loss: 0.006215821937554412\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07613010222382016\n",
      "Average test loss: 0.005868035129374928\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07587266268995073\n",
      "Average test loss: 0.0059956266697910095\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0763712341454294\n",
      "Average test loss: 0.005952303604533275\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07578558846314748\n",
      "Average test loss: 0.005843506284885936\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07555932290024228\n",
      "Average test loss: 0.005989162225276232\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07582550907797284\n",
      "Average test loss: 0.0058300148737099435\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07565518612331815\n",
      "Average test loss: 0.0061043323820663825\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07573129554920727\n",
      "Average test loss: 0.006016709536314011\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07540599686569638\n",
      "Average test loss: 0.00593979577637381\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07557992058992385\n",
      "Average test loss: 0.005969691996359163\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07527807497315937\n",
      "Average test loss: 0.006018199680786994\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07512083569500182\n",
      "Average test loss: 0.006023625757131312\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07499340410364999\n",
      "Average test loss: 0.0060545570668247015\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0752482643061214\n",
      "Average test loss: 0.005880593790362279\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07498822161224153\n",
      "Average test loss: 0.005871841530832979\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0751298012567891\n",
      "Average test loss: 0.006186408420403799\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07485352957579824\n",
      "Average test loss: 0.0059166883387499385\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07472309507264031\n",
      "Average test loss: 0.005933859607411755\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07461006902323829\n",
      "Average test loss: 0.005923435160683261\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07501347848773003\n",
      "Average test loss: 0.006026192062844833\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07448361935880449\n",
      "Average test loss: 0.005936513139141931\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07467630751927694\n",
      "Average test loss: 0.005955420312782129\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07444143270783954\n",
      "Average test loss: 0.00594411139893863\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07433343856533368\n",
      "Average test loss: 0.0069958996342288126\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07427579461203682\n",
      "Average test loss: 0.007535479937990507\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07424558730257882\n",
      "Average test loss: 0.0058539859019219875\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07426739300953017\n",
      "Average test loss: 0.006032273694458935\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07450085328022639\n",
      "Average test loss: 0.006016412977543142\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07394698359900051\n",
      "Average test loss: 0.006400421977457073\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07385640282101101\n",
      "Average test loss: 0.005982105540732543\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07397582039899296\n",
      "Average test loss: 0.008291324680050213\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07392379400134087\n",
      "Average test loss: 0.005994604824317826\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6639626122050815\n",
      "Average test loss: 0.00820956890616152\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5789970798757341\n",
      "Average test loss: 0.006699321105248398\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3828968214723799\n",
      "Average test loss: 0.006264929522242811\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2912921136485206\n",
      "Average test loss: 0.005592176136043336\n",
      "Epoch 5/300\n",
      "Average training loss: 0.23732022409968906\n",
      "Average test loss: 0.005645255968388584\n",
      "Epoch 6/300\n",
      "Average training loss: 0.203060909403695\n",
      "Average test loss: 0.005393992249336507\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1790348346763187\n",
      "Average test loss: 0.005136700933178266\n",
      "Epoch 8/300\n",
      "Average training loss: 0.16109767701228458\n",
      "Average test loss: 0.004827870116879543\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14699230784840053\n",
      "Average test loss: 0.004825797246976031\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1360876085890664\n",
      "Average test loss: 0.005064997550927931\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1268418681356642\n",
      "Average test loss: 0.0046166835774978\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11957744177182515\n",
      "Average test loss: 0.005682084997081094\n",
      "Epoch 13/300\n",
      "Average training loss: 0.11328637305233213\n",
      "Average test loss: 0.004370517765482267\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10849346222480138\n",
      "Average test loss: 0.004374663579795096\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10376643623246087\n",
      "Average test loss: 0.004580252120892207\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10045896261268192\n",
      "Average test loss: 0.005101310068534481\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09660619008541108\n",
      "Average test loss: 0.004276927350709836\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09417836957507664\n",
      "Average test loss: 0.003842100593364901\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09076319620344374\n",
      "Average test loss: 0.0037950509794884256\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08842168439759149\n",
      "Average test loss: 0.004117433598058091\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08529162980450525\n",
      "Average test loss: 0.0037935537348190943\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08270127381218804\n",
      "Average test loss: 0.004034729738616281\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08065716183847851\n",
      "Average test loss: 0.003613279081467125\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07858740844991471\n",
      "Average test loss: 0.0036218988936808374\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07657337332765261\n",
      "Average test loss: 0.0036800885221196546\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07449932070242034\n",
      "Average test loss: 0.0036374590111275516\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07321496969461441\n",
      "Average test loss: 0.0035020197799636257\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07147761846250958\n",
      "Average test loss: 0.0036042120742301145\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07004928913381364\n",
      "Average test loss: 0.0034703640122380522\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0687690488629871\n",
      "Average test loss: 0.003487308143741555\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06750552021132575\n",
      "Average test loss: 0.0034866070192721157\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06705239486032062\n",
      "Average test loss: 0.003912735970897807\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06617544824547238\n",
      "Average test loss: 0.003541783423266477\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0652719880607393\n",
      "Average test loss: 0.0034992391322222023\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06440795293781493\n",
      "Average test loss: 0.0036386515899664825\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06400841200351715\n",
      "Average test loss: 0.003871908512380388\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06348197036981583\n",
      "Average test loss: 0.003410385528786315\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06270526446236505\n",
      "Average test loss: 0.0034610285153612494\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0625734381907516\n",
      "Average test loss: 0.003939057429631551\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06159450521071752\n",
      "Average test loss: 0.003360918741880192\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0615506774617566\n",
      "Average test loss: 0.0033561403358148205\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06089613654878404\n",
      "Average test loss: 0.0036138787782854505\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06064707058668137\n",
      "Average test loss: 0.003366955580810706\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06027215980158912\n",
      "Average test loss: 0.0036312378872599867\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05999994847509596\n",
      "Average test loss: 0.0034809073760277694\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05946648618247774\n",
      "Average test loss: 0.003387765805754397\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05928713490565618\n",
      "Average test loss: 0.0033641780459632478\n",
      "Epoch 48/300\n",
      "Average training loss: 0.060011150598526\n",
      "Average test loss: 0.018924808658659458\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06556390600734287\n",
      "Average test loss: 0.0034572510911772647\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0599162550634808\n",
      "Average test loss: 0.005316231121205621\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05917931705050998\n",
      "Average test loss: 0.0034248314110769166\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05885627294911279\n",
      "Average test loss: 0.0033253734407739507\n",
      "Epoch 53/300\n",
      "Average training loss: 0.058795232319169576\n",
      "Average test loss: 0.0032780189501742524\n",
      "Epoch 54/300\n",
      "Average training loss: 0.058218568454186125\n",
      "Average test loss: 0.003307902952449189\n",
      "Epoch 55/300\n",
      "Average training loss: 0.058008813894457285\n",
      "Average test loss: 0.00331401513537599\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05779690431223975\n",
      "Average test loss: 0.0033436081206632984\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05773284817404217\n",
      "Average test loss: 0.003447820122457213\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05771796181135708\n",
      "Average test loss: 0.04278584105107519\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08372566519843208\n",
      "Average test loss: 0.0039845334535671606\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07108740799294577\n",
      "Average test loss: 0.0034414991144504814\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06436114538378186\n",
      "Average test loss: 0.003342546066476239\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06218812315993839\n",
      "Average test loss: 0.0033338273093104363\n",
      "Epoch 63/300\n",
      "Average training loss: 0.060810319205125175\n",
      "Average test loss: 0.003342703423152367\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05979103237390518\n",
      "Average test loss: 0.0033366106214622656\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05913825466897753\n",
      "Average test loss: 0.003384132098406553\n",
      "Epoch 66/300\n",
      "Average training loss: 0.058466055505805546\n",
      "Average test loss: 0.003393910582280821\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05804414003425174\n",
      "Average test loss: 0.0033119808245036337\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05761562293436792\n",
      "Average test loss: 0.008920039628942807\n",
      "Epoch 69/300\n",
      "Average training loss: 0.057550440377659266\n",
      "Average test loss: 0.0033691418812506727\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05734919541080793\n",
      "Average test loss: 0.003422742911097076\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05703749539123641\n",
      "Average test loss: 0.003340253971103165\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05684140431880951\n",
      "Average test loss: 0.003287341018103891\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05663084236615234\n",
      "Average test loss: 0.0051260035820305345\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05659645456075668\n",
      "Average test loss: 0.003322227894432015\n",
      "Epoch 75/300\n",
      "Average training loss: 0.057202518145243325\n",
      "Average test loss: 0.004189551013625331\n",
      "Epoch 76/300\n",
      "Average training loss: 0.056376909658312796\n",
      "Average test loss: 0.003385139403036899\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05603556056155099\n",
      "Average test loss: 0.003285071346287926\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0566762678888109\n",
      "Average test loss: 0.003428538242975871\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05574600983990563\n",
      "Average test loss: 0.0032901498441480926\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05567461517122057\n",
      "Average test loss: 0.003819215765843789\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05576177799039417\n",
      "Average test loss: 0.0033801268751008644\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05534241720371776\n",
      "Average test loss: 0.003438864709602462\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05526206156942579\n",
      "Average test loss: 0.0035432546018726294\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05520290893316269\n",
      "Average test loss: 0.003298074473316471\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05557830591334237\n",
      "Average test loss: 0.003450046585044927\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05683372961481412\n",
      "Average test loss: 0.0033185701316429507\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05499261247283883\n",
      "Average test loss: 0.003479999742160241\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0545973967578676\n",
      "Average test loss: 0.003329146351872219\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05445561462971899\n",
      "Average test loss: 0.04530427915520138\n",
      "Epoch 90/300\n",
      "Average training loss: 0.054495662987232206\n",
      "Average test loss: 0.0033130633872416283\n",
      "Epoch 91/300\n",
      "Average training loss: 0.054325045618746015\n",
      "Average test loss: 0.004500160230116712\n",
      "Epoch 92/300\n",
      "Average training loss: 0.054280551489856506\n",
      "Average test loss: 0.0034606767205728425\n",
      "Epoch 93/300\n",
      "Average training loss: 0.054174256609545814\n",
      "Average test loss: 0.003293371827238136\n",
      "Epoch 94/300\n",
      "Average training loss: 0.054075195809205376\n",
      "Average test loss: 0.003405594561249018\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05386663799815708\n",
      "Average test loss: 0.003341391377357973\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05451714785562621\n",
      "Average test loss: 0.0242572825551033\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05520400134060118\n",
      "Average test loss: 0.0033642897841831046\n",
      "Epoch 98/300\n",
      "Average training loss: 0.053805742336644063\n",
      "Average test loss: 0.0034145077942974037\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05347295687596003\n",
      "Average test loss: 0.0034145711086069545\n",
      "Epoch 100/300\n",
      "Average training loss: 0.053616842028167515\n",
      "Average test loss: 0.0033382371117671333\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05339987151821454\n",
      "Average test loss: 0.0037180682544906934\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05333078267176946\n",
      "Average test loss: 0.0048912938903603286\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05319843751854367\n",
      "Average test loss: 0.003321254841776358\n",
      "Epoch 104/300\n",
      "Average training loss: 0.053104260381725096\n",
      "Average test loss: 0.0034636485404852366\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05309091869658894\n",
      "Average test loss: 0.0033146611683898502\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05348464563157823\n",
      "Average test loss: 0.0033495305495129693\n",
      "Epoch 107/300\n",
      "Average training loss: 0.052707611040936575\n",
      "Average test loss: 0.0033332623766942158\n",
      "Epoch 108/300\n",
      "Average training loss: 0.052709111806419164\n",
      "Average test loss: 0.012854949769046572\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05288062838382191\n",
      "Average test loss: 0.003322800959563918\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05309134253528383\n",
      "Average test loss: 0.00340590138009025\n",
      "Epoch 111/300\n",
      "Average training loss: 0.058456287768152024\n",
      "Average test loss: 0.003483451073989272\n",
      "Epoch 112/300\n",
      "Average training loss: 0.053193433791399\n",
      "Average test loss: 0.0033016556658678584\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05260626603166262\n",
      "Average test loss: 0.003459675947825114\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05251015832689073\n",
      "Average test loss: 0.003462917401144902\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05239131609267659\n",
      "Average test loss: 0.003323635344290071\n",
      "Epoch 116/300\n",
      "Average training loss: 0.052360496583912106\n",
      "Average test loss: 0.003381562486704853\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05219923847251468\n",
      "Average test loss: 0.003350110550928447\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05226675801475843\n",
      "Average test loss: 0.0035318158214083977\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05260689599646462\n",
      "Average test loss: 0.0033169883379919663\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07844752672976918\n",
      "Average test loss: 0.0035826474705504046\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06223592097891702\n",
      "Average test loss: 0.0033760200542294317\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05762479732433955\n",
      "Average test loss: 0.0033628757134493855\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05503502638803588\n",
      "Average test loss: 0.0033247002874397568\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05373005306720734\n",
      "Average test loss: 0.00335025140705208\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05282991422547234\n",
      "Average test loss: 0.00374233547763692\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05246242180797789\n",
      "Average test loss: 0.0033897686128815017\n",
      "Epoch 127/300\n",
      "Average training loss: 0.052018193003204136\n",
      "Average test loss: 0.003394728811664714\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05181946814722485\n",
      "Average test loss: 0.003555196251306269\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05183837386634615\n",
      "Average test loss: 0.003637301106833749\n",
      "Epoch 130/300\n",
      "Average training loss: 0.051740453657176756\n",
      "Average test loss: 0.0033913180546628102\n",
      "Epoch 131/300\n",
      "Average training loss: 0.051601701236433456\n",
      "Average test loss: 0.0033597064810908504\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05162084266543388\n",
      "Average test loss: 0.24464567825860448\n",
      "Epoch 133/300\n",
      "Average training loss: 0.051680403504106735\n",
      "Average test loss: 0.0038692599957187972\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05179193892081579\n",
      "Average test loss: 0.0034711706545203923\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05192007782724169\n",
      "Average test loss: 0.0035896468696494897\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05159603456987275\n",
      "Average test loss: 0.0038096012642814055\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05142915580670039\n",
      "Average test loss: 0.0034070288634134665\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05122476038999028\n",
      "Average test loss: 0.003520187230159839\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05138646611240175\n",
      "Average test loss: 0.00965646033692691\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05126569736335013\n",
      "Average test loss: 0.0037217904755638704\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05136382845375273\n",
      "Average test loss: 0.003443249934663375\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05110966934098138\n",
      "Average test loss: 0.003349373150823845\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05121937598784765\n",
      "Average test loss: 0.0034068351965397597\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0509369848701689\n",
      "Average test loss: 0.004966701519985994\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05090488790803485\n",
      "Average test loss: 0.0034593667323804564\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05070278550187746\n",
      "Average test loss: 0.0033588557774201036\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05084006118774414\n",
      "Average test loss: 0.0033794441755033203\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05134112222492695\n",
      "Average test loss: 0.003804987549367878\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05073329597049289\n",
      "Average test loss: 0.006371553694208463\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0507463183634811\n",
      "Average test loss: 0.004484080147412088\n",
      "Epoch 151/300\n",
      "Average training loss: 0.050663617852661345\n",
      "Average test loss: 0.003425136243303617\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05042322247889307\n",
      "Average test loss: 0.0035658844910148118\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05039247582356135\n",
      "Average test loss: 0.0035548386679341397\n",
      "Epoch 154/300\n",
      "Average training loss: 0.050260380850897894\n",
      "Average test loss: 0.0033919242959883477\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05051366617613368\n",
      "Average test loss: 0.16343010570605596\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05059452532728513\n",
      "Average test loss: 0.0035421091475420527\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050496891150871914\n",
      "Average test loss: 0.003928449717660745\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0502072675426801\n",
      "Average test loss: 0.003511235325079825\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05000180516640345\n",
      "Average test loss: 0.0034664101805537938\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05008018344309595\n",
      "Average test loss: 0.00338895282190707\n",
      "Epoch 161/300\n",
      "Average training loss: 0.050066524217526116\n",
      "Average test loss: 0.0034221134090589153\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04997754902640979\n",
      "Average test loss: 0.003412598405447271\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05007003127866321\n",
      "Average test loss: 0.003484562092357212\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04981706648733881\n",
      "Average test loss: 0.0034613187466230656\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0498338200615512\n",
      "Average test loss: 0.0034366505957312055\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04977849586804708\n",
      "Average test loss: 0.003689867478588389\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04969772144489818\n",
      "Average test loss: 0.004387779239151213\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04971786673532592\n",
      "Average test loss: 0.0036291966332743565\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04978732923997773\n",
      "Average test loss: 0.003443654459383753\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04966754843791326\n",
      "Average test loss: 0.0034580193182660473\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04979972924126519\n",
      "Average test loss: 0.0035425332950221166\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04950515819920434\n",
      "Average test loss: 0.13366840498646101\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04975125709838337\n",
      "Average test loss: 0.003550661928123898\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04942698379026519\n",
      "Average test loss: 0.0034589841585192416\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04965468130177922\n",
      "Average test loss: 0.004001704232767224\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049624184711111914\n",
      "Average test loss: 0.0033796595933122766\n",
      "Epoch 177/300\n",
      "Average training loss: 0.049478926367229885\n",
      "Average test loss: 0.0034379878350430065\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05032125576668316\n",
      "Average test loss: 0.0035982189526160557\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04950208036435975\n",
      "Average test loss: 0.00343480598140094\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04910576649506887\n",
      "Average test loss: 0.0034749144727571142\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04908415226141612\n",
      "Average test loss: 0.0035115092914137576\n",
      "Epoch 182/300\n",
      "Average training loss: 0.049146001083983315\n",
      "Average test loss: 0.004279173396734727\n",
      "Epoch 183/300\n",
      "Average training loss: 0.049130167418056064\n",
      "Average test loss: 0.003578857631319099\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0492518269320329\n",
      "Average test loss: 0.003599945862053169\n",
      "Epoch 185/300\n",
      "Average training loss: 0.049103586932023366\n",
      "Average test loss: 0.0036473487723204827\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04916219943761826\n",
      "Average test loss: 0.00602559934101171\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04955969982014762\n",
      "Average test loss: 0.0034976735637626713\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04898324652844005\n",
      "Average test loss: 0.003455636941310432\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04877248956097497\n",
      "Average test loss: 0.003476335723573963\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04879649311635229\n",
      "Average test loss: 0.0035015801141659417\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04881978504690859\n",
      "Average test loss: 0.0035740977387047476\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04907367378142145\n",
      "Average test loss: 0.0035638958137068485\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04916873604721493\n",
      "Average test loss: 0.0035705964720497527\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04900487503740523\n",
      "Average test loss: 0.0035216790673633417\n",
      "Epoch 195/300\n",
      "Average training loss: 0.048570107483201556\n",
      "Average test loss: 0.003440125234425068\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0488057359026538\n",
      "Average test loss: 0.0035286947819921704\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0488931953476535\n",
      "Average test loss: 0.003506227729635106\n",
      "Epoch 198/300\n",
      "Average training loss: 0.048593451718489326\n",
      "Average test loss: 0.0035127825167857937\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04853778804341952\n",
      "Average test loss: 0.0035354126557293865\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04866392837961515\n",
      "Average test loss: 0.003525992766643564\n",
      "Epoch 201/300\n",
      "Average training loss: 0.048686826921171614\n",
      "Average test loss: 10.665309026930068\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04851365795400408\n",
      "Average test loss: 0.0034385483691261876\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04854160847266515\n",
      "Average test loss: 0.003507217219306363\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04850080418586731\n",
      "Average test loss: 0.0035435126173413464\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04842272227671411\n",
      "Average test loss: 0.0034916149535112913\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04872785010271602\n",
      "Average test loss: 0.00349919988711675\n",
      "Epoch 207/300\n",
      "Average training loss: 0.048382034944163425\n",
      "Average test loss: 0.003510474335609211\n",
      "Epoch 208/300\n",
      "Average training loss: 0.048218383928140006\n",
      "Average test loss: 0.003564466856833961\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04828600623541408\n",
      "Average test loss: 0.04580427038338449\n",
      "Epoch 210/300\n",
      "Average training loss: 0.048219411886400644\n",
      "Average test loss: 0.0035473220344218945\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04848218082719379\n",
      "Average test loss: 0.0035341185554862023\n",
      "Epoch 212/300\n",
      "Average training loss: 0.048108291337887445\n",
      "Average test loss: 0.003504348824421565\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04817580874098672\n",
      "Average test loss: 0.0035648038116180233\n",
      "Epoch 214/300\n",
      "Average training loss: 0.048241002695428\n",
      "Average test loss: 0.004083222365627686\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04805732074379921\n",
      "Average test loss: 0.0034977806078063116\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04819769596391254\n",
      "Average test loss: 0.00428652298098637\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04823854383826256\n",
      "Average test loss: 0.004029819941769043\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04797410297062662\n",
      "Average test loss: 0.003640275813225243\n",
      "Epoch 219/300\n",
      "Average training loss: 0.048353379080692926\n",
      "Average test loss: 0.003595038207454814\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04785746310485734\n",
      "Average test loss: 0.003922649271786213\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04799043976267179\n",
      "Average test loss: 0.004033152671530843\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04794029263324208\n",
      "Average test loss: 0.0035544827301055193\n",
      "Epoch 223/300\n",
      "Average training loss: 0.048076985170443856\n",
      "Average test loss: 0.003602437431199683\n",
      "Epoch 224/300\n",
      "Average training loss: 0.047823823859294255\n",
      "Average test loss: 0.0035034784271071357\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04787045978837543\n",
      "Average test loss: 0.00357500029520856\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04793403908279207\n",
      "Average test loss: 0.003733992475188441\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04801395747727818\n",
      "Average test loss: 0.0037048018601619536\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04810935189657741\n",
      "Average test loss: 0.003673308502054877\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04781908916764789\n",
      "Average test loss: 0.003683771238765783\n",
      "Epoch 232/300\n",
      "Average training loss: 0.048200184431340956\n",
      "Average test loss: 0.005024658588071664\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04763935274548001\n",
      "Average test loss: 0.0034642156842682095\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0477128295633528\n",
      "Average test loss: 0.00350959642810954\n",
      "Epoch 237/300\n",
      "Average training loss: 0.047535491628779306\n",
      "Average test loss: 0.0035456385616627006\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04794339418411255\n",
      "Average test loss: 0.020957901673184502\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04761156501703792\n",
      "Average test loss: 0.0035381650645285845\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0474771635333697\n",
      "Average test loss: 0.0035119334643499717\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04745695702897178\n",
      "Average test loss: 0.003611731556140714\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04769496641225285\n",
      "Average test loss: 0.0036632894252737365\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04752755385306146\n",
      "Average test loss: 0.004038633262117704\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0474673912955655\n",
      "Average test loss: 0.5574005304442512\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04755444886287053\n",
      "Average test loss: 0.003553504387123717\n",
      "Epoch 246/300\n",
      "Average training loss: 0.047300378915336394\n",
      "Average test loss: 0.00424193635645012\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04763486362497012\n",
      "Average test loss: 0.003570544097572565\n",
      "Epoch 248/300\n",
      "Average training loss: 0.047394085102611117\n",
      "Average test loss: 0.0042190932321051755\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04754179698228836\n",
      "Average test loss: 0.003481696630724602\n",
      "Epoch 250/300\n",
      "Average training loss: 0.047239922536744014\n",
      "Average test loss: 0.004100792968852652\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04724256802598635\n",
      "Average test loss: 0.00381526091032558\n",
      "Epoch 252/300\n",
      "Average training loss: 0.047434867534372545\n",
      "Average test loss: 0.0036168370093736385\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04740791371795866\n",
      "Average test loss: 0.003920196384191513\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04723146431975894\n",
      "Average test loss: 0.003563292456169923\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04727865827745861\n",
      "Average test loss: 0.0035301507392691243\n",
      "Epoch 256/300\n",
      "Average training loss: 0.047572271628512276\n",
      "Average test loss: 0.003694581965605418\n",
      "Epoch 257/300\n",
      "Average training loss: 0.047244149463044276\n",
      "Average test loss: 0.0037858623017867406\n",
      "Epoch 258/300\n",
      "Average training loss: 0.047136789735820556\n",
      "Average test loss: 0.00359432057539622\n",
      "Epoch 261/300\n",
      "Average training loss: 0.046975396507316164\n",
      "Average test loss: 0.0035522140657736193\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0467987550397714\n",
      "Average test loss: 0.003622086258398162\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04692427537507481\n",
      "Average test loss: 0.0036197782254053486\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04711826597981983\n",
      "Average test loss: 0.0036044566660291617\n",
      "Epoch 265/300\n",
      "Average training loss: 0.047069367786248525\n",
      "Average test loss: 0.0036798737169139915\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04706728410720825\n",
      "Average test loss: 0.05963555696606636\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04710238046447436\n",
      "Average test loss: 0.003635451829060912\n",
      "Epoch 268/300\n",
      "Average training loss: 0.047372733169131806\n",
      "Average test loss: 0.003706502790459328\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04702030637198024\n",
      "Average test loss: 0.0035799572279469833\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04698753277460734\n",
      "Average test loss: 0.0044146386101428\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04710034744607078\n",
      "Average test loss: 0.0035501226495123573\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04695852641926872\n",
      "Average test loss: 0.0034953009382718138\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04707200383808878\n",
      "Average test loss: 0.12946766120195388\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04699527751406034\n",
      "Average test loss: 0.0036894494944976434\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04692821462949117\n",
      "Average test loss: 0.003771196595910523\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04678670167591837\n",
      "Average test loss: 0.003561302100204759\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04684456735849381\n",
      "Average test loss: 0.006432196160985364\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04683457197745641\n",
      "Average test loss: 0.0036812047231942415\n",
      "Epoch 279/300\n",
      "Average training loss: 0.046848490175273684\n",
      "Average test loss: 0.0035641750300096143\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04701640842854977\n",
      "Average test loss: 0.0036592503775739007\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04705057228604952\n",
      "Average test loss: 0.0036011492512706255\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04677769154641363\n",
      "Average test loss: 0.003588802702931894\n",
      "Epoch 283/300\n",
      "Average training loss: 0.046791955573691264\n",
      "Average test loss: 0.003710497407656577\n",
      "Epoch 284/300\n",
      "Average training loss: 0.046959931039147904\n",
      "Average test loss: 0.0036437812749710347\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04670542271600829\n",
      "Average test loss: 0.0038092114608734846\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04671549493736691\n",
      "Average test loss: 0.0036572786344008313\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04680145274268256\n",
      "Average test loss: 0.0037290663070355854\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04686508969465891\n",
      "Average test loss: 0.0035730661768466235\n",
      "Epoch 289/300\n",
      "Average training loss: 0.047141144143210516\n",
      "Average test loss: 0.0035853099999949337\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04675381969412168\n",
      "Average test loss: 0.0035331116639491585\n",
      "Epoch 291/300\n",
      "Average training loss: 0.046586737126111985\n",
      "Average test loss: 0.0036253910888400342\n",
      "Epoch 292/300\n",
      "Average training loss: 0.046698811272780104\n",
      "Average test loss: 0.00382814619400435\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04674122002389696\n",
      "Average test loss: 0.003785414801289638\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04653750042782889\n",
      "Average test loss: 0.00469052226220568\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04674636617468463\n",
      "Average test loss: 0.003693201047885749\n",
      "Epoch 296/300\n",
      "Average training loss: 0.046883009811242424\n",
      "Average test loss: 0.00361842555180192\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04645620206329558\n",
      "Average test loss: 0.014346547413203452\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04647409844729635\n",
      "Average test loss: 0.0035800635578731696\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04649098559551769\n",
      "Average test loss: 0.0036442134545909034\n",
      "Epoch 300/300\n",
      "Average training loss: 0.046552201476362014\n",
      "Average test loss: 0.0035581716150045396\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4449618046548631\n",
      "Average test loss: 0.005767867672360606\n",
      "Epoch 2/300\n",
      "Average training loss: 0.22479973108238643\n",
      "Average test loss: 0.004190251830137438\n",
      "Epoch 5/300\n",
      "Average training loss: 0.18151349143187204\n",
      "Average test loss: 0.003949792765908771\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1546584173573388\n",
      "Average test loss: 0.003744040089348952\n",
      "Epoch 7/300\n",
      "Average training loss: 0.13477107311619652\n",
      "Average test loss: 0.0037070880834427144\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1211826374663247\n",
      "Average test loss: 0.004439036782003111\n",
      "Epoch 9/300\n",
      "Average training loss: 0.11076208827230666\n",
      "Average test loss: 0.003771245573957761\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10272149739000533\n",
      "Average test loss: 0.003402735081811746\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09626804220676421\n",
      "Average test loss: 0.0036023845637424123\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09096667429473665\n",
      "Average test loss: 0.0038295822847220634\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08684622693724102\n",
      "Average test loss: 0.004128013675825463\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08249138768513997\n",
      "Average test loss: 0.0044402558755957415\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07909709524446064\n",
      "Average test loss: 0.003135552326424254\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07561887355645498\n",
      "Average test loss: 0.0029029233834395804\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07295061633984248\n",
      "Average test loss: 0.0027736696696115865\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0708758345776134\n",
      "Average test loss: 0.0027360025818149248\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06777833678987291\n",
      "Average test loss: 0.002807428838685155\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06552223331398434\n",
      "Average test loss: 0.004029260775074363\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06419614275958803\n",
      "Average test loss: 0.002957101536086864\n",
      "Epoch 22/300\n",
      "Average training loss: 0.061908105820417406\n",
      "Average test loss: 0.002721554991685682\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05953485031922658\n",
      "Average test loss: 0.0025487199758903846\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05815855062339041\n",
      "Average test loss: 0.0025899164167543253\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05325989720887608\n",
      "Average test loss: 0.002530453352050649\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05202223976784282\n",
      "Average test loss: 0.0024703879559205637\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05118101773990525\n",
      "Average test loss: 0.0025026855292833512\n",
      "Epoch 30/300\n",
      "Average training loss: 0.050246450735463036\n",
      "Average test loss: 0.0024505558932820955\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04961081529657046\n",
      "Average test loss: 0.0024843784369942214\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04894443145063188\n",
      "Average test loss: 0.002397691776562068\n",
      "Epoch 33/300\n",
      "Average training loss: 0.048112403773599204\n",
      "Average test loss: 0.0024133151198426884\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04776501399940915\n",
      "Average test loss: 0.0024195112801260418\n",
      "Epoch 35/300\n",
      "Average training loss: 0.055916125569078656\n",
      "Average test loss: 0.0025246491043104066\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06955787135163943\n",
      "Average test loss: 0.0024855648879375724\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05250745989547836\n",
      "Average test loss: 0.002461292110900912\n",
      "Epoch 38/300\n",
      "Average training loss: 0.050341950393385355\n",
      "Average test loss: 0.002435752779038416\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04912153923180368\n",
      "Average test loss: 0.21240687497456867\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04832605064577526\n",
      "Average test loss: 0.0023892242500765455\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04773904009825654\n",
      "Average test loss: 0.0023647288270294666\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04741196193297704\n",
      "Average test loss: 0.0023572758512778414\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04699921341405974\n",
      "Average test loss: 0.0023448223121878176\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04648455936378903\n",
      "Average test loss: 0.0023664935301575397\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04628504971994294\n",
      "Average test loss: 0.0023733641703923545\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04599572677413623\n",
      "Average test loss: 0.002654696446015603\n",
      "Epoch 47/300\n",
      "Average training loss: 0.045711170425017676\n",
      "Average test loss: 0.002696316215209663\n",
      "Epoch 48/300\n",
      "Average training loss: 0.045352020687527125\n",
      "Average test loss: 0.0024371700903607738\n",
      "Epoch 49/300\n",
      "Average training loss: 0.044635490112834506\n",
      "Average test loss: 0.0023231112682777976\n",
      "Epoch 52/300\n",
      "Average training loss: 0.044433921847078534\n",
      "Average test loss: 0.0025925309939516914\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04413765796356731\n",
      "Average test loss: 0.00231767254219287\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04400477604733573\n",
      "Average test loss: 0.00229784052963886\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04435698884394434\n",
      "Average test loss: 0.002462449896045857\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06524466530150837\n",
      "Average test loss: 0.002460753599802653\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04741089529461331\n",
      "Average test loss: 0.0023914568157245714\n",
      "Epoch 58/300\n",
      "Average training loss: 0.045691862881183626\n",
      "Average test loss: 0.002345855142093367\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04491208691067166\n",
      "Average test loss: 0.002336194959365659\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04435992035932011\n",
      "Average test loss: 0.0023274814964582523\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04400487214989132\n",
      "Average test loss: 0.0023080816217180755\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0438461906876829\n",
      "Average test loss: 0.0023853373022543057\n",
      "Epoch 63/300\n",
      "Average training loss: 0.043574159658617447\n",
      "Average test loss: 0.0023001277941382594\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04332735353045993\n",
      "Average test loss: 0.0023568768188771275\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04322689771652222\n",
      "Average test loss: 0.7797629980511136\n",
      "Epoch 66/300\n",
      "Average training loss: 0.043046762512789834\n",
      "Average test loss: 0.002317658887348241\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04297658949759271\n",
      "Average test loss: 0.002340800257606639\n",
      "Epoch 68/300\n",
      "Average training loss: 0.042876361078686184\n",
      "Average test loss: 0.002325305892775456\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04236954508887397\n",
      "Average test loss: 0.002334396550224887\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04219210587773058\n",
      "Average test loss: 0.0023396564709643524\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04217812481853697\n",
      "Average test loss: 0.0024112377878692413\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04191068793998824\n",
      "Average test loss: 0.0022897961851623323\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04178582935863071\n",
      "Average test loss: 0.002331514137486617\n",
      "Epoch 76/300\n",
      "Average training loss: 0.041610721020234956\n",
      "Average test loss: 0.0024847801538805166\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04157585502664248\n",
      "Average test loss: 0.0022623824619998536\n",
      "Epoch 78/300\n",
      "Average training loss: 0.041305429594384296\n",
      "Average test loss: 0.0023084958802080816\n",
      "Epoch 79/300\n",
      "Average training loss: 0.041275634659661185\n",
      "Average test loss: 0.002803261623614364\n",
      "Epoch 80/300\n",
      "Average training loss: 0.041153700643115576\n",
      "Average test loss: 0.0022953469639437067\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04102694210244549\n",
      "Average test loss: 0.0024012346907208364\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04086301675273313\n",
      "Average test loss: 0.0024257341497060324\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04073406963878208\n",
      "Average test loss: 0.0023112286838392416\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04068446644643942\n",
      "Average test loss: 0.0023413868894179663\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04072427987390095\n",
      "Average test loss: 0.002264641059976485\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04131294446852472\n",
      "Average test loss: 0.002307214054796431\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0405397402478589\n",
      "Average test loss: 0.0022929644734702177\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04029133180114958\n",
      "Average test loss: 0.002288963500617279\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04019126237101025\n",
      "Average test loss: 0.0023606253787875177\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04010064529048072\n",
      "Average test loss: 0.0023143148654037053\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04006991852323214\n",
      "Average test loss: 0.003932651011066305\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03996416711807251\n",
      "Average test loss: 0.002277310187721418\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0399037703010771\n",
      "Average test loss: 0.24632829596267805\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0399124556614293\n",
      "Average test loss: 0.0023254340280675226\n",
      "Epoch 95/300\n",
      "Average training loss: 0.039741411553488835\n",
      "Average test loss: 0.0023845888775669865\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03979115831520822\n",
      "Average test loss: 0.002300774641852412\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03955699169635773\n",
      "Average test loss: 0.07565591018564172\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03957975906133652\n",
      "Average test loss: 0.003746389776468277\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03947563362121582\n",
      "Average test loss: 0.0023664594578246276\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03938269485367669\n",
      "Average test loss: 0.002279800447117951\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03923387205600738\n",
      "Average test loss: 0.002306482344037957\n",
      "Epoch 102/300\n",
      "Average training loss: 0.039228510464231175\n",
      "Average test loss: 0.0028451107372012402\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03918506366345618\n",
      "Average test loss: 0.0023182904215322602\n",
      "Epoch 104/300\n",
      "Average training loss: 0.039326870030826994\n",
      "Average test loss: 0.002297287773961822\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03916939276456833\n",
      "Average test loss: 0.002292086451831791\n",
      "Epoch 106/300\n",
      "Average training loss: 0.038854096379545\n",
      "Average test loss: 0.0023079067783223256\n",
      "Epoch 107/300\n",
      "Average training loss: 0.038881583097908234\n",
      "Average test loss: 0.0024045796882775096\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03886762714385986\n",
      "Average test loss: 0.0023004196720818678\n",
      "Epoch 109/300\n",
      "Average training loss: 0.038735983868439995\n",
      "Average test loss: 0.002294960938807991\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03857407729824384\n",
      "Average test loss: 0.20027171822885673\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03851664018630981\n",
      "Average test loss: 0.0023458086682690516\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03862030606799655\n",
      "Average test loss: 0.0022927319911412065\n",
      "Epoch 115/300\n",
      "Average training loss: 0.038368649181392454\n",
      "Average test loss: 0.002387837053173118\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03837199468082852\n",
      "Average test loss: 0.0022991262146582207\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03834069028331174\n",
      "Average test loss: 0.002377602873577012\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03830071591006385\n",
      "Average test loss: 0.002314644609888395\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03890697799457444\n",
      "Average test loss: 0.0023210554981811177\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03805007666349411\n",
      "Average test loss: 0.0023141990840021107\n",
      "Epoch 121/300\n",
      "Average training loss: 0.038117757283978995\n",
      "Average test loss: 0.0023206513791034617\n",
      "Epoch 122/300\n",
      "Average training loss: 0.038029515993263985\n",
      "Average test loss: 0.0024661121086941825\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03808838175733884\n",
      "Average test loss: 0.002438328204779989\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03813663152522511\n",
      "Average test loss: 0.0023357724661214486\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03793043318721983\n",
      "Average test loss: 0.016077249554296336\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03782562257846196\n",
      "Average test loss: 0.0023125749996138942\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03799821968873342\n",
      "Average test loss: 0.08041465962843762\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03778986288772689\n",
      "Average test loss: 0.0026900187318937645\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03778024451931317\n",
      "Average test loss: 0.002416135346310006\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03762418908543057\n",
      "Average test loss: 0.0037062648170524174\n",
      "Epoch 131/300\n",
      "Average training loss: 0.037678815801938374\n",
      "Average test loss: 0.002340639140767356\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03759666669368744\n",
      "Average test loss: 0.002546172152583798\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03766321707765261\n",
      "Average test loss: 0.0023180371613966094\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03755189337333043\n",
      "Average test loss: 0.002363451581241356\n",
      "Epoch 135/300\n",
      "Average training loss: 0.037526773600114714\n",
      "Average test loss: 0.006837833220346107\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03739815557665295\n",
      "Average test loss: 0.0028323007192876605\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03740032951037089\n",
      "Average test loss: 0.007394222072429128\n",
      "Epoch 138/300\n",
      "Average training loss: 0.037394009504053326\n",
      "Average test loss: 0.00243986227715181\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03737629390756289\n",
      "Average test loss: 0.002511833730050259\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03728039078580009\n",
      "Average test loss: 0.0036491159696338906\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03726050325565868\n",
      "Average test loss: 0.0024116222799445194\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03719713436894947\n",
      "Average test loss: 0.002354781538868944\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0373341396384769\n",
      "Average test loss: 0.008225702786197265\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03712171658211284\n",
      "Average test loss: 0.002404933498241007\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03720893844630983\n",
      "Average test loss: 0.002480335852338208\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03722543965776761\n",
      "Average test loss: 0.007973951946943999\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03733209255999989\n",
      "Average test loss: 0.0026361822579056026\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0369322443207105\n",
      "Average test loss: 0.002352696071482367\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03689400676223967\n",
      "Average test loss: 0.003802970887058311\n",
      "Epoch 152/300\n",
      "Average training loss: 0.036896225055058796\n",
      "Average test loss: 0.0026565622333437205\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03688629321257273\n",
      "Average test loss: 0.002407757620430655\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0367442200448778\n",
      "Average test loss: 0.0023494912709833846\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03673337147302098\n",
      "Average test loss: 0.002510974352558454\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03683843091461394\n",
      "Average test loss: 0.002363293354296022\n",
      "Epoch 157/300\n",
      "Average training loss: 0.036790866547160676\n",
      "Average test loss: 0.0033145382499529257\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03672322237491608\n",
      "Average test loss: 0.0024124206453561784\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03666711239516735\n",
      "Average test loss: 0.0026329226955357525\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036656431294149826\n",
      "Average test loss: 0.002405149244910313\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03662066117260191\n",
      "Average test loss: 0.0024259323715749713\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03656836610039075\n",
      "Average test loss: 0.00240787803950823\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036571794907251994\n",
      "Average test loss: 0.002414001986798313\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03654852759010262\n",
      "Average test loss: 0.0024559464580896826\n",
      "Epoch 165/300\n",
      "Average training loss: 0.036493160229590205\n",
      "Average test loss: 0.0024496942282550864\n",
      "Epoch 166/300\n",
      "Average training loss: 0.036501013043853975\n",
      "Average test loss: 0.0023711118075168797\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0364159910413954\n",
      "Average test loss: 0.0038159804962989356\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03640540668699477\n",
      "Average test loss: 0.00237933506609665\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03639803985423512\n",
      "Average test loss: 0.002367848033292426\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03636008340451453\n",
      "Average test loss: 0.018739642897413835\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03626130371458001\n",
      "Average test loss: 0.0024647138975560667\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03628349089953634\n",
      "Average test loss: 0.002504139968711469\n",
      "Epoch 175/300\n",
      "Average training loss: 0.036296398669481277\n",
      "Average test loss: 0.0025305565293464397\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03625149452189604\n",
      "Average test loss: 0.0027038583903469973\n",
      "Epoch 177/300\n",
      "Average training loss: 0.036216516494750974\n",
      "Average test loss: 0.002384821892198589\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0361371765004264\n",
      "Average test loss: 0.002486997785874539\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03620702591207292\n",
      "Average test loss: 0.0026546708003928263\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03618458631965849\n",
      "Average test loss: 0.0033309561444653407\n",
      "Epoch 181/300\n",
      "Average training loss: 0.036096268715129956\n",
      "Average test loss: 0.0026713635152619745\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03609401790963279\n",
      "Average test loss: 0.002885812096711662\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03602639887730281\n",
      "Average test loss: 0.0023961335301606193\n",
      "Epoch 184/300\n",
      "Average training loss: 0.036063318447934256\n",
      "Average test loss: 0.0029282030016183854\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03608481050531069\n",
      "Average test loss: 0.002444009063558446\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03599067785011398\n",
      "Average test loss: 7.192228602939182\n",
      "Epoch 187/300\n",
      "Average training loss: 0.036135933647553124\n",
      "Average test loss: 0.0024312258538686567\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03590449596775903\n",
      "Average test loss: 0.0025019225840353303\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0358835160765383\n",
      "Average test loss: 0.008858906362619665\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0359981404112445\n",
      "Average test loss: 0.0024745014259177777\n",
      "Epoch 193/300\n",
      "Average training loss: 0.035841065641906526\n",
      "Average test loss: 0.002539462065117227\n",
      "Epoch 194/300\n",
      "Average training loss: 0.035846815125809776\n",
      "Average test loss: 0.002390004599880841\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03584380241566234\n",
      "Average test loss: 0.00274918826835023\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03577921759751108\n",
      "Average test loss: 0.002733151708419124\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03605652155478795\n",
      "Average test loss: 0.0024349126296324864\n",
      "Epoch 198/300\n",
      "Average training loss: 0.035641240121589764\n",
      "Average test loss: 0.0025675680513183277\n",
      "Epoch 199/300\n",
      "Average training loss: 0.035764538493421344\n",
      "Average test loss: 0.002550609975225396\n",
      "Epoch 200/300\n",
      "Average training loss: 0.035770997365315756\n",
      "Average test loss: 0.002556359114125371\n",
      "Epoch 201/300\n",
      "Average training loss: 0.035705776979525886\n",
      "Average test loss: 0.0025086342555781203\n",
      "Epoch 202/300\n",
      "Average training loss: 0.035718557414081364\n",
      "Average test loss: 0.002456942189277874\n",
      "Epoch 203/300\n",
      "Average training loss: 0.035641853229867085\n",
      "Average test loss: 0.0025408538930738966\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03578265837166045\n",
      "Average test loss: 0.0024442150770790047\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03564943843748834\n",
      "Average test loss: 0.002454146104554335\n",
      "Epoch 206/300\n",
      "Average training loss: 0.035652544382545684\n",
      "Average test loss: 0.002544835512422853\n",
      "Epoch 207/300\n",
      "Average test loss: 0.0025705179466555517\n",
      "Epoch 210/300\n",
      "Average training loss: 0.035560249166356195\n",
      "Average test loss: 0.002442347766417596\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03563492765691545\n",
      "Average test loss: 0.002452290419075224\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03543492766221364\n",
      "Average test loss: 0.0025141447480354043\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03552318938242065\n",
      "Average test loss: 0.002453639074332184\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03546377954714828\n",
      "Average test loss: 0.002520017167035904\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03541970313919915\n",
      "Average test loss: 0.002703851641776661\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03546639244755109\n",
      "Average test loss: 0.0024787991667787235\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03550281478630172\n",
      "Average test loss: 0.0025181474284165437\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03544004519780477\n",
      "Average test loss: 0.0024872990917000504\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03542026991479927\n",
      "Average test loss: 0.0024724925172825655\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03529026361306509\n",
      "Average test loss: 0.002713924446867572\n",
      "Epoch 221/300\n",
      "Average training loss: 0.035475091834863026\n",
      "Average test loss: 0.0025943693584865994\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03534481475419468\n",
      "Average test loss: 0.00288475552904937\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03554262479477459\n",
      "Average test loss: 0.002431713634274072\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03523774144384596\n",
      "Average test loss: 0.0024701942269586856\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03529853492975235\n",
      "Average test loss: 0.007392545549612906\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03534396728542116\n",
      "Average test loss: 0.002459141714912322\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0353012555539608\n",
      "Average test loss: 0.0024737788062128754\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03519082820415497\n",
      "Average test loss: 0.00247264204836554\n",
      "Epoch 231/300\n",
      "Average training loss: 0.035354966084162394\n",
      "Average test loss: 0.002423967999095718\n",
      "Epoch 232/300\n",
      "Average training loss: 0.035185263011190626\n",
      "Average test loss: 0.0024611786012020374\n",
      "Epoch 233/300\n",
      "Average training loss: 0.035225852962997226\n",
      "Average test loss: 0.0024831390728553135\n",
      "Epoch 234/300\n",
      "Average training loss: 0.035159733159674536\n",
      "Average test loss: 0.0024497685665264726\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03530560423268212\n",
      "Average test loss: 0.0024771405299090677\n",
      "Epoch 236/300\n",
      "Average training loss: 0.035149990287091995\n",
      "Average test loss: 0.002524566450363232\n",
      "Epoch 237/300\n",
      "Average training loss: 0.035107081416580414\n",
      "Average test loss: 0.0025471599089602628\n",
      "Epoch 238/300\n",
      "Average training loss: 0.035126048783461254\n",
      "Average test loss: 0.004295515265315771\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035123704069190555\n",
      "Average test loss: 0.0024715016557731563\n",
      "Epoch 240/300\n",
      "Average training loss: 0.035203642070293424\n",
      "Average test loss: 0.00253104885853827\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03510014565620157\n",
      "Average test loss: 0.0024587510004639625\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03511222705410586\n",
      "Average test loss: 0.0025769504057243466\n",
      "Epoch 243/300\n",
      "Average training loss: 0.035109370074338384\n",
      "Average test loss: 0.00252858277130872\n",
      "Epoch 244/300\n",
      "Average training loss: 0.035487250889341036\n",
      "Average test loss: 0.002825766187782089\n",
      "Epoch 245/300\n",
      "Average training loss: 0.034950449148813886\n",
      "Average test loss: 0.0025159926729069818\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03492945579191049\n",
      "Average test loss: 0.0025932307626224224\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03501023486256599\n",
      "Average test loss: 0.0027777161988326244\n",
      "Epoch 248/300\n",
      "Average training loss: 0.035056740439600415\n",
      "Average test loss: 0.0025903282027898563\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03511627160840564\n",
      "Average test loss: 0.0028112258377174537\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03490444747441345\n",
      "Average test loss: 0.0025152995327694548\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03491225161486202\n",
      "Average test loss: 0.0024878276367671787\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03489556016524633\n",
      "Average test loss: 0.0030490585789084436\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03488919952346219\n",
      "Average test loss: 0.0025748858108288713\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03489331923921903\n",
      "Average test loss: 0.0024674816206097603\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03495637340015835\n",
      "Average test loss: 0.00263633270892832\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03503080831964811\n",
      "Average test loss: 0.002551779870564739\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03481668277581533\n",
      "Average test loss: 0.005781976392699613\n",
      "Epoch 260/300\n",
      "Average training loss: 0.034998604440026816\n",
      "Average test loss: 0.0026069081659532253\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03478835647967127\n",
      "Average test loss: 0.0026163685512211586\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03481340792775154\n",
      "Average test loss: 0.002558805526130729\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034786062217421004\n",
      "Average test loss: 0.0024863600445290406\n",
      "Epoch 264/300\n",
      "Average training loss: 0.034867853313684465\n",
      "Average test loss: 0.002482561823187603\n",
      "Epoch 265/300\n",
      "Average training loss: 0.034834652211931015\n",
      "Average test loss: 0.002468839737483197\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03480399367544386\n",
      "Average test loss: 0.002455703110848036\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03477462375495169\n",
      "Average test loss: 0.00258190883965128\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03477496427959866\n",
      "Average test loss: 0.004256046225627264\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03478522282342116\n",
      "Average test loss: 0.0028152467851630517\n",
      "Epoch 270/300\n",
      "Average training loss: 0.034792492823468316\n",
      "Average test loss: 0.002873079900526338\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03472048641906844\n",
      "Average test loss: 0.00248282924749785\n",
      "Epoch 272/300\n",
      "Average training loss: 0.034814890002210934\n",
      "Average test loss: 0.003341540277728604\n",
      "Epoch 273/300\n",
      "Average training loss: 0.034680770945217876\n",
      "Average test loss: 0.0031502717362923757\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03470841681957245\n",
      "Average test loss: 0.002496262845272819\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03468997362918324\n",
      "Average test loss: 0.002484050301214059\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03479397326376703\n",
      "Average test loss: 0.003728470332920551\n",
      "Epoch 280/300\n",
      "Average training loss: 0.034755138632324004\n",
      "Average test loss: 0.0029236911520775823\n",
      "Epoch 281/300\n",
      "Average training loss: 0.034732608361376656\n",
      "Average test loss: 0.0024874957191447417\n",
      "Epoch 282/300\n",
      "Average training loss: 0.034582214925024245\n",
      "Average test loss: 0.002565685536298487\n",
      "Epoch 283/300\n",
      "Average training loss: 0.034549079325464034\n",
      "Average test loss: 0.002788278188970354\n",
      "Epoch 284/300\n",
      "Average test loss: 0.002675289449799392\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03460635285576184\n",
      "Average test loss: 0.0024942905481697784\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0345369205739763\n",
      "Average test loss: 0.002866886489921146\n",
      "Epoch 289/300\n",
      "Average training loss: 0.034582544651296405\n",
      "Average test loss: 0.002586416688644224\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03460470760365327\n",
      "Average test loss: 0.002524740915848977\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03452853630317582\n",
      "Average test loss: 0.0024734529563122326\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03458290432890256\n",
      "Average test loss: 0.002597756906929943\n",
      "Epoch 293/300\n",
      "Average training loss: 0.034482787430286405\n",
      "Average test loss: 0.002489263950329688\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03459892662697368\n",
      "Average test loss: 0.002927679849271145\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03457050183084276\n",
      "Average test loss: 0.002536265657386846\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03445350333385997\n",
      "Average test loss: 0.0025172264284143847\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0344693686068058\n",
      "Average test loss: 0.0025656288996752765\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03455807436174817\n",
      "Average test loss: 0.0025173934364898336\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03451252760158645\n",
      "Average test loss: 0.0025092070074751974\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03452427302963204\n",
      "Average test loss: 0.0025474306914127537\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4101783800654941\n",
      "Average test loss: 0.005112273863620228\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4103431461122301\n",
      "Average test loss: 0.004126366022146411\n",
      "Epoch 3/300\n",
      "Average training loss: 0.267390947871738\n",
      "Average test loss: 0.0036843995195296074\n",
      "Epoch 4/300\n",
      "Average training loss: 0.19857573069466486\n",
      "Average test loss: 0.0036169892799937063\n",
      "Epoch 5/300\n",
      "Average training loss: 0.15902615102132162\n",
      "Average test loss: 0.0032631051929460633\n",
      "Epoch 6/300\n",
      "Average training loss: 0.13320113196637895\n",
      "Average test loss: 0.0032449148955444496\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11615021111567815\n",
      "Average test loss: 0.00499599969221486\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10370352493392096\n",
      "Average test loss: 0.00310283843262328\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07669312997659047\n",
      "Average test loss: 0.0025945067732698387\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07251109498076969\n",
      "Average test loss: 0.002556473823988603\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06895699190762308\n",
      "Average test loss: 0.0026091021429747343\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06599749585655\n",
      "Average test loss: 0.00243734008880953\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06361026753981908\n",
      "Average test loss: 0.002788245827994413\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0607109809451633\n",
      "Average test loss: 0.0026460895523842836\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05856527723537551\n",
      "Average test loss: 0.0023502888534631993\n",
      "Epoch 19/300\n",
      "Average training loss: 0.056510956611898214\n",
      "Average test loss: 0.0023599493364906974\n",
      "Epoch 20/300\n",
      "Average training loss: 0.054445601950089136\n",
      "Average test loss: 0.002851332394199239\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05271998505128755\n",
      "Average test loss: 0.002346822515130043\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05119631281495094\n",
      "Average test loss: 0.002275106193911698\n",
      "Epoch 23/300\n",
      "Average training loss: 0.049664774258931475\n",
      "Average test loss: 0.00773797525651753\n",
      "Epoch 24/300\n",
      "Average training loss: 0.047730671071343954\n",
      "Average test loss: 0.0020436448802550636\n",
      "Epoch 25/300\n",
      "Average training loss: 0.046464849084615706\n",
      "Average test loss: 0.0019451959393918515\n",
      "Epoch 26/300\n",
      "Average training loss: 0.045066515018542606\n",
      "Average test loss: 0.0019940323633038334\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04376968351172077\n",
      "Average test loss: 0.001955347851436171\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04258948954608705\n",
      "Average test loss: 0.001932839430661665\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0416503626273738\n",
      "Average test loss: 0.001907332107424736\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04070576407512029\n",
      "Average test loss: 0.0018552966099232436\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04016126247909334\n",
      "Average test loss: 0.0018341255330791076\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03977374684479502\n",
      "Average test loss: 0.0018753008543410236\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04016096915470229\n",
      "Average test loss: 0.0018942248035843173\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0385546376456817\n",
      "Average test loss: 0.001884851438510749\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03786652564340168\n",
      "Average test loss: 0.0019008543921841515\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03661766439014011\n",
      "Average test loss: 0.0017952953786071805\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03624939666688442\n",
      "Average test loss: 0.0017459694082952208\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03608710568812158\n",
      "Average test loss: 0.0022519656837814385\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03580952322151926\n",
      "Average test loss: 0.0017884142357442114\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03544755790630976\n",
      "Average test loss: 0.0017552957613435056\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03525133072005378\n",
      "Average test loss: 0.00174995882757422\n",
      "Epoch 44/300\n",
      "Average training loss: 0.035009927014509834\n",
      "Average test loss: 0.0019813557534168165\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03483538281255298\n",
      "Average test loss: 0.0017718685153457854\n",
      "Epoch 46/300\n",
      "Average training loss: 0.034554775095648235\n",
      "Average test loss: 0.0033063350125319426\n",
      "Epoch 47/300\n",
      "Average training loss: 0.034394555682937304\n",
      "Average test loss: 0.0017436221072243319\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03439058101508353\n",
      "Average test loss: 0.0018819855546785725\n",
      "Epoch 49/300\n",
      "Average training loss: 0.034039687062303224\n",
      "Average test loss: 0.0018415822443655794\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03396163440744082\n",
      "Average test loss: 0.005578397472699483\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03384572559926245\n",
      "Average test loss: 0.0017596295824688342\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03365544558233685\n",
      "Average test loss: 0.0043677783672594365\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03368211450179418\n",
      "Average test loss: 0.010101209729909896\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03332356117831336\n",
      "Average test loss: 0.0016972368866619136\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03336206368439727\n",
      "Average test loss: 0.0016999813742521737\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03328309054838287\n",
      "Average test loss: 0.0017625963404360745\n",
      "Epoch 57/300\n",
      "Average training loss: 0.033023076597187256\n",
      "Average test loss: 0.0017164907926279637\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03304173273510403\n",
      "Average test loss: 0.0017360306380109654\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03288665404419104\n",
      "Average test loss: 0.0017169759536368979\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03295393631193373\n",
      "Average test loss: 0.0017131904609915282\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03260965845982234\n",
      "Average test loss: 0.0020185307790007855\n",
      "Epoch 62/300\n",
      "Average training loss: 0.032579849918683366\n",
      "Average test loss: 0.0018530120882723066\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03230365642242961\n",
      "Average test loss: 0.0016841056975018646\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03278506695230802\n",
      "Average test loss: 0.001854627924453881\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03207574244009124\n",
      "Average test loss: 0.0016960312428159845\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03199734767609173\n",
      "Average test loss: 0.001840888719384869\n",
      "Epoch 70/300\n",
      "Average training loss: 0.031875573178132374\n",
      "Average test loss: 0.0022841697964403364\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03190941232939561\n",
      "Average test loss: 0.0018620329852112466\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03178565565745036\n",
      "Average test loss: 0.0017068637841277652\n",
      "Epoch 75/300\n",
      "Average training loss: 0.031638737032810844\n",
      "Average test loss: 0.0017149851856132349\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03157768881983227\n",
      "Average test loss: 0.00191890166948239\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03163471868634224\n",
      "Average test loss: 0.002047017167011897\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0314442630343967\n",
      "Average test loss: 0.0017898794268775317\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03150863743159506\n",
      "Average test loss: 0.0017269614409758812\n",
      "Epoch 80/300\n",
      "Average training loss: 0.031418394509289\n",
      "Average test loss: 0.004615736817216708\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03134897718992498\n",
      "Average test loss: 0.0017825683760974143\n",
      "Epoch 82/300\n",
      "Average training loss: 0.031369202742973966\n",
      "Average test loss: 0.0017012726401703226\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03132450787557496\n",
      "Average test loss: 0.0017472974890843033\n",
      "Epoch 84/300\n",
      "Average training loss: 0.031197105523612764\n",
      "Average test loss: 0.0017938425252214075\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03112430600159698\n",
      "Average test loss: 0.0017279716898790664\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03101290022002326\n",
      "Average test loss: 0.002555285712911023\n",
      "Epoch 87/300\n",
      "Average training loss: 0.031049580287602213\n",
      "Average test loss: 0.001883380319405761\n",
      "Epoch 88/300\n",
      "Average training loss: 0.030948305441273583\n",
      "Average test loss: 0.004089934152033594\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03099520159429974\n",
      "Average test loss: 0.0017833753935992717\n",
      "Epoch 90/300\n",
      "Average training loss: 0.030854106593463156\n",
      "Average test loss: 0.0017219724143958755\n",
      "Epoch 91/300\n",
      "Average training loss: 0.030838028263714578\n",
      "Average test loss: 0.0021743549638324313\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03061898455851608\n",
      "Average test loss: 0.0018784015937190917\n",
      "Epoch 95/300\n",
      "Average training loss: 0.030637924830118814\n",
      "Average test loss: 0.0018917063851323392\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03165470298793581\n",
      "Average test loss: 0.0018285012231725785\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03077956504623095\n",
      "Average test loss: 0.0017221608143299818\n",
      "Epoch 98/300\n",
      "Average training loss: 0.030465993920962015\n",
      "Average test loss: 0.001731344756980737\n",
      "Epoch 99/300\n",
      "Average training loss: 0.030462620394097435\n",
      "Average test loss: 0.001772173524937696\n",
      "Epoch 100/300\n",
      "Average training loss: 0.030403079387214448\n",
      "Average test loss: 0.0020306130720095504\n",
      "Epoch 101/300\n",
      "Average training loss: 0.030410599478416973\n",
      "Average test loss: 0.0017618266910107599\n",
      "Epoch 102/300\n",
      "Average training loss: 0.030375691850980123\n",
      "Average test loss: 0.0017405061202330722\n",
      "Epoch 103/300\n",
      "Average training loss: 0.030639958853522938\n",
      "Average test loss: 0.0018487321404326294\n",
      "Epoch 104/300\n",
      "Average training loss: 0.030235492532451946\n",
      "Average test loss: 0.001846035537827346\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030207233856121698\n",
      "Average test loss: 0.0017544863942182726\n",
      "Epoch 106/300\n",
      "Average training loss: 0.030205555114481183\n",
      "Average test loss: 0.001737833561996619\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03073398592405849\n",
      "Average test loss: 0.0018413669587009484\n",
      "Epoch 108/300\n",
      "Average training loss: 0.030114761327703795\n",
      "Average test loss: 0.002137388446057836\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03005806958509816\n",
      "Average test loss: 0.0017777057853009966\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0300508360804783\n",
      "Average test loss: 0.0018244194708143672\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030043906541334257\n",
      "Average test loss: 0.0017410317212343215\n",
      "Epoch 112/300\n",
      "Average training loss: 0.030261030891703233\n",
      "Average test loss: 0.0017931113956082198\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03000211207734214\n",
      "Average test loss: 0.0021841142922639847\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02994109119971593\n",
      "Average test loss: 0.0017703618249959416\n",
      "Epoch 115/300\n",
      "Average training loss: 0.030375106735361947\n",
      "Average test loss: 0.0017500092653143736\n",
      "Epoch 116/300\n",
      "Average training loss: 0.029831169615189234\n",
      "Average test loss: 0.0017527439635660912\n",
      "Epoch 117/300\n",
      "Average training loss: 0.029818752490811878\n",
      "Average training loss: 0.030063963370190728\n",
      "Average test loss: 0.0017725028737137715\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02967345316708088\n",
      "Average test loss: 0.0032791678913765485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.029654853362176152\n",
      "Average test loss: 0.0018337793021152417\n",
      "Epoch 123/300\n",
      "Average training loss: 0.032322023527489765\n",
      "Average test loss: 0.001808720656256709\n",
      "Epoch 124/300\n",
      "Average training loss: 0.029850492431057824\n",
      "Average test loss: 0.0017684995467878051\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02954199651049243\n",
      "Average test loss: 0.002089565431078275\n",
      "Epoch 126/300\n",
      "Average training loss: 0.029505703075064553\n",
      "Average test loss: 0.0018077125336147016\n",
      "Epoch 127/300\n",
      "Average training loss: 0.029513749352759786\n",
      "Average test loss: 0.0018511137005148663\n",
      "Epoch 128/300\n",
      "Average training loss: 0.029528993904590607\n",
      "Average test loss: 0.0018525965391761726\n",
      "Epoch 129/300\n",
      "Average training loss: 0.029547009751200677\n",
      "Average test loss: 0.0018268887379931078\n",
      "Epoch 130/300\n",
      "Average training loss: 0.029503059272964795\n",
      "Average test loss: 0.0018752023602525392\n",
      "Epoch 131/300\n",
      "Average training loss: 0.029491802328162724\n",
      "Average test loss: 0.0017658050322077342\n",
      "Epoch 132/300\n",
      "Average training loss: 0.029448608332210118\n",
      "Average test loss: 0.0018085701186209917\n",
      "Epoch 133/300\n",
      "Average training loss: 0.029652763611740535\n",
      "Average test loss: 0.0017617993949809007\n",
      "Epoch 134/300\n",
      "Average training loss: 0.029361404054694704\n",
      "Average test loss: 0.0017658542800280783\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02942606246471405\n",
      "Average test loss: 0.0018577213294597135\n",
      "Epoch 136/300\n",
      "Average training loss: 0.029337412175205017\n",
      "Average test loss: 0.002064475902666648\n",
      "Epoch 137/300\n",
      "Average training loss: 0.029398934829566212\n",
      "Average test loss: 0.001775588855975204\n",
      "Epoch 138/300\n",
      "Average training loss: 0.029328655044237772\n",
      "Average test loss: 0.002141172050601906\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02943597298529413\n",
      "Average test loss: 0.001782349177222285\n",
      "Epoch 140/300\n",
      "Average training loss: 0.029270264208316802\n",
      "Average test loss: 0.0017571163059522709\n",
      "Epoch 141/300\n",
      "Average training loss: 0.029255871219767464\n",
      "Average test loss: 0.002339566121912665\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02917383861872885\n",
      "Average test loss: 0.0017911154390830132\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02914170988731914\n",
      "Average test loss: 0.0017629819815564486\n",
      "Epoch 147/300\n",
      "Average training loss: 0.029217931360006333\n",
      "Average test loss: 0.0017760888310149312\n",
      "Epoch 148/300\n",
      "Average training loss: 0.029012851774692535\n",
      "Average test loss: 0.001905590682807896\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02902812524802155\n",
      "Average test loss: 0.002621752506535914\n",
      "Epoch 150/300\n",
      "Average training loss: 0.029105772025055356\n",
      "Average test loss: 0.002284702608982722\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02908964494533009\n",
      "Average test loss: 0.0021293446867623263\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030004706202281847\n",
      "Average test loss: 0.0018319621359308561\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02896161553594801\n",
      "Average test loss: 0.00188189448033356\n",
      "Epoch 156/300\n",
      "Average training loss: 0.028837320026424196\n",
      "Average test loss: 0.0018042815408358971\n",
      "Epoch 157/300\n",
      "Average training loss: 0.028839521689547433\n",
      "Average test loss: 0.002046764185134735\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028906452937258614\n",
      "Average test loss: 0.0017843812455733618\n",
      "Epoch 159/300\n",
      "Average training loss: 0.028822653399573434\n",
      "Average test loss: 0.0018024134376189774\n",
      "Epoch 160/300\n",
      "Average training loss: 0.028898749253816074\n",
      "Average test loss: 0.0018908101703143782\n",
      "Epoch 161/300\n",
      "Average training loss: 0.028981716194086606\n",
      "Average test loss: 0.0018050144794914456\n",
      "Epoch 162/300\n",
      "Average training loss: 0.028916588697168562\n",
      "Average test loss: 0.0018290882548317312\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02870662833750248\n",
      "Average test loss: 0.006003607942826218\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029088998231622906\n",
      "Average test loss: 0.0017754808579468066\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02870821519030465\n",
      "Average test loss: 0.0017826368984662825\n",
      "Epoch 166/300\n",
      "Average training loss: 0.028713510980208714\n",
      "Average test loss: 0.001807279328091277\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02878633946345912\n",
      "Average test loss: 0.00214594464811186\n",
      "Epoch 168/300\n",
      "Average training loss: 0.028635533743434482\n",
      "Average test loss: 0.0018372365575697687\n",
      "Epoch 169/300\n",
      "Average training loss: 0.028742413090334998\n",
      "Average test loss: 0.0023630000315606596\n",
      "Epoch 170/300\n",
      "Average training loss: 0.028714500460359785\n",
      "Average test loss: 0.0018797565694484446\n",
      "Epoch 171/300\n",
      "Average training loss: 0.028681438480814298\n",
      "Average test loss: 0.0018989079060653846\n",
      "Epoch 172/300\n",
      "Average training loss: 0.028692446319593324\n",
      "Average test loss: 0.001901833944229616\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02893816608687242\n",
      "Average test loss: 0.0029982382617890833\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02877228568163183\n",
      "Average test loss: 0.0018032538619720273\n",
      "Epoch 175/300\n",
      "Average training loss: 0.028514754056930543\n",
      "Average test loss: 0.0018399748572458824\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02852021667526828\n",
      "Average test loss: 0.0018433015145775345\n",
      "Epoch 177/300\n",
      "Average training loss: 0.028638891978396308\n",
      "Average test loss: 0.001811302897416883\n",
      "Epoch 178/300\n",
      "Average training loss: 0.028569896669851408\n",
      "Average test loss: 0.004387329972659548\n",
      "Epoch 179/300\n",
      "Average training loss: 0.028507397409942414\n",
      "Average test loss: 0.0019398224550402827\n",
      "Epoch 180/300\n",
      "Average training loss: 0.028531983421908484\n",
      "Average test loss: 0.002031230047552122\n",
      "Epoch 181/300\n",
      "Average training loss: 0.028474917007817162\n",
      "Average test loss: 0.003188822510962685\n",
      "Epoch 182/300\n",
      "Average training loss: 0.028517532555593385\n",
      "Average test loss: 0.0017951394888675875\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02858261611726549\n",
      "Average test loss: 0.0019109845670560997\n",
      "Epoch 184/300\n",
      "Average training loss: 0.028456254944205284\n",
      "Average test loss: 0.0021642931583854886\n",
      "Epoch 185/300\n",
      "Average training loss: 0.028787402785486645\n",
      "Average test loss: 2.000941033336851\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028921448219153618\n",
      "Average test loss: 0.0018470763974926538\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02836359742614958\n",
      "Average test loss: 0.0018487622067332268\n",
      "Epoch 189/300\n",
      "Average training loss: 0.028416773405339983\n",
      "Average test loss: 0.0029972631888878012\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02824592718647586\n",
      "Average test loss: 0.0018908649514325791\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02885517403152254\n",
      "Average test loss: 0.0018378870895960264\n",
      "Epoch 196/300\n",
      "Average training loss: 0.028352382289038764\n",
      "Average test loss: 0.001877006069964005\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028288498040702607\n",
      "Average test loss: 0.0018077510009623236\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02823179565370083\n",
      "Average test loss: 0.0018157157802747355\n",
      "Epoch 202/300\n",
      "Average training loss: 0.028271599375539355\n",
      "Average test loss: 0.00263601608429518\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02820355009039243\n",
      "Average test loss: 0.0018526088195956415\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02821222692065769\n",
      "Average test loss: 0.003185250234376225\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028210438051157527\n",
      "Average test loss: 0.0018417727688534392\n",
      "Epoch 206/300\n",
      "Average training loss: 0.028150697234604095\n",
      "Average test loss: 0.0018277113924009933\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028497658416628838\n",
      "Average test loss: 0.001862793170950479\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02807457770076063\n",
      "Average test loss: 0.0024624544738067523\n",
      "Epoch 209/300\n",
      "Average training loss: 0.028110374074843195\n",
      "Average test loss: 0.0019658815519263345\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028094808614916273\n",
      "Average test loss: 0.0018288524300894803\n",
      "Epoch 211/300\n",
      "Average training loss: 0.028235370291603935\n",
      "Average test loss: 0.010792310097979175\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02808011280331347\n",
      "Average test loss: 0.0018257656041532754\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02803294659157594\n",
      "Average test loss: 0.0018379894798207614\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02824318081802792\n",
      "Average test loss: 0.001821446645177073\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02805354129605823\n",
      "Average test loss: 0.0018296237184355657\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02806234618028005\n",
      "Average test loss: 0.001983677404622237\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028017698698573644\n",
      "Average test loss: 0.0025579341078797975\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028116148480110698\n",
      "Average test loss: 0.0018849438424739572\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028012191726101768\n",
      "Average test loss: 0.0018389549309180842\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028143256111277475\n",
      "Average test loss: 0.0018250396365506781\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02803910432755947\n",
      "Average test loss: 0.0020616170299342937\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028043541320496135\n",
      "Average test loss: 0.0018035652947922549\n",
      "Epoch 223/300\n",
      "Average training loss: 0.028467019794715775\n",
      "Average test loss: 0.001862001339180602\n",
      "Epoch 224/300\n",
      "Average training loss: 0.027941705647442076\n",
      "Average test loss: 0.0018577154882045257\n",
      "Epoch 228/300\n",
      "Average training loss: 0.028117249223921035\n",
      "Average test loss: 0.001889605639088485\n",
      "Epoch 229/300\n",
      "Average training loss: 0.027874886709782814\n",
      "Average test loss: 0.001854886548800601\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02793604826927185\n",
      "Average test loss: 0.002899854980202185\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02785907651649581\n",
      "Average test loss: 0.0020192109771693745\n",
      "Epoch 232/300\n",
      "Average training loss: 0.027908966766463386\n",
      "Average test loss: 0.0019114203427193894\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02794547640118334\n",
      "Average test loss: 0.002043045939049787\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027816277437739904\n",
      "Average test loss: 0.0032199987607697644\n",
      "Epoch 235/300\n",
      "Average training loss: 0.027866753020220333\n",
      "Average test loss: 0.001845969582390454\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02790543111993207\n",
      "Average test loss: 0.0018625749901144042\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0278299800157547\n",
      "Average test loss: 0.0018542083255532716\n",
      "Epoch 238/300\n",
      "Average training loss: 0.027878540416558583\n",
      "Average test loss: 0.003189851801014609\n",
      "Epoch 239/300\n",
      "Average training loss: 0.027862371650007035\n",
      "Average test loss: 0.001908656300769912\n",
      "Epoch 240/300\n",
      "Average training loss: 0.027763753104541038\n",
      "Average test loss: 0.0021985248970902626\n",
      "Epoch 241/300\n",
      "Average training loss: 0.027876213550567627\n",
      "Average test loss: 0.0019015601189393135\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027744222573108143\n",
      "Average test loss: 0.001902067926298413\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02801688571108712\n",
      "Average test loss: 0.002479237254605525\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02774508110185464\n",
      "Average test loss: 0.0018363331829508145\n",
      "Epoch 245/300\n",
      "Average training loss: 0.027784372705552313\n",
      "Average test loss: 0.0019049291732824512\n",
      "Epoch 246/300\n",
      "Average training loss: 0.027775780863232083\n",
      "Average test loss: 0.002368986714631319\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02805500074558788\n",
      "Average test loss: 0.0018847818592977192\n",
      "Epoch 248/300\n",
      "Average training loss: 0.027704856703678768\n",
      "Average test loss: 0.0025988617069605323\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027737912237644196\n",
      "Average test loss: 0.001876979513714711\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02771185624599457\n",
      "Average test loss: 0.0018793100166238016\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02774878787663248\n",
      "Average test loss: 0.007588271800014708\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027660933249526554\n",
      "Average test loss: 0.001875803815614846\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02765919025407897\n",
      "Average test loss: 0.00197925069110675\n",
      "Epoch 257/300\n",
      "Average training loss: 0.027645524385902617\n",
      "Average test loss: 0.0019038905818015337\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0278645216524601\n",
      "Average test loss: 0.0018201473786806067\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02757515385084682\n",
      "Average test loss: 0.001843387604277167\n",
      "Epoch 260/300\n",
      "Average training loss: 0.027668697522746193\n",
      "Average test loss: 0.0018661636509415176\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027946974703007273\n",
      "Average test loss: 0.0018677108093268341\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027636282654272187\n",
      "Average test loss: 0.002278662361825506\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02760877297818661\n",
      "Average test loss: 0.001846031620581117\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02757343749867545\n",
      "Average test loss: 0.002258646729712685\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027630169625083606\n",
      "Average test loss: 0.0019469742744954097\n",
      "Epoch 266/300\n",
      "Average training loss: 0.027711853050523334\n",
      "Average test loss: 0.001953444454094602\n",
      "Epoch 267/300\n",
      "Average training loss: 0.027544537850552137\n",
      "Average test loss: 0.0018637964937628971\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02783462055855327\n",
      "Average test loss: 0.002442370788090759\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0275911693043179\n",
      "Average test loss: 0.0019499579231358236\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027487733806173007\n",
      "Average test loss: 0.001852025330480602\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02753140803674857\n",
      "Average test loss: 0.001941821288317442\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027592829318510162\n",
      "Average test loss: 0.004730100498638219\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027595008061991798\n",
      "Average test loss: 0.002228001867938373\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027584379461076523\n",
      "Average test loss: 0.0018493445247618689\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027489499340454738\n",
      "Average test loss: 0.002003738565577401\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02743961986237102\n",
      "Average test loss: 0.0018889081904457676\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027476099327206613\n",
      "Average test loss: 0.0018556310393744045\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027583565298053954\n",
      "Average test loss: 0.0018414057554263208\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027499434676435258\n",
      "Average test loss: 0.0019991159392520786\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02744651102523009\n",
      "Average test loss: 0.001867891333376368\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02744467782974243\n",
      "Average test loss: 0.001861894599472483\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027481594896978802\n",
      "Average test loss: 0.0018832806032150983\n",
      "Epoch 286/300\n",
      "Average training loss: 0.027473483443260192\n",
      "Average test loss: 0.0018720370783574052\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02803461433450381\n",
      "Average test loss: 0.0018192616254091264\n",
      "Epoch 288/300\n",
      "Average training loss: 0.027455800751845044\n",
      "Average test loss: 0.0018544850659867127\n",
      "Epoch 289/300\n",
      "Average training loss: 0.027390582753552332\n",
      "Average test loss: 0.0018676529495666424\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02731852923002508\n",
      "Average test loss: 0.0027016631609035864\n",
      "Epoch 291/300\n",
      "Average training loss: 0.027407928576072056\n",
      "Average test loss: 0.002111194547679689\n",
      "Epoch 292/300\n",
      "Average training loss: 0.027389447033405304\n",
      "Average test loss: 0.0021765604733179015\n",
      "Epoch 293/300\n",
      "Average training loss: 0.027414016190502377\n",
      "Average test loss: 0.003432065395741827\n",
      "Epoch 294/300\n",
      "Average training loss: 0.027406150966882706\n",
      "Average test loss: 0.0018608806969390974\n",
      "Epoch 295/300\n",
      "Average training loss: 0.027463523129622143\n",
      "Average test loss: 5.673710301823086\n",
      "Epoch 296/300\n",
      "Average training loss: 0.027533446002337668\n",
      "Average test loss: 0.0022918252800073887\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02729710800614622\n",
      "Average test loss: 0.0020931607067791952\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027432365329729185\n",
      "Average test loss: 0.00184848875241975\n",
      "Epoch 299/300\n",
      "Average training loss: 0.027871659280525315\n",
      "Average test loss: 0.0018588603008538485\n",
      "Epoch 300/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive-.025/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 23.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.08\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.75\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.81\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 17.035380304972332\n",
      "Average test loss: 0.013302623805900415\n",
      "Epoch 2/300\n",
      "Average training loss: 9.444275538974338\n",
      "Average test loss: 0.010775287286274964\n",
      "Epoch 3/300\n",
      "Average training loss: 7.104689140743679\n",
      "Average test loss: 0.009619573559198114\n",
      "Epoch 4/300\n",
      "Average training loss: 5.781412685818142\n",
      "Average test loss: 0.01020106866541836\n",
      "Epoch 5/300\n",
      "Average training loss: 5.07729153696696\n",
      "Average test loss: 0.008771260405580203\n",
      "Epoch 6/300\n",
      "Average training loss: 4.846248065524631\n",
      "Average test loss: 0.008358446300857598\n",
      "Epoch 7/300\n",
      "Average training loss: 4.311109864552816\n",
      "Average test loss: 0.016668826003869375\n",
      "Epoch 8/300\n",
      "Average training loss: 4.0164016473558215\n",
      "Average test loss: 0.008982638282908333\n",
      "Epoch 9/300\n",
      "Average training loss: 3.5138334291246203\n",
      "Average test loss: 0.008243970065895055\n",
      "Epoch 10/300\n",
      "Average training loss: 3.017673257827759\n",
      "Average test loss: 0.007665864098403189\n",
      "Epoch 11/300\n",
      "Average training loss: 2.7346647989485\n",
      "Average test loss: 0.007678837364332543\n",
      "Epoch 12/300\n",
      "Average training loss: 2.3574368940989174\n",
      "Average test loss: 0.007414908457547427\n",
      "Epoch 13/300\n",
      "Average training loss: 2.0543569934633044\n",
      "Average test loss: 0.0072321966050399675\n",
      "Epoch 14/300\n",
      "Average training loss: 1.8543931503295898\n",
      "Average test loss: 0.007012642352117433\n",
      "Epoch 15/300\n",
      "Average training loss: 1.2393142893049451\n",
      "Average test loss: 0.007963867300914394\n",
      "Epoch 17/300\n",
      "Average training loss: 1.065169509569804\n",
      "Average test loss: 0.0067171198121375505\n",
      "Epoch 18/300\n",
      "Average training loss: 0.7489571409225464\n",
      "Average test loss: 0.0063761189116372\n",
      "Epoch 21/300\n",
      "Average training loss: 0.6820368543730841\n",
      "Average test loss: 0.007537523248957263\n",
      "Epoch 22/300\n",
      "Average training loss: 0.62435117732154\n",
      "Average test loss: 0.006177741762664583\n",
      "Epoch 23/300\n",
      "Average training loss: 0.5748925660451253\n",
      "Average test loss: 0.012171123411920335\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5318061292436388\n",
      "Average test loss: 0.006281753587639994\n",
      "Epoch 25/300\n",
      "Average training loss: 0.48857528893152874\n",
      "Average test loss: 0.006081203265736501\n",
      "Epoch 26/300\n",
      "Average training loss: 0.44807549876636926\n",
      "Average test loss: 0.00643424512197574\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4095546009010739\n",
      "Average test loss: 0.006230828113853931\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3742663690249125\n",
      "Average test loss: 0.009015677614344492\n",
      "Epoch 29/300\n",
      "Average training loss: 0.3443718998961979\n",
      "Average test loss: 0.006263239719801479\n",
      "Epoch 30/300\n",
      "Average training loss: 0.3220308631791009\n",
      "Average test loss: 0.005923445264498393\n",
      "Epoch 31/300\n",
      "Average training loss: 0.30354134276178146\n",
      "Average test loss: 0.00693036722474628\n",
      "Epoch 32/300\n",
      "Average training loss: 0.27588805688752066\n",
      "Average test loss: 0.00586083432369762\n",
      "Epoch 34/300\n",
      "Average training loss: 0.265685414897071\n",
      "Average test loss: 0.3445716877381007\n",
      "Epoch 35/300\n",
      "Average training loss: 0.25489448040061524\n",
      "Average test loss: 0.0056801914100845655\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2473599945969052\n",
      "Average test loss: 0.005652945362445381\n",
      "Epoch 37/300\n",
      "Average training loss: 0.2438623349269231\n",
      "Average test loss: 0.006040027340667115\n",
      "Epoch 38/300\n",
      "Average training loss: 0.23521536799271903\n",
      "Average test loss: 0.0055958470693892905\n",
      "Epoch 39/300\n",
      "Average training loss: 0.229113645474116\n",
      "Average test loss: 0.007039223138656881\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2251200779808892\n",
      "Average test loss: 0.005571775583757294\n",
      "Epoch 41/300\n",
      "Average training loss: 0.2215039473109775\n",
      "Average test loss: 0.009404639162123203\n",
      "Epoch 42/300\n",
      "Average training loss: 0.21745992464489408\n",
      "Average test loss: 0.005627207776738538\n",
      "Epoch 43/300\n",
      "Average training loss: 0.21240757743517558\n",
      "Average test loss: 0.006467192218949398\n",
      "Epoch 44/300\n",
      "Average training loss: 0.20906748343838585\n",
      "Average test loss: 0.008669408697221014\n",
      "Epoch 45/300\n",
      "Average training loss: 0.20627256280846065\n",
      "Average test loss: 0.06937502357694837\n",
      "Epoch 46/300\n",
      "Average training loss: 0.2032893008126153\n",
      "Average test loss: 0.005493769784354501\n",
      "Epoch 47/300\n",
      "Average training loss: 0.19574188521173266\n",
      "Average test loss: 0.009960567016568448\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1948700054221683\n",
      "Average test loss: 0.006517335218273931\n",
      "Epoch 51/300\n",
      "Average training loss: 0.19202730707327526\n",
      "Average test loss: 0.0054416775496469606\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1899026775625017\n",
      "Average test loss: 0.006010233737942245\n",
      "Epoch 53/300\n",
      "Average training loss: 0.18934463102287716\n",
      "Average test loss: 0.029480927500459882\n",
      "Epoch 54/300\n",
      "Average training loss: 0.18751963996887208\n",
      "Average test loss: 0.0054889398896031906\n",
      "Epoch 55/300\n",
      "Average training loss: 0.18596532845497132\n",
      "Average test loss: 0.005876951082713074\n",
      "Epoch 56/300\n",
      "Average training loss: 0.18449196378389995\n",
      "Average test loss: 0.006299425112704436\n",
      "Epoch 57/300\n",
      "Average training loss: 0.183764900220765\n",
      "Average test loss: 0.005369225028488371\n",
      "Epoch 58/300\n",
      "Average training loss: 0.18215358997715844\n",
      "Average test loss: 0.006910058281487889\n",
      "Epoch 59/300\n",
      "Average training loss: 0.18111212355560727\n",
      "Average test loss: 0.005396752390182681\n",
      "Epoch 60/300\n",
      "Average training loss: 0.17959050042099423\n",
      "Average test loss: 0.005887212273147371\n",
      "Epoch 61/300\n",
      "Average training loss: 0.17856678366661072\n",
      "Average test loss: 0.005507049488110675\n",
      "Epoch 62/300\n",
      "Average training loss: 0.17677955329418182\n",
      "Average test loss: 4.481096417003208\n",
      "Epoch 64/300\n",
      "Average training loss: 0.18115693958600362\n",
      "Average test loss: 0.09102215386844344\n",
      "Epoch 65/300\n",
      "Average training loss: 0.17730518551667532\n",
      "Average test loss: 0.006358368796192938\n",
      "Epoch 66/300\n",
      "Average training loss: 0.17371433244811163\n",
      "Average test loss: 0.0056043561514880925\n",
      "Epoch 67/300\n",
      "Average training loss: 0.17227460363176134\n",
      "Average test loss: 0.006584433954623011\n",
      "Epoch 68/300\n",
      "Average training loss: 0.17257773399353027\n",
      "Average test loss: 0.0071730353521804015\n",
      "Epoch 69/300\n",
      "Average training loss: 0.17180273865328893\n",
      "Average test loss: 0.17293247429529826\n",
      "Epoch 70/300\n",
      "Average training loss: 0.17040903814633687\n",
      "Average test loss: 0.005555188058151139\n",
      "Epoch 71/300\n",
      "Average training loss: 0.17235674423641628\n",
      "Average test loss: 0.005447748565425475\n",
      "Epoch 72/300\n",
      "Average training loss: 0.1691606418026818\n",
      "Average test loss: 0.005503571893605921\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1750434951649772\n",
      "Average test loss: 0.005857151022387875\n",
      "Epoch 74/300\n",
      "Average training loss: 0.16908059016863505\n",
      "Average test loss: 0.0054563303765737346\n",
      "Epoch 75/300\n",
      "Average training loss: 0.16828383791446686\n",
      "Average test loss: 0.005356742793487178\n",
      "Epoch 76/300\n",
      "Average training loss: 0.16642243650886748\n",
      "Average test loss: 0.005988472520891163\n",
      "Epoch 78/300\n",
      "Average training loss: 0.16585404316584268\n",
      "Average test loss: 0.005939920743720399\n",
      "Epoch 79/300\n",
      "Average training loss: 0.16551402525107065\n",
      "Average test loss: 617.6762008586062\n",
      "Epoch 80/300\n",
      "Average training loss: 0.16744330542617375\n",
      "Average test loss: 0.0059896553305702075\n",
      "Epoch 81/300\n",
      "Average training loss: 0.16266980947388543\n",
      "Average test loss: 0.00778136886159579\n",
      "Epoch 84/300\n",
      "Average training loss: 0.16737063562870025\n",
      "Average test loss: 0.006326218297498094\n",
      "Epoch 85/300\n",
      "Average training loss: 0.161965741859542\n",
      "Average test loss: 0.00708913190331724\n",
      "Epoch 86/300\n",
      "Average training loss: 0.16168982615735797\n",
      "Average test loss: 0.005455041424267822\n",
      "Epoch 87/300\n",
      "Average training loss: 0.16114933019214206\n",
      "Average test loss: 0.005481583546226223\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1607225144704183\n",
      "Average test loss: 0.0059662564049164455\n",
      "Epoch 89/300\n",
      "Average training loss: 0.16113287315103741\n",
      "Average test loss: 0.005988136096547047\n",
      "Epoch 90/300\n",
      "Average training loss: 0.16005961505572\n",
      "Average test loss: 0.005451091499792205\n",
      "Epoch 91/300\n",
      "Average training loss: 0.16004028781255086\n",
      "Average test loss: 0.005830343215829796\n",
      "Epoch 92/300\n",
      "Average training loss: 0.15835567149851057\n",
      "Average test loss: 0.005972058083862066\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1582238175868988\n",
      "Average test loss: 0.00547803403602706\n",
      "Epoch 94/300\n",
      "Average training loss: 0.16054663062095642\n",
      "Average test loss: 0.00559771567169163\n",
      "Epoch 95/300\n",
      "Average training loss: 0.1567296136021614\n",
      "Average test loss: 0.007474191747605801\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1569562460449007\n",
      "Average test loss: 0.014602586076077487\n",
      "Epoch 97/300\n",
      "Average training loss: 0.156565357181761\n",
      "Average test loss: 0.005643362991097901\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1548122903108597\n",
      "Average test loss: 0.005988180117474662\n",
      "Epoch 100/300\n",
      "Average training loss: 0.15718561850653753\n",
      "Average test loss: 0.005519780895155337\n",
      "Epoch 101/300\n",
      "Average training loss: 1560530.9804010205\n",
      "Average test loss: 5.783332727657424\n",
      "Epoch 102/300\n",
      "Average training loss: 21.051373691134984\n",
      "Average test loss: 0.03899913307362132\n",
      "Epoch 103/300\n",
      "Average training loss: 18.427665773179797\n",
      "Average test loss: 0.4199546053525474\n",
      "Epoch 104/300\n",
      "Average training loss: 16.72153923288981\n",
      "Average test loss: 0.9747978653510412\n",
      "Epoch 105/300\n",
      "Average training loss: 15.356213994344076\n",
      "Average test loss: 0.017689066670007175\n",
      "Epoch 106/300\n",
      "Average training loss: 14.236059861924913\n",
      "Average test loss: 0.03903580685456594\n",
      "Epoch 107/300\n",
      "Average training loss: 13.209157374911838\n",
      "Average test loss: 2.0320388213594756\n",
      "Epoch 108/300\n",
      "Average training loss: 12.35797817993164\n",
      "Average test loss: 3.224041827445229\n",
      "Epoch 109/300\n",
      "Average training loss: 11.584666701422798\n",
      "Average test loss: 8.527006723456912\n",
      "Epoch 110/300\n",
      "Average training loss: 10.88769812435574\n",
      "Average test loss: 0.02210639932917224\n",
      "Epoch 111/300\n",
      "Average training loss: 10.244918492635092\n",
      "Average test loss: 0.031478729494743875\n",
      "Epoch 112/300\n",
      "Average training loss: 9.670450881958008\n",
      "Average test loss: 0.07335156323181258\n",
      "Epoch 113/300\n",
      "Average training loss: 9.155909055074057\n",
      "Average test loss: 2.0982679935031467\n",
      "Epoch 114/300\n",
      "Average training loss: 8.64776554192437\n",
      "Average test loss: 0.29326286329991286\n",
      "Epoch 115/300\n",
      "Average training loss: 8.105275262196859\n",
      "Average test loss: 0.029881408203807143\n",
      "Epoch 116/300\n",
      "Average training loss: 7.599221521165636\n",
      "Average test loss: 0.9339345945434437\n",
      "Epoch 117/300\n",
      "Average training loss: 7.177027019500732\n",
      "Average test loss: 0.1336774766271313\n",
      "Epoch 118/300\n",
      "Average training loss: 6.797694716559516\n",
      "Average test loss: 3.895234960049391\n",
      "Epoch 119/300\n",
      "Average training loss: 6.4364275538126625\n",
      "Average test loss: 0.015182535137981176\n",
      "Epoch 120/300\n",
      "Average training loss: 6.076048698425293\n",
      "Average test loss: 0.007445700616886219\n",
      "Epoch 121/300\n",
      "Average training loss: 5.719748613145616\n",
      "Average test loss: 0.08330525313814481\n",
      "Epoch 122/300\n",
      "Average training loss: 5.338568419986301\n",
      "Average test loss: 0.008288212810125616\n",
      "Epoch 123/300\n",
      "Average training loss: 4.953610850863987\n",
      "Average test loss: 0.008376876135667165\n",
      "Epoch 124/300\n",
      "Average training loss: 4.552415399763319\n",
      "Average test loss: 0.5826177683017321\n",
      "Epoch 125/300\n",
      "Average training loss: 4.167950719833374\n",
      "Average test loss: 0.006720672522981961\n",
      "Epoch 126/300\n",
      "Average training loss: 3.8103406624264187\n",
      "Average test loss: 0.059556485320130984\n",
      "Epoch 127/300\n",
      "Average training loss: 3.50352055782742\n",
      "Average test loss: 0.007631484702229499\n",
      "Epoch 128/300\n",
      "Average training loss: 3.2503317489624024\n",
      "Average test loss: 0.015399006770716773\n",
      "Epoch 129/300\n",
      "Average training loss: 3.023059203677707\n",
      "Average test loss: 0.015412099523676765\n",
      "Epoch 130/300\n",
      "Average training loss: 2.815555252710978\n",
      "Average test loss: 0.011162709658344587\n",
      "Epoch 131/300\n",
      "Average training loss: 2.594230981190999\n",
      "Average test loss: 0.012354978614383274\n",
      "Epoch 132/300\n",
      "Average training loss: 2.391439472410414\n",
      "Average test loss: 0.10146496077958081\n",
      "Epoch 133/300\n",
      "Average training loss: 2.2021352236005995\n",
      "Average test loss: 0.005916723783231444\n",
      "Epoch 134/300\n",
      "Average training loss: 2.0336565763685437\n",
      "Average test loss: 0.006448633955584632\n",
      "Epoch 135/300\n",
      "Average training loss: 1.8781185103522406\n",
      "Average test loss: 0.006379033601946301\n",
      "Epoch 136/300\n",
      "Average training loss: 1.7244988567564223\n",
      "Average test loss: 0.005793381214555767\n",
      "Epoch 137/300\n",
      "Average training loss: 1.574872165573968\n",
      "Average test loss: 0.005869281860689322\n",
      "Epoch 138/300\n",
      "Average training loss: 1.4258773747550118\n",
      "Average test loss: 0.005898939948529005\n",
      "Epoch 139/300\n",
      "Average training loss: 1.2789267620510525\n",
      "Average test loss: 0.005742076848944028\n",
      "Epoch 140/300\n",
      "Average training loss: 1.138021118481954\n",
      "Average test loss: 0.005693755281882154\n",
      "Epoch 141/300\n",
      "Average training loss: 1.0031404207547505\n",
      "Average test loss: 0.009949499375290341\n",
      "Epoch 142/300\n",
      "Average training loss: 0.8820665583610535\n",
      "Average test loss: 0.00561567246582773\n",
      "Epoch 143/300\n",
      "Average training loss: 0.7726924689080981\n",
      "Average test loss: 0.00569413576564855\n",
      "Epoch 144/300\n",
      "Average training loss: 0.6793615836567349\n",
      "Average test loss: 0.00597992219361994\n",
      "Epoch 145/300\n",
      "Average training loss: 0.5971104808913337\n",
      "Average test loss: 0.006576855392091804\n",
      "Epoch 146/300\n",
      "Average training loss: 0.5251954387558831\n",
      "Average test loss: 0.005710589472618368\n",
      "Epoch 147/300\n",
      "Average training loss: 0.46007990482118394\n",
      "Average test loss: 0.005551343853481942\n",
      "Epoch 148/300\n",
      "Average training loss: 0.4060509597195519\n",
      "Average test loss: 0.007241889022290706\n",
      "Epoch 149/300\n",
      "Average training loss: 0.3594186897277832\n",
      "Average test loss: 0.005779120352533128\n",
      "Epoch 150/300\n",
      "Average training loss: 0.3201570467684004\n",
      "Average test loss: 0.006046017191890213\n",
      "Epoch 151/300\n",
      "Average training loss: 0.2879203913344277\n",
      "Average test loss: 0.005703425697154469\n",
      "Epoch 152/300\n",
      "Average training loss: 0.26291757297515866\n",
      "Average test loss: 0.006354636492828528\n",
      "Epoch 153/300\n",
      "Average training loss: 0.24255600232548183\n",
      "Average test loss: 0.00542259040599068\n",
      "Epoch 154/300\n",
      "Average training loss: 0.23028655918439228\n",
      "Average test loss: 0.005424875476708015\n",
      "Epoch 155/300\n",
      "Average training loss: 0.21985889723565843\n",
      "Average test loss: 0.005733857504195637\n",
      "Epoch 156/300\n",
      "Average training loss: 0.2129204542371962\n",
      "Average test loss: 0.0053679716346992385\n",
      "Epoch 157/300\n",
      "Average training loss: 0.2078858967622121\n",
      "Average test loss: 0.005556737197770013\n",
      "Epoch 158/300\n",
      "Average training loss: 0.20228273114893172\n",
      "Average test loss: 0.005401796429521507\n",
      "Epoch 159/300\n",
      "Average training loss: 0.19967586370309193\n",
      "Average test loss: 0.005412904690537188\n",
      "Epoch 160/300\n",
      "Average training loss: 0.19438496551248763\n",
      "Average test loss: 0.00598762339208689\n",
      "Epoch 161/300\n",
      "Average training loss: 0.1916222976313697\n",
      "Average test loss: 0.00620091604689757\n",
      "Epoch 162/300\n",
      "Average training loss: 0.18794231788317362\n",
      "Average test loss: 0.005803478130863773\n",
      "Epoch 163/300\n",
      "Average training loss: 0.18949723015891182\n",
      "Average test loss: 0.005391337397197883\n",
      "Epoch 164/300\n",
      "Average training loss: 0.18969495248794555\n",
      "Average test loss: 0.005364053181476063\n",
      "Epoch 165/300\n",
      "Average training loss: 0.18190106451511384\n",
      "Average test loss: 0.0055220446574191255\n",
      "Epoch 166/300\n",
      "Average training loss: 0.17999384869469537\n",
      "Average test loss: 0.00545895843497581\n",
      "Epoch 167/300\n",
      "Average training loss: 0.17757377084096274\n",
      "Average test loss: 0.005440580881511172\n",
      "Epoch 168/300\n",
      "Average training loss: 0.17769349994924333\n",
      "Average test loss: 0.005507829380946027\n",
      "Epoch 169/300\n",
      "Average training loss: 0.17403986292415194\n",
      "Average test loss: 0.006094338166837891\n",
      "Epoch 170/300\n",
      "Average training loss: 0.17253598674138387\n",
      "Average test loss: 0.005714350821243392\n",
      "Epoch 171/300\n",
      "Average training loss: 0.17115623422463735\n",
      "Average test loss: 0.0054080845717754625\n",
      "Epoch 172/300\n",
      "Average training loss: 0.16959388744831086\n",
      "Average test loss: 0.005537066682759258\n",
      "Epoch 173/300\n",
      "Average training loss: 0.16857408985826705\n",
      "Average test loss: 0.006680056316571103\n",
      "Epoch 174/300\n",
      "Average training loss: 0.16712926020887162\n",
      "Average test loss: 0.03898054857386483\n",
      "Epoch 175/300\n",
      "Average training loss: 0.3011790597703722\n",
      "Average test loss: 0.005598608855572011\n",
      "Epoch 176/300\n",
      "Average training loss: 0.20132371412383185\n",
      "Average test loss: 0.0056216291483077736\n",
      "Epoch 177/300\n",
      "Average training loss: 0.18569458399878608\n",
      "Average test loss: 0.005428951916181379\n",
      "Epoch 178/300\n",
      "Average training loss: 0.17821417175398932\n",
      "Average test loss: 0.005445585761633185\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1728887170155843\n",
      "Average test loss: 0.005372836680048042\n",
      "Epoch 180/300\n",
      "Average training loss: 0.16946458617846172\n",
      "Average test loss: 0.005482025492108531\n",
      "Epoch 181/300\n",
      "Average test loss: 0.005372859045863152\n",
      "Epoch 182/300\n",
      "Average training loss: 0.16716154451502693\n",
      "Average test loss: 0.008571551473604309\n",
      "Epoch 183/300\n",
      "Average training loss: 0.16358617319001093\n",
      "Average test loss: 0.006535246946331528\n",
      "Epoch 184/300\n",
      "Average training loss: 0.1633582284450531\n",
      "Average test loss: 0.005489541383253203\n",
      "Epoch 185/300\n",
      "Average training loss: 0.16269813876681857\n",
      "Average test loss: 0.00558640091949039\n",
      "Epoch 186/300\n",
      "Average training loss: 0.1606552634636561\n",
      "Average test loss: 0.005748204522248772\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1602276403374142\n",
      "Average test loss: 0.005449118683321609\n",
      "Epoch 188/300\n",
      "Average training loss: 0.15948347861236997\n",
      "Average test loss: 0.005573643039291104\n",
      "Epoch 189/300\n",
      "Average training loss: 0.1589245464404424\n",
      "Average test loss: 0.16961397839917078\n",
      "Epoch 190/300\n",
      "Average training loss: 0.1577731926838557\n",
      "Average test loss: 0.005410035664422644\n",
      "Epoch 191/300\n",
      "Average training loss: 0.15653114081753625\n",
      "Average test loss: 0.006036935761984851\n",
      "Epoch 193/300\n",
      "Average training loss: 0.15580291992425918\n",
      "Average test loss: 0.0056015582676562994\n",
      "Epoch 194/300\n",
      "Average training loss: 0.15581225775347815\n",
      "Average test loss: 0.0056293293837871815\n",
      "Epoch 195/300\n",
      "Average training loss: 0.15471105864312915\n",
      "Average test loss: 0.005569755984677209\n",
      "Epoch 196/300\n",
      "Average training loss: 0.15308009705278608\n",
      "Average test loss: 0.005601776812639501\n",
      "Epoch 199/300\n",
      "Average training loss: 0.1521835593779882\n",
      "Average test loss: 0.0063202592022717\n",
      "Epoch 200/300\n",
      "Average training loss: 0.15233873040146298\n",
      "Average test loss: 0.006543237273891767\n",
      "Epoch 201/300\n",
      "Average training loss: 0.1516441641383701\n",
      "Average test loss: 0.00560472032138043\n",
      "Epoch 202/300\n",
      "Average training loss: 0.15204562493165333\n",
      "Average test loss: 0.006095066172381242\n",
      "Epoch 203/300\n",
      "Average training loss: 0.1507490243514379\n",
      "Average test loss: 0.005471233353225721\n",
      "Epoch 204/300\n",
      "Average training loss: 0.15035662665632035\n",
      "Average test loss: 0.005734589535329077\n",
      "Epoch 205/300\n",
      "Average training loss: 0.1498620377116733\n",
      "Average test loss: 0.006337739802483055\n",
      "Epoch 206/300\n",
      "Average training loss: 0.15132698888248866\n",
      "Average test loss: 0.005604382639129956\n",
      "Epoch 207/300\n",
      "Average training loss: 0.1487992986705568\n",
      "Average test loss: 0.005699421300656266\n",
      "Epoch 208/300\n",
      "Average training loss: 0.14907543228069942\n",
      "Average test loss: 0.005692919608619478\n",
      "Epoch 209/300\n",
      "Average training loss: 0.14841252464056015\n",
      "Average test loss: 0.04491750439339214\n",
      "Epoch 211/300\n",
      "Average training loss: 0.1517912488910887\n",
      "Average test loss: 62.9324745610555\n",
      "Epoch 212/300\n",
      "Average training loss: 234.95720375661054\n",
      "Average test loss: 2286.683308103694\n",
      "Epoch 213/300\n",
      "Average training loss: 8.579387263403998\n",
      "Average test loss: 42.280263611868854\n",
      "Epoch 214/300\n",
      "Average training loss: 6.195657761891683\n",
      "Average test loss: 7744.400787640389\n",
      "Epoch 215/300\n",
      "Average training loss: 5.0079191415574815\n",
      "Average test loss: 39.452116457998756\n",
      "Epoch 216/300\n",
      "Average training loss: 4.342275433434381\n",
      "Average test loss: 2541.37333843316\n",
      "Epoch 217/300\n",
      "Average training loss: 3.909726660410563\n",
      "Average test loss: 30718.790992755705\n",
      "Epoch 218/300\n",
      "Average training loss: 3.5807634018792047\n",
      "Average test loss: 43790.31834724541\n",
      "Epoch 219/300\n",
      "Average training loss: 3.2567524856991237\n",
      "Average test loss: 0.018619647807131212\n",
      "Epoch 220/300\n",
      "Average training loss: 2.9751280057695175\n",
      "Average test loss: 0.007259469614674647\n",
      "Epoch 221/300\n",
      "Average training loss: 2.7101016720665827\n",
      "Average test loss: 0.007914113994687796\n",
      "Epoch 222/300\n",
      "Average training loss: 2.458560035281711\n",
      "Average test loss: 476.2042050231596\n",
      "Epoch 223/300\n",
      "Average training loss: 2.1902360168033175\n",
      "Average test loss: 11.486397053665584\n",
      "Epoch 224/300\n",
      "Average training loss: 1.9202094220055475\n",
      "Average test loss: 0.0318742408686214\n",
      "Epoch 225/300\n",
      "Average training loss: 1.6823525288899739\n",
      "Average test loss: 0.025991732590728334\n",
      "Epoch 226/300\n",
      "Average training loss: 1.4828634382883707\n",
      "Average test loss: 0.29054426376024883\n",
      "Epoch 227/300\n",
      "Average training loss: 1.311139099544949\n",
      "Average test loss: 0.006233157441847854\n",
      "Epoch 228/300\n",
      "Average training loss: 1.156987914191352\n",
      "Average test loss: 0.009723364450865322\n",
      "Epoch 229/300\n",
      "Average training loss: 1.0196128330230714\n",
      "Average test loss: 1.8934676560560861\n",
      "Epoch 230/300\n",
      "Average training loss: 0.8955731335745918\n",
      "Average test loss: 0.0061330046335028275\n",
      "Epoch 231/300\n",
      "Average training loss: 0.7814671682781643\n",
      "Average test loss: 0.006206854811973042\n",
      "Epoch 232/300\n",
      "Average training loss: 0.6798234248691135\n",
      "Average test loss: 0.006453099425882101\n",
      "Epoch 233/300\n",
      "Average training loss: 0.5912004138628641\n",
      "Average test loss: 0.005782015394833353\n",
      "Epoch 234/300\n",
      "Average training loss: 0.5183937176863352\n",
      "Average test loss: 0.006018949426296685\n",
      "Epoch 235/300\n",
      "Average training loss: 0.44709664358033074\n",
      "Average test loss: 0.005828725882702403\n",
      "Epoch 236/300\n",
      "Average training loss: 0.39355425985654197\n",
      "Average test loss: 0.005829461475834251\n",
      "Epoch 237/300\n",
      "Average training loss: 0.34978172794977824\n",
      "Average test loss: 0.005602448055313693\n",
      "Epoch 238/300\n",
      "Average training loss: 0.31478848528861997\n",
      "Average test loss: 0.006693021357059479\n",
      "Epoch 239/300\n",
      "Average training loss: 0.2903857877254486\n",
      "Average test loss: 0.006737180173397064\n",
      "Epoch 240/300\n",
      "Average training loss: 0.26641813537809583\n",
      "Average test loss: 0.005679320077101389\n",
      "Epoch 241/300\n",
      "Average training loss: 0.24694862874348958\n",
      "Average test loss: 0.006181302978760666\n",
      "Epoch 242/300\n",
      "Average training loss: 0.2330853363143073\n",
      "Average test loss: 0.005507215419577227\n",
      "Epoch 243/300\n",
      "Average training loss: 0.2202917567094167\n",
      "Average test loss: 0.005702662590477202\n",
      "Epoch 244/300\n",
      "Average training loss: 0.2102064032819536\n",
      "Average test loss: 0.005461371754192644\n",
      "Epoch 245/300\n",
      "Average training loss: 0.20180449748039245\n",
      "Average test loss: 0.15867586732241842\n",
      "Epoch 246/300\n",
      "Average training loss: 0.19420087920294868\n",
      "Average test loss: 0.0058525257594883445\n",
      "Epoch 247/300\n",
      "Average training loss: 0.2059053556786643\n",
      "Average test loss: 0.005534983670132027\n",
      "Epoch 248/300\n",
      "Average training loss: 0.1851901356379191\n",
      "Average test loss: 0.0057297770695553885\n",
      "Epoch 249/300\n",
      "Average training loss: 0.17975506956047482\n",
      "Average test loss: 0.005720352163745297\n",
      "Epoch 250/300\n",
      "Average training loss: 0.1759394120640225\n",
      "Average test loss: 0.005474135874874062\n",
      "Epoch 251/300\n",
      "Average training loss: 0.17254470823870766\n",
      "Average test loss: 0.005486426750404967\n",
      "Epoch 252/300\n",
      "Average training loss: 0.17018390390608046\n",
      "Average test loss: 0.005582831334322691\n",
      "Epoch 253/300\n",
      "Average training loss: 0.16630872005886502\n",
      "Average test loss: 0.006060684309237533\n",
      "Epoch 254/300\n",
      "Average training loss: 0.16358390338553322\n",
      "Average test loss: 0.00567449905930294\n",
      "Epoch 255/300\n",
      "Average training loss: 0.16055186552471584\n",
      "Average test loss: 0.006201359436329868\n",
      "Epoch 256/300\n",
      "Average training loss: 0.15858213529321882\n",
      "Average test loss: 0.00739858817971415\n",
      "Epoch 257/300\n",
      "Average training loss: 0.15570892186959584\n",
      "Average test loss: 0.005533737651382884\n",
      "Epoch 258/300\n",
      "Average training loss: 0.1584364063474867\n",
      "Average test loss: 0.005571662861853838\n",
      "Epoch 259/300\n",
      "Average training loss: 0.15289273479249743\n",
      "Average test loss: 459.9162066107856\n",
      "Epoch 260/300\n",
      "Average training loss: 21113.235346677568\n",
      "Average test loss: 11411830.942756398\n",
      "Epoch 261/300\n",
      "Average training loss: 10.514557006835938\n",
      "Average test loss: 62048753153.151184\n",
      "Epoch 262/300\n",
      "Average training loss: 9.419491183810765\n",
      "Average test loss: 14474767.858591551\n",
      "Epoch 263/300\n",
      "Average training loss: 8.618236171298557\n",
      "Average test loss: 5040353.35933162\n",
      "Epoch 264/300\n",
      "Average training loss: 7.950041593339708\n",
      "Average test loss: 474921.06685804634\n",
      "Epoch 265/300\n",
      "Average training loss: 7.350641135321723\n",
      "Average test loss: 1252851.778876932\n",
      "Epoch 266/300\n",
      "Average training loss: 6.7881009470621745\n",
      "Average test loss: 1807.1423920024831\n",
      "Epoch 267/300\n",
      "Average training loss: 6.27571220948961\n",
      "Average test loss: 182806969768.68518\n",
      "Epoch 268/300\n",
      "Average training loss: 5.799872147878011\n",
      "Average test loss: 578.7498891040951\n",
      "Epoch 269/300\n",
      "Average training loss: 5.361104085286458\n",
      "Average test loss: 55354.73753645755\n",
      "Epoch 270/300\n",
      "Average training loss: 4.9517043728298615\n",
      "Average test loss: 2.0471467754149604\n",
      "Epoch 271/300\n",
      "Average training loss: 4.588194865332709\n",
      "Average test loss: 1864.094002622386\n",
      "Epoch 272/300\n",
      "Average training loss: 4.2657076534695095\n",
      "Average test loss: 5.499761410595642\n",
      "Epoch 273/300\n",
      "Average training loss: 3.9734385325113935\n",
      "Average test loss: 0.008762522109266785\n",
      "Epoch 274/300\n",
      "Average training loss: 3.710810483932495\n",
      "Average test loss: 0.008570984635088178\n",
      "Epoch 275/300\n",
      "Average training loss: 3.463147503958808\n",
      "Average test loss: 0.00670883538077275\n",
      "Epoch 276/300\n",
      "Average training loss: 3.221479548772176\n",
      "Average test loss: 0.02571463419497013\n",
      "Epoch 277/300\n",
      "Average training loss: 2.9831173735724557\n",
      "Average test loss: 0.006609698046826654\n",
      "Epoch 278/300\n",
      "Average training loss: 2.7485161226060657\n",
      "Average test loss: 0.2763850798706214\n",
      "Epoch 279/300\n",
      "Average training loss: 2.53392623647054\n",
      "Average test loss: 0.011391651244627104\n",
      "Epoch 280/300\n",
      "Average training loss: 2.3333310108184815\n",
      "Average test loss: 1170.4769173855252\n",
      "Epoch 281/300\n",
      "Average training loss: 2.14790907435947\n",
      "Average test loss: 4053.550988305092\n",
      "Epoch 282/300\n",
      "Average training loss: 1.973383144378662\n",
      "Average test loss: 0.07385900319450431\n",
      "Epoch 283/300\n",
      "Average training loss: 1.8042777483198378\n",
      "Average test loss: 0.00710158908739686\n",
      "Epoch 284/300\n",
      "Average training loss: 1.6367168460422092\n",
      "Average test loss: 0.006907097691877021\n",
      "Epoch 285/300\n",
      "Average training loss: 1.4701724026997884\n",
      "Average test loss: 0.0058764757158027755\n",
      "Epoch 286/300\n",
      "Average training loss: 1.3008493394851686\n",
      "Average test loss: 0.005977850250899792\n",
      "Epoch 287/300\n",
      "Average training loss: 1.1433931708865694\n",
      "Average test loss: 0.005736256180538072\n",
      "Epoch 288/300\n",
      "Average training loss: 1.000842959298028\n",
      "Average test loss: 0.005671573453065422\n",
      "Epoch 289/300\n",
      "Average training loss: 0.876995939095815\n",
      "Average test loss: 0.0056802010188500085\n",
      "Epoch 290/300\n",
      "Average training loss: 0.7677910625669692\n",
      "Average test loss: 0.005735592283308506\n",
      "Epoch 291/300\n",
      "Average training loss: 0.6689998715188769\n",
      "Average test loss: 0.7299370419979095\n",
      "Epoch 292/300\n",
      "Average training loss: 0.6637348418765597\n",
      "Average test loss: 0.005583022702071402\n",
      "Epoch 293/300\n",
      "Average training loss: 0.5223677286571926\n",
      "Average test loss: 0.005573017168376181\n",
      "Epoch 294/300\n",
      "Average training loss: 0.4591719532807668\n",
      "Average test loss: 0.005593794676578707\n",
      "Epoch 295/300\n",
      "Average training loss: 0.39934500651889376\n",
      "Average test loss: 0.005508570003426737\n",
      "Epoch 296/300\n",
      "Average training loss: 0.3484107444021437\n",
      "Average test loss: 0.005440055076032877\n",
      "Epoch 297/300\n",
      "Average training loss: 0.30749688421355353\n",
      "Average test loss: 0.008292901683184835\n",
      "Epoch 298/300\n",
      "Average training loss: 0.2932227518558502\n",
      "Average test loss: 0.1494820305109024\n",
      "Epoch 299/300\n",
      "Average training loss: 0.25431520819664\n",
      "Average test loss: 0.005583704469518529\n",
      "Epoch 300/300\n",
      "Average training loss: 0.23173261878225537\n",
      "Average test loss: 0.0054207815271284845\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 17.629112972683377\n",
      "Average test loss: 6.747447231868903\n",
      "Epoch 2/300\n",
      "Average training loss: 7.7364539900885685\n",
      "Average test loss: 0.007111577894538641\n",
      "Epoch 3/300\n",
      "Average training loss: 5.6664024963378905\n",
      "Average test loss: 0.0061335284271174005\n",
      "Epoch 4/300\n",
      "Average training loss: 4.647433094448513\n",
      "Average test loss: 0.008017735866622792\n",
      "Epoch 5/300\n",
      "Average training loss: 3.865166063944499\n",
      "Average test loss: 0.005512644061197837\n",
      "Epoch 6/300\n",
      "Average training loss: 3.466247536129422\n",
      "Average test loss: 0.005522717478788562\n",
      "Epoch 7/300\n",
      "Average training loss: 2.8428843089209663\n",
      "Average test loss: 0.005446600648678012\n",
      "Epoch 8/300\n",
      "Average training loss: 2.958855417675442\n",
      "Average test loss: 0.005225825964369707\n",
      "Epoch 9/300\n",
      "Average training loss: 2.367125755098131\n",
      "Average test loss: 0.004902194858839115\n",
      "Epoch 10/300\n",
      "Average training loss: 2.0556195522944134\n",
      "Average test loss: 0.004673812424970998\n",
      "Epoch 11/300\n",
      "Average training loss: 1.7209009550942316\n",
      "Average test loss: 0.0046088429610762335\n",
      "Epoch 12/300\n",
      "Average training loss: 1.5225469428168403\n",
      "Average test loss: 0.00452293443472849\n",
      "Epoch 13/300\n",
      "Average training loss: 1.318817396375868\n",
      "Average test loss: 0.004416205087469684\n",
      "Epoch 14/300\n",
      "Average training loss: 1.1096668367385865\n",
      "Average test loss: 0.0042583974179708295\n",
      "Epoch 15/300\n",
      "Average training loss: 0.9275829245249431\n",
      "Average test loss: 0.004178467168576188\n",
      "Epoch 16/300\n",
      "Average training loss: 0.7877231955528259\n",
      "Average test loss: 0.0050487753483984205\n",
      "Epoch 17/300\n",
      "Average training loss: 0.671412483215332\n",
      "Average test loss: 0.004074430444174343\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5800568503273857\n",
      "Average test loss: 0.0038994915820658206\n",
      "Epoch 19/300\n",
      "Average training loss: 0.5077634913126627\n",
      "Average test loss: 0.004523565100712909\n",
      "Epoch 20/300\n",
      "Average training loss: 0.45139521188206144\n",
      "Average test loss: 0.0038931821851680678\n",
      "Epoch 21/300\n",
      "Average training loss: 0.40593966219160293\n",
      "Average test loss: 0.003994610795130332\n",
      "Epoch 22/300\n",
      "Average training loss: 0.36750888146294486\n",
      "Average test loss: 0.0036922333509557775\n",
      "Epoch 23/300\n",
      "Average training loss: 0.33217595171928405\n",
      "Average test loss: 0.004236118999620279\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3045997885598077\n",
      "Average test loss: 0.0036329145421170527\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2792827132542928\n",
      "Average test loss: 0.0036158871681739885\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2574860122468736\n",
      "Average test loss: 0.0036780558770729437\n",
      "Epoch 27/300\n",
      "Average training loss: 0.23870283325513203\n",
      "Average test loss: 0.004617289603170422\n",
      "Epoch 28/300\n",
      "Average training loss: 0.22389762004216512\n",
      "Average test loss: 0.003587165449021591\n",
      "Epoch 29/300\n",
      "Average training loss: 0.21010264046986898\n",
      "Average test loss: 0.003535384031219615\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1988115757703781\n",
      "Average test loss: 0.004119635947462585\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1909558691183726\n",
      "Average test loss: 0.004077484779473808\n",
      "Epoch 32/300\n",
      "Average training loss: 0.18148360753721662\n",
      "Average test loss: 0.0034776200225783717\n",
      "Epoch 33/300\n",
      "Average training loss: 0.17432784841458002\n",
      "Average test loss: 0.003545183296004931\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1686738114754359\n",
      "Average test loss: 0.0034459077529609204\n",
      "Epoch 35/300\n",
      "Average training loss: 0.16288872034019894\n",
      "Average test loss: 0.0035220705345273017\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15820802468723721\n",
      "Average test loss: 0.0034460709819363225\n",
      "Epoch 37/300\n",
      "Average training loss: 0.15369444525241852\n",
      "Average test loss: 0.0034183288032395972\n",
      "Epoch 38/300\n",
      "Average training loss: 0.15029880323674943\n",
      "Average test loss: 0.0034785529799345465\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1450120862589942\n",
      "Average test loss: 0.0033782895190848246\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1427747165295813\n",
      "Average test loss: 0.003410047575003571\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1378833586970965\n",
      "Average test loss: 0.003534202853838603\n",
      "Epoch 42/300\n",
      "Average training loss: 0.13266145251194636\n",
      "Average test loss: 0.4839695832596885\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1297762183878157\n",
      "Average test loss: 0.0034978454110936984\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1276757935219341\n",
      "Average test loss: 0.003408306765266591\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1269827044275072\n",
      "Average test loss: 0.00493887917449077\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12315996458133062\n",
      "Average test loss: 0.0034753283901760973\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12243887942367129\n",
      "Average test loss: 0.0034038976530234018\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12011927790111966\n",
      "Average test loss: 0.003282512051777707\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11916599218050639\n",
      "Average test loss: 0.004205624011862609\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11822896452744802\n",
      "Average test loss: 0.0035731895168622336\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11501628980371686\n",
      "Average test loss: 0.0034708789587020875\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11345772322018942\n",
      "Average test loss: 0.0032868252268268\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11257244241899914\n",
      "Average test loss: 0.010136653619507949\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11211067071225908\n",
      "Average test loss: 0.003269525856193569\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11199494054582385\n",
      "Average test loss: 0.003257090407113234\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1101106477909618\n",
      "Average test loss: 0.0034876984734502105\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11001809728145599\n",
      "Average test loss: 0.0032805946028480926\n",
      "Epoch 61/300\n",
      "Average training loss: 0.2536930995848444\n",
      "Average test loss: 0.003566429910560449\n",
      "Epoch 62/300\n",
      "Average training loss: 0.14658889086378946\n",
      "Average test loss: 0.0034004349162181217\n",
      "Epoch 63/300\n",
      "Average training loss: 0.13259318606058756\n",
      "Average test loss: 0.003325343943511446\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12629054698679182\n",
      "Average test loss: 0.0033026659509374037\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12223157901896371\n",
      "Average test loss: 0.0032616943863944875\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11978022538291083\n",
      "Average test loss: 0.017182906008429\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11737805299626457\n",
      "Average test loss: 0.0032557251329223317\n",
      "Epoch 68/300\n",
      "Average training loss: 0.144270520971881\n",
      "Average test loss: 0.040211159686247507\n",
      "Epoch 69/300\n",
      "Average training loss: 0.17286906209256914\n",
      "Average test loss: 0.0034571438481410346\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12433923478921255\n",
      "Average test loss: 0.0033668130325774353\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11942006319099002\n",
      "Average test loss: 0.003308620762804316\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11439867791202334\n",
      "Average test loss: 0.003276749808341265\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11321961130036248\n",
      "Average test loss: 0.003251575316302478\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11194662302070194\n",
      "Average test loss: 0.0032694342558582624\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11166772939099207\n",
      "Average test loss: 0.0032934271268960503\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11003329081005521\n",
      "Average test loss: 0.003279752662405372\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10934161684248182\n",
      "Average test loss: 0.0032730343902690542\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10879482134845522\n",
      "Average test loss: 46.07784001498752\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10918242413798968\n",
      "Average test loss: 0.004045071999542415\n",
      "Epoch 81/300\n",
      "Average test loss: 0.0032088937386870386\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10670966766940222\n",
      "Average test loss: 0.003741573746833536\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10589244481590059\n",
      "Average test loss: 0.0032554676758332385\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10571480266915427\n",
      "Average test loss: 0.0034121127074791327\n",
      "Epoch 86/300\n",
      "Average training loss: 0.105125006655852\n",
      "Average test loss: 0.003632608358437816\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10490722264846165\n",
      "Average test loss: 0.003239252711335818\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10453069222635693\n",
      "Average test loss: 0.0032058342904266383\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10355881568458346\n",
      "Average test loss: 0.0032723181320147383\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10332292175292969\n",
      "Average test loss: 0.0042439589794311255\n",
      "Epoch 91/300\n",
      "Average training loss: 0.1032313790983624\n",
      "Average test loss: 0.003358485626677672\n",
      "Epoch 92/300\n",
      "Average training loss: 1.0245521303812664\n",
      "Average test loss: 19652207.535220854\n",
      "Epoch 93/300\n",
      "Average training loss: 0.26107695611317955\n",
      "Average test loss: 0.04125205075285501\n",
      "Epoch 96/300\n",
      "Average training loss: 0.21970169207784865\n",
      "Average test loss: 0.005012050065729353\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1940978581243091\n",
      "Average test loss: 0.006066633569697539\n",
      "Epoch 98/300\n",
      "Average training loss: 0.17682579159736633\n",
      "Average test loss: 1.5941154108378621\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1619968739218182\n",
      "Average test loss: 0.0033836108355058563\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1474531418747372\n",
      "Average test loss: 0.0033257456661926377\n",
      "Epoch 101/300\n",
      "Average training loss: 0.13854338624742296\n",
      "Average test loss: 0.0035914804765747655\n",
      "Epoch 102/300\n",
      "Average training loss: 0.1327822782728407\n",
      "Average test loss: 0.0035513200569483967\n",
      "Epoch 103/300\n",
      "Average training loss: 0.12843470525079304\n",
      "Average test loss: 0.0032595034084386297\n",
      "Epoch 104/300\n",
      "Average training loss: 0.12490034774276945\n",
      "Average test loss: 0.004690108689996931\n",
      "Epoch 105/300\n",
      "Average training loss: 0.12183614807658726\n",
      "Average test loss: 0.0032785363118681644\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11948241693443722\n",
      "Average test loss: 0.00358889266786476\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11737025726503796\n",
      "Average test loss: 0.003318534868458907\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11518227939473258\n",
      "Average test loss: 0.003571110781489147\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11342287281486724\n",
      "Average test loss: 0.003212389001208875\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11191538338528739\n",
      "Average test loss: 0.0043820689844174514\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11083071743779713\n",
      "Average test loss: 0.0034149519256833527\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10974435357252757\n",
      "Average test loss: 0.003256224508397281\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10820877603027555\n",
      "Average test loss: 0.0033357753879908057\n",
      "Epoch 114/300\n",
      "Average training loss: 0.1069912038313018\n",
      "Average test loss: 0.003231860697062479\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10734589829709795\n",
      "Average test loss: 0.0032370424469312033\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10535750969250997\n",
      "Average test loss: 0.004559375554944078\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10641181125243505\n",
      "Average test loss: 0.0037999376832611033\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10422479148374664\n",
      "Average test loss: 0.017883788448241022\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10414958302842246\n",
      "Average test loss: 0.003685646183167895\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10303581429190106\n",
      "Average test loss: 0.003196914261413945\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10272145190503862\n",
      "Average test loss: 0.00322281509410176\n",
      "Epoch 122/300\n",
      "Average training loss: 0.1022117895020379\n",
      "Average test loss: 0.0033998097431742484\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10174833717611101\n",
      "Average test loss: 0.0031979992567665047\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10140629767709308\n",
      "Average test loss: 0.0035166692435741427\n",
      "Epoch 125/300\n",
      "Average training loss: 0.100726094186306\n",
      "Average test loss: 0.014603309757179684\n",
      "Epoch 126/300\n",
      "Average training loss: 0.12105535959535176\n",
      "Average test loss: 0.00402569141652849\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10734277543756697\n",
      "Average test loss: 0.003219303422400521\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10311697924799389\n",
      "Average test loss: 0.004204070680671268\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10154348182015949\n",
      "Average test loss: 0.003294001949330171\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10071092571152582\n",
      "Average test loss: 0.01029905223308338\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10017830146683587\n",
      "Average test loss: 0.0032533334729572135\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0996089912586742\n",
      "Average test loss: 0.0032669561410115823\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09939549606376225\n",
      "Average test loss: 0.0034761925279680225\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09928520896699694\n",
      "Average test loss: 0.0033296678366346493\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09920124463240305\n",
      "Average test loss: 0.003286647976272636\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09850536316633224\n",
      "Average test loss: 0.0032819150721447336\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09818219273620181\n",
      "Average test loss: 0.003349137794226408\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09835261211792629\n",
      "Average test loss: 0.0037076005078852175\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09778915437724855\n",
      "Average test loss: 0.003638995647016499\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0979605398244328\n",
      "Average test loss: 0.003324274400456084\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09701618917783102\n",
      "Average test loss: 0.0032498991932306026\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0966816574467553\n",
      "Average test loss: 0.0033621337631096443\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09630826400717099\n",
      "Average test loss: 0.003398300438498457\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09624649335278405\n",
      "Average test loss: 0.0033144301383031738\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10092550442616145\n",
      "Average test loss: 0.004586479937450753\n",
      "Epoch 146/300\n",
      "Average training loss: 0.11315916117694642\n",
      "Average test loss: 0.003291281541602479\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09822729098796844\n",
      "Average test loss: 0.003329657626234823\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09616189858648512\n",
      "Average test loss: 0.0033580335028883484\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09551680203941133\n",
      "Average test loss: 279.2474806857639\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0954797684152921\n",
      "Average test loss: 0.0034514350537210702\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09504909713400735\n",
      "Average test loss: 0.003452395310004552\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09478562102052901\n",
      "Average test loss: 0.0034000484752986166\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09437566152546141\n",
      "Average test loss: 0.0038014930439078144\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0942192850013574\n",
      "Average test loss: 0.003369084482184715\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09412958956427045\n",
      "Average test loss: 0.0044645957450071974\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09469445543156729\n",
      "Average test loss: 0.003633770260338982\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09318184742662641\n",
      "Average test loss: 0.004157637584126658\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09364667685826619\n",
      "Average test loss: 0.003415997237794929\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09300189357333713\n",
      "Average test loss: 0.0033545175368587176\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09283265314499538\n",
      "Average test loss: 0.029757394185910623\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09231503377358119\n",
      "Average test loss: 0.004726588675545321\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09242684911357033\n",
      "Average test loss: 0.0033679044101801183\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09229312564267053\n",
      "Average test loss: 0.003305338149683343\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09190915988551246\n",
      "Average test loss: 0.003408385953762465\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09223398070865207\n",
      "Average test loss: 0.0034081304472767643\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09261186334821914\n",
      "Average test loss: 0.00518407786058055\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09261694325341119\n",
      "Average test loss: 0.0033901382920642695\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09090595840083228\n",
      "Average test loss: 0.0033658196359044974\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09241431739264064\n",
      "Average test loss: 0.003412380159108175\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0904280778500769\n",
      "Average test loss: 0.00389062131030692\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0904871675769488\n",
      "Average test loss: 0.003712207404896617\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09040423806508383\n",
      "Average test loss: 0.003390751650556922\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09083680369787746\n",
      "Average test loss: 0.003916749811420838\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08997321422232522\n",
      "Average test loss: 0.003498387138462729\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08982214893897375\n",
      "Average test loss: 0.0109272188047568\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09080236812432607\n",
      "Average test loss: 0.0033317378585537277\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08939125649134318\n",
      "Average test loss: 0.003430577189765043\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0894760524696774\n",
      "Average test loss: 0.0038260250985622407\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08957733986775081\n",
      "Average test loss: 0.003461272032517526\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08903112381696701\n",
      "Average test loss: 0.0033707032383730015\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08922808531257842\n",
      "Average test loss: 0.004765204208799534\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08926357538832559\n",
      "Average test loss: 0.012817128064731757\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08903079939550823\n",
      "Average test loss: 0.07698569659557608\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08816043661700354\n",
      "Average test loss: 0.003478864625510242\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08885440842310588\n",
      "Average test loss: 0.0034854324410359063\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08819056638744142\n",
      "Average test loss: 0.004928473440723287\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08874735196431478\n",
      "Average test loss: 0.15049083994370369\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08780219719145033\n",
      "Average test loss: 0.003687177956311239\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08828270900249481\n",
      "Average test loss: 0.0035005055570767984\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09168437143829133\n",
      "Average test loss: 0.008477066644984816\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08734158404005898\n",
      "Average test loss: 0.00334381547363268\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08776015824741787\n",
      "Average test loss: 0.006006970831917392\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0872043900291125\n",
      "Average test loss: 0.0034139247557355298\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08706527971559101\n",
      "Average test loss: 0.003484135651960969\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08701780580149757\n",
      "Average test loss: 0.0035842408506820598\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08724084766705831\n",
      "Average test loss: 0.0035109145835869843\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08695876723527908\n",
      "Average test loss: 0.0035134325509683954\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08723629772663116\n",
      "Average test loss: 0.0034173825103789566\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0866139973004659\n",
      "Average test loss: 0.003453243023198512\n",
      "Epoch 200/300\n",
      "Average training loss: 0.086511902001169\n",
      "Average test loss: 0.003916384612934457\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08813083883126577\n",
      "Average test loss: 0.00496122921961877\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08696277192234993\n",
      "Average test loss: 0.004383855539891455\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08679389336374072\n",
      "Average test loss: 0.003446606852942043\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0857417578432295\n",
      "Average test loss: 0.0038535581988592945\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08647663584020403\n",
      "Average test loss: 0.0034606311466130945\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08633520420392354\n",
      "Average test loss: 0.0037945880780203473\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08574275952577591\n",
      "Average test loss: 0.003497881682796611\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08672271593411764\n",
      "Average test loss: 0.004083180628716945\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08578234483798344\n",
      "Average test loss: 0.003427211883581347\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08626011969645818\n",
      "Average test loss: 0.003645198152710994\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08562748460637198\n",
      "Average test loss: 0.003574155900006493\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08533711091677348\n",
      "Average test loss: 0.0036876047139780387\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0856703596578704\n",
      "Average test loss: 0.010590240168074767\n",
      "Epoch 214/300\n",
      "Average training loss: 0.16080139360162946\n",
      "Average test loss: 0.0033231238110197916\n",
      "Epoch 215/300\n",
      "Average training loss: 0.11524387234780524\n",
      "Average test loss: 0.0035744438610143133\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10369997751050525\n",
      "Average test loss: 0.003304908213723037\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09767170210679373\n",
      "Average test loss: 0.0033855187489340703\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09361572348409229\n",
      "Average test loss: 0.0034316419449945293\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09039397777451409\n",
      "Average test loss: 0.0034716203088561693\n",
      "Epoch 220/300\n",
      "Average training loss: 0.707712607006232\n",
      "Average test loss: 0.003864488405899869\n",
      "Epoch 221/300\n",
      "Average training loss: 0.20085283381409116\n",
      "Average test loss: 0.07806559531721805\n",
      "Epoch 222/300\n",
      "Average training loss: 0.14833386617236669\n",
      "Average test loss: 0.0034285977660781807\n",
      "Epoch 223/300\n",
      "Average training loss: 0.1331639210979144\n",
      "Average test loss: 0.003673419675893254\n",
      "Epoch 224/300\n",
      "Average training loss: 0.12429855992396673\n",
      "Average test loss: 0.003861683145372404\n",
      "Epoch 225/300\n",
      "Average training loss: 0.11779708793428209\n",
      "Average test loss: 0.0035782565388621556\n",
      "Epoch 226/300\n",
      "Average training loss: 0.11623681421412362\n",
      "Average test loss: 0.0034096812994943724\n",
      "Epoch 227/300\n",
      "Average training loss: 0.10870952934688992\n",
      "Average test loss: 0.0036032623932179477\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10501887296968036\n",
      "Average test loss: 0.003334369623619649\n",
      "Epoch 229/300\n",
      "Average training loss: 0.10172214919990963\n",
      "Average test loss: 0.0035093468357291486\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09888621883922152\n",
      "Average test loss: 0.003488079404251443\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09615756129556233\n",
      "Average test loss: 0.0033660652664386564\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09413514055808385\n",
      "Average test loss: 0.003672635995886392\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09228709899054634\n",
      "Average test loss: 0.003397737213306957\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09115476502312554\n",
      "Average test loss: 0.007192277588364151\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0888376318944825\n",
      "Average test loss: 0.0034979718062612747\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08796181350284153\n",
      "Average test loss: 0.0034622892406251694\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08745481642087301\n",
      "Average test loss: 0.0036567649410830603\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08759433827797572\n",
      "Average test loss: 0.0033657732978463173\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08616874020629459\n",
      "Average test loss: 0.003455319465241498\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08596751922369003\n",
      "Average test loss: 0.0033557570210347573\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08852697327401902\n",
      "Average test loss: 0.0034802920296788217\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08536479314830568\n",
      "Average test loss: 0.0034588309671315883\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08492450194226371\n",
      "Average test loss: 0.0034688499747878975\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08504268371396595\n",
      "Average test loss: 0.004953082298652993\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08519046252965927\n",
      "Average test loss: 0.004784119680937794\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08565084843503104\n",
      "Average test loss: 0.004022720673845874\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08466086704201169\n",
      "Average test loss: 0.003715488695849975\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08604097832573784\n",
      "Average test loss: 0.004408047741072046\n",
      "Epoch 249/300\n",
      "Average training loss: 0.084945041951206\n",
      "Average test loss: 0.003524641814745135\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08450999748706818\n",
      "Average test loss: 0.003926046071781052\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08436324323548211\n",
      "Average test loss: 0.01761160308495164\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0844942070444425\n",
      "Average test loss: 0.003494927985800637\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08444054215484195\n",
      "Average test loss: 0.0035028939528597724\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08533651536040836\n",
      "Average test loss: 0.004171852795200216\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08424569456444846\n",
      "Average test loss: 0.0035128212386949195\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08409129827552371\n",
      "Average test loss: 0.0036900337700628573\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08390638416343266\n",
      "Average test loss: 0.0036374498241477544\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08407317835092544\n",
      "Average test loss: 0.004742819986823532\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08381830103529825\n",
      "Average test loss: 0.00349046703790211\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08436507354180019\n",
      "Average test loss: 0.0038348768572840424\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08320082139306598\n",
      "Average test loss: 0.015754817254841326\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08361293654309379\n",
      "Average test loss: 0.0035604535699304608\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08423986038896773\n",
      "Average test loss: 0.0036048327930685545\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08313151376114951\n",
      "Average test loss: 0.00350298559686376\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08459870626529058\n",
      "Average test loss: 0.003538990951039725\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0832049168282085\n",
      "Average test loss: 0.006072011542816957\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08313393810060289\n",
      "Average test loss: 0.0035834921981311506\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08327588886684842\n",
      "Average test loss: 0.0034801484267744753\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08274056000841988\n",
      "Average test loss: 0.0037346112608081764\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08283951748741998\n",
      "Average test loss: 0.004001288108113739\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08334383389022615\n",
      "Average test loss: 0.0086093535348773\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08314286748568217\n",
      "Average test loss: 0.0034413609988987446\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08415400646792517\n",
      "Average test loss: 0.0034718961928867633\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0824776976439688\n",
      "Average test loss: 0.0034712499080018864\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08270858863327238\n",
      "Average test loss: 0.003428117740485403\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08276852853430643\n",
      "Average test loss: 0.0035258996143109267\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08264447701639599\n",
      "Average test loss: 0.003584359533670876\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08230276338259379\n",
      "Average test loss: 0.003552339009526703\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08277258133888245\n",
      "Average test loss: 0.0034099095196773606\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08228736701938841\n",
      "Average test loss: 0.0040210167469663756\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08371981616152657\n",
      "Average test loss: 0.0036080922490606703\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08195073893997404\n",
      "Average test loss: 0.003646316274586651\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08215793804989921\n",
      "Average test loss: 0.0036002726782527234\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08251205211877823\n",
      "Average test loss: 0.003993025130281846\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08190819078021579\n",
      "Average test loss: 0.003532253179906143\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08186631117926703\n",
      "Average test loss: 0.003606453358092242\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08164716795417998\n",
      "Average test loss: 0.00382872358440525\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08261734633313285\n",
      "Average test loss: 0.003878322767300738\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0816578697959582\n",
      "Average test loss: 0.004448948204103444\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08160543180174298\n",
      "Average test loss: 0.00416369947956668\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08548770236968994\n",
      "Average test loss: 0.0034695956653190982\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08118761027521557\n",
      "Average test loss: 0.003462829892627067\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08131433585617277\n",
      "Average test loss: 0.003570850829386877\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08273811404572592\n",
      "Average test loss: 0.003534746844942371\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08128787118858762\n",
      "Average test loss: 0.0035238270751304095\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08147441232204437\n",
      "Average test loss: 0.0036377216602365174\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0814990498556031\n",
      "Average test loss: 58.03693132188585\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08222979903883404\n",
      "Average test loss: 0.003539749158753289\n",
      "Epoch 299/300\n",
      "Average training loss: 0.11388004679150052\n",
      "Average test loss: 0.0034626376177701685\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09876063484615749\n",
      "Average test loss: 0.003504672420521577\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.072163041856554\n",
      "Average test loss: 0.006959636833932665\n",
      "Epoch 2/300\n",
      "Average training loss: 6.1897277976142036\n",
      "Average test loss: 0.03331581622031\n",
      "Epoch 3/300\n",
      "Average training loss: 4.583025964948866\n",
      "Average test loss: 0.004875311181777054\n",
      "Epoch 4/300\n",
      "Average training loss: 3.8562605851491294\n",
      "Average test loss: 0.0045968348116924365\n",
      "Epoch 5/300\n",
      "Average training loss: 3.2044337743123372\n",
      "Average test loss: 0.06527687249705195\n",
      "Epoch 6/300\n",
      "Average training loss: 2.685675248463949\n",
      "Average test loss: 0.00420720242170824\n",
      "Epoch 7/300\n",
      "Average training loss: 2.3120993556976317\n",
      "Average test loss: 0.01039938753698435\n",
      "Epoch 8/300\n",
      "Average training loss: 2.074232063505385\n",
      "Average test loss: 0.0038609564242263637\n",
      "Epoch 9/300\n",
      "Average training loss: 1.738842002444797\n",
      "Average test loss: 0.004569532263196177\n",
      "Epoch 10/300\n",
      "Average training loss: 1.468500929620531\n",
      "Average test loss: 0.0035275084860622885\n",
      "Epoch 11/300\n",
      "Average training loss: 1.2556526981989542\n",
      "Average test loss: 0.003533035257831216\n",
      "Epoch 12/300\n",
      "Average training loss: 1.0665692890485128\n",
      "Average test loss: 0.0035450448526276484\n",
      "Epoch 13/300\n",
      "Average training loss: 0.9110798448456658\n",
      "Average test loss: 0.005138687349028058\n",
      "Epoch 14/300\n",
      "Average training loss: 0.777869252257877\n",
      "Average test loss: 0.003694774735098084\n",
      "Epoch 15/300\n",
      "Average training loss: 0.6646527926656935\n",
      "Average test loss: 0.0031938056488417916\n",
      "Epoch 16/300\n",
      "Average training loss: 0.5710894567701552\n",
      "Average test loss: 0.00299910464282665\n",
      "Epoch 17/300\n",
      "Average training loss: 0.49288774739371405\n",
      "Average test loss: 0.003220894846237368\n",
      "Epoch 18/300\n",
      "Average training loss: 0.4282932393021054\n",
      "Average test loss: 0.0052078602715498875\n",
      "Epoch 19/300\n",
      "Average training loss: 0.37544911683930293\n",
      "Average test loss: 0.0028333936660654016\n",
      "Epoch 20/300\n",
      "Average training loss: 0.33172164249420166\n",
      "Average test loss: 0.002796942370219363\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2957601137558619\n",
      "Average test loss: 0.002966974809765816\n",
      "Epoch 22/300\n",
      "Average training loss: 0.2651761574347814\n",
      "Average test loss: 0.0027352219459911187\n",
      "Epoch 23/300\n",
      "Average training loss: 0.23826567063066695\n",
      "Average test loss: 0.002628138134462966\n",
      "Epoch 24/300\n",
      "Average training loss: 0.21630373093816968\n",
      "Average test loss: 0.004338384434373842\n",
      "Epoch 25/300\n",
      "Average training loss: 0.19929310638374753\n",
      "Average test loss: 0.0026279395379953914\n",
      "Epoch 26/300\n",
      "Average training loss: 0.18421831427680121\n",
      "Average test loss: 0.0025264101752804384\n",
      "Epoch 27/300\n",
      "Average training loss: 0.17349904555744594\n",
      "Average test loss: 0.1587609935535325\n",
      "Epoch 28/300\n",
      "Average training loss: 0.16158431463771397\n",
      "Average test loss: 0.002545606907043192\n",
      "Epoch 29/300\n",
      "Average training loss: 0.152866481423378\n",
      "Average test loss: 0.002714330380368564\n",
      "Epoch 30/300\n",
      "Average training loss: 0.14489826104376052\n",
      "Average test loss: 0.005093150708824396\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1395990116265085\n",
      "Average test loss: 0.002641613133251667\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13525009625487858\n",
      "Average test loss: 0.0029527621244390804\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12829843326409657\n",
      "Average test loss: 0.002441736887726519\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12447991700967152\n",
      "Average test loss: 0.0024169131949957874\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11982887857490115\n",
      "Average test loss: 0.0024312355539037123\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11545219090249803\n",
      "Average test loss: 0.0024473904091864822\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11256108448902766\n",
      "Average test loss: 0.002353445627933575\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10999576532178455\n",
      "Average test loss: 0.0023735284542457926\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10634268858697679\n",
      "Average test loss: 0.002573434996936056\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1040010551015536\n",
      "Average test loss: 0.0023796594373674857\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10126895585987303\n",
      "Average test loss: 0.0023476950155778063\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09826422162188424\n",
      "Average test loss: 0.0024511023625317548\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09794646565781699\n",
      "Average test loss: 0.0024580436680052015\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09575164745251338\n",
      "Average test loss: 0.0025357189174327584\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09268130362696118\n",
      "Average test loss: 0.0023251339147488275\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09045255213975906\n",
      "Average test loss: 0.0022651372702999248\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08944110716051526\n",
      "Average test loss: 0.0023180074023289813\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08797371115287145\n",
      "Average test loss: 0.003433049200516608\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08650934287574556\n",
      "Average test loss: 0.0024006570974985757\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08692406330506007\n",
      "Average test loss: 0.002291655340956317\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08414010039965311\n",
      "Average test loss: 0.0038901486554079588\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08351326024532318\n",
      "Average test loss: 0.002241973388319214\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09645444675948885\n",
      "Average test loss: 0.0023864334075608187\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09960181970728768\n",
      "Average test loss: 0.0022894054280800952\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08826323082711962\n",
      "Average test loss: 0.0023348002282695636\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08533854903777441\n",
      "Average test loss: 0.002483004486395253\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08388946288824081\n",
      "Average test loss: 0.0022884310527394214\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08279359308216307\n",
      "Average test loss: 0.002260850186770161\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08198757300111982\n",
      "Average test loss: 0.0022509885606252487\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08294046927160686\n",
      "Average test loss: 0.004592965293675661\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08429603298505148\n",
      "Average test loss: 0.0023512437459495333\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08242751694387859\n",
      "Average test loss: 0.0022332080254952114\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08053802327315013\n",
      "Average test loss: 0.0023492355731626353\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07980277854866452\n",
      "Average test loss: 0.002767024076440268\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07922651021348105\n",
      "Average test loss: 0.002296803722985917\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07917070843776067\n",
      "Average test loss: 0.0022232265188876126\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07829470521873898\n",
      "Average test loss: 0.002315983194857836\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07816852976878484\n",
      "Average test loss: 0.002277080142042703\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07759334362546604\n",
      "Average test loss: 0.002245704216365185\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07714481779270702\n",
      "Average test loss: 0.0036124452404263947\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07680129866467582\n",
      "Average test loss: 0.002257055032170481\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07631462752819061\n",
      "Average test loss: 0.0033050580649740167\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07614639254411061\n",
      "Average test loss: 0.002308390030430423\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07545461418231328\n",
      "Average test loss: 0.002357823727859391\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07523939357201258\n",
      "Average test loss: 0.00224221865253316\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07480351568592919\n",
      "Average test loss: 0.002361065718448824\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07456120526790619\n",
      "Average test loss: 0.028996315341856746\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07435258499119017\n",
      "Average test loss: 0.002333961571670241\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07398172005017599\n",
      "Average test loss: 0.0023582792499413094\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0733769360780716\n",
      "Average test loss: 0.002250473809738954\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07316748017734952\n",
      "Average test loss: 0.0023247892887641986\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07311898026863733\n",
      "Average test loss: 0.12779492281542884\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07244906455940671\n",
      "Average test loss: 0.0022915300097730426\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07232601516114341\n",
      "Average test loss: 0.0022498642593208286\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07225415230459638\n",
      "Average test loss: 0.0022899790965020656\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07157687335544162\n",
      "Average test loss: 0.002612263132507602\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07185099230209986\n",
      "Average test loss: 0.004586257433725728\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07461105919546551\n",
      "Average test loss: 0.0022705761887547044\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07456408112578922\n",
      "Average test loss: 0.0022709446723262468\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07137069772349464\n",
      "Average test loss: 0.0022477651877949637\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07074713026152717\n",
      "Average test loss: 0.005368216848621766\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07080278105868233\n",
      "Average test loss: 0.002251020832711624\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07027535630928146\n",
      "Average test loss: 0.002339254985873898\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07042436759339439\n",
      "Average test loss: 0.002353318347906073\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06999958902597428\n",
      "Average test loss: 0.0023451255686167214\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06987931406497955\n",
      "Average test loss: 0.002295478927799397\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07348121423191495\n",
      "Average test loss: 0.010984272696077824\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07764518653353056\n",
      "Average test loss: 0.0022483094410142967\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07060294480456246\n",
      "Average test loss: 0.0022430717800226477\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06949516169230144\n",
      "Average test loss: 0.002314120223124822\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06925132468011644\n",
      "Average test loss: 0.0026172653798841768\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06910824537277221\n",
      "Average test loss: 0.0023266934355099995\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0688041638467047\n",
      "Average test loss: 0.0024191717751738096\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06861695532666312\n",
      "Average test loss: 0.0023745386246591805\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06875218020213975\n",
      "Average test loss: 0.0023380008325394656\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06810662339131038\n",
      "Average test loss: 0.002571099984459579\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06787259246905644\n",
      "Average test loss: 0.0023013517080495757\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06866831684112548\n",
      "Average test loss: 0.0023103026962942546\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06757904383209017\n",
      "Average test loss: 0.0023505354043510227\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06739455193281174\n",
      "Average test loss: 0.002290063900873065\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06732875318659677\n",
      "Average test loss: 0.007880089475462834\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06760664082898034\n",
      "Average test loss: 0.0024121757441510757\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06767028055919541\n",
      "Average test loss: 0.0023087265704654983\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06686546972725127\n",
      "Average test loss: 0.0023667490571323366\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06704238781001833\n",
      "Average test loss: 0.0025375099728504815\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06699735566311413\n",
      "Average test loss: 0.002289962835402952\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06738174111644427\n",
      "Average test loss: 0.002353313380645381\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06603898800081677\n",
      "Average test loss: 0.00247557000319163\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06598949544628462\n",
      "Average test loss: 0.002745213080611494\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06587164049016105\n",
      "Average test loss: 0.0023505860577440925\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06592493558923404\n",
      "Average test loss: 0.002410009669760863\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06571390767892202\n",
      "Average test loss: 0.002406086201676064\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06648452326986524\n",
      "Average test loss: 0.0023851689547300338\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06557216929064856\n",
      "Average test loss: 0.0031309844083670115\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06507608103752137\n",
      "Average test loss: 0.00317120098695159\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06566885791222254\n",
      "Average test loss: 0.004275337548512552\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06490010935730404\n",
      "Average test loss: 0.0024575682387997706\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0649712842338615\n",
      "Average test loss: 0.002381529767273201\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06507657711373435\n",
      "Average test loss: 0.0025970665401675633\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06480875990125869\n",
      "Average test loss: 0.0025166653734114437\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06478270735012161\n",
      "Average test loss: 0.0024152333312150503\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06492824246817165\n",
      "Average test loss: 0.0023496206062328484\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06459344122476048\n",
      "Average test loss: 0.002363865020788378\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06426963071028391\n",
      "Average test loss: 0.0025860343577547205\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06543644300434324\n",
      "Average test loss: 0.005067046937429242\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06377292746636602\n",
      "Average test loss: 0.003362281904452377\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0638987469110224\n",
      "Average test loss: 0.0025447576550973786\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06407657094134225\n",
      "Average test loss: 0.011970142967998981\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0647447519964642\n",
      "Average test loss: 0.002390862610294587\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06353541882832844\n",
      "Average test loss: 0.002427283000200987\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0642872548268901\n",
      "Average test loss: 0.002371782208275464\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06343552666240268\n",
      "Average test loss: 0.004831252659153607\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06370110045539008\n",
      "Average test loss: 0.0023523829770791863\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06333650681376457\n",
      "Average test loss: 0.0025498565297780765\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06374930308924781\n",
      "Average test loss: 0.0024926981586549015\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06361224890086387\n",
      "Average test loss: 0.08632646379868189\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0629254737297694\n",
      "Average test loss: 0.002575000785291195\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06323195683293872\n",
      "Average test loss: 0.0024471370267371336\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06372210595673986\n",
      "Average test loss: 0.0023959506425178714\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06279120496908823\n",
      "Average test loss: 0.0028260928684224686\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06366502344608307\n",
      "Average test loss: 0.0024678735935853586\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06255119352208244\n",
      "Average test loss: 0.0024371170539201964\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06256517346037758\n",
      "Average test loss: 0.002754286898713973\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06239993330174022\n",
      "Average test loss: 0.002406257931970888\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06334589482016034\n",
      "Average test loss: 0.0024033571812841627\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06208282269371881\n",
      "Average test loss: 0.0027874932405021454\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06336549048291312\n",
      "Average test loss: 0.0024484217101708055\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06258134407798449\n",
      "Average test loss: 0.002562872507298986\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06302388470040428\n",
      "Average test loss: 0.002418436183180246\n",
      "Epoch 160/300\n",
      "Average training loss: 0.062110034969117905\n",
      "Average test loss: 0.00241631813844045\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06191086985667547\n",
      "Average test loss: 0.002466102016882764\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0623624804019928\n",
      "Average test loss: 0.002442513741966751\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06264735080798467\n",
      "Average test loss: 0.002476945024397638\n",
      "Epoch 165/300\n",
      "Average training loss: 0.061987908075253166\n",
      "Average test loss: 0.007593963915482164\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06176762177546819\n",
      "Average test loss: 0.0025875633087837032\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06392639032337401\n",
      "Average test loss: 0.002399170723847217\n",
      "Epoch 168/300\n",
      "Average training loss: 0.061345561795764496\n",
      "Average test loss: 0.016205258106191955\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06254455699192153\n",
      "Average test loss: 0.002461060869404011\n",
      "Epoch 171/300\n",
      "Average training loss: 0.061169173604912225\n",
      "Average test loss: 0.002505635667178366\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06128834452893999\n",
      "Average test loss: 0.004016224413075381\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06334413996669981\n",
      "Average test loss: 0.0024735639678935208\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06139234717024697\n",
      "Average test loss: 0.0032541190661076044\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06116500341229968\n",
      "Average test loss: 0.002591311850481563\n",
      "Epoch 176/300\n",
      "Average training loss: 0.061355299807257124\n",
      "Average test loss: 0.003265483177991377\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06156000245279736\n",
      "Average test loss: 0.00276040627890163\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06137772940927082\n",
      "Average test loss: 0.003097029559314251\n",
      "Epoch 179/300\n",
      "Average training loss: 0.060987023386690355\n",
      "Average test loss: 0.0025566570502188472\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06165251280201806\n",
      "Average test loss: 0.002873295081158479\n",
      "Epoch 181/300\n",
      "Average training loss: 0.17020092184676064\n",
      "Average test loss: 0.002394176394575172\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08797669126590094\n",
      "Average test loss: 0.0025258494187146425\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07959160368972354\n",
      "Average test loss: 0.0025498494054708217\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0747106202840805\n",
      "Average test loss: 0.0032410316622505583\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07083937683039242\n",
      "Average test loss: 0.003466801517125633\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06787720934549968\n",
      "Average test loss: 8.044918771717283\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06583745523956087\n",
      "Average test loss: 0.002422526159427232\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06342087439695994\n",
      "Average test loss: 0.002396143292904728\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06225703617599276\n",
      "Average test loss: 0.002446776291355491\n",
      "Epoch 191/300\n",
      "Average training loss: 0.061389433410432605\n",
      "Average test loss: 0.0023878011107444762\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10634588438272476\n",
      "Average test loss: 0.002589970004227426\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08888132743371857\n",
      "Average test loss: 0.002424191637999482\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07425755830605825\n",
      "Average test loss: 0.0024244051182435618\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06873010459211137\n",
      "Average test loss: 0.0031020564205116695\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06543716293904517\n",
      "Average test loss: 0.0025017919685277673\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06350707209441397\n",
      "Average test loss: 0.0026241213706218536\n",
      "Epoch 198/300\n",
      "Average training loss: 0.062346748683187694\n",
      "Average test loss: 0.010237110962470373\n",
      "Epoch 199/300\n",
      "Average training loss: 0.061955090830723446\n",
      "Average test loss: 0.002440389233951767\n",
      "Epoch 200/300\n",
      "Average training loss: 0.061464386327399145\n",
      "Average test loss: 0.0024445002139028577\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06210331915484534\n",
      "Average test loss: 0.0023946085663305388\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06082850697636604\n",
      "Average test loss: 0.0024045523861423133\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06079897558689117\n",
      "Average test loss: 0.002442232367065218\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06091664693752925\n",
      "Average test loss: 0.00245196959686776\n",
      "Epoch 205/300\n",
      "Average training loss: 0.060936931782298615\n",
      "Average test loss: 0.0025875084789262876\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06145946385463079\n",
      "Average test loss: 0.0026273601088258954\n",
      "Epoch 207/300\n",
      "Average training loss: 0.060800411581993105\n",
      "Average test loss: 0.0032961198764128816\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06329744971460766\n",
      "Average test loss: 0.002375245089539223\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06061719089746475\n",
      "Average test loss: 0.0024372435493601693\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06037421529822879\n",
      "Average test loss: 0.002458317291819387\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06068517597516378\n",
      "Average test loss: 0.0024471619974614847\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06526281009448899\n",
      "Average test loss: 0.0024409915175702835\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06050935532980495\n",
      "Average test loss: 0.0029423195117463667\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05996014128128688\n",
      "Average test loss: 0.002465947390637464\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05996749537520939\n",
      "Average test loss: 0.002433218972550498\n",
      "Epoch 216/300\n",
      "Average training loss: 0.060850201490852567\n",
      "Average test loss: 0.009598799800707235\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06068829983472824\n",
      "Average test loss: 0.002542425907527407\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06002842462062836\n",
      "Average test loss: 0.002659379883772797\n",
      "Epoch 219/300\n",
      "Average training loss: 0.061101452979776594\n",
      "Average test loss: 0.0024036153968837524\n",
      "Epoch 220/300\n",
      "Average training loss: 0.059893086857265895\n",
      "Average test loss: 0.003659875747230318\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06053213757607672\n",
      "Average test loss: 0.002430974448306693\n",
      "Epoch 223/300\n",
      "Average training loss: 0.060988700873321956\n",
      "Average test loss: 0.003788244776841667\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05966011339426041\n",
      "Average test loss: 0.0036341797419720224\n",
      "Epoch 225/300\n",
      "Average training loss: 0.059668112317721046\n",
      "Average test loss: 0.002954927393545707\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05982271830240885\n",
      "Average test loss: 0.018117927859226862\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0600562229487631\n",
      "Average test loss: 0.0024593695571853056\n",
      "Epoch 228/300\n",
      "Average training loss: 0.060031717833545474\n",
      "Average test loss: 0.00243466967675421\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05997439025508033\n",
      "Average test loss: 0.0024642722080979083\n",
      "Epoch 230/300\n",
      "Average training loss: 0.061626608848571775\n",
      "Average test loss: 0.054961052003833985\n",
      "Epoch 231/300\n",
      "Average training loss: 0.060935141894552444\n",
      "Average test loss: 0.0024757003951817753\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0591356207397249\n",
      "Average test loss: 0.0024869460119969315\n",
      "Epoch 233/300\n",
      "Average training loss: 0.059114388995700415\n",
      "Average test loss: 0.002410900386257304\n",
      "Epoch 234/300\n",
      "Average training loss: 0.060380362729231515\n",
      "Average test loss: 0.0030278530737592115\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05927815451886919\n",
      "Average test loss: 0.002427426612004638\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05949976228674253\n",
      "Average test loss: 0.0031297839884128834\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05913648783167203\n",
      "Average test loss: 0.0024623704600251384\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06012932116455502\n",
      "Average test loss: 0.0025035486525545516\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05920219565762414\n",
      "Average test loss: 0.0029293823319797713\n",
      "Epoch 241/300\n",
      "Average training loss: 0.059026519977384145\n",
      "Average test loss: 0.0024612007348073853\n",
      "Epoch 242/300\n",
      "Average training loss: 0.059031446566184365\n",
      "Average test loss: 0.002744662313411633\n",
      "Epoch 243/300\n",
      "Average training loss: 0.060045989394187926\n",
      "Average test loss: 0.0025975060901708073\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05883773467275832\n",
      "Average test loss: 0.0024070383847380676\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05913155574268765\n",
      "Average test loss: 0.0027301787390477126\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05971205137173335\n",
      "Average test loss: 0.006083369774950875\n",
      "Epoch 247/300\n",
      "Average training loss: 0.058843411640988455\n",
      "Average test loss: 0.0024361984197878175\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05885315304332309\n",
      "Average test loss: 0.0024869666072643466\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05862284062306086\n",
      "Average test loss: 0.0024513820711937217\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05868645072645611\n",
      "Average test loss: 0.002532547718845308\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0603749843802717\n",
      "Average test loss: 0.0024631166864807407\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05839619889193111\n",
      "Average test loss: 0.002630447215711077\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05849496313598421\n",
      "Average test loss: 0.006669232053475247\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05910813070999252\n",
      "Average test loss: 0.002444190034021934\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05887457860840691\n",
      "Average test loss: 0.0025251718978914948\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0594134643541442\n",
      "Average test loss: 0.002694129123042027\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05890316993991534\n",
      "Average test loss: 0.0030240455584393607\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05840849133332571\n",
      "Average test loss: 0.0025251460634171962\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06192561521795061\n",
      "Average test loss: 0.002440146146549119\n",
      "Epoch 263/300\n",
      "Average training loss: 0.058010075006220074\n",
      "Average test loss: 0.002465928712238868\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05825907373428345\n",
      "Average test loss: 0.0024548235080308384\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05850276430447896\n",
      "Average test loss: 0.003681219033483002\n",
      "Epoch 266/300\n",
      "Average training loss: 0.058897777686516446\n",
      "Average test loss: 0.002464768914091918\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05818612724542618\n",
      "Average test loss: 0.0027177293279932605\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05831270775530074\n",
      "Average test loss: 0.002434444988146424\n",
      "Epoch 269/300\n",
      "Average training loss: 0.060081352806753585\n",
      "Average test loss: 0.002814200996214317\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05787590083811018\n",
      "Average test loss: 0.0037158905006945135\n",
      "Epoch 271/300\n",
      "Average training loss: 0.058353462172879116\n",
      "Average test loss: 0.004479470339086321\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05908477738168504\n",
      "Average test loss: 0.0038006181799703173\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05966843110985226\n",
      "Average test loss: 0.002458411766216159\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05784679368138313\n",
      "Average test loss: 0.0035736502694586913\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05797145677275128\n",
      "Average test loss: 0.002481734003664719\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05867555542786916\n",
      "Average test loss: 0.0025939616718226007\n",
      "Epoch 278/300\n",
      "Average training loss: 0.057877895093626446\n",
      "Average test loss: 0.0025242526839590737\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05884947362873289\n",
      "Average test loss: 0.0027343557919893\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05880585875113805\n",
      "Average test loss: 0.0024097248739045527\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05759618974063131\n",
      "Average test loss: 0.002481137609316243\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05787823322746489\n",
      "Average test loss: 0.00253325190457205\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05831157242589527\n",
      "Average test loss: 0.002922071928779284\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05812370105915599\n",
      "Average test loss: 0.002468470995210939\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05755681481957436\n",
      "Average test loss: 0.0038489272077050474\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0576500640379058\n",
      "Average test loss: 0.0025189884576118653\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06106756609678268\n",
      "Average test loss: 0.002558383312697212\n",
      "Epoch 288/300\n",
      "Average test loss: 0.0025069074217850965\n",
      "Epoch 290/300\n",
      "Average training loss: 0.057720037675566147\n",
      "Average test loss: 0.0024795296508818864\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07813604662153456\n",
      "Average test loss: 0.05464504149887297\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08279916888144281\n",
      "Average test loss: 0.0028669135419444906\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06662829392817285\n",
      "Average test loss: 0.0024364568926393986\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06025151581565539\n",
      "Average test loss: 0.002468410124588344\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05794030712379349\n",
      "Average test loss: 0.0025056225362544257\n",
      "Epoch 296/300\n",
      "Average training loss: 0.059548548380533854\n",
      "Average test loss: 0.002446335533426868\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05701227114266819\n",
      "Average test loss: 0.002590992503489057\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05684940334823396\n",
      "Average test loss: 0.0024803400052090486\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05723186284634802\n",
      "Average test loss: 0.0027757166913814015\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05813560456699795\n",
      "Average test loss: 0.002597888523505794\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 669.2484917331271\n",
      "Average test loss: 281.240370845841\n",
      "Epoch 2/300\n",
      "Average training loss: 12.365679727342394\n",
      "Average test loss: 29.45628135613766\n",
      "Epoch 3/300\n",
      "Average training loss: 9.54508069864909\n",
      "Average test loss: 0.4371595619602336\n",
      "Epoch 4/300\n",
      "Average training loss: 6.013969748602973\n",
      "Average test loss: 2.6066913496665656\n",
      "Epoch 7/300\n",
      "Average training loss: 5.1420767627292205\n",
      "Average test loss: 0.0062062799342804485\n",
      "Epoch 8/300\n",
      "Average training loss: 4.331717494116889\n",
      "Average test loss: 0.005460758244825734\n",
      "Epoch 9/300\n",
      "Average training loss: 3.8520803163316515\n",
      "Average test loss: 0.005296365703559584\n",
      "Epoch 10/300\n",
      "Average training loss: 3.5387003578609892\n",
      "Average test loss: 0.0044464802597132\n",
      "Epoch 11/300\n",
      "Average training loss: 3.1777491238911946\n",
      "Average test loss: 0.08897555274350775\n",
      "Epoch 12/300\n",
      "Average training loss: 2.8240475747850207\n",
      "Average test loss: 0.004283176434950696\n",
      "Epoch 13/300\n",
      "Average training loss: 2.457530924267239\n",
      "Average test loss: 0.004616312538997995\n",
      "Epoch 14/300\n",
      "Average training loss: 2.1736028231514823\n",
      "Average test loss: 0.0037466769373665255\n",
      "Epoch 15/300\n",
      "Average training loss: 1.9452686638302272\n",
      "Average test loss: 0.0035471953538556895\n",
      "Epoch 16/300\n",
      "Average training loss: 1.7378310712178549\n",
      "Average test loss: 0.0037979925276918542\n",
      "Epoch 17/300\n",
      "Average training loss: 1.2246189717186822\n",
      "Average test loss: 0.0036246531651251846\n",
      "Epoch 20/300\n",
      "Average training loss: 1.0807505127588908\n",
      "Average test loss: 0.00309760957935618\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9521447740660773\n",
      "Average test loss: 0.0030250182271831564\n",
      "Epoch 22/300\n",
      "Average training loss: 0.8367213506698609\n",
      "Average test loss: 0.0028960413829320006\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7331670953432718\n",
      "Average test loss: 0.002898283848538995\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6387656036482917\n",
      "Average test loss: 0.0027288169271002215\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5561273618804083\n",
      "Average test loss: 0.003434516625271903\n",
      "Epoch 26/300\n",
      "Average training loss: 0.4842794988685184\n",
      "Average test loss: 0.003002310716236631\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4232034564283159\n",
      "Average test loss: 0.002435159670499464\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3715254091421763\n",
      "Average test loss: 0.0025229596533916064\n",
      "Epoch 29/300\n",
      "Average training loss: 0.327203505092197\n",
      "Average test loss: 0.002308698830505212\n",
      "Epoch 30/300\n",
      "Average training loss: 0.25382836210727694\n",
      "Average test loss: 0.0024171701609674427\n",
      "Epoch 32/300\n",
      "Average training loss: 0.22532981679174635\n",
      "Average test loss: 0.0033483918259541194\n",
      "Epoch 33/300\n",
      "Average training loss: 0.20207417039076486\n",
      "Average test loss: 0.0021779691863598096\n",
      "Epoch 34/300\n",
      "Average training loss: 0.18266968936390346\n",
      "Average test loss: 0.0021883529029372664\n",
      "Epoch 35/300\n",
      "Average training loss: 0.16567920299371083\n",
      "Average test loss: 0.0024120794358766743\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15415759783320956\n",
      "Average test loss: 0.0027282114463547864\n",
      "Epoch 37/300\n",
      "Average training loss: 0.14168137178156112\n",
      "Average test loss: 0.002075165581372049\n",
      "Epoch 38/300\n",
      "Average training loss: 0.13403925377792783\n",
      "Average test loss: 0.0021642283219844103\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12487887844774458\n",
      "Average test loss: 0.002004356996880637\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12234089635478125\n",
      "Average test loss: 0.013090958497176567\n",
      "Epoch 41/300\n",
      "Average training loss: 0.13993908275498285\n",
      "Average test loss: 96.04878308142555\n",
      "Epoch 42/300\n",
      "Average training loss: 0.18538915464613173\n",
      "Average test loss: 0.002383961440374454\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11644779030481975\n",
      "Average test loss: 0.002378495020377967\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10854981729388237\n",
      "Average test loss: 0.0020745238384438885\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1029130941496955\n",
      "Average test loss: 0.0020904295915323825\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09891469362046984\n",
      "Average test loss: 0.002087980144139793\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08946391428179211\n",
      "Average test loss: 0.0019313275141434538\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08842204444938236\n",
      "Average test loss: 0.0023307119409243264\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08492810271845924\n",
      "Average test loss: 0.0018724423481358422\n",
      "Epoch 52/300\n",
      "Average training loss: 0.083818983823061\n",
      "Average test loss: 0.0026911106127210788\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0851142610973782\n",
      "Average test loss: 0.0027715013269335033\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08037958976295259\n",
      "Average test loss: 0.0019273863879756795\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07773926968706979\n",
      "Average test loss: 0.002092710652285152\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0761938022143311\n",
      "Average test loss: 0.0017953014586948686\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07400063304106394\n",
      "Average test loss: 0.010812971134152678\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07267512572142813\n",
      "Average test loss: 0.0018706377428025008\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07130008408758376\n",
      "Average test loss: 0.0018579913835144704\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06994796309206221\n",
      "Average test loss: 0.00190776407925619\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06889297789997524\n",
      "Average test loss: 0.0022179457969549628\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06740678573316998\n",
      "Average test loss: 0.0019148211131493251\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06801035214132733\n",
      "Average test loss: 0.0018926820915399327\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06559459520048565\n",
      "Average test loss: 0.0017896763177381622\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0652832589579953\n",
      "Average test loss: 0.003921243003052142\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06450138747692108\n",
      "Average test loss: 0.0017219281542218395\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06402090235551199\n",
      "Average test loss: 0.001811183197837737\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06354852078358332\n",
      "Average test loss: 0.0017364599922051033\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06301037095321549\n",
      "Average test loss: 0.0017425536210131313\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0621017138560613\n",
      "Average test loss: 0.0017495701485830877\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06183685692482525\n",
      "Average test loss: 0.001692896311274833\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0617751455075211\n",
      "Average test loss: 0.001713843651012414\n",
      "Epoch 74/300\n",
      "Average training loss: 0.060809502558575736\n",
      "Average test loss: 0.0018061942069066896\n",
      "Epoch 75/300\n",
      "Average training loss: 0.060095678627490996\n",
      "Average test loss: 0.006723669218520323\n",
      "Epoch 78/300\n",
      "Average training loss: 0.059241241531239615\n",
      "Average test loss: 0.08455405154410335\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05950566545459959\n",
      "Average test loss: 0.0020901642280320328\n",
      "Epoch 80/300\n",
      "Average training loss: 0.058862751338216994\n",
      "Average test loss: 0.0018165282040006584\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05850428161025047\n",
      "Average test loss: 0.0016925082734475533\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05909830249349276\n",
      "Average test loss: 0.01851943712019258\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05817056903574202\n",
      "Average test loss: 0.0016979593858122825\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05753286334872246\n",
      "Average test loss: 0.002402248324515919\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05747254635228051\n",
      "Average test loss: 0.001668521802355018\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05729015879829725\n",
      "Average test loss: 0.0016693193561707934\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0573705623348554\n",
      "Average test loss: 0.0019222284429189231\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05671264491809739\n",
      "Average test loss: 0.0017983049595107635\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05708244323399332\n",
      "Average test loss: 0.0035020901006129054\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05626298858059777\n",
      "Average test loss: 0.0018134705858925979\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0559084950155682\n",
      "Average test loss: 0.001750971938483417\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06037311907940441\n",
      "Average test loss: 0.0016839681074230207\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05539257245262464\n",
      "Average test loss: 0.0022092029574430653\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05522575825121668\n",
      "Average test loss: 0.019833018731739784\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05528585715757476\n",
      "Average test loss: 0.001685885943679346\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05527560663223267\n",
      "Average test loss: 0.0017067454200134508\n",
      "Epoch 99/300\n",
      "Average training loss: 0.054605779323312974\n",
      "Average test loss: 0.0017367725945595237\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05456829430328475\n",
      "Average test loss: 0.0016866014757090145\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05457457442416085\n",
      "Average test loss: 0.0018069717512569493\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05466497408019172\n",
      "Average test loss: 0.003590605426579714\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05397316115432316\n",
      "Average test loss: 0.0017198910690430138\n",
      "Epoch 104/300\n",
      "Average training loss: 0.053979818758037354\n",
      "Average test loss: 0.0017551462672029932\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05383150813976924\n",
      "Average test loss: 0.0021275340949909556\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05355853384402063\n",
      "Average test loss: 0.001686685639537043\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05322137031621403\n",
      "Average test loss: 0.0017118483824241492\n",
      "Epoch 109/300\n",
      "Average training loss: 0.061960695481962626\n",
      "Average test loss: 0.0017183973568802079\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05367301709784402\n",
      "Average test loss: 0.001724426190679272\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05289005510012309\n",
      "Average test loss: 0.0017285240170442397\n",
      "Epoch 112/300\n",
      "Average training loss: 0.052890089021788704\n",
      "Average test loss: 0.0020756696054918897\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05269821340507931\n",
      "Average test loss: 0.0024598637885517543\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05269220542576578\n",
      "Average test loss: 0.0017174301277846098\n",
      "Epoch 115/300\n",
      "Average training loss: 0.053063428935077456\n",
      "Average test loss: 0.11898652610017194\n",
      "Epoch 116/300\n",
      "Average training loss: 0.052415869464476905\n",
      "Average test loss: 0.0018930603992193937\n",
      "Epoch 117/300\n",
      "Average training loss: 0.053060276488463086\n",
      "Average test loss: 11.366616864005724\n",
      "Epoch 118/300\n",
      "Average training loss: 0.058703511675198876\n",
      "Average test loss: 0.001930083464210232\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0529632480442524\n",
      "Average test loss: 0.0024019010461245973\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05220400715205405\n",
      "Average test loss: 0.0018424006960251265\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05207743081781599\n",
      "Average test loss: 0.0017760911903654535\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05161531017555131\n",
      "Average test loss: 0.0017461119612885846\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05203718167543411\n",
      "Average test loss: 0.0018633726040522258\n",
      "Epoch 124/300\n",
      "Average training loss: 0.052168159822622934\n",
      "Average test loss: 0.0023521526460018424\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05153385978274875\n",
      "Average test loss: 0.0017152340652214157\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05213365928663148\n",
      "Average test loss: 0.0017394095283622543\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05285763074623214\n",
      "Average test loss: 0.0021002200051314303\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05125575094752841\n",
      "Average test loss: 0.0017702987287193538\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05090954602095816\n",
      "Average test loss: 0.0017967341147984068\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05111776033706135\n",
      "Average test loss: 0.001830841839313507\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05140389319592052\n",
      "Average test loss: 0.0031649939800716107\n",
      "Epoch 132/300\n",
      "Average training loss: 0.050698158237669204\n",
      "Average test loss: 0.0019156476030540135\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05173187755544981\n",
      "Average test loss: 0.3610094240680337\n",
      "Epoch 134/300\n",
      "Average training loss: 0.050572315331962374\n",
      "Average test loss: 0.0023057710006833074\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05038595037659009\n",
      "Average test loss: 0.0018315663991702927\n",
      "Epoch 136/300\n",
      "Average training loss: 0.050576553973886704\n",
      "Average test loss: 0.0021579678888536164\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05074718137251006\n",
      "Average test loss: 0.0031346873527185785\n",
      "Epoch 138/300\n",
      "Average training loss: 0.050429204006989796\n",
      "Average test loss: 0.0019095978494733572\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05066285846961869\n",
      "Average test loss: 0.0019987440179619525\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0499326570365164\n",
      "Average test loss: 0.002185611457667417\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05042548861437374\n",
      "Average test loss: 0.0017844272655331427\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05048858421378666\n",
      "Average test loss: 0.010086176902883583\n",
      "Epoch 143/300\n",
      "Average training loss: 0.049730672597885134\n",
      "Average test loss: 0.0018791282814409997\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04994165527158313\n",
      "Average test loss: 0.0017911199066374037\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05035181983311971\n",
      "Average test loss: 0.0018321639231095711\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04965982279512617\n",
      "Average test loss: 0.0018380531987382306\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04965791075759464\n",
      "Average test loss: 0.0020842370047337478\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04962914961245325\n",
      "Average test loss: 0.0017910922028952175\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0495877914097574\n",
      "Average test loss: 0.0017830339597745073\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05027362183067534\n",
      "Average test loss: 0.0017363998676753706\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04944343425499068\n",
      "Average test loss: 0.0018841969520888395\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04988611697819498\n",
      "Average test loss: 0.0021690648415436348\n",
      "Epoch 153/300\n",
      "Average training loss: 0.048870480630132884\n",
      "Average test loss: 0.0018470348314278656\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04960726130670971\n",
      "Average test loss: 0.0018070335444062948\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04896710429257817\n",
      "Average test loss: 0.002214259340117375\n",
      "Epoch 156/300\n",
      "Average training loss: 0.050268664674626456\n",
      "Average test loss: 0.001751997512868709\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04901347796122233\n",
      "Average test loss: 0.001824880892617835\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04883447956707743\n",
      "Average test loss: 0.0018138817145178716\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04898017022675938\n",
      "Average test loss: 0.002022010498990615\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04873829946915308\n",
      "Average test loss: 0.06939636757969857\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05048591488930914\n",
      "Average test loss: 0.001962653149333265\n",
      "Epoch 162/300\n",
      "Average training loss: 0.048347917523649\n",
      "Average test loss: 0.0036366758313443924\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04837908736202452\n",
      "Average test loss: 0.001773615104249782\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04849684933490223\n",
      "Average test loss: 0.002384644052427676\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0492508549756474\n",
      "Average test loss: 0.0018137186275174221\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04841322555144628\n",
      "Average test loss: 0.0018451269851583573\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04876253055201636\n",
      "Average test loss: 0.0022860666883902417\n",
      "Epoch 168/300\n",
      "Average training loss: 0.048376581960254246\n",
      "Average test loss: 0.0019099162670059336\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07254916378193431\n",
      "Average test loss: 0.002712691567941672\n",
      "Epoch 170/300\n",
      "Average training loss: 0.055868493384785126\n",
      "Average test loss: 0.002451701007369492\n",
      "Epoch 171/300\n",
      "Average training loss: 0.050817047940360176\n",
      "Average test loss: 0.001798814774180452\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04875142787893613\n",
      "Average test loss: 0.0017684026871704393\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04810139963693089\n",
      "Average test loss: 0.0020857008552282222\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04789957890576786\n",
      "Average test loss: 0.0020291318976216844\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04803408798906538\n",
      "Average test loss: 0.0018514082481463751\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04821147440539466\n",
      "Average test loss: 0.0018955117236408923\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04842282243900829\n",
      "Average test loss: 0.0026381108595265284\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04780213557349311\n",
      "Average test loss: 0.0017886326192981666\n",
      "Epoch 179/300\n",
      "Average training loss: 0.048265641844934884\n",
      "Average test loss: 0.0018094304789685542\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04860302212834358\n",
      "Average test loss: 0.0018394898660480975\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04793515362342199\n",
      "Average test loss: 0.0018515413482156065\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04896483106414477\n",
      "Average test loss: 0.001762183614489105\n",
      "Epoch 183/300\n",
      "Average training loss: 0.047669617179367275\n",
      "Average test loss: 0.002887947767972946\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04891701139675246\n",
      "Average test loss: 0.001839145545123352\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04757785298095809\n",
      "Average test loss: 0.001994021075260308\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04766160598728392\n",
      "Average test loss: 0.0018737678067344758\n",
      "Epoch 187/300\n",
      "Average training loss: 0.047730980147918066\n",
      "Average test loss: 0.0019282756596803665\n",
      "Epoch 188/300\n",
      "Average training loss: 0.048092870950698856\n",
      "Average test loss: 0.0020844572343760065\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04818218583696418\n",
      "Average test loss: 0.0024291712544444533\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04745230115784539\n",
      "Average test loss: 0.0020563591686594816\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10981603920459747\n",
      "Average test loss: 0.0018269460317161348\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06547246852848265\n",
      "Average test loss: 0.002479136464703414\n",
      "Epoch 193/300\n",
      "Average training loss: 0.059535101155440015\n",
      "Average test loss: 0.0017045510991786916\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05596437810858091\n",
      "Average test loss: 0.0018623867680629094\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05293314720193545\n",
      "Average test loss: 0.0017498811097401712\n",
      "Epoch 196/300\n",
      "Average training loss: 0.050890531092882156\n",
      "Average test loss: 0.0019031931056848002\n",
      "Epoch 197/300\n",
      "Average training loss: 0.049781791706879935\n",
      "Average test loss: 0.0017888381057936285\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04887120787302653\n",
      "Average test loss: 0.0018524593425293763\n",
      "Epoch 199/300\n",
      "Average training loss: 0.048189115620321694\n",
      "Average test loss: 0.002786329597234726\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04793071787224876\n",
      "Average test loss: 0.0019525866648182272\n",
      "Epoch 201/300\n",
      "Average training loss: 0.050790785935189986\n",
      "Average test loss: 0.0018007379735095633\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04732323834300041\n",
      "Average test loss: 0.00188633988202653\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04756707851919863\n",
      "Average test loss: 0.0018411825058153935\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04712574775020281\n",
      "Average test loss: 0.0018186623464441962\n",
      "Epoch 205/300\n",
      "Average training loss: 0.047390653484397464\n",
      "Average test loss: 0.0018559718614237176\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05019306822286712\n",
      "Average test loss: 0.001962224022795757\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04755074497395092\n",
      "Average test loss: 0.0030613294585297504\n",
      "Epoch 208/300\n",
      "Average training loss: 0.047132406718201104\n",
      "Average test loss: 0.002116547765934633\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0472089067697525\n",
      "Average test loss: 0.001798067480429179\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047298075038525796\n",
      "Average test loss: 0.0020647526466184194\n",
      "Epoch 211/300\n",
      "Average training loss: 0.047397125207715565\n",
      "Average test loss: 0.002027780700268017\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04744826422135035\n",
      "Average test loss: 0.0019890672284074954\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04694164615869522\n",
      "Average test loss: 0.030403113476104207\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04724996963805623\n",
      "Average test loss: 0.0019559388079990942\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04705479209952884\n",
      "Average test loss: 0.009932179540395736\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04737898204723994\n",
      "Average test loss: 0.002007254505323039\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04720654880007108\n",
      "Average test loss: 0.017172543476853104\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04684879638751348\n",
      "Average test loss: 0.0033113535592953366\n",
      "Epoch 219/300\n",
      "Average training loss: 0.047236643304427464\n",
      "Average test loss: 0.0019054572826458348\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04692834838396973\n",
      "Average test loss: 0.0021733229112707907\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04685017309917344\n",
      "Average test loss: 0.0022249673233470982\n",
      "Epoch 222/300\n",
      "Average training loss: 0.047830029444562064\n",
      "Average test loss: 0.0018524320262173811\n",
      "Epoch 223/300\n",
      "Average training loss: 0.046535103125704656\n",
      "Average test loss: 0.0019389524096623063\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04686366766691208\n",
      "Average test loss: 0.027243451333708233\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04666660657193926\n",
      "Average test loss: 0.00263464815583494\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04702849054336548\n",
      "Average test loss: 0.0039372115530487565\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04643250887923771\n",
      "Average test loss: 0.001851835676572389\n",
      "Epoch 228/300\n",
      "Average training loss: 0.046846779654423396\n",
      "Average test loss: 0.0018348754714760515\n",
      "Epoch 229/300\n",
      "Average training loss: 0.046585164344973035\n",
      "Average test loss: 0.0028629539435108503\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04686739472548167\n",
      "Average test loss: 0.0018145826170738373\n",
      "Epoch 231/300\n",
      "Average training loss: 0.046546327713463044\n",
      "Average test loss: 0.0020876905529035464\n",
      "Epoch 232/300\n",
      "Average training loss: 0.046699242797162796\n",
      "Average test loss: 0.0018564259444052975\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04657765892479155\n",
      "Average test loss: 0.0018121872529801395\n",
      "Epoch 234/300\n",
      "Average training loss: 0.046483366737763085\n",
      "Average test loss: 0.008303036605525348\n",
      "Epoch 235/300\n",
      "Average training loss: 0.046513093060917325\n",
      "Average test loss: 0.0018970792192137904\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04649610585305426\n",
      "Average test loss: 0.0018506095562544134\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0462718845307827\n",
      "Average test loss: 0.0018712270196734203\n",
      "Epoch 238/300\n",
      "Average training loss: 0.047463421559996076\n",
      "Average test loss: 0.0023178889407879776\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04588384391533004\n",
      "Average test loss: 0.002108623279258609\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04624237830109067\n",
      "Average test loss: 0.0018339477998928892\n",
      "Epoch 241/300\n",
      "Average training loss: 0.048722587315572635\n",
      "Average test loss: 0.005340060450136661\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04652692951427566\n",
      "Average test loss: 0.0019445729504028956\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04598398037751516\n",
      "Average test loss: 0.001824580806410975\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04616297280457285\n",
      "Average test loss: 0.0021666794488620425\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04599583595328861\n",
      "Average test loss: 0.0018968497453153961\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04624131010638343\n",
      "Average test loss: 0.0070965725026196904\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0473219734330972\n",
      "Average test loss: 0.001933957150619891\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04591764863994387\n",
      "Average test loss: 620.6383430584503\n",
      "Epoch 249/300\n",
      "Average training loss: 0.048593544132179686\n",
      "Average test loss: 0.002075308795604441\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04552115794022878\n",
      "Average test loss: 0.0018826851417414016\n",
      "Epoch 251/300\n",
      "Average training loss: 0.045775702940093144\n",
      "Average test loss: 0.0024678347971704272\n",
      "Epoch 252/300\n",
      "Average training loss: 0.12378931389252344\n",
      "Average test loss: 0.001739731057650513\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06329656761222416\n",
      "Average test loss: 0.00194452067071365\n",
      "Epoch 254/300\n",
      "Average training loss: 0.058101849307616554\n",
      "Average test loss: 0.0017709019825690323\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05481517818239\n",
      "Average test loss: 0.0018025444392114878\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05203644754489263\n",
      "Average test loss: 0.0018571305516072445\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04996859048803647\n",
      "Average test loss: 0.004063958964103626\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04814836489491992\n",
      "Average test loss: 0.001799081596856316\n",
      "Epoch 259/300\n",
      "Average training loss: 0.047194085872835585\n",
      "Average test loss: 0.00189902076497674\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04643896460202005\n",
      "Average test loss: 0.001893328325305548\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04665206751889653\n",
      "Average test loss: 0.002073247959630357\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04626788296633297\n",
      "Average test loss: 0.0018368280506175425\n",
      "Epoch 263/300\n",
      "Average training loss: 0.046343423230780494\n",
      "Average test loss: 0.00208523049702247\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04615886689888107\n",
      "Average test loss: 0.002128592001688149\n",
      "Epoch 265/300\n",
      "Average training loss: 0.046135683208703995\n",
      "Average test loss: 0.0018918007609123985\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04611142968138059\n",
      "Average test loss: 0.02831045244468583\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04690426973501841\n",
      "Average test loss: 0.0019465152023153173\n",
      "Epoch 268/300\n",
      "Average training loss: 0.045610036257240506\n",
      "Average test loss: 0.004438550163474348\n",
      "Epoch 269/300\n",
      "Average training loss: 0.045925061447752845\n",
      "Average test loss: 0.00697243120148778\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0456620541744762\n",
      "Average test loss: 0.0026262090334461794\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04706106261081166\n",
      "Average test loss: 0.0019464469345079527\n",
      "Epoch 272/300\n",
      "Average training loss: 0.046399805843830105\n",
      "Average test loss: 0.004539358630569445\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04610065848959817\n",
      "Average test loss: 0.002068782756415506\n",
      "Epoch 276/300\n",
      "Average training loss: 0.045236485100454756\n",
      "Average test loss: 0.001848401474973394\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04578257683912913\n",
      "Average test loss: 0.0018681843895465135\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04565740701887343\n",
      "Average test loss: 0.0018839947510924605\n",
      "Epoch 279/300\n",
      "Average training loss: 0.046749653895696006\n",
      "Average test loss: 0.0018684783038786716\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04543847191996045\n",
      "Average test loss: 0.0019589692479413416\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0480042931371265\n",
      "Average test loss: 0.0021909117537240188\n",
      "Epoch 282/300\n",
      "Average training loss: 0.045484434462255904\n",
      "Average test loss: 3.758192774640189\n",
      "Epoch 283/300\n",
      "Average training loss: 0.045423756274912096\n",
      "Average test loss: 0.0019002773319888445\n",
      "Epoch 284/300\n",
      "Average training loss: 0.045548248337374794\n",
      "Average test loss: 0.0019209338821884658\n",
      "Epoch 285/300\n",
      "Average training loss: 0.045486145387093224\n",
      "Average test loss: 0.009073430241809951\n",
      "Epoch 286/300\n",
      "Average training loss: 0.045960860070255065\n",
      "Average test loss: 0.0028392900286449327\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04538480326533317\n",
      "Average test loss: 0.0018100445398853886\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04598933947748608\n",
      "Average test loss: 0.0034430723396233387\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04533122997482618\n",
      "Average test loss: 0.0018074213108047843\n",
      "Epoch 290/300\n",
      "Average training loss: 0.045362857643100954\n",
      "Average test loss: 0.0018998437848769957\n",
      "Epoch 291/300\n",
      "Average training loss: 0.045666793475548424\n",
      "Average test loss: 0.001937448229537242\n",
      "Epoch 292/300\n",
      "Average training loss: 0.045221647848685584\n",
      "Average test loss: 0.002040844638728433\n",
      "Epoch 293/300\n",
      "Average training loss: 0.045605650342173044\n",
      "Average test loss: 0.0018895959282914797\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0458000122639868\n",
      "Average test loss: 0.0020503933649096222\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04512564760115412\n",
      "Average test loss: 0.0021772874189126823\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04558615458673901\n",
      "Average test loss: 0.0018297978685133988\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04500403024421798\n",
      "Average test loss: 0.003802119387106763\n",
      "Epoch 298/300\n",
      "Average training loss: 0.045059281518061954\n",
      "Average test loss: 0.03218680575821135\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04533222794201639\n",
      "Average test loss: 0.001856148805676235\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05101567323009173\n",
      "Average test loss: 0.0018500517127621504\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive-.025/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.13\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.55\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.81\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.06\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.75\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.61\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.91\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.02\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.23\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.24\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.49\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 25.57\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 25.67\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 25.71\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 25.73\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 25.78\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 25.72\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 25.88\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 25.78\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 25.81\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.92\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 25.92\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 25.82\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.09\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.12\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.74\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.01\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.45\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.61\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.82\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.81\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.65\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.76\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.25\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.39\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.45\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.47\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.71\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.70\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd8e4e6-53b1-47a7-93ff-45be25c54389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def zip_folder(source_folder, output_zip_file):\n",
    "    # Ensure the output does not contain an extension (shutil will append .zip)\n",
    "    output_zip_file = output_zip_file.rstrip('.zip')\n",
    "\n",
    "    # Zip the folder\n",
    "    shutil.make_archive(output_zip_file, 'zip', source_folder)\n",
    "\n",
    "# Define the folder to zip and the location of the output file\n",
    "source_folder = 'Memory_Residual-Additive-.025'  # Replace with the path to your folder\n",
    "output_zip_file = 'Memory_Residual-Additive-.025.zip'  # Replace with the new zip location without .zip\n",
    "\n",
    "# Call the function to zip the folder\n",
    "zip_folder(source_folder, output_zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fa013-599f-4c78-83f6-97b9916264f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
