{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.05)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.05)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.05)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.05)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.0602772186630302\n",
      "Average test loss: 0.00515740947984159\n",
      "Epoch 2/300\n",
      "Average training loss: 0.025124020978808403\n",
      "Average test loss: 0.0047443203025807935\n",
      "Epoch 3/300\n",
      "Average training loss: 0.023691981360316278\n",
      "Average test loss: 0.004646016363468435\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02311703473329544\n",
      "Average test loss: 0.004569923967743914\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02280214234524303\n",
      "Average test loss: 0.004533564103974237\n",
      "Epoch 6/300\n",
      "Average training loss: 0.022586254258950552\n",
      "Average test loss: 0.0045138113807058995\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02241035039888488\n",
      "Average test loss: 0.0044519647633035975\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022256671054495705\n",
      "Average test loss: 0.004443925421684981\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022142436797420183\n",
      "Average test loss: 0.004447896713183986\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0220388561901119\n",
      "Average test loss: 0.004397497144010332\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021930267676711083\n",
      "Average test loss: 0.004371863410704666\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02185640425152249\n",
      "Average test loss: 0.004362517255047957\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0217695812922385\n",
      "Average test loss: 0.004344696568118201\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021689198919468456\n",
      "Average test loss: 0.004344802727508876\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021627252843644883\n",
      "Average test loss: 0.004317515862484773\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021545917857024404\n",
      "Average test loss: 0.004314080080224408\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021506432379285494\n",
      "Average test loss: 0.004306614974720611\n",
      "Epoch 18/300\n",
      "Average training loss: 0.021445211801264023\n",
      "Average test loss: 0.004311887090404828\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021389369325505364\n",
      "Average test loss: 0.004294318881506721\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02134890593836705\n",
      "Average test loss: 0.004285115623225769\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021299712090028657\n",
      "Average test loss: 0.0042780751312772435\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02125197538899051\n",
      "Average test loss: 0.004256268770330482\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02121234749091996\n",
      "Average test loss: 0.004252486904048257\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021173641012774573\n",
      "Average test loss: 0.004254802302146951\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02114521665374438\n",
      "Average test loss: 0.00424969969689846\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021103065012229815\n",
      "Average test loss: 0.004243093305991755\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021072430357336996\n",
      "Average test loss: 0.004263219576535953\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02103672891192966\n",
      "Average test loss: 0.004226818431996637\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021004424658086565\n",
      "Average test loss: 0.0042334120739251375\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02098168178399404\n",
      "Average test loss: 0.004759537955539094\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02096003777782122\n",
      "Average test loss: 0.0042205611372159585\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02092842511501577\n",
      "Average test loss: 0.004229386296951109\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020905496108863088\n",
      "Average test loss: 0.004209853733165397\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02088332372489903\n",
      "Average test loss: 0.004224414722787009\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020861437484622002\n",
      "Average test loss: 0.00421244903239939\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020843193685015044\n",
      "Average test loss: 0.004221388971846964\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02081128540966246\n",
      "Average test loss: 0.0042028809380200175\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020789656811290316\n",
      "Average test loss: 0.004199307465097971\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020774964144660367\n",
      "Average test loss: 0.0042216982266141305\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020751738106211025\n",
      "Average test loss: 0.004226621752811802\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020736935648653244\n",
      "Average test loss: 0.0041965090152290135\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020715324551694923\n",
      "Average test loss: 0.0041916453672779935\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020701309613055652\n",
      "Average test loss: 0.004204641797476345\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020678641732368205\n",
      "Average test loss: 0.004211931066794528\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020663479815754626\n",
      "Average test loss: 0.004187920974774493\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0206501727319426\n",
      "Average test loss: 0.004196942345549663\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02062900639242596\n",
      "Average test loss: 0.004194819489080045\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02060793194671472\n",
      "Average test loss: 0.004224063826104005\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020590330238143602\n",
      "Average test loss: 0.004198189619928599\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02057864655388726\n",
      "Average test loss: 0.004207986057632499\n",
      "Epoch 51/300\n",
      "Average training loss: 0.020558230110340647\n",
      "Average test loss: 0.0042004107400361035\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0205482855124606\n",
      "Average test loss: 0.004207995650255018\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02053061293065548\n",
      "Average test loss: 0.004209779396446215\n",
      "Epoch 54/300\n",
      "Average training loss: 0.020506355337798595\n",
      "Average test loss: 0.004197357565992408\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02049403321577443\n",
      "Average test loss: 0.004200013015005323\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02048382825983895\n",
      "Average test loss: 0.004179683818999264\n",
      "Epoch 57/300\n",
      "Average training loss: 0.020462380646003617\n",
      "Average test loss: 0.004188940396739377\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0204498432237241\n",
      "Average test loss: 0.004204670073878434\n",
      "Epoch 59/300\n",
      "Average training loss: 0.020436029081543287\n",
      "Average test loss: 0.00417383317856325\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020411264225840567\n",
      "Average test loss: 0.004200803629640076\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020403952113456195\n",
      "Average test loss: 0.004195373091639744\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020388236403465272\n",
      "Average test loss: 0.004206232657863034\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02037236457069715\n",
      "Average test loss: 0.004216341248402993\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02036085296008322\n",
      "Average test loss: 0.004188123290116588\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020346812973419826\n",
      "Average test loss: 0.004359261309107145\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020337130481998127\n",
      "Average test loss: 0.004221322196639246\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02031423035595152\n",
      "Average test loss: 0.004200080468008916\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020292507123616008\n",
      "Average test loss: 0.004186731951932112\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020289378217524953\n",
      "Average test loss: 0.004191612173285749\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020276185859408644\n",
      "Average test loss: 0.00422709910530183\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020260083792938127\n",
      "Average test loss: 0.004221733909514215\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020245727211236953\n",
      "Average test loss: 0.004211792727725374\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020233694741295445\n",
      "Average test loss: 0.004218249521735642\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020219109207391738\n",
      "Average test loss: 0.004203193007864886\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020191668424341412\n",
      "Average test loss: 0.004210137414021624\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020177204562558067\n",
      "Average test loss: 0.004186296893490685\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02017947365509139\n",
      "Average test loss: 0.00420236702511708\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020157201435830857\n",
      "Average test loss: 0.00419117034971714\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02014005967643526\n",
      "Average test loss: 0.004226736351847648\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02012675598677662\n",
      "Average test loss: 0.004233068431417147\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02010857105586264\n",
      "Average test loss: 0.004234386419670449\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02009895253347026\n",
      "Average test loss: 0.004306313484493229\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020085248172283174\n",
      "Average test loss: 0.004191557891459929\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02006388637257947\n",
      "Average test loss: 0.004207598619163037\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02004558274149895\n",
      "Average test loss: 0.004203820795234707\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020033995408150884\n",
      "Average test loss: 0.004224213352840808\n",
      "Epoch 87/300\n",
      "Average training loss: 0.020023801159527566\n",
      "Average test loss: 0.004212491811977493\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02000157552957535\n",
      "Average test loss: 0.004215942610055208\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019996214522255792\n",
      "Average test loss: 0.004243898036993213\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019974292849500974\n",
      "Average test loss: 0.004235494448079003\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01996898232979907\n",
      "Average test loss: 0.004262172608325879\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019954075871242417\n",
      "Average test loss: 0.004219893842935562\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019925967973967392\n",
      "Average test loss: 0.004272179151988692\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019920096259978082\n",
      "Average test loss: 0.004240160835699902\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019906733459068668\n",
      "Average test loss: 0.004248479207356771\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019891336374812656\n",
      "Average test loss: 0.004283627440532048\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019884398074613677\n",
      "Average test loss: 0.004231638662724031\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019860407589210403\n",
      "Average test loss: 0.004221935312781069\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019840924662020473\n",
      "Average test loss: 0.004305791054748827\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019835927688413198\n",
      "Average test loss: 0.0042685704458918835\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019814849234289594\n",
      "Average test loss: 0.004229984203146564\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019811136309471396\n",
      "Average test loss: 0.004244161604593197\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019787845224142075\n",
      "Average test loss: 0.00426523161182801\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01978378127846453\n",
      "Average test loss: 0.004319276486006048\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01977087733646234\n",
      "Average test loss: 0.0042125825666719015\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019747542739742333\n",
      "Average test loss: 0.00439582284188105\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019727910740507972\n",
      "Average test loss: 0.00432762012961838\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019717897478077145\n",
      "Average test loss: 0.0042791769848101666\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01971107754442427\n",
      "Average test loss: 0.004357791878283024\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019698562810818353\n",
      "Average test loss: 0.004278562244441774\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01968642882009347\n",
      "Average test loss: 0.004320513755497005\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01967346511615647\n",
      "Average test loss: 0.004271474313404825\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01965252163923449\n",
      "Average test loss: 0.004317459807627731\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019645024940371514\n",
      "Average test loss: 0.004368945180128018\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01961570562256707\n",
      "Average test loss: 0.004264035801506705\n",
      "Epoch 116/300\n",
      "Average training loss: 0.019633814298444324\n",
      "Average test loss: 0.004281923690603839\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01959988933470514\n",
      "Average test loss: 0.0043062793620758585\n",
      "Epoch 118/300\n",
      "Average training loss: 0.019593900758359168\n",
      "Average test loss: 0.004345275742312272\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019577526646355786\n",
      "Average test loss: 0.004336288524170717\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01956534702413612\n",
      "Average test loss: 0.004323724776092503\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019545534488227632\n",
      "Average test loss: 0.004297010961092181\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01953907211124897\n",
      "Average test loss: 0.004263601904114087\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019530656481782597\n",
      "Average test loss: 0.0042650601100176575\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019522489769591225\n",
      "Average test loss: 0.00427625412080023\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019503499800960222\n",
      "Average test loss: 0.004449980554067427\n",
      "Epoch 126/300\n",
      "Average training loss: 0.019489338419503635\n",
      "Average test loss: 0.0042847277737326096\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019488699588510724\n",
      "Average test loss: 0.0043012898471206425\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01945112258527014\n",
      "Average test loss: 0.004357845329162147\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019454380485746596\n",
      "Average test loss: 0.004323167212307453\n",
      "Epoch 130/300\n",
      "Average training loss: 0.019453452616930007\n",
      "Average test loss: 0.004259420816683107\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019430771128998863\n",
      "Average test loss: 0.004350998487323523\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01941523683236705\n",
      "Average test loss: 0.00432788710296154\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01939183931880527\n",
      "Average test loss: 0.004281667356689771\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01940612432691786\n",
      "Average test loss: 0.004301135929301381\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019398351524439122\n",
      "Average test loss: 0.00430327934357855\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019372196610603066\n",
      "Average test loss: 0.004354054811721047\n",
      "Epoch 137/300\n",
      "Average training loss: 0.019368768033881982\n",
      "Average test loss: 0.004479164241088761\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01935263379249308\n",
      "Average test loss: 0.004306249382595221\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019335926670167182\n",
      "Average test loss: 0.004336492096384367\n",
      "Epoch 140/300\n",
      "Average training loss: 0.019335609215001267\n",
      "Average test loss: 0.004426747474405501\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01932264536453618\n",
      "Average test loss: 0.0043864604661034215\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0193049942056338\n",
      "Average test loss: 0.004355364418485099\n",
      "Epoch 143/300\n",
      "Average training loss: 0.019299334516127906\n",
      "Average test loss: 0.00431400688447886\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01928984100288815\n",
      "Average test loss: 0.004372348789539602\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01927399495078458\n",
      "Average test loss: 0.004413268191532956\n",
      "Epoch 146/300\n",
      "Average training loss: 0.019275739970306554\n",
      "Average test loss: 0.004325998455079066\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01925424002110958\n",
      "Average test loss: 0.004292443030824264\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01924730447265837\n",
      "Average test loss: 0.004415050376620558\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019246148861116832\n",
      "Average test loss: 0.004303252689126465\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019224250627888573\n",
      "Average test loss: 0.004360478900372982\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0192117364158233\n",
      "Average test loss: 0.004523388215651115\n",
      "Epoch 152/300\n",
      "Average training loss: 0.019204136515657107\n",
      "Average test loss: 0.0043398081333272985\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019203361118833223\n",
      "Average test loss: 0.004377001867112186\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01918360556082593\n",
      "Average test loss: 0.0043174830675125125\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01917350702153312\n",
      "Average test loss: 0.004392735815296571\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019155865389439794\n",
      "Average test loss: 0.004319651444546051\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01915687549776501\n",
      "Average test loss: 0.004337245084552301\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019150920925868883\n",
      "Average test loss: 0.004391836459024085\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01914495199918747\n",
      "Average test loss: 0.004404734761350684\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01913964816265636\n",
      "Average test loss: 0.004307938788500097\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019115626419583958\n",
      "Average test loss: 0.004394693152772056\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019111825578742556\n",
      "Average test loss: 0.0043110432682765855\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01909537108242512\n",
      "Average test loss: 0.004333897314551804\n",
      "Epoch 164/300\n",
      "Average training loss: 0.019105081326431697\n",
      "Average test loss: 0.004449968873419695\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019082560343874826\n",
      "Average test loss: 0.004298848849203851\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019069327579604254\n",
      "Average test loss: 0.004388147949758503\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019065497249364852\n",
      "Average test loss: 0.004460903121365442\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019077531130777464\n",
      "Average test loss: 0.004487426672130823\n",
      "Epoch 169/300\n",
      "Average training loss: 0.019053779720432228\n",
      "Average test loss: 0.004368026874752508\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01904337685969141\n",
      "Average test loss: 0.004369364762057861\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019030163660645485\n",
      "Average test loss: 0.004383020027023223\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019030626045333014\n",
      "Average test loss: 0.004347031386155221\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019015600285596318\n",
      "Average test loss: 0.00448320292847024\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019018995003567803\n",
      "Average test loss: 0.004533847366356187\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019012584244211515\n",
      "Average test loss: 0.0043520876057446005\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01899480510254701\n",
      "Average test loss: 0.004559598587453365\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018977800399065017\n",
      "Average test loss: 0.004455192626350456\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018977546411255996\n",
      "Average test loss: 0.004441674463243948\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0189647725539075\n",
      "Average test loss: 0.0042965369265940455\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01896488769021299\n",
      "Average test loss: 0.004410043208963341\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018968162789940835\n",
      "Average test loss: 0.0044520594775676724\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018940188166168\n",
      "Average test loss: 0.004427527971565723\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018940186738967895\n",
      "Average test loss: 0.004367060341354873\n",
      "Epoch 184/300\n",
      "Average training loss: 0.018938804926143752\n",
      "Average test loss: 0.004466774182394147\n",
      "Epoch 185/300\n",
      "Average training loss: 0.018935604602098467\n",
      "Average test loss: 0.004383833371723692\n",
      "Epoch 186/300\n",
      "Average training loss: 0.018919319367243184\n",
      "Average test loss: 0.004461165335857206\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018899293467402458\n",
      "Average test loss: 0.004419373311930232\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01889980363845825\n",
      "Average test loss: 0.004391900622389383\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01889618412984742\n",
      "Average test loss: 0.004397438737667269\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018884751031796137\n",
      "Average test loss: 0.004457925575888819\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018886642475922904\n",
      "Average test loss: 0.004369327418506146\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018882348471217687\n",
      "Average test loss: 0.004499812060346206\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018864202765127024\n",
      "Average test loss: 0.0044222646984789105\n",
      "Epoch 194/300\n",
      "Average training loss: 0.018857954248785974\n",
      "Average test loss: 0.004405151219831573\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01884945104022821\n",
      "Average test loss: 0.004442023401872979\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01884571722149849\n",
      "Average test loss: 0.004393821263685822\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01883877296745777\n",
      "Average test loss: 0.004338428358236949\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01882998221864303\n",
      "Average test loss: 0.004484347213473585\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018834036932223373\n",
      "Average test loss: 0.004448032127900256\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018823470753100185\n",
      "Average test loss: 0.004387133484913243\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01881232300400734\n",
      "Average test loss: 0.004411065500229597\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018796550169587135\n",
      "Average test loss: 0.004490127931038539\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01881475687523683\n",
      "Average test loss: 0.004458587968515025\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01879754856394397\n",
      "Average test loss: 0.004434783466988139\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01878140498863326\n",
      "Average test loss: 0.004437470463828908\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018793478262093332\n",
      "Average test loss: 0.004407214738428593\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0187700804538197\n",
      "Average test loss: 0.0044383871269722775\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018764264300465582\n",
      "Average test loss: 0.004444051359262732\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018759718134999274\n",
      "Average test loss: 0.004377036680570907\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0187625895705488\n",
      "Average test loss: 0.004479837424639198\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01874312736093998\n",
      "Average test loss: 0.00448691077488992\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018735785212781693\n",
      "Average test loss: 0.004516060825851228\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018735796190798282\n",
      "Average test loss: 0.004507485119005044\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018728742990228864\n",
      "Average test loss: 0.004354789913114574\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01872089096903801\n",
      "Average test loss: 0.004464641048676438\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01872258957889345\n",
      "Average test loss: 0.004606658053066996\n",
      "Epoch 217/300\n",
      "Average training loss: 0.018701804806788763\n",
      "Average test loss: 0.004441819973496927\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018706176597211095\n",
      "Average test loss: 0.004463564460476239\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01870963361031479\n",
      "Average test loss: 0.0043792825324667824\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018699077712992826\n",
      "Average test loss: 0.004568140861681766\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018680521657069526\n",
      "Average test loss: 0.004391353329850568\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018678285314804977\n",
      "Average test loss: 0.00452546919364896\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018682753892408477\n",
      "Average test loss: 0.004459966274599234\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018670418661501674\n",
      "Average test loss: 0.004527284435100026\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01867824552787675\n",
      "Average test loss: 0.004480267352114121\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01865897035598755\n",
      "Average test loss: 0.00448078184409274\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018654795898331536\n",
      "Average test loss: 0.004444511224412256\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01865834892458386\n",
      "Average test loss: 0.004656777049932215\n",
      "Epoch 229/300\n",
      "Average training loss: 0.018644198331567975\n",
      "Average test loss: 0.004395369860447115\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01863710815211137\n",
      "Average test loss: 0.004496973502760132\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018634264891346296\n",
      "Average test loss: 0.00447253137992488\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01863474835952123\n",
      "Average test loss: 0.004448594252682394\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018627465527918604\n",
      "Average test loss: 0.004472493759045998\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018623456731438635\n",
      "Average test loss: 0.004527221850636933\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018618476536538867\n",
      "Average test loss: 0.00440126928438743\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018609712302684784\n",
      "Average test loss: 0.004351476932565371\n",
      "Epoch 237/300\n",
      "Average training loss: 0.018607627428240247\n",
      "Average test loss: 0.004485552835795614\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01859889113240772\n",
      "Average test loss: 0.004538002297282219\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018600260976288054\n",
      "Average test loss: 0.004444209544195069\n",
      "Epoch 240/300\n",
      "Average training loss: 0.018588510789804988\n",
      "Average test loss: 0.00462660223369797\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01857288414819373\n",
      "Average test loss: 0.004427348100890716\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018571164578199385\n",
      "Average test loss: 0.004375486027035448\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01857671753813823\n",
      "Average test loss: 0.004466147233835525\n",
      "Epoch 244/300\n",
      "Average training loss: 0.018578281783395344\n",
      "Average test loss: 0.004446736958498756\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01855612321777476\n",
      "Average test loss: 0.0044899236909631225\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018561304826703335\n",
      "Average test loss: 0.0045141677214867535\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018551895558834075\n",
      "Average test loss: 0.00445878690150049\n",
      "Epoch 248/300\n",
      "Average training loss: 0.018533937444289526\n",
      "Average test loss: 0.004449080286340581\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01854604051510493\n",
      "Average test loss: 0.004529027494705386\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01854801536268658\n",
      "Average test loss: 0.004490426557759444\n",
      "Epoch 251/300\n",
      "Average training loss: 0.018543163816134134\n",
      "Average test loss: 0.004450986462748713\n",
      "Epoch 252/300\n",
      "Average training loss: 0.018526452920503086\n",
      "Average test loss: 0.004552549597703748\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01852622589468956\n",
      "Average test loss: 0.004462753867109617\n",
      "Epoch 254/300\n",
      "Average training loss: 0.018519847937756115\n",
      "Average test loss: 0.004414934559000863\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01851710123485989\n",
      "Average test loss: 0.004480031651962134\n",
      "Epoch 256/300\n",
      "Average training loss: 0.018503450375464227\n",
      "Average test loss: 0.0046214143555197455\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01850358178714911\n",
      "Average test loss: 0.004533032120515903\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018498749524354936\n",
      "Average test loss: 0.004489644843878018\n",
      "Epoch 259/300\n",
      "Average training loss: 0.018500595449573463\n",
      "Average test loss: 0.004637880529173547\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018491742549671068\n",
      "Average test loss: 0.004474600863953432\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01849529011713134\n",
      "Average test loss: 0.004475821274229222\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01849072142938773\n",
      "Average test loss: 0.00453003732032246\n",
      "Epoch 263/300\n",
      "Average training loss: 0.018479294846455257\n",
      "Average test loss: 0.0045187739125556415\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01847754587398635\n",
      "Average test loss: 0.004603338710549805\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018473051362567477\n",
      "Average test loss: 0.00458932854856054\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018462171677086088\n",
      "Average test loss: 0.004491164069622755\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018472530941168467\n",
      "Average test loss: 0.004456233910802338\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018470249532825417\n",
      "Average test loss: 0.004570698030706909\n",
      "Epoch 269/300\n",
      "Average training loss: 0.018452297783560223\n",
      "Average test loss: 0.004574588963141044\n",
      "Epoch 270/300\n",
      "Average training loss: 0.018446168886290658\n",
      "Average test loss: 0.004607578574162391\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018449155105484855\n",
      "Average test loss: 0.004532428482754363\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018447023529145453\n",
      "Average test loss: 0.0045507655365185605\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018432128628922833\n",
      "Average test loss: 0.004535185298158063\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01843590479426914\n",
      "Average test loss: 0.004421791200422578\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018424421437084674\n",
      "Average test loss: 0.004491381715983152\n",
      "Epoch 276/300\n",
      "Average training loss: 0.018419733059902986\n",
      "Average test loss: 0.004401461892243889\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018431464263134533\n",
      "Average test loss: 0.004488345052219099\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018409006746278867\n",
      "Average test loss: 0.004433015309481157\n",
      "Epoch 279/300\n",
      "Average training loss: 0.018417106345295906\n",
      "Average test loss: 0.0046812942355043356\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018423971874846352\n",
      "Average test loss: 0.004383713780384925\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018400296926498413\n",
      "Average test loss: 0.004540688186883926\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018396765697333547\n",
      "Average test loss: 0.004461183023535543\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0183889529771275\n",
      "Average test loss: 0.004495512363811334\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01839475352731016\n",
      "Average test loss: 0.004514613562160068\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01839180860751205\n",
      "Average test loss: 0.0043574178512725565\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018369761385851436\n",
      "Average test loss: 0.004560384118101663\n",
      "Epoch 287/300\n",
      "Average training loss: 0.018391127733720675\n",
      "Average test loss: 0.004613922513193554\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01837480127894216\n",
      "Average test loss: 0.004490524065784282\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01838515250219239\n",
      "Average test loss: 0.004477688823102249\n",
      "Epoch 290/300\n",
      "Average training loss: 0.018360966813233164\n",
      "Average test loss: 0.004496717436446084\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018358914482924673\n",
      "Average test loss: 0.00448870901349518\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01837424500948853\n",
      "Average test loss: 0.004553171695106559\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01837091026041243\n",
      "Average test loss: 0.004656533982190821\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018351868814892237\n",
      "Average test loss: 0.004487458212715056\n",
      "Epoch 295/300\n",
      "Average training loss: 0.018378509914709462\n",
      "Average test loss: 0.004464364118874073\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01835059819287724\n",
      "Average test loss: 0.004572947596510252\n",
      "Epoch 297/300\n",
      "Average training loss: 0.018351129149397214\n",
      "Average test loss: 0.004471871556507217\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018326731036106745\n",
      "Average test loss: 0.004416898665949702\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01833443499604861\n",
      "Average test loss: 0.004479671308563815\n",
      "Epoch 300/300\n",
      "Average training loss: 0.018341299772262572\n",
      "Average test loss: 0.0045080924510127966\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.056065132626228864\n",
      "Average test loss: 0.0045486694566077655\n",
      "Epoch 2/300\n",
      "Average training loss: 0.022530476704239844\n",
      "Average test loss: 0.0044960358155270415\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02145424501432313\n",
      "Average test loss: 0.004205266700850593\n",
      "Epoch 4/300\n",
      "Average training loss: 0.020906611272030407\n",
      "Average test loss: 0.004096514631476667\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02051484750211239\n",
      "Average test loss: 0.004012802605413728\n",
      "Epoch 6/300\n",
      "Average training loss: 0.020187101976739036\n",
      "Average test loss: 0.003940608603672849\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019898651296893756\n",
      "Average test loss: 0.003927056727310022\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01964407152103053\n",
      "Average test loss: 0.0038606760762631893\n",
      "Epoch 9/300\n",
      "Average training loss: 0.019436342492699623\n",
      "Average test loss: 0.0038221015652848615\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01924217871328195\n",
      "Average test loss: 0.003766051682126191\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01904141847623719\n",
      "Average test loss: 0.0037300044256779884\n",
      "Epoch 12/300\n",
      "Average training loss: 0.018874635926551288\n",
      "Average test loss: 0.003725207111073865\n",
      "Epoch 13/300\n",
      "Average training loss: 0.018729699758191904\n",
      "Average test loss: 0.0036744384072307083\n",
      "Epoch 14/300\n",
      "Average training loss: 0.018607396822836665\n",
      "Average test loss: 0.0036521083290378255\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018491365684403314\n",
      "Average test loss: 0.0036520856405711836\n",
      "Epoch 16/300\n",
      "Average training loss: 0.018396016779873105\n",
      "Average test loss: 0.0036168881921718517\n",
      "Epoch 17/300\n",
      "Average training loss: 0.018283347424533632\n",
      "Average test loss: 0.00360664912354615\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0182086050187548\n",
      "Average test loss: 0.0035912451032135222\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01810646739560697\n",
      "Average test loss: 0.0036589196651346154\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018043811662329566\n",
      "Average test loss: 0.003562056404434972\n",
      "Epoch 21/300\n",
      "Average training loss: 0.017956247742805215\n",
      "Average test loss: 0.003536322496831417\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01790625975529353\n",
      "Average test loss: 0.003523182932701376\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017825749956899218\n",
      "Average test loss: 0.003532876236881647\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01776742853389846\n",
      "Average test loss: 0.0035231900142712724\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017724333978361553\n",
      "Average test loss: 0.003539208891491095\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017669624858432344\n",
      "Average test loss: 0.0034948856153835853\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017617602304451994\n",
      "Average test loss: 0.003513836659077141\n",
      "Epoch 28/300\n",
      "Average training loss: 0.017574177790019248\n",
      "Average test loss: 0.003503296106018954\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017525676717360814\n",
      "Average test loss: 0.0034964619357552794\n",
      "Epoch 30/300\n",
      "Average training loss: 0.017492083552810882\n",
      "Average test loss: 0.0034820377820481857\n",
      "Epoch 31/300\n",
      "Average training loss: 0.017443002733919355\n",
      "Average test loss: 0.003475815280340612\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017410260702172914\n",
      "Average test loss: 0.0034668515506717894\n",
      "Epoch 33/300\n",
      "Average training loss: 0.017376344642705387\n",
      "Average test loss: 0.0034677454243517585\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017344520116017923\n",
      "Average test loss: 0.0035034136043654547\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017304603243867556\n",
      "Average test loss: 0.003465736499884062\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017269443568256167\n",
      "Average test loss: 0.003503864194369978\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01724006609287527\n",
      "Average test loss: 0.003453873365703556\n",
      "Epoch 38/300\n",
      "Average training loss: 0.017197247127691905\n",
      "Average test loss: 0.003446604777748386\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017177379892932044\n",
      "Average test loss: 0.0034501433204859495\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01714444858994749\n",
      "Average test loss: 0.0034706247347510524\n",
      "Epoch 41/300\n",
      "Average training loss: 0.017117543218864335\n",
      "Average test loss: 0.003431936333576838\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017090569757752947\n",
      "Average test loss: 0.003429148146054811\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01706230354309082\n",
      "Average test loss: 0.003445161858987477\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0170362302992079\n",
      "Average test loss: 0.0034365775926659503\n",
      "Epoch 45/300\n",
      "Average training loss: 0.017017757864048084\n",
      "Average test loss: 0.0034414851210183566\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016973374909824795\n",
      "Average test loss: 0.003420442177603642\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01696106363336245\n",
      "Average test loss: 0.0034229709547426964\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016921056444446247\n",
      "Average test loss: 0.003480757847842243\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01690533202389876\n",
      "Average test loss: 0.0034404581849359806\n",
      "Epoch 50/300\n",
      "Average training loss: 0.016884163030319744\n",
      "Average test loss: 0.003439624466208948\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01684917102422979\n",
      "Average test loss: 0.003431886673801475\n",
      "Epoch 52/300\n",
      "Average training loss: 0.016824836389886008\n",
      "Average test loss: 0.0034121431244744193\n",
      "Epoch 53/300\n",
      "Average training loss: 0.016803010273310874\n",
      "Average test loss: 0.0034324687622073625\n",
      "Epoch 54/300\n",
      "Average training loss: 0.016782741443978417\n",
      "Average test loss: 0.0034441079878144796\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016768536653783587\n",
      "Average test loss: 0.0034503806779781977\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016730405880345238\n",
      "Average test loss: 0.00345078587366475\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0167231545365519\n",
      "Average test loss: 0.0034870372576018176\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0166762900998195\n",
      "Average test loss: 0.0034442758067614503\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01666489384075006\n",
      "Average test loss: 0.0034314243509951563\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01663557819525401\n",
      "Average test loss: 0.0034368410607179007\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016618584098087415\n",
      "Average test loss: 0.0034197532356613214\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016607657493816482\n",
      "Average test loss: 0.0034352738166020975\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016573837477299902\n",
      "Average test loss: 0.0034907573798878327\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016565669967896407\n",
      "Average test loss: 0.003479375676355428\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01651795962949594\n",
      "Average test loss: 0.003426696229726076\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0165137061956856\n",
      "Average test loss: 0.0034758977703750133\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01648802961740229\n",
      "Average test loss: 0.003473802956442038\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016461218766868114\n",
      "Average test loss: 0.0034473729551666313\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016456202859679858\n",
      "Average test loss: 0.003482280694569151\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01643032084405422\n",
      "Average test loss: 0.003463881188796626\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01640810410181681\n",
      "Average test loss: 0.0034504036787483426\n",
      "Epoch 72/300\n",
      "Average training loss: 0.016389014237456853\n",
      "Average test loss: 0.003467226104603873\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016359571766522196\n",
      "Average test loss: 0.003446350504954656\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016340852853324678\n",
      "Average test loss: 0.0034641370380090342\n",
      "Epoch 75/300\n",
      "Average training loss: 0.016320766362879012\n",
      "Average test loss: 0.0034343090332630607\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01630787940737274\n",
      "Average test loss: 0.0034420979453457724\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016282636820442148\n",
      "Average test loss: 0.0034666024374051226\n",
      "Epoch 78/300\n",
      "Average training loss: 0.016278899358378517\n",
      "Average test loss: 0.0034447334679878422\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01625996867650085\n",
      "Average test loss: 0.0034670806183583207\n",
      "Epoch 80/300\n",
      "Average training loss: 0.016235703801943197\n",
      "Average test loss: 0.0034647218307687177\n",
      "Epoch 81/300\n",
      "Average training loss: 0.016211205541259713\n",
      "Average test loss: 0.0034808447832862534\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01619958665801419\n",
      "Average test loss: 0.003563477805505196\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01616757629646195\n",
      "Average test loss: 0.0034482723029537333\n",
      "Epoch 84/300\n",
      "Average training loss: 0.016155510976910592\n",
      "Average test loss: 0.003480882591671414\n",
      "Epoch 85/300\n",
      "Average training loss: 0.016139959136645\n",
      "Average test loss: 0.003468405133734147\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016122117002805074\n",
      "Average test loss: 0.0034645861561099687\n",
      "Epoch 87/300\n",
      "Average training loss: 0.016111232504248618\n",
      "Average test loss: 0.0034992596262858975\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01608747781978713\n",
      "Average test loss: 0.003598812353693777\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016083782545394366\n",
      "Average test loss: 0.0035543272884355653\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01604730191330115\n",
      "Average test loss: 0.0035380337277634275\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016041008774605063\n",
      "Average test loss: 0.0034844898250367905\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01602691086133321\n",
      "Average test loss: 0.0034640228466855153\n",
      "Epoch 93/300\n",
      "Average training loss: 0.016008492951591808\n",
      "Average test loss: 0.0035476445932355193\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015991049636569287\n",
      "Average test loss: 0.0034888068423089055\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015976795583963393\n",
      "Average test loss: 0.003494058258831501\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01596107728448179\n",
      "Average test loss: 0.0035039235583196084\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015946474549671016\n",
      "Average test loss: 0.0034823076309015355\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015928580169876416\n",
      "Average test loss: 0.0034545522226641576\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01590525497827265\n",
      "Average test loss: 0.00354391367526518\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01590206867456436\n",
      "Average test loss: 0.003517205007788208\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01587493894249201\n",
      "Average test loss: 0.0035084271006700063\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015860438155631224\n",
      "Average test loss: 0.0035166952359593578\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01587015861272812\n",
      "Average test loss: 0.0034488127823505137\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015846032882730166\n",
      "Average test loss: 0.0034728313947303426\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015825765185058117\n",
      "Average test loss: 0.0035482544729279145\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015807236618465847\n",
      "Average test loss: 0.0034999324186808533\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015792253078685866\n",
      "Average test loss: 0.003561846101035674\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015787148642871115\n",
      "Average test loss: 0.0035288437956737147\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015776833434899648\n",
      "Average test loss: 0.003486499700281355\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015748419218593174\n",
      "Average test loss: 0.003498207830099596\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015741408507029216\n",
      "Average test loss: 0.003564059723375572\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015718424515591727\n",
      "Average test loss: 0.0034778794954634373\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01571286274658309\n",
      "Average test loss: 0.003565954052325752\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015707807837261095\n",
      "Average test loss: 0.0034595504299634035\n",
      "Epoch 115/300\n",
      "Average training loss: 0.015699167316158613\n",
      "Average test loss: 0.0035611831959750915\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015677352777785727\n",
      "Average test loss: 0.0035408203167219955\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015664089296427038\n",
      "Average test loss: 0.003592001340455479\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01566098225613435\n",
      "Average test loss: 0.0035518713988777666\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015638101260695193\n",
      "Average test loss: 0.0035848173579821983\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01564569893396563\n",
      "Average test loss: 0.003520020223326153\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015612104946540462\n",
      "Average test loss: 0.0036718990308129127\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015596505688296423\n",
      "Average test loss: 0.0035608347919252182\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015596883014672332\n",
      "Average test loss: 0.0036471996584700214\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015574963963694042\n",
      "Average test loss: 0.0035321211193998655\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015573800685505072\n",
      "Average test loss: 0.0035400013195143808\n",
      "Epoch 126/300\n",
      "Average training loss: 0.015559374322493871\n",
      "Average test loss: 0.003542156239350637\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015558306030929088\n",
      "Average test loss: 0.003584086564886901\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01553674355149269\n",
      "Average test loss: 0.0035703452008052005\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015540479919148815\n",
      "Average test loss: 0.003572026600233383\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01550955513616403\n",
      "Average test loss: 0.0035792093123826717\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01548947491331233\n",
      "Average test loss: 0.003555259077085389\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015499140704671541\n",
      "Average test loss: 0.003671724598440859\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015492949013908705\n",
      "Average test loss: 0.0036455677135123145\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015463625226583746\n",
      "Average test loss: 0.0035924925752398042\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015458550905187924\n",
      "Average test loss: 0.0035329858331630626\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015451645894183052\n",
      "Average test loss: 0.0036324317873352103\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015444324900706609\n",
      "Average test loss: 0.003558451614860031\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01542942054818074\n",
      "Average test loss: 0.0036394907923208344\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01541501566602124\n",
      "Average test loss: 0.003657346125692129\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015421197975675266\n",
      "Average test loss: 0.003765214635680119\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01540643451942338\n",
      "Average test loss: 0.0036375705043060914\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015396952538026703\n",
      "Average test loss: 0.003596565250514282\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01538687732650174\n",
      "Average test loss: 0.0035888807040949664\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015370634135272768\n",
      "Average test loss: 0.0035060482803318236\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015374835025933054\n",
      "Average test loss: 0.0036409993627005153\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015357273116707803\n",
      "Average test loss: 0.003551381094795134\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01535240415069792\n",
      "Average test loss: 0.0036039444980108075\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015350497104227543\n",
      "Average test loss: 0.0035581184782915646\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015320162736707263\n",
      "Average test loss: 0.0036103897957752147\n",
      "Epoch 150/300\n",
      "Average training loss: 0.015316629764106538\n",
      "Average test loss: 0.0036661614610089197\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015325928234391743\n",
      "Average test loss: 0.0035512726625634566\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01530551697810491\n",
      "Average test loss: 0.0035638747409813933\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015303435208068953\n",
      "Average test loss: 0.0035586800430383947\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015283230415648883\n",
      "Average test loss: 0.003671155066953765\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015278429233365589\n",
      "Average test loss: 0.0035771268101202117\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015277402134405243\n",
      "Average test loss: 0.0037483364625109567\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01527380015535487\n",
      "Average test loss: 0.003602495609472195\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015253164069520103\n",
      "Average test loss: 0.003604016345408228\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01524266231722302\n",
      "Average test loss: 0.0035731778365249434\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015247903174824185\n",
      "Average test loss: 0.0035636304300278427\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01523621674378713\n",
      "Average test loss: 0.0036500521641638544\n",
      "Epoch 162/300\n",
      "Average training loss: 0.015200024884608056\n",
      "Average test loss: 0.0035526799824502735\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015221408120459981\n",
      "Average test loss: 0.003646154675218794\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015214526598652203\n",
      "Average test loss: 0.0035728447855346733\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015215384357505374\n",
      "Average test loss: 0.0036732943252556854\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015188544159134228\n",
      "Average test loss: 0.0036176412858896787\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015190189422832595\n",
      "Average test loss: 0.0036478349829299583\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015193937121993966\n",
      "Average test loss: 0.003669670618035727\n",
      "Epoch 169/300\n",
      "Average training loss: 0.015173178951773378\n",
      "Average test loss: 0.0036732969499296613\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015162632658249802\n",
      "Average test loss: 0.003553787902411487\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015164531795514954\n",
      "Average test loss: 0.003598251789704793\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015159079367915789\n",
      "Average test loss: 0.003665520389046934\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015148926690220834\n",
      "Average test loss: 0.0036322279733916126\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015139652109808392\n",
      "Average test loss: 0.003623113740649488\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015133471232321527\n",
      "Average test loss: 0.003586332243039376\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01512272425658173\n",
      "Average test loss: 0.003670896292146709\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01511579753127363\n",
      "Average test loss: 0.0035896075636976297\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015110686111781332\n",
      "Average test loss: 0.0037001966658151814\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015107451067202621\n",
      "Average test loss: 0.003628342155367136\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01509192869067192\n",
      "Average test loss: 0.0035915352108163968\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015092757577697437\n",
      "Average test loss: 0.003629178062495258\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015090262757407295\n",
      "Average test loss: 0.0036199105802095597\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015089031654927466\n",
      "Average test loss: 0.003617965568270948\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015068168710503314\n",
      "Average test loss: 0.0036667990123646125\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015050128089057075\n",
      "Average test loss: 0.0035598587439292007\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015067722119390965\n",
      "Average test loss: 0.0037154632899910212\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015043775480654505\n",
      "Average test loss: 0.0036151616556776896\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015054770439035362\n",
      "Average test loss: 0.0036957743449343577\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015052363896535503\n",
      "Average test loss: 0.003644272422211038\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01501943801343441\n",
      "Average test loss: 0.0036108646810882623\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015031156044039462\n",
      "Average test loss: 0.003626935842136542\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015020994401640363\n",
      "Average test loss: 0.0036096716922604374\n",
      "Epoch 193/300\n",
      "Average training loss: 0.015048033911320898\n",
      "Average test loss: 0.003765779670741823\n",
      "Epoch 194/300\n",
      "Average training loss: 0.015001825490759478\n",
      "Average test loss: 0.0036400395141293605\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015015318216549025\n",
      "Average test loss: 0.0036471408450355135\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015000624078015487\n",
      "Average test loss: 0.003672107736269633\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014989451158377859\n",
      "Average test loss: 0.0038151134865151513\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015001781991786428\n",
      "Average test loss: 0.0036509972202281157\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014985631578498417\n",
      "Average test loss: 0.0037141987660692798\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014971831576691734\n",
      "Average test loss: 0.003714137227791879\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014972175211542182\n",
      "Average test loss: 0.0036363364712645612\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01496507448123561\n",
      "Average test loss: 0.0036121582241935863\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014970054718355337\n",
      "Average test loss: 0.0036427265262852114\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014945614019201862\n",
      "Average test loss: 0.003724151459005144\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01495982698765066\n",
      "Average test loss: 0.0036073808177477782\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014961814163459672\n",
      "Average test loss: 0.003603191684310635\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014944818077815903\n",
      "Average test loss: 0.00358830806819929\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014934235899812645\n",
      "Average test loss: 0.003717276521234049\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014916000886923736\n",
      "Average test loss: 0.003677909287934502\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01493296427031358\n",
      "Average test loss: 0.0037462185410161815\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014928096279501915\n",
      "Average test loss: 0.0035701000806358124\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014931393882466688\n",
      "Average test loss: 0.0036001431840575406\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014902961356772318\n",
      "Average test loss: 0.0036820905903975167\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01490458342929681\n",
      "Average test loss: 0.003648184930698739\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014896706448660956\n",
      "Average test loss: 0.003857699096823732\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014898301800092061\n",
      "Average test loss: 0.003713879639489783\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014908276942869027\n",
      "Average test loss: 0.0036862626348932582\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014900320765044954\n",
      "Average test loss: 0.003811337538063526\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014877916212711069\n",
      "Average test loss: 0.0037093258367644415\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014874773095051447\n",
      "Average test loss: 0.003594324537863334\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014878380803598297\n",
      "Average test loss: 0.0035837081211308637\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014853696852094598\n",
      "Average test loss: 0.0037055432041072185\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014846978407767084\n",
      "Average test loss: 0.003719192494534784\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014867243819766575\n",
      "Average test loss: 0.0036425123589320313\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014875723562306828\n",
      "Average test loss: 0.0035835852455347776\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014848481898506483\n",
      "Average test loss: 0.003748856596648693\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014839406616157956\n",
      "Average test loss: 0.003635475870842735\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014845372534460492\n",
      "Average test loss: 0.0036868734256260923\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014837285441656908\n",
      "Average test loss: 0.0036287139635533096\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014842714143296083\n",
      "Average test loss: 0.0036193979926821256\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014822437685396936\n",
      "Average test loss: 0.0037311530895531176\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014829987259374724\n",
      "Average test loss: 0.0036819838368230395\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014819065369665622\n",
      "Average test loss: 0.003714454590446419\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014821494387255775\n",
      "Average test loss: 0.0036469050633410613\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014806952063408162\n",
      "Average test loss: 0.003642950185471111\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014812581338816218\n",
      "Average test loss: 0.003827851900209983\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014800230658716626\n",
      "Average test loss: 0.0036846546760449807\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01479810209158394\n",
      "Average test loss: 0.003752586665459805\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014795457428528202\n",
      "Average test loss: 0.003684279453009367\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014791855742534001\n",
      "Average test loss: 0.003652393494008316\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014793513268232346\n",
      "Average test loss: 0.0037336041273342236\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01478272471908066\n",
      "Average test loss: 0.00368486515349812\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014784510552883148\n",
      "Average test loss: 0.003625669489718146\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014786017237438095\n",
      "Average test loss: 0.003714677733058731\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014778835208051734\n",
      "Average test loss: 0.003672105983934469\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014780065275728702\n",
      "Average test loss: 0.00360171079594228\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014751223700741927\n",
      "Average test loss: 0.003669277075264189\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01476455787403716\n",
      "Average test loss: 0.0037319678993274767\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014748735631505649\n",
      "Average test loss: 0.0037364677608840995\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01476379224161307\n",
      "Average test loss: 0.0036812961830033197\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014749882808162106\n",
      "Average test loss: 0.0035772427479839985\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014738321195046106\n",
      "Average test loss: 0.0038014198160833783\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01474269084384044\n",
      "Average test loss: 0.0037460040243135558\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014737643897533417\n",
      "Average test loss: 0.003705269201348225\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014725590873095724\n",
      "Average test loss: 0.0037544356485207874\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01472317226893372\n",
      "Average test loss: 0.0037836852241307497\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014720741710729068\n",
      "Average test loss: 0.0036875307713117866\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014723904357188278\n",
      "Average test loss: 0.0037299191442628703\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01471546171605587\n",
      "Average test loss: 0.003780358052915997\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014705167432626088\n",
      "Average test loss: 0.003717018806686004\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01470376808817188\n",
      "Average test loss: 0.003849403035723501\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014708427071571351\n",
      "Average test loss: 0.003671515521282951\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014710749535097016\n",
      "Average test loss: 0.0036415542831851376\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01469032793160942\n",
      "Average test loss: 0.0037160732510189217\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014699803031980992\n",
      "Average test loss: 0.003701965835566322\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014698307492666774\n",
      "Average test loss: 0.0036277395006683137\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014693217303189966\n",
      "Average test loss: 0.003689423227061828\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014693799971706337\n",
      "Average test loss: 0.003789348755859666\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014676632030142679\n",
      "Average test loss: 0.0037608037926256657\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014674687412877878\n",
      "Average test loss: 0.003798193554083506\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014679155914319886\n",
      "Average test loss: 0.0037088985211319392\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014680339002774821\n",
      "Average test loss: 0.003778896489284105\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014671085155672498\n",
      "Average test loss: 0.0038031301266617246\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014662806370192103\n",
      "Average test loss: 0.0036377157748987278\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014672177215417226\n",
      "Average test loss: 0.0036603546334016655\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014668658354216152\n",
      "Average test loss: 0.0037805519302686057\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01465145653900173\n",
      "Average test loss: 0.003638040609036883\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014653556447062227\n",
      "Average test loss: 0.0036938438680436878\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014651345923542976\n",
      "Average test loss: 0.003668167086939017\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014650517923136552\n",
      "Average test loss: 0.0037739873555385404\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014647839318546983\n",
      "Average test loss: 0.0036846255239927107\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014628532914237843\n",
      "Average test loss: 0.0036546197136243185\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014645494371652603\n",
      "Average test loss: 0.0037604629202849333\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014641242692040073\n",
      "Average test loss: 0.003746191262991892\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014633576379881965\n",
      "Average test loss: 0.0037530617012331885\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01462945731729269\n",
      "Average test loss: 0.003697183089537753\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014622841387159295\n",
      "Average test loss: 0.003699665990140703\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014621538920534981\n",
      "Average test loss: 0.00365794320921931\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014605938577817546\n",
      "Average test loss: 0.0037017709467973975\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014623760547075006\n",
      "Average test loss: 0.003681477946953641\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014614501224623787\n",
      "Average test loss: 0.0036820204369723795\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014609906608859698\n",
      "Average test loss: 0.003838543183894621\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014604526239964697\n",
      "Average test loss: 0.003686100544201003\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014605205422474279\n",
      "Average test loss: 0.0037475790288299324\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014608130711648199\n",
      "Average test loss: 0.003819191847410467\n",
      "Epoch 296/300\n",
      "Average training loss: 0.014609326807989015\n",
      "Average test loss: 0.0037907031633787683\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014601484820246697\n",
      "Average test loss: 0.003709140123385522\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014601336452696059\n",
      "Average test loss: 0.0036951468067450656\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014591610807511542\n",
      "Average test loss: 0.003666427678739031\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014572090055379603\n",
      "Average test loss: 0.0036747434760133427\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05208175712161594\n",
      "Average test loss: 0.004114924902717272\n",
      "Epoch 2/300\n",
      "Average training loss: 0.020311756925450432\n",
      "Average test loss: 0.003896180424425337\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019189742291967075\n",
      "Average test loss: 0.003666691492829058\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018512599368062283\n",
      "Average test loss: 0.003525503900109066\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01798274429142475\n",
      "Average test loss: 0.0034768708615253367\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017541731503274705\n",
      "Average test loss: 0.0033323255081971486\n",
      "Epoch 7/300\n",
      "Average training loss: 0.017178555252651374\n",
      "Average test loss: 0.0032690403247252107\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016871870970560445\n",
      "Average test loss: 0.003218923861988717\n",
      "Epoch 9/300\n",
      "Average training loss: 0.016599210689465203\n",
      "Average test loss: 0.003227176533598039\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01636296530233489\n",
      "Average test loss: 0.0031801387293057307\n",
      "Epoch 11/300\n",
      "Average training loss: 0.016156076187888783\n",
      "Average test loss: 0.0031714221665428742\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0159675194579694\n",
      "Average test loss: 0.0031668197212533816\n",
      "Epoch 13/300\n",
      "Average training loss: 0.015815023140774834\n",
      "Average test loss: 0.003008962776925829\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01565870821475983\n",
      "Average test loss: 0.0030255191893213324\n",
      "Epoch 15/300\n",
      "Average training loss: 0.015533959881299072\n",
      "Average test loss: 0.0032099691414170795\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015406696486804221\n",
      "Average test loss: 0.002967685422135724\n",
      "Epoch 17/300\n",
      "Average training loss: 0.015308326630128755\n",
      "Average test loss: 0.002951683643998371\n",
      "Epoch 18/300\n",
      "Average training loss: 0.01521596571058035\n",
      "Average test loss: 0.002922418361840149\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015119248100452953\n",
      "Average test loss: 0.002900006840626399\n",
      "Epoch 20/300\n",
      "Average training loss: 0.015034341995914777\n",
      "Average test loss: 0.00291916515553991\n",
      "Epoch 21/300\n",
      "Average training loss: 0.014971425392561489\n",
      "Average test loss: 0.002882926708087325\n",
      "Epoch 22/300\n",
      "Average training loss: 0.014897634257045057\n",
      "Average test loss: 0.0028932637218385933\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01484200557983584\n",
      "Average test loss: 0.0028750933216263853\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01477867129113939\n",
      "Average test loss: 0.0028854433993498485\n",
      "Epoch 25/300\n",
      "Average training loss: 0.014737034356428516\n",
      "Average test loss: 0.002848428670109974\n",
      "Epoch 26/300\n",
      "Average training loss: 0.014675233074360423\n",
      "Average test loss: 0.0028532159756869077\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01462198617309332\n",
      "Average test loss: 0.002834874538497792\n",
      "Epoch 28/300\n",
      "Average training loss: 0.014584940737320317\n",
      "Average test loss: 0.002824246692367726\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014536332820852597\n",
      "Average test loss: 0.002849833935499191\n",
      "Epoch 30/300\n",
      "Average training loss: 0.014505449332296848\n",
      "Average test loss: 0.0028156705420050356\n",
      "Epoch 31/300\n",
      "Average training loss: 0.014449919202261501\n",
      "Average test loss: 0.002796343006607559\n",
      "Epoch 32/300\n",
      "Average training loss: 0.014428649283117718\n",
      "Average test loss: 0.002820735884209474\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01439297447188033\n",
      "Average test loss: 0.0027922556112623875\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01433889074375232\n",
      "Average test loss: 0.002801412792255481\n",
      "Epoch 35/300\n",
      "Average training loss: 0.014316683558954133\n",
      "Average test loss: 0.0027991345582736864\n",
      "Epoch 36/300\n",
      "Average training loss: 0.014286742406586806\n",
      "Average test loss: 0.0027852704858200416\n",
      "Epoch 37/300\n",
      "Average training loss: 0.014250053393344085\n",
      "Average test loss: 0.002802496320671505\n",
      "Epoch 38/300\n",
      "Average training loss: 0.014227552502519555\n",
      "Average test loss: 0.002785588435828686\n",
      "Epoch 39/300\n",
      "Average training loss: 0.014190872465570768\n",
      "Average test loss: 0.002792158807317416\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014174148504932722\n",
      "Average test loss: 0.002767462648658289\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014140681589643161\n",
      "Average test loss: 0.002770917655279239\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014103465021484428\n",
      "Average test loss: 0.00278164267167449\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014099863259328736\n",
      "Average test loss: 0.0027826432080732453\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014055634703901079\n",
      "Average test loss: 0.0027875654072397286\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014040875319391488\n",
      "Average test loss: 0.0028344299437271223\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014014452296826574\n",
      "Average test loss: 0.002792028231960204\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0139809735260076\n",
      "Average test loss: 0.002793156608318289\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01396887923611535\n",
      "Average test loss: 0.002763566316001945\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01394389537225167\n",
      "Average test loss: 0.0027633501311971082\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013930639950765504\n",
      "Average test loss: 0.0027756682388070555\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013900355009569062\n",
      "Average test loss: 0.0027594324569735263\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013877018923560778\n",
      "Average test loss: 0.0027690431262469956\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013865029378897614\n",
      "Average test loss: 0.0027551984197149676\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013827429639796416\n",
      "Average test loss: 0.0027738617503394684\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013812164134449429\n",
      "Average test loss: 0.002810422345581982\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013792026412983735\n",
      "Average test loss: 0.0027879330026399757\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013792644312812222\n",
      "Average test loss: 0.0027867220160033967\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013748614264031251\n",
      "Average test loss: 0.00277970639264418\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013735614713695315\n",
      "Average test loss: 0.002970464247175389\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013712905095683204\n",
      "Average test loss: 0.0028592631046970686\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01369920040667057\n",
      "Average test loss: 0.002750727353617549\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013682890426781443\n",
      "Average test loss: 0.0028217730973329807\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013659421474569373\n",
      "Average test loss: 0.0027539353429650265\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013639990381896496\n",
      "Average test loss: 0.0028120165914297104\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013621779767175516\n",
      "Average test loss: 0.002797109564973248\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013613253811995189\n",
      "Average test loss: 0.0027826108199854693\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013581725228991773\n",
      "Average test loss: 0.0028122242126199935\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013570438290635745\n",
      "Average test loss: 0.002807973042751352\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013554617967042657\n",
      "Average test loss: 0.0027710522384279306\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013537042046586673\n",
      "Average test loss: 0.0027751830855591427\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01351758388015959\n",
      "Average test loss: 0.0027791111999087865\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013503408376541402\n",
      "Average test loss: 0.0028037833805299468\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013480564436978764\n",
      "Average test loss: 0.0028319553714245558\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013469699562423759\n",
      "Average test loss: 0.002766448159391681\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013456033241417673\n",
      "Average test loss: 0.0027898924965411424\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013430019741257031\n",
      "Average test loss: 0.0027947902542849383\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013420422945585516\n",
      "Average test loss: 0.0028096471244676248\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01340099918262826\n",
      "Average test loss: 0.002794909175278412\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01338223118086656\n",
      "Average test loss: 0.0028157142096509537\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013361644299493896\n",
      "Average test loss: 0.0028239753858910668\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01335807261450423\n",
      "Average test loss: 0.00283054474927485\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013336873044570288\n",
      "Average test loss: 0.0027784093665993875\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013323607853717273\n",
      "Average test loss: 0.002840228299092915\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013335714800490273\n",
      "Average test loss: 0.002798801959388786\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013294363051652908\n",
      "Average test loss: 0.002826713050612145\n",
      "Epoch 86/300\n",
      "Average training loss: 0.013278951851030191\n",
      "Average test loss: 0.0028685838834693034\n",
      "Epoch 87/300\n",
      "Average training loss: 0.013261659979820251\n",
      "Average test loss: 0.0027979452723844184\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01325425190312995\n",
      "Average test loss: 0.002781684233703547\n",
      "Epoch 89/300\n",
      "Average training loss: 0.013235181983974244\n",
      "Average test loss: 0.0028458641034861407\n",
      "Epoch 90/300\n",
      "Average training loss: 0.013221760336723593\n",
      "Average test loss: 0.002805371075661646\n",
      "Epoch 91/300\n",
      "Average training loss: 0.013216225125723414\n",
      "Average test loss: 0.002841976760576169\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01319630291726854\n",
      "Average test loss: 0.0027989120011528332\n",
      "Epoch 93/300\n",
      "Average training loss: 0.013185259415871568\n",
      "Average test loss: 0.0028145655451549422\n",
      "Epoch 94/300\n",
      "Average training loss: 0.013175306791232692\n",
      "Average test loss: 0.002808544104711877\n",
      "Epoch 95/300\n",
      "Average training loss: 0.013160972426335018\n",
      "Average test loss: 0.0028654240237341987\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013143219398955503\n",
      "Average test loss: 0.0027829366742322843\n",
      "Epoch 97/300\n",
      "Average training loss: 0.013136099076105488\n",
      "Average test loss: 0.0028326848114116326\n",
      "Epoch 98/300\n",
      "Average training loss: 0.013116610484818618\n",
      "Average test loss: 0.0028422226789924835\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01311695364697112\n",
      "Average test loss: 0.0028012738393412696\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013100213076505397\n",
      "Average test loss: 0.0028049836965898674\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013088003848989805\n",
      "Average test loss: 0.002863975647319522\n",
      "Epoch 102/300\n",
      "Average training loss: 0.013064079203539424\n",
      "Average test loss: 0.0028440939458087085\n",
      "Epoch 103/300\n",
      "Average training loss: 0.013060609068307612\n",
      "Average test loss: 0.0028243908809704914\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013060806929237313\n",
      "Average test loss: 0.002804157752336727\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013036814221077496\n",
      "Average test loss: 0.0028732046760204764\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013024019823306136\n",
      "Average test loss: 0.0028356909145497615\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013024282452132966\n",
      "Average test loss: 0.002840781597627534\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012994357911249001\n",
      "Average test loss: 0.0028945901828507582\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01298863724867503\n",
      "Average test loss: 0.0028384208486725887\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012984344995684094\n",
      "Average test loss: 0.00284959577375816\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01296006343430943\n",
      "Average test loss: 0.002860818015411496\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012962444158891837\n",
      "Average test loss: 0.0028381950077083374\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012948636580672529\n",
      "Average test loss: 0.0028873707060184744\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01293972660601139\n",
      "Average test loss: 0.003318597057627307\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012913630001246929\n",
      "Average test loss: 0.0029064806912922196\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012909501027729775\n",
      "Average test loss: 0.0028397265436748663\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012914836825595962\n",
      "Average test loss: 0.0028412020326488547\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012901886777745352\n",
      "Average test loss: 0.0028451049310258695\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012902128308183616\n",
      "Average test loss: 0.002877102883118722\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012878782716890176\n",
      "Average test loss: 0.0028553567056854566\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012868332906729646\n",
      "Average test loss: 0.002869977535059055\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0128435718032221\n",
      "Average test loss: 0.0030644646063447\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012849008777903187\n",
      "Average test loss: 0.002868449741664032\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012833462038801776\n",
      "Average test loss: 0.0028226834293454887\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012835918532477484\n",
      "Average test loss: 0.0028102741815770665\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012814326130681567\n",
      "Average test loss: 0.002819445310367478\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012819594547980361\n",
      "Average test loss: 0.0028460864935898117\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012807012043065495\n",
      "Average test loss: 0.0029126386408590606\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012806744748519526\n",
      "Average test loss: 0.0029466964323073626\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012777966597841845\n",
      "Average test loss: 0.002911307591944933\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012757233363058833\n",
      "Average test loss: 0.002939195078280237\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012766611498263147\n",
      "Average test loss: 0.0028468770637280413\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012754110902547836\n",
      "Average test loss: 0.00307093336350388\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012746180375417073\n",
      "Average test loss: 0.0028649786259565086\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012754895836114883\n",
      "Average test loss: 0.00293415770928065\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012735700246360566\n",
      "Average test loss: 0.003036702015954587\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012721266775495477\n",
      "Average test loss: 0.0028545519463304015\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012721707498033842\n",
      "Average test loss: 0.002872084801395734\n",
      "Epoch 139/300\n",
      "Average training loss: 0.012708685114979744\n",
      "Average test loss: 0.0029517200994822715\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012705820017390781\n",
      "Average test loss: 0.0029043911302255258\n",
      "Epoch 141/300\n",
      "Average training loss: 0.012689359390073353\n",
      "Average test loss: 0.002831304485392239\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0126800450583299\n",
      "Average test loss: 0.0028855069977127843\n",
      "Epoch 143/300\n",
      "Average training loss: 0.012663045491609308\n",
      "Average test loss: 0.0028671730595330397\n",
      "Epoch 144/300\n",
      "Average training loss: 0.012670257257090674\n",
      "Average test loss: 0.002854757370427251\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012686990984612042\n",
      "Average test loss: 0.002882159442951282\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012666042459507783\n",
      "Average test loss: 0.002961683061180843\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012646490964624616\n",
      "Average test loss: 0.0028689201881902087\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012633058982590834\n",
      "Average test loss: 0.0029168953262269496\n",
      "Epoch 149/300\n",
      "Average training loss: 0.012633923633231058\n",
      "Average test loss: 0.0029875421971082687\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012637101978891426\n",
      "Average test loss: 0.002917095396046837\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012615515132745107\n",
      "Average test loss: 0.0028839589526048967\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012618077529801262\n",
      "Average test loss: 0.002851366652175784\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012621346842911508\n",
      "Average test loss: 0.0029733059174484678\n",
      "Epoch 154/300\n",
      "Average training loss: 0.012595415356258551\n",
      "Average test loss: 0.0029688939512189892\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01258755346801546\n",
      "Average test loss: 0.002876042829619514\n",
      "Epoch 156/300\n",
      "Average training loss: 0.012585095480912262\n",
      "Average test loss: 0.002882661284257968\n",
      "Epoch 157/300\n",
      "Average training loss: 0.012586170024342007\n",
      "Average test loss: 0.002894523842467202\n",
      "Epoch 158/300\n",
      "Average training loss: 0.012566778455343512\n",
      "Average test loss: 0.002866450076922774\n",
      "Epoch 159/300\n",
      "Average training loss: 0.012567074648208089\n",
      "Average test loss: 0.002902597263869312\n",
      "Epoch 160/300\n",
      "Average training loss: 0.012559277952545219\n",
      "Average test loss: 0.002926714290347364\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01256427603877253\n",
      "Average test loss: 0.0029503361309568088\n",
      "Epoch 162/300\n",
      "Average training loss: 0.012549152623448107\n",
      "Average test loss: 0.0028707367711597017\n",
      "Epoch 163/300\n",
      "Average training loss: 0.012539355643921429\n",
      "Average test loss: 0.002967397206980321\n",
      "Epoch 164/300\n",
      "Average training loss: 0.012521645137005382\n",
      "Average test loss: 0.002889335151968731\n",
      "Epoch 165/300\n",
      "Average training loss: 0.012542382462984985\n",
      "Average test loss: 0.0028796755876392124\n",
      "Epoch 166/300\n",
      "Average training loss: 0.012532754476699564\n",
      "Average test loss: 0.00290540982886321\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01251411092115773\n",
      "Average test loss: 0.0028848763273821936\n",
      "Epoch 168/300\n",
      "Average training loss: 0.012499696265492174\n",
      "Average test loss: 0.003111825419796838\n",
      "Epoch 169/300\n",
      "Average training loss: 0.012515170603990555\n",
      "Average test loss: 0.0029342230881253878\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01249878962089618\n",
      "Average test loss: 0.003066686902816097\n",
      "Epoch 171/300\n",
      "Average training loss: 0.012501681249174806\n",
      "Average test loss: 0.002877236088116964\n",
      "Epoch 172/300\n",
      "Average training loss: 0.012474550363090304\n",
      "Average test loss: 0.002939104509850343\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0124953129283256\n",
      "Average test loss: 0.002910804263626536\n",
      "Epoch 174/300\n",
      "Average training loss: 0.012502983998921182\n",
      "Average test loss: 0.00290875867754221\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0124679019972682\n",
      "Average test loss: 0.003032528789093097\n",
      "Epoch 176/300\n",
      "Average training loss: 0.012460136771202087\n",
      "Average test loss: 0.002913206542324689\n",
      "Epoch 177/300\n",
      "Average training loss: 0.012450344449943966\n",
      "Average test loss: 0.002937413004744384\n",
      "Epoch 178/300\n",
      "Average training loss: 0.012457995215223895\n",
      "Average test loss: 0.0028943364347020785\n",
      "Epoch 179/300\n",
      "Average training loss: 0.012461290358669228\n",
      "Average test loss: 0.0029116053262518513\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01243638568951024\n",
      "Average test loss: 0.0029040321001989975\n",
      "Epoch 181/300\n",
      "Average training loss: 0.012452045373618603\n",
      "Average test loss: 0.0029296570356107422\n",
      "Epoch 182/300\n",
      "Average training loss: 0.012429606295294231\n",
      "Average test loss: 0.002981987458964189\n",
      "Epoch 183/300\n",
      "Average training loss: 0.012418434459302161\n",
      "Average test loss: 0.0028504121570537486\n",
      "Epoch 184/300\n",
      "Average training loss: 0.012424616824421617\n",
      "Average test loss: 0.0029409335835112464\n",
      "Epoch 185/300\n",
      "Average training loss: 0.012418974096576373\n",
      "Average test loss: 0.002975246150460508\n",
      "Epoch 186/300\n",
      "Average training loss: 0.012412642122142844\n",
      "Average test loss: 0.002892993467549483\n",
      "Epoch 187/300\n",
      "Average training loss: 0.012417333836356799\n",
      "Average test loss: 0.002876879970750047\n",
      "Epoch 188/300\n",
      "Average training loss: 0.012408599257469177\n",
      "Average test loss: 0.0029282634171346822\n",
      "Epoch 189/300\n",
      "Average training loss: 0.012392507933080197\n",
      "Average test loss: 0.002960386229472028\n",
      "Epoch 190/300\n",
      "Average training loss: 0.012390240884489484\n",
      "Average test loss: 0.0029209324227025112\n",
      "Epoch 191/300\n",
      "Average training loss: 0.012397874480320347\n",
      "Average test loss: 0.0029491782039403917\n",
      "Epoch 192/300\n",
      "Average training loss: 0.012392174097398918\n",
      "Average test loss: 0.0029306623857054447\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0123767552210225\n",
      "Average test loss: 0.0029109691015134255\n",
      "Epoch 194/300\n",
      "Average training loss: 0.012369932124184238\n",
      "Average test loss: 0.0029195743679172464\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01237906597306331\n",
      "Average test loss: 0.002850325774401426\n",
      "Epoch 196/300\n",
      "Average training loss: 0.012365684541563194\n",
      "Average test loss: 0.0029055515314555832\n",
      "Epoch 197/300\n",
      "Average training loss: 0.012348697792324755\n",
      "Average test loss: 0.0029030139684263203\n",
      "Epoch 198/300\n",
      "Average training loss: 0.012354607014192475\n",
      "Average test loss: 0.0029315474428650407\n",
      "Epoch 199/300\n",
      "Average training loss: 0.012344790167278713\n",
      "Average test loss: 0.00320839594076905\n",
      "Epoch 200/300\n",
      "Average training loss: 0.012352817616528934\n",
      "Average test loss: 0.002932830348610878\n",
      "Epoch 201/300\n",
      "Average training loss: 0.012337800375703308\n",
      "Average test loss: 0.0029700278486642574\n",
      "Epoch 202/300\n",
      "Average training loss: 0.012332036337918706\n",
      "Average test loss: 0.002988170218964418\n",
      "Epoch 203/300\n",
      "Average training loss: 0.012342906826900112\n",
      "Average test loss: 0.002924551378728615\n",
      "Epoch 204/300\n",
      "Average training loss: 0.012331577178504732\n",
      "Average test loss: 0.0030381108656939533\n",
      "Epoch 205/300\n",
      "Average training loss: 0.012332283282445537\n",
      "Average test loss: 0.0028808212207837238\n",
      "Epoch 206/300\n",
      "Average training loss: 0.012313758277230793\n",
      "Average test loss: 0.002914938470452196\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01232048042698039\n",
      "Average test loss: 0.003064293838209576\n",
      "Epoch 208/300\n",
      "Average training loss: 0.012322054664293924\n",
      "Average test loss: 0.0029395698598689503\n",
      "Epoch 209/300\n",
      "Average training loss: 0.012309753498269451\n",
      "Average test loss: 0.0029119014559934537\n",
      "Epoch 210/300\n",
      "Average training loss: 0.012306559075083998\n",
      "Average test loss: 0.0029018348678946496\n",
      "Epoch 211/300\n",
      "Average training loss: 0.012301871688001686\n",
      "Average test loss: 0.002971589405503538\n",
      "Epoch 212/300\n",
      "Average training loss: 0.012292889243198766\n",
      "Average test loss: 0.0029000567766941255\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012300621218151517\n",
      "Average test loss: 0.003015098504101237\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012290009237825871\n",
      "Average test loss: 0.0030645527698927454\n",
      "Epoch 215/300\n",
      "Average training loss: 0.012274701197942097\n",
      "Average test loss: 0.00291420848439965\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012282555202643076\n",
      "Average test loss: 0.002918775046658185\n",
      "Epoch 217/300\n",
      "Average training loss: 0.012278504182895025\n",
      "Average test loss: 0.002938247705499331\n",
      "Epoch 218/300\n",
      "Average training loss: 0.012268950233856837\n",
      "Average test loss: 0.002934252780344751\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012264534218443765\n",
      "Average test loss: 0.0029509878584908114\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012277628893653551\n",
      "Average test loss: 0.0029365698562728035\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012267221570014954\n",
      "Average test loss: 0.0030069719552993776\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012261048129449288\n",
      "Average test loss: 0.003079999548693498\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012258567127916549\n",
      "Average test loss: 0.0029689199659559463\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01224748206552532\n",
      "Average test loss: 0.003016020009501113\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012243262589805655\n",
      "Average test loss: 0.002991120986226532\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012241555566589037\n",
      "Average test loss: 0.003061169591214922\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01225099689596229\n",
      "Average test loss: 0.0029593684565689827\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012244061516390906\n",
      "Average test loss: 0.003006789801435338\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012233507110840744\n",
      "Average test loss: 0.003018831615232759\n",
      "Epoch 230/300\n",
      "Average training loss: 0.012228725051714314\n",
      "Average test loss: 0.002920578408572409\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012222601497338878\n",
      "Average test loss: 0.0029539812666674456\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012224747541050116\n",
      "Average test loss: 0.0029179239693201253\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012226327516966395\n",
      "Average test loss: 0.003037207702588704\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012203750664989153\n",
      "Average test loss: 0.0030075877627564803\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012213532522320747\n",
      "Average test loss: 0.0029386263166864715\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012199891143374973\n",
      "Average test loss: 0.0029734693811171585\n",
      "Epoch 237/300\n",
      "Average training loss: 0.012199741166498926\n",
      "Average test loss: 0.0030458691022876238\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01220494846916861\n",
      "Average test loss: 0.003039985339467724\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012200640049245622\n",
      "Average test loss: 0.002985652394592762\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012190058928396967\n",
      "Average test loss: 0.002987147817284697\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012183985215922196\n",
      "Average test loss: 0.0029088950111634203\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012197310931152767\n",
      "Average test loss: 0.002989733877695269\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012181800671749645\n",
      "Average test loss: 0.002946602739393711\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012173622692624729\n",
      "Average test loss: 0.00301802393525011\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012188210329247846\n",
      "Average test loss: 0.002957092943083909\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01218280757466952\n",
      "Average test loss: 0.0029704280236942902\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012163179567290678\n",
      "Average test loss: 0.003002168883052137\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012149896729323598\n",
      "Average test loss: 0.0030137727831800777\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012167888703445593\n",
      "Average test loss: 0.002906129665569299\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012173586275842455\n",
      "Average test loss: 0.003057411711042126\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012164726200203101\n",
      "Average test loss: 0.002865929199589623\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012164219190677008\n",
      "Average test loss: 0.0030200991183519364\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012155115185512436\n",
      "Average test loss: 0.002979042965401378\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01216483306636413\n",
      "Average test loss: 0.0030024606672426064\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012150842021736833\n",
      "Average test loss: 0.0029481013525898257\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012144947523044215\n",
      "Average test loss: 0.002933254610747099\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012137553009721969\n",
      "Average test loss: 0.0029139834847301243\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012139848915239174\n",
      "Average test loss: 0.002957815901065866\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012135682659016715\n",
      "Average test loss: 0.002914306950238016\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012144699735773935\n",
      "Average test loss: 0.0029407466271271308\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012129829298290942\n",
      "Average test loss: 0.002991163871768448\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012129483758575386\n",
      "Average test loss: 0.003027169010291497\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012125144585967064\n",
      "Average test loss: 0.0029687509548125997\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012154673931499322\n",
      "Average test loss: 0.0029659431003448036\n",
      "Epoch 265/300\n",
      "Average training loss: 0.012124938819143507\n",
      "Average test loss: 0.002963097473192546\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012115871467524105\n",
      "Average test loss: 0.003007632324885991\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012103779280351269\n",
      "Average test loss: 0.0030211767714677586\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012102307396630447\n",
      "Average test loss: 0.0029444157253536913\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012116397509144412\n",
      "Average test loss: 0.003051129936137133\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01211970768786139\n",
      "Average test loss: 0.003003664655610919\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012107219181127019\n",
      "Average test loss: 0.003000867584513293\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01209847999446922\n",
      "Average test loss: 0.0029638577190538246\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01209917185207208\n",
      "Average test loss: 0.003305276907980442\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012091785463194052\n",
      "Average test loss: 0.0030273328717384074\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012097371063298648\n",
      "Average test loss: 0.002956829962010185\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012094215832650662\n",
      "Average test loss: 0.0029799926423778136\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012082783337268565\n",
      "Average test loss: 0.0030165940392762425\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012094057738780976\n",
      "Average test loss: 0.0029818495478894976\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012082225677039888\n",
      "Average test loss: 0.0029779709049810965\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012079693938295047\n",
      "Average test loss: 0.0029170968346297742\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012066028144624498\n",
      "Average test loss: 0.002992367684841156\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012085079666641024\n",
      "Average test loss: 0.003026826764560408\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012073879471255673\n",
      "Average test loss: 0.0029711339212954045\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012062310303250948\n",
      "Average test loss: 0.002921144167582194\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012064944153030714\n",
      "Average test loss: 0.002956719786963529\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012068991652793355\n",
      "Average test loss: 0.003001091514610582\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012062704127695825\n",
      "Average test loss: 0.003004070979853471\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012061096460454994\n",
      "Average test loss: 0.0029567278938161003\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012056452023486297\n",
      "Average test loss: 0.0029699999052617285\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012060269474983216\n",
      "Average test loss: 0.003006453233460585\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012064696878194808\n",
      "Average test loss: 0.003020142384287384\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01203959345486429\n",
      "Average test loss: 0.003009669069407715\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012039710672365294\n",
      "Average test loss: 0.0029650734981728926\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012040874996119076\n",
      "Average test loss: 0.003110176861907045\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012037572032047642\n",
      "Average test loss: 0.0030162865368442405\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01205001317212979\n",
      "Average test loss: 0.003035172036745482\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012044189306596915\n",
      "Average test loss: 0.0031775453628765213\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012040204404956765\n",
      "Average test loss: 0.0029048781264573334\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012028693340718747\n",
      "Average test loss: 0.0030118249360885883\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012030586225291092\n",
      "Average test loss: 0.0029590484034270046\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04909536678426796\n",
      "Average test loss: 0.003850234467536211\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01789118327200413\n",
      "Average test loss: 0.0034072621177054113\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016670132882065244\n",
      "Average test loss: 0.0032873032643563217\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01587350048455927\n",
      "Average test loss: 0.003059147424581978\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01527628066804674\n",
      "Average test loss: 0.0028575965404096576\n",
      "Epoch 6/300\n",
      "Average training loss: 0.014795452632837826\n",
      "Average test loss: 0.002772525771210591\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014419638553427325\n",
      "Average test loss: 0.0027060563806444405\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014087341184417406\n",
      "Average test loss: 0.002655580202117562\n",
      "Epoch 9/300\n",
      "Average training loss: 0.013807313882642322\n",
      "Average test loss: 0.0026383511264704996\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013556878750522932\n",
      "Average test loss: 0.0025450443416419957\n",
      "Epoch 11/300\n",
      "Average training loss: 0.013354941064284908\n",
      "Average test loss: 0.0025001466141806707\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01316645496338606\n",
      "Average test loss: 0.002562724989114536\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013006562101344267\n",
      "Average test loss: 0.0024372237347480325\n",
      "Epoch 14/300\n",
      "Average training loss: 0.012873803349832694\n",
      "Average test loss: 0.002443620476250847\n",
      "Epoch 15/300\n",
      "Average training loss: 0.012749459072947502\n",
      "Average test loss: 0.002420634492610892\n",
      "Epoch 16/300\n",
      "Average training loss: 0.012619106159442001\n",
      "Average test loss: 0.002381354158019854\n",
      "Epoch 17/300\n",
      "Average training loss: 0.012534669460521805\n",
      "Average test loss: 0.0023668467897093957\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012438176768521467\n",
      "Average test loss: 0.002367551501426432\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012362947704891363\n",
      "Average test loss: 0.0023612517102931936\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012289197948243883\n",
      "Average test loss: 0.002323652907067703\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012222701283792655\n",
      "Average test loss: 0.0023176289405673744\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01216093248460028\n",
      "Average test loss: 0.002325639056455758\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012099931899872091\n",
      "Average test loss: 0.0023014439656916593\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012054839628438155\n",
      "Average test loss: 0.0023321513703299895\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012007302448981337\n",
      "Average test loss: 0.002306402200005121\n",
      "Epoch 26/300\n",
      "Average training loss: 0.011964133982029225\n",
      "Average test loss: 0.0022840461981379324\n",
      "Epoch 27/300\n",
      "Average training loss: 0.011913989755014578\n",
      "Average test loss: 0.0022733924564801986\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01187611661851406\n",
      "Average test loss: 0.0022981009915884996\n",
      "Epoch 29/300\n",
      "Average training loss: 0.011836292410890261\n",
      "Average test loss: 0.002268206002087229\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01181125472817156\n",
      "Average test loss: 0.0022488156739208435\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011767132711079386\n",
      "Average test loss: 0.002241768809656302\n",
      "Epoch 32/300\n",
      "Average training loss: 0.011732663798663352\n",
      "Average test loss: 0.0022566279164618914\n",
      "Epoch 33/300\n",
      "Average training loss: 0.011707536885307894\n",
      "Average test loss: 0.00224804866551939\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011678111330502563\n",
      "Average test loss: 0.002264351744929122\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011650367092755106\n",
      "Average test loss: 0.0022750048235886627\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011615767040186457\n",
      "Average test loss: 0.002240301905199885\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01159496458288696\n",
      "Average test loss: 0.0022322625710318486\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01157120494544506\n",
      "Average test loss: 0.002223057597461674\n",
      "Epoch 39/300\n",
      "Average training loss: 0.011542274817824364\n",
      "Average test loss: 0.002230094383160273\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011525379728939798\n",
      "Average test loss: 0.002224195990918411\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011505478740566307\n",
      "Average test loss: 0.002222268012041847\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011473159463041359\n",
      "Average test loss: 0.002223697116598487\n",
      "Epoch 43/300\n",
      "Average training loss: 0.011457741097443633\n",
      "Average test loss: 0.0022096327390107844\n",
      "Epoch 44/300\n",
      "Average training loss: 0.011429038345813752\n",
      "Average test loss: 0.0022161136265430184\n",
      "Epoch 45/300\n",
      "Average training loss: 0.011413658003840182\n",
      "Average test loss: 0.0022158491452121073\n",
      "Epoch 46/300\n",
      "Average training loss: 0.011401731664521826\n",
      "Average test loss: 0.002222286097291443\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011373859965138965\n",
      "Average test loss: 0.002212261126169728\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01135356104787853\n",
      "Average test loss: 0.002213093213116129\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011336936134431097\n",
      "Average test loss: 0.002204745214432478\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011342976469960478\n",
      "Average test loss: 0.002201937673199508\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011311612999273671\n",
      "Average test loss: 0.002224470548745659\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011275609635644489\n",
      "Average test loss: 0.0022144420593976973\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011277071947024929\n",
      "Average test loss: 0.0022024287103364864\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011238193883664078\n",
      "Average test loss: 0.0022235554384274613\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011237992961373594\n",
      "Average test loss: 0.002235439225617382\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011216592243976062\n",
      "Average test loss: 0.002227405989128682\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01120402784479989\n",
      "Average test loss: 0.0022039626226243044\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011195042152371671\n",
      "Average test loss: 0.0022336269207298754\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011174468807048268\n",
      "Average test loss: 0.0022055379643829334\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011159847061667177\n",
      "Average test loss: 0.002231195538615187\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011137783211966356\n",
      "Average test loss: 0.0022410444346153075\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0111297810425361\n",
      "Average test loss: 0.002482069477645887\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011107165939278072\n",
      "Average test loss: 0.0022055623052227827\n",
      "Epoch 64/300\n",
      "Average training loss: 0.011092758890655306\n",
      "Average test loss: 0.0022292242731071182\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011072615495158566\n",
      "Average test loss: 0.0022062542844149802\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011076286824213135\n",
      "Average test loss: 0.002196600743983355\n",
      "Epoch 67/300\n",
      "Average training loss: 0.011051131672329373\n",
      "Average test loss: 0.0022573338208927048\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011041694278518359\n",
      "Average test loss: 0.002203000275314682\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011021222756140762\n",
      "Average test loss: 0.0022237649077756537\n",
      "Epoch 70/300\n",
      "Average training loss: 0.011035860657691956\n",
      "Average test loss: 0.002208835220999188\n",
      "Epoch 71/300\n",
      "Average training loss: 0.010995712184243733\n",
      "Average test loss: 0.0022208989051481086\n",
      "Epoch 72/300\n",
      "Average training loss: 0.010991754450731807\n",
      "Average test loss: 0.002240814205362565\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010978130296700531\n",
      "Average test loss: 0.0022076789640511074\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010961891286075115\n",
      "Average test loss: 0.002272087328135967\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010947792032526599\n",
      "Average test loss: 0.002237905639741156\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01092750239206685\n",
      "Average test loss: 0.0022158686770126224\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010949231960707241\n",
      "Average test loss: 0.00223716682381928\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010916347675853305\n",
      "Average test loss: 0.0022455104059643217\n",
      "Epoch 79/300\n",
      "Average training loss: 0.010894342614544762\n",
      "Average test loss: 0.0022299674519648156\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010886356501115692\n",
      "Average test loss: 0.0022717890962958335\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010877819686300225\n",
      "Average test loss: 0.0022365430562446514\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010864043971730603\n",
      "Average test loss: 0.002212151272636321\n",
      "Epoch 83/300\n",
      "Average training loss: 0.010897057622671128\n",
      "Average test loss: 0.002265188271800677\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010837661046120856\n",
      "Average test loss: 0.0022368643176224495\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010834755353629589\n",
      "Average test loss: 0.002204411747141017\n",
      "Epoch 86/300\n",
      "Average training loss: 0.010822174168295331\n",
      "Average test loss: 0.0022053718767646286\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010807599652144643\n",
      "Average test loss: 0.002244952352510558\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010798013333645131\n",
      "Average test loss: 0.002231097532850173\n",
      "Epoch 89/300\n",
      "Average training loss: 0.010792225214342276\n",
      "Average test loss: 0.0022598088460250034\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010774076002339521\n",
      "Average test loss: 0.00234624915694197\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010770444830258687\n",
      "Average test loss: 0.002228393436099092\n",
      "Epoch 92/300\n",
      "Average training loss: 0.010767187784943316\n",
      "Average test loss: 0.0022205209121521977\n",
      "Epoch 93/300\n",
      "Average training loss: 0.010743821668956014\n",
      "Average test loss: 0.0022172082838498885\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010735019316275915\n",
      "Average test loss: 0.0022234554612595176\n",
      "Epoch 95/300\n",
      "Average training loss: 0.010739042611171801\n",
      "Average test loss: 0.0022640572821514475\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010718827147450712\n",
      "Average test loss: 0.0022705092320425644\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010710239190194341\n",
      "Average test loss: 0.002324774729915791\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010698691049383747\n",
      "Average test loss: 0.002251052167266607\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010681072903176148\n",
      "Average test loss: 0.0022769577062378326\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01067604010552168\n",
      "Average test loss: 0.0022289021145552395\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01067066089477804\n",
      "Average test loss: 0.0022339745848957036\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010664861478739315\n",
      "Average test loss: 0.0022902610018435453\n",
      "Epoch 103/300\n",
      "Average training loss: 0.010654437729467949\n",
      "Average test loss: 0.002231869569255246\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01064497562787599\n",
      "Average test loss: 0.0022481559255263872\n",
      "Epoch 105/300\n",
      "Average training loss: 0.010637372698220942\n",
      "Average test loss: 0.0022156379564354815\n",
      "Epoch 106/300\n",
      "Average training loss: 0.010649477924737666\n",
      "Average test loss: 0.0022546240027166075\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010621436537967788\n",
      "Average test loss: 0.0022364816291050777\n",
      "Epoch 108/300\n",
      "Average training loss: 0.010608276478946209\n",
      "Average test loss: 0.0022718091352532306\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01059527740958664\n",
      "Average test loss: 0.0022410442851897743\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010596683424380091\n",
      "Average test loss: 0.0027447597620387874\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010593148701720767\n",
      "Average test loss: 0.002324472437509232\n",
      "Epoch 112/300\n",
      "Average training loss: 0.010576620204581154\n",
      "Average test loss: 0.0022810208686730926\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01057504669825236\n",
      "Average test loss: 0.00224152644433909\n",
      "Epoch 114/300\n",
      "Average training loss: 0.010558121148082945\n",
      "Average test loss: 0.0022347021874868206\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01056385886338022\n",
      "Average test loss: 0.0022631581508451037\n",
      "Epoch 116/300\n",
      "Average training loss: 0.010548106953915622\n",
      "Average test loss: 0.00225857854841484\n",
      "Epoch 117/300\n",
      "Average training loss: 0.010535613348086675\n",
      "Average test loss: 0.0022720061105986436\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010550984555648433\n",
      "Average test loss: 0.0022500069273842706\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010533302751680216\n",
      "Average test loss: 0.0022608364251338773\n",
      "Epoch 120/300\n",
      "Average training loss: 0.010522042158577178\n",
      "Average test loss: 0.0022587695891658466\n",
      "Epoch 121/300\n",
      "Average training loss: 0.010511111556655831\n",
      "Average test loss: 0.002254410549584362\n",
      "Epoch 122/300\n",
      "Average training loss: 0.010498871091339323\n",
      "Average test loss: 0.0025069502161608804\n",
      "Epoch 123/300\n",
      "Average training loss: 0.010494784215258228\n",
      "Average test loss: 0.0022812137552019624\n",
      "Epoch 124/300\n",
      "Average training loss: 0.010487515468564298\n",
      "Average test loss: 0.002274398192970289\n",
      "Epoch 125/300\n",
      "Average training loss: 0.010483306717541482\n",
      "Average test loss: 0.002277866604220536\n",
      "Epoch 126/300\n",
      "Average training loss: 0.010477154557075765\n",
      "Average test loss: 0.0022964802628590002\n",
      "Epoch 127/300\n",
      "Average training loss: 0.010473560767869155\n",
      "Average test loss: 0.0022564995584802494\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01045363877962033\n",
      "Average test loss: 0.0022587520484295158\n",
      "Epoch 129/300\n",
      "Average training loss: 0.010455046390493711\n",
      "Average test loss: 0.0022597802766702244\n",
      "Epoch 130/300\n",
      "Average training loss: 0.010446722819573349\n",
      "Average test loss: 0.002275737637033065\n",
      "Epoch 131/300\n",
      "Average training loss: 0.010430859120355712\n",
      "Average test loss: 0.002243576983610789\n",
      "Epoch 132/300\n",
      "Average training loss: 0.010428819840153059\n",
      "Average test loss: 0.002271739905493127\n",
      "Epoch 133/300\n",
      "Average training loss: 0.010424204060600864\n",
      "Average test loss: 0.0022718685040664342\n",
      "Epoch 134/300\n",
      "Average training loss: 0.010427599872979853\n",
      "Average test loss: 0.0023356609820491736\n",
      "Epoch 135/300\n",
      "Average training loss: 0.010415653171638648\n",
      "Average test loss: 0.0022878706900195944\n",
      "Epoch 136/300\n",
      "Average training loss: 0.010413440097951226\n",
      "Average test loss: 0.0022745950306869217\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010396407507359982\n",
      "Average test loss: 0.0022744877187328206\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0103905660741859\n",
      "Average test loss: 0.0022755310347096786\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010382096670567989\n",
      "Average test loss: 0.0023327539956404103\n",
      "Epoch 140/300\n",
      "Average training loss: 0.010386489153736167\n",
      "Average test loss: 0.002275562018776933\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01037596242212587\n",
      "Average test loss: 0.0023091327926764884\n",
      "Epoch 142/300\n",
      "Average training loss: 0.010374766814212004\n",
      "Average test loss: 0.0022950562534646856\n",
      "Epoch 143/300\n",
      "Average training loss: 0.010358766505287753\n",
      "Average test loss: 0.002264492895040247\n",
      "Epoch 144/300\n",
      "Average training loss: 0.010359365128808551\n",
      "Average test loss: 0.002282535468124681\n",
      "Epoch 145/300\n",
      "Average training loss: 0.010351943851345115\n",
      "Average test loss: 0.0022763476320024996\n",
      "Epoch 146/300\n",
      "Average training loss: 0.010344528348909483\n",
      "Average test loss: 0.0023274125446461967\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01033882140285439\n",
      "Average test loss: 0.002268411456296841\n",
      "Epoch 148/300\n",
      "Average training loss: 0.010337835672828886\n",
      "Average test loss: 0.0022754401399029626\n",
      "Epoch 149/300\n",
      "Average training loss: 0.010326409089896413\n",
      "Average test loss: 0.002300492476258013\n",
      "Epoch 150/300\n",
      "Average training loss: 0.010328322887420653\n",
      "Average test loss: 0.002263432185889946\n",
      "Epoch 151/300\n",
      "Average training loss: 0.010315091700189644\n",
      "Average test loss: 0.0022817620852341254\n",
      "Epoch 152/300\n",
      "Average training loss: 0.010306560904615455\n",
      "Average test loss: 0.0022589906262647772\n",
      "Epoch 153/300\n",
      "Average training loss: 0.010314231239259243\n",
      "Average test loss: 0.002335664130333397\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01030224707391527\n",
      "Average test loss: 0.0022483109703494444\n",
      "Epoch 155/300\n",
      "Average training loss: 0.010299725539154477\n",
      "Average test loss: 0.0025173286222335365\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01029631227089299\n",
      "Average test loss: 0.002298832548264828\n",
      "Epoch 157/300\n",
      "Average training loss: 0.010293617373539342\n",
      "Average test loss: 0.002289057864290145\n",
      "Epoch 158/300\n",
      "Average training loss: 0.010279542316165236\n",
      "Average test loss: 0.002308494796976447\n",
      "Epoch 159/300\n",
      "Average training loss: 0.010278964978953203\n",
      "Average test loss: 0.0023194324548045796\n",
      "Epoch 160/300\n",
      "Average training loss: 0.010268413732449213\n",
      "Average test loss: 0.002324260603843464\n",
      "Epoch 161/300\n",
      "Average training loss: 0.010266844344635805\n",
      "Average test loss: 0.00227259770863586\n",
      "Epoch 162/300\n",
      "Average training loss: 0.010269161141581006\n",
      "Average test loss: 0.00228774686012831\n",
      "Epoch 163/300\n",
      "Average training loss: 0.010257188128100501\n",
      "Average test loss: 0.002288068324327469\n",
      "Epoch 164/300\n",
      "Average training loss: 0.010259119590123494\n",
      "Average test loss: 0.0022935107342071004\n",
      "Epoch 165/300\n",
      "Average training loss: 0.010247236753503482\n",
      "Average test loss: 0.0023196774974672332\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01024921943495671\n",
      "Average test loss: 0.0023041375941700404\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010246349806586902\n",
      "Average test loss: 0.0022878087918377587\n",
      "Epoch 168/300\n",
      "Average training loss: 0.010234722682171397\n",
      "Average test loss: 0.002313773509942823\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010235346645116807\n",
      "Average test loss: 0.002322219330920941\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010229688417580393\n",
      "Average test loss: 0.0023020361001706785\n",
      "Epoch 171/300\n",
      "Average training loss: 0.010220684565603733\n",
      "Average test loss: 0.002287471898831427\n",
      "Epoch 172/300\n",
      "Average training loss: 0.010211797684431076\n",
      "Average test loss: 0.0023532922347593637\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010217765259246031\n",
      "Average test loss: 0.002308926719965206\n",
      "Epoch 174/300\n",
      "Average training loss: 0.010209470270408524\n",
      "Average test loss: 0.0022920700104700196\n",
      "Epoch 175/300\n",
      "Average training loss: 0.010191473468310303\n",
      "Average test loss: 0.002292710482246346\n",
      "Epoch 176/300\n",
      "Average training loss: 0.010205087949004437\n",
      "Average test loss: 0.0022836506894479197\n",
      "Epoch 177/300\n",
      "Average training loss: 0.010202296065787474\n",
      "Average test loss: 0.00229915827843878\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010194752536714077\n",
      "Average test loss: 0.002325527610671189\n",
      "Epoch 179/300\n",
      "Average training loss: 0.010196857585675187\n",
      "Average test loss: 0.002343023236013121\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010185953431659275\n",
      "Average test loss: 0.0023053936270169086\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010190469244288073\n",
      "Average test loss: 0.0022799444794654846\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010177861442168553\n",
      "Average test loss: 0.002279707093619638\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010172751883665721\n",
      "Average test loss: 0.0022785489410161973\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010167785058418909\n",
      "Average test loss: 0.00236364453451501\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010160061379273732\n",
      "Average test loss: 0.00232291566228701\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010168197941448954\n",
      "Average test loss: 0.0023628820770730573\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010161190532147884\n",
      "Average test loss: 0.0023137358538806436\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010156205020017094\n",
      "Average test loss: 0.002319077029927737\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010149499216841327\n",
      "Average test loss: 0.0023173745177272292\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010136494850118955\n",
      "Average test loss: 0.002379033972819646\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010141536749899387\n",
      "Average test loss: 0.0023285403021921715\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010127404735320145\n",
      "Average test loss: 0.0023481952140314713\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010132050093677309\n",
      "Average test loss: 0.002298813453772002\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010128104338215456\n",
      "Average test loss: 0.002348112630347411\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010115077042745219\n",
      "Average test loss: 0.002335920577454898\n",
      "Epoch 196/300\n",
      "Average training loss: 0.010119447687433825\n",
      "Average test loss: 0.0023178841936298544\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010120413761999871\n",
      "Average test loss: 0.0022979283810903627\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010110235455135504\n",
      "Average test loss: 0.00234259506708218\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010118929207324982\n",
      "Average test loss: 0.014543413486745623\n",
      "Epoch 200/300\n",
      "Average training loss: 0.010125113971531392\n",
      "Average test loss: 0.002326073789244725\n",
      "Epoch 201/300\n",
      "Average training loss: 0.010103729494743877\n",
      "Average test loss: 0.002315820541853706\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01009722113278177\n",
      "Average test loss: 0.0023080454307297864\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010099029271139039\n",
      "Average test loss: 0.002314762992784381\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010091377432147662\n",
      "Average test loss: 0.0023180291890684103\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010090697934230168\n",
      "Average test loss: 0.0023514033616003063\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010096852777732743\n",
      "Average test loss: 0.0023113624588068987\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01009800591899289\n",
      "Average test loss: 0.002333257566516598\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010077196275194485\n",
      "Average test loss: 0.0023414708860218527\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01008724102212323\n",
      "Average test loss: 0.002313170464709401\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010080529756844044\n",
      "Average test loss: 0.0023258846373193794\n",
      "Epoch 211/300\n",
      "Average training loss: 0.010078924617833562\n",
      "Average test loss: 0.002374895160396894\n",
      "Epoch 212/300\n",
      "Average training loss: 0.010065502706501219\n",
      "Average test loss: 0.0023415743716888957\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010072172423203787\n",
      "Average test loss: 0.00236198748047981\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010069146888123619\n",
      "Average test loss: 0.002312073058034811\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010077323156926367\n",
      "Average test loss: 0.002303340359487467\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010060891015662087\n",
      "Average test loss: 0.0023838250790205267\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010054369334959321\n",
      "Average test loss: 0.0023577066354660524\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010052480724122789\n",
      "Average test loss: 0.0022957076850450703\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010047918416559696\n",
      "Average test loss: 0.0023437917987919514\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0100546439687411\n",
      "Average test loss: 0.0023736717003501125\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010047046607567204\n",
      "Average test loss: 0.0023073498093419604\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010038329483320316\n",
      "Average test loss: 0.0023369182159917222\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010047070420450634\n",
      "Average test loss: 0.002323815990653303\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010038073647353384\n",
      "Average test loss: 0.0023903539259400633\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010041357319388124\n",
      "Average test loss: 0.002352941235734357\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01002716142932574\n",
      "Average test loss: 0.002324835324866904\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010028522248897288\n",
      "Average test loss: 0.00237672758474946\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01003172538595067\n",
      "Average test loss: 0.0023238199630545246\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0100231396133701\n",
      "Average test loss: 0.0023106998252785866\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010017322428524494\n",
      "Average test loss: 0.002363693108368251\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010027566470205784\n",
      "Average test loss: 0.002329022416844964\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010008076008823183\n",
      "Average test loss: 0.0023662155616200635\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010014393345349365\n",
      "Average test loss: 0.0023395079272902674\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010021278432673877\n",
      "Average test loss: 0.0023645302415308024\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010009487609896395\n",
      "Average test loss: 0.0023426426638745597\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010000912779735194\n",
      "Average test loss: 0.002337853430149456\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010000467794636885\n",
      "Average test loss: 0.0023433000705101424\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009997919897238413\n",
      "Average test loss: 0.0023039268449776703\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009998187286986244\n",
      "Average test loss: 0.0023322183375971183\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010005554028683238\n",
      "Average test loss: 0.0023237711533697115\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00999155883739392\n",
      "Average test loss: 0.0023892770038089818\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009989545525776015\n",
      "Average test loss: 0.002465103776401116\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009987752930985557\n",
      "Average test loss: 0.002349190553236339\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0099793806001544\n",
      "Average test loss: 0.002379701858179437\n",
      "Epoch 245/300\n",
      "Average training loss: 0.009979538429114554\n",
      "Average test loss: 0.002342056829896238\n",
      "Epoch 246/300\n",
      "Average training loss: 0.009977795416282282\n",
      "Average test loss: 0.0023730417201295495\n",
      "Epoch 247/300\n",
      "Average training loss: 0.009981837071478367\n",
      "Average test loss: 0.002355179622562395\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009973588197595543\n",
      "Average test loss: 0.0023479711169170007\n",
      "Epoch 249/300\n",
      "Average training loss: 0.00997840222550763\n",
      "Average test loss: 0.002320359364979797\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009976497297485669\n",
      "Average test loss: 0.002513107667987545\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009967536860042148\n",
      "Average test loss: 0.0023499434749699303\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009966257745193111\n",
      "Average test loss: 0.0023352342554264598\n",
      "Epoch 253/300\n",
      "Average training loss: 0.009955334949824545\n",
      "Average test loss: 0.0023667911133832403\n",
      "Epoch 254/300\n",
      "Average training loss: 0.009964289736416604\n",
      "Average test loss: 0.002331597694195807\n",
      "Epoch 255/300\n",
      "Average training loss: 0.009962587466670407\n",
      "Average test loss: 0.0023451218577101828\n",
      "Epoch 256/300\n",
      "Average training loss: 0.009957317941718632\n",
      "Average test loss: 0.002349201303285857\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009954030488100316\n",
      "Average test loss: 0.002367499317559931\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0099473332837224\n",
      "Average test loss: 0.0023579184065262476\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009947218701243401\n",
      "Average test loss: 0.0023587501611974505\n",
      "Epoch 260/300\n",
      "Average training loss: 0.009950072018636598\n",
      "Average test loss: 0.0023278572702159486\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009946627576318052\n",
      "Average test loss: 0.002373419461357925\n",
      "Epoch 262/300\n",
      "Average training loss: 0.00993386031438907\n",
      "Average test loss: 0.0023565321647458608\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009940896607935429\n",
      "Average test loss: 0.0023211849204575023\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00993931503345569\n",
      "Average test loss: 0.00235293929444419\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009936322266028986\n",
      "Average test loss: 0.002358719582979878\n",
      "Epoch 266/300\n",
      "Average training loss: 0.009933977355559667\n",
      "Average test loss: 0.0023412431236356497\n",
      "Epoch 267/300\n",
      "Average training loss: 0.009935750264260504\n",
      "Average test loss: 0.002370667206123471\n",
      "Epoch 268/300\n",
      "Average training loss: 0.009928586880366007\n",
      "Average test loss: 0.0023762225423836045\n",
      "Epoch 269/300\n",
      "Average training loss: 0.009927596986293794\n",
      "Average test loss: 0.002332264122967091\n",
      "Epoch 270/300\n",
      "Average training loss: 0.009923812292516232\n",
      "Average test loss: 0.002345526266325679\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009920167724291483\n",
      "Average test loss: 0.0023341137822717427\n",
      "Epoch 272/300\n",
      "Average training loss: 0.00992864300393396\n",
      "Average test loss: 0.002361448272648785\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00992389858679639\n",
      "Average test loss: 0.0023276488261504305\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009908797854350672\n",
      "Average test loss: 0.002443459969634811\n",
      "Epoch 275/300\n",
      "Average training loss: 0.009915196295413707\n",
      "Average test loss: 0.002360886798343725\n",
      "Epoch 276/300\n",
      "Average training loss: 0.009913599258495701\n",
      "Average test loss: 0.002887912056098382\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009920407834980223\n",
      "Average test loss: 0.002372514075703091\n",
      "Epoch 278/300\n",
      "Average training loss: 0.009900736808776855\n",
      "Average test loss: 0.0024049024128665526\n",
      "Epoch 279/300\n",
      "Average training loss: 0.009906221838461028\n",
      "Average test loss: 0.0023526865326695973\n",
      "Epoch 280/300\n",
      "Average training loss: 0.009900156300928858\n",
      "Average test loss: 0.0023384791442917453\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009901442755427625\n",
      "Average test loss: 0.0024360773894521923\n",
      "Epoch 282/300\n",
      "Average training loss: 0.009906819369230005\n",
      "Average test loss: 0.002410296716209915\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009903129309415818\n",
      "Average test loss: 0.0023956706925398774\n",
      "Epoch 284/300\n",
      "Average training loss: 0.009907192397448751\n",
      "Average test loss: 0.0023450151320753825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.009890992013116677\n",
      "Average test loss: 0.0023621218825380006\n",
      "Epoch 286/300\n",
      "Average training loss: 0.009889005676739745\n",
      "Average test loss: 0.0023320562535276016\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009895212470657295\n",
      "Average test loss: 0.0023706876703848442\n",
      "Epoch 288/300\n",
      "Average training loss: 0.009887087930407788\n",
      "Average test loss: 0.002391547176366051\n",
      "Epoch 289/300\n",
      "Average training loss: 0.009884548669887914\n",
      "Average test loss: 0.0023558651583476197\n",
      "Epoch 290/300\n",
      "Average training loss: 0.009894059261514081\n",
      "Average test loss: 0.0023699053683214717\n",
      "Epoch 291/300\n",
      "Average training loss: 0.009884780113895734\n",
      "Average test loss: 0.002323392701438732\n",
      "Epoch 292/300\n",
      "Average training loss: 0.009877374954521656\n",
      "Average test loss: 0.0024071308889736733\n",
      "Epoch 293/300\n",
      "Average training loss: 0.009884810414579179\n",
      "Average test loss: 0.0024154633610612816\n",
      "Epoch 294/300\n",
      "Average training loss: 0.00986999629272355\n",
      "Average test loss: 0.0023541104009168015\n",
      "Epoch 295/300\n",
      "Average training loss: 0.009878620750374264\n",
      "Average test loss: 0.0023425246487475105\n",
      "Epoch 296/300\n",
      "Average training loss: 0.009884324892527527\n",
      "Average test loss: 0.0023903567664739158\n",
      "Epoch 297/300\n",
      "Average training loss: 0.009871445881823699\n",
      "Average test loss: 0.002394313078580631\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00987509167773856\n",
      "Average test loss: 0.002415470179894732\n",
      "Epoch 299/300\n",
      "Average training loss: 0.00987604730659061\n",
      "Average test loss: 0.002400101084883014\n",
      "Epoch 300/300\n",
      "Average training loss: 0.009866684753861692\n",
      "Average test loss: 0.0023346514199963876\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.49\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9241031314531962\n",
      "Average test loss: 0.009295482078360187\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2027832249403\n",
      "Average test loss: 0.005007180937048461\n",
      "Epoch 3/300\n",
      "Average training loss: 0.13723297853602304\n",
      "Average test loss: 0.004774250752396054\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11088633626699447\n",
      "Average test loss: 0.004678993406809039\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09695050894551807\n",
      "Average test loss: 0.0046206773627135485\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08834427513678869\n",
      "Average test loss: 0.00458905244908399\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08256961786084704\n",
      "Average test loss: 0.004544020625452201\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0786205049753189\n",
      "Average test loss: 0.0045066703019870655\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07566627437869708\n",
      "Average test loss: 0.004466735914142595\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07343689996004105\n",
      "Average test loss: 0.0044340348020195965\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0715422309504615\n",
      "Average test loss: 0.004433660165096323\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07011754898230235\n",
      "Average test loss: 0.00440554007059998\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06893923503491614\n",
      "Average test loss: 0.004382010784000159\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06805498194032245\n",
      "Average test loss: 0.004370230560087496\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0674180876215299\n",
      "Average test loss: 0.004351118896570471\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06690353824032677\n",
      "Average test loss: 0.004344824211258027\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06648572750224008\n",
      "Average test loss: 0.00433971393853426\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0661462272339397\n",
      "Average test loss: 0.004431754445450174\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06582129675481055\n",
      "Average test loss: 0.004321329052663512\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0655133717490567\n",
      "Average test loss: 0.004294712650279204\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06526545227567356\n",
      "Average test loss: 0.004276504449546337\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06498414250214894\n",
      "Average test loss: 0.004273423167359498\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0647892504731814\n",
      "Average test loss: 0.00430048106610775\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06456604744328392\n",
      "Average test loss: 0.0042844273398319884\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06438673356837696\n",
      "Average test loss: 0.004260250981069274\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06417616156074736\n",
      "Average test loss: 0.004249374910361237\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06401836737328105\n",
      "Average test loss: 0.0042396612068017325\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06384574935833613\n",
      "Average test loss: 0.004256094176322222\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06373322084546089\n",
      "Average test loss: 0.004252647720277309\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0635284795165062\n",
      "Average test loss: 0.004248724928746621\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06338363829255104\n",
      "Average test loss: 0.004223113489026825\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06328334456019931\n",
      "Average test loss: 0.0042484347451892165\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0631492335398992\n",
      "Average test loss: 0.004206142354756593\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06300554275512696\n",
      "Average test loss: 0.004195081121598681\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06290744361612532\n",
      "Average test loss: 0.0043931284960773255\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06279554876354006\n",
      "Average test loss: 0.004203685656810801\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06266842934820387\n",
      "Average test loss: 0.004195666411684619\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06256882043348419\n",
      "Average test loss: 0.004185820018458698\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06246822591622671\n",
      "Average test loss: 0.004181666776537895\n",
      "Epoch 40/300\n",
      "Average training loss: 0.062352193388673996\n",
      "Average test loss: 0.0042169808434943355\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06222911339998245\n",
      "Average test loss: 0.004180681169860893\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06217742849058575\n",
      "Average test loss: 0.004178878844612175\n",
      "Epoch 43/300\n",
      "Average training loss: 0.062031955884562597\n",
      "Average test loss: 0.004178488468544351\n",
      "Epoch 44/300\n",
      "Average training loss: 0.061986244946718214\n",
      "Average test loss: 0.004234516093714369\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06186662335197131\n",
      "Average test loss: 0.004194374481009113\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06178665069739024\n",
      "Average test loss: 0.004155941211928924\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06171496472093794\n",
      "Average test loss: 0.0041756267791820895\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06164549492133988\n",
      "Average test loss: 0.004192296935452355\n",
      "Epoch 49/300\n",
      "Average training loss: 0.061527481337388355\n",
      "Average test loss: 0.004157239898625347\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06148660832312372\n",
      "Average test loss: 0.004187652023923066\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06139834957321485\n",
      "Average test loss: 0.004158463875452678\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06132372684611215\n",
      "Average test loss: 0.004166208987848626\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06122579421268569\n",
      "Average test loss: 0.00417759163863957\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06120696371793747\n",
      "Average test loss: 0.004156030839516057\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06108519388238589\n",
      "Average test loss: 0.004150383756806453\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0610270145965947\n",
      "Average test loss: 0.004173083913822969\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06093029160963164\n",
      "Average test loss: 0.004170902132987976\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06083859046300252\n",
      "Average test loss: 0.004157540024568637\n",
      "Epoch 59/300\n",
      "Average training loss: 0.060783830559915965\n",
      "Average test loss: 0.004170475265425112\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06069584567348162\n",
      "Average test loss: 0.004160212925738758\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06061079344153404\n",
      "Average test loss: 0.004156539235471024\n",
      "Epoch 62/300\n",
      "Average training loss: 0.060546418799294364\n",
      "Average test loss: 0.004165046420776182\n",
      "Epoch 63/300\n",
      "Average training loss: 0.060505483183595866\n",
      "Average test loss: 0.004174695257097483\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06036994173791673\n",
      "Average test loss: 0.004188200671639707\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06027150942219628\n",
      "Average test loss: 0.0041575687761522\n",
      "Epoch 66/300\n",
      "Average training loss: 0.060262700345781114\n",
      "Average test loss: 0.00419878362533119\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06013310161564085\n",
      "Average test loss: 0.004259768090314336\n",
      "Epoch 68/300\n",
      "Average training loss: 0.060115200850698686\n",
      "Average test loss: 0.004174420424840517\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05999053830570645\n",
      "Average test loss: 0.004176593494911988\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0598978450761901\n",
      "Average test loss: 0.004184993370125691\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05988421688477198\n",
      "Average test loss: 0.004204189331995116\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05976700178119872\n",
      "Average test loss: 0.00419421308942967\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05966981186469396\n",
      "Average test loss: 0.004197105872134368\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0595744018720256\n",
      "Average test loss: 0.004203725093975663\n",
      "Epoch 75/300\n",
      "Average training loss: 0.059474343677361804\n",
      "Average test loss: 0.0041705353450444006\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05945313873224788\n",
      "Average test loss: 0.004169728219509125\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05932772660586569\n",
      "Average test loss: 0.0042387714919944605\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05926266014907095\n",
      "Average test loss: 0.004196158910377158\n",
      "Epoch 79/300\n",
      "Average training loss: 0.059196794768174486\n",
      "Average test loss: 0.0041896412115958\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05909124697248141\n",
      "Average test loss: 0.0042773063580195106\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05902330254846149\n",
      "Average test loss: 0.004208259856328368\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05887958508067661\n",
      "Average test loss: 0.004379091317248013\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05883667193518745\n",
      "Average test loss: 0.004276603639953666\n",
      "Epoch 84/300\n",
      "Average training loss: 0.058715899533695645\n",
      "Average test loss: 0.004202500483642022\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05868289342191484\n",
      "Average test loss: 0.004186483235201902\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05859229400091701\n",
      "Average test loss: 0.004240434301810132\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05848045408725738\n",
      "Average test loss: 0.004214931675129467\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05839322193132507\n",
      "Average test loss: 0.004223378945555952\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05829678318566746\n",
      "Average test loss: 0.0042877563672761125\n",
      "Epoch 90/300\n",
      "Average training loss: 0.058205130603578355\n",
      "Average test loss: 0.004237548487881819\n",
      "Epoch 91/300\n",
      "Average training loss: 0.058168305658631854\n",
      "Average test loss: 0.004205183193708459\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05813779032561514\n",
      "Average test loss: 0.004257480262468258\n",
      "Epoch 93/300\n",
      "Average training loss: 0.057961507423056495\n",
      "Average test loss: 0.004238323215188251\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05787724927067757\n",
      "Average test loss: 0.004358621531890499\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05787854950295554\n",
      "Average test loss: 0.004228483506995771\n",
      "Epoch 96/300\n",
      "Average training loss: 0.057783405227793586\n",
      "Average test loss: 0.004266785137148367\n",
      "Epoch 97/300\n",
      "Average training loss: 0.057675954315397474\n",
      "Average test loss: 0.004270340871065855\n",
      "Epoch 98/300\n",
      "Average training loss: 0.057609618620740044\n",
      "Average test loss: 0.0042367907654907964\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05753085981474983\n",
      "Average test loss: 0.004283142961975601\n",
      "Epoch 100/300\n",
      "Average training loss: 0.057468255895707344\n",
      "Average test loss: 0.0043210620259245235\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05735932755470276\n",
      "Average test loss: 0.004420480195432902\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05735551006926431\n",
      "Average test loss: 0.004350706457263894\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05718138240774472\n",
      "Average test loss: 0.004301844026893377\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05715111966265572\n",
      "Average test loss: 0.0042938678372237415\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05706315596898397\n",
      "Average test loss: 0.0043471931856539515\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05703198948502541\n",
      "Average test loss: 0.004276800942917665\n",
      "Epoch 107/300\n",
      "Average training loss: 0.056946876751052006\n",
      "Average test loss: 0.004246243958671888\n",
      "Epoch 108/300\n",
      "Average training loss: 0.056893662861651845\n",
      "Average test loss: 0.004278396760837899\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05689801669783062\n",
      "Average test loss: 0.004302741883736518\n",
      "Epoch 110/300\n",
      "Average training loss: 0.056796144627862505\n",
      "Average test loss: 0.004308132346098622\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05663995626237657\n",
      "Average test loss: 0.004378469995740387\n",
      "Epoch 112/300\n",
      "Average training loss: 0.056558652447329626\n",
      "Average test loss: 0.004274671804987722\n",
      "Epoch 113/300\n",
      "Average training loss: 0.056473990168836384\n",
      "Average test loss: 0.004419384723529219\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05647651430302196\n",
      "Average test loss: 0.004365637910034921\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05634563308623102\n",
      "Average test loss: 0.004390687841094203\n",
      "Epoch 116/300\n",
      "Average training loss: 0.056310937533775966\n",
      "Average test loss: 0.00443499519634578\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05630639871623781\n",
      "Average test loss: 0.004342568816824092\n",
      "Epoch 118/300\n",
      "Average training loss: 0.056182049943341146\n",
      "Average test loss: 0.004373386982414458\n",
      "Epoch 119/300\n",
      "Average training loss: 0.056156954798433516\n",
      "Average test loss: 0.0043423334920985835\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05609128350019455\n",
      "Average test loss: 0.004337721656594012\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05605727305677202\n",
      "Average test loss: 0.00442233994230628\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05602137119902505\n",
      "Average test loss: 0.004416271785067188\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0559364772008525\n",
      "Average test loss: 0.004387023942338096\n",
      "Epoch 124/300\n",
      "Average training loss: 0.055787188245190515\n",
      "Average test loss: 0.004462143686496549\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05573853489756584\n",
      "Average test loss: 0.004386457824044758\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05575100348393122\n",
      "Average test loss: 0.004371095845889714\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05567265420820978\n",
      "Average test loss: 0.004340228959504101\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05563724651270443\n",
      "Average test loss: 0.004402823047919406\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05551309695177608\n",
      "Average test loss: 0.0042722470454043815\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05547927662730217\n",
      "Average test loss: 0.004418225940316916\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05546494414409002\n",
      "Average test loss: 0.0043340113601750795\n",
      "Epoch 132/300\n",
      "Average training loss: 0.055418219397465385\n",
      "Average test loss: 0.004601723491317696\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05529047738346789\n",
      "Average test loss: 0.004355426095426083\n",
      "Epoch 134/300\n",
      "Average training loss: 0.055327830586168504\n",
      "Average test loss: 0.004290935763675305\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05521319751938184\n",
      "Average test loss: 0.004377742023103767\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05517116530570719\n",
      "Average test loss: 0.004423840193284882\n",
      "Epoch 137/300\n",
      "Average training loss: 0.055066603862577015\n",
      "Average test loss: 0.0043172695682280595\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05512429986728562\n",
      "Average test loss: 0.004437898072724541\n",
      "Epoch 139/300\n",
      "Average training loss: 0.055022136367029614\n",
      "Average test loss: 0.004372096374630928\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05506008623043696\n",
      "Average test loss: 0.0043259697827614015\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05491890495353275\n",
      "Average test loss: 0.00431248569695486\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0548226152824031\n",
      "Average test loss: 0.004384951158530183\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0548527095913887\n",
      "Average test loss: 0.004312104569748044\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05482387273841434\n",
      "Average test loss: 0.004355739281616277\n",
      "Epoch 145/300\n",
      "Average training loss: 0.054765048215786614\n",
      "Average test loss: 0.00443936876538727\n",
      "Epoch 146/300\n",
      "Average training loss: 0.054692876074049206\n",
      "Average test loss: 0.0043559874130619895\n",
      "Epoch 147/300\n",
      "Average training loss: 0.054619851466682225\n",
      "Average test loss: 0.004387176192882988\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05461641630861494\n",
      "Average test loss: 0.004331534164647262\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0545965876976649\n",
      "Average test loss: 0.004376760796540313\n",
      "Epoch 150/300\n",
      "Average training loss: 0.054495295852422715\n",
      "Average test loss: 0.00442383827889959\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0544394884970453\n",
      "Average test loss: 0.004426003242739372\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05442780280444357\n",
      "Average test loss: 0.00444256758565704\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05433450420035256\n",
      "Average test loss: 0.004414067951755391\n",
      "Epoch 154/300\n",
      "Average training loss: 0.054340628766351276\n",
      "Average test loss: 0.004343785578178035\n",
      "Epoch 155/300\n",
      "Average training loss: 0.054271927615006765\n",
      "Average test loss: 0.0043425300510393245\n",
      "Epoch 156/300\n",
      "Average training loss: 0.054221503916713924\n",
      "Average test loss: 0.004508197060682707\n",
      "Epoch 157/300\n",
      "Average training loss: 0.054200793226559955\n",
      "Average test loss: 0.004406294526325332\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05413245299458504\n",
      "Average test loss: 0.0044536354963978135\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05412591749429703\n",
      "Average test loss: 0.0044267908322314425\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05407958591646618\n",
      "Average test loss: 0.004391508609470394\n",
      "Epoch 161/300\n",
      "Average training loss: 0.054020059807433025\n",
      "Average test loss: 0.00439117321289248\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05401111381252607\n",
      "Average test loss: 0.004485309400285284\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05393006694316864\n",
      "Average test loss: 0.004409624354292949\n",
      "Epoch 164/300\n",
      "Average training loss: 0.053915520422988465\n",
      "Average test loss: 0.004411434152887927\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05388189574413829\n",
      "Average test loss: 0.004506517508791553\n",
      "Epoch 166/300\n",
      "Average training loss: 0.053824635826879075\n",
      "Average test loss: 0.004510550399621328\n",
      "Epoch 167/300\n",
      "Average training loss: 0.053838309390677346\n",
      "Average test loss: 0.004418399597207705\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05379701005419096\n",
      "Average test loss: 0.004508240549514691\n",
      "Epoch 169/300\n",
      "Average training loss: 0.053707039021783406\n",
      "Average test loss: 0.004350556874440776\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05368987895713912\n",
      "Average test loss: 0.004454881221883826\n",
      "Epoch 171/300\n",
      "Average training loss: 0.053683120333486135\n",
      "Average test loss: 0.004349284129010306\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05363807944787873\n",
      "Average test loss: 0.004371515698317024\n",
      "Epoch 173/300\n",
      "Average training loss: 0.053569492939445705\n",
      "Average test loss: 0.0043915985737823775\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05354070529341698\n",
      "Average test loss: 0.00445892283692956\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05352567729353905\n",
      "Average test loss: 0.00446527535489036\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05349352944228384\n",
      "Average test loss: 0.004415951989591121\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05339166580637296\n",
      "Average test loss: 0.004422209588189919\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05341557382212745\n",
      "Average test loss: 0.00455506735170881\n",
      "Epoch 179/300\n",
      "Average training loss: 0.053363179660505716\n",
      "Average test loss: 0.00444218952063885\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05338904914259911\n",
      "Average test loss: 0.004474506337609556\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05332520132263501\n",
      "Average test loss: 0.004363432157991661\n",
      "Epoch 182/300\n",
      "Average training loss: 0.053288416494925814\n",
      "Average test loss: 0.004426944655676683\n",
      "Epoch 183/300\n",
      "Average training loss: 0.053185960494809684\n",
      "Average test loss: 0.004421509475757679\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05320907543765174\n",
      "Average test loss: 0.004488507910321156\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0531578258143531\n",
      "Average test loss: 0.004391318916653593\n",
      "Epoch 186/300\n",
      "Average training loss: 0.053173586822218365\n",
      "Average test loss: 0.004408534333937698\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05306736761662695\n",
      "Average test loss: 0.004494719867077139\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05308911767601967\n",
      "Average test loss: 0.004380162746335069\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05309011689490742\n",
      "Average test loss: 0.0044741095966762965\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05314925343460507\n",
      "Average test loss: 0.004422419188130233\n",
      "Epoch 191/300\n",
      "Average training loss: 0.052999146040942934\n",
      "Average test loss: 0.004379979471779532\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05292675442496936\n",
      "Average test loss: 0.004536732671161493\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05292253187629912\n",
      "Average test loss: 0.004506941043254402\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05292907941010263\n",
      "Average test loss: 0.00452252482664254\n",
      "Epoch 195/300\n",
      "Average training loss: 0.052889345626036324\n",
      "Average test loss: 0.004425497371703386\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05284271899527974\n",
      "Average test loss: 0.004508317872054047\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05280232314599885\n",
      "Average test loss: 0.0045656262321604624\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05276043613089455\n",
      "Average test loss: 0.004471933203852839\n",
      "Epoch 199/300\n",
      "Average training loss: 0.052739796138472024\n",
      "Average test loss: 0.004397128291014168\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05269878226187494\n",
      "Average test loss: 0.004340943240871032\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05272455342941814\n",
      "Average test loss: 0.004542524415999651\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05265297798977958\n",
      "Average test loss: 0.00457814473244879\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0526430677374204\n",
      "Average test loss: 0.0044912801374577815\n",
      "Epoch 204/300\n",
      "Average training loss: 0.052592629032002555\n",
      "Average test loss: 0.004569031261735492\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05253364357683394\n",
      "Average test loss: 0.004449763517826796\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05255917352437973\n",
      "Average test loss: 0.004542934982106089\n",
      "Epoch 207/300\n",
      "Average training loss: 0.052500918441348604\n",
      "Average test loss: 0.004526174321563707\n",
      "Epoch 208/300\n",
      "Average training loss: 0.052528974334398904\n",
      "Average test loss: 0.004478620055649016\n",
      "Epoch 209/300\n",
      "Average training loss: 0.052424907839960525\n",
      "Average test loss: 0.004478248428967264\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0524624440603786\n",
      "Average test loss: 0.0044558467761509945\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052469877696699564\n",
      "Average test loss: 0.00443159988667402\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05241743269231584\n",
      "Average test loss: 0.004470071573638254\n",
      "Epoch 213/300\n",
      "Average training loss: 0.052315115676985845\n",
      "Average test loss: 0.004643963900705179\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05229948563708199\n",
      "Average test loss: 0.0044364206449439125\n",
      "Epoch 215/300\n",
      "Average training loss: 0.052263370681140155\n",
      "Average test loss: 0.004489854772471719\n",
      "Epoch 216/300\n",
      "Average training loss: 0.052451698061492705\n",
      "Average test loss: 0.004426001551457577\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05226388877961371\n",
      "Average test loss: 0.00445756049040291\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05220181859532992\n",
      "Average test loss: 0.004453545357825027\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05220899376935429\n",
      "Average test loss: 0.004486086200508807\n",
      "Epoch 220/300\n",
      "Average training loss: 0.052164647301038106\n",
      "Average test loss: 0.0043673357338541085\n",
      "Epoch 221/300\n",
      "Average training loss: 0.052222375071711014\n",
      "Average test loss: 0.004477678709146049\n",
      "Epoch 222/300\n",
      "Average training loss: 0.052145025932126575\n",
      "Average test loss: 0.004547221946633525\n",
      "Epoch 223/300\n",
      "Average training loss: 0.052100295338365764\n",
      "Average test loss: 0.004444861339612139\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05210457885927624\n",
      "Average test loss: 0.004471418620397647\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05209083404474788\n",
      "Average test loss: 0.0045137625460823375\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05201543199353748\n",
      "Average test loss: 0.0045270845567186675\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05199948257870144\n",
      "Average test loss: 0.0045308211499618155\n",
      "Epoch 228/300\n",
      "Average training loss: 0.052010428690248064\n",
      "Average test loss: 0.004516300232874023\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05200910622543759\n",
      "Average test loss: 0.004591500853498777\n",
      "Epoch 230/300\n",
      "Average training loss: 0.051973309212260774\n",
      "Average test loss: 0.004393219309548537\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0518545533782906\n",
      "Average test loss: 0.004517594712062014\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05193706235951848\n",
      "Average test loss: 0.004451778976039754\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05189730899532636\n",
      "Average test loss: 0.004532739125606086\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05181797584891319\n",
      "Average test loss: 0.004428917448553774\n",
      "Epoch 235/300\n",
      "Average training loss: 0.051815876026948295\n",
      "Average test loss: 0.004477977885140313\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05184334614210659\n",
      "Average test loss: 0.004567548188070456\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05172013822197914\n",
      "Average test loss: 0.004425259770204623\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05178417206141684\n",
      "Average test loss: 0.004482400808069441\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05178158376448684\n",
      "Average test loss: 0.004558421648624871\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05168633449077606\n",
      "Average test loss: 0.00448643029646741\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05172499731845326\n",
      "Average test loss: 0.004520630213949416\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05166017175714175\n",
      "Average test loss: 0.00452054062154558\n",
      "Epoch 243/300\n",
      "Average training loss: 0.051657747983932495\n",
      "Average test loss: 0.0045491904792272385\n",
      "Epoch 244/300\n",
      "Average training loss: 0.051591700073745514\n",
      "Average test loss: 0.00447095688059926\n",
      "Epoch 245/300\n",
      "Average training loss: 0.051672096560398735\n",
      "Average test loss: 0.00449399514330758\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05160968706674046\n",
      "Average test loss: 0.00443116123891539\n",
      "Epoch 247/300\n",
      "Average training loss: 0.051568814625342685\n",
      "Average test loss: 0.00451295799927579\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05163014496035046\n",
      "Average test loss: 0.004524926787863175\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05156255176332262\n",
      "Average test loss: 0.004385807134624984\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05151499084300465\n",
      "Average test loss: 0.004440674806841545\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05147365215089586\n",
      "Average test loss: 0.004513180928511752\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05151016757223341\n",
      "Average test loss: 0.004674755193707016\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05145878024564849\n",
      "Average test loss: 0.004536593944455186\n",
      "Epoch 254/300\n",
      "Average training loss: 0.051417961319287615\n",
      "Average test loss: 0.004549497434869408\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05142530000872082\n",
      "Average test loss: 0.0045775441502531366\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05140889479054345\n",
      "Average test loss: 0.004442913065767951\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05137529342373212\n",
      "Average test loss: 0.004523487930082612\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05133350540200869\n",
      "Average test loss: 0.0046660039847095805\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05131912363568942\n",
      "Average test loss: 0.0044882283146596615\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05130010563135147\n",
      "Average test loss: 0.004450021288461155\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05128727952639262\n",
      "Average test loss: 0.004501608897828394\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05131056458420224\n",
      "Average test loss: 0.004511378828850057\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05127473684483104\n",
      "Average test loss: 0.0045966513502515025\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05116525320543183\n",
      "Average test loss: 0.004448589243408707\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05127819454007679\n",
      "Average test loss: 0.004532502524968651\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05120554398165809\n",
      "Average test loss: 0.004459343889935149\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0511829710205396\n",
      "Average test loss: 0.004556041794932551\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05119200133283933\n",
      "Average test loss: 0.004740759358223942\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05119369551208284\n",
      "Average test loss: 0.004526809379044506\n",
      "Epoch 270/300\n",
      "Average training loss: 0.051112733261452784\n",
      "Average test loss: 0.004571267188009288\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05111867129802704\n",
      "Average test loss: 0.004415546049674352\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05107060115204917\n",
      "Average test loss: 0.0045179881052010585\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05103385646475686\n",
      "Average test loss: 0.004605476161258088\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05105994795759519\n",
      "Average test loss: 0.004513359473811256\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05109134205844667\n",
      "Average test loss: 0.004535964879310793\n",
      "Epoch 276/300\n",
      "Average training loss: 0.051006145500474505\n",
      "Average test loss: 0.004652773670852184\n",
      "Epoch 277/300\n",
      "Average training loss: 0.051020326193836\n",
      "Average test loss: 0.004522254971994294\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05095562932888667\n",
      "Average test loss: 0.004586686283349991\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05098604772157139\n",
      "Average test loss: 0.004564146926419602\n",
      "Epoch 280/300\n",
      "Average training loss: 0.050942544837792716\n",
      "Average test loss: 0.0045256454220248595\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05087353528208203\n",
      "Average test loss: 0.004599525910284784\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05089285709791713\n",
      "Average test loss: 0.004483210052880976\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0508711528579394\n",
      "Average test loss: 0.0045430423265530005\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05095073899626732\n",
      "Average test loss: 0.004927818696325024\n",
      "Epoch 285/300\n",
      "Average training loss: 0.050885010474258\n",
      "Average test loss: 0.004486367860188087\n",
      "Epoch 286/300\n",
      "Average training loss: 0.050835106015205384\n",
      "Average test loss: 0.0046140184932284885\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05078162335356077\n",
      "Average test loss: 0.004564640929715501\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05075711914565828\n",
      "Average test loss: 0.004590202706969446\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05079793401559194\n",
      "Average test loss: 0.0046172255476315816\n",
      "Epoch 290/300\n",
      "Average training loss: 0.050809114595254265\n",
      "Average test loss: 0.0045912098619672985\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05078840250107977\n",
      "Average test loss: 0.004475785109731886\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05070973964532216\n",
      "Average test loss: 0.004454628621124559\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05070445585913128\n",
      "Average test loss: 0.004459762329028712\n",
      "Epoch 294/300\n",
      "Average training loss: 0.050683951838148965\n",
      "Average test loss: 0.004466548470159371\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05068557133277257\n",
      "Average test loss: 0.0045051065840654905\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05067894208431244\n",
      "Average test loss: 0.004586732192999787\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05068652919265959\n",
      "Average test loss: 0.004651549451467064\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05062641387515598\n",
      "Average test loss: 0.004619320041189591\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05066860918866264\n",
      "Average test loss: 0.004451229343811671\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05058874111705356\n",
      "Average test loss: 0.004488769541184107\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9375289581616719\n",
      "Average test loss: 0.005108303181827068\n",
      "Epoch 2/300\n",
      "Average training loss: 0.26697379202312893\n",
      "Average test loss: 0.004651692991869317\n",
      "Epoch 3/300\n",
      "Average training loss: 0.16861017947064505\n",
      "Average test loss: 0.004480769708959592\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1280271503329277\n",
      "Average test loss: 0.004358748049785694\n",
      "Epoch 5/300\n",
      "Average training loss: 0.10593686358133952\n",
      "Average test loss: 0.004202740688704782\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09269388417402903\n",
      "Average test loss: 0.004093326459328334\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08406576444705327\n",
      "Average test loss: 0.004056147591521343\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07809646072321468\n",
      "Average test loss: 0.003979781958791945\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07353026996718513\n",
      "Average test loss: 0.003938149589217371\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06979584511783388\n",
      "Average test loss: 0.003915320058663686\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06698854806688097\n",
      "Average test loss: 0.0038337373352713053\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06485191113419003\n",
      "Average test loss: 0.00408259837453564\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06319923813475503\n",
      "Average test loss: 0.0037626503138906424\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06184949860639042\n",
      "Average test loss: 0.0037710345304674572\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06079159379998843\n",
      "Average test loss: 0.003730376255595022\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05991920992732048\n",
      "Average test loss: 0.003751796883220474\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05916646960377693\n",
      "Average test loss: 0.0036392931466301282\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05854283116261164\n",
      "Average test loss: 0.0035999806519183847\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05797739215691884\n",
      "Average test loss: 0.0035759734246465897\n",
      "Epoch 20/300\n",
      "Average training loss: 0.057409649127059516\n",
      "Average test loss: 0.003569933173764083\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0569447232650386\n",
      "Average test loss: 0.0035515656789971723\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05646345112721125\n",
      "Average test loss: 0.0036754918446143467\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05610999149415228\n",
      "Average test loss: 0.003530855302181509\n",
      "Epoch 24/300\n",
      "Average training loss: 0.055695932898256514\n",
      "Average test loss: 0.0035382094081077313\n",
      "Epoch 25/300\n",
      "Average training loss: 0.055295942783355714\n",
      "Average test loss: 0.0035094004062314827\n",
      "Epoch 26/300\n",
      "Average training loss: 0.054933226911558046\n",
      "Average test loss: 0.003501276441125406\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05462863849600156\n",
      "Average test loss: 0.0034855799364546937\n",
      "Epoch 28/300\n",
      "Average training loss: 0.054294654034905965\n",
      "Average test loss: 0.0034551455643441944\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05400559842917654\n",
      "Average test loss: 0.003466200131095118\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0537089587284459\n",
      "Average test loss: 0.003453050151260363\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05340397588411967\n",
      "Average test loss: 0.0034342229291796683\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0531962359978093\n",
      "Average test loss: 0.00343579019813074\n",
      "Epoch 33/300\n",
      "Average training loss: 0.052918388386567435\n",
      "Average test loss: 0.0034407422772298256\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05267325955629349\n",
      "Average test loss: 0.0034226416574998034\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05246021146244473\n",
      "Average test loss: 0.0034230176518774695\n",
      "Epoch 36/300\n",
      "Average training loss: 0.052277624630265765\n",
      "Average test loss: 0.003430920302040047\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05199756279918882\n",
      "Average test loss: 0.0034128194014645284\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05179799752102958\n",
      "Average test loss: 0.003401112027362817\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05164571156435543\n",
      "Average test loss: 0.0034517257381230593\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05148953131503529\n",
      "Average test loss: 0.003399407304202517\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0513007077674071\n",
      "Average test loss: 0.003367991449518336\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0510601916677422\n",
      "Average test loss: 0.0033935606992906995\n",
      "Epoch 43/300\n",
      "Average training loss: 0.050915934754742515\n",
      "Average test loss: 0.003379751248906056\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0507746432920297\n",
      "Average test loss: 0.0033705788457559214\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05060245715578397\n",
      "Average test loss: 0.003454348733441697\n",
      "Epoch 46/300\n",
      "Average training loss: 0.050427945345640185\n",
      "Average test loss: 0.003364981081129776\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05031496747334798\n",
      "Average test loss: 0.00337562641253074\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05020179854498969\n",
      "Average test loss: 0.0033748655412346126\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04995735971464051\n",
      "Average test loss: 0.003364066768437624\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04984423127439287\n",
      "Average test loss: 0.003388614490835203\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04976588060127364\n",
      "Average test loss: 0.003406267895259791\n",
      "Epoch 52/300\n",
      "Average training loss: 0.049569772276613445\n",
      "Average test loss: 0.0033683265012999374\n",
      "Epoch 53/300\n",
      "Average training loss: 0.049407301664352414\n",
      "Average test loss: 0.0033546011971516744\n",
      "Epoch 54/300\n",
      "Average training loss: 0.049291367921564314\n",
      "Average test loss: 0.0033550420722199812\n",
      "Epoch 55/300\n",
      "Average training loss: 0.049167584151029585\n",
      "Average test loss: 0.003363139271322224\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04899999701976776\n",
      "Average test loss: 0.003380252050442828\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04884562084741063\n",
      "Average test loss: 0.003455092310698496\n",
      "Epoch 58/300\n",
      "Average training loss: 0.048717450078990726\n",
      "Average test loss: 0.003724053394463327\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04864219107892778\n",
      "Average test loss: 0.0033866223411427605\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04847452310058806\n",
      "Average test loss: 0.003388357380612029\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04827170638574494\n",
      "Average test loss: 0.003354115421573321\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0482027151286602\n",
      "Average test loss: 0.003360095608772503\n",
      "Epoch 63/300\n",
      "Average training loss: 0.048075133197837405\n",
      "Average test loss: 0.003370130600821641\n",
      "Epoch 64/300\n",
      "Average training loss: 0.047968053370714185\n",
      "Average test loss: 0.0033629397499478526\n",
      "Epoch 65/300\n",
      "Average training loss: 0.047782378170225355\n",
      "Average test loss: 0.0033884587658362256\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0476718960305055\n",
      "Average test loss: 0.003388785948976874\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04751526384883457\n",
      "Average test loss: 0.0033790633053415353\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04742353548937374\n",
      "Average test loss: 0.0033834437419556908\n",
      "Epoch 69/300\n",
      "Average training loss: 0.047237824830744\n",
      "Average test loss: 0.003418913097017341\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04711927517917421\n",
      "Average test loss: 0.0034398291115131643\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04703660519255532\n",
      "Average test loss: 0.00342423146425022\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04694069848789109\n",
      "Average test loss: 0.0034097575301097497\n",
      "Epoch 73/300\n",
      "Average training loss: 0.046798760453859965\n",
      "Average test loss: 0.003476991890205277\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04671588234768973\n",
      "Average test loss: 0.00345363925728533\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04653001830975215\n",
      "Average test loss: 0.003412631995148129\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04643560000757376\n",
      "Average test loss: 0.00347441591674255\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04630363921986686\n",
      "Average test loss: 0.003470451485986511\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04622098719080289\n",
      "Average test loss: 0.0034421177030437523\n",
      "Epoch 79/300\n",
      "Average training loss: 0.046136653694841594\n",
      "Average test loss: 0.003493329679179523\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04598105138209131\n",
      "Average test loss: 0.00353864127314753\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0458897616830137\n",
      "Average test loss: 0.0034992897912032076\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04578136450714535\n",
      "Average test loss: 0.003446628376013703\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04570394269625346\n",
      "Average test loss: 0.0035061425773633853\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04560396825273832\n",
      "Average test loss: 0.0034847754411813287\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04551032840212186\n",
      "Average test loss: 0.003476682469455732\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04537564891907904\n",
      "Average test loss: 0.0035096769941349824\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04527211279835966\n",
      "Average test loss: 0.0035186938465469413\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04520007802380456\n",
      "Average test loss: 0.003464524333468742\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04517325967550278\n",
      "Average test loss: 0.0035556799069874816\n",
      "Epoch 90/300\n",
      "Average training loss: 0.045023040910561876\n",
      "Average test loss: 0.003448282659881645\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04491421486934026\n",
      "Average test loss: 0.0034888214371684525\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04484086431728469\n",
      "Average test loss: 0.003468129600915644\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04481116478641828\n",
      "Average test loss: 0.0034752681333985592\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04466873619622654\n",
      "Average test loss: 0.003483188772574067\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04465828146206008\n",
      "Average test loss: 0.0034810938143895733\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04446536104546653\n",
      "Average test loss: 0.003461001404457622\n",
      "Epoch 97/300\n",
      "Average training loss: 0.044485125650962194\n",
      "Average test loss: 0.00360150362799565\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0443510947227478\n",
      "Average test loss: 0.003505964924270908\n",
      "Epoch 99/300\n",
      "Average training loss: 0.044282407230801055\n",
      "Average test loss: 0.003516701956383056\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04426686758465237\n",
      "Average test loss: 0.0034788956327570808\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04416812621719307\n",
      "Average test loss: 0.0036059129018750457\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04404496003190676\n",
      "Average test loss: 0.0035170680096166\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04402292727265093\n",
      "Average test loss: 0.0034886998486601643\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04395261243647999\n",
      "Average test loss: 0.0034789220603803793\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04382406066854795\n",
      "Average test loss: 0.0034939359919064577\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04382724626859029\n",
      "Average test loss: 0.00361496208794415\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04369161443577872\n",
      "Average test loss: 0.0034647649853593774\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04372137969732284\n",
      "Average test loss: 0.003523766150077184\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04364109614160326\n",
      "Average test loss: 0.00356786245562964\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04355059290925662\n",
      "Average test loss: 0.003522466477420595\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04344652869966295\n",
      "Average test loss: 0.0035511759341590936\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04340656877557437\n",
      "Average test loss: 0.0035182212841593555\n",
      "Epoch 113/300\n",
      "Average training loss: 0.043383724550406136\n",
      "Average test loss: 0.003496304828673601\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04331383631626765\n",
      "Average test loss: 0.0035957579302291075\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04326688689324591\n",
      "Average test loss: 0.0035153898803724185\n",
      "Epoch 116/300\n",
      "Average training loss: 0.043226694802443184\n",
      "Average test loss: 0.0035980728415565357\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04320128603114022\n",
      "Average test loss: 0.0035172547602819073\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04309754924972852\n",
      "Average test loss: 0.003530254496157997\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04305286505818367\n",
      "Average test loss: 0.003541396023498641\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04294831983248393\n",
      "Average test loss: 0.003646874115698867\n",
      "Epoch 121/300\n",
      "Average training loss: 0.042891923483875063\n",
      "Average test loss: 0.0036573680941429402\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04291843976577123\n",
      "Average test loss: 0.003631947704901298\n",
      "Epoch 123/300\n",
      "Average training loss: 0.042836391548315686\n",
      "Average test loss: 0.0035582750748015114\n",
      "Epoch 124/300\n",
      "Average training loss: 0.042792708069086076\n",
      "Average test loss: 0.0036287102893822723\n",
      "Epoch 125/300\n",
      "Average training loss: 0.042763861384656696\n",
      "Average test loss: 0.0035562649704515936\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04267587773998578\n",
      "Average test loss: 0.0036030396196163363\n",
      "Epoch 127/300\n",
      "Average training loss: 0.042680535988675224\n",
      "Average test loss: 0.0035335389284623995\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0425769836405913\n",
      "Average test loss: 0.0035505599168439705\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04254874834252728\n",
      "Average test loss: 0.0035889594554901123\n",
      "Epoch 130/300\n",
      "Average training loss: 0.042471356958150866\n",
      "Average test loss: 0.0035776283774111004\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04250109187099669\n",
      "Average test loss: 0.0034775675855990913\n",
      "Epoch 132/300\n",
      "Average training loss: 0.042429858889844685\n",
      "Average test loss: 0.0035253806126614414\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04238903781109386\n",
      "Average test loss: 0.0035662532436350983\n",
      "Epoch 134/300\n",
      "Average training loss: 0.042369346304072276\n",
      "Average test loss: 0.003661648667520947\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04228733441233635\n",
      "Average test loss: 0.003528919062680668\n",
      "Epoch 136/300\n",
      "Average training loss: 0.042169845862521066\n",
      "Average test loss: 0.00394074570097857\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04218756773736742\n",
      "Average test loss: 0.0035632648886077933\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04216183652480443\n",
      "Average test loss: 0.003636599534812073\n",
      "Epoch 139/300\n",
      "Average training loss: 0.042092219448751876\n",
      "Average test loss: 0.003650500504506959\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04206074944138527\n",
      "Average test loss: 0.003767839665628142\n",
      "Epoch 141/300\n",
      "Average training loss: 0.042016559925344255\n",
      "Average test loss: 0.0036080889176163407\n",
      "Epoch 142/300\n",
      "Average training loss: 0.041982210795084636\n",
      "Average test loss: 0.0035438242374608913\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04201921106709374\n",
      "Average test loss: 0.0035965027045458554\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04193180337217119\n",
      "Average test loss: 0.0035933096301224497\n",
      "Epoch 145/300\n",
      "Average training loss: 0.041854998999171784\n",
      "Average test loss: 0.00357449606143766\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04187720039486885\n",
      "Average test loss: 0.003515187690862351\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04178705681860447\n",
      "Average test loss: 0.00357197135914531\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04174455403453774\n",
      "Average test loss: 0.003590083642138375\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04171690739194552\n",
      "Average test loss: 0.0036055717261301145\n",
      "Epoch 150/300\n",
      "Average training loss: 0.041756079491641784\n",
      "Average test loss: 0.0036092293279038536\n",
      "Epoch 151/300\n",
      "Average training loss: 0.041750830127133265\n",
      "Average test loss: 0.0037078816013203725\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04164414135283894\n",
      "Average test loss: 0.003581244692620304\n",
      "Epoch 153/300\n",
      "Average training loss: 0.041600934359762407\n",
      "Average test loss: 0.0035857981596555976\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04154654112458229\n",
      "Average test loss: 0.0036373471406598887\n",
      "Epoch 155/300\n",
      "Average training loss: 0.041516546103689406\n",
      "Average test loss: 0.003675559598952532\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0415201936893993\n",
      "Average test loss: 0.003637279015034437\n",
      "Epoch 157/300\n",
      "Average training loss: 0.041481409129169255\n",
      "Average test loss: 0.0036043325091401736\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04150321016046736\n",
      "Average test loss: 0.0036454485420965486\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04135648056202464\n",
      "Average test loss: 0.0036023236540042693\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04137561144762569\n",
      "Average test loss: 0.003522672158562475\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04130548209283087\n",
      "Average test loss: 0.0036074152770969605\n",
      "Epoch 162/300\n",
      "Average training loss: 0.041304888745148975\n",
      "Average test loss: 0.003597707565873861\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04128724339935515\n",
      "Average test loss: 0.003686317979461617\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04123738303449419\n",
      "Average test loss: 0.0035943028090728653\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04117563270197974\n",
      "Average test loss: 0.0036264242827892305\n",
      "Epoch 166/300\n",
      "Average training loss: 0.041140325155523086\n",
      "Average test loss: 0.003607361170359784\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04113863702283965\n",
      "Average test loss: 0.0035510388964580166\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04115250250365999\n",
      "Average test loss: 0.0037166435844782327\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04108818432688713\n",
      "Average test loss: 0.0035892118194864858\n",
      "Epoch 170/300\n",
      "Average training loss: 0.041101242257489096\n",
      "Average test loss: 0.0036478309685157406\n",
      "Epoch 171/300\n",
      "Average training loss: 0.041066354827748404\n",
      "Average test loss: 0.0037158556069350904\n",
      "Epoch 172/300\n",
      "Average training loss: 0.040989207211467954\n",
      "Average test loss: 0.0037124225310981272\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04092859845028983\n",
      "Average test loss: 0.0036383984407616986\n",
      "Epoch 174/300\n",
      "Average training loss: 0.041040284676684274\n",
      "Average test loss: 0.0036537521320084733\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04089311192101903\n",
      "Average test loss: 0.00369130038904647\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04093384062581592\n",
      "Average test loss: 0.003567664438651668\n",
      "Epoch 177/300\n",
      "Average training loss: 0.040862875726487904\n",
      "Average test loss: 0.00362562376467718\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04081151282456186\n",
      "Average test loss: 0.0036779238120135333\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04079878315991826\n",
      "Average test loss: 0.003642300762443079\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04080756537285116\n",
      "Average test loss: 0.003621573373675346\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04071943081418673\n",
      "Average test loss: 0.0036282070494360395\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04070681269963582\n",
      "Average test loss: 0.0037426742896851566\n",
      "Epoch 183/300\n",
      "Average training loss: 0.040726060887177784\n",
      "Average test loss: 0.0036203698818054464\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04071650198433134\n",
      "Average test loss: 0.0037071210578497912\n",
      "Epoch 185/300\n",
      "Average training loss: 0.040732684903674654\n",
      "Average test loss: 0.0036669934193293255\n",
      "Epoch 186/300\n",
      "Average training loss: 0.040642196519507304\n",
      "Average test loss: 0.003807109880157643\n",
      "Epoch 187/300\n",
      "Average training loss: 0.040640994518995284\n",
      "Average test loss: 0.0036823908815988237\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0406154424349467\n",
      "Average test loss: 0.0036440963798926936\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0405850573612584\n",
      "Average test loss: 0.0036609720031006467\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04052033751871851\n",
      "Average test loss: 0.0035879804798298414\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04053436016705301\n",
      "Average test loss: 0.0036836646414465375\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04046550119254324\n",
      "Average test loss: 0.0037066792626347807\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04047319932116403\n",
      "Average test loss: 0.0035975389040799606\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04045589413907793\n",
      "Average test loss: 0.0036863276946047943\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04037354433205392\n",
      "Average test loss: 0.00366848633148604\n",
      "Epoch 196/300\n",
      "Average training loss: 0.040418287038803104\n",
      "Average test loss: 0.003693291554020511\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04037744641304016\n",
      "Average test loss: 0.003583939783482088\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04040608528587553\n",
      "Average test loss: 0.003650866804851426\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04031547717584504\n",
      "Average test loss: 0.0036886147544201876\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04037540760967467\n",
      "Average test loss: 0.00358032250073221\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04030702106985781\n",
      "Average test loss: 0.0037103164481619992\n",
      "Epoch 202/300\n",
      "Average training loss: 0.040231540673308905\n",
      "Average test loss: 0.003580330005950398\n",
      "Epoch 203/300\n",
      "Average training loss: 0.040327417472998305\n",
      "Average test loss: 0.0036300469777650305\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04021608887447251\n",
      "Average test loss: 0.0036775663673049874\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04014458616574605\n",
      "Average test loss: 0.003621588971051905\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0401040941145685\n",
      "Average test loss: 0.003647315889183018\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0401226028766897\n",
      "Average test loss: 0.0037131112677355606\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04017929793563154\n",
      "Average test loss: 0.003730657664645049\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04008544698688719\n",
      "Average test loss: 0.003728889932649003\n",
      "Epoch 210/300\n",
      "Average training loss: 0.040041537384192145\n",
      "Average test loss: 0.0036267913745509253\n",
      "Epoch 211/300\n",
      "Average training loss: 0.040036575920052\n",
      "Average test loss: 0.0036542984710799323\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04000367297728857\n",
      "Average test loss: 0.00371484572502474\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04007146346237924\n",
      "Average test loss: 0.0036816337803999583\n",
      "Epoch 214/300\n",
      "Average training loss: 0.039994182070096335\n",
      "Average test loss: 0.003736492676453458\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03997027291523086\n",
      "Average test loss: 0.0037372183750073117\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04000303049882253\n",
      "Average test loss: 0.003880629837099049\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03995488591988881\n",
      "Average test loss: 0.0036805779298560485\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03994195291399956\n",
      "Average test loss: 0.003650915253493521\n",
      "Epoch 219/300\n",
      "Average training loss: 0.039917721622520025\n",
      "Average test loss: 0.0037675386153989366\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03984641762740082\n",
      "Average test loss: 0.0037533898568815655\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03992844933933682\n",
      "Average test loss: 0.003753882272582915\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03982168331742287\n",
      "Average test loss: 0.0036451691900276475\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03982313329643673\n",
      "Average test loss: 0.003619619250918428\n",
      "Epoch 224/300\n",
      "Average training loss: 0.039797127889262304\n",
      "Average test loss: 0.0037121244797276125\n",
      "Epoch 225/300\n",
      "Average training loss: 0.039835019684500164\n",
      "Average test loss: 0.0036392335833774674\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0397711593012015\n",
      "Average test loss: 0.003777172103938129\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03976057277123134\n",
      "Average test loss: 0.003658658411974708\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0397407442662451\n",
      "Average test loss: 0.0038350464654051597\n",
      "Epoch 229/300\n",
      "Average training loss: 0.039800781074497436\n",
      "Average test loss: 0.003704244704503152\n",
      "Epoch 230/300\n",
      "Average training loss: 0.039758424474133386\n",
      "Average test loss: 0.003727896176278591\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03959995161824756\n",
      "Average test loss: 0.0041329021052353915\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03966389556394683\n",
      "Average test loss: 0.0037406327101505467\n",
      "Epoch 233/300\n",
      "Average training loss: 0.039620500769880086\n",
      "Average test loss: 0.0036297035811262\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0396616784416967\n",
      "Average test loss: 0.0037209948094354735\n",
      "Epoch 235/300\n",
      "Average training loss: 0.039638840463426375\n",
      "Average test loss: 0.0038211036461095016\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03965105350150002\n",
      "Average test loss: 0.003656782105565071\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03962247148487303\n",
      "Average test loss: 0.0036733504980802534\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03959313910537296\n",
      "Average test loss: 0.003839477777067158\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0395602348877324\n",
      "Average test loss: 0.0037022911686864163\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03952177361647288\n",
      "Average test loss: 0.0037408787144554987\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03949595630500052\n",
      "Average test loss: 0.0036864252736171087\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0395227368010415\n",
      "Average test loss: 0.0036514529171917175\n",
      "Epoch 243/300\n",
      "Average training loss: 0.039485744688245984\n",
      "Average test loss: 0.003817142619027032\n",
      "Epoch 244/300\n",
      "Average training loss: 0.039437858359681235\n",
      "Average test loss: 0.004173907302320003\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03944297932916217\n",
      "Average test loss: 0.003667592814192176\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03940678535567389\n",
      "Average test loss: 0.004079313448319833\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0394010608361827\n",
      "Average test loss: 0.0036810839788781274\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03942773880561193\n",
      "Average test loss: 0.0037428915252288183\n",
      "Epoch 249/300\n",
      "Average training loss: 0.039361541476514605\n",
      "Average test loss: 0.0036505048353638915\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03934623395072089\n",
      "Average test loss: 0.0038238002020451758\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03932930435736974\n",
      "Average test loss: 0.0038282421922518147\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03938196190529399\n",
      "Average test loss: 0.0036923972629010677\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03929520338111454\n",
      "Average test loss: 0.003714612299369441\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03933335183726417\n",
      "Average test loss: 0.0037848888556990356\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0393007683356603\n",
      "Average test loss: 0.0037631248351600436\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03932141412297885\n",
      "Average test loss: 0.0037842039708048105\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03925405604309506\n",
      "Average test loss: 0.003701035587117076\n",
      "Epoch 258/300\n",
      "Average training loss: 0.039254869321982064\n",
      "Average test loss: 0.004016261302762562\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03925783232185576\n",
      "Average test loss: 0.003731457361330589\n",
      "Epoch 260/300\n",
      "Average training loss: 0.039208482616477545\n",
      "Average test loss: 0.003766409049845404\n",
      "Epoch 261/300\n",
      "Average training loss: 0.039246492240164015\n",
      "Average test loss: 0.003667682725522253\n",
      "Epoch 262/300\n",
      "Average training loss: 0.039214892783098754\n",
      "Average test loss: 0.003760578674574693\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03914684938722186\n",
      "Average test loss: 0.0037630407344549895\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03916169772711065\n",
      "Average test loss: 0.0037280905739300783\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0391352436972989\n",
      "Average test loss: 0.0037741747225324314\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03912095649374856\n",
      "Average test loss: 0.003930092269761695\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03919313536749946\n",
      "Average test loss: 0.0037979690047601857\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03911822607119878\n",
      "Average test loss: 0.0037005236273010573\n",
      "Epoch 269/300\n",
      "Average training loss: 0.039113184773259695\n",
      "Average test loss: 0.003745694804522726\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03905545465482606\n",
      "Average test loss: 0.003799708995761143\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03907040074467659\n",
      "Average test loss: 0.0036791247663398585\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03905215762390031\n",
      "Average test loss: 0.003641184180561039\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03909469978345765\n",
      "Average test loss: 0.0038000239605704943\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03903520687421163\n",
      "Average test loss: 0.0037089463896635504\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03895108151435852\n",
      "Average test loss: 0.0037970625969270867\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03896248968773418\n",
      "Average test loss: 0.0037544012429813543\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03899850809905264\n",
      "Average test loss: 0.003826006911074122\n",
      "Epoch 278/300\n",
      "Average training loss: 0.039003880050447255\n",
      "Average test loss: 0.0036360763104425534\n",
      "Epoch 279/300\n",
      "Average training loss: 0.038939240737093817\n",
      "Average test loss: 0.00371947477840715\n",
      "Epoch 280/300\n",
      "Average training loss: 0.039018933557801776\n",
      "Average test loss: 0.00368759668763313\n",
      "Epoch 281/300\n",
      "Average training loss: 0.038966381265057456\n",
      "Average test loss: 0.0037193361717379756\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03888298462165726\n",
      "Average test loss: 0.003966918898953332\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03891784126063188\n",
      "Average test loss: 0.003779377325541443\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03890549491180314\n",
      "Average test loss: 0.003775710793832938\n",
      "Epoch 285/300\n",
      "Average training loss: 0.038931380569934844\n",
      "Average test loss: 0.0036843047115123935\n",
      "Epoch 286/300\n",
      "Average training loss: 0.038863302674558425\n",
      "Average test loss: 0.003705757184368041\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0388545985089408\n",
      "Average test loss: 0.00380837073094315\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03885567618078656\n",
      "Average test loss: 0.003760820218672355\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03888780763414171\n",
      "Average test loss: 0.003802882037435969\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03878035815556844\n",
      "Average test loss: 0.0036442233711067172\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03879293940795792\n",
      "Average test loss: 0.003705137555590934\n",
      "Epoch 292/300\n",
      "Average training loss: 0.038815910405582854\n",
      "Average test loss: 0.0037913235933002497\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0387856589059035\n",
      "Average test loss: 0.0036900231507089403\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03874716884891192\n",
      "Average test loss: 0.0036535680490649407\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03873751866651906\n",
      "Average test loss: 0.00374403119356268\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03872618082662423\n",
      "Average test loss: 0.0037304803542792795\n",
      "Epoch 297/300\n",
      "Average training loss: 0.038710130370325516\n",
      "Average test loss: 0.003973673043151696\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03871440479490492\n",
      "Average test loss: 0.0036981239509251383\n",
      "Epoch 299/300\n",
      "Average training loss: 0.038729109899865256\n",
      "Average test loss: 0.003775858249515295\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03868321438961559\n",
      "Average test loss: 0.003673065171059635\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.8801610254446666\n",
      "Average test loss: 0.004731571032769151\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2291516906950209\n",
      "Average test loss: 0.004272987082186672\n",
      "Epoch 3/300\n",
      "Average training loss: 0.14248126967748007\n",
      "Average test loss: 0.003996184360649851\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1081430167224672\n",
      "Average test loss: 0.003829519940747155\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09056131870879067\n",
      "Average test loss: 0.0036829159495731195\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0798064307835367\n",
      "Average test loss: 0.0035736607141378853\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07197757323582966\n",
      "Average test loss: 0.003482264350271887\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06594573726256689\n",
      "Average test loss: 0.0034003194046931134\n",
      "Epoch 9/300\n",
      "Average training loss: 0.061762726777129705\n",
      "Average test loss: 0.0033872043022678956\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05905045693450504\n",
      "Average test loss: 0.0033128748858968415\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05708897347913848\n",
      "Average test loss: 0.0032032890168743\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0555209296743075\n",
      "Average test loss: 0.0032598721115953393\n",
      "Epoch 13/300\n",
      "Average training loss: 0.054238539046711394\n",
      "Average test loss: 0.003097127045194308\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05316885244846344\n",
      "Average test loss: 0.0031967041813251045\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05216755201088057\n",
      "Average test loss: 0.003104728081367082\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05134185838699341\n",
      "Average test loss: 0.003020123418006632\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05057673632105192\n",
      "Average test loss: 0.0030067180113659965\n",
      "Epoch 18/300\n",
      "Average training loss: 0.049896106771296925\n",
      "Average test loss: 0.002960645442207654\n",
      "Epoch 19/300\n",
      "Average training loss: 0.049232394817802644\n",
      "Average test loss: 0.0029444634825405145\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04861264899041918\n",
      "Average test loss: 0.0029056616872549057\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04809302985005909\n",
      "Average test loss: 0.0029966398489971956\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04756458862291442\n",
      "Average test loss: 0.002876141998916864\n",
      "Epoch 23/300\n",
      "Average training loss: 0.047023540162377885\n",
      "Average test loss: 0.0028731030033280454\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04649803347885609\n",
      "Average test loss: 0.002909844303710593\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04611947326527702\n",
      "Average test loss: 0.0029187745766507253\n",
      "Epoch 26/300\n",
      "Average training loss: 0.045658981415960524\n",
      "Average test loss: 0.0028636102090693182\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04535114594962862\n",
      "Average test loss: 0.0028750345666582384\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04489843786756197\n",
      "Average test loss: 0.0028227460264331765\n",
      "Epoch 29/300\n",
      "Average training loss: 0.044588060604201424\n",
      "Average test loss: 0.0028610399626195432\n",
      "Epoch 30/300\n",
      "Average training loss: 0.044310085505247114\n",
      "Average test loss: 0.002790791266494327\n",
      "Epoch 31/300\n",
      "Average training loss: 0.043990267071459026\n",
      "Average test loss: 0.00276059725197653\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04368901857237021\n",
      "Average test loss: 0.0027759295627474783\n",
      "Epoch 33/300\n",
      "Average training loss: 0.043428648836082884\n",
      "Average test loss: 0.0027771573613087335\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04323151551352607\n",
      "Average test loss: 0.0027548280983335444\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0429382882449362\n",
      "Average test loss: 0.0027745407194726996\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04277232230703036\n",
      "Average test loss: 0.002751801650557253\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04250969508621428\n",
      "Average test loss: 0.002743370946082804\n",
      "Epoch 38/300\n",
      "Average training loss: 0.042380267586972976\n",
      "Average test loss: 0.002758824956085947\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04215555580457052\n",
      "Average test loss: 0.0027762703336775304\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04190499754746755\n",
      "Average test loss: 0.002745349558898144\n",
      "Epoch 41/300\n",
      "Average training loss: 0.041757967111137176\n",
      "Average test loss: 0.002714780570939183\n",
      "Epoch 42/300\n",
      "Average training loss: 0.041580847041474446\n",
      "Average test loss: 0.0027068016851941744\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04139834526843495\n",
      "Average test loss: 0.0027531156823452974\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04131708089179463\n",
      "Average test loss: 0.002698259525001049\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04104042302237617\n",
      "Average test loss: 0.0027172516669250195\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04096645943654908\n",
      "Average test loss: 0.002747294300339288\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04076825845572683\n",
      "Average test loss: 0.0027237551680041683\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0406824445327123\n",
      "Average test loss: 0.002699006154305405\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04050561654733287\n",
      "Average test loss: 0.0027215158788280354\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04033524764577548\n",
      "Average test loss: 0.00269215656320254\n",
      "Epoch 51/300\n",
      "Average training loss: 0.040249592539336944\n",
      "Average test loss: 0.0027147795715265805\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04006912440723843\n",
      "Average test loss: 0.002742365979693002\n",
      "Epoch 53/300\n",
      "Average training loss: 0.039916945212417176\n",
      "Average test loss: 0.002697086125405298\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03978918845454852\n",
      "Average test loss: 0.002717473493475053\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03967038317521413\n",
      "Average test loss: 0.0027285057329055336\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03958031763301955\n",
      "Average test loss: 0.002740586184172167\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03941551874412431\n",
      "Average test loss: 0.002722584877991014\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03931569125586086\n",
      "Average test loss: 0.002743407035453452\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03920500318871604\n",
      "Average test loss: 0.002707585156377819\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03907444126738442\n",
      "Average test loss: 0.0026965849799000553\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03888971722126007\n",
      "Average test loss: 0.0027221392827729385\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03877388556963868\n",
      "Average test loss: 0.002750437170267105\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03866989051964548\n",
      "Average test loss: 0.002762965571342243\n",
      "Epoch 64/300\n",
      "Average training loss: 0.038533181177245246\n",
      "Average test loss: 0.0027021360293858583\n",
      "Epoch 65/300\n",
      "Average training loss: 0.038408565426866215\n",
      "Average test loss: 0.0028335370400713548\n",
      "Epoch 66/300\n",
      "Average training loss: 0.038267854309744304\n",
      "Average test loss: 0.002829936281053556\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03828757908112473\n",
      "Average test loss: 0.0027612288747396736\n",
      "Epoch 68/300\n",
      "Average training loss: 0.038059327506356766\n",
      "Average test loss: 0.0027791996645844647\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0380151003797849\n",
      "Average test loss: 0.0028464710449592934\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03781462331612905\n",
      "Average test loss: 0.002727835593331191\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03772121888067987\n",
      "Average test loss: 0.002764239228847954\n",
      "Epoch 72/300\n",
      "Average training loss: 0.037612075093719693\n",
      "Average test loss: 0.0027630603903283676\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03753155662284957\n",
      "Average test loss: 0.0031021249244610467\n",
      "Epoch 74/300\n",
      "Average training loss: 0.037476663264963364\n",
      "Average test loss: 0.0028223208950625524\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03730822617477841\n",
      "Average test loss: 0.0027505294972409804\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03727202701237466\n",
      "Average test loss: 0.002764471045591765\n",
      "Epoch 77/300\n",
      "Average training loss: 0.037149340093135835\n",
      "Average test loss: 0.0028174625413699282\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03703249428007338\n",
      "Average test loss: 0.0027623981181532143\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03693667810161908\n",
      "Average test loss: 0.002754151740628812\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03686045692033238\n",
      "Average test loss: 0.002756305910750396\n",
      "Epoch 81/300\n",
      "Average training loss: 0.036754315487212606\n",
      "Average test loss: 0.0028328614501903454\n",
      "Epoch 82/300\n",
      "Average training loss: 0.036643639789687266\n",
      "Average test loss: 0.0027838671677228475\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03662462090452512\n",
      "Average test loss: 0.002800460649240348\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03649290703071488\n",
      "Average test loss: 0.0027978732236143616\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03646150842971272\n",
      "Average test loss: 0.002773446477535698\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03629467766152488\n",
      "Average test loss: 0.002736612976425224\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03627062531974581\n",
      "Average test loss: 0.00278727607222067\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03616951860487461\n",
      "Average test loss: 0.0028531333544395037\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03604882784684499\n",
      "Average test loss: 0.002751717762607667\n",
      "Epoch 90/300\n",
      "Average training loss: 0.036007873975568344\n",
      "Average test loss: 0.002845059924034609\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03599258739087317\n",
      "Average test loss: 0.0030553683133588898\n",
      "Epoch 92/300\n",
      "Average training loss: 0.035903438412480886\n",
      "Average test loss: 0.0027739120978448125\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03579293781187799\n",
      "Average test loss: 0.0027809689979379376\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03571972173452377\n",
      "Average test loss: 0.0028293387720154392\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03566792736781968\n",
      "Average test loss: 0.002787587366791235\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03561259671714571\n",
      "Average test loss: 0.002772242596372962\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03553246121274101\n",
      "Average test loss: 0.0028011113421784507\n",
      "Epoch 98/300\n",
      "Average training loss: 0.035459195704923736\n",
      "Average test loss: 0.0029953613097055093\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03542535509665807\n",
      "Average test loss: 0.0027797749402622383\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03532887069384257\n",
      "Average test loss: 0.002797740726214316\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03522597406804562\n",
      "Average test loss: 0.0028473516559849184\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03523150073985259\n",
      "Average test loss: 0.0028759942551453907\n",
      "Epoch 103/300\n",
      "Average training loss: 0.035150398357046975\n",
      "Average test loss: 0.002814496827415294\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03512245015303294\n",
      "Average test loss: 0.002781820364606877\n",
      "Epoch 105/300\n",
      "Average training loss: 0.035047444000840186\n",
      "Average test loss: 0.0028049734390030306\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03499752268526289\n",
      "Average test loss: 0.0028834863282326197\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034924097130695976\n",
      "Average test loss: 0.0028200946394354104\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03487524834606383\n",
      "Average test loss: 0.0028554365676310327\n",
      "Epoch 109/300\n",
      "Average training loss: 0.034842225693994096\n",
      "Average test loss: 0.00301690235402849\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03474481565422482\n",
      "Average test loss: 0.0028674845550623203\n",
      "Epoch 111/300\n",
      "Average training loss: 0.034729725867509845\n",
      "Average test loss: 0.0027865065986083615\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03468132077819771\n",
      "Average test loss: 0.0028402122602694565\n",
      "Epoch 113/300\n",
      "Average training loss: 0.034600173867411085\n",
      "Average test loss: 0.002841422671245204\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03458622990714179\n",
      "Average test loss: 0.002869064143134488\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0344983288579517\n",
      "Average test loss: 0.0029333986573749117\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03447252661652035\n",
      "Average test loss: 0.00279852812571658\n",
      "Epoch 117/300\n",
      "Average training loss: 0.034436465316348606\n",
      "Average test loss: 0.00282386201288965\n",
      "Epoch 118/300\n",
      "Average training loss: 0.034362507111496396\n",
      "Average test loss: 0.002860405216200484\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03433924114372995\n",
      "Average test loss: 0.002898529771508442\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03428186499244637\n",
      "Average test loss: 0.0028335357933408686\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03425231539209684\n",
      "Average test loss: 0.0028826966668582623\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03434998241563638\n",
      "Average test loss: 0.00297570758457813\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03420524329609341\n",
      "Average test loss: 0.0028489605327033335\n",
      "Epoch 124/300\n",
      "Average training loss: 0.034155198398563597\n",
      "Average test loss: 0.002861800564039085\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03411562604043219\n",
      "Average test loss: 0.002831354145995445\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03402701725562413\n",
      "Average test loss: 0.0028935362454503774\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03400965817107095\n",
      "Average test loss: 0.0028764987755566834\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03400386678179105\n",
      "Average test loss: 0.002960140023380518\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03393814907140202\n",
      "Average test loss: 0.0028777714795950385\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03389413999517759\n",
      "Average test loss: 0.002883577551278803\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03388645444644822\n",
      "Average test loss: 0.002907024240121245\n",
      "Epoch 132/300\n",
      "Average training loss: 0.033819942355155945\n",
      "Average test loss: 0.003028884006664157\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03384096118145519\n",
      "Average test loss: 0.0029171674203955465\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03373727513187461\n",
      "Average test loss: 0.0029793816945619055\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03372532564236058\n",
      "Average test loss: 0.0033109569230841264\n",
      "Epoch 136/300\n",
      "Average training loss: 0.033681585527128646\n",
      "Average test loss: 0.0029169673454016447\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03368333294490973\n",
      "Average test loss: 0.002969866248054637\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03361789689626959\n",
      "Average test loss: 0.002958268493620886\n",
      "Epoch 139/300\n",
      "Average training loss: 0.033596400433116486\n",
      "Average test loss: 0.002904309408739209\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03353336533241802\n",
      "Average test loss: 0.005713002609709899\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0335712360839049\n",
      "Average test loss: 0.002974363568549355\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03348440862529808\n",
      "Average test loss: 0.0028713629469275474\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03344932010769844\n",
      "Average test loss: 0.0028626838185720972\n",
      "Epoch 144/300\n",
      "Average training loss: 0.033463029811779656\n",
      "Average test loss: 0.0029152763997101123\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03342649801903301\n",
      "Average test loss: 0.0030322078847222858\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03337780469324854\n",
      "Average test loss: 0.003069257284204165\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0333407869040966\n",
      "Average test loss: 0.0029634497275369035\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03330932135714425\n",
      "Average test loss: 0.0029209889601916074\n",
      "Epoch 149/300\n",
      "Average training loss: 0.033309586316347124\n",
      "Average test loss: 0.002907599405075113\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03329203400015831\n",
      "Average test loss: 0.00290155170361201\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03322083660960198\n",
      "Average test loss: 0.0028569509068297016\n",
      "Epoch 152/300\n",
      "Average training loss: 0.033170432253016364\n",
      "Average test loss: 0.002922529042388002\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03322735252645281\n",
      "Average test loss: 0.002948665423111783\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03312957743472523\n",
      "Average test loss: 0.002880877620023158\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03313965903056992\n",
      "Average test loss: 0.002916634825575683\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03312008308370908\n",
      "Average test loss: 0.002982867911251055\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03304368539320098\n",
      "Average test loss: 0.009998121253732178\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03300732211603059\n",
      "Average test loss: 0.002900195671038495\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03299503787027465\n",
      "Average test loss: 0.00338721190744804\n",
      "Epoch 160/300\n",
      "Average training loss: 0.033043676594893136\n",
      "Average test loss: 0.002899843192555838\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0329121902220779\n",
      "Average test loss: 0.0029751729923817847\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03292788327071402\n",
      "Average test loss: 0.0029588294064419136\n",
      "Epoch 163/300\n",
      "Average training loss: 0.032933738794591695\n",
      "Average test loss: 0.002861276081763208\n",
      "Epoch 164/300\n",
      "Average training loss: 0.032900417062971324\n",
      "Average test loss: 0.002990889365888304\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03289631898370054\n",
      "Average test loss: 0.003349252837089201\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03281183477573925\n",
      "Average test loss: 0.0028965923262553083\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03280916545126173\n",
      "Average test loss: 0.002947570585645735\n",
      "Epoch 168/300\n",
      "Average training loss: 0.032785435979564986\n",
      "Average test loss: 0.0029486809906860193\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03271878110369047\n",
      "Average test loss: 0.002878449600810806\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03275369604594178\n",
      "Average test loss: 0.00295907933347755\n",
      "Epoch 171/300\n",
      "Average training loss: 0.032808078391684425\n",
      "Average test loss: 0.0029042134963803822\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03270709262622727\n",
      "Average test loss: 0.0029808170799579883\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03263976471622785\n",
      "Average test loss: 0.0029292067976461516\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0326690786547131\n",
      "Average test loss: 0.0030004203741749126\n",
      "Epoch 175/300\n",
      "Average training loss: 0.032653568347295124\n",
      "Average test loss: 0.002975292413598961\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03257454936371909\n",
      "Average test loss: 0.0030514555320971543\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03261648736397425\n",
      "Average test loss: 0.0028747007701959873\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03254151801599397\n",
      "Average test loss: 0.0029677692869057256\n",
      "Epoch 179/300\n",
      "Average training loss: 0.032580685016181735\n",
      "Average test loss: 0.0030284529742267397\n",
      "Epoch 180/300\n",
      "Average training loss: 0.032594911015696\n",
      "Average test loss: 0.0036279281965560383\n",
      "Epoch 181/300\n",
      "Average training loss: 0.032525862567954596\n",
      "Average test loss: 0.002982705319714215\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03244249000814226\n",
      "Average test loss: 0.002907840029439992\n",
      "Epoch 183/300\n",
      "Average training loss: 0.032492969973219765\n",
      "Average test loss: 0.0029243914123831525\n",
      "Epoch 184/300\n",
      "Average training loss: 0.032461055556933086\n",
      "Average test loss: 0.00295738957884411\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03244039056698481\n",
      "Average test loss: 0.0029528540807465713\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03238298275404506\n",
      "Average test loss: 0.0029427030086517335\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03240409082174301\n",
      "Average test loss: 0.0029797512497752904\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03237506533993615\n",
      "Average test loss: 0.0029601584039628506\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03238150755398803\n",
      "Average test loss: 0.0031522672339859936\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0323562656905916\n",
      "Average test loss: 0.0029652230653704867\n",
      "Epoch 191/300\n",
      "Average training loss: 0.032370987431870564\n",
      "Average test loss: 0.0037037840768073997\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03230626825326019\n",
      "Average test loss: 0.0029289263161934084\n",
      "Epoch 193/300\n",
      "Average training loss: 0.032258620275391475\n",
      "Average test loss: 0.002964067578315735\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03222707321908739\n",
      "Average test loss: 0.002923957920736737\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03226670874820815\n",
      "Average test loss: 0.003071489865374234\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03226803336540858\n",
      "Average test loss: 0.0029318516701459883\n",
      "Epoch 197/300\n",
      "Average training loss: 0.032215984231895874\n",
      "Average test loss: 0.0029305739272385838\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03213230772150887\n",
      "Average test loss: 0.002955401427216\n",
      "Epoch 199/300\n",
      "Average training loss: 0.032177510652277204\n",
      "Average test loss: 0.003060969251104527\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03214527814586957\n",
      "Average test loss: 0.0029841995319972434\n",
      "Epoch 201/300\n",
      "Average training loss: 0.032155531404746904\n",
      "Average test loss: 0.003046125018969178\n",
      "Epoch 202/300\n",
      "Average training loss: 0.032189491700794964\n",
      "Average test loss: 0.0028944861870259047\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03211452320218086\n",
      "Average test loss: 0.0031249179458245637\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03208579890926679\n",
      "Average test loss: 0.0029563282421893543\n",
      "Epoch 205/300\n",
      "Average training loss: 0.032105780363082884\n",
      "Average test loss: 0.003030559521996313\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03211760467622015\n",
      "Average test loss: 0.002954897588222391\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0321757077831361\n",
      "Average test loss: 0.0031152031225048834\n",
      "Epoch 208/300\n",
      "Average training loss: 0.032002923836310704\n",
      "Average test loss: 0.00296331493018402\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03200092508395513\n",
      "Average test loss: 0.0029760478776362205\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03201669206718604\n",
      "Average test loss: 0.002937570900345842\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03201554625233014\n",
      "Average test loss: 0.0029285569393800366\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03200430730647511\n",
      "Average test loss: 0.002915318856636683\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03194027747048272\n",
      "Average test loss: 0.003000974003225565\n",
      "Epoch 214/300\n",
      "Average training loss: 0.031894647308521804\n",
      "Average test loss: 0.0030125558024479285\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03189570635557175\n",
      "Average test loss: 0.0030053879220245613\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03191186036997371\n",
      "Average test loss: 0.002986271747077505\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03190260442760256\n",
      "Average test loss: 0.0030579804587695333\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031892224007182654\n",
      "Average test loss: 0.0030049749120242067\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0318394841733906\n",
      "Average test loss: 0.003235870969792207\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03181362563868364\n",
      "Average test loss: 0.003329812075942755\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03185744106769562\n",
      "Average test loss: 0.0029612130817646782\n",
      "Epoch 222/300\n",
      "Average training loss: 0.031795754740635555\n",
      "Average test loss: 0.0030758576821535827\n",
      "Epoch 223/300\n",
      "Average training loss: 0.031842456844117906\n",
      "Average test loss: 0.0031614774010247653\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03177022214730581\n",
      "Average test loss: 0.0029368268185191683\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03177129426929686\n",
      "Average test loss: 0.0031990468464791776\n",
      "Epoch 226/300\n",
      "Average training loss: 0.031751560350259146\n",
      "Average test loss: 0.002955557050390376\n",
      "Epoch 227/300\n",
      "Average training loss: 0.031752026932107075\n",
      "Average test loss: 0.003015770998679929\n",
      "Epoch 228/300\n",
      "Average training loss: 0.031698758323987324\n",
      "Average test loss: 0.0030103840539231898\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03172164948119058\n",
      "Average test loss: 0.002982491594221857\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03167070907685492\n",
      "Average test loss: 0.003056306844784154\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03171534588105149\n",
      "Average test loss: 0.003043721301895049\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0316769268496169\n",
      "Average test loss: 0.00304136549640033\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03163558396531476\n",
      "Average test loss: 0.0030380670542104376\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03165098541975021\n",
      "Average test loss: 0.0029466265197843314\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03161919565995534\n",
      "Average test loss: 0.0030120189676268233\n",
      "Epoch 236/300\n",
      "Average training loss: 0.031644902881648805\n",
      "Average test loss: 0.0030483703032756844\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03164528176519606\n",
      "Average test loss: 0.002964468989521265\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03156791806717714\n",
      "Average test loss: 0.002977855988053812\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03160911230577363\n",
      "Average test loss: 0.0029748859556598794\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03163641094830301\n",
      "Average test loss: 0.0029822561043418116\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03155475811991427\n",
      "Average test loss: 0.0029788744944251244\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03154064133928882\n",
      "Average test loss: 0.002993026156599323\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03154014659424623\n",
      "Average test loss: 0.003016586293362909\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031551219968332184\n",
      "Average test loss: 0.0034844784918758604\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03151986238360405\n",
      "Average test loss: 0.002997631520653764\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0314766690797276\n",
      "Average test loss: 0.0030086982800728744\n",
      "Epoch 247/300\n",
      "Average training loss: 0.031480696734454895\n",
      "Average test loss: 0.0029706089958134625\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0314988976418972\n",
      "Average test loss: 0.003064459647362431\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03149792170524597\n",
      "Average test loss: 0.003230929234996438\n",
      "Epoch 250/300\n",
      "Average training loss: 0.031444724629322685\n",
      "Average test loss: 0.0029211867635862694\n",
      "Epoch 251/300\n",
      "Average training loss: 0.031419731679889894\n",
      "Average test loss: 0.0029672326462136375\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03145436914265156\n",
      "Average test loss: 0.003069538736095031\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03142763452728589\n",
      "Average test loss: 0.002984604023086528\n",
      "Epoch 254/300\n",
      "Average training loss: 0.031396259473429784\n",
      "Average test loss: 0.0035218290285103852\n",
      "Epoch 255/300\n",
      "Average training loss: 0.031362215841809905\n",
      "Average test loss: 0.003022427621194058\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03133449143005742\n",
      "Average test loss: 0.0030358272844718563\n",
      "Epoch 257/300\n",
      "Average training loss: 0.031371946540143754\n",
      "Average test loss: 0.0030760729662660095\n",
      "Epoch 258/300\n",
      "Average training loss: 0.031381760882006754\n",
      "Average test loss: 0.002967881267165972\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03132516285114818\n",
      "Average test loss: 0.0030353727721505693\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03134210129910045\n",
      "Average test loss: 0.00308378392809795\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03135585400958856\n",
      "Average test loss: 0.0029587790806674297\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03131021085381508\n",
      "Average test loss: 0.003132242803979251\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03129798358347681\n",
      "Average test loss: 0.0029580656741228367\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03130877876447307\n",
      "Average test loss: 0.003061236067985495\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03133482097254859\n",
      "Average test loss: 0.002941236407185594\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03129374912712309\n",
      "Average test loss: 0.003006944237380392\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03125276963909467\n",
      "Average test loss: 0.002998711873880691\n",
      "Epoch 268/300\n",
      "Average training loss: 0.031272234501110184\n",
      "Average test loss: 0.0029938464537262914\n",
      "Epoch 269/300\n",
      "Average training loss: 0.031280428207582894\n",
      "Average test loss: 0.0029832595879625942\n",
      "Epoch 270/300\n",
      "Average training loss: 0.031220410737726422\n",
      "Average test loss: 0.0030119703722496826\n",
      "Epoch 271/300\n",
      "Average training loss: 0.031187475677993563\n",
      "Average test loss: 0.002999392916758855\n",
      "Epoch 272/300\n",
      "Average training loss: 0.031178665896256764\n",
      "Average test loss: 0.00306740842324992\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03117340680791272\n",
      "Average test loss: 0.003066826329463058\n",
      "Epoch 274/300\n",
      "Average training loss: 0.031221962723467084\n",
      "Average test loss: 0.0029896264041049613\n",
      "Epoch 275/300\n",
      "Average training loss: 0.031178752149144808\n",
      "Average test loss: 0.003134535802735223\n",
      "Epoch 276/300\n",
      "Average training loss: 0.031240145628650982\n",
      "Average test loss: 0.0030718958386116556\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03116448243955771\n",
      "Average test loss: 0.0030363444032975367\n",
      "Epoch 278/300\n",
      "Average training loss: 0.031128951211770374\n",
      "Average test loss: 0.003110572776860661\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03117263686325815\n",
      "Average test loss: 0.010899740556461944\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03122384540571107\n",
      "Average test loss: 0.0030265405455397235\n",
      "Epoch 281/300\n",
      "Average training loss: 0.031090588900778027\n",
      "Average test loss: 0.0029396546735531754\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0311164568281836\n",
      "Average test loss: 0.0030163489484952557\n",
      "Epoch 283/300\n",
      "Average training loss: 0.031090444144275454\n",
      "Average test loss: 0.003070700022391975\n",
      "Epoch 284/300\n",
      "Average training loss: 0.031088411771588855\n",
      "Average test loss: 0.002975756980685724\n",
      "Epoch 285/300\n",
      "Average training loss: 0.031114356661836307\n",
      "Average test loss: 0.003095180151156253\n",
      "Epoch 286/300\n",
      "Average training loss: 0.031055259409877987\n",
      "Average test loss: 0.003098018046261536\n",
      "Epoch 287/300\n",
      "Average training loss: 0.031063162378138966\n",
      "Average test loss: 0.0029996174896756806\n",
      "Epoch 288/300\n",
      "Average training loss: 0.031024769322739707\n",
      "Average test loss: 0.003035295894679924\n",
      "Epoch 289/300\n",
      "Average training loss: 0.031081719944874447\n",
      "Average test loss: 0.0030659241423838667\n",
      "Epoch 290/300\n",
      "Average training loss: 0.031029899875322977\n",
      "Average test loss: 0.003042592666215367\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03103427759806315\n",
      "Average test loss: 0.0029963549874309037\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03102118432852957\n",
      "Average test loss: 0.0032206157034056055\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030994041214386623\n",
      "Average test loss: 0.0030292326509952544\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030991575711303287\n",
      "Average test loss: 0.003124062263303333\n",
      "Epoch 295/300\n",
      "Average training loss: 0.031039574864837858\n",
      "Average test loss: 0.003011428590450022\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030975285563204023\n",
      "Average test loss: 0.003107992131056057\n",
      "Epoch 297/300\n",
      "Average training loss: 0.030960233837366104\n",
      "Average test loss: 0.0030247518252581357\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03100510395069917\n",
      "Average test loss: 0.0030584941976186304\n",
      "Epoch 299/300\n",
      "Average training loss: 0.030921599646409354\n",
      "Average test loss: 0.00302429106252061\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03093387594819069\n",
      "Average test loss: 0.006730709483640062\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7948195751243168\n",
      "Average test loss: 0.004639213467637698\n",
      "Epoch 2/300\n",
      "Average training loss: 0.19693044300874074\n",
      "Average test loss: 0.0037781675797369744\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11379795157909393\n",
      "Average test loss: 0.0034935059007257224\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08563810430632697\n",
      "Average test loss: 0.0033203202366001074\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07258366864919663\n",
      "Average test loss: 0.0031274852131803832\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06484089175197813\n",
      "Average test loss: 0.0030416565138018793\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05953979845510589\n",
      "Average test loss: 0.00299979533172316\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05562676056226094\n",
      "Average test loss: 0.0028482666739987\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05262119949526257\n",
      "Average test loss: 0.002744292257250183\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05028669778174824\n",
      "Average test loss: 0.00286550128315058\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04836141449213028\n",
      "Average test loss: 0.002627038510309325\n",
      "Epoch 12/300\n",
      "Average training loss: 0.046844614081912574\n",
      "Average test loss: 0.002605262541315622\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04557263555129369\n",
      "Average test loss: 0.0025377544332295656\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04450026673409674\n",
      "Average test loss: 0.0025306682338317233\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04348287609550688\n",
      "Average test loss: 0.0024662672264708415\n",
      "Epoch 16/300\n",
      "Average training loss: 0.042660969227552414\n",
      "Average test loss: 0.002461591237328119\n",
      "Epoch 17/300\n",
      "Average training loss: 0.041844248473644256\n",
      "Average test loss: 0.0023971606124606397\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04111813006798426\n",
      "Average test loss: 0.002444066749678718\n",
      "Epoch 19/300\n",
      "Average training loss: 0.040440691858530046\n",
      "Average test loss: 0.0024069222826510668\n",
      "Epoch 20/300\n",
      "Average training loss: 0.039823666075865426\n",
      "Average test loss: 0.0023462929094417227\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03925239494111803\n",
      "Average test loss: 0.0027277745126436156\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03873659070001708\n",
      "Average test loss: 0.0022930701739258237\n",
      "Epoch 23/300\n",
      "Average training loss: 0.038230323473612465\n",
      "Average test loss: 0.002365379085764289\n",
      "Epoch 24/300\n",
      "Average training loss: 0.037779132313198516\n",
      "Average test loss: 0.002356469204649329\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03736297947996192\n",
      "Average test loss: 0.002258710166128973\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03690109736389584\n",
      "Average test loss: 0.0022817980425639283\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03660638393130567\n",
      "Average test loss: 0.0022484620159698857\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03624375189675225\n",
      "Average test loss: 0.0022526320724023715\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03593770624862777\n",
      "Average test loss: 0.0022319582187467152\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03559433707098166\n",
      "Average test loss: 0.0022236606551127303\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03537970942589972\n",
      "Average test loss: 0.0022071111496124005\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03510264109571775\n",
      "Average test loss: 0.002196920368613468\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03489813475476371\n",
      "Average test loss: 0.002201103064438535\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03469911474982897\n",
      "Average test loss: 0.002197089520800445\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03452173389659988\n",
      "Average test loss: 0.0022195860339949526\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03429280054569244\n",
      "Average test loss: 0.0021811588772883018\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03417106916507085\n",
      "Average test loss: 0.0021895993575453757\n",
      "Epoch 38/300\n",
      "Average training loss: 0.033941950708627704\n",
      "Average test loss: 0.0021733970205403035\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03375846237772041\n",
      "Average test loss: 0.002172423066571355\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03362986260321405\n",
      "Average test loss: 0.0021703851212643913\n",
      "Epoch 41/300\n",
      "Average training loss: 0.033489754595690306\n",
      "Average test loss: 0.002175173471578293\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03336288234094779\n",
      "Average test loss: 0.0021634559329185223\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03324470722178618\n",
      "Average test loss: 0.0021561194381987054\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03308919990559419\n",
      "Average test loss: 0.0022180528839429218\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03296330021487342\n",
      "Average test loss: 0.0021563814257582027\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03282587641643153\n",
      "Average test loss: 0.00217995238956064\n",
      "Epoch 47/300\n",
      "Average training loss: 0.032662913100587\n",
      "Average test loss: 0.002155282642899288\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03259842799603939\n",
      "Average test loss: 0.0021887915428313942\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03249316157400608\n",
      "Average test loss: 0.002220566362297783\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03233208883802096\n",
      "Average test loss: 0.0022111859652731154\n",
      "Epoch 51/300\n",
      "Average training loss: 0.032213725184400875\n",
      "Average test loss: 0.0021496858033869\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03222159754898813\n",
      "Average test loss: 0.002164054234719111\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0320163171324465\n",
      "Average test loss: 0.0021671147170580097\n",
      "Epoch 54/300\n",
      "Average training loss: 0.031919588305883935\n",
      "Average test loss: 0.0021596519706977737\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03178717224796613\n",
      "Average test loss: 0.002144993309966392\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03171537237366041\n",
      "Average test loss: 0.0021654831676019563\n",
      "Epoch 57/300\n",
      "Average training loss: 0.031609296575188635\n",
      "Average test loss: 0.0021597801045411162\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03158107728262742\n",
      "Average test loss: 0.002182408936114775\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03140826525621944\n",
      "Average test loss: 0.002171469236620598\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0312957971394062\n",
      "Average test loss: 0.0021597351127614576\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03116770618326134\n",
      "Average test loss: 0.002166089969790644\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0310825474427806\n",
      "Average test loss: 0.002184731692593131\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03098303943541315\n",
      "Average test loss: 0.0021553057328694398\n",
      "Epoch 64/300\n",
      "Average training loss: 0.030899536708990732\n",
      "Average test loss: 0.0021673005384703478\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03094955676794052\n",
      "Average test loss: 0.0021672183587733243\n",
      "Epoch 66/300\n",
      "Average training loss: 0.030747616420189538\n",
      "Average test loss: 0.002166313123992748\n",
      "Epoch 67/300\n",
      "Average training loss: 0.030669965270492764\n",
      "Average test loss: 0.002223540219374829\n",
      "Epoch 68/300\n",
      "Average training loss: 0.030533537609709633\n",
      "Average test loss: 0.0021801175224698253\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03043964058823056\n",
      "Average test loss: 0.002220966603193018\n",
      "Epoch 70/300\n",
      "Average training loss: 0.030434613171550964\n",
      "Average test loss: 0.002407741774080528\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03039792569147216\n",
      "Average test loss: 0.0021847081151273517\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03016168042520682\n",
      "Average test loss: 0.0022197951565807064\n",
      "Epoch 73/300\n",
      "Average training loss: 0.030204139260782137\n",
      "Average test loss: 0.002205925350801812\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03003846187227302\n",
      "Average test loss: 0.002189806387035383\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02993444121380647\n",
      "Average test loss: 0.0022066759262233974\n",
      "Epoch 76/300\n",
      "Average training loss: 0.029903340452247195\n",
      "Average test loss: 0.0021993085847546658\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02981998041768869\n",
      "Average test loss: 0.0024431444234732126\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02972635539041625\n",
      "Average test loss: 0.0022713997893863253\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02974405494166745\n",
      "Average test loss: 0.002531861275848415\n",
      "Epoch 80/300\n",
      "Average training loss: 0.029601689888371363\n",
      "Average test loss: 0.002233278075233102\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02953200862970617\n",
      "Average test loss: 0.0021766830450958676\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02943408504128456\n",
      "Average test loss: 0.0021959447003901005\n",
      "Epoch 83/300\n",
      "Average training loss: 0.029380448054936198\n",
      "Average test loss: 0.002198317687958479\n",
      "Epoch 84/300\n",
      "Average training loss: 0.029286643031570646\n",
      "Average test loss: 0.002219753180940946\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02926802097260952\n",
      "Average test loss: 0.0022322116440369024\n",
      "Epoch 86/300\n",
      "Average training loss: 0.029203332889411186\n",
      "Average test loss: 0.0029295391846034264\n",
      "Epoch 87/300\n",
      "Average training loss: 0.029120525386598376\n",
      "Average test loss: 0.002203511615594228\n",
      "Epoch 88/300\n",
      "Average training loss: 0.029069717622465557\n",
      "Average test loss: 0.0022349183873997796\n",
      "Epoch 89/300\n",
      "Average training loss: 0.029000799357891082\n",
      "Average test loss: 0.0022993267100925247\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02893138686981466\n",
      "Average test loss: 0.0022312558483746315\n",
      "Epoch 91/300\n",
      "Average training loss: 0.028879635052548515\n",
      "Average test loss: 0.002290918501507905\n",
      "Epoch 92/300\n",
      "Average training loss: 0.028826809111568662\n",
      "Average test loss: 0.002230249054212537\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0287929018156396\n",
      "Average test loss: 0.002243809342591299\n",
      "Epoch 94/300\n",
      "Average training loss: 0.028732385117146705\n",
      "Average test loss: 0.0022532819770276547\n",
      "Epoch 95/300\n",
      "Average training loss: 0.028635029445091883\n",
      "Average test loss: 0.00222813885503759\n",
      "Epoch 96/300\n",
      "Average training loss: 0.028662998076942232\n",
      "Average test loss: 0.002247280964731342\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02857798682815499\n",
      "Average test loss: 0.0022490776545471616\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02848566903836197\n",
      "Average test loss: 0.0022237084383765855\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028493363367186653\n",
      "Average test loss: 0.0022286969007303315\n",
      "Epoch 100/300\n",
      "Average training loss: 0.028404074528151087\n",
      "Average test loss: 0.0022204885421734715\n",
      "Epoch 101/300\n",
      "Average training loss: 0.028362844513522254\n",
      "Average test loss: 0.002218677697289321\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02830515026880635\n",
      "Average test loss: 0.0022302338203622233\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02827988033162223\n",
      "Average test loss: 0.0023394321971055535\n",
      "Epoch 104/300\n",
      "Average training loss: 0.028233925589256817\n",
      "Average test loss: 0.0022850181642505856\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02816082660191589\n",
      "Average test loss: 0.00223313639106022\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02815235473877854\n",
      "Average test loss: 0.0022567599597904417\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02808481362462044\n",
      "Average test loss: 0.0022904315176937315\n",
      "Epoch 108/300\n",
      "Average training loss: 0.028035758384399943\n",
      "Average test loss: 0.0023820273406389685\n",
      "Epoch 109/300\n",
      "Average training loss: 0.028026424194375673\n",
      "Average test loss: 0.0022918944329851205\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02801157381468349\n",
      "Average test loss: 0.002315951167502337\n",
      "Epoch 111/300\n",
      "Average training loss: 0.027903673829303848\n",
      "Average test loss: 0.0022463776225017176\n",
      "Epoch 112/300\n",
      "Average training loss: 0.027868765792912906\n",
      "Average test loss: 0.0023291292052922976\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027863741752174164\n",
      "Average test loss: 0.0022923348862677814\n",
      "Epoch 114/300\n",
      "Average training loss: 0.027842540863487454\n",
      "Average test loss: 0.0022413943625158735\n",
      "Epoch 115/300\n",
      "Average training loss: 0.027789138578706317\n",
      "Average test loss: 0.0023099216173092523\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02772540237175094\n",
      "Average test loss: 0.0022506442500485314\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02779119304815928\n",
      "Average test loss: 0.0023596829211132396\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027676604534188905\n",
      "Average test loss: 0.002328510919585824\n",
      "Epoch 119/300\n",
      "Average training loss: 0.027675311310423743\n",
      "Average test loss: 0.0022533894437882637\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02754942831893762\n",
      "Average test loss: 0.002316849629705151\n",
      "Epoch 121/300\n",
      "Average training loss: 0.027560429016749065\n",
      "Average test loss: 0.0022653974186008174\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02754108224478033\n",
      "Average test loss: 0.0023214715692318148\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027526905294921664\n",
      "Average test loss: 0.0022440147617210945\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027509801242086622\n",
      "Average test loss: 0.002311346591139833\n",
      "Epoch 125/300\n",
      "Average training loss: 0.027453311751286187\n",
      "Average test loss: 0.002448942465500699\n",
      "Epoch 126/300\n",
      "Average training loss: 0.027507069397303793\n",
      "Average test loss: 0.0022975805033412246\n",
      "Epoch 127/300\n",
      "Average training loss: 0.027373330718941158\n",
      "Average test loss: 0.0023146365436000956\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02736716796292199\n",
      "Average test loss: 0.0022772215855204397\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02732950972351763\n",
      "Average test loss: 0.0022782650767929026\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02728060957458284\n",
      "Average test loss: 0.002307744094481071\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02724824928243955\n",
      "Average test loss: 0.002270066244941619\n",
      "Epoch 132/300\n",
      "Average training loss: 0.027236839892135725\n",
      "Average test loss: 0.0023128187358379365\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02723483650883039\n",
      "Average test loss: 0.0022819575848471788\n",
      "Epoch 134/300\n",
      "Average training loss: 0.027180549467603364\n",
      "Average test loss: 0.002319217798920969\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02717012547618813\n",
      "Average test loss: 0.002366813905744089\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02712037234339449\n",
      "Average test loss: 0.002424762670778566\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02716232415040334\n",
      "Average test loss: 0.0022891755030593937\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02708471514284611\n",
      "Average test loss: 0.002378929040498204\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02703878198729621\n",
      "Average test loss: 0.002295690011026131\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02706553236146768\n",
      "Average test loss: 0.0022663785430292287\n",
      "Epoch 141/300\n",
      "Average training loss: 0.027040311727258893\n",
      "Average test loss: 0.002405682368203998\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027004721076952086\n",
      "Average test loss: 0.0023148106477326818\n",
      "Epoch 143/300\n",
      "Average training loss: 0.026924351389209428\n",
      "Average test loss: 0.0023389386342217524\n",
      "Epoch 144/300\n",
      "Average training loss: 0.026893156265219053\n",
      "Average test loss: 0.0022778901747531363\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02694365986684958\n",
      "Average test loss: 0.002311604833540817\n",
      "Epoch 146/300\n",
      "Average training loss: 0.026906943910651737\n",
      "Average test loss: 0.0022982666701492336\n",
      "Epoch 147/300\n",
      "Average training loss: 0.026839236136939792\n",
      "Average test loss: 0.0022995904684066774\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02679171359207895\n",
      "Average test loss: 0.0022980353370722797\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026864335694246823\n",
      "Average test loss: 0.0023894693427201773\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02678040345840984\n",
      "Average test loss: 0.0023363897072979146\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02676998071703646\n",
      "Average test loss: 0.002307737944026788\n",
      "Epoch 152/300\n",
      "Average training loss: 0.026731316657529938\n",
      "Average test loss: 0.002322925353836682\n",
      "Epoch 153/300\n",
      "Average training loss: 0.026764994170930652\n",
      "Average test loss: 0.002343922623536653\n",
      "Epoch 154/300\n",
      "Average training loss: 0.026722175291842884\n",
      "Average test loss: 0.0022918494306504726\n",
      "Epoch 155/300\n",
      "Average training loss: 0.026687501738468806\n",
      "Average test loss: 0.002294152851837377\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02664514739976989\n",
      "Average test loss: 0.0023105780047674974\n",
      "Epoch 157/300\n",
      "Average training loss: 0.026667520355847147\n",
      "Average test loss: 0.002369501586899989\n",
      "Epoch 158/300\n",
      "Average training loss: 0.026662043001916675\n",
      "Average test loss: 0.0023121380056771966\n",
      "Epoch 159/300\n",
      "Average training loss: 0.026578557615478834\n",
      "Average test loss: 0.0023472191244363785\n",
      "Epoch 160/300\n",
      "Average training loss: 0.026587762569387753\n",
      "Average test loss: 0.0023195386951168377\n",
      "Epoch 161/300\n",
      "Average training loss: 0.026534019865923457\n",
      "Average test loss: 0.002335855626397663\n",
      "Epoch 162/300\n",
      "Average training loss: 0.026560330536630417\n",
      "Average test loss: 0.0022920485347923307\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026490821131401592\n",
      "Average test loss: 0.002418066090800696\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026473516056934994\n",
      "Average test loss: 0.0023219998837966057\n",
      "Epoch 165/300\n",
      "Average training loss: 0.026445837795734405\n",
      "Average test loss: 0.0023065873957756494\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026495337280962202\n",
      "Average test loss: 0.00233722809350325\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02644441759917471\n",
      "Average test loss: 0.0024547042498985925\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026389636996719572\n",
      "Average test loss: 0.002373543857700295\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026458054319024087\n",
      "Average test loss: 0.0023574695804466804\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02640695818927553\n",
      "Average test loss: 0.002392593501135707\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02638544090754456\n",
      "Average test loss: 0.002395357504280077\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02640246532195144\n",
      "Average test loss: 0.0025921998599337205\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02636122176878982\n",
      "Average test loss: 0.002373423762205574\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026319477887617217\n",
      "Average test loss: 0.0023801900364665523\n",
      "Epoch 175/300\n",
      "Average training loss: 0.026315345979399152\n",
      "Average test loss: 0.002313397615113192\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02627720417910152\n",
      "Average test loss: 0.0023668337715789674\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026270842653181817\n",
      "Average test loss: 0.002315017624861664\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02629220664170053\n",
      "Average test loss: 0.0023538622551908094\n",
      "Epoch 179/300\n",
      "Average training loss: 0.026210528583990204\n",
      "Average test loss: 0.0023120768351687325\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02620701885720094\n",
      "Average test loss: 0.0023631928014672464\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026263989061117172\n",
      "Average test loss: 0.002382498373070525\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026208981353375647\n",
      "Average test loss: 0.002365630460696088\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02617085560493999\n",
      "Average test loss: 0.00237077180792888\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026201221636599964\n",
      "Average test loss: 0.002430776841731535\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026127220928668975\n",
      "Average test loss: 0.0023848941706948812\n",
      "Epoch 186/300\n",
      "Average training loss: 0.026134389446841346\n",
      "Average test loss: 0.0023525657134337557\n",
      "Epoch 187/300\n",
      "Average training loss: 0.026137874349951743\n",
      "Average test loss: 0.0024368407997406192\n",
      "Epoch 188/300\n",
      "Average training loss: 0.026118763430251015\n",
      "Average test loss: 0.0023331277968568933\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026100210931566028\n",
      "Average test loss: 0.0023625445438341963\n",
      "Epoch 190/300\n",
      "Average training loss: 0.026188778756393326\n",
      "Average test loss: 0.002374371122672326\n",
      "Epoch 191/300\n",
      "Average training loss: 0.026080746170547273\n",
      "Average test loss: 0.002529368121176958\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02604348468283812\n",
      "Average test loss: 0.0024090355152471196\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026040831769506136\n",
      "Average test loss: 0.002319650853259696\n",
      "Epoch 194/300\n",
      "Average training loss: 0.026004049147168796\n",
      "Average test loss: 0.0023905266373314793\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026035584956407547\n",
      "Average test loss: 0.0023832822609692814\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02602966879473792\n",
      "Average test loss: 0.0023697667128096023\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026018592523203956\n",
      "Average test loss: 0.00238049984164536\n",
      "Epoch 198/300\n",
      "Average training loss: 0.025948030152254633\n",
      "Average test loss: 0.00234055692050606\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02592233929203616\n",
      "Average test loss: 0.002387399126258161\n",
      "Epoch 200/300\n",
      "Average training loss: 0.025926251121693188\n",
      "Average test loss: 0.002414410407551461\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02595501537124316\n",
      "Average test loss: 0.002357093270454142\n",
      "Epoch 202/300\n",
      "Average training loss: 0.025942175537347793\n",
      "Average test loss: 0.0023784318543556664\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02592716656294134\n",
      "Average test loss: 0.002391456713486049\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0259019838621219\n",
      "Average test loss: 0.002342245821116699\n",
      "Epoch 205/300\n",
      "Average training loss: 0.025872282167275745\n",
      "Average test loss: 0.0024452169835567474\n",
      "Epoch 206/300\n",
      "Average training loss: 0.025865095040864414\n",
      "Average test loss: 0.0023786694556474685\n",
      "Epoch 207/300\n",
      "Average training loss: 0.025893201928999687\n",
      "Average test loss: 0.0023596811646388635\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02583986429207855\n",
      "Average test loss: 0.002525659757252369\n",
      "Epoch 209/300\n",
      "Average training loss: 0.025848164008723366\n",
      "Average test loss: 0.0023714683471868434\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025845630925562645\n",
      "Average test loss: 0.0023369697800113094\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02579708403845628\n",
      "Average test loss: 0.002370271973311901\n",
      "Epoch 212/300\n",
      "Average training loss: 0.025800766191548773\n",
      "Average test loss: 0.002355204386740095\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02574972681204478\n",
      "Average test loss: 0.002411566335397462\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02578415803776847\n",
      "Average test loss: 0.002364352519934376\n",
      "Epoch 215/300\n",
      "Average training loss: 0.025767885928352673\n",
      "Average test loss: 0.0023866577804502512\n",
      "Epoch 216/300\n",
      "Average training loss: 0.025716052075227102\n",
      "Average test loss: 0.002433329584490922\n",
      "Epoch 217/300\n",
      "Average training loss: 0.025748899628718694\n",
      "Average test loss: 0.002444469296269947\n",
      "Epoch 218/300\n",
      "Average training loss: 0.025747662923402255\n",
      "Average test loss: 0.0023967252362312543\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02571397903561592\n",
      "Average test loss: 0.0024271504316065048\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02569255331489775\n",
      "Average test loss: 0.0023674337344451084\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02571857139137056\n",
      "Average test loss: 0.0024025167230930593\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02569367170168294\n",
      "Average test loss: 0.0023528032667107053\n",
      "Epoch 223/300\n",
      "Average training loss: 0.025684662717911932\n",
      "Average test loss: 0.002388936523348093\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02564087638590071\n",
      "Average test loss: 0.002446933201410704\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02563866405023469\n",
      "Average test loss: 0.0024099703155871896\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02565776715344853\n",
      "Average test loss: 0.00237462499530779\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02560812167492178\n",
      "Average test loss: 0.0024402419552206995\n",
      "Epoch 228/300\n",
      "Average training loss: 0.025608030825853348\n",
      "Average test loss: 0.0025534525328419276\n",
      "Epoch 229/300\n",
      "Average training loss: 0.025624026576677957\n",
      "Average test loss: 0.002380955561581585\n",
      "Epoch 230/300\n",
      "Average training loss: 0.025591947635014854\n",
      "Average test loss: 0.0024129165806290177\n",
      "Epoch 231/300\n",
      "Average training loss: 0.025619244365228548\n",
      "Average test loss: 0.0023396971279548272\n",
      "Epoch 232/300\n",
      "Average training loss: 0.025601396441459656\n",
      "Average test loss: 0.0023866021488275794\n",
      "Epoch 233/300\n",
      "Average training loss: 0.025580262477199238\n",
      "Average test loss: 0.002461945914973815\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02558249351878961\n",
      "Average test loss: 0.0026288321558386087\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02551706458793746\n",
      "Average test loss: 0.002358759519747562\n",
      "Epoch 236/300\n",
      "Average training loss: 0.025558231209715208\n",
      "Average test loss: 0.0026576656688832574\n",
      "Epoch 237/300\n",
      "Average training loss: 0.025512297216388913\n",
      "Average test loss: 0.0023696821202627487\n",
      "Epoch 238/300\n",
      "Average training loss: 0.025493969425559045\n",
      "Average test loss: 0.0024084279742091893\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025511522917283907\n",
      "Average test loss: 0.0023529249986426696\n",
      "Epoch 240/300\n",
      "Average training loss: 0.025497095493806732\n",
      "Average test loss: 0.002537026413405935\n",
      "Epoch 241/300\n",
      "Average training loss: 0.025490710551540056\n",
      "Average test loss: 0.0024166646952637366\n",
      "Epoch 242/300\n",
      "Average training loss: 0.025481080169479052\n",
      "Average test loss: 0.0024252765984791847\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0254805808150106\n",
      "Average test loss: 0.0025032575226699313\n",
      "Epoch 244/300\n",
      "Average training loss: 0.025468908051649728\n",
      "Average test loss: 0.0023528853026736113\n",
      "Epoch 245/300\n",
      "Average training loss: 0.025465220001008777\n",
      "Average test loss: 0.002363088841860493\n",
      "Epoch 246/300\n",
      "Average training loss: 0.025452905178070067\n",
      "Average test loss: 0.0024134334514124525\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02547098148200247\n",
      "Average test loss: 0.002596363530597753\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02541445141699579\n",
      "Average test loss: 0.0027296556807640524\n",
      "Epoch 249/300\n",
      "Average training loss: 0.025418283868167135\n",
      "Average test loss: 0.0023739117400513755\n",
      "Epoch 250/300\n",
      "Average training loss: 0.025424972721272046\n",
      "Average test loss: 0.00234039717209008\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02541213487419817\n",
      "Average test loss: 0.0023775676482667525\n",
      "Epoch 252/300\n",
      "Average training loss: 0.025386766150593758\n",
      "Average test loss: 0.002384441952738497\n",
      "Epoch 253/300\n",
      "Average training loss: 0.025399159186416203\n",
      "Average test loss: 0.002373855494790607\n",
      "Epoch 254/300\n",
      "Average training loss: 0.025364844267566997\n",
      "Average test loss: 0.003330592987024122\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025355917404095332\n",
      "Average test loss: 0.0025974065237161187\n",
      "Epoch 256/300\n",
      "Average training loss: 0.025367933922343783\n",
      "Average test loss: 0.002381520503924953\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025371152384413613\n",
      "Average test loss: 0.002437567258460654\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025321213960647584\n",
      "Average test loss: 0.002409193792897794\n",
      "Epoch 259/300\n",
      "Average training loss: 0.025336992886331348\n",
      "Average test loss: 0.0023838873549054066\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02534248365296258\n",
      "Average test loss: 0.002348292143808471\n",
      "Epoch 261/300\n",
      "Average training loss: 0.025335411508878074\n",
      "Average test loss: 0.0024002405854149\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025310256457991072\n",
      "Average test loss: 0.0023949132673442364\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025275360975000594\n",
      "Average test loss: 0.002360033551024066\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025305584385991096\n",
      "Average test loss: 0.0024590439117617075\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025308188519544073\n",
      "Average test loss: 0.002467570981217755\n",
      "Epoch 266/300\n",
      "Average training loss: 0.025286822651823363\n",
      "Average test loss: 0.0024255901539905205\n",
      "Epoch 267/300\n",
      "Average training loss: 0.025256233725282882\n",
      "Average test loss: 0.0024138987332375512\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025242960749400987\n",
      "Average test loss: 0.002553159023531609\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025264424464768835\n",
      "Average test loss: 0.0024510288888381585\n",
      "Epoch 270/300\n",
      "Average training loss: 0.025224348819918103\n",
      "Average test loss: 0.0023972660887779463\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025253343699706925\n",
      "Average test loss: 0.0023906933625953064\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0252230424284935\n",
      "Average test loss: 0.002424794839281175\n",
      "Epoch 273/300\n",
      "Average training loss: 0.025256124367316564\n",
      "Average test loss: 0.0025091113137702147\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02523624805443817\n",
      "Average test loss: 0.002414258829835388\n",
      "Epoch 275/300\n",
      "Average training loss: 0.025197972319192356\n",
      "Average test loss: 0.00238857904345625\n",
      "Epoch 276/300\n",
      "Average training loss: 0.025193306146396532\n",
      "Average test loss: 0.002423800555989146\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02521779206726286\n",
      "Average test loss: 0.0023948747248699266\n",
      "Epoch 278/300\n",
      "Average training loss: 0.025191715204053455\n",
      "Average test loss: 0.002384440207439992\n",
      "Epoch 279/300\n",
      "Average training loss: 0.025160200094183287\n",
      "Average test loss: 0.0023895240786174932\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02517555983695719\n",
      "Average test loss: 0.0023651845515188243\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025176111860407722\n",
      "Average test loss: 0.0024113890466590723\n",
      "Epoch 282/300\n",
      "Average training loss: 0.025169194921851157\n",
      "Average test loss: 0.0024264941713255314\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025168166471852195\n",
      "Average test loss: 0.0024275464373123314\n",
      "Epoch 284/300\n",
      "Average training loss: 0.025120563005407652\n",
      "Average test loss: 0.0024171165172010662\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02510953626367781\n",
      "Average test loss: 0.0024073266249357\n",
      "Epoch 286/300\n",
      "Average training loss: 0.025141677333248985\n",
      "Average test loss: 0.0023828668476392825\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02511229928665691\n",
      "Average test loss: 0.0023697659522295\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02512894444498751\n",
      "Average test loss: 0.002355713573594888\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02508693712287479\n",
      "Average test loss: 0.002429184531379077\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02507767516043451\n",
      "Average test loss: 0.002571666061878204\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02507496596707238\n",
      "Average test loss: 0.0023645134874516064\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02509933178789086\n",
      "Average test loss: 0.0024236688661492534\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025085916287369198\n",
      "Average test loss: 0.0030500393360853193\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025101743979586495\n",
      "Average test loss: 0.0024151330408122805\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025055337354540824\n",
      "Average test loss: 0.0024148049659820066\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025032018961177932\n",
      "Average test loss: 0.0024005952671998076\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025054854796992408\n",
      "Average test loss: 0.0024174498202693133\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025109342742297383\n",
      "Average test loss: 0.0024126724749803543\n",
      "Epoch 299/300\n",
      "Average training loss: 0.025043424520227644\n",
      "Average test loss: 0.0023876282781776456\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02506271603703499\n",
      "Average test loss: 0.0028075682365645963\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.75\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.98\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.81\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.02\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.54\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.63\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.49556312052409\n",
      "Average test loss: 0.005807377976882789\n",
      "Epoch 2/300\n",
      "Average training loss: 4.77426125547621\n",
      "Average test loss: 0.005223605317374071\n",
      "Epoch 3/300\n",
      "Average training loss: 3.0016815172831217\n",
      "Average test loss: 0.005404656267414491\n",
      "Epoch 4/300\n",
      "Average training loss: 2.2281932770411172\n",
      "Average test loss: 0.004820709066672458\n",
      "Epoch 5/300\n",
      "Average training loss: 1.886591149330139\n",
      "Average test loss: 0.00470192802987165\n",
      "Epoch 6/300\n",
      "Average training loss: 1.6083479652404784\n",
      "Average test loss: 0.004634187559286754\n",
      "Epoch 7/300\n",
      "Average training loss: 1.3171310142940944\n",
      "Average test loss: 0.00471773281859027\n",
      "Epoch 8/300\n",
      "Average training loss: 1.0213194548288982\n",
      "Average test loss: 0.004569017844481601\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8711755483415392\n",
      "Average test loss: 0.004543588946676917\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7243043685489231\n",
      "Average test loss: 0.004547413654004534\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5927838851875729\n",
      "Average test loss: 0.004497287352672881\n",
      "Epoch 12/300\n",
      "Average training loss: 0.48542589211463927\n",
      "Average test loss: 0.004434911536673705\n",
      "Epoch 13/300\n",
      "Average training loss: 0.41094266883532204\n",
      "Average test loss: 0.004407814195586575\n",
      "Epoch 14/300\n",
      "Average training loss: 0.35409887165493437\n",
      "Average test loss: 0.004416927584757408\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30868766702546013\n",
      "Average test loss: 0.004383814421585864\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2747309061156379\n",
      "Average test loss: 0.004366153679374192\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2469312365319994\n",
      "Average test loss: 0.004368318614446454\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2247825614478853\n",
      "Average test loss: 0.00435250273367597\n",
      "Epoch 19/300\n",
      "Average training loss: 0.20734106548627218\n",
      "Average test loss: 0.004340223101692067\n",
      "Epoch 20/300\n",
      "Average training loss: 0.19249021486441295\n",
      "Average test loss: 0.004324966566016277\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1810894236034817\n",
      "Average test loss: 0.00431376883511742\n",
      "Epoch 22/300\n",
      "Average training loss: 0.17168922470675574\n",
      "Average test loss: 0.0042858139984309675\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16385368786917792\n",
      "Average test loss: 0.004312885496558415\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15745507848262788\n",
      "Average test loss: 0.004285506711237961\n",
      "Epoch 25/300\n",
      "Average training loss: 0.15196777064270445\n",
      "Average test loss: 0.00427362232365542\n",
      "Epoch 26/300\n",
      "Average training loss: 0.14764698794153003\n",
      "Average test loss: 0.00429749075571696\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1438692546553082\n",
      "Average test loss: 0.004267384582509597\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14066918426089817\n",
      "Average test loss: 0.004273734478279948\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13829814080397287\n",
      "Average test loss: 0.004248480070382357\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1363704320854611\n",
      "Average test loss: 0.0042375251168592105\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13468888317214117\n",
      "Average test loss: 0.00423511185310781\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13335636944240994\n",
      "Average test loss: 0.0042257618332902595\n",
      "Epoch 33/300\n",
      "Average training loss: 0.13231388175487518\n",
      "Average test loss: 0.00422621309881409\n",
      "Epoch 34/300\n",
      "Average training loss: 0.13139429554674362\n",
      "Average test loss: 0.004264878878576888\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1305793219804764\n",
      "Average test loss: 0.0041964643932878975\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12996825389729605\n",
      "Average test loss: 0.004203652730832497\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12939021334383222\n",
      "Average test loss: 0.004192449068857564\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12892664838499493\n",
      "Average test loss: 0.004188431848254469\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12854449516534805\n",
      "Average test loss: 0.00418313878795339\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12807298458947075\n",
      "Average test loss: 0.004199904802772734\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12770789777570302\n",
      "Average test loss: 0.00420239012170997\n",
      "Epoch 42/300\n",
      "Average training loss: 0.127487788438797\n",
      "Average test loss: 0.004168556779209111\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12694249064392513\n",
      "Average test loss: 0.004177603684779671\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1267356744143698\n",
      "Average test loss: 0.0050424094502296716\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12644647478395038\n",
      "Average test loss: 0.004196285831431548\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1261444564991527\n",
      "Average test loss: 0.004172946357271738\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1259456856118308\n",
      "Average test loss: 0.00416516694592105\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1256313359671169\n",
      "Average test loss: 0.004165960328860415\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1253567416270574\n",
      "Average test loss: 0.004378040255771743\n",
      "Epoch 50/300\n",
      "Average training loss: 0.12523392921686172\n",
      "Average test loss: 0.004165738347089953\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12487098689874013\n",
      "Average test loss: 0.004162129413750436\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1247296212249332\n",
      "Average test loss: 0.004171995910712415\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12438238326046203\n",
      "Average test loss: 0.004196038290444347\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12415370539161893\n",
      "Average test loss: 0.004144759746889273\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12405110073751874\n",
      "Average test loss: 0.004166384091807736\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12379328119423655\n",
      "Average test loss: 0.00422504884749651\n",
      "Epoch 57/300\n",
      "Average training loss: 0.12354683804512023\n",
      "Average test loss: 0.004148927621336447\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12346541396776835\n",
      "Average test loss: 0.004164009810321861\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1232138522664706\n",
      "Average test loss: 0.0041521803838097384\n",
      "Epoch 60/300\n",
      "Average training loss: 0.123013363301754\n",
      "Average test loss: 0.004151280954480171\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1226861327290535\n",
      "Average test loss: 0.004151931494681372\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12256843256950378\n",
      "Average test loss: 0.004147979225549433\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1223984713488155\n",
      "Average test loss: 0.00415693293677436\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12219338256120682\n",
      "Average test loss: 0.0041662561427801845\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12209841275215148\n",
      "Average test loss: 0.004172288092681103\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12191339256366093\n",
      "Average test loss: 0.004156068772491482\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12162911960813734\n",
      "Average test loss: 0.0041570810560757916\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12155525912841161\n",
      "Average test loss: 0.004181204365566373\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12124400395154954\n",
      "Average test loss: 0.004157681221763293\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12108055973384116\n",
      "Average test loss: 0.0041865500896755194\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12094470845990711\n",
      "Average test loss: 0.004171727895943655\n",
      "Epoch 72/300\n",
      "Average training loss: 0.12078700897428725\n",
      "Average test loss: 0.0041708984341886305\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12062134304973814\n",
      "Average test loss: 0.004166795643253459\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12029172011216481\n",
      "Average test loss: 0.00416456823858122\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12029752996895049\n",
      "Average test loss: 0.004155872053156297\n",
      "Epoch 76/300\n",
      "Average training loss: 0.12003717768854565\n",
      "Average test loss: 0.004192468309154113\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11982576112614737\n",
      "Average test loss: 0.004191417504515913\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11973356626431148\n",
      "Average test loss: 0.004170525279310015\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11941226830747392\n",
      "Average test loss: 0.0041871042860050995\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11920588428444333\n",
      "Average test loss: 0.004166591646356715\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11902930906083849\n",
      "Average test loss: 0.004204221403019296\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11884336272213195\n",
      "Average test loss: 0.004202155784393351\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11875401524702708\n",
      "Average test loss: 0.004215726922369665\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1183975289662679\n",
      "Average test loss: 0.004330029415173663\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11816714678870308\n",
      "Average test loss: 0.004278860179914369\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11814804540077846\n",
      "Average test loss: 0.004200873645643393\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11786566339598761\n",
      "Average test loss: 0.004176986174037059\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11760594599114524\n",
      "Average test loss: 0.004241976293838687\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11736895933416154\n",
      "Average test loss: 0.004177828040387895\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11717012867662642\n",
      "Average test loss: 0.004221215856158071\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11700129981835683\n",
      "Average test loss: 0.004238209137486087\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1167913071513176\n",
      "Average test loss: 0.004199520874975456\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11651426859696706\n",
      "Average test loss: 0.00423146040054659\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11637164258294636\n",
      "Average test loss: 0.00419140045841535\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11613827470938365\n",
      "Average test loss: 0.004308297814594375\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1159472927848498\n",
      "Average test loss: 0.004231528657592005\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11573487864600288\n",
      "Average test loss: 0.004192790538072586\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11538512014018165\n",
      "Average test loss: 0.00426363607061406\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11537603543864357\n",
      "Average test loss: 0.0043557732926888595\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11513754628764258\n",
      "Average test loss: 0.004215836157608364\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11472771361139085\n",
      "Average test loss: 0.004200720745863186\n",
      "Epoch 102/300\n",
      "Average training loss: 0.11460623892810609\n",
      "Average test loss: 0.004262389092395703\n",
      "Epoch 103/300\n",
      "Average training loss: 0.1143239362637202\n",
      "Average test loss: 0.004281071649036474\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11425118623839485\n",
      "Average test loss: 0.004422580389926831\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11402070716354581\n",
      "Average test loss: 0.004258763747082816\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11375607956118054\n",
      "Average test loss: 0.004310232857035266\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11353949532906214\n",
      "Average test loss: 0.00430053097796109\n",
      "Epoch 108/300\n",
      "Average training loss: 0.1132224319908354\n",
      "Average test loss: 0.004318778585642577\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11329394305414624\n",
      "Average test loss: 0.004305787491715616\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11287677445014317\n",
      "Average test loss: 0.004293625961989164\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11267346343729231\n",
      "Average test loss: 0.004324311442259286\n",
      "Epoch 112/300\n",
      "Average training loss: 0.11263893287711674\n",
      "Average test loss: 0.004302409432828426\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11217809538708792\n",
      "Average test loss: 0.00435178228509095\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11210864402850469\n",
      "Average test loss: 0.005135478248612748\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11192525734504064\n",
      "Average test loss: 0.004307037428228392\n",
      "Epoch 116/300\n",
      "Average training loss: 0.111829354054398\n",
      "Average test loss: 0.004283169875335362\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11152028083138996\n",
      "Average test loss: 0.004363129153226813\n",
      "Epoch 118/300\n",
      "Average training loss: 0.11133368406693141\n",
      "Average test loss: 0.004447464298042986\n",
      "Epoch 119/300\n",
      "Average training loss: 0.11117055997583601\n",
      "Average test loss: 0.004298776793397135\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11130240605274837\n",
      "Average test loss: 0.004391887501709991\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11095749341779285\n",
      "Average test loss: 0.004289838411741786\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11043400669760174\n",
      "Average test loss: 0.004580581315689617\n",
      "Epoch 123/300\n",
      "Average training loss: 0.1103523958325386\n",
      "Average test loss: 0.004273836922728353\n",
      "Epoch 124/300\n",
      "Average training loss: 0.1102029369937049\n",
      "Average test loss: 0.0044041127736369765\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11004552102088928\n",
      "Average test loss: 0.004340932957414124\n",
      "Epoch 126/300\n",
      "Average training loss: 0.11016454257567723\n",
      "Average test loss: 0.004377235152655178\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10991375357574887\n",
      "Average test loss: 0.004395584891032841\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10944853691922293\n",
      "Average test loss: 0.0044182920671171615\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10925058813889821\n",
      "Average test loss: 0.004462172042371498\n",
      "Epoch 130/300\n",
      "Average training loss: 0.1091806642015775\n",
      "Average test loss: 0.0043508187387552525\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10905242733822929\n",
      "Average test loss: 0.004312295808146397\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10901878597338995\n",
      "Average test loss: 0.004351993930836518\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10866295615169737\n",
      "Average test loss: 0.004364754642463393\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10855658787488938\n",
      "Average test loss: 0.0043436645004484385\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10863845219877032\n",
      "Average test loss: 0.00448444494108359\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10828328323364257\n",
      "Average test loss: 0.004384739606744713\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10823085214032067\n",
      "Average test loss: 0.0043920058686700136\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10810803547170428\n",
      "Average test loss: 0.0043427180349826814\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10775781357288361\n",
      "Average test loss: 0.004388113968902164\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1076865210003323\n",
      "Average test loss: 0.004503314379602671\n",
      "Epoch 141/300\n",
      "Average training loss: 0.10747855373223622\n",
      "Average test loss: 0.004432454751183589\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10744839955038495\n",
      "Average test loss: 0.004401324823498726\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10733165650235282\n",
      "Average test loss: 0.004356064746363296\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10720847696728177\n",
      "Average test loss: 0.006276425053055088\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10696770220332676\n",
      "Average test loss: 0.004365647168623077\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10691937880383598\n",
      "Average test loss: 0.0044231356411344475\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10664926791853375\n",
      "Average test loss: 0.004385953538119793\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10684622886445787\n",
      "Average test loss: 0.004374425845013724\n",
      "Epoch 149/300\n",
      "Average training loss: 0.1064453845222791\n",
      "Average test loss: 0.004371853240041269\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10633459640211529\n",
      "Average test loss: 0.004352017867482371\n",
      "Epoch 151/300\n",
      "Average training loss: 0.106305245882935\n",
      "Average test loss: 0.004473385951171319\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10602348398500018\n",
      "Average test loss: 0.004431923771690991\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10598116139570872\n",
      "Average test loss: 0.0045021266523334715\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10577291561497583\n",
      "Average test loss: 0.0045406895693805484\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10564575667513741\n",
      "Average test loss: 0.004587932296097279\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10557499472962485\n",
      "Average test loss: 0.004374520853161812\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10535452298985587\n",
      "Average test loss: 0.004465996656152937\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10535047351651722\n",
      "Average test loss: 0.004614359679735369\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10523555878798167\n",
      "Average test loss: 0.004480326547804806\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10535879126522277\n",
      "Average test loss: 0.0044352064018862115\n",
      "Epoch 161/300\n",
      "Average training loss: 0.1050601354572508\n",
      "Average test loss: 0.004538563781728347\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10476887808243433\n",
      "Average test loss: 0.004575922672119406\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10482481902837754\n",
      "Average test loss: 0.004475096926920944\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10469452797704273\n",
      "Average test loss: 0.004476824709938632\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10453363161616855\n",
      "Average test loss: 0.0045327088979797234\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10439269790384505\n",
      "Average test loss: 0.004569409683346748\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10434961755408181\n",
      "Average test loss: 0.004427692424298989\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10434763358698951\n",
      "Average test loss: 0.004525058404025104\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10393373081419204\n",
      "Average test loss: 0.004442665492908822\n",
      "Epoch 170/300\n",
      "Average training loss: 0.10403564630614387\n",
      "Average test loss: 0.004440788015930189\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10394559354252285\n",
      "Average test loss: 0.004438679966247744\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10375824159383774\n",
      "Average test loss: 0.004471888782249557\n",
      "Epoch 173/300\n",
      "Average training loss: 0.1036887778043747\n",
      "Average test loss: 0.004480802163895634\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10350231593847274\n",
      "Average test loss: 0.0044650637143188055\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10346667783790164\n",
      "Average test loss: 0.004504799626146754\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10356877112388611\n",
      "Average test loss: 0.004471105328864521\n",
      "Epoch 177/300\n",
      "Average training loss: 0.1032709717684322\n",
      "Average test loss: 0.0044882603304253685\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10332048034005695\n",
      "Average test loss: 0.00450929657700989\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10300363333357705\n",
      "Average test loss: 0.004394585805220737\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10313683211803436\n",
      "Average test loss: 0.0045337864499953055\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10296767889791064\n",
      "Average test loss: 0.004425218102418714\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10274044275946087\n",
      "Average test loss: 0.004463548511059748\n",
      "Epoch 183/300\n",
      "Average training loss: 0.1026655314233568\n",
      "Average test loss: 0.004572775265408887\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10275483687056436\n",
      "Average test loss: 0.004517881981614564\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10254882920450635\n",
      "Average test loss: 0.0045944819771167305\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10239390741454231\n",
      "Average test loss: 0.004468742464979489\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10231651803519991\n",
      "Average test loss: 0.004482273187902239\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10239039459493425\n",
      "Average test loss: 0.0045242933233579\n",
      "Epoch 189/300\n",
      "Average training loss: 0.1023683470553822\n",
      "Average test loss: 0.004665480829775333\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10210077942742242\n",
      "Average test loss: 0.004477128528058529\n",
      "Epoch 191/300\n",
      "Average training loss: 0.1019460455775261\n",
      "Average test loss: 0.004412768420866794\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10185228199428982\n",
      "Average test loss: 0.004509865988459852\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1018297602666749\n",
      "Average test loss: 0.004498143287996451\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10182341933250427\n",
      "Average test loss: 0.004479742588475347\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1017861115667555\n",
      "Average test loss: 0.004518376117158268\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10158036084969839\n",
      "Average test loss: 0.004552690454655223\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10170792267057631\n",
      "Average test loss: 0.004516661710623238\n",
      "Epoch 198/300\n",
      "Average training loss: 0.10154014550977283\n",
      "Average test loss: 0.004428836499651273\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10139929857518938\n",
      "Average test loss: 0.004498644630528159\n",
      "Epoch 200/300\n",
      "Average training loss: 0.10124711736043294\n",
      "Average test loss: 0.004517352225879828\n",
      "Epoch 201/300\n",
      "Average training loss: 0.10123162752389908\n",
      "Average test loss: 0.0045008132536378176\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10124047850900227\n",
      "Average test loss: 0.004569605780765414\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10113511418633991\n",
      "Average test loss: 0.004524056336118115\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10094925893677606\n",
      "Average test loss: 0.004528704185452726\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10080017478598488\n",
      "Average test loss: 0.004677569973799918\n",
      "Epoch 206/300\n",
      "Average training loss: 0.10093404784467486\n",
      "Average test loss: 0.004653454720146126\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10074722597996394\n",
      "Average test loss: 0.004708192656023635\n",
      "Epoch 208/300\n",
      "Average training loss: 0.10070553125275505\n",
      "Average test loss: 0.0044738408107724455\n",
      "Epoch 209/300\n",
      "Average training loss: 0.10065032739771737\n",
      "Average test loss: 0.004422306160959933\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10050184349219005\n",
      "Average test loss: 0.004521793445365296\n",
      "Epoch 211/300\n",
      "Average training loss: 0.1005316758553187\n",
      "Average test loss: 0.004459503696610531\n",
      "Epoch 212/300\n",
      "Average training loss: 0.1002776793440183\n",
      "Average test loss: 0.004665371082723141\n",
      "Epoch 213/300\n",
      "Average training loss: 0.10037392089764277\n",
      "Average test loss: 0.004708488303340144\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10030727403031456\n",
      "Average test loss: 0.004696982463821769\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10018248326910867\n",
      "Average test loss: 0.004631183983965052\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10001242869430119\n",
      "Average test loss: 0.004529019416206413\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09998399138450623\n",
      "Average test loss: 0.004651450737482972\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10010719651646084\n",
      "Average test loss: 0.0046209460623148416\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10002060939868292\n",
      "Average test loss: 0.004388587281935745\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09981416808234321\n",
      "Average test loss: 0.0044907689901689685\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09967677394549052\n",
      "Average test loss: 0.00458409506889681\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09971392019589742\n",
      "Average test loss: 0.004519564830180672\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09954950036274063\n",
      "Average test loss: 0.0046212168923682635\n",
      "Epoch 224/300\n",
      "Average training loss: 0.099739311489794\n",
      "Average test loss: 0.0045627633912695776\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09944902719391717\n",
      "Average test loss: 0.00452479661628604\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09935476623641121\n",
      "Average test loss: 0.004485493515100744\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09936377624670664\n",
      "Average test loss: 0.004552947591990233\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09943027257919311\n",
      "Average test loss: 0.00458654884621501\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09935913217067718\n",
      "Average test loss: 0.004531330302150713\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09908472777075238\n",
      "Average test loss: 0.00455963281624847\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09916619838608635\n",
      "Average test loss: 0.004541794167624579\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09892831486463546\n",
      "Average test loss: 0.004492495608826478\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09889219599962235\n",
      "Average test loss: 0.004599724839544958\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09901280355453491\n",
      "Average test loss: 0.004499633654745088\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09885192721419864\n",
      "Average test loss: 0.0046138677810215285\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09886404526895946\n",
      "Average test loss: 0.004548808289277884\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09857087907526228\n",
      "Average test loss: 0.004491122880329689\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09873544367816713\n",
      "Average test loss: 0.004580392607798179\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09869068155023787\n",
      "Average test loss: 0.004575546534111102\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09859284983740912\n",
      "Average test loss: 0.004455531581408448\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09836030846834183\n",
      "Average test loss: 0.004577972251507971\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09840347753630745\n",
      "Average test loss: 0.00463806386747294\n",
      "Epoch 243/300\n",
      "Average training loss: 0.098432176484002\n",
      "Average test loss: 0.00449996522669163\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09838275257746379\n",
      "Average test loss: 0.004486086318476332\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09824845694171058\n",
      "Average test loss: 0.004603277794188923\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09817961742480596\n",
      "Average test loss: 0.004601339221828513\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09837614750199848\n",
      "Average test loss: 0.004492534015327692\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0980972259177102\n",
      "Average test loss: 0.004791954689141777\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09794093024068408\n",
      "Average test loss: 0.004577882900420162\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0978567778997951\n",
      "Average test loss: 0.004773709853283233\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09795142600933711\n",
      "Average test loss: 0.004563312545004818\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09788957452774048\n",
      "Average test loss: 0.0044927823936773675\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09782416539722019\n",
      "Average test loss: 0.0045628133138848675\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09771229660511017\n",
      "Average test loss: 0.004598268156664239\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0975533356865247\n",
      "Average test loss: 0.0046090779908829265\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09762789664665858\n",
      "Average test loss: 0.004525329440211256\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09766955455144247\n",
      "Average test loss: 0.004449132160387105\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0976033888525433\n",
      "Average test loss: 0.004605600815680292\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09749433740642335\n",
      "Average test loss: 0.004489058517333534\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09750704038805431\n",
      "Average test loss: 0.0045976384439402156\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09730299541685317\n",
      "Average test loss: 0.00449891727252139\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09731963109970093\n",
      "Average test loss: 0.004556470642901129\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09736547210481432\n",
      "Average test loss: 0.004496549463520448\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09716452370087306\n",
      "Average test loss: 0.004498773184087541\n",
      "Epoch 265/300\n",
      "Average training loss: 0.10010371540983518\n",
      "Average test loss: 0.004560785001350774\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09782822898361418\n",
      "Average test loss: 0.004558122419027819\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09674384006526734\n",
      "Average test loss: 0.004581887572589848\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09670269271400239\n",
      "Average test loss: 0.0044979950129571886\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09659298944473267\n",
      "Average test loss: 0.004722486565303471\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09685637895266215\n",
      "Average test loss: 0.00455115678658088\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0968897923429807\n",
      "Average test loss: 0.0044696877242790325\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09685241558816698\n",
      "Average test loss: 0.004502633458210363\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09685230857796139\n",
      "Average test loss: 0.004564892989479833\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09667900533808602\n",
      "Average test loss: 0.0046512215986020036\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09668693857722813\n",
      "Average test loss: 0.004669013403356076\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09665369990799162\n",
      "Average test loss: 0.004571359963466724\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09669082270065943\n",
      "Average test loss: 0.004565887682967716\n",
      "Epoch 278/300\n",
      "Average training loss: 0.096578047255675\n",
      "Average test loss: 0.004524635619587368\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09651844269699521\n",
      "Average test loss: 0.0046306131722198595\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09655599018600251\n",
      "Average test loss: 0.004711257008214792\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09638667218221558\n",
      "Average test loss: 0.004559195573959085\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09639430496427748\n",
      "Average test loss: 0.00467739936336875\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09622960194614198\n",
      "Average test loss: 0.0046724736787792705\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09625306826498774\n",
      "Average test loss: 0.0045359219449261825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09621888256735271\n",
      "Average test loss: 0.004555888094835811\n",
      "Epoch 286/300\n",
      "Average training loss: 0.09626397469970915\n",
      "Average test loss: 0.0049252216017080675\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09630497483081288\n",
      "Average test loss: 0.004577413445959489\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09604078331920836\n",
      "Average test loss: 0.004463842281450828\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09600405896372265\n",
      "Average test loss: 0.004752858036094242\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09608604325188531\n",
      "Average test loss: 0.0047005630893011885\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09590505364206102\n",
      "Average test loss: 0.004731515883571572\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0958612191942003\n",
      "Average test loss: 0.004551000866625044\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09583945031960805\n",
      "Average test loss: 0.004576219365828567\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0958717364470164\n",
      "Average test loss: 0.004568143801556693\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09579856908983654\n",
      "Average test loss: 0.004725878243645032\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09571543424659305\n",
      "Average test loss: 0.004527731165289879\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09572887150446574\n",
      "Average test loss: 0.004590539206233289\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09562902074389988\n",
      "Average test loss: 0.004575781421528922\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09553206259012223\n",
      "Average test loss: 0.0046225738297734\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09565317675802443\n",
      "Average test loss: 0.004608419105203616\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.346515980614557\n",
      "Average test loss: 0.04964916508479251\n",
      "Epoch 2/300\n",
      "Average training loss: 4.442424740685357\n",
      "Average test loss: 0.004951908439811733\n",
      "Epoch 3/300\n",
      "Average training loss: 3.278699310302734\n",
      "Average test loss: 0.004541420420838727\n",
      "Epoch 4/300\n",
      "Average training loss: 2.656981201171875\n",
      "Average test loss: 0.004443871412840155\n",
      "Epoch 5/300\n",
      "Average training loss: 2.1124679742389256\n",
      "Average test loss: 0.0042979189426534704\n",
      "Epoch 6/300\n",
      "Average training loss: 1.8180068017111883\n",
      "Average test loss: 0.004244136954450773\n",
      "Epoch 7/300\n",
      "Average training loss: 1.6155388854344686\n",
      "Average test loss: 0.004174680422577593\n",
      "Epoch 8/300\n",
      "Average training loss: 1.3747970555623372\n",
      "Average test loss: 0.004079281665177809\n",
      "Epoch 9/300\n",
      "Average training loss: 1.219135688357883\n",
      "Average test loss: 0.004009616000991729\n",
      "Epoch 10/300\n",
      "Average training loss: 1.0704707724253337\n",
      "Average test loss: 0.0039667178351018165\n",
      "Epoch 11/300\n",
      "Average training loss: 0.947847033129798\n",
      "Average test loss: 0.004001280096876952\n",
      "Epoch 12/300\n",
      "Average training loss: 0.8316071990860833\n",
      "Average test loss: 0.0038628210301200547\n",
      "Epoch 13/300\n",
      "Average training loss: 0.7292220923635695\n",
      "Average test loss: 0.0038125163610610697\n",
      "Epoch 14/300\n",
      "Average training loss: 0.6370228478113811\n",
      "Average test loss: 0.0038010009535484842\n",
      "Epoch 15/300\n",
      "Average training loss: 0.5538817979229821\n",
      "Average test loss: 0.0037294512070301508\n",
      "Epoch 16/300\n",
      "Average training loss: 0.47819565523995294\n",
      "Average test loss: 0.0037754364390340115\n",
      "Epoch 17/300\n",
      "Average training loss: 0.41115831486384075\n",
      "Average test loss: 0.0037072276311616104\n",
      "Epoch 18/300\n",
      "Average training loss: 0.35427647399902346\n",
      "Average test loss: 0.0036507243329866063\n",
      "Epoch 19/300\n",
      "Average training loss: 0.3072282674047682\n",
      "Average test loss: 0.0037107353707154594\n",
      "Epoch 20/300\n",
      "Average training loss: 0.26874949577119617\n",
      "Average test loss: 0.0036462700280050437\n",
      "Epoch 21/300\n",
      "Average training loss: 0.23799940964910718\n",
      "Average test loss: 0.003579032331912054\n",
      "Epoch 22/300\n",
      "Average training loss: 0.2133294070959091\n",
      "Average test loss: 0.0035903922287333343\n",
      "Epoch 23/300\n",
      "Average training loss: 0.19332854078875647\n",
      "Average test loss: 0.003574176208426555\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1775048967997233\n",
      "Average test loss: 0.003537244889471266\n",
      "Epoch 25/300\n",
      "Average training loss: 0.16461510560247633\n",
      "Average test loss: 0.003515995755791664\n",
      "Epoch 26/300\n",
      "Average training loss: 0.15382585895061493\n",
      "Average test loss: 0.003513412720420294\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1450316175752216\n",
      "Average test loss: 0.0035055539740456473\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13803841767046188\n",
      "Average test loss: 0.0035099661571698056\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13262593156761593\n",
      "Average test loss: 0.003448709172093206\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12819396679931216\n",
      "Average test loss: 0.0034621574584808613\n",
      "Epoch 31/300\n",
      "Average training loss: 0.124532193991873\n",
      "Average test loss: 0.0034992230327592954\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12155307369761997\n",
      "Average test loss: 0.003452805200177762\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11915037684308158\n",
      "Average test loss: 0.0034473896517107886\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11714564090304905\n",
      "Average test loss: 0.003431531747803092\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11559628988636865\n",
      "Average test loss: 0.0034247856881055565\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11417859407928255\n",
      "Average test loss: 0.0034006756999426417\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11305233303043577\n",
      "Average test loss: 0.0034172179251909258\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11194062776035733\n",
      "Average test loss: 0.003386009683418605\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11101942739221785\n",
      "Average test loss: 0.0033929469341205227\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11021473656098048\n",
      "Average test loss: 0.0034183206405076715\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10927775356504653\n",
      "Average test loss: 0.003410702138725254\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10861579555273056\n",
      "Average test loss: 0.0034478141110804348\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1079207149942716\n",
      "Average test loss: 0.0035291343859086433\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10731840719117058\n",
      "Average test loss: 0.0033983535456160703\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10673270708322526\n",
      "Average test loss: 0.003365838187850184\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10602398192220264\n",
      "Average test loss: 0.0033761885518001186\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10548611924383375\n",
      "Average test loss: 0.003392358975071046\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10511188717683156\n",
      "Average test loss: 0.003367845428072744\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10451217363278072\n",
      "Average test loss: 0.003356035338300798\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10391359446114964\n",
      "Average test loss: 0.003382052520910899\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10354170371426476\n",
      "Average test loss: 0.003357881392041842\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1031635546485583\n",
      "Average test loss: 0.0033777832328859302\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10253618872827953\n",
      "Average test loss: 0.0033588632519046466\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10216851741075515\n",
      "Average test loss: 0.003382786331491338\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10181501427623961\n",
      "Average test loss: 0.00336335819421543\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10132406105597815\n",
      "Average test loss: 0.0033711137591550745\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10094538141621484\n",
      "Average test loss: 0.003346364524629381\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10058008293310801\n",
      "Average test loss: 0.003347887008968327\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10026414301660326\n",
      "Average test loss: 0.0034063179581943484\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09974200120899412\n",
      "Average test loss: 0.00335512347974711\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09951992375983132\n",
      "Average test loss: 0.0033493491605752043\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09898291895786922\n",
      "Average test loss: 0.003401759060927563\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09863641213708453\n",
      "Average test loss: 0.003338532093084521\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09817898309893078\n",
      "Average test loss: 0.0033583404612210063\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09792694187164307\n",
      "Average test loss: 0.0033867433598885935\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09761891502141952\n",
      "Average test loss: 0.0035317098788089224\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09725836549202602\n",
      "Average test loss: 0.003386353289294574\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09695651215314866\n",
      "Average test loss: 0.0034022920272416537\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09643070971965789\n",
      "Average test loss: 0.003356121459147996\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09617021160655552\n",
      "Average test loss: 0.0033524978650319908\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0959381902217865\n",
      "Average test loss: 0.0033662560130986903\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0954088992079099\n",
      "Average test loss: 0.0033570799879315827\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09509915235969756\n",
      "Average test loss: 0.003390680205490854\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0948211770190133\n",
      "Average test loss: 0.003356660200903813\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0943948226041264\n",
      "Average test loss: 0.0034021303053531383\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09404582721657223\n",
      "Average test loss: 0.003406472354920374\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09367162496513791\n",
      "Average test loss: 0.0034299928154796363\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0934033502538999\n",
      "Average test loss: 0.0034849064201116563\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09308291623327467\n",
      "Average test loss: 0.0035200940538197755\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09278511295053694\n",
      "Average test loss: 0.00345184033529626\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09244984565840828\n",
      "Average test loss: 0.003449175421976381\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09208786137898763\n",
      "Average test loss: 0.0033637133112384213\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09194620276490847\n",
      "Average test loss: 0.003544044100162056\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0916022802260187\n",
      "Average test loss: 0.003415949882215096\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09115690080324809\n",
      "Average test loss: 0.0034247063924041058\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09095988623301188\n",
      "Average test loss: 0.0035151966953029237\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09046196326944562\n",
      "Average test loss: 0.0034450157922175195\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09026326112614738\n",
      "Average test loss: 0.003428165571971072\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09001295417878363\n",
      "Average test loss: 0.003475590043597751\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08970178580946392\n",
      "Average test loss: 0.0034418629674861827\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08929042183028327\n",
      "Average test loss: 0.003467351894411776\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08923923519584867\n",
      "Average test loss: 0.0035695590749382972\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0888908434841368\n",
      "Average test loss: 0.0034198294197105698\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08857434033022986\n",
      "Average test loss: 0.0034127141336599987\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08845284403695\n",
      "Average test loss: 0.003534531636784474\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08808414006233216\n",
      "Average test loss: 0.0037156679729620614\n",
      "Epoch 97/300\n",
      "Average training loss: 0.087721655468146\n",
      "Average test loss: 0.0034699861746695306\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08758495337433285\n",
      "Average test loss: 0.003480499820369813\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08731593710184098\n",
      "Average test loss: 0.003556990993519624\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0871762634449535\n",
      "Average test loss: 0.0034852795331842368\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08679511555698183\n",
      "Average test loss: 0.003430722309483422\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08671332764956687\n",
      "Average test loss: 0.0035904373274081285\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08644478328360451\n",
      "Average test loss: 0.0034684774432745245\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08625514552990596\n",
      "Average test loss: 0.003618627349121703\n",
      "Epoch 105/300\n",
      "Average training loss: 0.085928735713164\n",
      "Average test loss: 0.0035275782922075853\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08566262053118812\n",
      "Average test loss: 0.0034807553982569113\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08556440677245458\n",
      "Average test loss: 0.003504589920035667\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08535799631476403\n",
      "Average test loss: 0.003534219036499659\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0852894610762596\n",
      "Average test loss: 0.0034698159547729623\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08504260902934604\n",
      "Average test loss: 0.003571456575145324\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08485661937793096\n",
      "Average test loss: 0.0035556181418812936\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08461655207475026\n",
      "Average test loss: 0.0035812396986616983\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08453579984770881\n",
      "Average test loss: 0.0035484827268454765\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08427141902181837\n",
      "Average test loss: 0.0035267832097080017\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0840989842414856\n",
      "Average test loss: 0.003552339019874732\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08381431673632728\n",
      "Average test loss: 0.003511789228146275\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08375513629118601\n",
      "Average test loss: 0.003521416952005691\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08347438884443707\n",
      "Average test loss: 0.003725570157584217\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08339180667863952\n",
      "Average test loss: 0.00361789292573101\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08321094367901484\n",
      "Average test loss: 0.0035135179110285307\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08294121926691797\n",
      "Average test loss: 0.003505605385328333\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08303323013914957\n",
      "Average test loss: 0.0035537354563259416\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08273617437813018\n",
      "Average test loss: 0.003498443758322133\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08255216995212766\n",
      "Average test loss: 0.003497868074518111\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08242573563920128\n",
      "Average test loss: 0.0035298466806610424\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08226614989837011\n",
      "Average test loss: 0.003497373513670431\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08212403233846029\n",
      "Average test loss: 0.003545999525528815\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08213034374184079\n",
      "Average test loss: 0.003842048489799102\n",
      "Epoch 129/300\n",
      "Average training loss: 0.08181439065933227\n",
      "Average test loss: 0.00351982963250743\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08169261225064596\n",
      "Average test loss: 0.003667436546335618\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08167744433217579\n",
      "Average test loss: 0.0035718619380560187\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08150137041012447\n",
      "Average test loss: 0.0035748075352360804\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08129226522975498\n",
      "Average test loss: 0.0035085408174329335\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08119404206673304\n",
      "Average test loss: 0.003638764494409164\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0811072480612331\n",
      "Average test loss: 0.0035398651079999077\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08103964481751125\n",
      "Average test loss: 0.003624720220350557\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08087321878141827\n",
      "Average test loss: 0.003551163256375326\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08084713392125235\n",
      "Average test loss: 0.0035525631378922197\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08072127929661009\n",
      "Average test loss: 0.00356259305568205\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08041185851560699\n",
      "Average test loss: 0.0035962824966344568\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0802694841557079\n",
      "Average test loss: 0.003619755334324307\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08024090583456887\n",
      "Average test loss: 0.0036669316585693093\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08006472491224607\n",
      "Average test loss: 0.0036380179725173448\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0799455487065845\n",
      "Average test loss: 0.00352700609424048\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07988899902833832\n",
      "Average test loss: 0.0035950821244882213\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07975742883152431\n",
      "Average test loss: 0.0036144811660051347\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07968488755491045\n",
      "Average test loss: 0.003622767439732949\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07944823424021404\n",
      "Average test loss: 0.0036991739500727918\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07948367755280601\n",
      "Average test loss: 0.004051589350319571\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07938213646080759\n",
      "Average test loss: 0.003633841504653295\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0792017003496488\n",
      "Average test loss: 0.0036068532433774737\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0793616625269254\n",
      "Average test loss: 0.0037227041895190873\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07923498137791951\n",
      "Average test loss: 0.003671241662982437\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07909305905302366\n",
      "Average test loss: 0.003578126695421007\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07893228226900101\n",
      "Average test loss: 0.0036120656637681856\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07880805873208575\n",
      "Average test loss: 0.0036676906392806106\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0786111660334799\n",
      "Average test loss: 0.0038934101230568355\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0785453512834178\n",
      "Average test loss: 0.0037242949772626163\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07840946171681086\n",
      "Average test loss: 0.003577874001943403\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07851550236013201\n",
      "Average test loss: 0.0036697750503404273\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07829949048492643\n",
      "Average test loss: 0.003692216541411148\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07819318320353826\n",
      "Average test loss: 0.0035297238654974435\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07807131699721019\n",
      "Average test loss: 0.003611439008058773\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07807757631937663\n",
      "Average test loss: 0.003754357800508539\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07795614701509476\n",
      "Average test loss: 0.003595182230696082\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07775877574417327\n",
      "Average test loss: 0.0036267325677391557\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07776131115357081\n",
      "Average test loss: 0.003662054559422864\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07768341255187988\n",
      "Average test loss: 0.0036098425363500913\n",
      "Epoch 169/300\n",
      "Average training loss: 0.077687618666225\n",
      "Average test loss: 0.0036401316376609935\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07759427222278382\n",
      "Average test loss: 0.0036799738705158235\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07744910791185167\n",
      "Average test loss: 0.0036877957632144294\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07726601971520318\n",
      "Average test loss: 0.003620587142391337\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07738507568173938\n",
      "Average test loss: 0.003638991356309917\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07714898937278324\n",
      "Average test loss: 0.0037271156513856516\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07697940347592036\n",
      "Average test loss: 0.0038049770374264983\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07711362475156784\n",
      "Average test loss: 0.0035180103985799683\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07691720510522525\n",
      "Average test loss: 0.003652574818374382\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07688029150830375\n",
      "Average test loss: 0.003650541386877497\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07677888567911254\n",
      "Average test loss: 0.0037125043008062577\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07684757146239281\n",
      "Average test loss: 0.0036208250837193594\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07681309070852067\n",
      "Average test loss: 0.0036941605001274084\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0767253425485558\n",
      "Average test loss: 0.0036950452141463756\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07650817155175739\n",
      "Average test loss: 0.003654122379504972\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07628305461009344\n",
      "Average test loss: 0.003703920313053661\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07638544476032257\n",
      "Average test loss: 0.0036336871315207748\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07631451192829344\n",
      "Average test loss: 0.003797342685361703\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07623556853334108\n",
      "Average test loss: 0.003717310120869014\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07604464188549254\n",
      "Average test loss: 0.0036433905890832343\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07616319648424784\n",
      "Average test loss: 0.0035694094565179614\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0759965107176039\n",
      "Average test loss: 0.003791970742866397\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07595450789398617\n",
      "Average test loss: 0.003612703510456615\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07610110572973887\n",
      "Average test loss: 0.003679650881224208\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07582242321968079\n",
      "Average test loss: 0.0035807842461185323\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07573280426859856\n",
      "Average test loss: 0.0037002335898578165\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07563708457681868\n",
      "Average test loss: 0.0036959695716698967\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07550646385219362\n",
      "Average test loss: 0.0036462683441738286\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07555745244026184\n",
      "Average test loss: 0.0037003002791768974\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07532797530624602\n",
      "Average test loss: 0.003721825683075521\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07540510249800152\n",
      "Average test loss: 0.0036427897030694616\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0753805878692203\n",
      "Average test loss: 0.003637156101357606\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07536078807380464\n",
      "Average test loss: 0.004096135067856974\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07545875454611248\n",
      "Average test loss: 0.0037520378983269135\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07522096025943756\n",
      "Average test loss: 0.0036896400820049975\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07507541999551985\n",
      "Average test loss: 0.0038309489720397523\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07501998804012934\n",
      "Average test loss: 0.0036608342925707497\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0749093048075835\n",
      "Average test loss: 0.0037216317844059733\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07500639041927126\n",
      "Average test loss: 0.003657629036034147\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07496535070737204\n",
      "Average test loss: 0.003754560927963919\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07475921142763561\n",
      "Average test loss: 0.0036471864734258915\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07476954431003995\n",
      "Average test loss: 0.003707710583590799\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07473570345507728\n",
      "Average test loss: 0.0037747918710940415\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0746237031949891\n",
      "Average test loss: 0.003696184110103382\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07463299429416656\n",
      "Average test loss: 0.0036602891482826736\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07450189095404414\n",
      "Average test loss: 0.0037234738589160972\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07461349402533637\n",
      "Average test loss: 0.0037028541478017966\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07452877287069956\n",
      "Average test loss: 0.003688097563675708\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07449325814512041\n",
      "Average test loss: 0.003619942298365964\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07448034131858083\n",
      "Average test loss: 0.003666722678889831\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07439739164378908\n",
      "Average test loss: 0.0037458808877401883\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07423381071289381\n",
      "Average test loss: 0.0037558667725986904\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07410475951433182\n",
      "Average test loss: 0.0039973970753037266\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07405854466226366\n",
      "Average test loss: 0.0038025301761097376\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07409447507063548\n",
      "Average test loss: 0.003828999548529585\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0739914218518469\n",
      "Average test loss: 0.003746014127921727\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07399058434698316\n",
      "Average test loss: 0.0038467802461236715\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07397365152173573\n",
      "Average test loss: 0.0036852217014465066\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0738381370835834\n",
      "Average test loss: 0.0038470812553746832\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07381024094422658\n",
      "Average test loss: 0.0038550390390058357\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0737925628821055\n",
      "Average test loss: 0.0036804205075734193\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07387328137954076\n",
      "Average test loss: 0.003903826187054316\n",
      "Epoch 231/300\n",
      "Average training loss: 0.073661621371905\n",
      "Average test loss: 0.0037866294853803185\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07361548401249779\n",
      "Average test loss: 0.003959752132495244\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07368519133329392\n",
      "Average test loss: 0.0037126126107242374\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07338872800270717\n",
      "Average test loss: 0.0037124734326369232\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07343783659405179\n",
      "Average test loss: 0.003719857206154201\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0733758367366261\n",
      "Average test loss: 0.0037385357427928184\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07349128825796974\n",
      "Average test loss: 0.0036521613813108866\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07327252774106131\n",
      "Average test loss: 0.0036835781087478\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07334201732609007\n",
      "Average test loss: 0.003676431104954746\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07335387565692265\n",
      "Average test loss: 0.0036651271741009422\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07328554219007492\n",
      "Average test loss: 0.003751739140186045\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07307263793216812\n",
      "Average test loss: 0.0037265600404805606\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07305256639586555\n",
      "Average test loss: 0.0036756880343374277\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07301727669768864\n",
      "Average test loss: 0.003692737464275625\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07300779482391145\n",
      "Average test loss: 0.0037242297588123215\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0729695428609848\n",
      "Average test loss: 0.0037441124464902614\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07292446206013362\n",
      "Average test loss: 0.0037469944117797744\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07282301031880908\n",
      "Average test loss: 0.0037725131917330955\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07293240901827812\n",
      "Average test loss: 0.00370751592475507\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07279114990433057\n",
      "Average test loss: 0.00374136787156264\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07270585413773854\n",
      "Average test loss: 0.00370869088317785\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07268962739573585\n",
      "Average test loss: 0.0036946327063358494\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07269019411669837\n",
      "Average test loss: 0.0036105871660013994\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0726596005294058\n",
      "Average test loss: 0.003722048591201504\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07269107196066114\n",
      "Average test loss: 0.0039120952491131095\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07253757994042502\n",
      "Average test loss: 0.00369547308401929\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07248608265982734\n",
      "Average test loss: 0.0037466540593239996\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07231654032733706\n",
      "Average test loss: 0.004159265934593147\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07242853458722433\n",
      "Average test loss: 0.0037062625463845002\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0725571598874198\n",
      "Average test loss: 0.003780894776806235\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07251302809847726\n",
      "Average test loss: 0.00809387586141626\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07230019421709909\n",
      "Average test loss: 0.003861197556472487\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07223930978112751\n",
      "Average test loss: 0.0038429257749683326\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07220086085465219\n",
      "Average test loss: 0.0037950620982382033\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07220413413974974\n",
      "Average test loss: 0.003674277080947326\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07205937045150333\n",
      "Average test loss: 0.003704630968057447\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07206457325485018\n",
      "Average test loss: 0.0037322513490087457\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07206357856591543\n",
      "Average test loss: 0.0036847947186066043\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07192057917515436\n",
      "Average test loss: 0.003913157125934958\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07190695133474138\n",
      "Average test loss: 0.0037388200490838953\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07196344482898712\n",
      "Average test loss: 0.0037982773940182396\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07185653832885953\n",
      "Average test loss: 0.0037280785710447364\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07181339571211073\n",
      "Average test loss: 0.00378189222369757\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07181978487968445\n",
      "Average test loss: 0.003721282715184821\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07180383998817867\n",
      "Average test loss: 0.0037021307767265373\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07176020473241806\n",
      "Average test loss: 0.0037105917479428982\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07177739157279332\n",
      "Average test loss: 0.003761140861031082\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07188697812954585\n",
      "Average test loss: 0.003694804854070147\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07186011774672402\n",
      "Average test loss: 0.0036730168083061775\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07159788182046678\n",
      "Average test loss: 0.003759655283143123\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07165639399819904\n",
      "Average test loss: 0.0037215399717291197\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07163153862953187\n",
      "Average test loss: 0.0037798001704116664\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07155493613084157\n",
      "Average test loss: 0.0038128505924509633\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0713131869898902\n",
      "Average test loss: 0.003802028763625357\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07136122183667289\n",
      "Average test loss: 0.0038077545906934473\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07133839192655352\n",
      "Average test loss: 0.003749348315513796\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07128715240293079\n",
      "Average test loss: 0.0037327826354238724\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0714245392481486\n",
      "Average test loss: 0.003719633769243956\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07135541254944272\n",
      "Average test loss: 0.0037711212899949817\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07117349538538191\n",
      "Average test loss: 0.0037145546356009114\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0712495691312684\n",
      "Average test loss: 0.003716610558331013\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0712178910308414\n",
      "Average test loss: 0.0038001438023315534\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0712639731698566\n",
      "Average test loss: 0.003721987576637831\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07115187069442537\n",
      "Average test loss: 0.0036709826420992615\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07105350525842773\n",
      "Average test loss: 0.003693469885736704\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07098485034373071\n",
      "Average test loss: 0.0036806657885511715\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07106442980633841\n",
      "Average test loss: 0.0037107737337549527\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07102392162879308\n",
      "Average test loss: 0.0037819904960278007\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07093163988987605\n",
      "Average test loss: 0.003787316975908147\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07096212476491928\n",
      "Average test loss: 0.003837536998093128\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.585345490773518\n",
      "Average test loss: 0.005219937899874317\n",
      "Epoch 2/300\n",
      "Average training loss: 4.199613636440701\n",
      "Average test loss: 0.0055533634705675974\n",
      "Epoch 3/300\n",
      "Average training loss: 3.0648295885721843\n",
      "Average test loss: 0.004158713481285506\n",
      "Epoch 4/300\n",
      "Average training loss: 2.356021113501655\n",
      "Average test loss: 0.00404813785561257\n",
      "Epoch 5/300\n",
      "Average training loss: 1.9281738006803724\n",
      "Average test loss: 0.004179252911152111\n",
      "Epoch 6/300\n",
      "Average training loss: 1.613160660955641\n",
      "Average test loss: 0.0038305667667753165\n",
      "Epoch 7/300\n",
      "Average training loss: 1.4049029467900593\n",
      "Average test loss: 0.0036989627712302737\n",
      "Epoch 8/300\n",
      "Average training loss: 1.2465820716222127\n",
      "Average test loss: 0.0035787268562449347\n",
      "Epoch 9/300\n",
      "Average training loss: 1.0740143696467082\n",
      "Average test loss: 0.003538472435540623\n",
      "Epoch 10/300\n",
      "Average training loss: 0.9327715461519029\n",
      "Average test loss: 0.003379503408032987\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7952958452436659\n",
      "Average test loss: 0.00334783093796836\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6808136018117269\n",
      "Average test loss: 0.0033632606810165776\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5780645553800795\n",
      "Average test loss: 0.003294616693837775\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4897240178055233\n",
      "Average test loss: 0.0031873480938374994\n",
      "Epoch 15/300\n",
      "Average training loss: 0.4154584043820699\n",
      "Average test loss: 0.003167799850097961\n",
      "Epoch 16/300\n",
      "Average training loss: 0.35255386646588643\n",
      "Average test loss: 0.0030670198071748016\n",
      "Epoch 17/300\n",
      "Average training loss: 0.29980435236295067\n",
      "Average test loss: 0.0031304850570029684\n",
      "Epoch 18/300\n",
      "Average training loss: 0.25830732414457536\n",
      "Average test loss: 0.003081731986254454\n",
      "Epoch 19/300\n",
      "Average training loss: 0.22557343130641513\n",
      "Average test loss: 0.002985280391242769\n",
      "Epoch 20/300\n",
      "Average training loss: 0.20132595846388074\n",
      "Average test loss: 0.0029428541099445686\n",
      "Epoch 21/300\n",
      "Average training loss: 0.18184564288457233\n",
      "Average test loss: 0.0029527544072932665\n",
      "Epoch 22/300\n",
      "Average training loss: 0.16621562153763242\n",
      "Average test loss: 0.0028987692352384327\n",
      "Epoch 23/300\n",
      "Average training loss: 0.15345330860879686\n",
      "Average test loss: 0.0029018846021758187\n",
      "Epoch 24/300\n",
      "Average training loss: 0.14304943639702267\n",
      "Average test loss: 0.002958452654381593\n",
      "Epoch 25/300\n",
      "Average training loss: 0.13463334759076437\n",
      "Average test loss: 0.002860574591077036\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12761951394875845\n",
      "Average test loss: 0.002835123329112927\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12160157389773263\n",
      "Average test loss: 0.0029009503364149066\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11668718454572889\n",
      "Average test loss: 0.002836452144094639\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11248061048322254\n",
      "Average test loss: 0.002801432700413797\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10891619234614902\n",
      "Average test loss: 0.0028494194779131146\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10585444982184304\n",
      "Average test loss: 0.002837793086758918\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10318435591459274\n",
      "Average test loss: 0.0027809749802367553\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10072931142648062\n",
      "Average test loss: 0.002786854253978365\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09882152877251307\n",
      "Average test loss: 0.0027772803329345255\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09711942020389769\n",
      "Average test loss: 0.002737711790535185\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0958378845387035\n",
      "Average test loss: 0.00275621419503457\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09454959447516335\n",
      "Average test loss: 0.0027424133910487094\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09352227886517843\n",
      "Average test loss: 0.0027471949383616446\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0925446159972085\n",
      "Average test loss: 0.0027406691012697086\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09150642914242214\n",
      "Average test loss: 0.002735970138882597\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09068207963969972\n",
      "Average test loss: 0.002738113814550969\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08978065665562948\n",
      "Average test loss: 0.0027371297284132907\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08910411417484283\n",
      "Average test loss: 0.0027023625233107145\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0883229391210609\n",
      "Average test loss: 0.0028112426160110367\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0879231293896834\n",
      "Average test loss: 0.0027194753129863077\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08692285225126478\n",
      "Average test loss: 0.0026851223152544765\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08641777516073651\n",
      "Average test loss: 0.0026917939589669307\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08589443740579818\n",
      "Average test loss: 0.0027020835114849938\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08524360522958968\n",
      "Average test loss: 0.0026777481285648213\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0846936154961586\n",
      "Average test loss: 0.002725479918014672\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08423834791448381\n",
      "Average test loss: 0.00271479155909684\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08367578570710288\n",
      "Average test loss: 0.0026799966231402426\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08340109855598873\n",
      "Average test loss: 0.0026995715104664365\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08277159684896469\n",
      "Average test loss: 0.0027165520400222803\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08239376222756174\n",
      "Average test loss: 0.002685403082312809\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08205715688069662\n",
      "Average test loss: 0.0027126891733043725\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08151570828755697\n",
      "Average test loss: 0.0026993336054599946\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08105685315529505\n",
      "Average test loss: 0.0026691973426689703\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08071550234158834\n",
      "Average test loss: 0.002678162585530016\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08045486287938224\n",
      "Average test loss: 0.0026790984742757346\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07997850965129004\n",
      "Average test loss: 0.002698039773437712\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07955383021963967\n",
      "Average test loss: 0.002684137170513471\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07912763404183917\n",
      "Average test loss: 0.0027108722943812607\n",
      "Epoch 64/300\n",
      "Average training loss: 0.078801354424821\n",
      "Average test loss: 0.0027214973071176146\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07839687677224477\n",
      "Average test loss: 0.0026981463852441974\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07815715035465029\n",
      "Average test loss: 0.002696091660608848\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0777122250729137\n",
      "Average test loss: 0.0026841074452838964\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07730846498409907\n",
      "Average test loss: 0.002694812358977894\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07704010146194035\n",
      "Average test loss: 0.0027120604119780993\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07668165340357357\n",
      "Average test loss: 0.0029776293186263908\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07650121268961164\n",
      "Average test loss: 0.002778003359834353\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0760371637278133\n",
      "Average test loss: 0.002750279866469403\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07568706558148067\n",
      "Average test loss: 0.0026862390589796835\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07542509923378626\n",
      "Average test loss: 0.002734715494223767\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07507780463165707\n",
      "Average test loss: 0.0027184725896351866\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07475840229458279\n",
      "Average test loss: 0.0027731213341984483\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07437858841816584\n",
      "Average test loss: 0.002804709161528283\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07429058192835913\n",
      "Average test loss: 0.0027537156922949686\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07375834392176735\n",
      "Average test loss: 0.002716318090342813\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07358467069599363\n",
      "Average test loss: 0.0027173946069346533\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07321509304311541\n",
      "Average test loss: 0.002732909982196159\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07295904762877359\n",
      "Average test loss: 0.002758875544493397\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07259878998332553\n",
      "Average test loss: 0.002719933630484674\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07245293438434601\n",
      "Average test loss: 0.0027451373932676185\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07208732417556975\n",
      "Average test loss: 0.002828869269746873\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07191105400853687\n",
      "Average test loss: 0.0028193814508203005\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0715942809118165\n",
      "Average test loss: 0.0027379474151465626\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07132579388883378\n",
      "Average test loss: 0.0027541280693064133\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07108227423826853\n",
      "Average test loss: 0.002800862180069089\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07103247028589249\n",
      "Average test loss: 0.0027460764015300406\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07048972170220481\n",
      "Average test loss: 0.002909228449480401\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07042099550035265\n",
      "Average test loss: 0.002760317055715455\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07014210296008322\n",
      "Average test loss: 0.00281030924556156\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06982695341772503\n",
      "Average test loss: 0.00279449793882668\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06985603919294145\n",
      "Average test loss: 0.002765897470836838\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06946884095006518\n",
      "Average test loss: 0.0027929359997312226\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06927499890989727\n",
      "Average test loss: 0.0028320502920283212\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06900622265206444\n",
      "Average test loss: 0.002794014099985361\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06876400140921275\n",
      "Average test loss: 0.002792579671781924\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06854252352317174\n",
      "Average test loss: 0.002865205910884672\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06847237916787466\n",
      "Average test loss: 0.0028129130982690385\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06822811561160617\n",
      "Average test loss: 0.0028633669912815096\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06810026578770743\n",
      "Average test loss: 0.0028395101009971568\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06792231287558874\n",
      "Average test loss: 0.0028008960793829626\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06768973174360063\n",
      "Average test loss: 0.0027933018325517573\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06745864893992742\n",
      "Average test loss: 0.0028695362800111373\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0673880430062612\n",
      "Average test loss: 0.00281285767050253\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06705446211496989\n",
      "Average test loss: 0.0027829752209492854\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06701151422990693\n",
      "Average test loss: 0.0028169476102209755\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0668758188287417\n",
      "Average test loss: 0.0027747729929784933\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06677605819039875\n",
      "Average test loss: 0.002910338452499774\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0665661640630828\n",
      "Average test loss: 0.0028128362237993215\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06635955101913876\n",
      "Average test loss: 0.002793916715722945\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06623812028434542\n",
      "Average test loss: 0.002827453277591202\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06597194184859594\n",
      "Average test loss: 0.002880938456290298\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06590755472580592\n",
      "Average test loss: 0.002863292633452349\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06577332076099184\n",
      "Average test loss: 0.0027981729385339552\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06562022121747335\n",
      "Average test loss: 0.002799645668102635\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06553846356934971\n",
      "Average test loss: 0.002808939412236214\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06527502006292343\n",
      "Average test loss: 0.002858481756928894\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06541705382201407\n",
      "Average test loss: 0.0029341613871769773\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06514393675327301\n",
      "Average test loss: 0.002892391101560659\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06499163791868422\n",
      "Average test loss: 0.003205242777657178\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06484813522961405\n",
      "Average test loss: 0.002941419018225537\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0647007342643208\n",
      "Average test loss: 0.0028730078472031487\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06473982923891809\n",
      "Average test loss: 0.002891402943473723\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06466116835342513\n",
      "Average test loss: 0.002924748763649\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06433924197488361\n",
      "Average test loss: 0.0028492716281778283\n",
      "Epoch 129/300\n",
      "Average training loss: 0.064264347937372\n",
      "Average test loss: 0.0028974902120729286\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06423342244492637\n",
      "Average test loss: 0.0029182057341353763\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06398870586686664\n",
      "Average test loss: 0.002944799898813168\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06383784937527445\n",
      "Average test loss: 0.0028927894696179363\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06387249836656782\n",
      "Average test loss: 0.002892445726527108\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06379050162765715\n",
      "Average test loss: 0.0028993707259909974\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06379180592298507\n",
      "Average test loss: 0.002882076182713111\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06352727052569389\n",
      "Average test loss: 0.0031591898523685006\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06352300100856358\n",
      "Average test loss: 0.0029403449063085847\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06335002156098683\n",
      "Average test loss: 0.002957346875634458\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06333543382088343\n",
      "Average test loss: 0.002917897882560889\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06323802613549763\n",
      "Average test loss: 0.0030179361775517465\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06310030311346054\n",
      "Average test loss: 0.002903675567772653\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0629828144941065\n",
      "Average test loss: 0.0029075499216301573\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06287053203582764\n",
      "Average test loss: 0.0029159014555108216\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06267560323079427\n",
      "Average test loss: 0.002933045830577612\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06276384466224247\n",
      "Average test loss: 0.0029087363303535512\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06265138529406654\n",
      "Average test loss: 0.0029746537959824005\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06241040313243866\n",
      "Average test loss: 0.0029694671612232925\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06253861693541209\n",
      "Average test loss: 0.002862666052662664\n",
      "Epoch 149/300\n",
      "Average training loss: 0.062415356735388436\n",
      "Average test loss: 0.0028922605958456795\n",
      "Epoch 150/300\n",
      "Average training loss: 0.062298661602867976\n",
      "Average test loss: 0.0030565312034967875\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06247578577862845\n",
      "Average test loss: 0.002858388955394427\n",
      "Epoch 152/300\n",
      "Average training loss: 0.062188762386639916\n",
      "Average test loss: 0.00284884338784549\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06196651930279202\n",
      "Average test loss: 0.002993659469195538\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06205464852518505\n",
      "Average test loss: 0.002904319800022576\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06187392392423418\n",
      "Average test loss: 0.002947059920264615\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06175554597377777\n",
      "Average test loss: 0.002938057791441679\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06179237494866053\n",
      "Average test loss: 0.0029989972545040977\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06172013081444634\n",
      "Average test loss: 0.0029253717282166085\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06170346857441796\n",
      "Average test loss: 0.002896076808994015\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06152018548382653\n",
      "Average test loss: 0.00289841887437635\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06128456540240182\n",
      "Average test loss: 0.0029473417308181525\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06148221745755937\n",
      "Average test loss: 0.002990210730375515\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06130570356713401\n",
      "Average test loss: 0.002918727998311321\n",
      "Epoch 164/300\n",
      "Average training loss: 0.061193239423963756\n",
      "Average test loss: 0.002876524909709891\n",
      "Epoch 165/300\n",
      "Average training loss: 0.061494722376267116\n",
      "Average test loss: 0.003921428317618039\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06106202376551098\n",
      "Average test loss: 0.0029704195256862373\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06106168764829636\n",
      "Average test loss: 0.0029486271072593\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06101801805032624\n",
      "Average test loss: 0.0032620031668080226\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06085486939880583\n",
      "Average test loss: 0.0030054716780367825\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06074142621623145\n",
      "Average test loss: 0.003264135864149365\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06070502271254857\n",
      "Average test loss: 0.002947604254509012\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06070562915669547\n",
      "Average test loss: 0.0028982523917737936\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06072087277968725\n",
      "Average test loss: 0.0029427420316884916\n",
      "Epoch 174/300\n",
      "Average training loss: 0.060678885287708706\n",
      "Average test loss: 0.003020339818050464\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0605309137834443\n",
      "Average test loss: 0.0031812729233254987\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06057818593912655\n",
      "Average test loss: 0.0036416946821328667\n",
      "Epoch 177/300\n",
      "Average training loss: 0.060445713837941485\n",
      "Average test loss: 0.0028762383169184127\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0603016622364521\n",
      "Average test loss: 0.003027022978083955\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06026947825153669\n",
      "Average test loss: 0.003006207451224327\n",
      "Epoch 180/300\n",
      "Average training loss: 0.060291702171166736\n",
      "Average test loss: 0.002952077650361591\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06026690168182055\n",
      "Average test loss: 0.0030948031664722498\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0601832841568523\n",
      "Average test loss: 0.0031461099981226854\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06015307301282883\n",
      "Average test loss: 0.003085349486519893\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06002916511230998\n",
      "Average test loss: 0.0030014161616563795\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05997732181019253\n",
      "Average test loss: 0.0028803295385506416\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05989886739850044\n",
      "Average test loss: 0.002945444131890933\n",
      "Epoch 187/300\n",
      "Average training loss: 0.059866074356767864\n",
      "Average test loss: 0.3143011523882548\n",
      "Epoch 188/300\n",
      "Average training loss: 0.059835851576593185\n",
      "Average test loss: 0.0029802028596815134\n",
      "Epoch 189/300\n",
      "Average training loss: 0.059756274693542055\n",
      "Average test loss: 0.0029779585116646357\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05980735647678375\n",
      "Average test loss: 0.0029209782808191248\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05966465059916178\n",
      "Average test loss: 0.0028965200999130805\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05963446115122901\n",
      "Average test loss: 0.002959475725061364\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05959527620507611\n",
      "Average test loss: 0.003065997083981832\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05948409434490734\n",
      "Average test loss: 0.0030084896383600103\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05948462734619776\n",
      "Average test loss: 0.0036383885544621283\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05945946500367588\n",
      "Average test loss: 0.002917344277103742\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05932562795943684\n",
      "Average test loss: 0.002999373625963926\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05930671073993047\n",
      "Average test loss: 0.0029862395642946166\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05925203418400553\n",
      "Average test loss: 0.00295536064915359\n",
      "Epoch 200/300\n",
      "Average training loss: 0.059315932091739444\n",
      "Average test loss: 0.0029841328859329225\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05923539524608188\n",
      "Average test loss: 0.003000062760379579\n",
      "Epoch 202/300\n",
      "Average training loss: 0.059092238230837715\n",
      "Average test loss: 0.0029830800857808854\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0590235557589266\n",
      "Average test loss: 0.0029502079799357387\n",
      "Epoch 204/300\n",
      "Average training loss: 0.059023208532068466\n",
      "Average test loss: 0.0029741819550593695\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05889700766735607\n",
      "Average test loss: 0.003027439162135124\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05905221477813191\n",
      "Average test loss: 0.0029256552869660985\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05881286416451136\n",
      "Average test loss: 0.003638333573109574\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0587792057328754\n",
      "Average test loss: 0.0030412893628494605\n",
      "Epoch 209/300\n",
      "Average training loss: 0.058851296567254595\n",
      "Average test loss: 0.0029698260525862375\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05890264482961761\n",
      "Average test loss: 0.0030104063970761165\n",
      "Epoch 211/300\n",
      "Average training loss: 0.058593726449542576\n",
      "Average test loss: 0.0029115867224625415\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05869542475210296\n",
      "Average test loss: 0.0029915540986176995\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05871909278796779\n",
      "Average test loss: 0.003193657843189107\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05855854321519534\n",
      "Average test loss: 0.0029834532851560247\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05856182368265258\n",
      "Average test loss: 0.0029727662801742553\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05858589985966683\n",
      "Average test loss: 0.003350132120359275\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05844792959756322\n",
      "Average test loss: 0.0029341867855853503\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05838994097709656\n",
      "Average test loss: 0.0029408358103699154\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05842225689358181\n",
      "Average test loss: 0.002937980428338051\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05829676102929645\n",
      "Average test loss: 0.002927321480173204\n",
      "Epoch 221/300\n",
      "Average training loss: 0.058337644325362314\n",
      "Average test loss: 0.002971027261681027\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05832924019959238\n",
      "Average test loss: 0.0029850043368836246\n",
      "Epoch 223/300\n",
      "Average training loss: 0.058151162140899236\n",
      "Average test loss: 0.0029462865814566612\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05824813032150269\n",
      "Average test loss: 0.0049408234626882605\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05817373773786757\n",
      "Average test loss: 0.002975200922538837\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05809637394216326\n",
      "Average test loss: 0.003139828315625588\n",
      "Epoch 227/300\n",
      "Average training loss: 0.058105053901672366\n",
      "Average test loss: 0.0029899106836981243\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05799395587709215\n",
      "Average test loss: 0.0030221633702102635\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05808756299813588\n",
      "Average test loss: 0.002961922075599432\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05795572924613952\n",
      "Average test loss: 0.002980660854735308\n",
      "Epoch 231/300\n",
      "Average training loss: 0.057930239180723826\n",
      "Average test loss: 0.003000112334680226\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05783094839255015\n",
      "Average test loss: 0.0029816689416766167\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05788976540168127\n",
      "Average test loss: 0.0029922708386762274\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0578404292066892\n",
      "Average test loss: 0.003009410932660103\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05768596255779267\n",
      "Average test loss: 0.002964108522153563\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05778209921386507\n",
      "Average test loss: 0.0029811259596091177\n",
      "Epoch 237/300\n",
      "Average training loss: 0.057746927211682\n",
      "Average test loss: 0.0029467844726103875\n",
      "Epoch 238/300\n",
      "Average training loss: 0.057691540684964925\n",
      "Average test loss: 0.0031666087574428983\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05768325201008055\n",
      "Average test loss: 0.0031894512242741056\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05764412092169126\n",
      "Average test loss: 0.0031302078151040605\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05745603398482005\n",
      "Average test loss: 0.002992569487955835\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05763741104139222\n",
      "Average test loss: 0.0032084449498603743\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05749088172780143\n",
      "Average test loss: 0.0030174596833272113\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05753575101494789\n",
      "Average test loss: 0.0029290140968643956\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05743634930915303\n",
      "Average test loss: 0.0030717169385817314\n",
      "Epoch 246/300\n",
      "Average training loss: 0.057340119255913626\n",
      "Average test loss: 0.0029727688257892926\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05735060638189316\n",
      "Average test loss: 0.0030216693193134336\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0574194810324245\n",
      "Average test loss: 0.0030108946437637013\n",
      "Epoch 249/300\n",
      "Average training loss: 0.057324851595693165\n",
      "Average test loss: 0.003022490793425176\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05730083309941822\n",
      "Average test loss: 0.0030300668943673372\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0571785107254982\n",
      "Average test loss: 0.002966759881315132\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05725051071908739\n",
      "Average test loss: 0.0029967828748954668\n",
      "Epoch 253/300\n",
      "Average training loss: 0.057212407701545294\n",
      "Average test loss: 0.0032307632012913623\n",
      "Epoch 254/300\n",
      "Average training loss: 0.057125527077251013\n",
      "Average test loss: 0.0029954350708673396\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05710957366228104\n",
      "Average test loss: 0.0030204410366714\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05696112233069208\n",
      "Average test loss: 0.003023018215886421\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05704511376221975\n",
      "Average test loss: 0.0030547918983631663\n",
      "Epoch 258/300\n",
      "Average training loss: 0.057023645139402813\n",
      "Average test loss: 0.0030027057801683744\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05703420807917913\n",
      "Average test loss: 0.003007400136234032\n",
      "Epoch 260/300\n",
      "Average training loss: 0.056947998848226336\n",
      "Average test loss: 0.003998635899689462\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05693195123473803\n",
      "Average test loss: 0.003135747878915734\n",
      "Epoch 262/300\n",
      "Average training loss: 0.056911965158250595\n",
      "Average test loss: 0.002978956972145372\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05682497354348501\n",
      "Average test loss: 0.0030069367583427163\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0568151440554195\n",
      "Average test loss: 0.0029910903711699776\n",
      "Epoch 265/300\n",
      "Average training loss: 0.056855604065789114\n",
      "Average test loss: 0.003057108651018805\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05673431592517429\n",
      "Average test loss: 0.0029751714823974505\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05679470012585322\n",
      "Average test loss: 0.0030044135523753035\n",
      "Epoch 268/300\n",
      "Average training loss: 0.056773298958937325\n",
      "Average test loss: 0.0029836941150327523\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05665353900525305\n",
      "Average test loss: 0.0031642107155380977\n",
      "Epoch 270/300\n",
      "Average training loss: 0.056645179993576475\n",
      "Average test loss: 0.0030555270651562347\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05659414537747701\n",
      "Average test loss: 0.003020370141085651\n",
      "Epoch 272/300\n",
      "Average training loss: 0.056542389425966476\n",
      "Average test loss: 0.0035111829816467234\n",
      "Epoch 273/300\n",
      "Average training loss: 0.056634152746862836\n",
      "Average test loss: 0.008113641641620133\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0566520463095771\n",
      "Average test loss: 0.002978510845452547\n",
      "Epoch 275/300\n",
      "Average training loss: 0.056507406526141696\n",
      "Average test loss: 0.003473473789791266\n",
      "Epoch 276/300\n",
      "Average training loss: 0.056570947209994\n",
      "Average test loss: 0.003016124213942223\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05653820828927888\n",
      "Average test loss: 0.0030402888177583614\n",
      "Epoch 278/300\n",
      "Average training loss: 0.056430067393514846\n",
      "Average test loss: 0.003135137928649783\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05646780359413889\n",
      "Average test loss: 0.0030464661253823177\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05639108500215742\n",
      "Average test loss: 0.0029504977039371927\n",
      "Epoch 281/300\n",
      "Average training loss: 0.056378471430804995\n",
      "Average test loss: 0.003060974657949474\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05640366281072299\n",
      "Average test loss: 0.0030511780826167926\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05634084908498658\n",
      "Average test loss: 0.003174395452357001\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05627283771832784\n",
      "Average test loss: 0.0029967911038547754\n",
      "Epoch 285/300\n",
      "Average training loss: 0.056313002003563775\n",
      "Average test loss: 0.003010987117679583\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05625056954556041\n",
      "Average test loss: 0.0030419590998854904\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05627429784006543\n",
      "Average test loss: 0.003090740855990185\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05612996196084552\n",
      "Average test loss: 0.00318597201216552\n",
      "Epoch 289/300\n",
      "Average training loss: 0.056275382611486645\n",
      "Average test loss: 0.0030074237034552627\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05617898913555675\n",
      "Average test loss: 0.003038824316321148\n",
      "Epoch 291/300\n",
      "Average training loss: 0.056189823398987454\n",
      "Average test loss: 0.003675469564480914\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05600037342972226\n",
      "Average test loss: 0.0034369563781138923\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05605639695790079\n",
      "Average test loss: 0.003237741598652469\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05604296040534973\n",
      "Average test loss: 0.003026386510994699\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05598104297121366\n",
      "Average test loss: 0.003044370079206096\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05592715675632159\n",
      "Average test loss: 0.002971369687985215\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05588606359230147\n",
      "Average test loss: 0.0035267250542011528\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05594994427760442\n",
      "Average test loss: 0.00300963195744488\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05591071599390772\n",
      "Average test loss: 0.0030383408191717335\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05587732484605577\n",
      "Average test loss: 0.003044852016079757\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 7.233552469677395\n",
      "Average test loss: 16.582006195073326\n",
      "Epoch 2/300\n",
      "Average training loss: 2.499607886208428\n",
      "Average test loss: 0.0038872688718967967\n",
      "Epoch 3/300\n",
      "Average training loss: 1.8363552555508085\n",
      "Average test loss: 0.0041129178293049335\n",
      "Epoch 4/300\n",
      "Average training loss: 1.3100592850579156\n",
      "Average test loss: 0.0034385338133821886\n",
      "Epoch 5/300\n",
      "Average training loss: 1.016482891559601\n",
      "Average test loss: 0.003431074927457505\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8019514888657464\n",
      "Average test loss: 0.0032320957703308928\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6142710466384887\n",
      "Average test loss: 0.0030636645594818725\n",
      "Epoch 8/300\n",
      "Average training loss: 0.48278376722335814\n",
      "Average test loss: 0.0030130339231756\n",
      "Epoch 9/300\n",
      "Average training loss: 0.3898298616144392\n",
      "Average test loss: 0.002866018589379059\n",
      "Epoch 10/300\n",
      "Average training loss: 0.318292388704088\n",
      "Average test loss: 0.0027957538285603126\n",
      "Epoch 11/300\n",
      "Average training loss: 0.26498698711395263\n",
      "Average test loss: 0.0027750136229313083\n",
      "Epoch 12/300\n",
      "Average training loss: 0.22443192303180695\n",
      "Average test loss: 0.002657579165779882\n",
      "Epoch 13/300\n",
      "Average training loss: 0.19592555648750729\n",
      "Average test loss: 0.0026312094384597405\n",
      "Epoch 14/300\n",
      "Average training loss: 0.17401459822389814\n",
      "Average test loss: 0.0025854207088963853\n",
      "Epoch 15/300\n",
      "Average training loss: 0.15655641010072496\n",
      "Average test loss: 0.0025351617601182728\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1429181393649843\n",
      "Average test loss: 0.0025765218943771388\n",
      "Epoch 17/300\n",
      "Average training loss: 0.13160986159907448\n",
      "Average test loss: 0.002440811324864626\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1223903318312433\n",
      "Average test loss: 0.002433297562516398\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11491962256034215\n",
      "Average test loss: 0.0024417605702247882\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10887874862882826\n",
      "Average test loss: 0.002409260542338921\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10369367121325598\n",
      "Average test loss: 0.002350825785141852\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09919934893316693\n",
      "Average test loss: 0.002365701648717125\n",
      "Epoch 23/300\n",
      "Average training loss: 0.09521559567583932\n",
      "Average test loss: 0.0023454525510056153\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0917794906894366\n",
      "Average test loss: 0.002325173302553594\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08907806138197581\n",
      "Average test loss: 0.0022976183984428646\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08680424654483795\n",
      "Average test loss: 0.0023081716757474675\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08474601003196504\n",
      "Average test loss: 0.0022928733142713706\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08305384717053838\n",
      "Average test loss: 0.0022503728662720984\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08151633032494121\n",
      "Average test loss: 0.0022691073868837622\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08010942875014411\n",
      "Average test loss: 0.0022356968528280657\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07886241007182333\n",
      "Average test loss: 0.0022720959617031945\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07759607566727532\n",
      "Average test loss: 0.002258960381564167\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07657061926523845\n",
      "Average test loss: 0.0022270730348924795\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0755789813929134\n",
      "Average test loss: 0.0022079523464457857\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07471280335717731\n",
      "Average test loss: 0.002205840579337544\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07376612315575282\n",
      "Average test loss: 0.00220294410818153\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07316955480310652\n",
      "Average test loss: 0.0021833731176124677\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07258285280068716\n",
      "Average test loss: 0.002200091418499748\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07149979711241192\n",
      "Average test loss: 0.0021717133453736702\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07088531733221477\n",
      "Average test loss: 0.002289910228509042\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07029274899429745\n",
      "Average test loss: 0.002168467389833596\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06973280255330934\n",
      "Average test loss: 0.0022295579198333953\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06931057917409474\n",
      "Average test loss: 0.002178350914580127\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06878766265511513\n",
      "Average test loss: 0.0021499244285126526\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06824763582812415\n",
      "Average test loss: 0.0021522957246957555\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06789318288697137\n",
      "Average test loss: 0.002179828511033621\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06745747346679369\n",
      "Average test loss: 0.0022129905776431162\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06712400675151083\n",
      "Average test loss: 0.0021330872349854973\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06646273241440455\n",
      "Average test loss: 0.0021464765090495348\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06621247679657406\n",
      "Average test loss: 0.0021699855654603905\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06578425782256656\n",
      "Average test loss: 0.002149739438874854\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0655670817858643\n",
      "Average test loss: 0.00216239745164704\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06516303497221734\n",
      "Average test loss: 0.002327702499201728\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06479325703779856\n",
      "Average test loss: 0.002140540896397498\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06437164137760798\n",
      "Average test loss: 0.0021801415130289063\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06401039570569993\n",
      "Average test loss: 0.002128205278681384\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06377275872561666\n",
      "Average test loss: 0.0023763954078571663\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06334754495488273\n",
      "Average test loss: 0.0021475074994895195\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06306394085619185\n",
      "Average test loss: 0.0021326703367133934\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06278859021928575\n",
      "Average test loss: 0.0021863420283835793\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06255411417285602\n",
      "Average test loss: 0.002145334646416207\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06222620624303818\n",
      "Average test loss: 0.0021775872260332107\n",
      "Epoch 63/300\n",
      "Average training loss: 0.061843363847997455\n",
      "Average test loss: 0.0021592593383457925\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06201611491044362\n",
      "Average test loss: 0.002137366379921635\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06133561382359928\n",
      "Average test loss: 0.002169155868391196\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06108187025454309\n",
      "Average test loss: 0.002230389423461424\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06069313410917918\n",
      "Average test loss: 0.0021416261796322134\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0604908065199852\n",
      "Average test loss: 0.002182315889849431\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0601446949839592\n",
      "Average test loss: 0.0021462111308549843\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05981030452582571\n",
      "Average test loss: 0.0021694656773987744\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05970890957117081\n",
      "Average test loss: 0.0022077617773579227\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0593151106039683\n",
      "Average test loss: 0.002165017480030656\n",
      "Epoch 73/300\n",
      "Average training loss: 0.058983187327782315\n",
      "Average test loss: 0.0021783965842591393\n",
      "Epoch 74/300\n",
      "Average training loss: 0.058827719887097674\n",
      "Average test loss: 0.002216785428838597\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05852759810619884\n",
      "Average test loss: 0.0021999884837617476\n",
      "Epoch 76/300\n",
      "Average training loss: 0.058330089731348886\n",
      "Average test loss: 0.00219001566639377\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05819043689303928\n",
      "Average test loss: 0.0021745513044297694\n",
      "Epoch 78/300\n",
      "Average training loss: 0.057690071880817415\n",
      "Average test loss: 0.0021864624503586025\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05757757162385517\n",
      "Average test loss: 0.0021878420184883807\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05730727412303289\n",
      "Average test loss: 0.002238135186334451\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05724966738621394\n",
      "Average test loss: 0.002175758383029865\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05710983541938994\n",
      "Average test loss: 0.0022166752092954187\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05695491202010049\n",
      "Average test loss: 0.0022058808112310037\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05661667300595177\n",
      "Average test loss: 0.0023698344148902427\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05621750384569168\n",
      "Average test loss: 0.002174480327094595\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05613251479466756\n",
      "Average test loss: 0.002193742426733176\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05582891775502099\n",
      "Average test loss: 0.002301145670107669\n",
      "Epoch 88/300\n",
      "Average training loss: 0.055731829828686186\n",
      "Average test loss: 0.0024647653781705433\n",
      "Epoch 89/300\n",
      "Average training loss: 0.055584043830633166\n",
      "Average test loss: 0.002252391181472275\n",
      "Epoch 90/300\n",
      "Average training loss: 0.055299212280246944\n",
      "Average test loss: 0.0022200134673880205\n",
      "Epoch 91/300\n",
      "Average training loss: 0.055173602445258034\n",
      "Average test loss: 0.002233458329199089\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05500421534644233\n",
      "Average test loss: 0.002358573673189514\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05484610042969386\n",
      "Average test loss: 0.002231571852022575\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05470386126968596\n",
      "Average test loss: 0.002209331297626098\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05454004953636064\n",
      "Average test loss: 0.00220893425701393\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0543385233812862\n",
      "Average test loss: 0.002298507041608294\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05416477250390583\n",
      "Average test loss: 0.002252202348369691\n",
      "Epoch 98/300\n",
      "Average training loss: 0.054040412892897925\n",
      "Average test loss: 0.0022960846568975185\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0539394418530994\n",
      "Average test loss: 0.002232035538388623\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05376167782478862\n",
      "Average test loss: 0.002209315369009144\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05368396908375952\n",
      "Average test loss: 0.0026284530844746363\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05347678853736983\n",
      "Average test loss: 0.0022824096536884705\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05328742210070292\n",
      "Average test loss: 0.002283681745537453\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0532241206963857\n",
      "Average test loss: 0.00232130320628898\n",
      "Epoch 105/300\n",
      "Average training loss: 0.053052149812380475\n",
      "Average test loss: 0.002277064924956196\n",
      "Epoch 106/300\n",
      "Average training loss: 0.053091348654694025\n",
      "Average test loss: 0.0022420119576983984\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05284483960270882\n",
      "Average test loss: 0.002256815186287794\n",
      "Epoch 108/300\n",
      "Average training loss: 0.052806074443790645\n",
      "Average test loss: 0.0022559699195747575\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05261354927553071\n",
      "Average test loss: 0.002270479558656613\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05248812931113773\n",
      "Average test loss: 0.0022738769720825882\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05247787387834655\n",
      "Average test loss: 0.002305660615985592\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05242544156312943\n",
      "Average test loss: 0.0025587331160075136\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05213423338863585\n",
      "Average test loss: 0.0023097109235823156\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05203635951545504\n",
      "Average test loss: 0.002252730797458854\n",
      "Epoch 115/300\n",
      "Average training loss: 0.052023667458030914\n",
      "Average test loss: 0.0022717971896959676\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05191480360097355\n",
      "Average test loss: 0.002340752002058758\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05175757947232988\n",
      "Average test loss: 0.0022858455125242473\n",
      "Epoch 118/300\n",
      "Average training loss: 0.051648425294293296\n",
      "Average test loss: 0.0023089938747386136\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05155145482222239\n",
      "Average test loss: 0.0023109683023972644\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05152583417958683\n",
      "Average test loss: 0.0025547462310642004\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05141322679320971\n",
      "Average test loss: 0.002432766397794088\n",
      "Epoch 122/300\n",
      "Average training loss: 0.051328943885034985\n",
      "Average test loss: 0.002351525058452454\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05125903282894029\n",
      "Average test loss: 0.0023184253414058023\n",
      "Epoch 124/300\n",
      "Average training loss: 0.051165527366929586\n",
      "Average test loss: 0.002332454646523628\n",
      "Epoch 125/300\n",
      "Average training loss: 0.051037053022119735\n",
      "Average test loss: 0.0022949147542110746\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05090940405925115\n",
      "Average test loss: 0.002281204628965093\n",
      "Epoch 127/300\n",
      "Average training loss: 0.050887795607248945\n",
      "Average test loss: 0.0022910571673678028\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0508457245628039\n",
      "Average test loss: 0.0023175730984658005\n",
      "Epoch 129/300\n",
      "Average training loss: 0.050780139840311475\n",
      "Average test loss: 0.00252622199513846\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05059606720010439\n",
      "Average test loss: 0.00231601820844743\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05077367486556371\n",
      "Average test loss: 0.0024384875678353838\n",
      "Epoch 132/300\n",
      "Average training loss: 0.050546284394131766\n",
      "Average test loss: 0.002288166865396003\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05037140837974018\n",
      "Average test loss: 0.0022973532668418353\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05033665525913238\n",
      "Average test loss: 0.002314219741978579\n",
      "Epoch 135/300\n",
      "Average training loss: 0.050272669798798034\n",
      "Average test loss: 0.002363458897401061\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05025623156958156\n",
      "Average test loss: 0.0023335849226762853\n",
      "Epoch 137/300\n",
      "Average training loss: 0.050083222895860674\n",
      "Average test loss: 0.0023166860762155717\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05009105899598863\n",
      "Average test loss: 0.002301055814863907\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0500296044283443\n",
      "Average test loss: 0.0023079591961577533\n",
      "Epoch 140/300\n",
      "Average training loss: 0.049907367100318276\n",
      "Average test loss: 0.0023313500750809907\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04994666192597813\n",
      "Average test loss: 0.0022832957382003466\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04977333311239878\n",
      "Average test loss: 0.00490403660842114\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04974807075659434\n",
      "Average test loss: 0.002297075385124319\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04970963430404663\n",
      "Average test loss: 0.002300295233519541\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04976838757594426\n",
      "Average test loss: 0.0032217514131011233\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04964881798956129\n",
      "Average test loss: 0.0026399091709819107\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04945815944009357\n",
      "Average test loss: 0.0023118589324876665\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04943379930986298\n",
      "Average test loss: 0.0023060064694533744\n",
      "Epoch 149/300\n",
      "Average training loss: 0.049328942196236715\n",
      "Average test loss: 0.0023398504292385445\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04933478749129507\n",
      "Average test loss: 0.0022897484130743476\n",
      "Epoch 151/300\n",
      "Average training loss: 0.049289094862010746\n",
      "Average test loss: 0.0025008922525578074\n",
      "Epoch 152/300\n",
      "Average training loss: 0.049242119822237224\n",
      "Average test loss: 0.0022911503788911633\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04916911090082592\n",
      "Average test loss: 0.0023092626651955977\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04919331091973517\n",
      "Average test loss: 0.002351994438924723\n",
      "Epoch 155/300\n",
      "Average training loss: 0.049094346639182836\n",
      "Average test loss: 0.002431135911701454\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04904222958286603\n",
      "Average test loss: 0.002320509774196479\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04892220800121625\n",
      "Average test loss: 0.0023754177395668295\n",
      "Epoch 158/300\n",
      "Average training loss: 0.048971617978480125\n",
      "Average test loss: 0.0024492666357093388\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04883210049404038\n",
      "Average test loss: 0.002374454616775943\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04886313655972481\n",
      "Average test loss: 0.002314466274653872\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04871871680352423\n",
      "Average test loss: 0.002382630653058489\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0486526040037473\n",
      "Average test loss: 0.0023651338734974464\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04863201113210784\n",
      "Average test loss: 0.0023196409735828636\n",
      "Epoch 164/300\n",
      "Average training loss: 0.048515953603718014\n",
      "Average test loss: 0.002476774608198967\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04862727648682064\n",
      "Average test loss: 0.002315638332731194\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04845621113313569\n",
      "Average test loss: 0.002381730074269904\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04850439984930886\n",
      "Average test loss: 0.003940223335805866\n",
      "Epoch 168/300\n",
      "Average training loss: 0.048501116577121944\n",
      "Average test loss: 0.002383333484745688\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04842178790105714\n",
      "Average test loss: 0.0023514614067971706\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04837496848238839\n",
      "Average test loss: 0.003375713665659229\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04822557434770796\n",
      "Average test loss: 0.0023993496293615965\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04820991571413146\n",
      "Average test loss: 0.0023573270425614385\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04822033249007331\n",
      "Average test loss: 0.0023296899311244486\n",
      "Epoch 174/300\n",
      "Average training loss: 0.048166703919569655\n",
      "Average test loss: 0.0023248586335943805\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04812189882331424\n",
      "Average test loss: 0.0023546045741273296\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04800904248820411\n",
      "Average test loss: 0.002756650036821763\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04816864231228828\n",
      "Average test loss: 0.002471825587666697\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04795611167616314\n",
      "Average test loss: 0.002327739455633693\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0479110972682635\n",
      "Average test loss: 0.0023550812631017632\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04793413818875949\n",
      "Average test loss: 0.002830431609931919\n",
      "Epoch 181/300\n",
      "Average training loss: 0.047913404809104075\n",
      "Average test loss: 0.0023407262315352758\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04781231497062577\n",
      "Average test loss: 0.002394210958853364\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0477676654888524\n",
      "Average test loss: 0.0023546486361366178\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04771940854854054\n",
      "Average test loss: 0.002442664093648394\n",
      "Epoch 185/300\n",
      "Average training loss: 0.047682825413015155\n",
      "Average test loss: 0.002410983067419794\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04781256925397449\n",
      "Average test loss: 0.0023946133320116333\n",
      "Epoch 187/300\n",
      "Average training loss: 0.047708185609843995\n",
      "Average test loss: 0.002358761910866532\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04756777093807856\n",
      "Average test loss: 0.002489532138531407\n",
      "Epoch 189/300\n",
      "Average training loss: 0.047495500129130154\n",
      "Average test loss: 0.0033333156444132327\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04739735046029091\n",
      "Average test loss: 0.0024874803795375757\n",
      "Epoch 191/300\n",
      "Average training loss: 0.047509727279345194\n",
      "Average test loss: 0.0023514749130441083\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04751190155413416\n",
      "Average test loss: 0.0025221072969337306\n",
      "Epoch 193/300\n",
      "Average training loss: 0.047522207998567156\n",
      "Average test loss: 0.002448387020164066\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04740686775578393\n",
      "Average test loss: 0.0025406870250072743\n",
      "Epoch 195/300\n",
      "Average training loss: 0.047242640452252495\n",
      "Average test loss: 0.0027943647236873706\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04727020128568014\n",
      "Average test loss: 0.0023811558433290986\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04730244973301888\n",
      "Average test loss: 0.0024055510740727186\n",
      "Epoch 198/300\n",
      "Average training loss: 0.047266135828362574\n",
      "Average test loss: 0.002381868952471349\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04730353418323729\n",
      "Average test loss: 0.002425132662471798\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04729800062709384\n",
      "Average test loss: 0.0027627514604892994\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04716764548089769\n",
      "Average test loss: 0.0023696033967037997\n",
      "Epoch 202/300\n",
      "Average training loss: 0.047095359534025195\n",
      "Average test loss: 0.002399189586751163\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04705611461897691\n",
      "Average test loss: 0.002395828190880517\n",
      "Epoch 204/300\n",
      "Average training loss: 0.047062492258018916\n",
      "Average test loss: 0.0023207167823695476\n",
      "Epoch 205/300\n",
      "Average training loss: 0.047072533001502354\n",
      "Average test loss: 0.0024767491670532357\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04714057885275947\n",
      "Average test loss: 0.002709855129838818\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04692541723118888\n",
      "Average test loss: 0.002376635932880971\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04688467753264639\n",
      "Average test loss: 0.00301525232133766\n",
      "Epoch 209/300\n",
      "Average training loss: 0.047040754543410404\n",
      "Average test loss: 0.0023314769489483703\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04676058181954755\n",
      "Average test loss: 0.0027117340647512013\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04673778388897578\n",
      "Average test loss: 0.0027100205924361943\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04683035229643186\n",
      "Average test loss: 0.002555647059654196\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04670903274416924\n",
      "Average test loss: 0.0024506572431160344\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04679179686639044\n",
      "Average test loss: 0.0023534357926497857\n",
      "Epoch 215/300\n",
      "Average training loss: 0.046619287424617344\n",
      "Average test loss: 0.0023817351785384946\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04668102198176914\n",
      "Average test loss: 0.0024004518934008148\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04663690634899669\n",
      "Average test loss: 0.0032701338708607686\n",
      "Epoch 218/300\n",
      "Average training loss: 0.046609426468610764\n",
      "Average test loss: 0.0023837452205932802\n",
      "Epoch 219/300\n",
      "Average training loss: 0.046624140593740676\n",
      "Average test loss: 0.0025353091326024797\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04661325230863359\n",
      "Average test loss: 0.0025512247182842757\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04653498175409105\n",
      "Average test loss: 0.002722931742668152\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04647745374507374\n",
      "Average test loss: 0.002500240964194139\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04647572764754295\n",
      "Average test loss: 0.0024585959658854537\n",
      "Epoch 224/300\n",
      "Average training loss: 0.046521999074353115\n",
      "Average test loss: 0.0024770635342639354\n",
      "Epoch 225/300\n",
      "Average training loss: 0.046358003199100495\n",
      "Average test loss: 0.0031811340231862333\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04636363509959645\n",
      "Average test loss: 0.002841151046049264\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04642033526301384\n",
      "Average test loss: 0.0023485702553557026\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04632351335220867\n",
      "Average test loss: 0.002370104430657294\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04634512481755681\n",
      "Average test loss: 0.0023784146439284086\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04634825090236134\n",
      "Average test loss: 0.0023709818805154\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04626736788948377\n",
      "Average test loss: 0.002410535364722212\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04620280797613992\n",
      "Average test loss: 0.002387266278059946\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04620648183094131\n",
      "Average test loss: 0.0023747307126306826\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04616193151805136\n",
      "Average test loss: 0.0030411206477632124\n",
      "Epoch 235/300\n",
      "Average training loss: 0.046228475444846685\n",
      "Average test loss: 0.004257967174467113\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04617721698681514\n",
      "Average test loss: 0.0024705251020689803\n",
      "Epoch 237/300\n",
      "Average training loss: 0.046061471823188996\n",
      "Average test loss: 0.0025608265606893435\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04609271930985981\n",
      "Average test loss: 0.0024286753974027105\n",
      "Epoch 239/300\n",
      "Average training loss: 0.046033842086791996\n",
      "Average test loss: 0.0024585266643100314\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04598350591460864\n",
      "Average test loss: 0.0023829537752187912\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04597499277856615\n",
      "Average test loss: 0.002404393612510628\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04599374120765262\n",
      "Average test loss: 0.002416964573164781\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04606315286623107\n",
      "Average test loss: 0.002432326487162047\n",
      "Epoch 244/300\n",
      "Average training loss: 0.045922969957192736\n",
      "Average test loss: 0.0024093487924999662\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0458706790473726\n",
      "Average test loss: 0.002380545478210681\n",
      "Epoch 246/300\n",
      "Average training loss: 0.045799303167396124\n",
      "Average test loss: 0.003107217627680964\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04595449291997486\n",
      "Average test loss: 0.0024749116856190895\n",
      "Epoch 248/300\n",
      "Average training loss: 0.045885789434115094\n",
      "Average test loss: 0.04723298558923933\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04577247680558099\n",
      "Average test loss: 0.002468178712659412\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04642903419004546\n",
      "Average test loss: 0.0024159943227552707\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04564873528149393\n",
      "Average test loss: 0.0024343538195308713\n",
      "Epoch 252/300\n",
      "Average training loss: 0.045646694011158416\n",
      "Average test loss: 0.0023751801372402246\n",
      "Epoch 253/300\n",
      "Average training loss: 0.045647635059224236\n",
      "Average test loss: 0.0024633807618584897\n",
      "Epoch 254/300\n",
      "Average training loss: 0.045698781934049394\n",
      "Average test loss: 0.0023761505039615765\n",
      "Epoch 255/300\n",
      "Average training loss: 0.045658803539143665\n",
      "Average test loss: 0.0024202027805149554\n",
      "Epoch 256/300\n",
      "Average training loss: 0.045727527611785466\n",
      "Average test loss: 0.0024390943524324232\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04566821202304628\n",
      "Average test loss: 0.0024622662169858814\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04558347013261583\n",
      "Average test loss: 0.0024024074835081897\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04555247921413846\n",
      "Average test loss: 0.0023827243198951084\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04572970242963897\n",
      "Average test loss: 0.0023851179236339197\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04546444235576524\n",
      "Average test loss: 0.003053146778502398\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04557521746555964\n",
      "Average test loss: 0.0026789843164798286\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04557281510035197\n",
      "Average test loss: 0.0023645458513249953\n",
      "Epoch 264/300\n",
      "Average training loss: 0.045499360173940656\n",
      "Average test loss: 0.00244144947350853\n",
      "Epoch 265/300\n",
      "Average training loss: 0.045427163624101215\n",
      "Average test loss: 0.0024283277495867677\n",
      "Epoch 266/300\n",
      "Average training loss: 0.045410114924112954\n",
      "Average test loss: 0.002635456724299325\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04553522904382812\n",
      "Average test loss: 0.00239269510573811\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04544731981224484\n",
      "Average test loss: 0.0023851616523332068\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04535509268773927\n",
      "Average test loss: 0.022820050502816835\n",
      "Epoch 270/300\n",
      "Average training loss: 0.045380371315611734\n",
      "Average test loss: 0.0026398144654101795\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0453190572294924\n",
      "Average test loss: 0.0024480439000245597\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04526515463325712\n",
      "Average test loss: 0.002402667957978944\n",
      "Epoch 273/300\n",
      "Average training loss: 0.045421242747041915\n",
      "Average test loss: 0.002639262196297447\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04536988903416528\n",
      "Average test loss: 0.002449824113709231\n",
      "Epoch 275/300\n",
      "Average training loss: 0.045223440908723406\n",
      "Average test loss: 0.002745771656330261\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04516693287756708\n",
      "Average test loss: 0.002513017433385054\n",
      "Epoch 277/300\n",
      "Average training loss: 0.045327123946613736\n",
      "Average test loss: 0.002362288287944264\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04525815912750032\n",
      "Average test loss: 0.002429371664300561\n",
      "Epoch 279/300\n",
      "Average training loss: 0.045196298354201844\n",
      "Average test loss: 0.002356501858267519\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04519400681555271\n",
      "Average test loss: 0.0024265167853898474\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0451233547296789\n",
      "Average test loss: 0.003312756660497851\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04508343336648411\n",
      "Average test loss: 0.0032050227580798997\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04515737110045221\n",
      "Average test loss: 0.0024866746566775772\n",
      "Epoch 284/300\n",
      "Average training loss: 0.045021809866031014\n",
      "Average test loss: 0.002406459386357003\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04514128839969635\n",
      "Average test loss: 0.002381211166580518\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04509107617868317\n",
      "Average test loss: 0.002398818799811933\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04504300493995349\n",
      "Average test loss: 0.002379886871824662\n",
      "Epoch 288/300\n",
      "Average training loss: 0.045026495234833826\n",
      "Average test loss: 0.002424244972359803\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04512871275345484\n",
      "Average test loss: 0.002441535199681918\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04499336483743456\n",
      "Average test loss: 0.0024230318665504455\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04502794691589144\n",
      "Average test loss: 0.003587264500765337\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0448543906642331\n",
      "Average test loss: 0.005271731172170904\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04482843186126815\n",
      "Average test loss: 0.00252460677569939\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04489586940407753\n",
      "Average test loss: 0.0024571936128454075\n",
      "Epoch 295/300\n",
      "Average training loss: 0.044949007855521306\n",
      "Average test loss: 0.0028077776552074486\n",
      "Epoch 296/300\n",
      "Average training loss: 0.044929449458916984\n",
      "Average test loss: 0.002359126431143118\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04476061203082402\n",
      "Average test loss: 0.0024022929813298914\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04479220589001973\n",
      "Average test loss: 0.0024701042471246587\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04481196319394642\n",
      "Average test loss: 0.0024425669358008437\n",
      "Epoch 300/300\n",
      "Average training loss: 0.044869601799382104\n",
      "Average test loss: 0.002376746896136966\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.68\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.90\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.93\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.02\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.12\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.17\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.10\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.91\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.45\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.46\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.05\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.22\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.58\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
