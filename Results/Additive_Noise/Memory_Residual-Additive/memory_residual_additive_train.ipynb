{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.ImageDataset import ImageDataset\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.05)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.05)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.05)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.05)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15948997675710255\n",
      "Average test loss: 0.011298070820669333\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06529930844571856\n",
      "Average test loss: 0.009895940134508742\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05897488435109456\n",
      "Average test loss: 0.0110298741352227\n",
      "Epoch 4/300\n",
      "Average training loss: 0.055486474553743996\n",
      "Average test loss: 0.00933764979408847\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05320913322104348\n",
      "Average test loss: 0.008755339318679438\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05088126142157449\n",
      "Average test loss: 0.00908469972676701\n",
      "Epoch 7/300\n",
      "Average training loss: 0.049470247397820154\n",
      "Average test loss: 0.008909200750291347\n",
      "Epoch 8/300\n",
      "Average training loss: 0.048239920841323\n",
      "Average test loss: 0.008777254226307074\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04738866111636162\n",
      "Average test loss: 0.00834872154560354\n",
      "Epoch 10/300\n",
      "Average training loss: 0.046577177478207485\n",
      "Average test loss: 0.008327506497502328\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04595024015174972\n",
      "Average test loss: 0.007992377422749997\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04521327621075842\n",
      "Average test loss: 0.007869665715429517\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04454355963733461\n",
      "Average test loss: 0.007996099061850044\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04405936469965511\n",
      "Average test loss: 0.00777855666478475\n",
      "Epoch 15/300\n",
      "Average training loss: 0.043555567433436713\n",
      "Average test loss: 0.008174836991561784\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04325803276896477\n",
      "Average test loss: 0.008084021717309951\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04295905297663477\n",
      "Average test loss: 0.007875918483568563\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04238551416993141\n",
      "Average test loss: 0.0074477012472020255\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04206416074103779\n",
      "Average test loss: 0.007501309396906032\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04173344464765655\n",
      "Average test loss: 0.007434656927155124\n",
      "Epoch 21/300\n",
      "Average training loss: 0.041440833393070434\n",
      "Average test loss: 0.007542676854464743\n",
      "Epoch 22/300\n",
      "Average training loss: 0.041175905737611984\n",
      "Average test loss: 0.007305486348768075\n",
      "Epoch 23/300\n",
      "Average training loss: 0.040995475182930625\n",
      "Average test loss: 0.008067775868707234\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04069015651279025\n",
      "Average test loss: 0.00723815166991618\n",
      "Epoch 25/300\n",
      "Average training loss: 0.040415040598975285\n",
      "Average test loss: 0.007316995561536815\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04038423236542278\n",
      "Average test loss: 0.007289731022384432\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04006665254963769\n",
      "Average test loss: 0.007128769077774551\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03975912542144457\n",
      "Average test loss: 0.007228819351229402\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039686607079373465\n",
      "Average test loss: 0.007091392261286577\n",
      "Epoch 30/300\n",
      "Average training loss: 0.039671540174219345\n",
      "Average test loss: 0.008441688607136408\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0393917414645354\n",
      "Average test loss: 0.007160754255950451\n",
      "Epoch 32/300\n",
      "Average training loss: 0.039216902931531274\n",
      "Average test loss: 0.00702277950445811\n",
      "Epoch 33/300\n",
      "Average training loss: 0.039063539216915764\n",
      "Average test loss: 0.0069895410347316\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03899286505248811\n",
      "Average test loss: 0.0070889664615194\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03879846419559585\n",
      "Average test loss: 0.007034369634257422\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03870524864726596\n",
      "Average test loss: 0.008657828468829393\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03863642658458816\n",
      "Average test loss: 0.0069659047176440555\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03855037769675255\n",
      "Average test loss: 0.006887616005208757\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03831320913301574\n",
      "Average test loss: 0.006907310311165121\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03822923340068923\n",
      "Average test loss: 0.0070964456370307336\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03826631302303738\n",
      "Average test loss: 0.0068682726609210175\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0380757565713591\n",
      "Average test loss: 0.006941872607502672\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03804976953400506\n",
      "Average test loss: 0.006957245962487327\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03787733210126559\n",
      "Average test loss: 0.006908354798952738\n",
      "Epoch 45/300\n",
      "Average training loss: 0.037794721361663605\n",
      "Average test loss: 0.006938275115357505\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0377314209540685\n",
      "Average test loss: 0.006864437383496099\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03774847907324632\n",
      "Average test loss: 0.006925340221159988\n",
      "Epoch 48/300\n",
      "Average training loss: 0.037660773512389924\n",
      "Average test loss: 0.007195604310267501\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03744461188051436\n",
      "Average test loss: 0.0068894863939947554\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03748628248771032\n",
      "Average test loss: 0.007188525976406204\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03739036323626836\n",
      "Average test loss: 0.007133010086499982\n",
      "Epoch 52/300\n",
      "Average training loss: 0.037328591141435837\n",
      "Average test loss: 0.006866028582056364\n",
      "Epoch 53/300\n",
      "Average training loss: 0.037271688408321804\n",
      "Average test loss: 0.006822930055773921\n",
      "Epoch 54/300\n",
      "Average training loss: 0.037104175951745776\n",
      "Average test loss: 0.006800768595602778\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03711060517695215\n",
      "Average test loss: 0.006825460989442137\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03709468763404422\n",
      "Average test loss: 0.00709545090711779\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03691053304407332\n",
      "Average test loss: 0.006803752899997764\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03693899773226844\n",
      "Average test loss: 0.007201253587587012\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03683914908104473\n",
      "Average test loss: 0.0068844837360084055\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03678984908262889\n",
      "Average test loss: 0.006843537851340241\n",
      "Epoch 61/300\n",
      "Average training loss: 0.036734531390998096\n",
      "Average test loss: 0.006983893820808993\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03668926596310403\n",
      "Average test loss: 0.009125622213714652\n",
      "Epoch 63/300\n",
      "Average training loss: 0.036695744110478296\n",
      "Average test loss: 0.00686307960583104\n",
      "Epoch 64/300\n",
      "Average training loss: 0.036607660743925304\n",
      "Average test loss: 0.00718116798872749\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03663869486252467\n",
      "Average test loss: 0.05578115591737959\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03705401011308034\n",
      "Average test loss: 0.0067729739873773525\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03637080291244719\n",
      "Average test loss: 0.00676172537687752\n",
      "Epoch 68/300\n",
      "Average training loss: 0.036365463654200236\n",
      "Average test loss: 0.006780051392399603\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03637667890058623\n",
      "Average test loss: 0.006934133607894182\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03633548368679153\n",
      "Average test loss: 0.006779973426212867\n",
      "Epoch 71/300\n",
      "Average training loss: 0.036282294829686486\n",
      "Average test loss: 0.006844771016389132\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03624575654003355\n",
      "Average test loss: 0.006896387553049458\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03620775133040216\n",
      "Average test loss: 0.006837073461876975\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03612339849935638\n",
      "Average test loss: 0.006820987027138472\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03611594449149238\n",
      "Average test loss: 0.006809014066225952\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03606559936536683\n",
      "Average test loss: 0.0068126004942589335\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03596340934766663\n",
      "Average test loss: 0.006980085087319215\n",
      "Epoch 78/300\n",
      "Average training loss: 0.036041776415374545\n",
      "Average test loss: 0.006935366040716568\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03590647187829018\n",
      "Average test loss: 0.006742450319644477\n",
      "Epoch 80/300\n",
      "Average training loss: 0.035886760900417966\n",
      "Average test loss: 0.0067815671670768\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03589192152685589\n",
      "Average test loss: 0.00693498811374108\n",
      "Epoch 82/300\n",
      "Average training loss: 0.036173325215776764\n",
      "Average test loss: 0.008517001904547215\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03574593673812018\n",
      "Average test loss: 0.006991344172507525\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0357820245789157\n",
      "Average test loss: 0.006850799258798361\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0357410712507036\n",
      "Average test loss: 0.006744038490785493\n",
      "Epoch 86/300\n",
      "Average training loss: 0.035625573813915255\n",
      "Average test loss: 0.006957758628659778\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03564721027678914\n",
      "Average test loss: 0.0070520969728628796\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03558144601186117\n",
      "Average test loss: 0.006921975093583266\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03556839838127295\n",
      "Average test loss: 0.007215874486085442\n",
      "Epoch 90/300\n",
      "Average training loss: 0.035582308663262265\n",
      "Average test loss: 0.006747281384964784\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03552193215654956\n",
      "Average test loss: 0.0068773545432421895\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03546160586343871\n",
      "Average test loss: 0.006981050871312618\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03548880782557858\n",
      "Average test loss: 0.006838514968752861\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03541438231534428\n",
      "Average test loss: 0.006771792742113272\n",
      "Epoch 95/300\n",
      "Average training loss: 0.035363602721028856\n",
      "Average test loss: 0.006839762038654751\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03534331785970264\n",
      "Average test loss: 0.006833393274495999\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03530171626143985\n",
      "Average test loss: 0.006704939322753085\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03528111077679528\n",
      "Average test loss: 0.006757510991560088\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03526280497180091\n",
      "Average test loss: 0.007336349195076359\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03523120620350043\n",
      "Average test loss: 0.0067692969023353524\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03519474604725838\n",
      "Average test loss: 0.006947327910198106\n",
      "Epoch 102/300\n",
      "Average training loss: 0.035196454289886686\n",
      "Average test loss: 0.006729476805776358\n",
      "Epoch 103/300\n",
      "Average training loss: 0.035118355919917424\n",
      "Average test loss: 0.006753625957088338\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0351129313674238\n",
      "Average test loss: 0.006726692054834631\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03508196892009841\n",
      "Average test loss: 0.006775011968281534\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03508496955037117\n",
      "Average test loss: 0.006989592719823122\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03501883218354649\n",
      "Average test loss: 0.0068230311125516896\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03498386081722048\n",
      "Average test loss: 0.012100928490360577\n",
      "Epoch 109/300\n",
      "Average training loss: 0.034989599350425934\n",
      "Average test loss: 0.006930594565139877\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03498623415165477\n",
      "Average test loss: 0.0067566777277323934\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0349216209186448\n",
      "Average test loss: 0.006744114060782724\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03484209426575237\n",
      "Average test loss: 0.007126032913724581\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03486607849929068\n",
      "Average test loss: 0.006994666769686672\n",
      "Epoch 114/300\n",
      "Average training loss: 0.034792257353663446\n",
      "Average test loss: 0.007118985124760204\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03486971697542403\n",
      "Average test loss: 0.006811903249058458\n",
      "Epoch 116/300\n",
      "Average training loss: 0.034758382158146965\n",
      "Average test loss: 0.006793577606893248\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03474756664699978\n",
      "Average test loss: 0.0072897429780827624\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03476046894656287\n",
      "Average test loss: 0.006904891016582648\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03465655433800485\n",
      "Average test loss: 0.006855890220238103\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03467700823148091\n",
      "Average test loss: 0.006806899547162983\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03466535288923316\n",
      "Average test loss: 0.006917800585428874\n",
      "Epoch 122/300\n",
      "Average training loss: 0.034663481530216006\n",
      "Average test loss: 0.006788712594244215\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03463206241528193\n",
      "Average test loss: 0.006859378810558054\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03456972168551551\n",
      "Average test loss: 0.00735617274708218\n",
      "Epoch 125/300\n",
      "Average training loss: 0.034605614696939783\n",
      "Average test loss: 0.006933983536230193\n",
      "Epoch 126/300\n",
      "Average training loss: 0.034518310421042975\n",
      "Average test loss: 0.0068093979735341335\n",
      "Epoch 127/300\n",
      "Average training loss: 0.034503744853867424\n",
      "Average test loss: 0.006956275108787748\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03449712745679749\n",
      "Average test loss: 0.0069604309954577025\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03446296689245436\n",
      "Average test loss: 0.006890081216891606\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03447164630558756\n",
      "Average test loss: 0.006789608618037568\n",
      "Epoch 131/300\n",
      "Average training loss: 0.034470359378390845\n",
      "Average test loss: 0.0067976913580464\n",
      "Epoch 132/300\n",
      "Average training loss: 0.034330945385826955\n",
      "Average test loss: 0.007154815626641114\n",
      "Epoch 133/300\n",
      "Average training loss: 0.034353458295265836\n",
      "Average test loss: 0.007034548783881797\n",
      "Epoch 134/300\n",
      "Average training loss: 0.034382251504394744\n",
      "Average test loss: 0.00752270891972714\n",
      "Epoch 135/300\n",
      "Average training loss: 0.034345203383101354\n",
      "Average test loss: 0.007273627032008436\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03431623546944724\n",
      "Average test loss: 0.006973327296061648\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03431959206693702\n",
      "Average test loss: 0.006948766490858462\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03429436989294158\n",
      "Average test loss: 0.006848554347124365\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03423741095761458\n",
      "Average test loss: 0.006873032958971129\n",
      "Epoch 140/300\n",
      "Average training loss: 0.034217286025484406\n",
      "Average test loss: 0.006870003422101339\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03428234794073635\n",
      "Average test loss: 0.0070809433807929355\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03426576490700245\n",
      "Average test loss: 0.006942233805027273\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03418825708495246\n",
      "Average test loss: 0.007081502820882532\n",
      "Epoch 144/300\n",
      "Average training loss: 0.034096207274330985\n",
      "Average test loss: 0.006860759459849861\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03416791463891665\n",
      "Average test loss: 0.006904814474284649\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03406546683443917\n",
      "Average test loss: 0.006912496639622582\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03412041271064017\n",
      "Average test loss: 0.006819262888282537\n",
      "Epoch 148/300\n",
      "Average training loss: 0.034134195269809826\n",
      "Average test loss: 0.0068689073129660555\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0340424641834365\n",
      "Average test loss: 0.006977462191548612\n",
      "Epoch 150/300\n",
      "Average training loss: 0.034017797738313676\n",
      "Average test loss: 0.00680639414033956\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03403953759206666\n",
      "Average test loss: 0.00800976761471894\n",
      "Epoch 152/300\n",
      "Average training loss: 0.034008632626798416\n",
      "Average test loss: 0.007359339106414053\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03395155701041221\n",
      "Average test loss: 0.007170173110647334\n",
      "Epoch 154/300\n",
      "Average training loss: 0.034000194821092816\n",
      "Average test loss: 0.006858638632214731\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03390329179830021\n",
      "Average test loss: 0.006864714492940241\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0338842051062319\n",
      "Average test loss: 0.007107370737526152\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03397343248128891\n",
      "Average test loss: 0.006846330491618978\n",
      "Epoch 158/300\n",
      "Average training loss: 0.033877814359135096\n",
      "Average test loss: 0.006960928240584002\n",
      "Epoch 159/300\n",
      "Average training loss: 0.033822116139862274\n",
      "Average test loss: 0.0068882410555250115\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03383942234185007\n",
      "Average test loss: 0.006958497983300024\n",
      "Epoch 161/300\n",
      "Average training loss: 0.033819417658779355\n",
      "Average test loss: 0.006946250101344453\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03376737124390072\n",
      "Average test loss: 0.006880959673060311\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03381659476955732\n",
      "Average test loss: 0.0070681844337119\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03379206124941508\n",
      "Average test loss: 0.007290303177303738\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03378332104947832\n",
      "Average test loss: 0.006895747173577547\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03368175210224258\n",
      "Average test loss: 0.007186067150698768\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03372240760922432\n",
      "Average test loss: 0.007046216632756922\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03367455759147803\n",
      "Average test loss: 0.00697193848548664\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03365966492063469\n",
      "Average test loss: 0.007294472680737575\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03365926293200917\n",
      "Average test loss: 0.006980818344073163\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0336667252017392\n",
      "Average test loss: 0.007183531113796764\n",
      "Epoch 172/300\n",
      "Average training loss: 0.033708791332112416\n",
      "Average test loss: 0.006988903006745709\n",
      "Epoch 173/300\n",
      "Average training loss: 0.033594002654155096\n",
      "Average test loss: 0.006907736156963639\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03357940328121185\n",
      "Average test loss: 0.0069991169787115524\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03362710813350148\n",
      "Average test loss: 0.007412333887484339\n",
      "Epoch 176/300\n",
      "Average training loss: 0.033576246135764654\n",
      "Average test loss: 0.026131628072924084\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03354344354569912\n",
      "Average test loss: 0.007389953040414387\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03352163140310181\n",
      "Average test loss: 0.008380880827291144\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0335373602575726\n",
      "Average test loss: 0.006895906922717889\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03359129777881834\n",
      "Average test loss: 0.007028113653676378\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03347990665502018\n",
      "Average test loss: 0.006919776508791579\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03346166589856148\n",
      "Average test loss: 0.006927791093372636\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03345446598860952\n",
      "Average test loss: 0.006901665471701159\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03349727833271027\n",
      "Average test loss: 0.007636329917444123\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03347895238134596\n",
      "Average test loss: 0.007292702324274513\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0334088662399186\n",
      "Average test loss: 0.006960047074904044\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0334448947277334\n",
      "Average test loss: 0.008358743626210424\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03343334711260266\n",
      "Average test loss: 0.006978907695660988\n",
      "Epoch 189/300\n",
      "Average training loss: 0.033375343539648585\n",
      "Average test loss: 0.006957510066529115\n",
      "Epoch 190/300\n",
      "Average training loss: 0.033399918337663016\n",
      "Average test loss: 0.007010585296485159\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03334747318757905\n",
      "Average test loss: 0.006908257517549726\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03337065485782093\n",
      "Average test loss: 0.0071153190545737745\n",
      "Epoch 193/300\n",
      "Average training loss: 0.033304964497685434\n",
      "Average test loss: 0.007095497007171313\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03338434208101696\n",
      "Average test loss: 0.00718220286981927\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03330066096451548\n",
      "Average test loss: 0.007340003978047106\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03327694236735503\n",
      "Average test loss: 0.007307077484826247\n",
      "Epoch 197/300\n",
      "Average training loss: 0.033303180774052935\n",
      "Average test loss: 0.007192038921846284\n",
      "Epoch 198/300\n",
      "Average training loss: 0.033300937882728046\n",
      "Average test loss: 0.007108333178692394\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03327950543993049\n",
      "Average test loss: 0.007201853706604904\n",
      "Epoch 200/300\n",
      "Average training loss: 0.033255668719609575\n",
      "Average test loss: 0.007860734704054064\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03324440326624446\n",
      "Average test loss: 0.007068723884721597\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03318357401755121\n",
      "Average test loss: 0.006975428832901849\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0333859683142768\n",
      "Average test loss: 0.007068073092649381\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03314137985805671\n",
      "Average test loss: 0.007173315710905526\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03319063063131438\n",
      "Average test loss: 0.007144336604409747\n",
      "Epoch 206/300\n",
      "Average training loss: 0.033131053374873265\n",
      "Average test loss: 0.006921436710904042\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03310528967115614\n",
      "Average test loss: 0.007379157688882616\n",
      "Epoch 208/300\n",
      "Average training loss: 0.033117742127842374\n",
      "Average test loss: 0.006955265266199906\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03311897724204593\n",
      "Average test loss: 0.006950712115814288\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03309207683139377\n",
      "Average test loss: 0.0071219699068201915\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03310300407144758\n",
      "Average test loss: 0.007044534458054436\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03309049369063642\n",
      "Average test loss: 0.0070181223294801184\n",
      "Epoch 213/300\n",
      "Average training loss: 0.033069189685914255\n",
      "Average test loss: 0.007409855646805631\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0330464289618863\n",
      "Average test loss: 0.007281726514299711\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03304291235076057\n",
      "Average test loss: 0.007280899113251103\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03305240974823634\n",
      "Average test loss: 0.00780397998003496\n",
      "Epoch 217/300\n",
      "Average training loss: 0.033006228374110325\n",
      "Average test loss: 0.0070513531780905195\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03300420177314017\n",
      "Average test loss: 0.007251039329088396\n",
      "Epoch 219/300\n",
      "Average training loss: 0.033001686258448494\n",
      "Average test loss: 0.0069598068011303745\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03305392717321714\n",
      "Average test loss: 0.007165833482311832\n",
      "Epoch 221/300\n",
      "Average training loss: 0.032984108123514384\n",
      "Average test loss: 0.007187697136153777\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0329181054631869\n",
      "Average test loss: 0.0075937466650373405\n",
      "Epoch 223/300\n",
      "Average training loss: 0.032933642513222165\n",
      "Average test loss: 0.00715649433599578\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03296847337153223\n",
      "Average test loss: 0.0071604550460146535\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03289201624194781\n",
      "Average test loss: 0.006995111513055033\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0329493062959777\n",
      "Average test loss: 0.007290740726722611\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0329608082804415\n",
      "Average test loss: 0.007393719152030018\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03290443664126926\n",
      "Average test loss: 0.007717824343591928\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03284669425090154\n",
      "Average test loss: 0.00700157131254673\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03282809916469786\n",
      "Average test loss: 0.007002312791844209\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03288939764433437\n",
      "Average test loss: 0.0072323474259012276\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03332356962561607\n",
      "Average test loss: 0.007060379814770487\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03284078416890568\n",
      "Average test loss: 0.0070720322637094395\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03281111905972163\n",
      "Average test loss: 0.007055092768536673\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03281477552321222\n",
      "Average test loss: 0.007189607864866654\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03283150463634067\n",
      "Average test loss: 0.006948960958255662\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03279113619526227\n",
      "Average test loss: 0.007289869975298643\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03287857498725255\n",
      "Average test loss: 0.007123073303037219\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03282581212454372\n",
      "Average test loss: 0.0072230045969287554\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03272202227512996\n",
      "Average test loss: 0.007169794362866216\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03274772370523877\n",
      "Average test loss: 0.007338731852670511\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03272816854053073\n",
      "Average test loss: 0.0072025295247634255\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03278430842194292\n",
      "Average test loss: 0.007599796006662978\n",
      "Epoch 244/300\n",
      "Average training loss: 0.032707132624255286\n",
      "Average test loss: 0.007069906435906887\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03271473190685113\n",
      "Average test loss: 0.007206108189705345\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03266845845017168\n",
      "Average test loss: 0.007078360906905598\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03277653940518697\n",
      "Average test loss: 0.007286286749773555\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03265666518939866\n",
      "Average test loss: 0.007228279470569558\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03276753235525555\n",
      "Average test loss: 0.007066960067798694\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0326625754237175\n",
      "Average test loss: 0.007241643458604813\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03268885737988684\n",
      "Average test loss: 0.0073653896450996395\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0327040772471163\n",
      "Average test loss: 0.007299835558566782\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03259766520063082\n",
      "Average test loss: 0.007091539319604635\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03263605332043436\n",
      "Average test loss: 0.0070099043763346145\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03260178993476762\n",
      "Average test loss: 0.007080255661987596\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03256982407304976\n",
      "Average test loss: 0.007055416547589832\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03263187184267574\n",
      "Average test loss: 0.007797747305283944\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03264600687060091\n",
      "Average test loss: 0.0075587985879845086\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03253343604008357\n",
      "Average test loss: 0.008111368724041515\n",
      "Epoch 260/300\n",
      "Average training loss: 0.032618677286638156\n",
      "Average test loss: 0.007129637415210406\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03260038032796648\n",
      "Average test loss: 0.0071008652270668085\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0326322153309981\n",
      "Average test loss: 0.007617716910938422\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03251432998975118\n",
      "Average test loss: 0.008156463130480714\n",
      "Epoch 264/300\n",
      "Average training loss: 0.032587497419781154\n",
      "Average test loss: 0.007355803160203828\n",
      "Epoch 265/300\n",
      "Average training loss: 0.032614623780051866\n",
      "Average test loss: 0.0072991792427168955\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03252367890874545\n",
      "Average test loss: 0.007251255587985118\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03249637523293495\n",
      "Average test loss: 0.007376852047940095\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0325134027964539\n",
      "Average test loss: 0.007173187693787946\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03256875436173545\n",
      "Average test loss: 0.007219808108276791\n",
      "Epoch 270/300\n",
      "Average training loss: 0.032478561017248365\n",
      "Average test loss: 0.007189952148745457\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03249871518545681\n",
      "Average test loss: 0.0187471871011787\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03249968706236945\n",
      "Average test loss: 0.0071793763956262005\n",
      "Epoch 273/300\n",
      "Average training loss: 0.032473451559742295\n",
      "Average test loss: 0.007278625960151355\n",
      "Epoch 274/300\n",
      "Average training loss: 0.032443595710727906\n",
      "Average test loss: 0.007164938508222501\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03247789678970973\n",
      "Average test loss: 0.007519942237271203\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03246592174304856\n",
      "Average test loss: 0.007278199706226588\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03242154851224687\n",
      "Average test loss: 0.014191448109017478\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03237452309661441\n",
      "Average test loss: 0.007139293164842658\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03247122421860695\n",
      "Average test loss: 0.007508193848033746\n",
      "Epoch 280/300\n",
      "Average training loss: 0.032394002662764654\n",
      "Average test loss: 0.007014467080434164\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03237723887297842\n",
      "Average test loss: 0.007123844648814864\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03237408448755741\n",
      "Average test loss: 0.00712800864295827\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03244363563259443\n",
      "Average test loss: 0.007084869795789321\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03237365858753522\n",
      "Average test loss: 0.007286016744044092\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03237314128213459\n",
      "Average test loss: 0.007125794385042456\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03234356427854962\n",
      "Average test loss: 0.00707309138485127\n",
      "Epoch 287/300\n",
      "Average training loss: 0.032378015745017266\n",
      "Average test loss: 0.0072099197676612275\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03245027972592248\n",
      "Average test loss: 0.007169867775506444\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03233875570529037\n",
      "Average test loss: 0.007294257163173622\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03228274484475454\n",
      "Average test loss: 0.007041993839873208\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03232381230758296\n",
      "Average test loss: 0.007322726500117117\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0324256746702724\n",
      "Average test loss: 0.007438869269771708\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0323571249710189\n",
      "Average test loss: 0.007346053209155798\n",
      "Epoch 294/300\n",
      "Average training loss: 0.032310022042857274\n",
      "Average test loss: 0.007457769226282835\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03241074864897463\n",
      "Average test loss: 0.007238177704728312\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03233509924345546\n",
      "Average test loss: 0.008163925964799193\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03227454023891025\n",
      "Average test loss: 0.007221845372269551\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0322747561832269\n",
      "Average test loss: 0.007177102401852608\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03220264396237003\n",
      "Average test loss: 0.007402887197418345\n",
      "Epoch 300/300\n",
      "Average training loss: 0.032308421515756185\n",
      "Average test loss: 0.007219865904914008\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1359286247226927\n",
      "Average test loss: 0.008223748726977242\n",
      "Epoch 2/300\n",
      "Average training loss: 0.052010491573148306\n",
      "Average test loss: 0.013941871939433945\n",
      "Epoch 3/300\n",
      "Average training loss: 0.046842667443884746\n",
      "Average test loss: 0.007245164023091396\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04351203003525734\n",
      "Average test loss: 0.006304327054156197\n",
      "Epoch 5/300\n",
      "Average training loss: 0.041125655876265634\n",
      "Average test loss: 0.006085352073527044\n",
      "Epoch 6/300\n",
      "Average training loss: 0.039321108020014234\n",
      "Average test loss: 0.008108857394920455\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03751441246271133\n",
      "Average test loss: 0.005913638518916236\n",
      "Epoch 8/300\n",
      "Average training loss: 0.036093516568342844\n",
      "Average test loss: 0.006008003376424312\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03530802734030618\n",
      "Average test loss: 0.005482172020193603\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03434592326813274\n",
      "Average test loss: 0.00556588742385308\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0335548549476597\n",
      "Average test loss: 0.005318265234430631\n",
      "Epoch 12/300\n",
      "Average training loss: 0.033031219747331406\n",
      "Average test loss: 0.005343879286199808\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03237352895902263\n",
      "Average test loss: 0.00505804859350125\n",
      "Epoch 14/300\n",
      "Average training loss: 0.032033494585090215\n",
      "Average test loss: 0.005098872951749298\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03159946115149392\n",
      "Average test loss: 0.005069340543407533\n",
      "Epoch 16/300\n",
      "Average training loss: 0.031182689396871462\n",
      "Average test loss: 0.004966451679459877\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030820569877823195\n",
      "Average test loss: 0.005053718615323305\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030486940304438274\n",
      "Average test loss: 0.004859233079685105\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03024965814418263\n",
      "Average test loss: 0.0052903595678508285\n",
      "Epoch 20/300\n",
      "Average training loss: 0.029914796077542834\n",
      "Average test loss: 0.004784692169891463\n",
      "Epoch 21/300\n",
      "Average training loss: 0.029800326397021612\n",
      "Average test loss: 0.004761688770933283\n",
      "Epoch 22/300\n",
      "Average training loss: 0.029549819133347935\n",
      "Average test loss: 0.004692134770668216\n",
      "Epoch 23/300\n",
      "Average training loss: 0.029274685659342343\n",
      "Average test loss: 0.0046565340184089215\n",
      "Epoch 24/300\n",
      "Average training loss: 0.029143434950047068\n",
      "Average test loss: 0.004663467686002453\n",
      "Epoch 25/300\n",
      "Average training loss: 0.028979601959387463\n",
      "Average test loss: 0.0046629893817007545\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02878098264336586\n",
      "Average test loss: 0.004653407061265574\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0286330354064703\n",
      "Average test loss: 0.0046116938400599695\n",
      "Epoch 28/300\n",
      "Average training loss: 0.028581063146392506\n",
      "Average test loss: 0.005377424636648761\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02835316217276785\n",
      "Average test loss: 0.004570698862688409\n",
      "Epoch 30/300\n",
      "Average training loss: 0.028303695905539723\n",
      "Average test loss: 0.004651360076334742\n",
      "Epoch 31/300\n",
      "Average training loss: 0.028212568690379462\n",
      "Average test loss: 0.004562047123702036\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02807262957261668\n",
      "Average test loss: 0.004554876039011611\n",
      "Epoch 33/300\n",
      "Average training loss: 0.027926207597057023\n",
      "Average test loss: 0.004559480703332358\n",
      "Epoch 34/300\n",
      "Average training loss: 0.027879041840632758\n",
      "Average test loss: 0.004769932813528511\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027795480004615254\n",
      "Average test loss: 0.004576081798722347\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02769297500120269\n",
      "Average test loss: 0.005025946321172847\n",
      "Epoch 37/300\n",
      "Average training loss: 0.027585450725422966\n",
      "Average test loss: 0.004445249117496941\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02754227497180303\n",
      "Average test loss: 0.004561775799012846\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0274435803062386\n",
      "Average test loss: 0.00449512551104029\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02738080491953426\n",
      "Average test loss: 0.004468997179634041\n",
      "Epoch 41/300\n",
      "Average training loss: 0.027361255120899944\n",
      "Average test loss: 0.004452564158373409\n",
      "Epoch 42/300\n",
      "Average training loss: 0.027316868162817424\n",
      "Average test loss: 0.004440285158654054\n",
      "Epoch 43/300\n",
      "Average training loss: 0.027196458432409497\n",
      "Average test loss: 0.004491897345003154\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02715703468521436\n",
      "Average test loss: 0.004609488794994023\n",
      "Epoch 45/300\n",
      "Average training loss: 0.027142882902589108\n",
      "Average test loss: 0.004482386783593231\n",
      "Epoch 46/300\n",
      "Average training loss: 0.027023092400696544\n",
      "Average test loss: 0.004385576599174075\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02703322677148713\n",
      "Average test loss: 0.0044375012473513685\n",
      "Epoch 48/300\n",
      "Average training loss: 0.026927759119206005\n",
      "Average test loss: 0.004436362767385112\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02689566762579812\n",
      "Average test loss: 0.0043746780736578835\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02687388294438521\n",
      "Average test loss: 0.004376814044598076\n",
      "Epoch 51/300\n",
      "Average training loss: 0.026781980994674895\n",
      "Average test loss: 0.004421264540818003\n",
      "Epoch 52/300\n",
      "Average training loss: 0.026756111097004678\n",
      "Average test loss: 0.004361320213311248\n",
      "Epoch 53/300\n",
      "Average training loss: 0.026718126765555805\n",
      "Average test loss: 0.0044011541580160456\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02669705077840222\n",
      "Average test loss: 0.004403521783856882\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02665772516363197\n",
      "Average test loss: 0.004488274304403199\n",
      "Epoch 56/300\n",
      "Average training loss: 0.026592074031631153\n",
      "Average test loss: 0.004452616521467765\n",
      "Epoch 57/300\n",
      "Average training loss: 0.026629398740000195\n",
      "Average test loss: 0.004456530106150442\n",
      "Epoch 58/300\n",
      "Average training loss: 0.026519691026873057\n",
      "Average test loss: 0.004388884179087149\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0265081149223778\n",
      "Average test loss: 0.004439275363046262\n",
      "Epoch 60/300\n",
      "Average training loss: 0.026447771908508406\n",
      "Average test loss: 0.00437859839739071\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02640905290345351\n",
      "Average test loss: 0.0044138988749020626\n",
      "Epoch 62/300\n",
      "Average training loss: 0.026576453788412942\n",
      "Average test loss: 0.004630674094375637\n",
      "Epoch 63/300\n",
      "Average training loss: 0.026377883834971323\n",
      "Average test loss: 0.004402135658181376\n",
      "Epoch 64/300\n",
      "Average training loss: 0.026348691701889037\n",
      "Average test loss: 0.004380543139245775\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02627641075021691\n",
      "Average test loss: 0.004398069622202052\n",
      "Epoch 66/300\n",
      "Average training loss: 0.026260320416755148\n",
      "Average test loss: 0.004351915097071065\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02622315529651112\n",
      "Average test loss: 0.004364058159291744\n",
      "Epoch 68/300\n",
      "Average training loss: 0.026208697661757468\n",
      "Average test loss: 0.0043964077259103455\n",
      "Epoch 69/300\n",
      "Average training loss: 0.026197592390908134\n",
      "Average test loss: 0.005465417885945903\n",
      "Epoch 70/300\n",
      "Average training loss: 0.026103962686326768\n",
      "Average test loss: 0.004468899737629626\n",
      "Epoch 71/300\n",
      "Average training loss: 0.026140883968936074\n",
      "Average test loss: 0.004386800107442671\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0261641130629513\n",
      "Average test loss: 0.004356061653130584\n",
      "Epoch 73/300\n",
      "Average training loss: 0.026044860046770837\n",
      "Average test loss: 0.0043809560156530805\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02602250165409512\n",
      "Average test loss: 0.004392256252881553\n",
      "Epoch 75/300\n",
      "Average training loss: 0.026022430241107942\n",
      "Average test loss: 0.004325703886234098\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02603471267885632\n",
      "Average test loss: 0.004484551755504476\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02597542209095425\n",
      "Average test loss: 0.004466549879560868\n",
      "Epoch 78/300\n",
      "Average training loss: 0.025924263359771833\n",
      "Average test loss: 0.004322390953699747\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02594218019809988\n",
      "Average test loss: 0.004341475418044461\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0259177365899086\n",
      "Average test loss: 0.004471136546797223\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02585605822669135\n",
      "Average test loss: 0.004439391984293858\n",
      "Epoch 82/300\n",
      "Average training loss: 0.02584237128165033\n",
      "Average test loss: 0.004423846893012524\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025867736635936632\n",
      "Average test loss: 0.00429738822620776\n",
      "Epoch 84/300\n",
      "Average training loss: 0.025762378435995843\n",
      "Average test loss: 0.004332496296614408\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02575431912806299\n",
      "Average test loss: 0.004404610998928547\n",
      "Epoch 86/300\n",
      "Average training loss: 0.025760528339280022\n",
      "Average test loss: 0.00433536155646046\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02573725561466482\n",
      "Average test loss: 0.00435725855620371\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025701616888244946\n",
      "Average test loss: 0.004371634527626965\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02568997647199366\n",
      "Average test loss: 0.004359629971285661\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02566132074263361\n",
      "Average test loss: 0.004550382291691171\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025661749778522387\n",
      "Average test loss: 0.00433207194134593\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02562303684320715\n",
      "Average test loss: 0.004317060611728165\n",
      "Epoch 93/300\n",
      "Average training loss: 0.025616757230626213\n",
      "Average test loss: 0.00438296574436956\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025611375860042043\n",
      "Average test loss: 0.004533823846321967\n",
      "Epoch 95/300\n",
      "Average training loss: 0.025544747402270637\n",
      "Average test loss: 0.0043388619335989155\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025553722790545887\n",
      "Average test loss: 0.004325925324319137\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02552473023533821\n",
      "Average test loss: 0.0043703154019183585\n",
      "Epoch 98/300\n",
      "Average training loss: 0.025507159431775412\n",
      "Average test loss: 0.004390008337381813\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0255296093026797\n",
      "Average test loss: 0.0043054248458809325\n",
      "Epoch 100/300\n",
      "Average training loss: 0.025455528115232787\n",
      "Average test loss: 0.004354853371365203\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0254591517481539\n",
      "Average test loss: 0.0043693005134248075\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02548040185537603\n",
      "Average test loss: 0.004320501839535104\n",
      "Epoch 103/300\n",
      "Average training loss: 0.025417526902423965\n",
      "Average test loss: 0.0043620377104315496\n",
      "Epoch 104/300\n",
      "Average training loss: 0.025399177779754003\n",
      "Average test loss: 0.004415956148256859\n",
      "Epoch 105/300\n",
      "Average training loss: 0.025414498042729165\n",
      "Average test loss: 0.004980295290963517\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02538025058640374\n",
      "Average test loss: 0.004495840056488912\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02533787792755498\n",
      "Average test loss: 0.004381632523404227\n",
      "Epoch 108/300\n",
      "Average training loss: 0.025359392017126083\n",
      "Average test loss: 0.0043126774802804\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02533009259733889\n",
      "Average test loss: 0.004325405007435216\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02526864442229271\n",
      "Average test loss: 0.004337825812813309\n",
      "Epoch 111/300\n",
      "Average training loss: 0.025310486745503215\n",
      "Average test loss: 0.0043889071063862905\n",
      "Epoch 112/300\n",
      "Average training loss: 0.025251474966605503\n",
      "Average test loss: 0.004533978758586778\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02528480636411243\n",
      "Average test loss: 0.005023710065003899\n",
      "Epoch 114/300\n",
      "Average training loss: 0.025247683660851585\n",
      "Average test loss: 0.004433975053744183\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02524885589712196\n",
      "Average test loss: 0.0043233634767433\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025250269855062166\n",
      "Average test loss: 0.004340652764671379\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02520659628179338\n",
      "Average test loss: 0.004420966190596422\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025140529591176245\n",
      "Average test loss: 0.004359797400318914\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02515297021965186\n",
      "Average test loss: 0.004520226043131617\n",
      "Epoch 120/300\n",
      "Average training loss: 0.025174605048365065\n",
      "Average test loss: 0.004321930706914928\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025153599244025018\n",
      "Average test loss: 0.004392890661333998\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02510300281478299\n",
      "Average test loss: 0.0044860732269783815\n",
      "Epoch 123/300\n",
      "Average training loss: 0.025134193395574888\n",
      "Average test loss: 0.004380916220446428\n",
      "Epoch 124/300\n",
      "Average training loss: 0.025089382285873096\n",
      "Average test loss: 0.004311522933344046\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025084200524621538\n",
      "Average test loss: 0.0044130573550032245\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0250699801908599\n",
      "Average test loss: 0.004422562163737085\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025056392525633176\n",
      "Average test loss: 0.005039810635977322\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025032817386918597\n",
      "Average test loss: 0.0044027215383119055\n",
      "Epoch 129/300\n",
      "Average training loss: 0.025009187587433392\n",
      "Average test loss: 0.004359545372840431\n",
      "Epoch 130/300\n",
      "Average training loss: 0.025042303512493768\n",
      "Average test loss: 0.004377443961385224\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02499144261247582\n",
      "Average test loss: 0.004364964965730906\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024996153675847583\n",
      "Average test loss: 0.004473002369619078\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02500955486463176\n",
      "Average test loss: 0.004464133461730348\n",
      "Epoch 134/300\n",
      "Average training loss: 0.024947744818197358\n",
      "Average test loss: 0.004353550632794698\n",
      "Epoch 135/300\n",
      "Average training loss: 0.024946975756022664\n",
      "Average test loss: 0.0046425499444206555\n",
      "Epoch 136/300\n",
      "Average training loss: 0.024939566703306305\n",
      "Average test loss: 0.004747588137785593\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02491434251434273\n",
      "Average test loss: 0.004565824010719856\n",
      "Epoch 138/300\n",
      "Average training loss: 0.024898468381828732\n",
      "Average test loss: 0.004363110513736805\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024895577538344597\n",
      "Average test loss: 0.004594718444678518\n",
      "Epoch 140/300\n",
      "Average training loss: 0.024895619314577844\n",
      "Average test loss: 0.004892058175885015\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024868784490558835\n",
      "Average test loss: 0.004420001775440242\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02489109064473046\n",
      "Average test loss: 0.008139069435497125\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024865015029907227\n",
      "Average test loss: 0.004361050201579928\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02480914463599523\n",
      "Average test loss: 0.0046125430659287505\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024795203588075108\n",
      "Average test loss: 0.004468063738197088\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024821345685256853\n",
      "Average test loss: 0.004368974109904633\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02481827433904012\n",
      "Average test loss: 0.004357279564771387\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02478922098212772\n",
      "Average test loss: 0.0044512662539879485\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02474973618818654\n",
      "Average test loss: 0.004480776958581474\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024753737954629793\n",
      "Average test loss: 0.004531224239203665\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02479322815934817\n",
      "Average test loss: 0.004375498945928282\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024738145652744504\n",
      "Average test loss: 0.004766484531884392\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024747249055239888\n",
      "Average test loss: 0.004460792469895549\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02471285918686125\n",
      "Average test loss: 0.004434316856579648\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024782972324225636\n",
      "Average test loss: 0.0044138663214527895\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02467151097125477\n",
      "Average test loss: 0.0045019486443036135\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024703020123971832\n",
      "Average test loss: 0.004423335815469424\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02464690086742242\n",
      "Average test loss: 0.004431622849570381\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02472038652499517\n",
      "Average test loss: 0.004333727034636669\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02466938407884704\n",
      "Average test loss: 0.004776717395832141\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02463688577711582\n",
      "Average test loss: 0.004416631355053849\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024607431355449887\n",
      "Average test loss: 0.004385889335225026\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02462067132525974\n",
      "Average test loss: 0.004419553866816891\n",
      "Epoch 164/300\n",
      "Average training loss: 0.024623221793108517\n",
      "Average test loss: 0.004371593329227633\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024634124626715978\n",
      "Average test loss: 0.005353942491114139\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02459828151596917\n",
      "Average test loss: 0.004489195673002137\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02459495179520713\n",
      "Average test loss: 0.0043749605640769\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02457417246533765\n",
      "Average test loss: 0.004422801527919041\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02457170681655407\n",
      "Average test loss: 0.004415782805946138\n",
      "Epoch 170/300\n",
      "Average training loss: 0.024564935619632404\n",
      "Average test loss: 0.004368339971949657\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02457913133667575\n",
      "Average test loss: 0.004357516212802794\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024524913281202317\n",
      "Average test loss: 0.0044887670535180306\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024550968420174388\n",
      "Average test loss: 0.004577666705681218\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02451152163412836\n",
      "Average test loss: 0.004795120272371504\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02453811132411162\n",
      "Average test loss: 0.004422534245583747\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024450014592872726\n",
      "Average test loss: 0.004449357292304436\n",
      "Epoch 177/300\n",
      "Average training loss: 0.024487682605783146\n",
      "Average test loss: 0.004424497495922777\n",
      "Epoch 178/300\n",
      "Average training loss: 0.024485058597392508\n",
      "Average test loss: 0.004415410096032752\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02446177276306682\n",
      "Average test loss: 0.004384717709074418\n",
      "Epoch 180/300\n",
      "Average training loss: 0.024456258464190694\n",
      "Average test loss: 0.00438142026319272\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024468731039100224\n",
      "Average test loss: 0.004443879834479756\n",
      "Epoch 182/300\n",
      "Average training loss: 0.024411158207390045\n",
      "Average test loss: 0.004376602926188045\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024423642466465632\n",
      "Average test loss: 0.004553983673867252\n",
      "Epoch 184/300\n",
      "Average training loss: 0.024419476656450164\n",
      "Average test loss: 0.0044324428186648425\n",
      "Epoch 185/300\n",
      "Average training loss: 0.024419873782330088\n",
      "Average test loss: 0.0045997350830584765\n",
      "Epoch 186/300\n",
      "Average training loss: 0.024445345472958353\n",
      "Average test loss: 0.004506428037666612\n",
      "Epoch 187/300\n",
      "Average training loss: 0.024369196471240785\n",
      "Average test loss: 0.004436644174158573\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02438709808058209\n",
      "Average test loss: 0.004674242289943828\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02436398078666793\n",
      "Average test loss: 0.00443804988782439\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02437972216308117\n",
      "Average test loss: 0.004393405134065284\n",
      "Epoch 191/300\n",
      "Average training loss: 0.024364781041940054\n",
      "Average test loss: 0.004563748457779487\n",
      "Epoch 192/300\n",
      "Average training loss: 0.024351944701539147\n",
      "Average test loss: 0.0044058957182698776\n",
      "Epoch 193/300\n",
      "Average training loss: 0.024331981827815374\n",
      "Average test loss: 0.004480911201486985\n",
      "Epoch 194/300\n",
      "Average training loss: 0.024369834553864267\n",
      "Average test loss: 0.0047019903701212675\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02432927778859933\n",
      "Average test loss: 0.004501166793828209\n",
      "Epoch 196/300\n",
      "Average training loss: 0.024329266296492683\n",
      "Average test loss: 0.004503922375539939\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02433254672918055\n",
      "Average test loss: 0.00461813695439034\n",
      "Epoch 198/300\n",
      "Average training loss: 0.024312655058172015\n",
      "Average test loss: 0.004929329046358665\n",
      "Epoch 199/300\n",
      "Average training loss: 0.024317801404330465\n",
      "Average test loss: 0.004413263234620293\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0242599383228355\n",
      "Average test loss: 0.004406311140913102\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02428661758535438\n",
      "Average test loss: 0.0043995088591343825\n",
      "Epoch 202/300\n",
      "Average training loss: 0.024260940725604693\n",
      "Average test loss: 0.00455714754636089\n",
      "Epoch 203/300\n",
      "Average training loss: 0.024265524448619947\n",
      "Average test loss: 0.004464639916188187\n",
      "Epoch 204/300\n",
      "Average training loss: 0.024250302284955977\n",
      "Average test loss: 0.0046755815686451065\n",
      "Epoch 205/300\n",
      "Average training loss: 0.024245002989967665\n",
      "Average test loss: 0.004471898825632202\n",
      "Epoch 206/300\n",
      "Average training loss: 0.024258872954381838\n",
      "Average test loss: 0.004401667332483663\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02421028753121694\n",
      "Average test loss: 0.004482248557731509\n",
      "Epoch 208/300\n",
      "Average training loss: 0.024210011020302773\n",
      "Average test loss: 0.07948962321546342\n",
      "Epoch 209/300\n",
      "Average training loss: 0.024213959448867374\n",
      "Average test loss: 0.004420521710895829\n",
      "Epoch 210/300\n",
      "Average training loss: 0.024204060304496025\n",
      "Average test loss: 0.0044599405599551065\n",
      "Epoch 211/300\n",
      "Average training loss: 0.024217456984851095\n",
      "Average test loss: 0.004384371916866965\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02417212621867657\n",
      "Average test loss: 0.004850006747163005\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02418268072605133\n",
      "Average test loss: 0.004437266289566954\n",
      "Epoch 214/300\n",
      "Average training loss: 0.024203887078497147\n",
      "Average test loss: 0.004428799852314922\n",
      "Epoch 215/300\n",
      "Average training loss: 0.024170346963736745\n",
      "Average test loss: 0.004706546077297794\n",
      "Epoch 216/300\n",
      "Average training loss: 0.024212770372629165\n",
      "Average test loss: 0.0045079795527789325\n",
      "Epoch 217/300\n",
      "Average training loss: 0.024158016302519375\n",
      "Average test loss: 0.004494528398331668\n",
      "Epoch 218/300\n",
      "Average training loss: 0.024140202910535865\n",
      "Average test loss: 0.004466912490212255\n",
      "Epoch 219/300\n",
      "Average training loss: 0.024130370982819133\n",
      "Average test loss: 0.004570998979525434\n",
      "Epoch 220/300\n",
      "Average training loss: 0.024132192277246053\n",
      "Average test loss: 0.004401249586708016\n",
      "Epoch 221/300\n",
      "Average training loss: 0.024148933519919712\n",
      "Average test loss: 0.004394809740699\n",
      "Epoch 222/300\n",
      "Average training loss: 0.024141547453072336\n",
      "Average test loss: 0.006596935868677166\n",
      "Epoch 223/300\n",
      "Average training loss: 0.024089796751737596\n",
      "Average test loss: 0.004515584922913048\n",
      "Epoch 224/300\n",
      "Average training loss: 0.024081731367442342\n",
      "Average test loss: 0.004484625047072768\n",
      "Epoch 225/300\n",
      "Average training loss: 0.024123502165079115\n",
      "Average test loss: 0.004415958778311809\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02408506751060486\n",
      "Average test loss: 0.004673444470183717\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02408242900504006\n",
      "Average test loss: 0.004442699894309044\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0240774423463477\n",
      "Average test loss: 0.004621963407844305\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02411630720893542\n",
      "Average test loss: 0.004430797895623578\n",
      "Epoch 230/300\n",
      "Average training loss: 0.024039063750041854\n",
      "Average test loss: 0.00442820053630405\n",
      "Epoch 231/300\n",
      "Average training loss: 0.024061968222260473\n",
      "Average test loss: 0.004437820987362001\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024047847756081157\n",
      "Average test loss: 0.004511466534601318\n",
      "Epoch 233/300\n",
      "Average training loss: 0.024067380828989876\n",
      "Average test loss: 0.00459323933472236\n",
      "Epoch 234/300\n",
      "Average training loss: 0.024041008608208764\n",
      "Average test loss: 0.004462746862736013\n",
      "Epoch 235/300\n",
      "Average training loss: 0.024016153501139746\n",
      "Average test loss: 0.004479815051373509\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02401162863439984\n",
      "Average test loss: 0.0046584667294389676\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02405143319070339\n",
      "Average test loss: 0.005301686756726768\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02398060327437189\n",
      "Average test loss: 0.004520690547923247\n",
      "Epoch 239/300\n",
      "Average training loss: 0.024000867878397305\n",
      "Average test loss: 0.004511528334683842\n",
      "Epoch 240/300\n",
      "Average training loss: 0.024052998213304415\n",
      "Average test loss: 0.004482521602056093\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02400347765452332\n",
      "Average test loss: 0.0047897949446406626\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02400671461555693\n",
      "Average test loss: 0.004485864750213093\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0239710453318225\n",
      "Average test loss: 0.004419341088996993\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02400014103121228\n",
      "Average test loss: 0.004517447430640459\n",
      "Epoch 245/300\n",
      "Average training loss: 0.023989436782068677\n",
      "Average test loss: 0.004521491604339746\n",
      "Epoch 246/300\n",
      "Average training loss: 0.023985355193416276\n",
      "Average test loss: 0.004430470640046729\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02394507681330045\n",
      "Average test loss: 0.004597383284734355\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023940584267179172\n",
      "Average test loss: 0.004534061454650428\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023971267097526127\n",
      "Average test loss: 0.0054724930057095155\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02396259961856736\n",
      "Average test loss: 0.004444107462962469\n",
      "Epoch 251/300\n",
      "Average training loss: 0.023939329422182506\n",
      "Average test loss: 0.004920776357460353\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023902497460444767\n",
      "Average test loss: 0.005431573837167687\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023900597769353125\n",
      "Average test loss: 0.004554623814506663\n",
      "Epoch 254/300\n",
      "Average training loss: 0.023925861080487568\n",
      "Average test loss: 0.0045112779562671975\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023890656759341558\n",
      "Average test loss: 0.0044778633208738435\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023908825053109064\n",
      "Average test loss: 0.004981694175965256\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023911917002664672\n",
      "Average test loss: 0.004457318575845824\n",
      "Epoch 258/300\n",
      "Average training loss: 0.023905185224281415\n",
      "Average test loss: 0.004515063533973363\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02386616967535681\n",
      "Average test loss: 0.004562698307136695\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023864455077383252\n",
      "Average test loss: 0.004474700894620684\n",
      "Epoch 261/300\n",
      "Average training loss: 0.023904234636988906\n",
      "Average test loss: 0.004818934959669908\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02385326407518652\n",
      "Average test loss: 0.004646837727477153\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02386993933551841\n",
      "Average test loss: 0.004580723730640279\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023849487198723686\n",
      "Average test loss: 0.00453244376141164\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023867971019612418\n",
      "Average test loss: 0.004456655314606097\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023874560097853343\n",
      "Average test loss: 0.00450886302234398\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023809149454037348\n",
      "Average test loss: 0.0046580624245107175\n",
      "Epoch 268/300\n",
      "Average training loss: 0.023837788459327484\n",
      "Average test loss: 0.004551361370417807\n",
      "Epoch 269/300\n",
      "Average training loss: 0.023813748507036103\n",
      "Average test loss: 0.004502741337857313\n",
      "Epoch 270/300\n",
      "Average training loss: 0.023805168964796595\n",
      "Average test loss: 0.004670685571514898\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02382613752451208\n",
      "Average test loss: 0.004622543271217082\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02381896091004213\n",
      "Average test loss: 0.00449375680502918\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02381730407476425\n",
      "Average test loss: 0.004548177401638693\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023769825708534982\n",
      "Average test loss: 0.004499190056903495\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023818592598040898\n",
      "Average test loss: 0.004481973933676878\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02377647394273016\n",
      "Average test loss: 0.004533990211577879\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02378751731581158\n",
      "Average test loss: 0.004474719476782614\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02376872009370062\n",
      "Average test loss: 0.004530992261237568\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023771782775719962\n",
      "Average test loss: 0.004540454003752934\n",
      "Epoch 280/300\n",
      "Average training loss: 0.023770324798093902\n",
      "Average test loss: 0.004606459128359954\n",
      "Epoch 281/300\n",
      "Average training loss: 0.023775573934117954\n",
      "Average test loss: 0.004633694641292095\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023765774227678775\n",
      "Average test loss: 0.004791281280832158\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02376102344857322\n",
      "Average test loss: 0.005628320144282447\n",
      "Epoch 284/300\n",
      "Average training loss: 0.023780032287041345\n",
      "Average test loss: 0.004500897756881184\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02373016090856658\n",
      "Average test loss: 0.004620862100687292\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02373882438739141\n",
      "Average test loss: 0.0045221714828577305\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02372141675816642\n",
      "Average test loss: 0.004768281770249208\n",
      "Epoch 288/300\n",
      "Average training loss: 0.023743988626533083\n",
      "Average test loss: 0.004487604726105928\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02372387964030107\n",
      "Average test loss: 0.00468798150950008\n",
      "Epoch 290/300\n",
      "Average training loss: 0.023741994304789436\n",
      "Average test loss: 0.004974515556875203\n",
      "Epoch 291/300\n",
      "Average training loss: 0.023723803501990107\n",
      "Average test loss: 0.004591412854691347\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02375351315571202\n",
      "Average test loss: 0.004632306724993719\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02370220541788472\n",
      "Average test loss: 0.0045080896539406645\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02369807447327508\n",
      "Average test loss: 0.00460147789079282\n",
      "Epoch 295/300\n",
      "Average training loss: 0.023690088273750413\n",
      "Average test loss: 0.0047573355978561774\n",
      "Epoch 296/300\n",
      "Average training loss: 0.023697417143318387\n",
      "Average test loss: 0.004560948230326176\n",
      "Epoch 297/300\n",
      "Average training loss: 0.023699058577418327\n",
      "Average test loss: 0.004751901184726092\n",
      "Epoch 298/300\n",
      "Average training loss: 0.023673213342825572\n",
      "Average test loss: 0.004532041054425968\n",
      "Epoch 299/300\n",
      "Average training loss: 0.023667745522326894\n",
      "Average test loss: 0.004586145276410712\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02369239924516943\n",
      "Average test loss: 0.004538181491403116\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1240801662504673\n",
      "Average test loss: 0.006781558137800958\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04428361406260067\n",
      "Average test loss: 0.005549002817935414\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03920659839775827\n",
      "Average test loss: 0.0051047541507416305\n",
      "Epoch 4/300\n",
      "Average training loss: 0.036389262068602775\n",
      "Average test loss: 0.0049871052474611335\n",
      "Epoch 5/300\n",
      "Average training loss: 0.034208502805895276\n",
      "Average test loss: 0.0049866588049464755\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0326490676469273\n",
      "Average test loss: 0.004509113946722614\n",
      "Epoch 7/300\n",
      "Average training loss: 0.031057876553800372\n",
      "Average test loss: 0.0044727278844349915\n",
      "Epoch 8/300\n",
      "Average training loss: 0.029877090172635186\n",
      "Average test loss: 0.0044102807583080396\n",
      "Epoch 9/300\n",
      "Average training loss: 0.029052544130219352\n",
      "Average test loss: 0.004462152451069818\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028155327356523936\n",
      "Average test loss: 0.004338504587403602\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027465069476101132\n",
      "Average test loss: 0.004332287388129367\n",
      "Epoch 12/300\n",
      "Average training loss: 0.026899507865309716\n",
      "Average test loss: 0.004097129594534636\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026392218922575313\n",
      "Average test loss: 0.00399859353651603\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02599275721775161\n",
      "Average test loss: 0.0037954071275889873\n",
      "Epoch 15/300\n",
      "Average training loss: 0.025550817504525185\n",
      "Average test loss: 0.004208735239588552\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02531121458941036\n",
      "Average test loss: 0.00397050481248233\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024880729311042363\n",
      "Average test loss: 0.003676072333421972\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024622880523403487\n",
      "Average test loss: 0.00370114197416438\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02437320819662677\n",
      "Average test loss: 0.0036921789550946817\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02415806508395407\n",
      "Average test loss: 0.003637962360348966\n",
      "Epoch 21/300\n",
      "Average training loss: 0.023968721816937127\n",
      "Average test loss: 0.003540699184561769\n",
      "Epoch 22/300\n",
      "Average training loss: 0.023801227466927633\n",
      "Average test loss: 0.003588403175274531\n",
      "Epoch 23/300\n",
      "Average training loss: 0.023669576557146178\n",
      "Average test loss: 0.0034694088072412545\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0234858794891172\n",
      "Average test loss: 0.003460531113255355\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02331453700694773\n",
      "Average test loss: 0.0034466743672059643\n",
      "Epoch 26/300\n",
      "Average training loss: 0.023222994736499258\n",
      "Average test loss: 0.0034352612035969893\n",
      "Epoch 27/300\n",
      "Average training loss: 0.023149464877115354\n",
      "Average test loss: 0.00344693328316013\n",
      "Epoch 28/300\n",
      "Average training loss: 0.023017098209924168\n",
      "Average test loss: 0.003392511671409011\n",
      "Epoch 29/300\n",
      "Average training loss: 0.022918450130356684\n",
      "Average test loss: 0.003541858885023329\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0228170984685421\n",
      "Average test loss: 0.0034282700485653347\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02272250044014719\n",
      "Average test loss: 0.0033803080239643655\n",
      "Epoch 32/300\n",
      "Average training loss: 0.022625512181884713\n",
      "Average test loss: 0.003445835126356946\n",
      "Epoch 33/300\n",
      "Average training loss: 0.022547567311260435\n",
      "Average test loss: 0.003357453493815329\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02248844596909152\n",
      "Average test loss: 0.003401843097888761\n",
      "Epoch 35/300\n",
      "Average training loss: 0.022402166426181792\n",
      "Average test loss: 0.003362435445189476\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02232385318643517\n",
      "Average test loss: 0.003335710964062148\n",
      "Epoch 37/300\n",
      "Average training loss: 0.022258970230817795\n",
      "Average test loss: 0.0033655584129608338\n",
      "Epoch 38/300\n",
      "Average training loss: 0.022220585324698023\n",
      "Average test loss: 0.0033003675581680402\n",
      "Epoch 39/300\n",
      "Average training loss: 0.022150241972671614\n",
      "Average test loss: 0.0032958976516707077\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02208923792011208\n",
      "Average test loss: 0.0032972742867552573\n",
      "Epoch 41/300\n",
      "Average training loss: 0.022071314801772435\n",
      "Average test loss: 0.0032678507512642276\n",
      "Epoch 42/300\n",
      "Average training loss: 0.021998750042584208\n",
      "Average test loss: 0.0037129508695668643\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02195536493261655\n",
      "Average test loss: 0.00326219557515449\n",
      "Epoch 44/300\n",
      "Average training loss: 0.021920446361104646\n",
      "Average test loss: 0.0034243511493421263\n",
      "Epoch 45/300\n",
      "Average training loss: 0.021865193234549628\n",
      "Average test loss: 0.003447966148869859\n",
      "Epoch 46/300\n",
      "Average training loss: 0.021820659491750927\n",
      "Average test loss: 0.003268005812747611\n",
      "Epoch 47/300\n",
      "Average training loss: 0.021778731870982383\n",
      "Average test loss: 0.003290122791296906\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02172195394668314\n",
      "Average test loss: 0.0032480961367901827\n",
      "Epoch 49/300\n",
      "Average training loss: 0.021693419001168675\n",
      "Average test loss: 0.0032474604304879905\n",
      "Epoch 50/300\n",
      "Average training loss: 0.021688371539115906\n",
      "Average test loss: 0.003367704497029384\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02164744809932179\n",
      "Average test loss: 0.0032624827396745483\n",
      "Epoch 52/300\n",
      "Average training loss: 0.021609195641345447\n",
      "Average test loss: 0.0032493959547330938\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021555009636614057\n",
      "Average test loss: 0.0032359388056728577\n",
      "Epoch 54/300\n",
      "Average training loss: 0.021509835484955046\n",
      "Average test loss: 0.003305226765366064\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021497619872291884\n",
      "Average test loss: 0.0032626714321474233\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021447292531530063\n",
      "Average test loss: 0.0032975708891948064\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021456673707399104\n",
      "Average test loss: 0.0036478246789839533\n",
      "Epoch 58/300\n",
      "Average training loss: 0.021367697523699866\n",
      "Average test loss: 0.0032227888049350846\n",
      "Epoch 59/300\n",
      "Average training loss: 0.021362717057267826\n",
      "Average test loss: 0.0032870581288718514\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021316640860504575\n",
      "Average test loss: 0.003336287185135815\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021318265455464523\n",
      "Average test loss: 0.0032162986751645803\n",
      "Epoch 62/300\n",
      "Average training loss: 0.021270852651860978\n",
      "Average test loss: 0.0032930268914335304\n",
      "Epoch 63/300\n",
      "Average training loss: 0.021264277678396965\n",
      "Average test loss: 0.0034347724376453293\n",
      "Epoch 64/300\n",
      "Average training loss: 0.021224459727605186\n",
      "Average test loss: 0.003252512741420004\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02119514714843697\n",
      "Average test loss: 0.0032178254485544233\n",
      "Epoch 66/300\n",
      "Average training loss: 0.021161555578311283\n",
      "Average test loss: 0.003244841140591436\n",
      "Epoch 67/300\n",
      "Average training loss: 0.021169139795833163\n",
      "Average test loss: 0.00324294980097976\n",
      "Epoch 68/300\n",
      "Average training loss: 0.021129367356499035\n",
      "Average test loss: 0.003292638805384437\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02110613115794129\n",
      "Average test loss: 0.0032534899388750394\n",
      "Epoch 70/300\n",
      "Average training loss: 0.021075600385665895\n",
      "Average test loss: 0.0032231157964302433\n",
      "Epoch 71/300\n",
      "Average training loss: 0.021077415163318316\n",
      "Average test loss: 0.0031975395565645563\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02105436853071054\n",
      "Average test loss: 0.0032418889419900047\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02100279268953535\n",
      "Average test loss: 0.003345779947315653\n",
      "Epoch 74/300\n",
      "Average training loss: 0.021015429506699244\n",
      "Average test loss: 0.003221388198849228\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020961900803777907\n",
      "Average test loss: 0.003220719903914465\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020936740752723483\n",
      "Average test loss: 0.003389205824583769\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02094989444481002\n",
      "Average test loss: 0.003440799604066544\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020903040296501585\n",
      "Average test loss: 0.0032587638567719195\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02090997840132978\n",
      "Average test loss: 0.0033023844791783227\n",
      "Epoch 80/300\n",
      "Average training loss: 0.020875936726729074\n",
      "Average test loss: 0.003277432326434387\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020868524842792086\n",
      "Average test loss: 0.003374534660950303\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020861894614166685\n",
      "Average test loss: 0.003217594113200903\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020803676942984264\n",
      "Average test loss: 0.0035210188728653723\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020808618307113648\n",
      "Average test loss: 0.0032283865890155236\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020797081894344753\n",
      "Average test loss: 0.00327685990722643\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020751228579216532\n",
      "Average test loss: 0.0034095212515029643\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0207525181737211\n",
      "Average test loss: 0.003225791679074367\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020739309249652758\n",
      "Average test loss: 0.0032054933077759213\n",
      "Epoch 89/300\n",
      "Average training loss: 0.020746347197228007\n",
      "Average test loss: 0.0033444057088345288\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02069227812687556\n",
      "Average test loss: 0.003403886317171984\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020653301235702304\n",
      "Average test loss: 0.003222750280259384\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02066425750984086\n",
      "Average test loss: 0.003199234231478638\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02065063436412149\n",
      "Average test loss: 0.0032124529704451563\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02065005328754584\n",
      "Average test loss: 0.0033267527932508124\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020640751999285487\n",
      "Average test loss: 0.003308946578659945\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020587957854072254\n",
      "Average test loss: 0.0031947325432880058\n",
      "Epoch 97/300\n",
      "Average training loss: 0.020585330102178785\n",
      "Average test loss: 0.003349825270473957\n",
      "Epoch 98/300\n",
      "Average training loss: 0.020579128083255557\n",
      "Average test loss: 0.003263612075812287\n",
      "Epoch 99/300\n",
      "Average training loss: 0.020551671193705663\n",
      "Average test loss: 0.003204209233737654\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02056392081744141\n",
      "Average test loss: 0.0032144725469665397\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02053916379312674\n",
      "Average test loss: 0.0034191657875974973\n",
      "Epoch 102/300\n",
      "Average training loss: 0.020553086754348544\n",
      "Average test loss: 0.0034635247219767834\n",
      "Epoch 103/300\n",
      "Average training loss: 0.020490171442429223\n",
      "Average test loss: 0.0032980404790076945\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0204787786487076\n",
      "Average test loss: 0.0032560503646317457\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0205024793661303\n",
      "Average test loss: 0.0032207146467020113\n",
      "Epoch 106/300\n",
      "Average training loss: 0.020478135792745485\n",
      "Average test loss: 0.0032765759324861897\n",
      "Epoch 107/300\n",
      "Average training loss: 0.020461298518710667\n",
      "Average test loss: 0.003210136261251238\n",
      "Epoch 108/300\n",
      "Average training loss: 0.020448497080140644\n",
      "Average test loss: 0.0032121022558874554\n",
      "Epoch 109/300\n",
      "Average training loss: 0.020416538952125442\n",
      "Average test loss: 0.003366360253137019\n",
      "Epoch 110/300\n",
      "Average training loss: 0.020453362685110835\n",
      "Average test loss: 0.0032374385469075705\n",
      "Epoch 111/300\n",
      "Average training loss: 0.020367817805873024\n",
      "Average test loss: 0.003239874384883377\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02037081924908691\n",
      "Average test loss: 0.0032524157340327897\n",
      "Epoch 113/300\n",
      "Average training loss: 0.020393835816118453\n",
      "Average test loss: 0.0036332141438292134\n",
      "Epoch 114/300\n",
      "Average training loss: 0.020359820200337303\n",
      "Average test loss: 0.004681556499666638\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020349955514073374\n",
      "Average test loss: 0.0033858497124165297\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02033520930343204\n",
      "Average test loss: 0.003204258388736182\n",
      "Epoch 117/300\n",
      "Average training loss: 0.020334959492087364\n",
      "Average test loss: 0.0032843476119968624\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02031641685962677\n",
      "Average test loss: 0.0032766269472324187\n",
      "Epoch 119/300\n",
      "Average training loss: 0.020334671965903706\n",
      "Average test loss: 0.003252740619911088\n",
      "Epoch 120/300\n",
      "Average training loss: 0.020285082375837697\n",
      "Average test loss: 0.0032153501564429867\n",
      "Epoch 121/300\n",
      "Average training loss: 0.020279180703891647\n",
      "Average test loss: 0.003244898803739084\n",
      "Epoch 122/300\n",
      "Average training loss: 0.020248403526842595\n",
      "Average test loss: 0.003317380105041795\n",
      "Epoch 123/300\n",
      "Average training loss: 0.020261002775695588\n",
      "Average test loss: 0.0032745289160973495\n",
      "Epoch 124/300\n",
      "Average training loss: 0.020239795294072892\n",
      "Average test loss: 0.003191671965436803\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02022020086314943\n",
      "Average test loss: 0.0032094950799105896\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02023719527489609\n",
      "Average test loss: 0.003196045806424485\n",
      "Epoch 127/300\n",
      "Average training loss: 0.020210289779636594\n",
      "Average test loss: 0.0031941323603192967\n",
      "Epoch 128/300\n",
      "Average training loss: 0.020223667265640365\n",
      "Average test loss: 0.003248031341367298\n",
      "Epoch 129/300\n",
      "Average training loss: 0.020288404039210742\n",
      "Average test loss: 0.00330142501120766\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02017265750798914\n",
      "Average test loss: 0.003960677528339955\n",
      "Epoch 131/300\n",
      "Average training loss: 0.020164800941944123\n",
      "Average test loss: 0.003326782680220074\n",
      "Epoch 132/300\n",
      "Average training loss: 0.020179934781458644\n",
      "Average test loss: 0.0032120796570347413\n",
      "Epoch 133/300\n",
      "Average training loss: 0.020147584120432535\n",
      "Average test loss: 0.003572535586853822\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02015083646443155\n",
      "Average test loss: 0.003197052538394928\n",
      "Epoch 135/300\n",
      "Average training loss: 0.020121641063027912\n",
      "Average test loss: 0.003253289083018899\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02014545779261324\n",
      "Average test loss: 0.0032642363003558585\n",
      "Epoch 137/300\n",
      "Average training loss: 0.020124613857931562\n",
      "Average test loss: 0.0032049016795224613\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020097868821687168\n",
      "Average test loss: 0.003233958494125141\n",
      "Epoch 139/300\n",
      "Average training loss: 0.020097595024440022\n",
      "Average test loss: 0.003244759682152006\n",
      "Epoch 140/300\n",
      "Average training loss: 0.020097423806786537\n",
      "Average test loss: 0.0032362750242981644\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02005677360130681\n",
      "Average test loss: 0.0032496567976971467\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02006904726392693\n",
      "Average test loss: 0.0032303847722295258\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02004583974348174\n",
      "Average test loss: 0.0032652671918686894\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020040690806176927\n",
      "Average test loss: 0.0032388611384150056\n",
      "Epoch 145/300\n",
      "Average training loss: 0.020044090603788693\n",
      "Average test loss: 0.0035149489384558464\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0200362738304668\n",
      "Average test loss: 0.0033028308500846226\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02004572693672445\n",
      "Average test loss: 0.0032104688400609624\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02000304003059864\n",
      "Average test loss: 0.0032788418485886523\n",
      "Epoch 149/300\n",
      "Average training loss: 0.020035438070694606\n",
      "Average test loss: 0.0035390718724164698\n",
      "Epoch 150/300\n",
      "Average training loss: 0.019979178898864323\n",
      "Average test loss: 0.0032442313687254987\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01998629205673933\n",
      "Average test loss: 0.003272856555879116\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01998810342616505\n",
      "Average test loss: 0.003236186175710625\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01995564196507136\n",
      "Average test loss: 0.0033048756851090325\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019982433829042648\n",
      "Average test loss: 0.003296063025378519\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01995147646135754\n",
      "Average test loss: 0.0032446040051678816\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019951271070374383\n",
      "Average test loss: 0.003205182812487086\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019917833565009965\n",
      "Average test loss: 0.0033226198040776783\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019913903996348382\n",
      "Average test loss: 0.0032601761654433275\n",
      "Epoch 159/300\n",
      "Average training loss: 0.019948684392703905\n",
      "Average test loss: 0.0032428003632360035\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019921666295992003\n",
      "Average test loss: 0.0032472942494269873\n",
      "Epoch 161/300\n",
      "Average training loss: 0.019918035500579412\n",
      "Average test loss: 0.0032084557430611714\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01988718244433403\n",
      "Average test loss: 0.0032150803473260667\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019865651016434033\n",
      "Average test loss: 0.07229621872802576\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01989851853582594\n",
      "Average test loss: 0.003210557549364037\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019909201939900718\n",
      "Average test loss: 0.003243829673156142\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019856586264239418\n",
      "Average test loss: 0.0032660096215291158\n",
      "Epoch 167/300\n",
      "Average training loss: 0.019830382911695376\n",
      "Average test loss: 0.0032413795271681416\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019840797742207844\n",
      "Average test loss: 0.0032881413888600137\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01982494054072433\n",
      "Average test loss: 0.0032940728780296116\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01983952274090714\n",
      "Average test loss: 0.0036197075593388745\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01982846805287732\n",
      "Average test loss: 0.003210864442711075\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019828263820873368\n",
      "Average test loss: 0.003262868206534121\n",
      "Epoch 173/300\n",
      "Average training loss: 0.019803221085005335\n",
      "Average test loss: 0.0032376594295104345\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019824470836255284\n",
      "Average test loss: 0.0032868792230470313\n",
      "Epoch 175/300\n",
      "Average training loss: 0.019792940421236888\n",
      "Average test loss: 0.003294940790782372\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01977170248826345\n",
      "Average test loss: 0.0032324132695794104\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01977252768807941\n",
      "Average test loss: 0.0033483656253665687\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019767127578457198\n",
      "Average test loss: 0.0032563241575327183\n",
      "Epoch 179/300\n",
      "Average training loss: 0.019776040848758485\n",
      "Average test loss: 0.0032598818457788893\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01976789580119981\n",
      "Average test loss: 0.003245512053370476\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01974861093196604\n",
      "Average test loss: 0.003506442594445414\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019754336716400252\n",
      "Average test loss: 0.0032579513096974958\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01972703224917253\n",
      "Average test loss: 0.0032706537076996433\n",
      "Epoch 184/300\n",
      "Average training loss: 0.019730724753604996\n",
      "Average test loss: 0.00323165918700397\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01971504441400369\n",
      "Average test loss: 0.0033695139677988157\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01971793667640951\n",
      "Average test loss: 0.003480074886646536\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019698121204972267\n",
      "Average test loss: 0.0032214241998477114\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01970643105937375\n",
      "Average test loss: 0.0034979454887410005\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019682952588631047\n",
      "Average test loss: 0.003282915832888749\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019695260364148353\n",
      "Average test loss: 0.0032479328345507382\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01968175056576729\n",
      "Average test loss: 0.0034040491154624358\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01968820063604249\n",
      "Average test loss: 0.0033451508809294967\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019668420761823655\n",
      "Average test loss: 0.0032386146562380924\n",
      "Epoch 194/300\n",
      "Average training loss: 0.019670161109831597\n",
      "Average test loss: 0.0032875859981609714\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01967540410326587\n",
      "Average test loss: 0.0034600348880307543\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019660321327547234\n",
      "Average test loss: 0.0032878984262545903\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01965177488161458\n",
      "Average test loss: 0.003279930866426892\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01963598626686467\n",
      "Average test loss: 0.003280727291272746\n",
      "Epoch 199/300\n",
      "Average training loss: 0.019636391076776716\n",
      "Average test loss: 0.003628509648144245\n",
      "Epoch 200/300\n",
      "Average training loss: 0.019626973981658618\n",
      "Average test loss: 0.0032557950009488397\n",
      "Epoch 201/300\n",
      "Average training loss: 0.019623070059551134\n",
      "Average test loss: 0.0033105982701397605\n",
      "Epoch 202/300\n",
      "Average training loss: 0.019618234160873625\n",
      "Average test loss: 0.0032974654665837684\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019621241519848506\n",
      "Average test loss: 0.005951421696692705\n",
      "Epoch 204/300\n",
      "Average training loss: 0.019609122551149792\n",
      "Average test loss: 0.0033898926886419454\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01963268358508746\n",
      "Average test loss: 0.0032500473556833134\n",
      "Epoch 206/300\n",
      "Average training loss: 0.019580785527825356\n",
      "Average test loss: 0.003269175133150485\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01958313767446412\n",
      "Average test loss: 0.0033512036154667536\n",
      "Epoch 208/300\n",
      "Average training loss: 0.019584416093097794\n",
      "Average test loss: 0.003295600098040369\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01955893122487598\n",
      "Average test loss: 0.003295217339363363\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019561013771428003\n",
      "Average test loss: 0.0032676278532793126\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019552896480593416\n",
      "Average test loss: 0.0034087980155729585\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01955127742720975\n",
      "Average test loss: 0.0032504463444153467\n",
      "Epoch 213/300\n",
      "Average training loss: 0.019555641432603202\n",
      "Average test loss: 0.003261954872144593\n",
      "Epoch 214/300\n",
      "Average training loss: 0.019538059542576473\n",
      "Average test loss: 0.0033345307041373518\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019532831248309877\n",
      "Average test loss: 0.0032411389365378353\n",
      "Epoch 216/300\n",
      "Average training loss: 0.019540579270985393\n",
      "Average test loss: 0.003333686406620675\n",
      "Epoch 217/300\n",
      "Average training loss: 0.019540618158049052\n",
      "Average test loss: 0.0032908007899920145\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019504053306248454\n",
      "Average test loss: 0.0033511242703017263\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01948878473126226\n",
      "Average test loss: 0.0033239677420092953\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01951947048637602\n",
      "Average test loss: 0.003296179259609845\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019502580839726658\n",
      "Average test loss: 0.003545889687620931\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019512672470675573\n",
      "Average test loss: 0.0033427140265703203\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019486187605394258\n",
      "Average test loss: 0.00331255038206776\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019490934424102307\n",
      "Average test loss: 0.0033081741382678348\n",
      "Epoch 225/300\n",
      "Average training loss: 0.019467496575580702\n",
      "Average test loss: 0.0033132938510841794\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019478083691663213\n",
      "Average test loss: 0.0033403610386368303\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01948633307715257\n",
      "Average test loss: 0.003260763838266333\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019480431753728124\n",
      "Average test loss: 0.0033086192876928384\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01944902463340097\n",
      "Average test loss: 0.003464296175787846\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019465684001644452\n",
      "Average test loss: 0.0032761409133672714\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019431150204605525\n",
      "Average test loss: 0.003255757809927066\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01945020101633337\n",
      "Average test loss: 0.003329002954893642\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019427087057795788\n",
      "Average test loss: 0.003294736832794216\n",
      "Epoch 234/300\n",
      "Average training loss: 0.019438867509365083\n",
      "Average test loss: 0.0033016677596088913\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01942840938270092\n",
      "Average test loss: 0.0033165695874227416\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019427660912275314\n",
      "Average test loss: 0.0033073202446103098\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019406622050537004\n",
      "Average test loss: 0.0033380975040296712\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019433470540576512\n",
      "Average test loss: 0.0036731110022713742\n",
      "Epoch 239/300\n",
      "Average training loss: 0.019414910362826453\n",
      "Average test loss: 0.003477626057755616\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01939317881812652\n",
      "Average test loss: 0.0033189805038273336\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01940504908727275\n",
      "Average test loss: 0.003262625645225247\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019387467717130978\n",
      "Average test loss: 0.003311359862693482\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01939824475016859\n",
      "Average test loss: 0.003374877140339878\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019383566233846877\n",
      "Average test loss: 0.0033629484199401405\n",
      "Epoch 245/300\n",
      "Average training loss: 0.019386424207025105\n",
      "Average test loss: 0.0033109417628082965\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01938519684225321\n",
      "Average test loss: 0.003297004234459665\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01936093411180708\n",
      "Average test loss: 0.003466651611857944\n",
      "Epoch 248/300\n",
      "Average training loss: 0.019361704154147043\n",
      "Average test loss: 0.003364704061506523\n",
      "Epoch 249/300\n",
      "Average training loss: 0.019344061738914915\n",
      "Average test loss: 0.003274636385548446\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019371871766116884\n",
      "Average test loss: 0.003418084147696694\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0193627403659953\n",
      "Average test loss: 0.0034731503178675968\n",
      "Epoch 252/300\n",
      "Average training loss: 0.019349747346507178\n",
      "Average test loss: 0.003300633901109298\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01933054173323843\n",
      "Average test loss: 0.0033143152145461902\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019350232445531423\n",
      "Average test loss: 0.003284763265815046\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019338134739134046\n",
      "Average test loss: 0.0033240130580961705\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019336432200339107\n",
      "Average test loss: 0.003305168892360396\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019326676812436844\n",
      "Average test loss: 0.003326769329400526\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01932336079908742\n",
      "Average test loss: 0.0033788825095527704\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019319897141721512\n",
      "Average test loss: 0.0033528120385275945\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019309527640541394\n",
      "Average test loss: 0.003316840572696593\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01931552287273937\n",
      "Average test loss: 0.003289098612550232\n",
      "Epoch 262/300\n",
      "Average training loss: 0.019303707634409268\n",
      "Average test loss: 0.003331001169565651\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01929054932627413\n",
      "Average test loss: 0.0033063357472419737\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01928535602490107\n",
      "Average test loss: 0.003285785284307268\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01929532715678215\n",
      "Average test loss: 0.0033061923357761568\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01930037593510416\n",
      "Average test loss: 0.003340852576204472\n",
      "Epoch 267/300\n",
      "Average training loss: 0.019275018006563185\n",
      "Average test loss: 0.003345560359251168\n",
      "Epoch 268/300\n",
      "Average training loss: 0.019269004705879424\n",
      "Average test loss: 0.00334088799978296\n",
      "Epoch 269/300\n",
      "Average training loss: 0.019266597034202682\n",
      "Average test loss: 0.0033127722622205815\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019280015099379752\n",
      "Average test loss: 0.003307506715464923\n",
      "Epoch 271/300\n",
      "Average training loss: 0.019278009151419005\n",
      "Average test loss: 0.0033129765867359108\n",
      "Epoch 272/300\n",
      "Average training loss: 0.019256189761062463\n",
      "Average test loss: 0.003352127183435692\n",
      "Epoch 273/300\n",
      "Average training loss: 0.019244950102435217\n",
      "Average test loss: 0.0034570628835095298\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019272220245665973\n",
      "Average test loss: 0.004103973321202729\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019247385698888037\n",
      "Average test loss: 0.0033618020566387307\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01921759058203962\n",
      "Average test loss: 0.0033277214080509212\n",
      "Epoch 277/300\n",
      "Average training loss: 0.019235525007049243\n",
      "Average test loss: 0.0033638482507732177\n",
      "Epoch 278/300\n",
      "Average training loss: 0.019246396113600996\n",
      "Average test loss: 0.003346743256474535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.019220739415122402\n",
      "Average test loss: 0.0033162537720054387\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019220080978340572\n",
      "Average test loss: 0.003312867595296767\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01922784450319078\n",
      "Average test loss: 0.0033116326820519237\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019232896629307005\n",
      "Average test loss: 0.0033958823800914815\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01921624470088217\n",
      "Average test loss: 0.003356045677844021\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01921164372066657\n",
      "Average test loss: 0.0033306363322254686\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01919189274807771\n",
      "Average test loss: 0.0033832711863021054\n",
      "Epoch 286/300\n",
      "Average training loss: 0.019207942017250592\n",
      "Average test loss: 0.00335808368668788\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019207494735717773\n",
      "Average test loss: 0.003332030525431037\n",
      "Epoch 288/300\n",
      "Average training loss: 0.019191533929771847\n",
      "Average test loss: 0.003315543945050902\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019204547168480024\n",
      "Average test loss: 0.0033158891203088894\n",
      "Epoch 290/300\n",
      "Average training loss: 0.019198994187845125\n",
      "Average test loss: 0.00335195959918201\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01918848674827152\n",
      "Average test loss: 0.003300619984252585\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01916013452079561\n",
      "Average test loss: 0.003379939129576087\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019170978385541174\n",
      "Average test loss: 0.0033156182559000123\n",
      "Epoch 294/300\n",
      "Average training loss: 0.019156877734594874\n",
      "Average test loss: 0.003375832911580801\n",
      "Epoch 295/300\n",
      "Average training loss: 0.019180448182755046\n",
      "Average test loss: 0.0033430568842838207\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019166251738866172\n",
      "Average test loss: 0.0033129324834379886\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019163899698191218\n",
      "Average test loss: 0.0038260166322191557\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019155635802282227\n",
      "Average test loss: 0.003386855557974842\n",
      "Epoch 299/300\n",
      "Average training loss: 0.019165487340754933\n",
      "Average test loss: 0.0037464830862979095\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01914420064124796\n",
      "Average test loss: 0.0033071584012359383\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11188045350048277\n",
      "Average test loss: 0.005538227260940605\n",
      "Epoch 2/300\n",
      "Average training loss: 0.038483576870626875\n",
      "Average test loss: 0.004732249768657817\n",
      "Epoch 3/300\n",
      "Average training loss: 0.033637054582436876\n",
      "Average test loss: 0.004860489829132954\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03110094130701489\n",
      "Average test loss: 0.004267391887803873\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02946340346336365\n",
      "Average test loss: 0.0037545610225449004\n",
      "Epoch 6/300\n",
      "Average training loss: 0.027674127587013774\n",
      "Average test loss: 0.0038707782212230894\n",
      "Epoch 7/300\n",
      "Average training loss: 0.026604095069898498\n",
      "Average test loss: 0.003752702212582032\n",
      "Epoch 8/300\n",
      "Average training loss: 0.025366245658861267\n",
      "Average test loss: 0.0034946539795233144\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02449584810435772\n",
      "Average test loss: 0.003494442960454358\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02376403582427237\n",
      "Average test loss: 0.003300684201427632\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02320065181950728\n",
      "Average test loss: 0.0032775984708633688\n",
      "Epoch 12/300\n",
      "Average training loss: 0.022678228593534893\n",
      "Average test loss: 0.003255692071384854\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02227765277855926\n",
      "Average test loss: 0.003043006061679787\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02193704706430435\n",
      "Average test loss: 0.003269744327912728\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021577452083428702\n",
      "Average test loss: 0.003242569333149327\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021280847248103883\n",
      "Average test loss: 0.002979201019431154\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021049409573276837\n",
      "Average test loss: 0.0029174253172758553\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020749536251028378\n",
      "Average test loss: 0.002928996829729941\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02059315519200431\n",
      "Average test loss: 0.002983789529237482\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020429198627670606\n",
      "Average test loss: 0.0027899764916963048\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02024565978679392\n",
      "Average test loss: 0.0028970841616392136\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02013365736603737\n",
      "Average test loss: 0.0027979135937574837\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01997670066853364\n",
      "Average test loss: 0.0027834242640270125\n",
      "Epoch 24/300\n",
      "Average training loss: 0.019868522023161254\n",
      "Average test loss: 0.002858563651434249\n",
      "Epoch 25/300\n",
      "Average training loss: 0.019750630519456334\n",
      "Average test loss: 0.0027641297872695657\n",
      "Epoch 26/300\n",
      "Average training loss: 0.019645773457156287\n",
      "Average test loss: 0.0027842631774644055\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01950670666164822\n",
      "Average test loss: 0.002728035907157593\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019475868339339893\n",
      "Average test loss: 0.0027779159198204678\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019352415824929872\n",
      "Average test loss: 0.0026960555352270603\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019269750386476516\n",
      "Average test loss: 0.0026985914264288213\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019214265303479302\n",
      "Average test loss: 0.0026582870210210482\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019116402177347076\n",
      "Average test loss: 0.002651613896091779\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019075649953550763\n",
      "Average test loss: 0.0026818196374095147\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019043815435634718\n",
      "Average test loss: 0.0026923203884313502\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018972602668735718\n",
      "Average test loss: 0.0026526647421220937\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018880646435750856\n",
      "Average test loss: 0.0027278844273338717\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0188468198734853\n",
      "Average test loss: 0.0026282815860791337\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018768365172876254\n",
      "Average test loss: 0.0026621057291825612\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018752398330304357\n",
      "Average test loss: 0.002635797616508272\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018722489575544992\n",
      "Average test loss: 0.0026132720990313424\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01864322379728158\n",
      "Average test loss: 0.002592019282488359\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018620339029365114\n",
      "Average test loss: 0.0026030740731706223\n",
      "Epoch 43/300\n",
      "Average training loss: 0.018596841409802438\n",
      "Average test loss: 0.0026206382889714507\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018535951198803052\n",
      "Average test loss: 0.0026165533726000122\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018546190864510006\n",
      "Average test loss: 0.003711941654069556\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01848413283377886\n",
      "Average test loss: 0.002613851844229632\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01840731939756208\n",
      "Average test loss: 0.002601170690316293\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01838746300008562\n",
      "Average test loss: 0.0025954907563411527\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01835464741786321\n",
      "Average test loss: 0.002602308542157213\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01835363881289959\n",
      "Average test loss: 0.0025946205167306793\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018324193719360563\n",
      "Average test loss: 0.0026033482342544528\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01825744748281108\n",
      "Average test loss: 0.0026166885720772877\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01826356224550141\n",
      "Average test loss: 0.0025686268169018956\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018217567286557622\n",
      "Average test loss: 0.0026844722469233804\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018204373333189222\n",
      "Average test loss: 0.0026355157604234084\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018155997458431456\n",
      "Average test loss: 0.0027441366877820755\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018148955177929665\n",
      "Average test loss: 0.0025787149591164457\n",
      "Epoch 58/300\n",
      "Average training loss: 0.018100102552109296\n",
      "Average test loss: 0.0025756832487467264\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01810271054506302\n",
      "Average test loss: 0.0025909158565724888\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01806600202785598\n",
      "Average test loss: 0.002596832178533077\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018040055721998214\n",
      "Average test loss: 0.002602163973574837\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018030946666995683\n",
      "Average test loss: 0.00264832751515011\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017996696909268698\n",
      "Average test loss: 0.002689622470798592\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017966998082068232\n",
      "Average test loss: 0.0026608814334289897\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017944615345862176\n",
      "Average test loss: 0.002553702034884029\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017951393299632603\n",
      "Average test loss: 0.0025592194646596907\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017927206400367948\n",
      "Average test loss: 0.0027742850962612364\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0179018926339017\n",
      "Average test loss: 0.0026890826928946707\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017881599268979496\n",
      "Average test loss: 0.0025676424203233588\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017862288208471404\n",
      "Average test loss: 0.002574000025375022\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017856218053234948\n",
      "Average test loss: 0.0025876433627886903\n",
      "Epoch 72/300\n",
      "Average training loss: 0.017803691614833143\n",
      "Average test loss: 0.00264809926909705\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01780939975546466\n",
      "Average test loss: 0.0025793726212448545\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01780592651002937\n",
      "Average test loss: 0.002590867701400485\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017755789935588837\n",
      "Average test loss: 0.0031146955461137825\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017761095143026776\n",
      "Average test loss: 0.0025446434741218885\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017741945810616017\n",
      "Average test loss: 0.002558373600244522\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017726978028813997\n",
      "Average test loss: 0.002901429136801097\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017699226644304063\n",
      "Average test loss: 0.0026138332163294156\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017695081322557395\n",
      "Average test loss: 0.0025742546156462694\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01766204580333498\n",
      "Average test loss: 0.0026052445729987488\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017657209982474643\n",
      "Average test loss: 0.002554979041425718\n",
      "Epoch 83/300\n",
      "Average training loss: 0.017621717954675358\n",
      "Average test loss: 0.002567411431214876\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017645615852541395\n",
      "Average test loss: 0.002802487308780352\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017617276339067352\n",
      "Average test loss: 0.00254503106760482\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017592118513253\n",
      "Average test loss: 0.002536970323158635\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01757539083643092\n",
      "Average test loss: 0.002623004866143068\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017567507955763076\n",
      "Average test loss: 0.0025687627910325923\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017555751393238703\n",
      "Average test loss: 0.002587973509811693\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01754124406725168\n",
      "Average test loss: 0.003197801633634501\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017535687352220216\n",
      "Average test loss: 0.0025543909606834253\n",
      "Epoch 92/300\n",
      "Average training loss: 0.017498396159046227\n",
      "Average test loss: 0.002569821616841687\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01749854079882304\n",
      "Average test loss: 0.0025734038191537064\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01748059555225902\n",
      "Average test loss: 0.0025504258084628317\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017478905304438536\n",
      "Average test loss: 0.0026101570857895743\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01746149596075217\n",
      "Average test loss: 0.0025645235046330424\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017447626289394166\n",
      "Average test loss: 0.002570236682270964\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01743707701563835\n",
      "Average test loss: 0.0025621538870036604\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01741160768436061\n",
      "Average test loss: 0.002945832680703865\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017398958287305304\n",
      "Average test loss: 0.0025619593972547186\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017400467928912904\n",
      "Average test loss: 0.0025664150809041326\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017392244444953072\n",
      "Average test loss: 0.0025377006888803507\n",
      "Epoch 103/300\n",
      "Average training loss: 0.017370395556092263\n",
      "Average test loss: 0.002537300006382995\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01736084278507365\n",
      "Average test loss: 0.002573711494397786\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017355746015906334\n",
      "Average test loss: 0.0025314657907519076\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017342334704266653\n",
      "Average test loss: 0.002574562472394771\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017326486865679423\n",
      "Average test loss: 0.0026468236104895673\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0173289385802216\n",
      "Average test loss: 0.002538009774353769\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01729621450106303\n",
      "Average test loss: 0.002543358650472429\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01728082379947106\n",
      "Average test loss: 0.011973659864316383\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017287987755404577\n",
      "Average test loss: 0.002685887248772714\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017276882307396994\n",
      "Average test loss: 0.002573764883292218\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017278333062099086\n",
      "Average test loss: 0.0025365341123607422\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017251861464646127\n",
      "Average test loss: 0.0025877512720310024\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01723069131705496\n",
      "Average test loss: 0.002625967328540153\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017246420449680753\n",
      "Average test loss: 0.0025640268847346307\n",
      "Epoch 117/300\n",
      "Average training loss: 0.017222148986326322\n",
      "Average test loss: 0.0025823596444808773\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01720870549066199\n",
      "Average test loss: 0.002561365629442864\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017198537530170546\n",
      "Average test loss: 0.0025712816363407504\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017192461339963806\n",
      "Average test loss: 0.0025599998438523876\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0172026516298453\n",
      "Average test loss: 0.0025405101935482687\n",
      "Epoch 122/300\n",
      "Average training loss: 0.017176061787539058\n",
      "Average test loss: 0.010969126719981431\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017176964268916184\n",
      "Average test loss: 0.002828441458857722\n",
      "Epoch 124/300\n",
      "Average training loss: 0.017155978868405024\n",
      "Average test loss: 0.002549821454824673\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017143522842062844\n",
      "Average test loss: 0.002627945379043619\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017125881367259555\n",
      "Average test loss: 0.00278662123117182\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017132280347247918\n",
      "Average test loss: 0.0025412571138391893\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017107334355513256\n",
      "Average test loss: 0.002547178735749589\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017100285490353903\n",
      "Average test loss: 0.0028419494316395785\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01710321926408344\n",
      "Average test loss: 0.0025567756628410683\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017072070240974426\n",
      "Average test loss: 0.002538230201850335\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01709476455549399\n",
      "Average test loss: 0.0025689721486220757\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017045399410857094\n",
      "Average test loss: 0.0027028730395767422\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01706131113651726\n",
      "Average test loss: 0.002600537391172515\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01706246469500992\n",
      "Average test loss: 0.0027861179372088776\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017030985364483463\n",
      "Average test loss: 0.00258355447753436\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017042854403456053\n",
      "Average test loss: 0.0025736253443691464\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017015245429343646\n",
      "Average test loss: 0.002565426736035281\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017012618540061846\n",
      "Average test loss: 0.0025928127318620682\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017024803303182127\n",
      "Average test loss: 0.0025699526394406956\n",
      "Epoch 141/300\n",
      "Average training loss: 0.017017688405182627\n",
      "Average test loss: 0.002578718143618769\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01700006334980329\n",
      "Average test loss: 0.0025979258173869715\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016965801656246185\n",
      "Average test loss: 0.002581688143312931\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016993797330392732\n",
      "Average test loss: 0.002746132684457633\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01697211333281464\n",
      "Average test loss: 0.0025541145863632363\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016965360849267908\n",
      "Average test loss: 0.002573467679735687\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016953216190139453\n",
      "Average test loss: 0.0025521999754839473\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01694949435856607\n",
      "Average test loss: 0.002691176781637801\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016929139903850025\n",
      "Average test loss: 0.002595134427977933\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016934525653719903\n",
      "Average test loss: 0.0025680050073812405\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01690624238219526\n",
      "Average test loss: 0.0026276088708804715\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016928575437929895\n",
      "Average test loss: 0.0025566848561995557\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016900628770391148\n",
      "Average test loss: 0.0025711192339658738\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016888344544503425\n",
      "Average test loss: 0.0025493239261623886\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01689794261753559\n",
      "Average test loss: 0.0026190579730189507\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016882006959782706\n",
      "Average test loss: 0.0025571159443093672\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01688287560062276\n",
      "Average test loss: 0.002580612522446447\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01686370314657688\n",
      "Average test loss: 0.0025854462540398043\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016869936931464408\n",
      "Average test loss: 0.002564731472896205\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016840595631135834\n",
      "Average test loss: 0.0025709426568614113\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016863225291172665\n",
      "Average test loss: 0.0025766638887839183\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01682696354554759\n",
      "Average test loss: 0.0025562415468609995\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016823468486467996\n",
      "Average test loss: 0.0025857939887791873\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016849450058407253\n",
      "Average test loss: 0.0025604542845653164\n",
      "Epoch 165/300\n",
      "Average training loss: 0.016804504601491823\n",
      "Average test loss: 0.0025958236137198078\n",
      "Epoch 166/300\n",
      "Average training loss: 0.016818989309999678\n",
      "Average test loss: 0.002563202350710829\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01681629086037477\n",
      "Average test loss: 0.0025537217379444175\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01680516037510501\n",
      "Average test loss: 0.0026003099742035072\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016806204410062895\n",
      "Average test loss: 0.002726552773370511\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01678242012196117\n",
      "Average test loss: 0.0027486994763215385\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01678153536054823\n",
      "Average test loss: 0.002581174943389164\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0167692013349798\n",
      "Average test loss: 0.0025769660433547364\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016769558363490634\n",
      "Average test loss: 0.0025561001640227107\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01676024527102709\n",
      "Average test loss: 0.0025545947804219192\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016762645370430415\n",
      "Average test loss: 0.0025983094626830685\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01676411432193385\n",
      "Average test loss: 0.0025706376654820308\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0167434630708562\n",
      "Average test loss: 0.002659695902839303\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01673146783808867\n",
      "Average test loss: 0.002598984913693534\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01673522247870763\n",
      "Average test loss: 0.0025672707425223457\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01673008859323131\n",
      "Average test loss: 0.002564422926141156\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016719454258680343\n",
      "Average test loss: 0.0025537611190229654\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016706835238469973\n",
      "Average test loss: 0.002701709577606784\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01669563749101427\n",
      "Average test loss: 0.002564256428844399\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01670414885216289\n",
      "Average test loss: 0.0026384889775266248\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01667568507956134\n",
      "Average test loss: 0.003838310420927074\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016705491249759993\n",
      "Average test loss: 0.002591638324575292\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0166844270295567\n",
      "Average test loss: 0.0025796152491950326\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016665656563308505\n",
      "Average test loss: 0.0028320178374027214\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01667405044204659\n",
      "Average test loss: 0.0026065118689503936\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01664608434173796\n",
      "Average test loss: 0.002662363407926427\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01665647970388333\n",
      "Average test loss: 0.002575784198525879\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01663703788154655\n",
      "Average test loss: 0.0026944379885163574\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016656321639815966\n",
      "Average test loss: 0.002566619130058421\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016649976255165205\n",
      "Average test loss: 0.002647552203387022\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01662626352243953\n",
      "Average test loss: 0.0025871731688578924\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01660619855754905\n",
      "Average test loss: 0.002586875647616883\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016619111811121304\n",
      "Average test loss: 0.002589350696860088\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0166181730694241\n",
      "Average test loss: 0.002615826076724463\n",
      "Epoch 199/300\n",
      "Average training loss: 0.016607554407583342\n",
      "Average test loss: 1.5604115749994913\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01671755135473278\n",
      "Average test loss: 0.002607294852224489\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0165934852908055\n",
      "Average test loss: 0.0026493804308896263\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01657548518644439\n",
      "Average test loss: 0.002568153272486395\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01660325183057123\n",
      "Average test loss: 0.00259278419903583\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01659543713596132\n",
      "Average test loss: 0.002618020467253195\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016567017310195498\n",
      "Average test loss: 0.002575839151731796\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01658708919833104\n",
      "Average test loss: 0.0026289872193915975\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016589183163311746\n",
      "Average test loss: 0.0026096708027438984\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016560820519924163\n",
      "Average test loss: 0.0029286341412613787\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016569234165880416\n",
      "Average test loss: 0.0025818621396190592\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01654112845990393\n",
      "Average test loss: 0.0027276072133746413\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01654144086771541\n",
      "Average test loss: 0.0026430298077563443\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01653735020591153\n",
      "Average test loss: 0.0026470544721103376\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01657233522666825\n",
      "Average test loss: 0.0025741067365225817\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016533890480796495\n",
      "Average test loss: 0.0028511754067407713\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01654298055006398\n",
      "Average test loss: 0.0027646223776456383\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0165352713531918\n",
      "Average test loss: 0.0025996410857058233\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01651381286813153\n",
      "Average test loss: 0.0025880818006893\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016504047410355674\n",
      "Average test loss: 0.0026944460334877175\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01651910698496633\n",
      "Average test loss: 0.0025928235647992956\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016499170579016207\n",
      "Average test loss: 0.0026107063568714594\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016516423790819114\n",
      "Average test loss: 0.002598854779990183\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016497664217319755\n",
      "Average test loss: 0.002591610391934713\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016489813163876533\n",
      "Average test loss: 0.002679931442356772\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01650794025593334\n",
      "Average test loss: 0.0026588578211764495\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016482225721081097\n",
      "Average test loss: 0.0025993947924839124\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01648602143757873\n",
      "Average test loss: 0.002649778068065643\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0164695298232966\n",
      "Average test loss: 0.0027122782092127537\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01649163009888596\n",
      "Average test loss: 0.002714027385123902\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016465373797549143\n",
      "Average test loss: 0.0025816529776073165\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01645971951716476\n",
      "Average test loss: 0.0027357608848768804\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016450237689746752\n",
      "Average test loss: 0.0025790353363586795\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016446658882002037\n",
      "Average test loss: 0.002604789782729414\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016463672642906508\n",
      "Average test loss: 0.0026073896808342803\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016450815881292026\n",
      "Average test loss: 0.00258339571290546\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01646265433728695\n",
      "Average test loss: 0.0026550590242776606\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016430117598838275\n",
      "Average test loss: 0.002651827089074585\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016431938926378887\n",
      "Average test loss: 0.0029022607459790175\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016426808569166396\n",
      "Average test loss: 0.002592260384311279\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016424028527405526\n",
      "Average test loss: 0.002604000614852541\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01641691479086876\n",
      "Average test loss: 0.002592595073911879\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016412529308762816\n",
      "Average test loss: 0.002644354230413834\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01640355331202348\n",
      "Average test loss: 0.0029893245024399627\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016410932146840626\n",
      "Average test loss: 0.002591674444369144\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01639397101600965\n",
      "Average test loss: 0.002611523780143923\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016382040339211625\n",
      "Average test loss: 0.002606447788990206\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016407028693291876\n",
      "Average test loss: 0.0027045696722343563\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016375225674774913\n",
      "Average test loss: 0.002611082969647315\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016384160114659204\n",
      "Average test loss: 0.0026202441259390776\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01639027287562688\n",
      "Average test loss: 0.0026304534581593343\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01636985491381751\n",
      "Average test loss: 0.002696876365277502\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016374623474147584\n",
      "Average test loss: 0.002594859316945076\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01638373701605532\n",
      "Average test loss: 0.002590504558239546\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01636206315788958\n",
      "Average test loss: 0.0026562555531660717\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01637544498178694\n",
      "Average test loss: 0.0026281112956090106\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01635657061967585\n",
      "Average test loss: 0.0025982322063710954\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016344835821125243\n",
      "Average test loss: 0.002624146905934645\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016357461044357882\n",
      "Average test loss: 0.0027065980076375934\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01633311660008298\n",
      "Average test loss: 0.002599408039616214\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016348909293611845\n",
      "Average test loss: 0.002620623011348976\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016331162500712607\n",
      "Average test loss: 0.0026066635027527808\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016319290967451203\n",
      "Average test loss: 0.002614135766815808\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016315218433737753\n",
      "Average test loss: 0.002598148610856798\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01633641716175609\n",
      "Average test loss: 0.002603533704040779\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016327460206217236\n",
      "Average test loss: 0.0026051093474444417\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016305077892210747\n",
      "Average test loss: 0.002622359670077761\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016316863793465825\n",
      "Average test loss: 0.0025962203128470315\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016315350062317317\n",
      "Average test loss: 0.002632572884360949\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016299258577326933\n",
      "Average test loss: 0.002616169763728976\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016293850645422936\n",
      "Average test loss: 0.0026019002181581325\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016294988526238334\n",
      "Average test loss: 0.002641075966983206\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01630597277896272\n",
      "Average test loss: 0.002603842387182845\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01627018045551247\n",
      "Average test loss: 0.002636260787645976\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016308785299460095\n",
      "Average test loss: 0.0026097936160448523\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01627807344827387\n",
      "Average test loss: 0.0030521900728344917\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01629050227999687\n",
      "Average test loss: 0.0025986500196158884\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01628813057641188\n",
      "Average test loss: 0.005440992864055766\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016280417408380245\n",
      "Average test loss: 0.0027630291405237382\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016265544234050643\n",
      "Average test loss: 0.0029861815557297735\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01626354820198483\n",
      "Average test loss: 0.002609382048042284\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016249283682968883\n",
      "Average test loss: 0.0026078704117486876\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01627026960088147\n",
      "Average test loss: 0.0034795398672835694\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016247115860382716\n",
      "Average test loss: 0.0029523452818393707\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01626746274365319\n",
      "Average test loss: 0.0030790221864978474\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01625381124847465\n",
      "Average test loss: 0.0026134755478964913\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016230095302065213\n",
      "Average test loss: 0.0026959056657635504\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01623744807475143\n",
      "Average test loss: 0.0025868875057333046\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016232879117131234\n",
      "Average test loss: 0.002710776039916608\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016281226110127237\n",
      "Average test loss: 0.0026450364378591378\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01621377140449153\n",
      "Average test loss: 0.0025914043730331793\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016216388658516937\n",
      "Average test loss: 0.002630675877134005\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016224529176950456\n",
      "Average test loss: 0.0025925990293423334\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0162045565214422\n",
      "Average test loss: 0.002785406766252385\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016230154754387008\n",
      "Average test loss: 0.0026239388657526837\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016216426329480275\n",
      "Average test loss: 0.0026325962968791524\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016218228254053328\n",
      "Average test loss: 0.002601475741300318\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01620994616051515\n",
      "Average test loss: 0.0026101522292527888\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016195376882950464\n",
      "Average test loss: 0.0026556690190401343\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016220593303442003\n",
      "Average test loss: 0.0026148374024778606\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01619536850353082\n",
      "Average test loss: 0.002665749369810025\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0161988364789221\n",
      "Average test loss: 0.002583764602533645\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.38\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.99\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.51\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.92\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.14\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.48\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.07\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.92\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.76\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.46\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.841318406952752\n",
      "Average test loss: 0.013515510789222187\n",
      "Epoch 2/300\n",
      "Average training loss: 0.6282312652269999\n",
      "Average test loss: 0.011755104781024987\n",
      "Epoch 3/300\n",
      "Average training loss: 0.42737956953048706\n",
      "Average test loss: 0.009394768654472299\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3385455581479602\n",
      "Average test loss: 0.008911108407295412\n",
      "Epoch 5/300\n",
      "Average training loss: 0.28525476124551563\n",
      "Average test loss: 0.008993442826800877\n",
      "Epoch 6/300\n",
      "Average training loss: 0.25116359305381775\n",
      "Average test loss: 0.008727741523749298\n",
      "Epoch 7/300\n",
      "Average training loss: 0.22738877329561447\n",
      "Average test loss: 0.00919193244063192\n",
      "Epoch 8/300\n",
      "Average training loss: 0.20662408632702298\n",
      "Average test loss: 0.0083265376297964\n",
      "Epoch 9/300\n",
      "Average training loss: 0.19541678338580662\n",
      "Average test loss: 0.007830656131936444\n",
      "Epoch 10/300\n",
      "Average training loss: 0.18414813339710234\n",
      "Average test loss: 0.007729164155821006\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17381893916924795\n",
      "Average test loss: 0.007760156130211221\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1654204896953371\n",
      "Average test loss: 0.008027991262988912\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15970870648490057\n",
      "Average test loss: 0.008891576146086056\n",
      "Epoch 14/300\n",
      "Average training loss: 0.15657250589794583\n",
      "Average test loss: 0.0077164140741030375\n",
      "Epoch 15/300\n",
      "Average training loss: 0.15067283246252272\n",
      "Average test loss: 0.00828066304905547\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1453357110288408\n",
      "Average test loss: 0.0076011825597120655\n",
      "Epoch 17/300\n",
      "Average training loss: 0.14272648270924887\n",
      "Average test loss: 0.0074836897452672324\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13817845822705163\n",
      "Average test loss: 0.10652424946096209\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13423302028576534\n",
      "Average test loss: 0.006999455192436775\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13133089978165097\n",
      "Average test loss: 0.006929912435511748\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12805220248301824\n",
      "Average test loss: 0.006964056604438358\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12596413923634422\n",
      "Average test loss: 0.0070255344067182805\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12401827893654506\n",
      "Average test loss: 0.007408267522851626\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12181501213709513\n",
      "Average test loss: 0.0070256605082088044\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12006962037086487\n",
      "Average test loss: 0.006591297723352909\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1190913388994005\n",
      "Average test loss: 0.0066844926364719864\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11701913621690538\n",
      "Average test loss: 0.006767006508178181\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11598058672746023\n",
      "Average test loss: 0.00657643282910188\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11466290637519624\n",
      "Average test loss: 0.006583368609762854\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11368642997741699\n",
      "Average test loss: 0.007321649981455671\n",
      "Epoch 31/300\n",
      "Average training loss: 0.11247113818592495\n",
      "Average test loss: 0.006569296287165747\n",
      "Epoch 32/300\n",
      "Average training loss: 0.11180980751249525\n",
      "Average test loss: 0.009226575166814858\n",
      "Epoch 33/300\n",
      "Average training loss: 0.11068737479050954\n",
      "Average test loss: 0.0067315178161693945\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11002266031503677\n",
      "Average test loss: 0.006604981640560759\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10905217318402396\n",
      "Average test loss: 0.007061907649868065\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10851900853713353\n",
      "Average test loss: 0.00639049731940031\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10773291016287274\n",
      "Average test loss: 0.007849433607525295\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10704300690359539\n",
      "Average test loss: 0.006502019423163599\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1065648534960217\n",
      "Average test loss: 0.006420677898658646\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10625678000185225\n",
      "Average test loss: 0.006790081756810347\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10689136220349206\n",
      "Average test loss: 0.006436845507472753\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10495687063535054\n",
      "Average test loss: 0.00635181607471572\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10435967371198866\n",
      "Average test loss: 0.006549863897264004\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10380916045440568\n",
      "Average test loss: 0.007123368556300799\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1032715248465538\n",
      "Average test loss: 0.006331066503706906\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10279422699080573\n",
      "Average test loss: 0.006526352041297489\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10238784679439333\n",
      "Average test loss: 0.006328873006420003\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10237794572777219\n",
      "Average test loss: 0.00714526603039768\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10169589180416531\n",
      "Average test loss: 0.0064157844401068155\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10123344612783856\n",
      "Average test loss: 0.006677808549669054\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10100944959455066\n",
      "Average test loss: 0.006578158094237248\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10047521714369456\n",
      "Average test loss: 0.006403505734685394\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11605718549092611\n",
      "Average test loss: 0.006663821483237876\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10212059103780323\n",
      "Average test loss: 0.006362368249230915\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10029744744963116\n",
      "Average test loss: 0.006434915827380286\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09975718763801787\n",
      "Average test loss: 0.04366128278026978\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09965843376186159\n",
      "Average test loss: 0.007335228922466437\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09911850808726416\n",
      "Average test loss: 0.006494372363719675\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09897027830945121\n",
      "Average test loss: 0.006364593480196264\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09869156344731649\n",
      "Average test loss: 0.01010180716175172\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09880468094348907\n",
      "Average test loss: 0.006401111316349771\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09804525706503127\n",
      "Average test loss: 0.006476242746743891\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09806238882409202\n",
      "Average test loss: 0.015252529566072755\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09766924093829261\n",
      "Average test loss: 0.008470863470600712\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09783638927671645\n",
      "Average test loss: 0.006343861054215166\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09724251433213552\n",
      "Average test loss: 0.007647544322742356\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09702659596999487\n",
      "Average test loss: 0.00652402530113856\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09694211277696821\n",
      "Average test loss: 0.007483057456298007\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09637413042783737\n",
      "Average test loss: 0.006416704550799396\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09623397221167883\n",
      "Average test loss: 0.006390467432638009\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09632104152441025\n",
      "Average test loss: 0.006395956080820825\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09630326201518377\n",
      "Average test loss: 0.006508571327974399\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09581575353940328\n",
      "Average test loss: 0.006354280226346519\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09560407451126311\n",
      "Average test loss: 0.006440456284830968\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09580785172515445\n",
      "Average test loss: 0.006605111480587058\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09487108203437593\n",
      "Average test loss: 0.00644516808539629\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09492711061901517\n",
      "Average test loss: 0.010429768114454216\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09471420168214374\n",
      "Average test loss: 0.006351722090194622\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09447700527641509\n",
      "Average test loss: 0.006361777606937621\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09409315519862704\n",
      "Average test loss: 0.006552556802829106\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09395360236697727\n",
      "Average test loss: 0.00643583685449428\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09431064343452454\n",
      "Average test loss: 0.022163066551089287\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09350216796663072\n",
      "Average test loss: 0.007161854968716701\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09372819126314587\n",
      "Average test loss: 0.006458968644340833\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0931877278553115\n",
      "Average test loss: 0.006419820178714063\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09316178649001651\n",
      "Average test loss: 0.006826927146150006\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09319432532787322\n",
      "Average test loss: 0.006852082113424937\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0927142271399498\n",
      "Average test loss: 0.0068774154641562035\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0925885135696994\n",
      "Average test loss: 0.0065299680083990095\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09297718308369318\n",
      "Average test loss: 0.04304793373743693\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09249148243665695\n",
      "Average test loss: 0.006379716037876076\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09258594315912988\n",
      "Average test loss: 0.006732810849116908\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09206950554582807\n",
      "Average test loss: 0.0070306078729530175\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09172055248419443\n",
      "Average test loss: 0.007009545936352677\n",
      "Epoch 95/300\n",
      "Average training loss: 0.09201590785053042\n",
      "Average test loss: 0.006387154118054443\n",
      "Epoch 96/300\n",
      "Average training loss: 0.09148431503110462\n",
      "Average test loss: 0.0067675348702404235\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09132689536942376\n",
      "Average test loss: 0.006686047381824918\n",
      "Epoch 98/300\n",
      "Average training loss: 0.09112331861257553\n",
      "Average test loss: 0.026379641630583338\n",
      "Epoch 99/300\n",
      "Average training loss: 0.09115581983327865\n",
      "Average test loss: 0.006437898338668876\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0909519732495149\n",
      "Average test loss: 0.006459635463439756\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0911343237294091\n",
      "Average test loss: 0.006627269352475802\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09063725237051645\n",
      "Average test loss: 0.006720267367031839\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09033821015225517\n",
      "Average test loss: 0.09559299726453092\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0910757846236229\n",
      "Average test loss: 0.008904688551608059\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0902104671796163\n",
      "Average test loss: 0.007752308614552021\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09007170502344768\n",
      "Average test loss: 0.0065511524275773106\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09007775302065743\n",
      "Average test loss: 0.0065756905865338115\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08967949171861013\n",
      "Average test loss: 0.006708064600825309\n",
      "Epoch 109/300\n",
      "Average training loss: 0.08975151585539182\n",
      "Average test loss: 0.00655084793559379\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0896743631164233\n",
      "Average test loss: 0.006623267944902181\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09079530090755887\n",
      "Average test loss: 0.007093830687304338\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08929636285040114\n",
      "Average test loss: 0.008990663356251186\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08874128456248177\n",
      "Average test loss: 0.006545404868407382\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08941600391599867\n",
      "Average test loss: 0.0066830884412758884\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08866945578654607\n",
      "Average test loss: 0.007032281991094351\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08872463281287088\n",
      "Average test loss: 0.006636712070968416\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08884344928794437\n",
      "Average test loss: 0.006508735155065854\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08854857841465208\n",
      "Average test loss: 0.006617576775865422\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08870043977101644\n",
      "Average test loss: 0.007038762451046043\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0881654420958625\n",
      "Average test loss: 0.009145477352870836\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08840617309676277\n",
      "Average test loss: 0.02952114175260067\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08905943060583539\n",
      "Average test loss: 0.006535297916167312\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08829813987016678\n",
      "Average test loss: 0.0066943425883849465\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0876450464129448\n",
      "Average test loss: 0.008502758445011244\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08786775127384398\n",
      "Average test loss: 0.0066167673464450575\n",
      "Epoch 126/300\n",
      "Average training loss: 0.087608265042305\n",
      "Average test loss: 0.0067614910755720405\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08756815099053912\n",
      "Average test loss: 0.00654167327657342\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08748331173923281\n",
      "Average test loss: 0.00858359474192063\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0873737922443284\n",
      "Average test loss: 0.010268751210636563\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08736326547463735\n",
      "Average test loss: 0.006822187993261549\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08709554410643047\n",
      "Average test loss: 0.006837623571356138\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08692039377159543\n",
      "Average test loss: 9.027766335129737\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08687253966927529\n",
      "Average test loss: 0.006684098513589965\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08687491054005093\n",
      "Average test loss: 0.006847503940678305\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08670861241552565\n",
      "Average test loss: 0.006805372960865498\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08657585552334786\n",
      "Average test loss: 0.0067552542487780256\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08648507531483968\n",
      "Average test loss: 0.006591434906340308\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08672201681137084\n",
      "Average test loss: 0.00672377111390233\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08636489486032062\n",
      "Average test loss: 0.0065845074711574445\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08630513419045342\n",
      "Average test loss: 0.006622738673869107\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08621202943722407\n",
      "Average test loss: 0.006651532179779477\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0860336421529452\n",
      "Average test loss: 0.006781644051687585\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08634290701813167\n",
      "Average test loss: 0.006526739962812927\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08592066976096895\n",
      "Average test loss: 0.006981518867943022\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0900098719365067\n",
      "Average test loss: 0.006711239932725827\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0854732425014178\n",
      "Average test loss: 0.006625147714383072\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08522448108593624\n",
      "Average test loss: 0.006806611091726356\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08553590091731814\n",
      "Average test loss: 0.006973288796014256\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08544185965591007\n",
      "Average test loss: 0.0066631006511549155\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08561610845724742\n",
      "Average test loss: 0.007088093304799663\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08568059541781743\n",
      "Average test loss: 0.0067709969538781375\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08510066258907319\n",
      "Average test loss: 0.0067261369286311995\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0850971888701121\n",
      "Average test loss: 0.006769760593358013\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08620161675744586\n",
      "Average test loss: 0.006813847880396578\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08471898826625612\n",
      "Average test loss: 0.0066593414958980345\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08508358923594157\n",
      "Average test loss: 0.0069029272596041365\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08459138580825594\n",
      "Average test loss: 0.006694434877898958\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08467433255248599\n",
      "Average test loss: 0.006767231130351623\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08474144323004616\n",
      "Average test loss: 0.006734813189340962\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08484825361437268\n",
      "Average test loss: 0.006815268728468153\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08450844977961647\n",
      "Average test loss: 0.006679003258132273\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08485510742664337\n",
      "Average test loss: 0.0094179140780535\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08432019534375933\n",
      "Average test loss: 0.006687535931666692\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08439360096057257\n",
      "Average test loss: 0.007141190504034361\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08459336170885298\n",
      "Average test loss: 0.007410494464139144\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08425474489397473\n",
      "Average test loss: 0.006813851469920741\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08419183887375725\n",
      "Average test loss: 0.05932093144953251\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08431335251198875\n",
      "Average test loss: 0.006929506976157427\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08408046367433336\n",
      "Average test loss: 0.05679112672474649\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08419634742206997\n",
      "Average test loss: 0.007013797574987014\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08402454645103878\n",
      "Average test loss: 0.006762190214047829\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08403695248232948\n",
      "Average test loss: 0.006644559834980302\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08363195594814088\n",
      "Average test loss: 0.007370745894809564\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08400777708821827\n",
      "Average test loss: 0.006784365272770325\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08378638915220897\n",
      "Average test loss: 0.007051594929148753\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08341055443551805\n",
      "Average test loss: 0.006753675682263242\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08389857823318905\n",
      "Average test loss: 0.00673974378945099\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0833249381184578\n",
      "Average test loss: 0.006835589357962211\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08402343839406967\n",
      "Average test loss: 0.006797361671924591\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08340317308240466\n",
      "Average test loss: 0.007060239765379164\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08349672226773368\n",
      "Average test loss: 0.006980202327999804\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08358646800120671\n",
      "Average test loss: 0.006693693931731913\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08308043162690269\n",
      "Average test loss: 0.006875209134899907\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08309072372317314\n",
      "Average test loss: 0.006824473280459643\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08337043944332334\n",
      "Average test loss: 0.006732686884701252\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0829528045852979\n",
      "Average test loss: 0.006675054708288775\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08313464301824569\n",
      "Average test loss: 0.021179143541389042\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08281006135543187\n",
      "Average test loss: 0.006853225655025906\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08285835689968533\n",
      "Average test loss: 0.0068352755904197695\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08305745685762829\n",
      "Average test loss: 0.006807994042833646\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08317059302661153\n",
      "Average test loss: 0.021267853496803178\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08272445998589198\n",
      "Average test loss: 0.006963916068275769\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0827146560880873\n",
      "Average test loss: 0.006860872738477256\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0826546196407742\n",
      "Average test loss: 0.006866697501391172\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08306094029214647\n",
      "Average test loss: 0.0066964829944901995\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0823833383652899\n",
      "Average test loss: 0.006780556901461548\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08254595855209562\n",
      "Average test loss: 0.007040937969254123\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08256221073203616\n",
      "Average test loss: 0.006716765734884474\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08290591104825337\n",
      "Average test loss: 0.006775392211145825\n",
      "Epoch 200/300\n",
      "Average training loss: 0.0823183115389612\n",
      "Average test loss: 0.006674372943739096\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08278621914651658\n",
      "Average test loss: 0.007048559022446474\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08213364099793964\n",
      "Average test loss: 0.006852973470257389\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08255832055542205\n",
      "Average test loss: 0.006788951995472113\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08208478184541067\n",
      "Average test loss: 0.007020053817994065\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08197274985578325\n",
      "Average test loss: 0.009030184596776963\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08332587856716579\n",
      "Average test loss: 0.006685682332764069\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08177276761664284\n",
      "Average test loss: 0.0068055165600445534\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08194455962048637\n",
      "Average test loss: 0.006903405722644594\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0820768558382988\n",
      "Average test loss: 0.007143494729780489\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08211837223503325\n",
      "Average test loss: 0.006883527999536859\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08656557250685162\n",
      "Average test loss: 0.0067662595489786734\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08144507359133826\n",
      "Average test loss: 0.0068951240823500685\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08142027022441228\n",
      "Average test loss: 0.0068996958492530715\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08155430150032043\n",
      "Average test loss: 0.0068196933505435785\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08154942350917392\n",
      "Average test loss: 0.0078111509084701535\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08191501143905852\n",
      "Average test loss: 0.006767461442699035\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08163439555300607\n",
      "Average test loss: 0.006786526073184278\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08176626245180765\n",
      "Average test loss: 0.008559495324061977\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08172117158439424\n",
      "Average test loss: 0.00694635433703661\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0815008819964197\n",
      "Average test loss: 0.006891758527192805\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08172016731235716\n",
      "Average test loss: 0.029936638159884347\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08210260569387012\n",
      "Average test loss: 0.006897979600148069\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08138318350580004\n",
      "Average test loss: 0.006814268589847618\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08132029640012317\n",
      "Average test loss: 0.006827706835750076\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08112684727377362\n",
      "Average test loss: 0.006763261564075947\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0813624018298255\n",
      "Average test loss: 0.00691435466789537\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08117641522487004\n",
      "Average test loss: 0.2552738291422526\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09990731279055277\n",
      "Average test loss: 0.006678478906138076\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08199667425950369\n",
      "Average test loss: 0.00864540686375565\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08033303954203924\n",
      "Average test loss: 0.006808846730738879\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08033038472466998\n",
      "Average test loss: 0.0069681979947619965\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08032142772277197\n",
      "Average test loss: 0.006806149328334464\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08177572691109446\n",
      "Average test loss: 0.0071351037472486495\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08032771599292755\n",
      "Average test loss: 0.007059615394307508\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08069332800971137\n",
      "Average test loss: 0.00687105375693904\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08117736699846055\n",
      "Average test loss: 0.006928769327700138\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08062187267012066\n",
      "Average test loss: 0.006776227405087815\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08102214144004716\n",
      "Average test loss: 0.006837524731540018\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08083248592747583\n",
      "Average test loss: 0.006963170549935765\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08109222463766734\n",
      "Average test loss: 0.006880326580256223\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08078453083170785\n",
      "Average test loss: 0.006834126199699111\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08071186403433482\n",
      "Average test loss: 0.006984640926950508\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08070255908038881\n",
      "Average test loss: 0.006891673922952679\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08080209594964981\n",
      "Average test loss: 0.00703583691890041\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08129944181442261\n",
      "Average test loss: 0.007055780443052451\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08055029581652748\n",
      "Average test loss: 0.006884850207302305\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08057799112796783\n",
      "Average test loss: 0.006840180287758509\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08076545147763359\n",
      "Average test loss: 0.006900902965416511\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08058254274394777\n",
      "Average test loss: 0.048976184440983665\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08033774109681448\n",
      "Average test loss: 0.009277499442299207\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08074967290295496\n",
      "Average test loss: 0.007393721548219522\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08067395658625497\n",
      "Average test loss: 0.0069132071679664986\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08033549851179123\n",
      "Average test loss: 0.007120779043684403\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08058852891789542\n",
      "Average test loss: 0.006940656747255061\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0802100828886032\n",
      "Average test loss: 0.00680984410064088\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08022066964705785\n",
      "Average test loss: 0.007194310547990931\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08062197807762359\n",
      "Average test loss: 0.008241908315155241\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08012097548776202\n",
      "Average test loss: 0.006923361942999893\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08020772524674734\n",
      "Average test loss: 0.0069681141202648485\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08106407716539171\n",
      "Average test loss: 0.025616399417320888\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08012066277530458\n",
      "Average test loss: 0.006858980784399642\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08026182146204842\n",
      "Average test loss: 0.006982538846217924\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08005310481786727\n",
      "Average test loss: 0.006914794942157136\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08012750664022233\n",
      "Average test loss: 0.006953956799374686\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0800224971903695\n",
      "Average test loss: 0.007166194773796532\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08007953760359023\n",
      "Average test loss: 0.006863817441380686\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07998176240258746\n",
      "Average test loss: 0.0069132739123370915\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08140115378300349\n",
      "Average test loss: 0.007251079173551666\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07977283367845747\n",
      "Average test loss: 0.00695500575668282\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07963189859522714\n",
      "Average test loss: 0.0073110334815250505\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08024945709440444\n",
      "Average test loss: 0.006916785207887491\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07962466814120611\n",
      "Average test loss: 0.0069250707063410015\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07956852242681715\n",
      "Average test loss: 0.007018797820640935\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08407469756735696\n",
      "Average test loss: 0.006813900898728106\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07939757251077229\n",
      "Average test loss: 0.006872445177700784\n",
      "Epoch 276/300\n",
      "Average training loss: 0.079257249371873\n",
      "Average test loss: 0.006966945512013303\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07928817778163486\n",
      "Average test loss: 0.007005824871361256\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07977515145805147\n",
      "Average test loss: 0.006862508873558707\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0795686181584994\n",
      "Average test loss: 0.007274203664726682\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08000040683481428\n",
      "Average test loss: 0.0077993374922209315\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07929221890370051\n",
      "Average test loss: 0.006998156355486976\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07954277367062039\n",
      "Average test loss: 0.006922010825739966\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0797111648718516\n",
      "Average test loss: 0.006980233363807201\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07986937837137116\n",
      "Average test loss: 0.007741196035924885\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07933737286263041\n",
      "Average test loss: 0.006847466030054622\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08013133249680202\n",
      "Average test loss: 0.007070941374533706\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07985512205627229\n",
      "Average test loss: 0.00696612319122586\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07915218885739644\n",
      "Average test loss: 0.007099136523488495\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07919902116722531\n",
      "Average test loss: 0.00686055656025807\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07973803389072419\n",
      "Average test loss: 0.006977155269847976\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07946383817990621\n",
      "Average test loss: 0.007067344155576494\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07916205456521776\n",
      "Average test loss: 0.0070432314657502704\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07925468646155463\n",
      "Average test loss: 0.0102718532913261\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07937409467167325\n",
      "Average test loss: 0.006834341175854206\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07905439719888899\n",
      "Average test loss: 0.007024197573877043\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07983696491850747\n",
      "Average test loss: 0.00712878946090738\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07898768396178882\n",
      "Average test loss: 0.007829660995552937\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07880877881579929\n",
      "Average test loss: 0.006910339439080821\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07932531291908688\n",
      "Average test loss: 0.00910945679495732\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07942921892470783\n",
      "Average test loss: 0.007007690372566382\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.77388679239485\n",
      "Average test loss: 0.008901637776858276\n",
      "Epoch 2/300\n",
      "Average training loss: 0.618426172097524\n",
      "Average test loss: 0.007641440713985099\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4143066150612301\n",
      "Average test loss: 0.006782595519390371\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3154467209974925\n",
      "Average test loss: 0.006193779313729869\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2567629365126292\n",
      "Average test loss: 0.006340898895843161\n",
      "Epoch 6/300\n",
      "Average training loss: 0.21843133393923442\n",
      "Average test loss: 0.005719285779943069\n",
      "Epoch 7/300\n",
      "Average training loss: 0.19134353200594584\n",
      "Average test loss: 0.005700767303713494\n",
      "Epoch 8/300\n",
      "Average training loss: 0.171930267545912\n",
      "Average test loss: 0.005506438594725397\n",
      "Epoch 9/300\n",
      "Average training loss: 0.15707942945427364\n",
      "Average test loss: 0.005304387798739804\n",
      "Epoch 10/300\n",
      "Average training loss: 0.14572286597887674\n",
      "Average test loss: 0.005774071982337369\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13623438057634565\n",
      "Average test loss: 0.005201103459215826\n",
      "Epoch 12/300\n",
      "Average training loss: 0.12870943109194438\n",
      "Average test loss: 0.006076220490452316\n",
      "Epoch 13/300\n",
      "Average training loss: 0.12240354894929462\n",
      "Average test loss: 0.004890370580057303\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11747615805599425\n",
      "Average test loss: 0.005258657974915372\n",
      "Epoch 15/300\n",
      "Average training loss: 0.11277044422096676\n",
      "Average test loss: 0.006462706795583168\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10954333982202742\n",
      "Average test loss: 0.005636009953088231\n",
      "Epoch 17/300\n",
      "Average training loss: 0.10585071005423864\n",
      "Average test loss: 0.004610518951796823\n",
      "Epoch 18/300\n",
      "Average training loss: 0.10372589313983917\n",
      "Average test loss: 0.00456491868570447\n",
      "Epoch 19/300\n",
      "Average training loss: 0.09997456115484238\n",
      "Average test loss: 0.004553264507402976\n",
      "Epoch 20/300\n",
      "Average training loss: 0.09779315406746335\n",
      "Average test loss: 0.004818106177366442\n",
      "Epoch 21/300\n",
      "Average training loss: 0.09439097656144037\n",
      "Average test loss: 0.0045143578524390856\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09214152981175316\n",
      "Average test loss: 0.004747897205253442\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08988001030683518\n",
      "Average test loss: 0.004343837587369813\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08772985951105754\n",
      "Average test loss: 0.004410802555580934\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08567106099923452\n",
      "Average test loss: 0.004715253418518437\n",
      "Epoch 26/300\n",
      "Average training loss: 0.08367933768696255\n",
      "Average test loss: 0.004429054888172282\n",
      "Epoch 27/300\n",
      "Average training loss: 0.08246181489361656\n",
      "Average test loss: 0.004288471046214303\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08077817747328017\n",
      "Average test loss: 0.004299405863715543\n",
      "Epoch 29/300\n",
      "Average training loss: 0.07941399041811625\n",
      "Average test loss: 0.004228226531296968\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07843108059300316\n",
      "Average test loss: 0.00424770471205314\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0771643782125579\n",
      "Average test loss: 0.0042370239239599965\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07665816556745106\n",
      "Average test loss: 0.00472033169410295\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07579951335986455\n",
      "Average test loss: 0.0044165620574106775\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07501049806674322\n",
      "Average test loss: 0.004275041190286478\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07415034727255504\n",
      "Average test loss: 0.004329984720175465\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0739430492983924\n",
      "Average test loss: 0.004378353830426931\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0731771914097998\n",
      "Average test loss: 0.0041758491351372665\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0725468465420935\n",
      "Average test loss: 0.004218268886208534\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07227160234583749\n",
      "Average test loss: 0.004375764590170648\n",
      "Epoch 40/300\n",
      "Average training loss: 0.071484911011325\n",
      "Average test loss: 0.004183996954518888\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07140558740827772\n",
      "Average test loss: 0.004128328428500228\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07090978283352323\n",
      "Average test loss: 0.004732880547642708\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07056424602535036\n",
      "Average test loss: 0.0041597862856255635\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0701027437183592\n",
      "Average test loss: 0.01506793391952912\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06989221178823048\n",
      "Average test loss: 0.005337089474830362\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06941327232784696\n",
      "Average test loss: 0.005071776099089119\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06945254679520925\n",
      "Average test loss: 0.004084644098248747\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0689777195221848\n",
      "Average test loss: 0.004185821413993835\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06870540148019791\n",
      "Average test loss: 0.004161227577262455\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06848517163263428\n",
      "Average test loss: 0.006202415714247359\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0682694776488675\n",
      "Average test loss: 0.004149819825672441\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06818545854753919\n",
      "Average test loss: 0.004101285190631946\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0679613716006279\n",
      "Average test loss: 0.004132496954252322\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06763716468546126\n",
      "Average test loss: 0.004077181120713552\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06750558455785116\n",
      "Average test loss: 0.004096537100358142\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06714706859985987\n",
      "Average test loss: 0.0042102020362185105\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06719087730844815\n",
      "Average test loss: 0.004163709305226803\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06721824223465389\n",
      "Average test loss: 0.005055776565439171\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06662333178520202\n",
      "Average test loss: 0.004112185645848513\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06659043518702189\n",
      "Average test loss: 0.0040855758587519326\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06621240051587422\n",
      "Average test loss: 0.004041179017888175\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06662736631102033\n",
      "Average test loss: 0.004164275977760553\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06598797859748204\n",
      "Average test loss: 0.0040580762653715085\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06591154374016656\n",
      "Average test loss: 0.0040657921766655315\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06582490597168604\n",
      "Average test loss: 0.004118286600336432\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06565343725019031\n",
      "Average test loss: 0.004089951050571269\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06542757323053149\n",
      "Average test loss: 0.004121962810763055\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06535743626952172\n",
      "Average test loss: 0.009405358905593554\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06519464752078057\n",
      "Average test loss: 0.004371946884940068\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06519204440050655\n",
      "Average test loss: 0.03161715194955468\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06554012950261434\n",
      "Average test loss: 0.004141058454083072\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06460024167431726\n",
      "Average test loss: 0.004057519685477019\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06446909626656108\n",
      "Average test loss: 0.004398259839249982\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0648768453962273\n",
      "Average test loss: 0.004066273139168818\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06440470747152964\n",
      "Average test loss: 0.004066874735471275\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06524530993236435\n",
      "Average test loss: 0.0041522286964787375\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06893609781397714\n",
      "Average test loss: 0.004136883541734682\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06561731781562169\n",
      "Average test loss: 0.004202021364122629\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06421373652087317\n",
      "Average test loss: 0.004121214691342579\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06397308189670245\n",
      "Average test loss: 0.004171993295351664\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06485595825645658\n",
      "Average test loss: 0.004321486580289072\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06376809149980545\n",
      "Average test loss: 0.004261671908199787\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06356181947390238\n",
      "Average test loss: 0.004528051576680607\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06353562122914526\n",
      "Average test loss: 0.005686376176360581\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06344654338889652\n",
      "Average test loss: 0.004417496456868119\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06317662864923478\n",
      "Average test loss: 0.004144118714249796\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06347355632318391\n",
      "Average test loss: 0.0043913714153071244\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06304973901642694\n",
      "Average test loss: 0.004075084815422694\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06304668107297685\n",
      "Average test loss: 5.056005822499593\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0629976362519794\n",
      "Average test loss: 0.004116775646184881\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06299819223748312\n",
      "Average test loss: 0.004789318247801728\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06270443930228552\n",
      "Average test loss: 0.004155696670421296\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06263785798019833\n",
      "Average test loss: 0.004091603677719832\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06257647422287199\n",
      "Average test loss: 0.0049437199301189845\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06237947385840946\n",
      "Average test loss: 0.004314358270416657\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0625378064778116\n",
      "Average test loss: 0.004186475757095549\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06228140793906318\n",
      "Average test loss: 0.004324503641161654\n",
      "Epoch 98/300\n",
      "Average training loss: 0.062250507877932654\n",
      "Average test loss: 0.004299779228038258\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06208032976587614\n",
      "Average test loss: 0.004352920314711001\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06210395082500246\n",
      "Average test loss: 0.004280129414051771\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06204651354418861\n",
      "Average test loss: 0.004582386315282848\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06169718548986647\n",
      "Average test loss: 0.011742128325833214\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06206916073958079\n",
      "Average test loss: 0.007838479162504276\n",
      "Epoch 104/300\n",
      "Average training loss: 0.061587957792811926\n",
      "Average test loss: 0.004207696999733647\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06132036593887541\n",
      "Average test loss: 0.004654296942469147\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0615059521595637\n",
      "Average test loss: 0.022013579392598737\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06139803922176361\n",
      "Average test loss: 0.0041245213386913145\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06134125216801961\n",
      "Average test loss: 0.004235692481613822\n",
      "Epoch 109/300\n",
      "Average training loss: 0.061442828072441946\n",
      "Average test loss: 0.004373727461530102\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06113047437535392\n",
      "Average test loss: 0.00434690418963631\n",
      "Epoch 111/300\n",
      "Average training loss: 0.061146483110056986\n",
      "Average test loss: 0.004377799656656053\n",
      "Epoch 112/300\n",
      "Average training loss: 0.060935959431860184\n",
      "Average test loss: 0.005310931844015916\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06113013986084196\n",
      "Average test loss: 0.004190116122364998\n",
      "Epoch 114/300\n",
      "Average training loss: 0.060861760632859335\n",
      "Average test loss: 0.004148022620628277\n",
      "Epoch 115/300\n",
      "Average training loss: 0.060856484684679246\n",
      "Average test loss: 0.004217630591657427\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0606100669503212\n",
      "Average test loss: 0.004425320515616072\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06138893749316533\n",
      "Average test loss: 0.004255945284954376\n",
      "Epoch 118/300\n",
      "Average training loss: 0.060411144468519425\n",
      "Average test loss: 0.004353357185506159\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06056995255417294\n",
      "Average test loss: 0.004247539096822342\n",
      "Epoch 120/300\n",
      "Average training loss: 0.060342906256516775\n",
      "Average test loss: 0.005056270127081209\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06100172133909332\n",
      "Average test loss: 0.004219217339737548\n",
      "Epoch 122/300\n",
      "Average training loss: 0.060266355825795065\n",
      "Average test loss: 0.004291949644891752\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0601387000977993\n",
      "Average test loss: 0.004428174359103044\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06017050155335003\n",
      "Average test loss: 0.004305420171883371\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06019040911065208\n",
      "Average test loss: 0.004361643878950013\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06018691653675503\n",
      "Average test loss: 0.004219666184857487\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05995528356896507\n",
      "Average test loss: 0.007503517526719306\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05994059751762284\n",
      "Average test loss: 0.005967584752788147\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06054424750473764\n",
      "Average test loss: 0.005987739535669486\n",
      "Epoch 130/300\n",
      "Average training loss: 0.059666859447956085\n",
      "Average test loss: 0.004206687553061379\n",
      "Epoch 131/300\n",
      "Average training loss: 0.059570185032155776\n",
      "Average test loss: 0.004223059275084071\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05980906927585602\n",
      "Average test loss: 0.004327470063749287\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05981134303741985\n",
      "Average test loss: 0.005134647867745823\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05959119785494275\n",
      "Average test loss: 0.004609248867879312\n",
      "Epoch 135/300\n",
      "Average training loss: 0.059489797946479586\n",
      "Average test loss: 0.004249253983298938\n",
      "Epoch 136/300\n",
      "Average training loss: 0.059390749759144254\n",
      "Average test loss: 0.0052994685851865345\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05958255014154646\n",
      "Average test loss: 0.004230002919418944\n",
      "Epoch 138/300\n",
      "Average training loss: 0.059374300403727424\n",
      "Average test loss: 0.024447338961478736\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05927774389915996\n",
      "Average test loss: 0.005542000832243098\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05919158300757408\n",
      "Average test loss: 0.004902742724451754\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05980661332607269\n",
      "Average test loss: 0.004372076957176129\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05914317819144991\n",
      "Average test loss: 0.004206589617455999\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05908192139863968\n",
      "Average test loss: 0.0043819654977156055\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0589564026163684\n",
      "Average test loss: 0.004343699990047349\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05914523152179188\n",
      "Average test loss: 0.004288387450286084\n",
      "Epoch 146/300\n",
      "Average training loss: 0.058852898087766434\n",
      "Average test loss: 0.004185668154516154\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05890587503380246\n",
      "Average test loss: 0.004247284249713023\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05889899204505814\n",
      "Average test loss: 0.004542484872457054\n",
      "Epoch 149/300\n",
      "Average training loss: 0.058744022266732324\n",
      "Average test loss: 0.008800067346543074\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05888658556010988\n",
      "Average test loss: 0.013274850166920158\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05943761196732521\n",
      "Average test loss: 0.005405235703620646\n",
      "Epoch 152/300\n",
      "Average training loss: 0.058566478490829466\n",
      "Average test loss: 0.01187047990742657\n",
      "Epoch 153/300\n",
      "Average training loss: 0.058932905697160295\n",
      "Average test loss: 0.004858435578851236\n",
      "Epoch 154/300\n",
      "Average training loss: 0.058350280271636114\n",
      "Average test loss: 0.004241526648402214\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05852202664481269\n",
      "Average test loss: 0.3770604726605945\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0586246534453498\n",
      "Average test loss: 0.0042889507450163365\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05835420337981648\n",
      "Average test loss: 0.004858152032726341\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05856136867735121\n",
      "Average test loss: 0.004228321885276172\n",
      "Epoch 159/300\n",
      "Average training loss: 0.058270922475390965\n",
      "Average test loss: 0.10038089888294538\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05838009005784989\n",
      "Average test loss: 0.00422932679971887\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05835545880264706\n",
      "Average test loss: 0.004418150471937325\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05822418522172504\n",
      "Average test loss: 0.004337769382115868\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05837498359216584\n",
      "Average test loss: 0.004439104703565439\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05810069847769207\n",
      "Average test loss: 0.004325184003346496\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05870154842734337\n",
      "Average test loss: 0.00425836532521579\n",
      "Epoch 166/300\n",
      "Average training loss: 0.058043210493193734\n",
      "Average test loss: 0.004388468725399839\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05801349601811833\n",
      "Average test loss: 0.0047490761176579525\n",
      "Epoch 168/300\n",
      "Average training loss: 0.058012715611192915\n",
      "Average test loss: 0.0043720304961833685\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05805256149172783\n",
      "Average test loss: 0.004341413815816243\n",
      "Epoch 170/300\n",
      "Average training loss: 0.057854849447806674\n",
      "Average test loss: 0.0043407140593561865\n",
      "Epoch 171/300\n",
      "Average training loss: 0.057906607104672325\n",
      "Average test loss: 0.004569668656008111\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05785288971993658\n",
      "Average test loss: 0.0044243193719949985\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05808702311250898\n",
      "Average test loss: 0.004463697983986802\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05783255289991697\n",
      "Average test loss: 0.004345977156112592\n",
      "Epoch 175/300\n",
      "Average training loss: 0.057736372225814396\n",
      "Average test loss: 0.004409783920894066\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05784556187523736\n",
      "Average test loss: 0.004210668728790349\n",
      "Epoch 177/300\n",
      "Average training loss: 0.057642052557733324\n",
      "Average test loss: 0.005138983798523744\n",
      "Epoch 178/300\n",
      "Average training loss: 0.057887086626556186\n",
      "Average test loss: 0.0042907429602411055\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05766029742691252\n",
      "Average test loss: 0.005766646154224873\n",
      "Epoch 180/300\n",
      "Average training loss: 0.057608748180998696\n",
      "Average test loss: 0.004284221150394943\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05840331866012679\n",
      "Average test loss: 0.004264094960358408\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05749288014902009\n",
      "Average test loss: 0.004912762456470066\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05740104095141093\n",
      "Average test loss: 0.009472175528191858\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05742706223991182\n",
      "Average test loss: 0.33236659601661894\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05753628847002983\n",
      "Average test loss: 0.0043289916287693716\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05745715513163143\n",
      "Average test loss: 0.017351086066828832\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0576333754559358\n",
      "Average test loss: 0.004322998493288954\n",
      "Epoch 188/300\n",
      "Average training loss: 0.057512731267346275\n",
      "Average test loss: 0.010303352986772854\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05724627056386736\n",
      "Average test loss: 0.004354958901802698\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05730713969469071\n",
      "Average test loss: 0.004334920571910011\n",
      "Epoch 191/300\n",
      "Average training loss: 0.057389760182963476\n",
      "Average test loss: 0.004406470123678446\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05786682414677408\n",
      "Average test loss: 0.004740858517587185\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05716331248150931\n",
      "Average test loss: 0.0044186278796858255\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05702182846930292\n",
      "Average test loss: 0.004341088304089176\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05709474335114161\n",
      "Average test loss: 0.004307715415540669\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05698671938313378\n",
      "Average test loss: 0.004382403357161416\n",
      "Epoch 197/300\n",
      "Average training loss: 0.057736279428005216\n",
      "Average test loss: 0.004284222586287393\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05704268930686845\n",
      "Average test loss: 0.004354793005105522\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05705895064936744\n",
      "Average test loss: 0.004353487908426258\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05701577353808615\n",
      "Average test loss: 0.005553588752945264\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0571783351831966\n",
      "Average test loss: 0.005219740123798449\n",
      "Epoch 202/300\n",
      "Average training loss: 0.056733332375685376\n",
      "Average test loss: 0.004501273908962806\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05698838663432333\n",
      "Average test loss: 0.004438236923060483\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05717236491044363\n",
      "Average test loss: 0.0043902895285023585\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0566767857670784\n",
      "Average test loss: 0.004289899879031711\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05787462807363934\n",
      "Average test loss: 0.004664484517027934\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05679843510521783\n",
      "Average test loss: 0.004321752905845642\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0566174296869172\n",
      "Average test loss: 0.004375193641831477\n",
      "Epoch 209/300\n",
      "Average training loss: 0.056749238948027296\n",
      "Average test loss: 0.006794788354386886\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05670888230535719\n",
      "Average test loss: 0.013636311315000057\n",
      "Epoch 211/300\n",
      "Average training loss: 0.056792847421434194\n",
      "Average test loss: 0.004463518342624108\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05658782276842329\n",
      "Average test loss: 0.004413552596751187\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05683201930258009\n",
      "Average test loss: 0.004381900429311726\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05662102285358641\n",
      "Average test loss: 0.0045349624711606236\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05653183470169703\n",
      "Average test loss: 0.005013481149242984\n",
      "Epoch 216/300\n",
      "Average training loss: 0.056646911601225536\n",
      "Average test loss: 0.004696167535665962\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05661572753720814\n",
      "Average test loss: 0.004733983172724644\n",
      "Epoch 218/300\n",
      "Average training loss: 0.056398023386796316\n",
      "Average test loss: 0.004501026554240121\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0565449201034175\n",
      "Average test loss: 0.004435948619825972\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07447252302037345\n",
      "Average test loss: 0.004383159175101254\n",
      "Epoch 221/300\n",
      "Average training loss: 0.058187312099668716\n",
      "Average test loss: 0.0044592394717037675\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05658262701167001\n",
      "Average test loss: 0.004304819595068693\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05618367130557696\n",
      "Average test loss: 0.0045479186495973\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05609079338444604\n",
      "Average test loss: 0.0043452659563885795\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05626761148373286\n",
      "Average test loss: 0.004502905523197518\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05647648233175278\n",
      "Average test loss: 0.0045165689856641825\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0562597642507818\n",
      "Average test loss: 0.004562427487224341\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05636860164668825\n",
      "Average test loss: 0.004775375816557142\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05666219085786078\n",
      "Average test loss: 0.004418599634948704\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05621207415064176\n",
      "Average test loss: 0.005056591564168533\n",
      "Epoch 231/300\n",
      "Average training loss: 0.056265389800071716\n",
      "Average test loss: 0.004510070910056432\n",
      "Epoch 232/300\n",
      "Average training loss: 0.056428770621617635\n",
      "Average test loss: 0.00450241285459035\n",
      "Epoch 233/300\n",
      "Average training loss: 0.056293479790290195\n",
      "Average test loss: 0.004336252054820458\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05628766391012404\n",
      "Average test loss: 0.00442232965429624\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05739595273137093\n",
      "Average test loss: 0.004583634303261836\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05604879044161903\n",
      "Average test loss: 0.004339604799946149\n",
      "Epoch 237/300\n",
      "Average training loss: 0.056040337671836214\n",
      "Average test loss: 0.004388960624734561\n",
      "Epoch 238/300\n",
      "Average training loss: 0.056193317459689245\n",
      "Average test loss: 0.004334168682909674\n",
      "Epoch 239/300\n",
      "Average training loss: 0.056195698105626635\n",
      "Average test loss: 0.004343110984605219\n",
      "Epoch 240/300\n",
      "Average training loss: 0.056294044693311056\n",
      "Average test loss: 0.004636266452570756\n",
      "Epoch 241/300\n",
      "Average training loss: 0.058139321933190026\n",
      "Average test loss: 0.004381833619955513\n",
      "Epoch 242/300\n",
      "Average training loss: 0.055848420507378045\n",
      "Average test loss: 0.00445812283621894\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05581603591971927\n",
      "Average test loss: 0.0046319000257386105\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0558971181511879\n",
      "Average test loss: 0.021219222280714246\n",
      "Epoch 245/300\n",
      "Average training loss: 0.056064093493753006\n",
      "Average test loss: 0.004384779241763883\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05582356564866172\n",
      "Average test loss: 0.004523947961835397\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05608046647575166\n",
      "Average test loss: 0.004348984619809522\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05618464153673914\n",
      "Average test loss: 0.0048044649747510755\n",
      "Epoch 249/300\n",
      "Average training loss: 0.055997253994146984\n",
      "Average test loss: 0.0043361626240528296\n",
      "Epoch 250/300\n",
      "Average training loss: 0.055796793821785186\n",
      "Average test loss: 0.004467001696220703\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05585604420966572\n",
      "Average test loss: 0.004605098283953137\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05588294533226225\n",
      "Average test loss: 0.004422299233989583\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05698156985309389\n",
      "Average test loss: 0.0044596158373687005\n",
      "Epoch 254/300\n",
      "Average training loss: 0.055621856566932464\n",
      "Average test loss: 0.004384381918857495\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05572574556205008\n",
      "Average test loss: 0.0043603131920099255\n",
      "Epoch 256/300\n",
      "Average training loss: 0.055921018868684766\n",
      "Average test loss: 0.0046315678099377285\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05588693261477682\n",
      "Average test loss: 0.004471463496072425\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05562348041600651\n",
      "Average test loss: 0.01207513868312041\n",
      "Epoch 259/300\n",
      "Average training loss: 0.057077461159891554\n",
      "Average test loss: 0.004518128279596567\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05593790777524312\n",
      "Average test loss: 0.004535778458333677\n",
      "Epoch 261/300\n",
      "Average training loss: 0.055726118127504984\n",
      "Average test loss: 0.007542869658933746\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05555443799495697\n",
      "Average test loss: 0.004510946047388845\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05566853661007351\n",
      "Average test loss: 0.004412352227088478\n",
      "Epoch 264/300\n",
      "Average training loss: 0.055624008539650174\n",
      "Average test loss: 0.004398896031495598\n",
      "Epoch 265/300\n",
      "Average training loss: 0.055710662474234895\n",
      "Average test loss: 0.005202933001435465\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05622168247236146\n",
      "Average test loss: 0.011144616932504707\n",
      "Epoch 267/300\n",
      "Average training loss: 0.055546787179178665\n",
      "Average test loss: 0.004454407226294279\n",
      "Epoch 268/300\n",
      "Average training loss: 0.055560450348589155\n",
      "Average test loss: 0.004502808858743972\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05547741786970033\n",
      "Average test loss: 0.004484957120070855\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05542835984627406\n",
      "Average test loss: 0.005138576658856538\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05564780710140864\n",
      "Average test loss: 0.004421055806593762\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05557583694325553\n",
      "Average test loss: 0.004296064164075587\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05575505191087723\n",
      "Average test loss: 0.007507479430072838\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05547180418504609\n",
      "Average test loss: 0.004575581130882104\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05538341372874048\n",
      "Average test loss: 0.004616603275140126\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0553562140862147\n",
      "Average test loss: 0.026782955727643436\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05584712495075332\n",
      "Average test loss: 0.00458036528279384\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05536248899168438\n",
      "Average test loss: 0.004690922457310888\n",
      "Epoch 279/300\n",
      "Average training loss: 0.055425472401910356\n",
      "Average test loss: 0.004448650693727864\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05599375971489483\n",
      "Average test loss: 0.004996696354614364\n",
      "Epoch 281/300\n",
      "Average training loss: 0.055360922260416875\n",
      "Average test loss: 0.004441867675632238\n",
      "Epoch 282/300\n",
      "Average training loss: 0.055214919143252904\n",
      "Average test loss: 0.004419480463282929\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05516687051786317\n",
      "Average test loss: 0.004635789185762405\n",
      "Epoch 284/300\n",
      "Average training loss: 0.055440945718023514\n",
      "Average test loss: 0.00947132803292738\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05535007269514931\n",
      "Average test loss: 0.004633314080536365\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05526011882887946\n",
      "Average test loss: 0.0044081336468872094\n",
      "Epoch 287/300\n",
      "Average training loss: 0.055555163390106625\n",
      "Average test loss: 0.004454332180321216\n",
      "Epoch 288/300\n",
      "Average training loss: 0.055135101331604854\n",
      "Average test loss: 0.0044379382977883025\n",
      "Epoch 289/300\n",
      "Average training loss: 0.055250088486406536\n",
      "Average test loss: 0.004550078304484486\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05538970430692037\n",
      "Average test loss: 0.004399916808638308\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05550235672129525\n",
      "Average test loss: 0.004623662936190764\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05529725114504496\n",
      "Average test loss: 0.005163202449058493\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05517896089951197\n",
      "Average test loss: 0.004477091703563928\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05509811684820387\n",
      "Average test loss: 0.004453151729371813\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0552245063053237\n",
      "Average test loss: 0.004566761840134859\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05655441536506017\n",
      "Average test loss: 0.0043756957451502486\n",
      "Epoch 297/300\n",
      "Average training loss: 0.054794497556156584\n",
      "Average test loss: 0.00453233108503951\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05488280019495222\n",
      "Average test loss: 0.0044221050246722165\n",
      "Epoch 299/300\n",
      "Average training loss: 0.055000988066196445\n",
      "Average test loss: 0.00440982549968693\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05530050008164512\n",
      "Average test loss: 0.004384889621908466\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4961092765596178\n",
      "Average test loss: 0.006470946619908015\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5045733980867598\n",
      "Average test loss: 0.005432287618517876\n",
      "Epoch 3/300\n",
      "Average training loss: 0.33101306756337484\n",
      "Average test loss: 0.005077578006105291\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2481457089583079\n",
      "Average test loss: 0.004795338043322166\n",
      "Epoch 5/300\n",
      "Average training loss: 0.19932888151539696\n",
      "Average test loss: 0.004526870899730259\n",
      "Epoch 6/300\n",
      "Average training loss: 0.16873310403029124\n",
      "Average test loss: 0.0043499527877817554\n",
      "Epoch 7/300\n",
      "Average training loss: 0.1469229910241233\n",
      "Average test loss: 0.0044154479453961054\n",
      "Epoch 8/300\n",
      "Average training loss: 0.13241044070323307\n",
      "Average test loss: 0.004828549447986815\n",
      "Epoch 9/300\n",
      "Average training loss: 0.12141766497161653\n",
      "Average test loss: 0.004373097932587067\n",
      "Epoch 10/300\n",
      "Average training loss: 0.11298394670751359\n",
      "Average test loss: 0.003947570749661989\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1059504945675532\n",
      "Average test loss: 0.004418183261321651\n",
      "Epoch 12/300\n",
      "Average training loss: 0.10032785111665726\n",
      "Average test loss: 0.004591383192274306\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09601066937711504\n",
      "Average test loss: 0.004707575854741864\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09153279438283708\n",
      "Average test loss: 0.004612979671607415\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08818500217464235\n",
      "Average test loss: 0.0037933845486905838\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08454871938625971\n",
      "Average test loss: 0.003777904332925876\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08197372246450849\n",
      "Average test loss: 0.0034720981462548176\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07986805550588502\n",
      "Average test loss: 0.003423851800047689\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07697497987084918\n",
      "Average test loss: 0.0036260350617683596\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07468421060509152\n",
      "Average test loss: 0.005783912601156367\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07353183693355984\n",
      "Average test loss: 0.003725590272082223\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07143026983075672\n",
      "Average test loss: 0.0036027372291104662\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06913095641798443\n",
      "Average test loss: 0.0032683865134086874\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06794841939210891\n",
      "Average test loss: 0.0033239310251341925\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06597014796071582\n",
      "Average test loss: 0.0033968139354967407\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06473848385943307\n",
      "Average test loss: 0.0033274212754848933\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06359859348005718\n",
      "Average test loss: 0.0032729054058177605\n",
      "Epoch 28/300\n",
      "Average training loss: 0.062306518524885175\n",
      "Average test loss: 0.0032880721212261254\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06133535396390491\n",
      "Average test loss: 0.0032923158792157966\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06020559324158563\n",
      "Average test loss: 0.003171907150704\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05944544289840592\n",
      "Average test loss: 0.0033607365536606976\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05867351000507673\n",
      "Average test loss: 0.003257480565458536\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05803275697098838\n",
      "Average test loss: 0.0031442618713610703\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05749991140100691\n",
      "Average test loss: 0.003133321945452028\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05709205210871167\n",
      "Average test loss: 0.003172108178751336\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05649261538187663\n",
      "Average test loss: 0.0030955517234073746\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05605267009470198\n",
      "Average test loss: 0.0031093975046856534\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05578554829623964\n",
      "Average test loss: 0.0032990500206748646\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05541958562864198\n",
      "Average test loss: 0.006981253230737315\n",
      "Epoch 40/300\n",
      "Average training loss: 0.054947970340649285\n",
      "Average test loss: 0.0031208579099426667\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05474644387430615\n",
      "Average test loss: 0.0030801711018300718\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05456006186207135\n",
      "Average test loss: 0.0030455057397484778\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05423802171150843\n",
      "Average test loss: 0.00304512342893415\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05392069161931674\n",
      "Average test loss: 0.0033259700420829987\n",
      "Epoch 45/300\n",
      "Average training loss: 0.053737917797433005\n",
      "Average test loss: 0.003027293699690037\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05352739399340418\n",
      "Average test loss: 0.003633228102285001\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05334059202008777\n",
      "Average test loss: 0.0030768158197816875\n",
      "Epoch 48/300\n",
      "Average training loss: 0.053091436793406804\n",
      "Average test loss: 0.003176126083980004\n",
      "Epoch 49/300\n",
      "Average training loss: 0.052848053952058154\n",
      "Average test loss: 0.003157771159791284\n",
      "Epoch 50/300\n",
      "Average training loss: 0.052754466483990355\n",
      "Average test loss: 0.003034269961425\n",
      "Epoch 51/300\n",
      "Average training loss: 0.052634798526763914\n",
      "Average test loss: 0.0030461875051259993\n",
      "Epoch 52/300\n",
      "Average training loss: 0.052531453205479514\n",
      "Average test loss: 0.0033095996781355805\n",
      "Epoch 53/300\n",
      "Average training loss: 0.052227675451172725\n",
      "Average test loss: 0.0031256324102481208\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05208337802026007\n",
      "Average test loss: 0.0030749155229164494\n",
      "Epoch 55/300\n",
      "Average training loss: 0.051904755913549\n",
      "Average test loss: 0.043325872224238185\n",
      "Epoch 56/300\n",
      "Average training loss: 0.051881857772668204\n",
      "Average test loss: 0.0033819046314391824\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05176863247818417\n",
      "Average test loss: 0.0031085584486524264\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05157344747914208\n",
      "Average test loss: 0.00318625333532691\n",
      "Epoch 59/300\n",
      "Average training loss: 0.051429923196633656\n",
      "Average test loss: 0.0030367909322182337\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05130418223473761\n",
      "Average test loss: 0.0030945870015356277\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05121284940838814\n",
      "Average test loss: 0.0030323746498260233\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05108418512675497\n",
      "Average test loss: 0.003223893730590741\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0509747037490209\n",
      "Average test loss: 0.003143730152398348\n",
      "Epoch 64/300\n",
      "Average training loss: 0.050748562842607495\n",
      "Average test loss: 0.0030346655858059725\n",
      "Epoch 65/300\n",
      "Average training loss: 0.051400365837746195\n",
      "Average test loss: 0.0034021240874297087\n",
      "Epoch 66/300\n",
      "Average training loss: 0.050496851603190104\n",
      "Average test loss: 0.003170233132524623\n",
      "Epoch 67/300\n",
      "Average training loss: 0.050596949464744995\n",
      "Average test loss: 0.018043524998343653\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0504194323486752\n",
      "Average test loss: 0.023282609285579788\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05039890667133861\n",
      "Average test loss: 0.004149147828833924\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05018400846587287\n",
      "Average test loss: 0.0030405986874053876\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05010903436276648\n",
      "Average test loss: 0.003176413485573398\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05003444857729806\n",
      "Average test loss: 0.003184501350753837\n",
      "Epoch 73/300\n",
      "Average training loss: 0.049952055358224445\n",
      "Average test loss: 0.004179681255999539\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05016980700691541\n",
      "Average test loss: 0.0033586432830327085\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04970908905069033\n",
      "Average test loss: 0.0030546370312157602\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04959233162800471\n",
      "Average test loss: 0.003077233940569891\n",
      "Epoch 77/300\n",
      "Average training loss: 0.049736783100499046\n",
      "Average test loss: 0.0030293613916469943\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04941567089822557\n",
      "Average test loss: 0.0036645490191876888\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04943314644363191\n",
      "Average test loss: 0.003038101982946197\n",
      "Epoch 80/300\n",
      "Average training loss: 0.049348478578858906\n",
      "Average test loss: 0.0030408179037686852\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04940013661318355\n",
      "Average test loss: 0.0033115429371181463\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04906601930658023\n",
      "Average test loss: 0.0030672405786398385\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04923896186219321\n",
      "Average test loss: 0.0032879786727329093\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0489077098303371\n",
      "Average test loss: 0.0032359032216999267\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04888116944167349\n",
      "Average test loss: 0.003048080393837558\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04884217689765824\n",
      "Average test loss: 0.0030996980406343935\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04883509705464045\n",
      "Average test loss: 0.01108556979915334\n",
      "Epoch 88/300\n",
      "Average training loss: 0.048661926839086746\n",
      "Average test loss: 0.0030537378755915497\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04870115016566383\n",
      "Average test loss: 0.0032806556469036473\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04854872608515951\n",
      "Average test loss: 0.003096512095381816\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04846782005164359\n",
      "Average test loss: 0.003322064731270075\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04843854072358873\n",
      "Average test loss: 0.003144124242166678\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0483392093843884\n",
      "Average test loss: 0.0032474051674620972\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04848306738005744\n",
      "Average test loss: 0.00309418081264529\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04813090975417031\n",
      "Average test loss: 0.0030685915677911707\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04816191479232576\n",
      "Average test loss: 0.003057518039933509\n",
      "Epoch 97/300\n",
      "Average training loss: 0.048070151597261426\n",
      "Average test loss: 0.0031815029355800814\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04819260724054442\n",
      "Average test loss: 722.5378688557943\n",
      "Epoch 99/300\n",
      "Average training loss: 0.047880531264675986\n",
      "Average test loss: 0.0030601755753159525\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04783381752504243\n",
      "Average test loss: 0.0030616041163189544\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04780495854881075\n",
      "Average test loss: 0.003277659307751391\n",
      "Epoch 102/300\n",
      "Average training loss: 0.047754170447587965\n",
      "Average test loss: 0.003995931176261769\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04779508159226841\n",
      "Average test loss: 0.0031702146749529574\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04773215937283304\n",
      "Average test loss: 0.0030825527470765843\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04756971412565973\n",
      "Average test loss: 0.003259171490660972\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04758484697673056\n",
      "Average test loss: 0.0035318702190286584\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04948217824101448\n",
      "Average test loss: 0.0030940535590052603\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04723184517357085\n",
      "Average test loss: 0.0030891159975694285\n",
      "Epoch 109/300\n",
      "Average training loss: 0.047148246655861537\n",
      "Average test loss: 0.0033711977348559433\n",
      "Epoch 110/300\n",
      "Average training loss: 0.047214758614699044\n",
      "Average test loss: 0.0032322617738197246\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0473413055770927\n",
      "Average test loss: 0.003113801210704777\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04716208139393065\n",
      "Average test loss: 0.0030734483537574607\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04848765670922067\n",
      "Average test loss: 0.003094112576295932\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04699813548061583\n",
      "Average test loss: 0.0031218798499968317\n",
      "Epoch 115/300\n",
      "Average training loss: 0.046968574199411604\n",
      "Average test loss: 0.007349549909225769\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04695042606857088\n",
      "Average test loss: 0.004882738632460435\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04699674569235908\n",
      "Average test loss: 0.003121137270703912\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04688644941316711\n",
      "Average test loss: 0.003122777105205589\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04684547445509169\n",
      "Average test loss: 0.003161860761957036\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04681592908832762\n",
      "Average test loss: 0.0031505431145843533\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04684942516684532\n",
      "Average test loss: 0.003294290540739894\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04681117933160729\n",
      "Average test loss: 0.0033266041073948143\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04699166100554996\n",
      "Average test loss: 0.003101133295438356\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04667197680804465\n",
      "Average test loss: 0.003171162359209524\n",
      "Epoch 125/300\n",
      "Average training loss: 0.046632745527558854\n",
      "Average test loss: 0.0031439830501460366\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04657609341873063\n",
      "Average test loss: 0.003117847953405645\n",
      "Epoch 127/300\n",
      "Average training loss: 0.046598058723741106\n",
      "Average test loss: 0.004037923324853182\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04641770475440555\n",
      "Average test loss: 0.003473851058632135\n",
      "Epoch 129/300\n",
      "Average training loss: 0.046659616791539724\n",
      "Average test loss: 0.003277303616205851\n",
      "Epoch 130/300\n",
      "Average training loss: 0.046257217513190375\n",
      "Average test loss: 0.003174653357722693\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04642148787114355\n",
      "Average test loss: 0.0031618217440942922\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04623603225416607\n",
      "Average test loss: 0.003483356318126122\n",
      "Epoch 133/300\n",
      "Average training loss: 0.046790198597643114\n",
      "Average test loss: 0.0031084042669584353\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04615956377983093\n",
      "Average test loss: 0.014659435576862759\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04615450158715248\n",
      "Average test loss: 0.00569318622806006\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04611915619505776\n",
      "Average test loss: 0.003643045907219251\n",
      "Epoch 137/300\n",
      "Average training loss: 0.046119255281156964\n",
      "Average test loss: 0.0039458043184131385\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04624459834893545\n",
      "Average test loss: 0.003198587285561694\n",
      "Epoch 139/300\n",
      "Average training loss: 0.046098637600739796\n",
      "Average test loss: 0.0031802584141906766\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04598910321129693\n",
      "Average test loss: 0.005092202725923724\n",
      "Epoch 141/300\n",
      "Average training loss: 0.047890211529201934\n",
      "Average test loss: 0.0032764233011338445\n",
      "Epoch 142/300\n",
      "Average training loss: 0.045840590798192554\n",
      "Average test loss: 0.0031068996193094386\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04584194864167108\n",
      "Average test loss: 0.0031355126173131997\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04567161797483762\n",
      "Average test loss: 0.003885137781086895\n",
      "Epoch 145/300\n",
      "Average training loss: 0.045818783664041095\n",
      "Average test loss: 0.003719575785928302\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04610203135675854\n",
      "Average test loss: 0.0032940524079733426\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04577937498357561\n",
      "Average test loss: 0.41344305988815094\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04573963929216067\n",
      "Average test loss: 0.003172361967050367\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04576132631964154\n",
      "Average test loss: 0.0033054796687016883\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04577814559472932\n",
      "Average test loss: 0.0031905067277451355\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0457924665345086\n",
      "Average test loss: 0.003142862845212221\n",
      "Epoch 152/300\n",
      "Average training loss: 0.045786260151200825\n",
      "Average test loss: 0.00330759632276992\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04553077585167355\n",
      "Average test loss: 0.003273871985160642\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04548598860700925\n",
      "Average test loss: 0.0032699568987720543\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04550119099352095\n",
      "Average test loss: 0.003202745714949237\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04551311585307121\n",
      "Average test loss: 0.003243807932982842\n",
      "Epoch 157/300\n",
      "Average training loss: 0.045801873293187884\n",
      "Average test loss: 0.09193556921349631\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04538262585136626\n",
      "Average test loss: 0.0031723661083314153\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04527969670957989\n",
      "Average test loss: 0.005541365135875013\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04536045479112201\n",
      "Average test loss: 0.0031717179270668162\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04531926774481932\n",
      "Average test loss: 0.0034121938476132023\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04527011511557632\n",
      "Average test loss: 0.003463271466601226\n",
      "Epoch 166/300\n",
      "Average training loss: 0.045240561856163874\n",
      "Average test loss: 0.004258070283258955\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04513951949609651\n",
      "Average test loss: 0.0038699134791062937\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04520009953445858\n",
      "Average test loss: 0.0031789870849914022\n",
      "Epoch 169/300\n",
      "Average training loss: 0.045852208720313176\n",
      "Average test loss: 0.0032889155172225503\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04501157349679205\n",
      "Average test loss: 0.0032386456222997773\n",
      "Epoch 171/300\n",
      "Average training loss: 0.044985874109798006\n",
      "Average test loss: 0.0033189035947952007\n",
      "Epoch 172/300\n",
      "Average training loss: 0.045505797776910994\n",
      "Average test loss: 0.0032262513869338563\n",
      "Epoch 173/300\n",
      "Average training loss: 0.044889466991027194\n",
      "Average test loss: 0.0032608961649239062\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04498817743526565\n",
      "Average test loss: 0.0032713430451436175\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0449178431265884\n",
      "Average test loss: 0.003212760229491525\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04493236934807565\n",
      "Average test loss: 0.0034033241828696596\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0451278580195374\n",
      "Average test loss: 0.003155378667430745\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04487994466556443\n",
      "Average test loss: 0.0033089357753180795\n",
      "Epoch 179/300\n",
      "Average training loss: 0.044933198971880806\n",
      "Average test loss: 10.935737351523505\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04503872217403518\n",
      "Average test loss: 0.0036068304549488756\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04470996110969119\n",
      "Average test loss: 0.0034427743500305545\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04486623207728068\n",
      "Average test loss: 0.003989457544767194\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04484715226623747\n",
      "Average test loss: 0.003202124596056011\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0447487724588977\n",
      "Average test loss: 0.0034642685366173586\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04562081018421385\n",
      "Average test loss: 0.0032420085987283124\n",
      "Epoch 186/300\n",
      "Average training loss: 0.044598063558340076\n",
      "Average test loss: 0.003334004084683127\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04480801150202751\n",
      "Average test loss: 0.0032639934108075167\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0446077277329233\n",
      "Average test loss: 0.0033767289080553586\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04501857613854938\n",
      "Average test loss: 0.00338127487566736\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04464328958921962\n",
      "Average test loss: 0.0033317637213816247\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04457860509885682\n",
      "Average test loss: 0.0035422165115467376\n",
      "Epoch 192/300\n",
      "Average training loss: 0.044642015549871655\n",
      "Average test loss: 0.003955535442257921\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04489727392130428\n",
      "Average test loss: 1.5782384353909227\n",
      "Epoch 194/300\n",
      "Average training loss: 0.045273655626508924\n",
      "Average test loss: 0.003199354037642479\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04441278766592344\n",
      "Average test loss: 0.016073942192312743\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04441850409573979\n",
      "Average test loss: 0.003428084958137737\n",
      "Epoch 197/300\n",
      "Average training loss: 0.044463679677910276\n",
      "Average test loss: 0.0033180745887673563\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04445392426186138\n",
      "Average test loss: 0.003281851790017552\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04453571047882239\n",
      "Average test loss: 0.0034168718722131517\n",
      "Epoch 200/300\n",
      "Average training loss: 0.045041714284155106\n",
      "Average test loss: 0.1815441435906622\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04441777367724312\n",
      "Average test loss: 0.0033622983569900197\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0443279689417945\n",
      "Average test loss: 0.0036497967874424323\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04431693271133635\n",
      "Average test loss: 0.0035515649426314565\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04446101517809762\n",
      "Average test loss: 0.0032043880484998225\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04439394247531891\n",
      "Average test loss: 0.00883827156946063\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04440089998311467\n",
      "Average test loss: 0.00819451449977027\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04488954085608323\n",
      "Average test loss: 0.0034162233794728914\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04418809030122227\n",
      "Average test loss: 0.0036295087532036835\n",
      "Epoch 209/300\n",
      "Average training loss: 0.044233627488215764\n",
      "Average test loss: 0.003224665585077471\n",
      "Epoch 210/300\n",
      "Average training loss: 0.044251228309339946\n",
      "Average test loss: 0.0038371992684486838\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04434297482503785\n",
      "Average test loss: 0.0032463705730107094\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04416088336043888\n",
      "Average test loss: 0.003299306063602368\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0442997839682632\n",
      "Average test loss: 0.11593694737553596\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04433836690253681\n",
      "Average test loss: 0.004942664009829362\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04406030085020595\n",
      "Average test loss: 0.003233939778059721\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04416181540489197\n",
      "Average test loss: 0.0032800207003537154\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0442746503882938\n",
      "Average test loss: 0.004400372204888198\n",
      "Epoch 218/300\n",
      "Average training loss: 0.044054594828022854\n",
      "Average test loss: 0.003398999239421553\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04432602211501863\n",
      "Average test loss: 0.0035235863890912796\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04406760700212584\n",
      "Average test loss: 0.03274681448274189\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04433591027061144\n",
      "Average test loss: 0.003269761116347379\n",
      "Epoch 222/300\n",
      "Average training loss: 0.043964335471391676\n",
      "Average test loss: 0.0033668178255773255\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04403364451064004\n",
      "Average test loss: 0.0035548640108770796\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04391679661472638\n",
      "Average test loss: 0.0033050398928009803\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04407460935910543\n",
      "Average test loss: 0.0035415938881536327\n",
      "Epoch 226/300\n",
      "Average training loss: 0.044099218365218906\n",
      "Average test loss: 0.003490218495329221\n",
      "Epoch 227/300\n",
      "Average training loss: 0.044000073571999865\n",
      "Average test loss: 0.003513223118045264\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04455605636371507\n",
      "Average test loss: 0.003304269569201602\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04385468668904569\n",
      "Average test loss: 0.0033142444379627703\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04379627572496732\n",
      "Average test loss: 0.0033325465222199756\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04398182350728247\n",
      "Average test loss: 0.003229645496027337\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04392120310664177\n",
      "Average test loss: 0.0035522150753272905\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04377978288796213\n",
      "Average test loss: 0.0033285240336424775\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04387583302458127\n",
      "Average test loss: 0.003230832460232907\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04382252209385236\n",
      "Average test loss: 0.003320922681854831\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0438914300137096\n",
      "Average test loss: 0.003324856530357566\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04378939339849684\n",
      "Average test loss: 0.003327092928191026\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04418532651331689\n",
      "Average test loss: 0.0032598763907121285\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04370294374558661\n",
      "Average test loss: 0.0032563942859156264\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04372097637255987\n",
      "Average test loss: 0.003379666513245967\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04373763612575001\n",
      "Average test loss: 0.00356639859928853\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04378263795375824\n",
      "Average test loss: 0.0032896400795628627\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04434266502658526\n",
      "Average test loss: 0.005537667159405019\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04366234338283539\n",
      "Average test loss: 0.004102516086979045\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04364270325170623\n",
      "Average test loss: 0.012225680720474984\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04370360144972801\n",
      "Average test loss: 0.0032954196439435083\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04404058390193515\n",
      "Average test loss: 0.0032818944636318417\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04357220646076732\n",
      "Average test loss: 0.0033332501656065383\n",
      "Epoch 249/300\n",
      "Average training loss: 0.043586622539493775\n",
      "Average test loss: 0.007133949863413969\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04355663135978911\n",
      "Average test loss: 0.0032276627665592562\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04407741529080603\n",
      "Average test loss: 0.0032015459943148827\n",
      "Epoch 252/300\n",
      "Average training loss: 0.043540239605638714\n",
      "Average test loss: 0.003400865082732505\n",
      "Epoch 253/300\n",
      "Average training loss: 0.043429267919725845\n",
      "Average test loss: 0.004201792556585537\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04378136268589232\n",
      "Average test loss: 0.0038130645412537785\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04356849644912614\n",
      "Average test loss: 0.0032846723852886094\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04350034782621596\n",
      "Average test loss: 0.0033091533821490078\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04351753588848644\n",
      "Average test loss: 0.003282332529210382\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04369260887636079\n",
      "Average test loss: 0.003365436009855734\n",
      "Epoch 259/300\n",
      "Average training loss: 0.043340342236889734\n",
      "Average test loss: 0.0034300624817195867\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04373056138555209\n",
      "Average test loss: 0.003410203113531073\n",
      "Epoch 261/300\n",
      "Average training loss: 0.043482907752195994\n",
      "Average test loss: 0.0032651626567045846\n",
      "Epoch 262/300\n",
      "Average training loss: 0.043428449157211516\n",
      "Average test loss: 0.004925584969835149\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04339968570404583\n",
      "Average test loss: 0.004091293268940515\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04348457383447223\n",
      "Average test loss: 0.0033055909257382154\n",
      "Epoch 265/300\n",
      "Average training loss: 0.043889486501614255\n",
      "Average test loss: 0.003234696721865071\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04330817366308636\n",
      "Average test loss: 0.0032630309646742212\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04335806600915061\n",
      "Average test loss: 0.0032689773249957297\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04332805809378624\n",
      "Average test loss: 0.0033034053465558423\n",
      "Epoch 269/300\n",
      "Average training loss: 0.043540411574973\n",
      "Average test loss: 0.003318517916318443\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04351579456859165\n",
      "Average test loss: 0.003263241193464233\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04326075204544597\n",
      "Average test loss: 0.0032885527163743974\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04342602618204223\n",
      "Average test loss: 0.0873931867380937\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04404096665316158\n",
      "Average test loss: 0.0033161332069171798\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04327493485477236\n",
      "Average test loss: 0.009576120837281148\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04336813296543227\n",
      "Average test loss: 0.008390292805929978\n",
      "Epoch 276/300\n",
      "Average training loss: 0.043265818890598086\n",
      "Average test loss: 0.003303966386243701\n",
      "Epoch 277/300\n",
      "Average training loss: 0.043199485705958475\n",
      "Average test loss: 0.0036132602981395192\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0433089169230726\n",
      "Average test loss: 0.0037352664897011387\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04337589219874806\n",
      "Average test loss: 0.004099956640973687\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04339530088173019\n",
      "Average test loss: 0.003326889896972312\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04317137524816725\n",
      "Average test loss: 0.0033080878379858204\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04320890999833743\n",
      "Average test loss: 0.012622754868533877\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04321170862515768\n",
      "Average test loss: 0.0033428047158651883\n",
      "Epoch 284/300\n",
      "Average training loss: 0.043195543825626376\n",
      "Average test loss: 0.0032304402082744574\n",
      "Epoch 285/300\n",
      "Average training loss: 0.043172007120317886\n",
      "Average test loss: 0.0034747741491430333\n",
      "Epoch 286/300\n",
      "Average training loss: 0.043598695105976526\n",
      "Average test loss: 0.005424536043985023\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04304062153233422\n",
      "Average test loss: 0.004766406623439657\n",
      "Epoch 288/300\n",
      "Average training loss: 0.043010093106163876\n",
      "Average test loss: 0.0033361656847927306\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04325141592158212\n",
      "Average test loss: 0.015185887963200608\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04366855858431922\n",
      "Average test loss: 0.0033119540364584987\n",
      "Epoch 291/300\n",
      "Average training loss: 0.042972287482685514\n",
      "Average test loss: 0.0033353235262135665\n",
      "Epoch 292/300\n",
      "Average training loss: 0.043097052690055634\n",
      "Average test loss: 0.003417461222037673\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0431116094523006\n",
      "Average test loss: 0.0033041549772024155\n",
      "Epoch 294/300\n",
      "Average training loss: 0.043076811449395284\n",
      "Average test loss: 0.0033675208983735905\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0432445302175151\n",
      "Average test loss: 0.003334952969517973\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04307492409149806\n",
      "Average test loss: 0.0034499476378162704\n",
      "Epoch 297/300\n",
      "Average training loss: 0.043037171641985575\n",
      "Average test loss: 0.0032632372081279754\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04305575968821843\n",
      "Average test loss: 0.0033627378892981345\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04341650030679173\n",
      "Average test loss: 0.0037516087506794266\n",
      "Epoch 300/300\n",
      "Average training loss: 0.042947432180245715\n",
      "Average test loss: 0.00364521369131075\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.4402108212576972\n",
      "Average test loss: 0.005766313462207714\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4611269679069519\n",
      "Average test loss: 0.004748334053903818\n",
      "Epoch 3/300\n",
      "Average training loss: 0.3038624154726664\n",
      "Average test loss: 0.00431262335802118\n",
      "Epoch 4/300\n",
      "Average training loss: 0.22759909023178948\n",
      "Average test loss: 0.004223694756834043\n",
      "Epoch 5/300\n",
      "Average training loss: 0.18328262751632265\n",
      "Average test loss: 0.0039045030085576907\n",
      "Epoch 6/300\n",
      "Average training loss: 0.153534946375423\n",
      "Average test loss: 0.003774107656131188\n",
      "Epoch 7/300\n",
      "Average training loss: 0.13328603682253096\n",
      "Average test loss: 0.005118064989646276\n",
      "Epoch 8/300\n",
      "Average training loss: 0.11851581050290001\n",
      "Average test loss: 0.0037861256297263834\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10743433521853553\n",
      "Average test loss: 0.0033849888377719454\n",
      "Epoch 10/300\n",
      "Average training loss: 0.09928639570871989\n",
      "Average test loss: 0.004168709892779589\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09287766658597522\n",
      "Average test loss: 0.0031984214124580225\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0875047158267763\n",
      "Average test loss: 0.003299353738418884\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08275323640637927\n",
      "Average test loss: 0.0031368077151063415\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07878382883138127\n",
      "Average test loss: 0.003155986299738288\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07556481399138769\n",
      "Average test loss: 0.0029438549095971715\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07288948637578223\n",
      "Average test loss: 0.003426522644650605\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0697138375904825\n",
      "Average test loss: 0.0030229821194791132\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06784164971113205\n",
      "Average test loss: 0.0029303380880090924\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06546395403146744\n",
      "Average test loss: 0.003124470499654611\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06337523423300849\n",
      "Average test loss: 0.0033317094759808645\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06164231323533588\n",
      "Average test loss: 0.0029361342880874874\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06014395109481282\n",
      "Average test loss: 0.002742556663437022\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05860890256696277\n",
      "Average test loss: 0.06163596475455496\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05678142805894216\n",
      "Average test loss: 0.002967895746645\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05566894302103254\n",
      "Average test loss: 0.002623355354906784\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05418671248356501\n",
      "Average test loss: 0.0027777849162618318\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0530451442764865\n",
      "Average test loss: 0.002593080672952864\n",
      "Epoch 28/300\n",
      "Average training loss: 0.051662352406316336\n",
      "Average test loss: 0.0030277274565564264\n",
      "Epoch 29/300\n",
      "Average training loss: 0.050678269065088696\n",
      "Average test loss: 0.002727083800981442\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0498038439684444\n",
      "Average test loss: 0.0025525634897251926\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04919338106446796\n",
      "Average test loss: 0.0025578793541838727\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04856894114282396\n",
      "Average test loss: 0.0025197468319286902\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04788883043328921\n",
      "Average test loss: 0.0025095713978840247\n",
      "Epoch 34/300\n",
      "Average training loss: 0.047426633516947425\n",
      "Average test loss: 0.0024920495924436385\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04701695055100653\n",
      "Average test loss: 0.002716158879092998\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04654997294147809\n",
      "Average test loss: 0.0024655168656673694\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04612656781077385\n",
      "Average test loss: 0.0024958819440669485\n",
      "Epoch 38/300\n",
      "Average training loss: 0.045802424189117216\n",
      "Average test loss: 0.0024809848378515905\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0454783500019047\n",
      "Average test loss: 0.0024477714140796\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04528251666492886\n",
      "Average test loss: 0.002691220142775112\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0450670922199885\n",
      "Average test loss: 0.0024769561965432433\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04468424992760023\n",
      "Average test loss: 0.0024585796985775233\n",
      "Epoch 43/300\n",
      "Average training loss: 0.044498997383647494\n",
      "Average test loss: 0.002449432383187943\n",
      "Epoch 44/300\n",
      "Average training loss: 0.044391269511646694\n",
      "Average test loss: 0.004625467259850767\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04422399468223254\n",
      "Average test loss: 0.002483061524315013\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04402887949347496\n",
      "Average test loss: 0.002779232419406374\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0436370769739151\n",
      "Average test loss: 0.0025535756788320012\n",
      "Epoch 48/300\n",
      "Average training loss: 0.043692184971438515\n",
      "Average test loss: 0.002466429060842428\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04343488209280703\n",
      "Average test loss: 0.005075597406985859\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04331662538150946\n",
      "Average test loss: 0.002600161637696955\n",
      "Epoch 51/300\n",
      "Average training loss: 0.043100123137235644\n",
      "Average test loss: 0.002416591273413764\n",
      "Epoch 52/300\n",
      "Average training loss: 0.043035308178928165\n",
      "Average test loss: 0.020393107695711985\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04302026814222336\n",
      "Average test loss: 0.0025066332726014985\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0426435384021865\n",
      "Average test loss: 0.00246358888120287\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04260647229684724\n",
      "Average test loss: 0.0024130965409179528\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04299258618222342\n",
      "Average test loss: 0.002452366064199143\n",
      "Epoch 57/300\n",
      "Average training loss: 0.042312054450313254\n",
      "Average test loss: 0.0024260691344324087\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04224569302797317\n",
      "Average test loss: 0.002469212892361813\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04216513207389249\n",
      "Average test loss: 0.0024232478040373986\n",
      "Epoch 60/300\n",
      "Average training loss: 0.042321170532041125\n",
      "Average test loss: 0.0024246360692712996\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04189882246653239\n",
      "Average test loss: 0.0029673261584507093\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04181616637772984\n",
      "Average test loss: 0.003916114527318213\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04188885372214847\n",
      "Average test loss: 0.0028254040833562612\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04170393074883355\n",
      "Average test loss: 0.0024392806275023355\n",
      "Epoch 65/300\n",
      "Average training loss: 0.041614430530203715\n",
      "Average test loss: 0.002430252236003677\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04160245258278317\n",
      "Average test loss: 0.0024178628884255885\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04159506523278025\n",
      "Average test loss: 0.002554212811920378\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0412792293826739\n",
      "Average test loss: 0.0028818860337552097\n",
      "Epoch 69/300\n",
      "Average training loss: 0.041326265470849145\n",
      "Average test loss: 0.0030663218016011846\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04124102711346414\n",
      "Average test loss: 0.014051839485764503\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04115542116430071\n",
      "Average test loss: 0.0024117053054894012\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0409826316303677\n",
      "Average test loss: 0.002557790451786584\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04142616977294286\n",
      "Average test loss: 0.0025641837850626974\n",
      "Epoch 74/300\n",
      "Average training loss: 0.040896236860089835\n",
      "Average test loss: 0.0025461456436249945\n",
      "Epoch 75/300\n",
      "Average training loss: 0.040746954871548544\n",
      "Average test loss: 0.002502594803356462\n",
      "Epoch 76/300\n",
      "Average training loss: 0.040724440945519344\n",
      "Average test loss: 0.0024768865942541097\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04064615339371893\n",
      "Average test loss: 0.0026521186476780307\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04078019542826546\n",
      "Average test loss: 0.0024639749868462482\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04052652135491371\n",
      "Average test loss: 0.004654797701372041\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04050670759545432\n",
      "Average test loss: 0.006682031066467364\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04047352830900086\n",
      "Average test loss: 0.003652520428515143\n",
      "Epoch 82/300\n",
      "Average training loss: 0.040325580133332144\n",
      "Average test loss: 0.0024440476588077015\n",
      "Epoch 83/300\n",
      "Average training loss: 0.040245712070001494\n",
      "Average test loss: 0.002464117469679978\n",
      "Epoch 84/300\n",
      "Average training loss: 0.041858455505636\n",
      "Average test loss: 0.002477452295521895\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04023741007844607\n",
      "Average test loss: 0.002548289459819595\n",
      "Epoch 86/300\n",
      "Average training loss: 0.039998139994011986\n",
      "Average test loss: 0.0026294274744060302\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03995913053386741\n",
      "Average test loss: 0.0026322975028306244\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03992784666187233\n",
      "Average test loss: 0.0035748973567452697\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04008231354090903\n",
      "Average test loss: 0.002438529791103469\n",
      "Epoch 90/300\n",
      "Average training loss: 0.039834354261557264\n",
      "Average test loss: 0.0024662055081377428\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04003151527047157\n",
      "Average test loss: 0.002474826182342238\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03969851279258728\n",
      "Average test loss: 0.0029698874356432095\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03968999373912811\n",
      "Average test loss: 0.0024947939736561643\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03968321693605847\n",
      "Average test loss: 0.005071615672773785\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03959439799851842\n",
      "Average test loss: 0.002527296054797868\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04010751370920075\n",
      "Average test loss: 0.002478918978944421\n",
      "Epoch 97/300\n",
      "Average training loss: 0.039469808667898176\n",
      "Average test loss: 0.0032867861837148665\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03941086367103789\n",
      "Average test loss: 0.002519675714481208\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03949761438038614\n",
      "Average test loss: 0.0027555099707096816\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03943078527185652\n",
      "Average test loss: 0.002531083251039187\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03923743349313736\n",
      "Average test loss: 0.0024730695214950377\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03926347351405356\n",
      "Average test loss: 0.0024644220218890244\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03922095017300712\n",
      "Average test loss: 0.002918182684832977\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0391793344285753\n",
      "Average test loss: 0.0025870027426216336\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03924318370554182\n",
      "Average test loss: 0.0025663270275625917\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03904829243487782\n",
      "Average test loss: 0.002467110617293252\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03930509029825528\n",
      "Average test loss: 0.0025017561190244226\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03900118660926819\n",
      "Average test loss: 0.0032323434520512818\n",
      "Epoch 109/300\n",
      "Average training loss: 0.039923273513714476\n",
      "Average test loss: 0.0024766249599763087\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0388105552809106\n",
      "Average test loss: 0.002592968695693546\n",
      "Epoch 111/300\n",
      "Average training loss: 0.038753465082910325\n",
      "Average test loss: 0.0024711361713707445\n",
      "Epoch 112/300\n",
      "Average training loss: 0.038916625027855235\n",
      "Average test loss: 0.0024919842773427564\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03887629423207707\n",
      "Average test loss: 0.005990920855353276\n",
      "Epoch 114/300\n",
      "Average training loss: 0.038722292297416264\n",
      "Average test loss: 0.0024619729372983177\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03873292295800315\n",
      "Average test loss: 0.002503986623138189\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03869421299960878\n",
      "Average test loss: 0.0420622177388933\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03873903963632054\n",
      "Average test loss: 0.0025989192068162892\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03859267223212454\n",
      "Average test loss: 0.002752425548724002\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03860534092121654\n",
      "Average test loss: 0.00248531048041251\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03843908861941762\n",
      "Average test loss: 0.0025181505986385877\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03856018487943543\n",
      "Average test loss: 0.002630035903511776\n",
      "Epoch 122/300\n",
      "Average training loss: 0.038875379466348225\n",
      "Average test loss: 0.002766429677191708\n",
      "Epoch 123/300\n",
      "Average training loss: 0.038339712020423675\n",
      "Average test loss: 0.002536590668062369\n",
      "Epoch 124/300\n",
      "Average training loss: 0.038386242053574986\n",
      "Average test loss: 0.0025957882617496783\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03884131988055176\n",
      "Average test loss: 0.0025837765745818616\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03826829718550046\n",
      "Average test loss: 0.0027187830253193775\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03821900729007191\n",
      "Average test loss: 0.0025319797160724797\n",
      "Epoch 128/300\n",
      "Average training loss: 0.038383005284600785\n",
      "Average test loss: 0.002657676006977757\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03820371380448341\n",
      "Average test loss: 0.002570764265126652\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03813981468809975\n",
      "Average test loss: 0.002571604573685262\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03826648010147943\n",
      "Average test loss: 0.00254224828692774\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03806263843178749\n",
      "Average test loss: 0.002546140382687251\n",
      "Epoch 133/300\n",
      "Average training loss: 0.038029384846488636\n",
      "Average test loss: 0.0030798463159137303\n",
      "Epoch 134/300\n",
      "Average training loss: 0.038158713344070644\n",
      "Average test loss: 0.003297030067278279\n",
      "Epoch 135/300\n",
      "Average training loss: 0.038058586693472334\n",
      "Average test loss: 0.0031757173269159264\n",
      "Epoch 136/300\n",
      "Average training loss: 0.038034623384475705\n",
      "Average test loss: 0.0030447055221431785\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03803415788544549\n",
      "Average test loss: 0.005248944378147523\n",
      "Epoch 138/300\n",
      "Average training loss: 0.038129653655820425\n",
      "Average test loss: 0.006476358531456855\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03792502725786633\n",
      "Average test loss: 0.002509566908288333\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03790503097573916\n",
      "Average test loss: 0.002490809689379401\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03790896389219496\n",
      "Average test loss: 0.003439301576879289\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03857478548751937\n",
      "Average test loss: 0.0024758355906233194\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037672957380612694\n",
      "Average test loss: 0.0029708386696875094\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03771373786197768\n",
      "Average test loss: 0.002634859573096037\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03770174378818936\n",
      "Average test loss: 0.0026733491972295775\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03774310111999512\n",
      "Average test loss: 0.002521992100609673\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03851979786819882\n",
      "Average test loss: 0.002527067858311865\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03753057989478111\n",
      "Average test loss: 0.002569844552833173\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0375682551248206\n",
      "Average test loss: 0.002868710216548708\n",
      "Epoch 150/300\n",
      "Average training loss: 0.037570585856835044\n",
      "Average test loss: 0.0026676195316637555\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03773328270845943\n",
      "Average test loss: 0.002528682052054339\n",
      "Epoch 152/300\n",
      "Average training loss: 0.037577650361590914\n",
      "Average test loss: 0.0025419794919176236\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03787715640001827\n",
      "Average test loss: 0.003533772755621208\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03799729691611396\n",
      "Average test loss: 0.002503686983552244\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03736122160487705\n",
      "Average test loss: 0.002894821050059464\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0374345303343402\n",
      "Average test loss: 0.0025373993994047246\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03748449784186151\n",
      "Average test loss: 0.002883300807947914\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03774409006370438\n",
      "Average test loss: 0.00256405576897992\n",
      "Epoch 159/300\n",
      "Average training loss: 0.037321104003323446\n",
      "Average test loss: 0.0025749761569831106\n",
      "Epoch 160/300\n",
      "Average training loss: 0.037357412497202556\n",
      "Average test loss: 0.0026678764718688196\n",
      "Epoch 161/300\n",
      "Average training loss: 0.037477670275502736\n",
      "Average test loss: 0.0025500093292858865\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03743790307972166\n",
      "Average test loss: 0.0026425243669913876\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03724318578508165\n",
      "Average test loss: 0.39000675719314154\n",
      "Epoch 164/300\n",
      "Average training loss: 0.037394832246833376\n",
      "Average test loss: 0.002654324791704615\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03733335280087259\n",
      "Average test loss: 0.0025117483656439516\n",
      "Epoch 166/300\n",
      "Average training loss: 0.037231043972902826\n",
      "Average test loss: 0.00255128592501084\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0372650705575943\n",
      "Average test loss: 0.004792598589426942\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03730517752634154\n",
      "Average test loss: 0.0027621569658319157\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03720869583388169\n",
      "Average test loss: 0.0028882237403757044\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03732328470879131\n",
      "Average test loss: 0.0026568508257882465\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03712304213311937\n",
      "Average test loss: 0.0026581343894617423\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03712445584932963\n",
      "Average test loss: 0.0026153427441087036\n",
      "Epoch 173/300\n",
      "Average training loss: 0.037202439351214305\n",
      "Average test loss: 0.002716985731075207\n",
      "Epoch 174/300\n",
      "Average training loss: 0.037140255759159725\n",
      "Average test loss: 0.0025726927613011663\n",
      "Epoch 175/300\n",
      "Average training loss: 0.037214538047711056\n",
      "Average test loss: 0.002752617641041676\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03698057638936573\n",
      "Average test loss: 0.003132933605876234\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03705923233429591\n",
      "Average test loss: 0.0026270111886163554\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03704579557809565\n",
      "Average test loss: 0.002539297819551494\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03709780335095194\n",
      "Average test loss: 0.002553489850420091\n",
      "Epoch 180/300\n",
      "Average training loss: 0.036935467498170005\n",
      "Average test loss: 0.0026587183198167217\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03701462415522999\n",
      "Average test loss: 0.00326580644150575\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03717094240254826\n",
      "Average test loss: 0.0025754308491531346\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03721247309115198\n",
      "Average test loss: 0.0035381361444791156\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03687456054488818\n",
      "Average test loss: 0.003654934979354342\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03687940925690863\n",
      "Average test loss: 0.0032429446948485244\n",
      "Epoch 186/300\n",
      "Average training loss: 0.036899451399842895\n",
      "Average test loss: 0.002616728304160966\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03687905087404781\n",
      "Average test loss: 0.00313908609499534\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03771615258521504\n",
      "Average test loss: 0.0025472703075243366\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03677568328049448\n",
      "Average test loss: 0.0026002230940179692\n",
      "Epoch 190/300\n",
      "Average training loss: 0.036713481856717\n",
      "Average test loss: 0.002681817016046908\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03681737474269337\n",
      "Average test loss: 0.002935047835111618\n",
      "Epoch 192/300\n",
      "Average training loss: 0.036762201494640774\n",
      "Average test loss: 0.002672130067522327\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03696103027794096\n",
      "Average test loss: 0.0025459988500095075\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03661784342096912\n",
      "Average test loss: 0.002691346619692114\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03671522891852591\n",
      "Average test loss: 0.0026773856861723793\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03671480529838138\n",
      "Average test loss: 0.0027908834125846623\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03682991210950746\n",
      "Average test loss: 0.0026175517804092833\n",
      "Epoch 198/300\n",
      "Average training loss: 0.036604563805792065\n",
      "Average test loss: 0.0027830960070714355\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0367546345392863\n",
      "Average test loss: 0.0028387134389744866\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03656996343864335\n",
      "Average test loss: 0.0026833640682614513\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03690351286861632\n",
      "Average test loss: 0.0026609814057333603\n",
      "Epoch 202/300\n",
      "Average training loss: 0.036642342256175145\n",
      "Average test loss: 0.002700414767695798\n",
      "Epoch 203/300\n",
      "Average training loss: 0.036522370951043236\n",
      "Average test loss: 0.0032576842688851887\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03664193807376755\n",
      "Average test loss: 0.002826150989780823\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03665410929090447\n",
      "Average test loss: 0.005190685137278504\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03656000372436311\n",
      "Average test loss: 0.0025949600042982236\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03650418839024173\n",
      "Average test loss: 0.00277225961515473\n",
      "Epoch 208/300\n",
      "Average training loss: 0.051098124525613256\n",
      "Average test loss: 0.0028826120679991112\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04115907179315885\n",
      "Average test loss: 0.002679950053907103\n",
      "Epoch 210/300\n",
      "Average training loss: 0.038341537104712595\n",
      "Average test loss: 0.002764317331628667\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03714663300911585\n",
      "Average test loss: 0.0025976990647614004\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03661396507753266\n",
      "Average test loss: 0.0026595371425565746\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03643835899399386\n",
      "Average test loss: 0.0038221320029762054\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03640047738618321\n",
      "Average test loss: 0.0026856207162555723\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03645499301287863\n",
      "Average test loss: 0.002642038365205129\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03642132935921351\n",
      "Average test loss: 0.0026355923821942675\n",
      "Epoch 217/300\n",
      "Average training loss: 0.036473756475581064\n",
      "Average test loss: 0.0029936191168510253\n",
      "Epoch 218/300\n",
      "Average training loss: 0.036458100373546284\n",
      "Average test loss: 0.0026247194098929566\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03649210587806172\n",
      "Average test loss: 0.0026338848697228566\n",
      "Epoch 220/300\n",
      "Average training loss: 0.036464591864082546\n",
      "Average test loss: 0.0025768717761254973\n",
      "Epoch 221/300\n",
      "Average training loss: 0.036590028764473065\n",
      "Average test loss: 0.0025866003452489775\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03644031309750345\n",
      "Average test loss: 0.0026270551366938486\n",
      "Epoch 223/300\n",
      "Average training loss: 0.036299205674065485\n",
      "Average test loss: 0.002750190095355113\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03639109583364593\n",
      "Average test loss: 0.002591041884281569\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03639222950736681\n",
      "Average test loss: 0.0027231053076684475\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03636402541067865\n",
      "Average test loss: 0.002896946827156676\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03817051559024387\n",
      "Average test loss: 0.0026005863808095456\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03613124670419428\n",
      "Average test loss: 0.002680037503855096\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03618917270832592\n",
      "Average test loss: 0.002765842426982191\n",
      "Epoch 230/300\n",
      "Average training loss: 0.036173560481932425\n",
      "Average test loss: 0.004726640774557988\n",
      "Epoch 231/300\n",
      "Average training loss: 0.036247291399372945\n",
      "Average test loss: 0.0026672263527289033\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03643873069021437\n",
      "Average test loss: 0.0032552022646284767\n",
      "Epoch 233/300\n",
      "Average training loss: 0.036207581490278244\n",
      "Average test loss: 0.002595464559685853\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03620844182372093\n",
      "Average test loss: 0.4299911547799905\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03624939821826087\n",
      "Average test loss: 0.002588042575555543\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03638179341289732\n",
      "Average test loss: 0.005634752284942402\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03627073785662651\n",
      "Average test loss: 0.002638597106974986\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03610827547311783\n",
      "Average test loss: 0.00266235425716473\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03613628563947148\n",
      "Average test loss: 0.0026558508781923187\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0391393992834621\n",
      "Average test loss: 0.0029208849817514418\n",
      "Epoch 241/300\n",
      "Average training loss: 0.035961279110776054\n",
      "Average test loss: 0.002643979904655781\n",
      "Epoch 242/300\n",
      "Average training loss: 0.035971040161119565\n",
      "Average test loss: 0.0026924662289934025\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03603603334890471\n",
      "Average test loss: 0.002914381992485788\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03635602280828688\n",
      "Average test loss: 0.002588537316562401\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0361407583206892\n",
      "Average test loss: 0.0026403993132213754\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0360833290318648\n",
      "Average test loss: 0.002636005599051714\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03616694140434265\n",
      "Average test loss: 0.002885170030304127\n",
      "Epoch 248/300\n",
      "Average training loss: 0.036119050612052284\n",
      "Average test loss: 0.004892759237645401\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03606958387295405\n",
      "Average test loss: 0.002976186253544357\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0361299349963665\n",
      "Average test loss: 0.002601201901419295\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03693339649339517\n",
      "Average test loss: 0.0028230464990354246\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03593851403395335\n",
      "Average test loss: 0.002806527538742456\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03593084616462389\n",
      "Average test loss: 0.0026189886457804175\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03601783181561364\n",
      "Average test loss: 0.22418867940041753\n",
      "Epoch 255/300\n",
      "Average training loss: 0.036027507142888174\n",
      "Average test loss: 0.0028058373536914588\n",
      "Epoch 256/300\n",
      "Average training loss: 0.035981850836012096\n",
      "Average test loss: 0.0025749206751998926\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03769925210873286\n",
      "Average test loss: 0.003243246028199792\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03579722429646386\n",
      "Average test loss: 0.002600222755430473\n",
      "Epoch 259/300\n",
      "Average training loss: 0.035819812731610404\n",
      "Average test loss: 0.002585932388073868\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03588493032256762\n",
      "Average test loss: 0.002834243156429794\n",
      "Epoch 261/300\n",
      "Average training loss: 0.035998518890804715\n",
      "Average test loss: 0.0026235252355949748\n",
      "Epoch 262/300\n",
      "Average training loss: 0.035925078183412555\n",
      "Average test loss: 0.003511386627538337\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03675629508164194\n",
      "Average test loss: 0.00259842027641005\n",
      "Epoch 264/300\n",
      "Average training loss: 0.035781197319428124\n",
      "Average test loss: 0.004068852306240134\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03589262643787596\n",
      "Average test loss: 0.00285404926021066\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03583994490404924\n",
      "Average test loss: 0.002656265625109275\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03610170876483122\n",
      "Average test loss: 0.002611467671684093\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0358123002714581\n",
      "Average test loss: 0.0028918025007264483\n",
      "Epoch 269/300\n",
      "Average training loss: 0.035957933477229545\n",
      "Average test loss: 0.0027643107107530036\n",
      "Epoch 270/300\n",
      "Average training loss: 0.036046235354410275\n",
      "Average test loss: 0.002612299372959468\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03577614880767133\n",
      "Average test loss: 0.0027341216910216543\n",
      "Epoch 272/300\n",
      "Average training loss: 0.035795013782050876\n",
      "Average test loss: 0.002763913766584463\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03591875498162376\n",
      "Average test loss: 0.003462051353106896\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03578616124722693\n",
      "Average test loss: 0.002608841598447826\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03583417337470585\n",
      "Average test loss: 0.0027375932857394217\n",
      "Epoch 276/300\n",
      "Average training loss: 0.035904767162270014\n",
      "Average test loss: 0.002761246581044462\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0358551532841391\n",
      "Average test loss: 0.0026478142829404936\n",
      "Epoch 278/300\n",
      "Average training loss: 0.035772968149847456\n",
      "Average test loss: 0.002625992982958754\n",
      "Epoch 279/300\n",
      "Average training loss: 0.035857483095592925\n",
      "Average test loss: 0.004368957856876982\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03571614156166712\n",
      "Average test loss: 0.002594764994457364\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03577439993951056\n",
      "Average test loss: 0.002575960063479013\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03584909073180623\n",
      "Average test loss: 0.0029484252379172377\n",
      "Epoch 283/300\n",
      "Average training loss: 0.035774525665574604\n",
      "Average test loss: 0.00875368815743261\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03621783040795061\n",
      "Average test loss: 0.002655403570168548\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03557576051685545\n",
      "Average test loss: 0.0026340891077286667\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03572091203927994\n",
      "Average test loss: 0.0031500227836271126\n",
      "Epoch 287/300\n",
      "Average training loss: 0.035605910589297615\n",
      "Average test loss: 0.002614079553426968\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03765530744857258\n",
      "Average test loss: 0.002588783748861816\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03549972778889868\n",
      "Average test loss: 0.0026372227559073105\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0355034225847986\n",
      "Average test loss: 0.0036493988575206863\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03557892113592889\n",
      "Average test loss: 0.002607798045501113\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03560153966810968\n",
      "Average test loss: 0.0033592114481661053\n",
      "Epoch 293/300\n",
      "Average training loss: 0.035611105567879144\n",
      "Average test loss: 0.0028406346721781624\n",
      "Epoch 294/300\n",
      "Average training loss: 0.035675278978215326\n",
      "Average test loss: 0.04523044780062305\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03602721379366186\n",
      "Average test loss: 0.0027305258210334514\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03548901171651152\n",
      "Average test loss: 0.0030247997554639977\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03556159677770403\n",
      "Average test loss: 0.0029605371225625275\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03563800342877706\n",
      "Average test loss: 0.002570455720027288\n",
      "Epoch 299/300\n",
      "Average training loss: 0.037015193618006176\n",
      "Average test loss: 0.002602505203957359\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0354208791355292\n",
      "Average test loss: 0.002836180173067583\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.35\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.95\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.10\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.17\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.33\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.40\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.42\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.60\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 24.66\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.68\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 24.69\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.32\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.89\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.34\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.57\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.65\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.73\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.94\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.45\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.42\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.68\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.61\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.48\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.62\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.636712245517307\n",
      "Average test loss: 0.013690913281506963\n",
      "Epoch 2/300\n",
      "Average training loss: 7.763323093414306\n",
      "Average test loss: 0.01102022772282362\n",
      "Epoch 3/300\n",
      "Average training loss: 5.836600591023763\n",
      "Average test loss: 0.010364015301068624\n",
      "Epoch 4/300\n",
      "Average training loss: 4.832971425798204\n",
      "Average test loss: 0.010328171860012744\n",
      "Epoch 5/300\n",
      "Average training loss: 3.9838033746083577\n",
      "Average test loss: 0.009303914826777246\n",
      "Epoch 6/300\n",
      "Average training loss: 3.234006671481662\n",
      "Average test loss: 0.008704828222592671\n",
      "Epoch 7/300\n",
      "Average training loss: 2.835748350567288\n",
      "Average test loss: 0.009614075902435515\n",
      "Epoch 8/300\n",
      "Average training loss: 2.321161980946859\n",
      "Average test loss: 0.008207414374997219\n",
      "Epoch 9/300\n",
      "Average training loss: 1.9815482142766316\n",
      "Average test loss: 0.00884528438333008\n",
      "Epoch 10/300\n",
      "Average training loss: 1.6794004350238376\n",
      "Average test loss: 0.008295783472557862\n",
      "Epoch 11/300\n",
      "Average training loss: 1.3989231530295478\n",
      "Average test loss: 0.009510583724826575\n",
      "Epoch 12/300\n",
      "Average training loss: 1.1774561593797472\n",
      "Average test loss: 0.007874205154677232\n",
      "Epoch 13/300\n",
      "Average training loss: 1.0155853155983818\n",
      "Average test loss: 0.007735510275595718\n",
      "Epoch 14/300\n",
      "Average training loss: 0.8774324031405979\n",
      "Average test loss: 0.007667272752357854\n",
      "Epoch 15/300\n",
      "Average training loss: 0.7587009167671204\n",
      "Average test loss: 0.00790826900758677\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6654123436080085\n",
      "Average test loss: 0.010122470881376001\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5905420050621033\n",
      "Average test loss: 0.007210826701588101\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5348195486068725\n",
      "Average test loss: 0.007377777440266476\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4891106783548991\n",
      "Average test loss: 0.0212468139015966\n",
      "Epoch 20/300\n",
      "Average training loss: 0.4502022693157196\n",
      "Average test loss: 0.007019935120311048\n",
      "Epoch 21/300\n",
      "Average training loss: 0.4182147692574395\n",
      "Average test loss: 0.008845140925712055\n",
      "Epoch 22/300\n",
      "Average training loss: 0.3914743960698446\n",
      "Average test loss: 0.006720977860606379\n",
      "Epoch 23/300\n",
      "Average training loss: 0.36710897970199585\n",
      "Average test loss: 0.008038346954517894\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3484425217045678\n",
      "Average test loss: 0.0070622509664131534\n",
      "Epoch 25/300\n",
      "Average training loss: 0.3305715123547448\n",
      "Average test loss: 0.006650950656996833\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3160829082330068\n",
      "Average test loss: 0.0074070740739504495\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3030048577255673\n",
      "Average test loss: 0.00721427049032516\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2921440533796946\n",
      "Average test loss: 0.009604802177184157\n",
      "Epoch 29/300\n",
      "Average training loss: 0.28224851789739397\n",
      "Average test loss: 0.006516690387494035\n",
      "Epoch 30/300\n",
      "Average training loss: 0.27460363086064654\n",
      "Average test loss: 0.006937970694982343\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2673372070789337\n",
      "Average test loss: 0.0069524554713732666\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2614732080830468\n",
      "Average test loss: 0.006594922403494517\n",
      "Epoch 33/300\n",
      "Average training loss: 0.2553667714065976\n",
      "Average test loss: 0.006545510075986386\n",
      "Epoch 34/300\n",
      "Average training loss: 0.24955948254797194\n",
      "Average test loss: 0.012003579639726215\n",
      "Epoch 35/300\n",
      "Average training loss: 0.24365705634488\n",
      "Average test loss: 0.006450612221327093\n",
      "Epoch 36/300\n",
      "Average training loss: 0.23955611262056561\n",
      "Average test loss: 0.006338642843067646\n",
      "Epoch 37/300\n",
      "Average training loss: 0.2368348403904173\n",
      "Average test loss: 0.006756283701707919\n",
      "Epoch 38/300\n",
      "Average training loss: 0.23216805737548404\n",
      "Average test loss: 0.0065375171349280415\n",
      "Epoch 39/300\n",
      "Average training loss: 0.22874592428737217\n",
      "Average test loss: 0.006483131799846888\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2262875488864051\n",
      "Average test loss: 0.006278948394788636\n",
      "Epoch 41/300\n",
      "Average training loss: 0.22319255509641436\n",
      "Average test loss: 0.008340835762934552\n",
      "Epoch 42/300\n",
      "Average training loss: 0.22139497939745584\n",
      "Average test loss: 0.006329630110826758\n",
      "Epoch 43/300\n",
      "Average training loss: 0.2181666776869032\n",
      "Average test loss: 0.0073994793407619\n",
      "Epoch 44/300\n",
      "Average training loss: 0.2167540793551339\n",
      "Average test loss: 6.29834018029107\n",
      "Epoch 45/300\n",
      "Average training loss: 0.2146447180642022\n",
      "Average test loss: 0.026919458121889166\n",
      "Epoch 46/300\n",
      "Average training loss: 0.21261374559667376\n",
      "Average test loss: 0.006223407494525115\n",
      "Epoch 47/300\n",
      "Average training loss: 0.21094728015528785\n",
      "Average test loss: 0.006432681659857432\n",
      "Epoch 48/300\n",
      "Average training loss: 0.20985167207982805\n",
      "Average test loss: 0.0062504256918198536\n",
      "Epoch 49/300\n",
      "Average training loss: 0.20841836133268143\n",
      "Average test loss: 136.02868446858724\n",
      "Epoch 50/300\n",
      "Average training loss: 0.20640785846445295\n",
      "Average test loss: 0.0069344546720385555\n",
      "Epoch 51/300\n",
      "Average training loss: 0.20551109176211887\n",
      "Average test loss: 0.006198753118928936\n",
      "Epoch 52/300\n",
      "Average training loss: 0.20404842338297102\n",
      "Average test loss: 0.006560477190547519\n",
      "Epoch 53/300\n",
      "Average training loss: 0.20274836283259923\n",
      "Average test loss: 0.006847259694503414\n",
      "Epoch 54/300\n",
      "Average training loss: 0.20212018079227873\n",
      "Average test loss: 0.006333685924609502\n",
      "Epoch 55/300\n",
      "Average training loss: 0.20084268539481692\n",
      "Average test loss: 0.006501324252121978\n",
      "Epoch 56/300\n",
      "Average training loss: 0.2000965552859836\n",
      "Average test loss: 0.0072468958575692445\n",
      "Epoch 57/300\n",
      "Average training loss: 0.19870440068509843\n",
      "Average test loss: 0.006122584649672111\n",
      "Epoch 58/300\n",
      "Average training loss: 0.200530869629648\n",
      "Average test loss: 0.0065411398224532605\n",
      "Epoch 59/300\n",
      "Average training loss: 0.19778534281253815\n",
      "Average test loss: 0.0061795307902826205\n",
      "Epoch 60/300\n",
      "Average training loss: 0.19693823957443238\n",
      "Average test loss: 0.00660108371410105\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1981163954337438\n",
      "Average test loss: 0.006381224147147603\n",
      "Epoch 62/300\n",
      "Average training loss: 0.19396203943093618\n",
      "Average test loss: 0.006483270432386133\n",
      "Epoch 63/300\n",
      "Average training loss: 0.19331475857893626\n",
      "Average test loss: 0.2288664087984297\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1933413678937488\n",
      "Average test loss: 0.006192876891129547\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1918873862690396\n",
      "Average test loss: 0.008091188568621874\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1918035212357839\n",
      "Average test loss: 0.006247689299285411\n",
      "Epoch 67/300\n",
      "Average training loss: 0.19249365618493822\n",
      "Average test loss: 0.006232220692146156\n",
      "Epoch 68/300\n",
      "Average training loss: 0.18987387376361423\n",
      "Average test loss: 0.006196971354385217\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1889742913246155\n",
      "Average test loss: 0.026761223554611205\n",
      "Epoch 70/300\n",
      "Average training loss: 0.18796115168597963\n",
      "Average test loss: 0.006255523987114429\n",
      "Epoch 71/300\n",
      "Average training loss: 0.18979201508892907\n",
      "Average test loss: 0.006445596014873849\n",
      "Epoch 72/300\n",
      "Average training loss: 0.18832352843549516\n",
      "Average test loss: 0.006428620522220929\n",
      "Epoch 73/300\n",
      "Average training loss: 0.18639462440543705\n",
      "Average test loss: 0.006774584173328347\n",
      "Epoch 74/300\n",
      "Average training loss: 0.18701785809463925\n",
      "Average test loss: 0.006243202419744597\n",
      "Epoch 75/300\n",
      "Average training loss: 0.18603322508600023\n",
      "Average test loss: 0.00617328781551785\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1846490893628862\n",
      "Average test loss: 0.006439357198360893\n",
      "Epoch 77/300\n",
      "Average training loss: 0.18499615865945815\n",
      "Average test loss: 0.006301224088917176\n",
      "Epoch 78/300\n",
      "Average training loss: 0.18367855054802365\n",
      "Average test loss: 0.006303921158115069\n",
      "Epoch 79/300\n",
      "Average training loss: 0.18496642514069875\n",
      "Average test loss: 0.8019083255198267\n",
      "Epoch 80/300\n",
      "Average training loss: 0.18807516021198697\n",
      "Average test loss: 0.006370486164258586\n",
      "Epoch 81/300\n",
      "Average training loss: 0.1823074077102873\n",
      "Average test loss: 0.007953380834725169\n",
      "Epoch 82/300\n",
      "Average training loss: 0.18093798791037666\n",
      "Average test loss: 0.006676893729302618\n",
      "Epoch 83/300\n",
      "Average training loss: 0.18050239698092144\n",
      "Average test loss: 0.010917294951362743\n",
      "Epoch 84/300\n",
      "Average training loss: 0.20865902332464853\n",
      "Average test loss: 0.0064675290791524785\n",
      "Epoch 85/300\n",
      "Average training loss: 0.187894410610199\n",
      "Average test loss: 0.006846444079445468\n",
      "Epoch 86/300\n",
      "Average training loss: 0.18067780154281193\n",
      "Average test loss: 0.00624428272454275\n",
      "Epoch 87/300\n",
      "Average training loss: 0.18003568959236144\n",
      "Average test loss: 0.0062728884518146516\n",
      "Epoch 88/300\n",
      "Average training loss: 0.17838241594367557\n",
      "Average test loss: 0.007000662710103724\n",
      "Epoch 89/300\n",
      "Average training loss: 0.17897104863325755\n",
      "Average test loss: 0.006873902485602432\n",
      "Epoch 90/300\n",
      "Average training loss: 0.17839029766453637\n",
      "Average test loss: 0.007182908900082111\n",
      "Epoch 91/300\n",
      "Average training loss: 0.17838203828864627\n",
      "Average test loss: 0.006252884590791331\n",
      "Epoch 92/300\n",
      "Average training loss: 0.17793229477935368\n",
      "Average test loss: 0.006774023385097583\n",
      "Epoch 93/300\n",
      "Average training loss: 0.17749340675936806\n",
      "Average test loss: 0.0063992218379345205\n",
      "Epoch 94/300\n",
      "Average training loss: 0.17812826273176405\n",
      "Average test loss: 0.006215162999927998\n",
      "Epoch 95/300\n",
      "Average training loss: 0.17590826296806336\n",
      "Average test loss: 0.007146391857001517\n",
      "Epoch 96/300\n",
      "Average training loss: 0.17691007117430368\n",
      "Average test loss: 0.0063031292102403115\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1758911672698127\n",
      "Average test loss: 0.00628519726337658\n",
      "Epoch 98/300\n",
      "Average training loss: 0.17611907601356505\n",
      "Average test loss: 0.007344292308307357\n",
      "Epoch 99/300\n",
      "Average training loss: 0.17494889126883612\n",
      "Average test loss: 0.0072637560338609745\n",
      "Epoch 100/300\n",
      "Average training loss: 0.1750506786108017\n",
      "Average test loss: 0.006434795661932892\n",
      "Epoch 101/300\n",
      "Average training loss: 0.174219011121326\n",
      "Average test loss: 0.006501888170010513\n",
      "Epoch 102/300\n",
      "Average training loss: 0.17788652679655287\n",
      "Average test loss: 0.006369122350795401\n",
      "Epoch 103/300\n",
      "Average training loss: 0.17241487550735474\n",
      "Average test loss: 0.006475363749182886\n",
      "Epoch 104/300\n",
      "Average training loss: 0.17226886769135794\n",
      "Average test loss: 0.008779965323292546\n",
      "Epoch 105/300\n",
      "Average training loss: 0.17499056957827674\n",
      "Average test loss: 0.006364614555405246\n",
      "Epoch 106/300\n",
      "Average training loss: 0.17119135407606761\n",
      "Average test loss: 0.0063804551764494845\n",
      "Epoch 107/300\n",
      "Average training loss: 0.17241790619161393\n",
      "Average test loss: 0.034939115862051645\n",
      "Epoch 108/300\n",
      "Average training loss: 0.17028480863571166\n",
      "Average test loss: 0.006505759331915113\n",
      "Epoch 109/300\n",
      "Average training loss: 0.17089316029018825\n",
      "Average test loss: 0.00703156416118145\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1715593507024977\n",
      "Average test loss: 0.006285743452608585\n",
      "Epoch 111/300\n",
      "Average training loss: 0.17036946490075852\n",
      "Average test loss: 0.007609964726699723\n",
      "Epoch 112/300\n",
      "Average training loss: 0.17021783966488307\n",
      "Average test loss: 0.007214032103617986\n",
      "Epoch 113/300\n",
      "Average training loss: 0.16963944874869452\n",
      "Average test loss: 0.007201838481757376\n",
      "Epoch 114/300\n",
      "Average training loss: 0.1700185206333796\n",
      "Average test loss: 0.0071691170839799775\n",
      "Epoch 115/300\n",
      "Average training loss: 0.1686371797323227\n",
      "Average test loss: 0.006588417959709963\n",
      "Epoch 116/300\n",
      "Average training loss: 0.16939498709307776\n",
      "Average test loss: 0.006452219467610121\n",
      "Epoch 117/300\n",
      "Average training loss: 0.16809667552842034\n",
      "Average test loss: 0.006488923545512888\n",
      "Epoch 118/300\n",
      "Average training loss: 3.628567440642251\n",
      "Average test loss: 0.02285155417356226\n",
      "Epoch 119/300\n",
      "Average training loss: 0.9199773904482523\n",
      "Average test loss: 0.006992277651818262\n",
      "Epoch 120/300\n",
      "Average training loss: 0.5618767879803975\n",
      "Average test loss: 0.006548025157302618\n",
      "Epoch 121/300\n",
      "Average training loss: 0.41833086530367536\n",
      "Average test loss: 0.006414525906244914\n",
      "Epoch 122/300\n",
      "Average training loss: 0.34865940578778587\n",
      "Average test loss: 0.0068924392718407845\n",
      "Epoch 123/300\n",
      "Average training loss: 0.30765021713574725\n",
      "Average test loss: 0.0073910074176059826\n",
      "Epoch 124/300\n",
      "Average training loss: 0.2795971632798513\n",
      "Average test loss: 0.006319968423081769\n",
      "Epoch 125/300\n",
      "Average training loss: 0.25989129388332366\n",
      "Average test loss: 0.006492508239630196\n",
      "Epoch 126/300\n",
      "Average training loss: 0.24672774293687608\n",
      "Average test loss: 0.006253038120766481\n",
      "Epoch 127/300\n",
      "Average training loss: 0.23704211245642767\n",
      "Average test loss: 0.006351359313560857\n",
      "Epoch 128/300\n",
      "Average training loss: 0.22889299555619558\n",
      "Average test loss: 0.006267897018127971\n",
      "Epoch 129/300\n",
      "Average training loss: 0.22277861796485054\n",
      "Average test loss: 0.006435913240744008\n",
      "Epoch 130/300\n",
      "Average training loss: 0.21638201048639086\n",
      "Average test loss: 0.006691135192496909\n",
      "Epoch 131/300\n",
      "Average training loss: 0.21157195093896655\n",
      "Average test loss: 0.0062690979979104465\n",
      "Epoch 132/300\n",
      "Average training loss: 0.20828997490141127\n",
      "Average test loss: 157.76663352613318\n",
      "Epoch 133/300\n",
      "Average training loss: 0.20393472680780622\n",
      "Average test loss: 0.006214035062326325\n",
      "Epoch 134/300\n",
      "Average training loss: 0.20128286627928416\n",
      "Average test loss: 0.006296854772915443\n",
      "Epoch 135/300\n",
      "Average training loss: 0.19769836240344577\n",
      "Average test loss: 0.0062359998437265555\n",
      "Epoch 136/300\n",
      "Average training loss: 0.1948272041214837\n",
      "Average test loss: 0.006233148479627238\n",
      "Epoch 137/300\n",
      "Average training loss: 0.19214746186468337\n",
      "Average test loss: 0.006381861998803085\n",
      "Epoch 138/300\n",
      "Average training loss: 0.18953485822677613\n",
      "Average test loss: 0.006168979367034303\n",
      "Epoch 139/300\n",
      "Average training loss: 0.18783437773916456\n",
      "Average test loss: 0.006388193441761864\n",
      "Epoch 140/300\n",
      "Average training loss: 0.18639570572641162\n",
      "Average test loss: 0.006617375954985618\n",
      "Epoch 141/300\n",
      "Average training loss: 0.18454596706231435\n",
      "Average test loss: 0.013012174314922756\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1849626093705495\n",
      "Average test loss: 0.007265268291036288\n",
      "Epoch 143/300\n",
      "Average training loss: 0.1791533303923077\n",
      "Average test loss: 0.006346627616633972\n",
      "Epoch 144/300\n",
      "Average training loss: 0.177249537203047\n",
      "Average test loss: 0.0072879074464241664\n",
      "Epoch 145/300\n",
      "Average training loss: 0.17733035597536298\n",
      "Average test loss: 0.006395004904932446\n",
      "Epoch 146/300\n",
      "Average training loss: 0.173808773179849\n",
      "Average test loss: 0.006482904811286264\n",
      "Epoch 147/300\n",
      "Average training loss: 0.17756226432323455\n",
      "Average test loss: 0.006381183310929272\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1726886434952418\n",
      "Average test loss: 0.006474630785485109\n",
      "Epoch 149/300\n",
      "Average training loss: 0.16945871637927162\n",
      "Average test loss: 0.00674897042537729\n",
      "Epoch 150/300\n",
      "Average training loss: 0.16886206461323633\n",
      "Average test loss: 0.006372795698957311\n",
      "Epoch 151/300\n",
      "Average training loss: 0.17100517569647894\n",
      "Average test loss: 0.0065339057652486696\n",
      "Epoch 152/300\n",
      "Average training loss: 0.1678425299194124\n",
      "Average test loss: 0.0071461148903601696\n",
      "Epoch 153/300\n",
      "Average training loss: 0.16834987578789393\n",
      "Average test loss: 0.00645921463974648\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1668487575451533\n",
      "Average test loss: 0.006731020401335425\n",
      "Epoch 155/300\n",
      "Average training loss: 0.16682610261440278\n",
      "Average test loss: 0.006774684333552917\n",
      "Epoch 156/300\n",
      "Average training loss: 0.16790982370906407\n",
      "Average test loss: 0.006473128962847922\n",
      "Epoch 157/300\n",
      "Average training loss: 0.16545272182756\n",
      "Average test loss: 0.006588549803942442\n",
      "Epoch 158/300\n",
      "Average training loss: 0.16552124845319324\n",
      "Average test loss: 0.006617582303782304\n",
      "Epoch 159/300\n",
      "Average training loss: 0.16792961593468983\n",
      "Average test loss: 0.0064112566651569475\n",
      "Epoch 160/300\n",
      "Average training loss: 0.16369629343350728\n",
      "Average test loss: 0.007583485425760349\n",
      "Epoch 161/300\n",
      "Average training loss: 0.16379973390367297\n",
      "Average test loss: 0.006766312186916669\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1634861571656333\n",
      "Average test loss: 0.007063953316046132\n",
      "Epoch 163/300\n",
      "Average training loss: 0.16541859743330214\n",
      "Average test loss: 0.006450663476354546\n",
      "Epoch 164/300\n",
      "Average training loss: 0.16474804306030275\n",
      "Average test loss: 0.4670537747674518\n",
      "Epoch 165/300\n",
      "Average training loss: 0.16525989525847964\n",
      "Average test loss: 0.006667794753279951\n",
      "Epoch 166/300\n",
      "Average training loss: 0.1616533434258567\n",
      "Average test loss: 0.006490063812583685\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1616764416297277\n",
      "Average test loss: 0.006561213293423255\n",
      "Epoch 168/300\n",
      "Average training loss: 0.16358090681499904\n",
      "Average test loss: 0.006524389279385407\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1611110439300537\n",
      "Average test loss: 0.006483901222546895\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1631896531979243\n",
      "Average test loss: 0.006574535694387224\n",
      "Epoch 171/300\n",
      "Average training loss: 0.1607770372364256\n",
      "Average test loss: 0.006567964000420438\n",
      "Epoch 172/300\n",
      "Average training loss: 0.609091174059444\n",
      "Average test loss: 0.007287630466123422\n",
      "Epoch 173/300\n",
      "Average training loss: 0.3260318197939131\n",
      "Average test loss: 0.006571046583768394\n",
      "Epoch 174/300\n",
      "Average training loss: 0.24356603580050998\n",
      "Average test loss: 0.0065044560842216015\n",
      "Epoch 175/300\n",
      "Average training loss: 0.22233220700422923\n",
      "Average test loss: 0.006249667577445507\n",
      "Epoch 176/300\n",
      "Average training loss: 0.21189188455210792\n",
      "Average test loss: 0.006502744946628809\n",
      "Epoch 177/300\n",
      "Average training loss: 0.20379782402515412\n",
      "Average test loss: 0.00645450225017137\n",
      "Epoch 178/300\n",
      "Average training loss: 0.19815024047427707\n",
      "Average test loss: 0.0062927306145429615\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1923786548640993\n",
      "Average test loss: 0.0063497028379804555\n",
      "Epoch 180/300\n",
      "Average training loss: 0.1884210031694836\n",
      "Average test loss: 0.006370859547621674\n",
      "Epoch 181/300\n",
      "Average training loss: 0.18370179979006449\n",
      "Average test loss: 0.006625086905227767\n",
      "Epoch 182/300\n",
      "Average training loss: 0.1793254806333118\n",
      "Average test loss: 0.006358091198735767\n",
      "Epoch 183/300\n",
      "Average training loss: 0.17442260432243348\n",
      "Average test loss: 0.0065324792112741205\n",
      "Epoch 184/300\n",
      "Average training loss: 0.17111251509189607\n",
      "Average test loss: 0.006451091555671559\n",
      "Epoch 185/300\n",
      "Average training loss: 0.16707390443483988\n",
      "Average test loss: 0.007018736343002981\n",
      "Epoch 186/300\n",
      "Average training loss: 0.1634967796007792\n",
      "Average test loss: 0.006396308934109078\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1621880852513843\n",
      "Average test loss: 0.00638449790287349\n",
      "Epoch 188/300\n",
      "Average training loss: 0.16357078323099347\n",
      "Average test loss: 0.007220151786175039\n",
      "Epoch 189/300\n",
      "Average training loss: 0.16041486934820812\n",
      "Average test loss: 13.927748470730252\n",
      "Epoch 190/300\n",
      "Average training loss: 0.16114689422978296\n",
      "Average test loss: 0.006439562122854922\n",
      "Epoch 191/300\n",
      "Average training loss: 0.1607325677871704\n",
      "Average test loss: 0.006452001739707258\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1602426594098409\n",
      "Average test loss: 0.006728353601776892\n",
      "Epoch 193/300\n",
      "Average training loss: 0.16030530797110665\n",
      "Average test loss: 0.00669016383588314\n",
      "Epoch 194/300\n",
      "Average training loss: 0.1602678260670768\n",
      "Average test loss: 0.00681188097761737\n",
      "Epoch 195/300\n",
      "Average training loss: 0.16129493674967024\n",
      "Average test loss: 0.006694460451602936\n",
      "Epoch 196/300\n",
      "Average training loss: 0.15894264101982117\n",
      "Average test loss: 0.006655095224579175\n",
      "Epoch 197/300\n",
      "Average training loss: 0.1595141323539946\n",
      "Average test loss: 0.006498184249632889\n",
      "Epoch 198/300\n",
      "Average training loss: 0.1594964285294215\n",
      "Average test loss: 0.006662305004894733\n",
      "Epoch 199/300\n",
      "Average training loss: 0.1585019376675288\n",
      "Average test loss: 0.006924639619059033\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1583680670923657\n",
      "Average test loss: 0.0068087206565671495\n",
      "Epoch 201/300\n",
      "Average training loss: 0.15894784788290658\n",
      "Average test loss: 0.0069990935573975245\n",
      "Epoch 202/300\n",
      "Average training loss: 0.1584857156806522\n",
      "Average test loss: 0.007777987120052178\n",
      "Epoch 203/300\n",
      "Average training loss: 0.1584354590177536\n",
      "Average test loss: 0.0067673959711359605\n",
      "Epoch 204/300\n",
      "Average training loss: 0.1598006643454234\n",
      "Average test loss: 0.006559350922703743\n",
      "Epoch 205/300\n",
      "Average training loss: 0.15698785928885142\n",
      "Average test loss: 0.00763271782381667\n",
      "Epoch 206/300\n",
      "Average training loss: 0.15680001735687255\n",
      "Average test loss: 0.0071701772034996085\n",
      "Epoch 207/300\n",
      "Average training loss: 0.1644304696056578\n",
      "Average test loss: 0.006713764563202858\n",
      "Epoch 208/300\n",
      "Average training loss: 0.15647688869635265\n",
      "Average test loss: 0.006612228688266542\n",
      "Epoch 209/300\n",
      "Average training loss: 0.1565103896326489\n",
      "Average test loss: 0.006941057578143147\n",
      "Epoch 210/300\n",
      "Average training loss: 0.15599710178375245\n",
      "Average test loss: 0.08397051772144105\n",
      "Epoch 211/300\n",
      "Average training loss: 0.15663052484724257\n",
      "Average test loss: 0.008269316456384129\n",
      "Epoch 212/300\n",
      "Average training loss: 0.15621701151794856\n",
      "Average test loss: 0.006920907189448674\n",
      "Epoch 213/300\n",
      "Average training loss: 0.15628923654556273\n",
      "Average test loss: 0.006668747543874714\n",
      "Epoch 214/300\n",
      "Average training loss: 0.15636212615172068\n",
      "Average test loss: 0.007199764735168881\n",
      "Epoch 215/300\n",
      "Average training loss: 0.1565446061823103\n",
      "Average test loss: 0.008656484407683214\n",
      "Epoch 216/300\n",
      "Average training loss: 0.1566951090561019\n",
      "Average test loss: 0.006800112209386296\n",
      "Epoch 217/300\n",
      "Average training loss: 0.15496145615312787\n",
      "Average test loss: 0.00710548468430837\n",
      "Epoch 218/300\n",
      "Average training loss: 0.15577984538343218\n",
      "Average test loss: 0.0068223960933585966\n",
      "Epoch 219/300\n",
      "Average training loss: 0.1554627899063958\n",
      "Average test loss: 0.011773948753045665\n",
      "Epoch 220/300\n",
      "Average training loss: 0.15502972428003947\n",
      "Average test loss: 0.006870694870750109\n",
      "Epoch 221/300\n",
      "Average training loss: 0.15424693927499983\n",
      "Average test loss: 0.029293657826052772\n",
      "Epoch 222/300\n",
      "Average training loss: 0.15457831336392297\n",
      "Average test loss: 0.018061739424036608\n",
      "Epoch 223/300\n",
      "Average training loss: 0.154451428896851\n",
      "Average test loss: 0.006906428381800652\n",
      "Epoch 224/300\n",
      "Average training loss: 0.15518221566412185\n",
      "Average test loss: 0.006639571925418244\n",
      "Epoch 225/300\n",
      "Average training loss: 0.15444606531990898\n",
      "Average test loss: 0.01772469704515404\n",
      "Epoch 226/300\n",
      "Average training loss: 0.15412977200084263\n",
      "Average test loss: 0.015043784255782762\n",
      "Epoch 227/300\n",
      "Average training loss: 0.15400217515892453\n",
      "Average test loss: 0.00655212248613437\n",
      "Epoch 228/300\n",
      "Average training loss: 0.15452914881706237\n",
      "Average test loss: 0.01053511301552256\n",
      "Epoch 229/300\n",
      "Average training loss: 0.15445188709100088\n",
      "Average test loss: 0.00674823531963759\n",
      "Epoch 230/300\n",
      "Average training loss: 0.15249035913414424\n",
      "Average test loss: 0.006597274955362081\n",
      "Epoch 231/300\n",
      "Average training loss: 0.1549904178513421\n",
      "Average test loss: 0.006622177028821574\n",
      "Epoch 232/300\n",
      "Average training loss: 0.15249549523989359\n",
      "Average test loss: 0.006628107391297817\n",
      "Epoch 233/300\n",
      "Average training loss: 0.15392017694976595\n",
      "Average test loss: 0.006753390512118737\n",
      "Epoch 234/300\n",
      "Average training loss: 0.5791906751261817\n",
      "Average test loss: 0.007146621410631471\n",
      "Epoch 235/300\n",
      "Average training loss: 0.3130057394637002\n",
      "Average test loss: 0.00681952286677228\n",
      "Epoch 236/300\n",
      "Average training loss: 0.23627423458629185\n",
      "Average test loss: 0.006323181323914065\n",
      "Epoch 237/300\n",
      "Average training loss: 0.21483333914809757\n",
      "Average test loss: 0.006299157688187228\n",
      "Epoch 238/300\n",
      "Average training loss: 0.20328696699937185\n",
      "Average test loss: 0.006264762843648593\n",
      "Epoch 239/300\n",
      "Average training loss: 0.19432305875089434\n",
      "Average test loss: 0.006929434539957179\n",
      "Epoch 240/300\n",
      "Average training loss: 0.1866892471710841\n",
      "Average test loss: 0.006408014653871457\n",
      "Epoch 241/300\n",
      "Average training loss: 0.18015319221549564\n",
      "Average test loss: 0.007213948090871175\n",
      "Epoch 242/300\n",
      "Average training loss: 0.17429644401868186\n",
      "Average test loss: 0.006553546181983418\n",
      "Epoch 243/300\n",
      "Average training loss: 0.1682752585808436\n",
      "Average test loss: 0.006514358288298051\n",
      "Epoch 244/300\n",
      "Average training loss: 0.16393872137864432\n",
      "Average test loss: 0.006528793561375803\n",
      "Epoch 245/300\n",
      "Average training loss: 0.16012829472621282\n",
      "Average test loss: 0.016189299426145023\n",
      "Epoch 246/300\n",
      "Average training loss: 0.15666675358348423\n",
      "Average test loss: 0.0067492883197135395\n",
      "Epoch 247/300\n",
      "Average training loss: 0.16163592823346457\n",
      "Average test loss: 0.006579445858796437\n",
      "Epoch 248/300\n",
      "Average training loss: 0.1540565642780728\n",
      "Average test loss: 0.007819946345355776\n",
      "Epoch 249/300\n",
      "Average training loss: 0.15879542877939012\n",
      "Average test loss: 0.00655187615669436\n",
      "Epoch 250/300\n",
      "Average training loss: 0.15223815010653602\n",
      "Average test loss: 0.006915104726536407\n",
      "Epoch 251/300\n",
      "Average training loss: 0.15308659349547493\n",
      "Average test loss: 0.006619711762087212\n",
      "Epoch 252/300\n",
      "Average training loss: 0.1531764125029246\n",
      "Average test loss: 0.007059507478442457\n",
      "Epoch 253/300\n",
      "Average training loss: 0.1521229529645708\n",
      "Average test loss: 0.018549971571399107\n",
      "Epoch 254/300\n",
      "Average training loss: 0.15260609847307205\n",
      "Average test loss: 0.006759388563947545\n",
      "Epoch 255/300\n",
      "Average training loss: 0.19556290590763092\n",
      "Average test loss: 0.006641479987237188\n",
      "Epoch 256/300\n",
      "Average training loss: 0.1610692771938112\n",
      "Average test loss: 0.006908785758746995\n",
      "Epoch 257/300\n",
      "Average training loss: 0.1534189890358183\n",
      "Average test loss: 0.006544415798866087\n",
      "Epoch 258/300\n",
      "Average training loss: 0.1524117861058977\n",
      "Average test loss: 0.006599651068862941\n",
      "Epoch 259/300\n",
      "Average training loss: 0.15608182937569087\n",
      "Average test loss: 0.008411704627590047\n",
      "Epoch 260/300\n",
      "Average training loss: 0.1513912938170963\n",
      "Average test loss: 0.00716521809829606\n",
      "Epoch 261/300\n",
      "Average training loss: 0.15085986742046145\n",
      "Average test loss: 0.0072468638693292935\n",
      "Epoch 262/300\n",
      "Average training loss: 0.15146721580293443\n",
      "Average test loss: 0.013363050669431687\n",
      "Epoch 263/300\n",
      "Average training loss: 0.15339380598068236\n",
      "Average test loss: 0.006662577413022518\n",
      "Epoch 264/300\n",
      "Average training loss: 0.15043335339758132\n",
      "Average test loss: 0.006714920124659935\n",
      "Epoch 265/300\n",
      "Average training loss: 0.1511792070335812\n",
      "Average test loss: 0.006882841476135783\n",
      "Epoch 266/300\n",
      "Average training loss: 0.1524728444284863\n",
      "Average test loss: 0.0067442535327540505\n",
      "Epoch 267/300\n",
      "Average training loss: 0.15089432901806302\n",
      "Average test loss: 0.008386294550365872\n",
      "Epoch 268/300\n",
      "Average training loss: 0.152044669230779\n",
      "Average test loss: 0.0076210415412982305\n",
      "Epoch 269/300\n",
      "Average training loss: 0.15123262144459618\n",
      "Average test loss: 0.010622098922729492\n",
      "Epoch 270/300\n",
      "Average training loss: 0.15014969625075658\n",
      "Average test loss: 0.006807603354255358\n",
      "Epoch 271/300\n",
      "Average training loss: 0.15022240704960294\n",
      "Average test loss: 0.006641845658007595\n",
      "Epoch 272/300\n",
      "Average training loss: 0.15050152126948038\n",
      "Average test loss: 0.0065819238426370755\n",
      "Epoch 273/300\n",
      "Average training loss: 0.15101648921436733\n",
      "Average test loss: 0.006830775286174482\n",
      "Epoch 274/300\n",
      "Average training loss: 0.15037254936827554\n",
      "Average test loss: 0.006711676485008663\n",
      "Epoch 275/300\n",
      "Average training loss: 0.15005557264884312\n",
      "Average test loss: 0.0066372448032101\n",
      "Epoch 276/300\n",
      "Average training loss: 0.14918561871846517\n",
      "Average test loss: 0.006796513470510641\n",
      "Epoch 277/300\n",
      "Average training loss: 0.43323627728886077\n",
      "Average test loss: 0.006635298387457927\n",
      "Epoch 278/300\n",
      "Average training loss: 0.23600981754726832\n",
      "Average test loss: 0.006354430498762264\n",
      "Epoch 279/300\n",
      "Average training loss: 0.20837558070818582\n",
      "Average test loss: 0.007715358256465859\n",
      "Epoch 280/300\n",
      "Average training loss: 0.19598017791906994\n",
      "Average test loss: 0.006499982815235853\n",
      "Epoch 281/300\n",
      "Average training loss: 0.18581505209869809\n",
      "Average test loss: 0.006713429900507132\n",
      "Epoch 282/300\n",
      "Average training loss: 0.17593300975693596\n",
      "Average test loss: 0.0065788648269242715\n",
      "Epoch 283/300\n",
      "Average training loss: 0.16637375117672815\n",
      "Average test loss: 0.0799581068067087\n",
      "Epoch 284/300\n",
      "Average training loss: 0.1586360861990187\n",
      "Average test loss: 0.008144669986847375\n",
      "Epoch 285/300\n",
      "Average training loss: 0.15434879130125045\n",
      "Average test loss: 0.006673419219752153\n",
      "Epoch 286/300\n",
      "Average training loss: 0.15294559980763328\n",
      "Average test loss: 3.903369504186842\n",
      "Epoch 287/300\n",
      "Average training loss: 0.1503704719742139\n",
      "Average test loss: 0.006713483347247045\n",
      "Epoch 288/300\n",
      "Average training loss: 0.15210275401009454\n",
      "Average test loss: 0.0066340283246503935\n",
      "Epoch 289/300\n",
      "Average training loss: 0.14879444428947236\n",
      "Average test loss: 0.006996224328875542\n",
      "Epoch 290/300\n",
      "Average training loss: 0.15366483034027947\n",
      "Average test loss: 0.006821128533118301\n",
      "Epoch 291/300\n",
      "Average training loss: 0.1480240388446384\n",
      "Average test loss: 0.007332207591583331\n",
      "Epoch 292/300\n",
      "Average training loss: 0.14838470555676353\n",
      "Average test loss: 0.006891332351913055\n",
      "Epoch 293/300\n",
      "Average training loss: 0.25135476258065964\n",
      "Average test loss: 0.006333827213694652\n",
      "Epoch 294/300\n",
      "Average training loss: 0.18192022522952822\n",
      "Average test loss: 0.006574476878676149\n",
      "Epoch 295/300\n",
      "Average training loss: 0.16641749470763736\n",
      "Average test loss: 0.006737433135509491\n",
      "Epoch 296/300\n",
      "Average training loss: 0.15681498229503632\n",
      "Average test loss: 0.006632543377164337\n",
      "Epoch 297/300\n",
      "Average training loss: 0.151719029545784\n",
      "Average test loss: 0.006700149969922172\n",
      "Epoch 298/300\n",
      "Average training loss: 0.14964368552631802\n",
      "Average test loss: 0.006969411763466066\n",
      "Epoch 299/300\n",
      "Average training loss: 0.14903654556804233\n",
      "Average test loss: 0.006693444266501401\n",
      "Epoch 300/300\n",
      "Average training loss: 0.14766799415482415\n",
      "Average test loss: 0.006673164231495725\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.187676478915744\n",
      "Average test loss: 4.500410548041264\n",
      "Epoch 2/300\n",
      "Average training loss: 5.3013032870822485\n",
      "Average test loss: 0.007844659760594367\n",
      "Epoch 3/300\n",
      "Average training loss: 3.5178335626390247\n",
      "Average test loss: 0.006559921043200625\n",
      "Epoch 4/300\n",
      "Average training loss: 2.722992746988932\n",
      "Average test loss: 0.012594029665407208\n",
      "Epoch 5/300\n",
      "Average training loss: 2.109306921535068\n",
      "Average test loss: 0.005994572634912199\n",
      "Epoch 6/300\n",
      "Average training loss: 1.7338781578275893\n",
      "Average test loss: 0.005930351080165969\n",
      "Epoch 7/300\n",
      "Average training loss: 1.414446880552504\n",
      "Average test loss: 0.005871915333386925\n",
      "Epoch 8/300\n",
      "Average training loss: 1.1694355533387926\n",
      "Average test loss: 0.005512718867924479\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9641781582302518\n",
      "Average test loss: 0.005356006632662481\n",
      "Epoch 10/300\n",
      "Average training loss: 0.80615484518475\n",
      "Average test loss: 0.005390336287932264\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6853668587472703\n",
      "Average test loss: 0.0055158551848597\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5913508527543809\n",
      "Average test loss: 0.005046885035518143\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5152259057362875\n",
      "Average test loss: 0.005091407492756843\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4540230009290907\n",
      "Average test loss: 0.004909034029477172\n",
      "Epoch 15/300\n",
      "Average training loss: 0.40445082857873704\n",
      "Average test loss: 0.00479311655379004\n",
      "Epoch 16/300\n",
      "Average training loss: 0.36638986722628275\n",
      "Average test loss: 0.005089931572477023\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3333641158474816\n",
      "Average test loss: 0.004805575355473492\n",
      "Epoch 18/300\n",
      "Average training loss: 0.3046770781675974\n",
      "Average test loss: 0.005416056635893053\n",
      "Epoch 19/300\n",
      "Average training loss: 0.28128871840900843\n",
      "Average test loss: 0.00472219898013605\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2611037551826901\n",
      "Average test loss: 0.004599901396160324\n",
      "Epoch 21/300\n",
      "Average training loss: 0.24500772444407146\n",
      "Average test loss: 0.004644711377720038\n",
      "Epoch 22/300\n",
      "Average training loss: 0.23256711498896282\n",
      "Average test loss: 0.0044506124142143465\n",
      "Epoch 23/300\n",
      "Average training loss: 0.21945763395892248\n",
      "Average test loss: 0.006393987821622028\n",
      "Epoch 24/300\n",
      "Average training loss: 0.2129456691344579\n",
      "Average test loss: 0.004384104471239779\n",
      "Epoch 25/300\n",
      "Average training loss: 0.20423960416846806\n",
      "Average test loss: 0.004385219573146767\n",
      "Epoch 26/300\n",
      "Average training loss: 0.19764603987005022\n",
      "Average test loss: 0.005848134522222811\n",
      "Epoch 27/300\n",
      "Average training loss: 0.19135694226953717\n",
      "Average test loss: 0.0045105438708431195\n",
      "Epoch 28/300\n",
      "Average training loss: 0.18654114173518288\n",
      "Average test loss: 0.004361915760156181\n",
      "Epoch 29/300\n",
      "Average training loss: 0.18144543396102059\n",
      "Average test loss: 0.004397165885402097\n",
      "Epoch 30/300\n",
      "Average training loss: 0.17668696775701312\n",
      "Average test loss: 0.004278102721191115\n",
      "Epoch 31/300\n",
      "Average training loss: 0.17465418771902722\n",
      "Average test loss: 0.004756629266051782\n",
      "Epoch 32/300\n",
      "Average training loss: 0.16922274854448108\n",
      "Average test loss: 0.00481458462940322\n",
      "Epoch 33/300\n",
      "Average training loss: 0.16628775458865697\n",
      "Average test loss: 0.004182701418383254\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1631879710621304\n",
      "Average test loss: 0.0041974748940103584\n",
      "Epoch 35/300\n",
      "Average training loss: 0.16011290211147733\n",
      "Average test loss: 0.004209777892670697\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15719644943873087\n",
      "Average test loss: 0.004249579081104861\n",
      "Epoch 37/300\n",
      "Average training loss: 0.15547993593745763\n",
      "Average test loss: 0.0044190182267791695\n",
      "Epoch 38/300\n",
      "Average training loss: 0.15205510293112862\n",
      "Average test loss: 0.004168354063398308\n",
      "Epoch 39/300\n",
      "Average training loss: 0.14890415740013122\n",
      "Average test loss: 0.004151023626741436\n",
      "Epoch 40/300\n",
      "Average training loss: 0.14720250513818528\n",
      "Average test loss: 0.004451334620929426\n",
      "Epoch 41/300\n",
      "Average training loss: 0.14513321734799278\n",
      "Average test loss: 0.004320813712974389\n",
      "Epoch 42/300\n",
      "Average training loss: 0.14333748564455245\n",
      "Average test loss: 0.004502939881963862\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14193281894922258\n",
      "Average test loss: 0.011789704729699426\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1406752606895235\n",
      "Average test loss: 0.004176471768360999\n",
      "Epoch 45/300\n",
      "Average training loss: 0.13932754193411934\n",
      "Average test loss: 0.004093532902292079\n",
      "Epoch 46/300\n",
      "Average training loss: 0.13827236485481262\n",
      "Average test loss: 0.004134045095079475\n",
      "Epoch 47/300\n",
      "Average training loss: 0.13700203998883564\n",
      "Average test loss: 0.004393227256627546\n",
      "Epoch 48/300\n",
      "Average training loss: 0.13647333081563315\n",
      "Average test loss: 0.004129266892042425\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1348927351501253\n",
      "Average test loss: 0.004187176550014151\n",
      "Epoch 50/300\n",
      "Average training loss: 0.13401033310757743\n",
      "Average test loss: 0.40123860088239116\n",
      "Epoch 51/300\n",
      "Average training loss: 0.13377173153559366\n",
      "Average test loss: 0.00411029221324457\n",
      "Epoch 52/300\n",
      "Average training loss: 0.14136677010854085\n",
      "Average test loss: 0.004062164239585399\n",
      "Epoch 53/300\n",
      "Average training loss: 0.13288308636347454\n",
      "Average test loss: 0.004068523152420918\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1315747139255206\n",
      "Average test loss: 0.004390190025170644\n",
      "Epoch 55/300\n",
      "Average training loss: 0.13063203737470838\n",
      "Average test loss: 0.004119761097348399\n",
      "Epoch 56/300\n",
      "Average training loss: 0.13011960306432513\n",
      "Average test loss: 0.00423947438183758\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1296291609870063\n",
      "Average test loss: 0.00400989481061697\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12965320987171597\n",
      "Average test loss: 0.00402418561383254\n",
      "Epoch 59/300\n",
      "Average training loss: 0.12828457850880093\n",
      "Average test loss: 0.00458624022702376\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12815601077344682\n",
      "Average test loss: 0.004150531455046601\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12721851346227858\n",
      "Average test loss: 0.00402928847923047\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12707747875981862\n",
      "Average test loss: 0.00521078064913551\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12685785199536218\n",
      "Average test loss: 0.004042533497429556\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12598038739628262\n",
      "Average test loss: 0.006838663954701688\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12539272542794547\n",
      "Average test loss: 0.004048032527582513\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12568154546949598\n",
      "Average test loss: 0.015320146850413746\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12447752838664584\n",
      "Average test loss: 0.0039994089090161855\n",
      "Epoch 68/300\n",
      "Average training loss: 0.1243108664618598\n",
      "Average test loss: 0.004084187109437254\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12351838268836339\n",
      "Average test loss: 0.004115075178858307\n",
      "Epoch 70/300\n",
      "Average training loss: 0.1268830477926466\n",
      "Average test loss: 0.0041313107108904255\n",
      "Epoch 71/300\n",
      "Average training loss: 0.1270793554385503\n",
      "Average test loss: 0.00410555153236621\n",
      "Epoch 72/300\n",
      "Average training loss: 0.12291921056641472\n",
      "Average test loss: 0.00404273445924951\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12194432535436418\n",
      "Average test loss: 0.0040729262895054285\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12182376741038428\n",
      "Average test loss: 0.0041832397532545855\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1214875798424085\n",
      "Average test loss: 0.004543378649486436\n",
      "Epoch 76/300\n",
      "Average training loss: 0.12234684512350294\n",
      "Average test loss: 0.004378408922917313\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1205647637711631\n",
      "Average test loss: 0.0044713547196653155\n",
      "Epoch 78/300\n",
      "Average training loss: 0.12108685411347284\n",
      "Average test loss: 0.00426340626945926\n",
      "Epoch 79/300\n",
      "Average training loss: 0.12004638683795929\n",
      "Average test loss: 98.90645990698536\n",
      "Epoch 80/300\n",
      "Average training loss: 0.12003527420096927\n",
      "Average test loss: 0.004284179504546854\n",
      "Epoch 81/300\n",
      "Average training loss: 0.12008836742904451\n",
      "Average test loss: 0.004030116020598345\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11902561741405063\n",
      "Average test loss: 0.004091661850611369\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11959834450483323\n",
      "Average test loss: 0.004574126597079966\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11833329734537336\n",
      "Average test loss: 0.0042359247319400314\n",
      "Epoch 85/300\n",
      "Average training loss: 0.117812840865718\n",
      "Average test loss: 0.004175722976111703\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11841030295027627\n",
      "Average test loss: 0.004424502929879559\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11748911315864986\n",
      "Average test loss: 0.004102920160525375\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11888110170099471\n",
      "Average test loss: 0.004119983182185226\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11818357109361224\n",
      "Average test loss: 0.004292752091876335\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11639048304822709\n",
      "Average test loss: 0.010317191175702546\n",
      "Epoch 91/300\n",
      "Average training loss: 0.1160129214392768\n",
      "Average test loss: 0.00440006970283058\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11578677203920153\n",
      "Average test loss: 0.004109775556872288\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11571797080834707\n",
      "Average test loss: 0.004438723388231463\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11733091068267822\n",
      "Average test loss: 0.00415407537503375\n",
      "Epoch 95/300\n",
      "Average training loss: 0.1146379160284996\n",
      "Average test loss: 0.006187082649519046\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11494767458571328\n",
      "Average test loss: 0.004556970794167783\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11542283419768015\n",
      "Average test loss: 0.004693382014830908\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11433040191729864\n",
      "Average test loss: 0.019031637493107052\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11404062485032611\n",
      "Average test loss: 0.005955529207570686\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11620365269316567\n",
      "Average test loss: 0.004156626315580474\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11300609860155318\n",
      "Average test loss: 0.02084294924305545\n",
      "Epoch 102/300\n",
      "Average training loss: 0.11414527091715071\n",
      "Average test loss: 0.005330253314640787\n",
      "Epoch 103/300\n",
      "Average training loss: 0.11344892693890465\n",
      "Average test loss: 0.004120494636189607\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11242291935284933\n",
      "Average test loss: 0.004641945357537932\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11245267123646206\n",
      "Average test loss: 0.004153530554225048\n",
      "Epoch 106/300\n",
      "Average training loss: 0.11272405347559188\n",
      "Average test loss: 3.032336814252867\n",
      "Epoch 107/300\n",
      "Average training loss: 0.11234362914164861\n",
      "Average test loss: 0.0043854274555212925\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11714829574359788\n",
      "Average test loss: 0.004200404749976264\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11161873108810849\n",
      "Average test loss: 0.004162568187341094\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11167530716790093\n",
      "Average test loss: 0.004395176966571146\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11137872166103786\n",
      "Average test loss: 0.004568241419063674\n",
      "Epoch 112/300\n",
      "Average training loss: 0.11113340102963977\n",
      "Average test loss: 0.004799729211462869\n",
      "Epoch 113/300\n",
      "Average training loss: 0.11176727279027303\n",
      "Average test loss: 0.039790089040994646\n",
      "Epoch 114/300\n",
      "Average training loss: 0.11093025557200113\n",
      "Average test loss: 0.00427409878000617\n",
      "Epoch 115/300\n",
      "Average training loss: 0.1119866891172197\n",
      "Average test loss: 0.03616994095842044\n",
      "Epoch 116/300\n",
      "Average training loss: 0.11068625370661418\n",
      "Average test loss: 0.009541848052706983\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11035178861353133\n",
      "Average test loss: 0.005659590440905756\n",
      "Epoch 118/300\n",
      "Average training loss: 0.11046098885933558\n",
      "Average test loss: 0.019624607539839214\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10952124755250082\n",
      "Average test loss: 0.004775890870433714\n",
      "Epoch 120/300\n",
      "Average training loss: 0.1103455559346411\n",
      "Average test loss: 0.004164562062670787\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11038993803660074\n",
      "Average test loss: 0.004287393482608927\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10956865571604835\n",
      "Average test loss: 0.0045750934125648604\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10887920877668593\n",
      "Average test loss: 0.0041854435520039665\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11002169173955917\n",
      "Average test loss: 0.004187947691728672\n",
      "Epoch 125/300\n",
      "Average training loss: 0.12063661193847657\n",
      "Average test loss: 0.013096556279394362\n",
      "Epoch 126/300\n",
      "Average training loss: 0.1704813828335868\n",
      "Average test loss: 0.00496955964093407\n",
      "Epoch 127/300\n",
      "Average training loss: 0.1264281956354777\n",
      "Average test loss: 0.004080020166933537\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1194128173854616\n",
      "Average test loss: 0.009081904112464852\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11498941457271576\n",
      "Average test loss: 0.00416095722011394\n",
      "Epoch 130/300\n",
      "Average training loss: 0.11209602582454681\n",
      "Average test loss: 0.004935935878919231\n",
      "Epoch 131/300\n",
      "Average training loss: 0.11053598940372467\n",
      "Average test loss: 0.004339059129771259\n",
      "Epoch 132/300\n",
      "Average training loss: 0.11105740956465403\n",
      "Average test loss: 0.004358631058078673\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10994995607932409\n",
      "Average test loss: 0.004212556267364158\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10826352832052442\n",
      "Average test loss: 0.11744649043679237\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10851285113228692\n",
      "Average test loss: 0.004279225399924649\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10893061825301913\n",
      "Average test loss: 0.0047135799328486125\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10820710160997178\n",
      "Average test loss: 0.004532211958534187\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10864805795749029\n",
      "Average test loss: 0.019154325490196546\n",
      "Epoch 139/300\n",
      "Average training loss: 0.11077091410424975\n",
      "Average test loss: 0.004423828074915542\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1074471866356002\n",
      "Average test loss: 0.004407132400820653\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1075508580605189\n",
      "Average test loss: 0.004206493797401587\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10788634265793694\n",
      "Average test loss: 0.005240000875045856\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10819096042050255\n",
      "Average test loss: 0.004317341449773974\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10727062658468882\n",
      "Average test loss: 0.004228640603936381\n",
      "Epoch 145/300\n",
      "Average training loss: 0.1069546557466189\n",
      "Average test loss: 0.012180681163238154\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10686718068520228\n",
      "Average test loss: 0.004291562406967084\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10914785537454817\n",
      "Average test loss: 0.004215696952823135\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1064977952506807\n",
      "Average test loss: 0.0043596133101317614\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10700899298323525\n",
      "Average test loss: 0.024007537916302682\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10630797362327575\n",
      "Average test loss: 0.004209335183517801\n",
      "Epoch 151/300\n",
      "Average training loss: 0.1068435035612848\n",
      "Average test loss: 0.016870381731126042\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10714887626965841\n",
      "Average test loss: 0.004401629865997367\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10583542778756884\n",
      "Average test loss: 0.007187515357302295\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1161423551109102\n",
      "Average test loss: 0.00461230381909344\n",
      "Epoch 155/300\n",
      "Average training loss: 0.1080789085759057\n",
      "Average test loss: 0.005512239509158664\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10550941122902764\n",
      "Average test loss: 0.006255857860876454\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10661295688814587\n",
      "Average test loss: 0.01002646934820546\n",
      "Epoch 158/300\n",
      "Average training loss: 0.1054939338962237\n",
      "Average test loss: 0.004385602496150467\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10582138015164269\n",
      "Average test loss: 0.004263293991072311\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10517435222201878\n",
      "Average test loss: 0.005490568236758311\n",
      "Epoch 161/300\n",
      "Average training loss: 0.105598590042856\n",
      "Average test loss: 0.007984870413111316\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10561754433314005\n",
      "Average test loss: 0.005383650467420618\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10540889432695177\n",
      "Average test loss: 0.004281751699331734\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10478986688454946\n",
      "Average test loss: 0.004411875721481111\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10496708464622498\n",
      "Average test loss: 0.00500664223689172\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10698387179109785\n",
      "Average test loss: 0.011177777889702055\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1041508953836229\n",
      "Average test loss: 0.004467531709207429\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10501790292395485\n",
      "Average test loss: 0.004246025854514705\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10438400265905592\n",
      "Average test loss: 0.004383877172031337\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1063338133096695\n",
      "Average test loss: 0.004339654192328453\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10395886835787031\n",
      "Average test loss: 0.004503027559775445\n",
      "Epoch 172/300\n",
      "Average training loss: 0.10408519407113394\n",
      "Average test loss: 0.0052654563461740815\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10466669076681137\n",
      "Average test loss: 0.004395456232544449\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10418527634276284\n",
      "Average test loss: 0.004402327794788613\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10389473370711008\n",
      "Average test loss: 0.0273167701529132\n",
      "Epoch 176/300\n",
      "Average training loss: 0.10419114371803072\n",
      "Average test loss: 0.004349712064282762\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10396746857961019\n",
      "Average test loss: 0.004445328446726004\n",
      "Epoch 178/300\n",
      "Average training loss: 0.10436413696077135\n",
      "Average test loss: 0.004804415728069014\n",
      "Epoch 179/300\n",
      "Average training loss: 0.10382102473576864\n",
      "Average test loss: 0.0043645921798629896\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10890935401121775\n",
      "Average test loss: 0.004162972154716651\n",
      "Epoch 181/300\n",
      "Average training loss: 0.10367607662412856\n",
      "Average test loss: 0.004439895420852635\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10314508873886533\n",
      "Average test loss: 0.005024147865672906\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10304621969328986\n",
      "Average test loss: 0.004819355631868045\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10304899591869777\n",
      "Average test loss: 0.004326810660875506\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10567189271582497\n",
      "Average test loss: 0.004379380034903685\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10249335590998332\n",
      "Average test loss: 0.004817755986418989\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10323425721459918\n",
      "Average test loss: 0.004696116710909539\n",
      "Epoch 188/300\n",
      "Average training loss: 0.10255337153540717\n",
      "Average test loss: 0.004351728699894415\n",
      "Epoch 189/300\n",
      "Average training loss: 0.11113455410798391\n",
      "Average test loss: 0.004395573084967004\n",
      "Epoch 190/300\n",
      "Average training loss: 0.10323725278509988\n",
      "Average test loss: 0.005006857647250096\n",
      "Epoch 191/300\n",
      "Average training loss: 0.10238900171385872\n",
      "Average test loss: 0.004239226678593291\n",
      "Epoch 192/300\n",
      "Average training loss: 0.10240438064601687\n",
      "Average test loss: 0.0046033118938406305\n",
      "Epoch 193/300\n",
      "Average training loss: 0.10216050448682573\n",
      "Average test loss: 0.004532416562032369\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10294052868419223\n",
      "Average test loss: 0.004392654534429312\n",
      "Epoch 195/300\n",
      "Average training loss: 0.11404528622494804\n",
      "Average test loss: 0.004327350336644385\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10160366043117311\n",
      "Average test loss: 0.004303533886869749\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10184056523111132\n",
      "Average test loss: 0.005711638286709785\n",
      "Epoch 198/300\n",
      "Average training loss: 0.10164949289957682\n",
      "Average test loss: 0.004358270721510052\n",
      "Epoch 199/300\n",
      "Average training loss: 0.10173902337418662\n",
      "Average test loss: 0.004429655210218496\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1090985478427675\n",
      "Average test loss: 0.004689432255095906\n",
      "Epoch 201/300\n",
      "Average training loss: 0.1015001318918334\n",
      "Average test loss: 0.004732228094918861\n",
      "Epoch 202/300\n",
      "Average training loss: 0.1012778353492419\n",
      "Average test loss: 0.02562073737548457\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10173508965969086\n",
      "Average test loss: 0.004365579055001338\n",
      "Epoch 204/300\n",
      "Average training loss: 0.10162630210982429\n",
      "Average test loss: 0.0048289472469025185\n",
      "Epoch 205/300\n",
      "Average training loss: 0.10289728531572553\n",
      "Average test loss: 0.004518834495710002\n",
      "Epoch 206/300\n",
      "Average training loss: 0.10189414221710628\n",
      "Average test loss: 0.004491221975121233\n",
      "Epoch 207/300\n",
      "Average training loss: 0.10223623061180115\n",
      "Average test loss: 0.00431832812850674\n",
      "Epoch 208/300\n",
      "Average training loss: 0.10190553638007906\n",
      "Average test loss: 0.00574878853559494\n",
      "Epoch 209/300\n",
      "Average training loss: 0.10201772779888577\n",
      "Average test loss: 0.004345000781946712\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10166863044102986\n",
      "Average test loss: 0.004363696486999591\n",
      "Epoch 211/300\n",
      "Average training loss: 0.10269326063659456\n",
      "Average test loss: 0.005298230500684844\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10113456885682212\n",
      "Average test loss: 0.004488787715426749\n",
      "Epoch 213/300\n",
      "Average training loss: 0.10146385247839822\n",
      "Average test loss: 0.00491699286032882\n",
      "Epoch 214/300\n",
      "Average training loss: 0.11394616492589314\n",
      "Average test loss: 0.0042715646556268135\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10221787816617224\n",
      "Average test loss: 0.0046223012531797095\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10081667837831709\n",
      "Average test loss: 0.004302031324141555\n",
      "Epoch 217/300\n",
      "Average training loss: 0.10032130130794313\n",
      "Average test loss: 0.004398264529390468\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10615882399347093\n",
      "Average test loss: 0.004367720936735471\n",
      "Epoch 219/300\n",
      "Average training loss: 0.10041524298985799\n",
      "Average test loss: 0.004333559001485507\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10746390117539299\n",
      "Average test loss: 0.004305635729597674\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09996939775016572\n",
      "Average test loss: 0.004900485487654805\n",
      "Epoch 222/300\n",
      "Average training loss: 0.1001680492957433\n",
      "Average test loss: 0.004291571212725507\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10161860528257158\n",
      "Average test loss: 0.004582596105006006\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10041525773207347\n",
      "Average test loss: 0.00498589438945055\n",
      "Epoch 225/300\n",
      "Average training loss: 0.10075927685366737\n",
      "Average test loss: 0.004533973055374291\n",
      "Epoch 226/300\n",
      "Average training loss: 0.1022355856762992\n",
      "Average test loss: 0.009363352537155152\n",
      "Epoch 227/300\n",
      "Average training loss: 0.10027712997463015\n",
      "Average test loss: 0.007032427634629938\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10035750039418538\n",
      "Average test loss: 0.004446820760352744\n",
      "Epoch 229/300\n",
      "Average training loss: 0.10005640912718243\n",
      "Average test loss: 0.0047850091072420275\n",
      "Epoch 230/300\n",
      "Average training loss: 0.10851593662632836\n",
      "Average test loss: 0.004484484782649411\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0999841786093182\n",
      "Average test loss: 0.004386295421255959\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09999116230010986\n",
      "Average test loss: 0.004661978494996825\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09979394529925452\n",
      "Average test loss: 0.004297489966783258\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10242097340689765\n",
      "Average test loss: 0.005077432263642549\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09941964031590356\n",
      "Average test loss: 0.004388503469940689\n",
      "Epoch 236/300\n",
      "Average training loss: 0.10115653375122283\n",
      "Average test loss: 0.004379071023315191\n",
      "Epoch 237/300\n",
      "Average training loss: 0.10139855206012725\n",
      "Average test loss: 0.022735582398043738\n",
      "Epoch 238/300\n",
      "Average training loss: 0.10367603970898523\n",
      "Average test loss: 0.004287422765460279\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0992281132141749\n",
      "Average test loss: 0.004334058198250002\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0998084061410692\n",
      "Average test loss: 0.005534708994130294\n",
      "Epoch 241/300\n",
      "Average training loss: 0.10033350684245428\n",
      "Average test loss: 0.00452055295308431\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09984116080734465\n",
      "Average test loss: 0.00454858434241679\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09970132480727302\n",
      "Average test loss: 0.004425637071538303\n",
      "Epoch 244/300\n",
      "Average training loss: 0.10024655268589655\n",
      "Average test loss: 0.0043272923077974056\n",
      "Epoch 245/300\n",
      "Average training loss: 0.10221939926015006\n",
      "Average test loss: 0.011262496338122421\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09924942766957813\n",
      "Average test loss: 0.004584781104077895\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10054904713895586\n",
      "Average test loss: 0.005787055923293034\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0989657300511996\n",
      "Average test loss: 0.004625590327713224\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09966003231538667\n",
      "Average test loss: 0.004431928345312675\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09911692061026892\n",
      "Average test loss: 0.004480784211721685\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09968178539143668\n",
      "Average test loss: 0.004756274110741085\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09901909112930297\n",
      "Average test loss: 0.004726505652277006\n",
      "Epoch 253/300\n",
      "Average training loss: 0.12022907448477216\n",
      "Average test loss: 0.004390552682595121\n",
      "Epoch 254/300\n",
      "Average training loss: 0.10050233497222265\n",
      "Average test loss: 0.004348282186935345\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09839058711793687\n",
      "Average test loss: 0.004522859397861693\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09895967994795905\n",
      "Average test loss: 0.004436671884109577\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09897231223848131\n",
      "Average test loss: 0.004554962241815196\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09848734696706136\n",
      "Average test loss: 0.0050392813583215075\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09993144083023071\n",
      "Average test loss: 0.004364264761925572\n",
      "Epoch 260/300\n",
      "Average training loss: 0.10114796314636866\n",
      "Average test loss: 0.004520205673244264\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09821829840872023\n",
      "Average test loss: 0.00580464335779349\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09855751571390364\n",
      "Average test loss: 0.0044146383251580925\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09889276366763644\n",
      "Average test loss: 0.004464499736825625\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09942212563421991\n",
      "Average test loss: 0.004751442956013812\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09856236284308964\n",
      "Average test loss: 0.004415867920757995\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09900616290171942\n",
      "Average test loss: 0.004415951276818911\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09921771517064837\n",
      "Average test loss: 0.004628413850027654\n",
      "Epoch 268/300\n",
      "Average training loss: 0.10009352519114813\n",
      "Average test loss: 0.004559433755775293\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10204293608665466\n",
      "Average test loss: 0.0046275006228437025\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09807707460721334\n",
      "Average test loss: 0.004479131514827411\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09843388130929735\n",
      "Average test loss: 0.004334131243535214\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09832746271954643\n",
      "Average test loss: 0.004312073580506775\n",
      "Epoch 273/300\n",
      "Average training loss: 0.1003302459584342\n",
      "Average test loss: 0.004401820389760865\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09802753822008768\n",
      "Average test loss: 0.004376726003570689\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09809541457891464\n",
      "Average test loss: 0.004290973965078593\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0979219775001208\n",
      "Average test loss: 0.004939300891011953\n",
      "Epoch 277/300\n",
      "Average training loss: 0.11470852321717474\n",
      "Average test loss: 0.004576281772719489\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09840209564897749\n",
      "Average test loss: 0.004432512868609693\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0974583072066307\n",
      "Average test loss: 0.004305893652555015\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09739581384923723\n",
      "Average test loss: 0.004439100480328004\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09760261311133703\n",
      "Average test loss: 0.00447642893013027\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09781557906336255\n",
      "Average test loss: 0.004900767598094212\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09833801917235056\n",
      "Average test loss: 0.004436775510095888\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09742940908008152\n",
      "Average test loss: 0.00467022021032042\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09897177502844069\n",
      "Average test loss: 0.005260828969793187\n",
      "Epoch 286/300\n",
      "Average training loss: 0.09781212692790561\n",
      "Average test loss: 0.004398991479848822\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0978146044810613\n",
      "Average test loss: 0.043392471383015316\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09848056550820669\n",
      "Average test loss: 0.004465224057435989\n",
      "Epoch 289/300\n",
      "Average training loss: 0.101082286298275\n",
      "Average test loss: 0.004866664590520991\n",
      "Epoch 290/300\n",
      "Average training loss: 0.27878336808416576\n",
      "Average test loss: 0.004873167423738374\n",
      "Epoch 291/300\n",
      "Average training loss: 0.17695879753430685\n",
      "Average test loss: 0.0042261004555556505\n",
      "Epoch 292/300\n",
      "Average training loss: 0.14628790377908282\n",
      "Average test loss: 0.0041217650295131735\n",
      "Epoch 293/300\n",
      "Average training loss: 0.13482122180196973\n",
      "Average test loss: 0.004110502830603056\n",
      "Epoch 294/300\n",
      "Average training loss: 0.12733564376831055\n",
      "Average test loss: 0.004923966839288672\n",
      "Epoch 295/300\n",
      "Average training loss: 0.12123395336336559\n",
      "Average test loss: 0.00417279842744271\n",
      "Epoch 296/300\n",
      "Average training loss: 0.1150419289138582\n",
      "Average test loss: 0.004845871198508474\n",
      "Epoch 297/300\n",
      "Average training loss: 0.10891647709078259\n",
      "Average test loss: 0.004501031113995446\n",
      "Epoch 298/300\n",
      "Average training loss: 0.10426314373148812\n",
      "Average test loss: 0.004376788771607809\n",
      "Epoch 299/300\n",
      "Average training loss: 0.10167997083399032\n",
      "Average test loss: 0.004858670378724734\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09928873903221554\n",
      "Average test loss: 0.004493710833705134\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.501051620059544\n",
      "Average test loss: 0.00709669543719954\n",
      "Epoch 2/300\n",
      "Average training loss: 6.834022548675537\n",
      "Average test loss: 0.24251763126916356\n",
      "Epoch 3/300\n",
      "Average training loss: 5.446823465135362\n",
      "Average test loss: 0.005270020540803671\n",
      "Epoch 4/300\n",
      "Average training loss: 4.352524384816488\n",
      "Average test loss: 0.005152811432878176\n",
      "Epoch 5/300\n",
      "Average training loss: 4.058333856370714\n",
      "Average test loss: 1.4776621133403645\n",
      "Epoch 6/300\n",
      "Average training loss: 3.3525493846469456\n",
      "Average test loss: 0.004756732188165188\n",
      "Epoch 7/300\n",
      "Average training loss: 2.8516726979149714\n",
      "Average test loss: 0.015255178343297707\n",
      "Epoch 8/300\n",
      "Average training loss: 2.242961237165663\n",
      "Average test loss: 0.0043943501942687565\n",
      "Epoch 9/300\n",
      "Average training loss: 1.9636573082605997\n",
      "Average test loss: 0.012541644141077996\n",
      "Epoch 10/300\n",
      "Average training loss: 1.7875636971791586\n",
      "Average test loss: 0.004245088064008289\n",
      "Epoch 11/300\n",
      "Average training loss: 1.527127636273702\n",
      "Average test loss: 0.004056496636735068\n",
      "Epoch 12/300\n",
      "Average training loss: 1.2993843043645223\n",
      "Average test loss: 0.003991013118790255\n",
      "Epoch 13/300\n",
      "Average training loss: 1.129577096409268\n",
      "Average test loss: 0.005297645223637422\n",
      "Epoch 14/300\n",
      "Average training loss: 0.9626723507775201\n",
      "Average test loss: 0.0038122127627333007\n",
      "Epoch 15/300\n",
      "Average training loss: 0.8137521668010288\n",
      "Average test loss: 0.003749210807391339\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6986263880199857\n",
      "Average test loss: 0.003640467076251904\n",
      "Epoch 17/300\n",
      "Average training loss: 0.6042425016297235\n",
      "Average test loss: 0.004067331150174141\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5250961503452725\n",
      "Average test loss: 0.004879118879636129\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4591467119322883\n",
      "Average test loss: 0.0035060347902278106\n",
      "Epoch 20/300\n",
      "Average training loss: 0.40418781357341343\n",
      "Average test loss: 0.0034205314407332074\n",
      "Epoch 21/300\n",
      "Average training loss: 0.3587553064823151\n",
      "Average test loss: 0.003441718099017938\n",
      "Epoch 22/300\n",
      "Average training loss: 0.32030579566955564\n",
      "Average test loss: 0.003470800177504619\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2870358015696208\n",
      "Average test loss: 0.0033792976300335595\n",
      "Epoch 24/300\n",
      "Average training loss: 0.25995991269747415\n",
      "Average test loss: 0.006894808772123522\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23851096312204997\n",
      "Average test loss: 0.003284393364770545\n",
      "Epoch 26/300\n",
      "Average training loss: 0.21977772484885322\n",
      "Average test loss: 0.0033474362823698255\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2050009446806378\n",
      "Average test loss: 0.0077101088391823905\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1914217556979921\n",
      "Average test loss: 0.003239598026913073\n",
      "Epoch 29/300\n",
      "Average training loss: 0.18100039993392097\n",
      "Average test loss: 0.003211041775635547\n",
      "Epoch 30/300\n",
      "Average training loss: 0.17177198943826888\n",
      "Average test loss: 0.003341442155341307\n",
      "Epoch 31/300\n",
      "Average training loss: 0.16475026122728983\n",
      "Average test loss: 0.003266139097304808\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15893741277853649\n",
      "Average test loss: 0.0035498075532830425\n",
      "Epoch 33/300\n",
      "Average training loss: 0.15332579555776385\n",
      "Average test loss: 0.0031644439794537094\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14777533153692882\n",
      "Average test loss: 0.003360784810450342\n",
      "Epoch 35/300\n",
      "Average training loss: 0.14331931014855703\n",
      "Average test loss: 0.003138464348597659\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13934162770377265\n",
      "Average test loss: 0.0031729567067490683\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1352961531612608\n",
      "Average test loss: 0.003133690274010102\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1328726305299335\n",
      "Average test loss: 0.0033278674290825924\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1290345341563225\n",
      "Average test loss: 0.003183665026807123\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12668119358354143\n",
      "Average test loss: 0.003161582761547632\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12356553524070316\n",
      "Average test loss: 0.0031139292996376754\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12095701684554418\n",
      "Average test loss: 0.0032550757895741197\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11928766683075163\n",
      "Average test loss: 0.003050889546258582\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11720414565006892\n",
      "Average test loss: 0.0034436822330786124\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11557976288265652\n",
      "Average test loss: 0.006294122970766492\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11283921802043914\n",
      "Average test loss: 0.003353542685508728\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11212460945712195\n",
      "Average test loss: 0.003018632613329424\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11004660417636236\n",
      "Average test loss: 0.004803801100287173\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10847923902670542\n",
      "Average test loss: 0.0030090874725331863\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10727482250001695\n",
      "Average test loss: 0.0031844243382414183\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1059103697405921\n",
      "Average test loss: 0.003082898518277539\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1055361419386334\n",
      "Average test loss: 0.005213364922751983\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10465474918815824\n",
      "Average test loss: 0.003047259704416825\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10315864666965273\n",
      "Average test loss: 0.013869665706323253\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10280362305376264\n",
      "Average test loss: 0.003646852667753895\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1021525381538603\n",
      "Average test loss: 0.0033187788960834345\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10140641491942935\n",
      "Average test loss: 0.0030682612616154885\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10098277076747682\n",
      "Average test loss: 0.003040677256261309\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10020833188295364\n",
      "Average test loss: 0.003026668938083781\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09964906967348523\n",
      "Average test loss: 0.004155146234565311\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1024083332154486\n",
      "Average test loss: 0.003070795227256086\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09902538686328465\n",
      "Average test loss: 0.0029791743645651474\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0980000891354349\n",
      "Average test loss: 0.0032277528670512966\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09790768042206764\n",
      "Average test loss: 0.00355901299810244\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09736693423986435\n",
      "Average test loss: 0.0039055953799850412\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09756888938612408\n",
      "Average test loss: 0.0030475221717109285\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09649275994300842\n",
      "Average test loss: 0.0031155819416873983\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09621451858679453\n",
      "Average test loss: 0.0029914407222014334\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0959668291343583\n",
      "Average test loss: 0.003099051593699389\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09540107051531474\n",
      "Average test loss: 0.0036696863230317833\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09536119821336535\n",
      "Average test loss: 0.002995648021499316\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09500507945484585\n",
      "Average test loss: 0.0036068823339624537\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09490962047047086\n",
      "Average test loss: 0.003215906539931893\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09449974793195724\n",
      "Average test loss: 0.00401713291514251\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09400068777799607\n",
      "Average test loss: 0.0030173726682033806\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09434095141622756\n",
      "Average test loss: 0.003174320749938488\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0931986624399821\n",
      "Average test loss: 0.006194002307123608\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09318461087677214\n",
      "Average test loss: 0.0030312461766103904\n",
      "Epoch 79/300\n",
      "Average training loss: 0.093006561352147\n",
      "Average test loss: 0.003092428466305137\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09256765381495158\n",
      "Average test loss: 0.003112744788121846\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09247438408931097\n",
      "Average test loss: 0.003068535406349434\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09169453886482451\n",
      "Average test loss: 0.0030802666429016324\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09168472346332338\n",
      "Average test loss: 0.0032952795192185374\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09139659811390771\n",
      "Average test loss: 0.0031326490549577605\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09122533147202598\n",
      "Average test loss: 0.0031170793746908504\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09082462797562281\n",
      "Average test loss: 0.003071312189102173\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0907498466041353\n",
      "Average test loss: 0.0030703496649447416\n",
      "Epoch 88/300\n",
      "Average training loss: 0.21210272999604543\n",
      "Average test loss: 0.0034384866069174474\n",
      "Epoch 89/300\n",
      "Average training loss: 0.12361906055609385\n",
      "Average test loss: 0.0031199395153671504\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11085598549577926\n",
      "Average test loss: 0.0030314687757442395\n",
      "Epoch 91/300\n",
      "Average training loss: 0.105689253171285\n",
      "Average test loss: 0.003614142035030656\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10242690959903929\n",
      "Average test loss: 0.0029872725136164163\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09989627001020643\n",
      "Average test loss: 0.0033693639097942246\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09796826277176539\n",
      "Average test loss: 0.003196274154095186\n",
      "Epoch 95/300\n",
      "Average training loss: 0.09657044851779938\n",
      "Average test loss: 0.0030518661615335277\n",
      "Epoch 96/300\n",
      "Average training loss: 0.09513845381471846\n",
      "Average test loss: 0.003806704315667351\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09506270267566046\n",
      "Average test loss: 0.0032392802358501486\n",
      "Epoch 98/300\n",
      "Average training loss: 0.09323459094762802\n",
      "Average test loss: 0.0031555293227235478\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0926231409046385\n",
      "Average test loss: 0.0030426385241250197\n",
      "Epoch 100/300\n",
      "Average training loss: 0.09223614607916938\n",
      "Average test loss: 0.0033065400959716904\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0915527934299575\n",
      "Average test loss: 0.0035115417829818195\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09131255343887541\n",
      "Average test loss: 0.003031267781224516\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09124027509159512\n",
      "Average test loss: 0.0032117344606667755\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09088373433881336\n",
      "Average test loss: 0.003107520202588704\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08993045562505722\n",
      "Average test loss: 0.004629248805344105\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08955654368135664\n",
      "Average test loss: 0.0037152906503114436\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08924503060844209\n",
      "Average test loss: 0.004688749161445432\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09060325124528673\n",
      "Average test loss: 0.0031631240631557174\n",
      "Epoch 109/300\n",
      "Average training loss: 0.08879704454210069\n",
      "Average test loss: 0.00312068460861014\n",
      "Epoch 110/300\n",
      "Average training loss: 0.08862130223380195\n",
      "Average test loss: 0.0030554622010224397\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09071922295623355\n",
      "Average test loss: 0.003099677967528502\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08791994039879905\n",
      "Average test loss: 0.0031561219572193095\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08768611133760876\n",
      "Average test loss: 0.003123106777254078\n",
      "Epoch 114/300\n",
      "Average training loss: 0.08751345583465364\n",
      "Average test loss: 0.0048573850182195505\n",
      "Epoch 115/300\n",
      "Average training loss: 0.08737996113300324\n",
      "Average test loss: 0.0034079109173682\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08782359431187312\n",
      "Average test loss: 0.003080078427369396\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08830597298012839\n",
      "Average test loss: 0.003123431158769462\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08674594481786092\n",
      "Average test loss: 0.0034650229290127755\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08650466603040695\n",
      "Average test loss: 0.003122269461966223\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10188608464267519\n",
      "Average test loss: 0.0031291875657108096\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09124698598517313\n",
      "Average test loss: 0.003083279518617524\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08764365995592541\n",
      "Average test loss: 0.0032376115599440205\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08656778769360648\n",
      "Average test loss: 0.0035314721502363684\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0861278637846311\n",
      "Average test loss: 0.0033947644649694364\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08585963129997254\n",
      "Average test loss: 0.00481503231326739\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08899205399221845\n",
      "Average test loss: 0.003931867689929074\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08807788992590375\n",
      "Average test loss: 0.0031838105734851626\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0854012776348326\n",
      "Average test loss: 0.0031823755649642814\n",
      "Epoch 129/300\n",
      "Average training loss: 0.08523489404386944\n",
      "Average test loss: 0.0032571727585875327\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08520665182007683\n",
      "Average test loss: 0.0035784524370812707\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08509389046827952\n",
      "Average test loss: 0.003396300552619828\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08554190182685852\n",
      "Average test loss: 0.0030867957576281493\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08469622815648714\n",
      "Average test loss: 0.0031490708148727816\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10232956269052293\n",
      "Average test loss: 0.0030919449577728906\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08761838005648719\n",
      "Average test loss: 0.003238913015152017\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08525356286764145\n",
      "Average test loss: 0.003835839081555605\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08457343527343537\n",
      "Average test loss: 0.0030973732262435886\n",
      "Epoch 138/300\n",
      "Average training loss: 0.08473156266742282\n",
      "Average test loss: 0.010986214847200446\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08415028088622623\n",
      "Average test loss: 0.003223798628275593\n",
      "Epoch 140/300\n",
      "Average training loss: 0.08399141482512155\n",
      "Average test loss: 0.003407527690960301\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08415173873636457\n",
      "Average test loss: 0.003141342442896631\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0841036408609814\n",
      "Average test loss: 0.003374384573971232\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08370129010412428\n",
      "Average test loss: 0.0031605222569778563\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08352741065952513\n",
      "Average test loss: 0.10085938724378744\n",
      "Epoch 145/300\n",
      "Average training loss: 0.08355181790060467\n",
      "Average test loss: 0.0032845849647290177\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08341343428028955\n",
      "Average test loss: 0.003325836676483353\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08297335952520371\n",
      "Average test loss: 0.0031442231842213207\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0863647850619422\n",
      "Average test loss: 0.0031588281734536094\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08280796321233114\n",
      "Average test loss: 0.0031133219823241236\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0829916630188624\n",
      "Average test loss: 0.0032071868936634725\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08296258192592197\n",
      "Average test loss: 0.012115756201247375\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08234599707523982\n",
      "Average test loss: 0.003164304931130674\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08699094212055206\n",
      "Average test loss: 0.02572806506521172\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08205796870920393\n",
      "Average test loss: 0.0037198806814849376\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08230094438791274\n",
      "Average test loss: 0.0037032091619653832\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08184005223380195\n",
      "Average test loss: 0.003893381155613396\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08221061830388175\n",
      "Average test loss: 0.0032445463155292803\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0820090872115559\n",
      "Average test loss: 0.003224463320026795\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08209514555003908\n",
      "Average test loss: 0.0031223900104976362\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08257446351978515\n",
      "Average test loss: 0.003705756094927589\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08131474378373887\n",
      "Average test loss: 0.0031685213552167016\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0813787734442287\n",
      "Average test loss: 0.003538886820483539\n",
      "Epoch 163/300\n",
      "Average training loss: 0.24916438486178716\n",
      "Average test loss: 0.004674697922335731\n",
      "Epoch 164/300\n",
      "Average training loss: 0.11887700057029724\n",
      "Average test loss: 0.003114304303915964\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10750595100720724\n",
      "Average test loss: 0.003324821250099275\n",
      "Epoch 166/300\n",
      "Average training loss: 0.10200715877612432\n",
      "Average test loss: 0.004958880547848012\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09816008061170578\n",
      "Average test loss: 0.0031045736436628633\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09490420537524753\n",
      "Average test loss: 0.004002868584460683\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09176853583918677\n",
      "Average test loss: 0.003627250603503651\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08888581255409453\n",
      "Average test loss: 0.0032442510202527045\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08630624836020999\n",
      "Average test loss: 0.003286082316810886\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08489874519242181\n",
      "Average test loss: 0.0031204902667345273\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08305866692463557\n",
      "Average test loss: 0.0040324048639999496\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08232184469037586\n",
      "Average test loss: 0.003449043374508619\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08196473229593701\n",
      "Average test loss: 0.0035122729904121823\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08139803608258565\n",
      "Average test loss: 0.004579062153067854\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08205456809865104\n",
      "Average test loss: 0.0032828049663868215\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08113252709971534\n",
      "Average test loss: 0.004032288533118036\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08114090881082747\n",
      "Average test loss: 0.0034155934117734432\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08122013717227512\n",
      "Average test loss: 0.005286159939236111\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0808688592976994\n",
      "Average test loss: 0.0037865707230650717\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08194354124863942\n",
      "Average test loss: 0.0039016603235569264\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08066555743747288\n",
      "Average test loss: 0.007287295845233732\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08080865412288242\n",
      "Average test loss: 0.0033409991032547423\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08169210622045729\n",
      "Average test loss: 0.003882337119637264\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08019194887081782\n",
      "Average test loss: 0.003267053950784935\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0809522432949808\n",
      "Average test loss: 0.007633802861389187\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0803511154850324\n",
      "Average test loss: 0.004281037404512366\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08307765646775564\n",
      "Average test loss: 0.003169467323977086\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08000744534201092\n",
      "Average test loss: 0.003131395007794102\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0797161669201321\n",
      "Average test loss: 0.003132354024797678\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08958727412753635\n",
      "Average test loss: 0.00325280105902089\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08246013712220722\n",
      "Average test loss: 0.0034965695527692637\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07949026968081792\n",
      "Average test loss: 0.0037645144773026306\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07937927423583137\n",
      "Average test loss: 0.004022560752721296\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07950458102093802\n",
      "Average test loss: 0.003303710579665171\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07993682987160153\n",
      "Average test loss: 0.019051895504196486\n",
      "Epoch 198/300\n",
      "Average training loss: 0.07943222983678182\n",
      "Average test loss: 0.071817526406712\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07938441541459826\n",
      "Average test loss: 0.004676637488727768\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07971519233783086\n",
      "Average test loss: 0.00318102379474375\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07959650172789892\n",
      "Average test loss: 0.003252460906489028\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08376213010152181\n",
      "Average test loss: 0.0031938881874084475\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07894195440411568\n",
      "Average test loss: 0.0032687280529903042\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07892859054274029\n",
      "Average test loss: 0.0032099975422024726\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07893720750676261\n",
      "Average test loss: 0.0039832170067562\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07917840379807684\n",
      "Average test loss: 0.003977460929089122\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09069569820165634\n",
      "Average test loss: 0.0034330555713839002\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0792631336318122\n",
      "Average test loss: 0.0031714506877793206\n",
      "Epoch 209/300\n",
      "Average training loss: 0.25266336203946005\n",
      "Average test loss: 0.00363969443904029\n",
      "Epoch 210/300\n",
      "Average training loss: 0.12064634074105157\n",
      "Average test loss: 0.003095530028972361\n",
      "Epoch 211/300\n",
      "Average training loss: 0.1083910951283243\n",
      "Average test loss: 0.0837327586279975\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10220651639832391\n",
      "Average test loss: 0.0031181112153248654\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09765753413571251\n",
      "Average test loss: 0.012377512691335546\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09344450291660097\n",
      "Average test loss: 0.003131678706035018\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09019569752613703\n",
      "Average test loss: 0.0032049019680254988\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08750178079472648\n",
      "Average test loss: 0.005410068603025542\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08512877923250198\n",
      "Average test loss: 0.003142984176054597\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08289794756306543\n",
      "Average test loss: 0.0038277765636642773\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08273655715253618\n",
      "Average test loss: 0.003158655865738789\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08008245684040917\n",
      "Average test loss: 0.004793787919812732\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08050783038139343\n",
      "Average test loss: 0.0032177049501074684\n",
      "Epoch 222/300\n",
      "Average training loss: 0.07895950618717405\n",
      "Average test loss: 0.0035984403253015544\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07883117105563482\n",
      "Average test loss: 0.003896427118529876\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08196634896596273\n",
      "Average test loss: 0.003460681253630254\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07843288756079143\n",
      "Average test loss: 0.003234065459834205\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07814785597059462\n",
      "Average test loss: 0.00446742871341606\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07843030761347877\n",
      "Average test loss: 0.0038242358881980182\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07825331434938643\n",
      "Average test loss: 0.0032510843837840688\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07894338434934615\n",
      "Average test loss: 0.0035436331048193906\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08011175659630033\n",
      "Average test loss: 0.039763726005951565\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08019529822799895\n",
      "Average test loss: 0.0032435089920957883\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07773427430126402\n",
      "Average test loss: 0.003628145008451409\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07980277434984843\n",
      "Average test loss: 0.003196467349926631\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0778803213040034\n",
      "Average test loss: 0.003189778141056498\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07784956488344405\n",
      "Average test loss: 0.0034770433579881985\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07871512798137136\n",
      "Average test loss: 0.004255527641210291\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07806456364856826\n",
      "Average test loss: 0.003303188297897577\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07790872812933392\n",
      "Average test loss: 0.0033149155601859093\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08016727182600233\n",
      "Average test loss: 0.003213840472822388\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07778115057282978\n",
      "Average test loss: 0.003276638039491243\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07736410485373603\n",
      "Average test loss: 0.003272095714385311\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07797262808680534\n",
      "Average test loss: 0.0035832818806585337\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07972349663906628\n",
      "Average test loss: 0.003592254229096903\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0772868754863739\n",
      "Average test loss: 0.003221792506157524\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07731544613175922\n",
      "Average test loss: 0.005616843668950928\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07754889832602607\n",
      "Average test loss: 0.0047410534462995\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08041248861286375\n",
      "Average test loss: 0.003241243820844425\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07717839083406661\n",
      "Average test loss: 0.003360545157765349\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07732740985353788\n",
      "Average test loss: 0.004035689151949353\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07726859536435869\n",
      "Average test loss: 0.0033018609411600564\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0772280715505282\n",
      "Average test loss: 0.0038002945774545274\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07730285761091445\n",
      "Average test loss: 0.0032971614038364754\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07729719342456924\n",
      "Average test loss: 0.0033372569479462174\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08111785486671659\n",
      "Average test loss: 0.003317545313388109\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07679709572262235\n",
      "Average test loss: 0.003333702954567141\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07655702169405089\n",
      "Average test loss: 0.0032772907538132535\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07665118908219867\n",
      "Average test loss: 0.003565338013900651\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07936923981375164\n",
      "Average test loss: 0.0033333134837448598\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0766258873740832\n",
      "Average test loss: 0.0036314043543405005\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07757130392392476\n",
      "Average test loss: 0.003610721329020129\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07651559093925688\n",
      "Average test loss: 0.0032738780942228106\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07747311700383822\n",
      "Average test loss: 0.003248685979594787\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07645773873726527\n",
      "Average test loss: 0.0032902972578174538\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08454844577444924\n",
      "Average test loss: 0.0032245414772381384\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07679231461551454\n",
      "Average test loss: 0.0032670038808137176\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07613006705707974\n",
      "Average test loss: 0.00325477693685227\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07629449418518279\n",
      "Average test loss: 0.0033200606850700247\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07638784765534931\n",
      "Average test loss: 0.003248805571761396\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0803550540275044\n",
      "Average test loss: 0.0034988494898296066\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07607097001208199\n",
      "Average test loss: 0.0035151255170090333\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07598572709825303\n",
      "Average test loss: 0.0034120799489319323\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07626566887232993\n",
      "Average test loss: 0.08136143376347091\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07655094940794839\n",
      "Average test loss: 0.013215149321075943\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08017187852991951\n",
      "Average test loss: 0.003190728366995851\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07579338360495037\n",
      "Average test loss: 0.006012374290989505\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07572255768378576\n",
      "Average test loss: 0.0032445081892526813\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07688815263244841\n",
      "Average test loss: 0.003429224892415934\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07616833038462532\n",
      "Average test loss: 0.004280807961192396\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07573576037420167\n",
      "Average test loss: 0.0040471022919648224\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07613074945078956\n",
      "Average test loss: 0.003309377813194361\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07761520657936732\n",
      "Average test loss: 0.004739617355581787\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07694578282038371\n",
      "Average test loss: 0.0032932523584200276\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07564491199122535\n",
      "Average test loss: 0.5314318751560317\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07561253203948338\n",
      "Average test loss: 0.003231052477740579\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08325598586930169\n",
      "Average test loss: 0.0030990213758001726\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07665105424655809\n",
      "Average test loss: 0.0033183340409563646\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07535599829090966\n",
      "Average test loss: 0.003333489496467842\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07541390742195977\n",
      "Average test loss: 0.0032914473842829465\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07540089885393779\n",
      "Average test loss: 0.0038129120541529524\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07551735667387645\n",
      "Average test loss: 0.003285470181455215\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07878670572572284\n",
      "Average test loss: 0.0035208187910417714\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07511111649870872\n",
      "Average test loss: 0.0035048114446302255\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07522589393456777\n",
      "Average test loss: 0.0033363647587183447\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07549476175838046\n",
      "Average test loss: 0.0034622102112819753\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07737514507770539\n",
      "Average test loss: 0.003284031309808294\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07510055800278982\n",
      "Average test loss: 0.003262828029795653\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07520465815067291\n",
      "Average test loss: 0.004090712902032667\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08356683282057445\n",
      "Average test loss: 0.003743595485885938\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07517747145891189\n",
      "Average test loss: 0.0032964691426604987\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0747481235894892\n",
      "Average test loss: 0.003426454092272454\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 649.3217694897122\n",
      "Average test loss: 928.72874927384\n",
      "Epoch 2/300\n",
      "Average training loss: 11.469133832295736\n",
      "Average test loss: 1.8394071906473901\n",
      "Epoch 3/300\n",
      "Average training loss: 9.804902018229166\n",
      "Average test loss: 0.062137505292892456\n",
      "Epoch 4/300\n",
      "Average training loss: 8.532003011491563\n",
      "Average test loss: 0.008648683639450205\n",
      "Epoch 5/300\n",
      "Average training loss: 6.664970188140869\n",
      "Average test loss: 1.3836389236508144\n",
      "Epoch 6/300\n",
      "Average training loss: 5.988064484490288\n",
      "Average test loss: 3.0758590097017584\n",
      "Epoch 7/300\n",
      "Average training loss: 4.995619529724121\n",
      "Average test loss: 0.006526029004818863\n",
      "Epoch 8/300\n",
      "Average training loss: 4.4065032034979925\n",
      "Average test loss: 0.006022171741972367\n",
      "Epoch 9/300\n",
      "Average training loss: 3.831635256661309\n",
      "Average test loss: 0.005857027034792635\n",
      "Epoch 10/300\n",
      "Average training loss: 3.4412292866177028\n",
      "Average test loss: 0.0049972595568332405\n",
      "Epoch 11/300\n",
      "Average training loss: 3.0317488687303333\n",
      "Average test loss: 0.03105124902518259\n",
      "Epoch 12/300\n",
      "Average training loss: 2.76092496893141\n",
      "Average test loss: 0.004683613745702637\n",
      "Epoch 13/300\n",
      "Average training loss: 2.4775860748291016\n",
      "Average test loss: 0.004629003708975183\n",
      "Epoch 14/300\n",
      "Average training loss: 2.221370864444309\n",
      "Average test loss: 0.004357301758602262\n",
      "Epoch 15/300\n",
      "Average training loss: 1.9737635641098021\n",
      "Average test loss: 0.004153717010799382\n",
      "Epoch 16/300\n",
      "Average training loss: 1.7609121710459392\n",
      "Average test loss: 0.00432234764078425\n",
      "Epoch 17/300\n",
      "Average training loss: 1.561097012731764\n",
      "Average test loss: 0.0039280185345560315\n",
      "Epoch 18/300\n",
      "Average training loss: 1.3841035475201078\n",
      "Average test loss: 0.0037687698586119545\n",
      "Epoch 19/300\n",
      "Average training loss: 1.2240604208840264\n",
      "Average test loss: 0.003931247746364938\n",
      "Epoch 20/300\n",
      "Average training loss: 1.081198840989007\n",
      "Average test loss: 0.003563983795957433\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9554184570842319\n",
      "Average test loss: 0.003476969006988737\n",
      "Epoch 22/300\n",
      "Average training loss: 0.846064397599962\n",
      "Average test loss: 0.0034546487410035396\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7499774867163764\n",
      "Average test loss: 0.0034880859553813935\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6648780061933729\n",
      "Average test loss: 0.003193109349451131\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5892714255121019\n",
      "Average test loss: 0.0033351461080213386\n",
      "Epoch 26/300\n",
      "Average training loss: 0.5211673330995772\n",
      "Average test loss: 0.003355630220638381\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4609882460435232\n",
      "Average test loss: 0.0030259124733921553\n",
      "Epoch 28/300\n",
      "Average training loss: 0.40853619498676724\n",
      "Average test loss: 0.003360837663627333\n",
      "Epoch 29/300\n",
      "Average training loss: 0.36356202909681534\n",
      "Average test loss: 0.0029811574858095912\n",
      "Epoch 30/300\n",
      "Average training loss: 0.3238534820874532\n",
      "Average test loss: 0.0030517908558249473\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2896608748965793\n",
      "Average test loss: 0.002958526718326741\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2601474767261081\n",
      "Average test loss: 0.0030105099024044144\n",
      "Epoch 33/300\n",
      "Average training loss: 0.23553637107213338\n",
      "Average test loss: 0.002768185717571113\n",
      "Epoch 34/300\n",
      "Average training loss: 0.21388252517912124\n",
      "Average test loss: 0.0027687506290773553\n",
      "Epoch 35/300\n",
      "Average training loss: 0.19560733603106606\n",
      "Average test loss: 0.00271001348644495\n",
      "Epoch 36/300\n",
      "Average training loss: 0.18127765749560462\n",
      "Average test loss: 0.0027427384642263256\n",
      "Epoch 37/300\n",
      "Average training loss: 0.16862006714608935\n",
      "Average test loss: 0.0027144593621293706\n",
      "Epoch 38/300\n",
      "Average training loss: 0.15770139797528585\n",
      "Average test loss: 0.0027275838922295307\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1487412244081497\n",
      "Average test loss: 0.0026402086791478927\n",
      "Epoch 40/300\n",
      "Average training loss: 0.14154303455352782\n",
      "Average test loss: 0.002730917296682795\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1374107227590349\n",
      "Average test loss: 0.010551324465208584\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1459392450120714\n",
      "Average test loss: 0.002863839778014355\n",
      "Epoch 43/300\n",
      "Average training loss: 0.2147056079175737\n",
      "Average test loss: 0.005192789650211731\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1761867296695709\n",
      "Average test loss: 0.0028124633863982227\n",
      "Epoch 45/300\n",
      "Average training loss: 0.13511856393019359\n",
      "Average test loss: 0.0028440334087030754\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12581362704435983\n",
      "Average test loss: 0.0027074821088463067\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1205874374906222\n",
      "Average test loss: 0.002775803140261107\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11616567205720478\n",
      "Average test loss: 0.003231526652144061\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11221876929865943\n",
      "Average test loss: 0.0026708956079350577\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10994396868679258\n",
      "Average test loss: 0.0032255761664774685\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10649305372767978\n",
      "Average test loss: 0.00257722225992216\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1050343527926339\n",
      "Average test loss: 0.0028550413845934803\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10274040496349335\n",
      "Average test loss: 0.002638726016299592\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10479645688003963\n",
      "Average test loss: 0.0026304081595606276\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10014438565572104\n",
      "Average test loss: 0.002672076104519268\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09720520665910509\n",
      "Average test loss: 0.0025391527321189642\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09505102870199415\n",
      "Average test loss: 0.0027590047323869334\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09362310928106309\n",
      "Average test loss: 0.0025522100906819104\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09211269192563162\n",
      "Average test loss: 0.002536660684686568\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09092406930526098\n",
      "Average test loss: 0.002498704673929347\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08979893312851588\n",
      "Average test loss: 0.0025456084339982935\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09040402049488491\n",
      "Average test loss: 0.002571950186250938\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08941666980584463\n",
      "Average test loss: 0.002551365111437109\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0875705769194497\n",
      "Average test loss: 0.0024875725691931114\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09860130178266101\n",
      "Average test loss: 0.003414930175162024\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09987437168094847\n",
      "Average test loss: 0.0025718308579590586\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09083451473050648\n",
      "Average test loss: 0.0024948579863541655\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08816741712225808\n",
      "Average test loss: 0.0024699890942623216\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08759100753731197\n",
      "Average test loss: 0.0025877475988947685\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08620356016688877\n",
      "Average test loss: 0.0026425042059272526\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08544585358434253\n",
      "Average test loss: 0.0024549114778637886\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08510027441713545\n",
      "Average test loss: 0.002451649106418093\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08450388839509752\n",
      "Average test loss: 0.0025556761214716567\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08374791926145554\n",
      "Average test loss: 0.0024354789118385977\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08330254160033332\n",
      "Average test loss: 0.02213667462600602\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0833257692058881\n",
      "Average test loss: 0.0028895111915965875\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08250268669260873\n",
      "Average test loss: 0.002678078322360913\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08180160548951891\n",
      "Average test loss: 0.002522266438437833\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0815849313735962\n",
      "Average test loss: 0.004443224930928813\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08110179589192072\n",
      "Average test loss: 0.004903629967321952\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08072316739956538\n",
      "Average test loss: 0.0025239264164119957\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0803612451950709\n",
      "Average test loss: 0.30690421307086946\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08021758438481225\n",
      "Average test loss: 0.0031057788961463503\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07962825811571544\n",
      "Average test loss: 0.0024486050222896866\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07972187362776863\n",
      "Average test loss: 0.002424294012081292\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07865266915162404\n",
      "Average test loss: 0.0024259241393042937\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07938770780298444\n",
      "Average test loss: 0.002590075110188789\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07824806775649389\n",
      "Average test loss: 0.005623034300075637\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07802003310786353\n",
      "Average test loss: 0.0031272000380688243\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07760670026805666\n",
      "Average test loss: 0.0024554201310707465\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07736278804805544\n",
      "Average test loss: 0.002766483647748828\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07727249569363064\n",
      "Average test loss: 0.0024460009788680407\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07697237482998107\n",
      "Average test loss: 0.0023958001777322755\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07661294108629227\n",
      "Average test loss: 73959.86115451388\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07659433538383907\n",
      "Average test loss: 0.6488693623807695\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07623895290162828\n",
      "Average test loss: 0.025474106521656117\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07586779256661733\n",
      "Average test loss: 0.0023762344339241584\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07579900564087762\n",
      "Average test loss: 0.002514926603167421\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07530852397282918\n",
      "Average test loss: 0.005502679446919097\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0774534938832124\n",
      "Average test loss: 0.0024142861823654837\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07532951264911228\n",
      "Average test loss: 0.0024585697828895517\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07522708673609628\n",
      "Average test loss: 0.007450487975652019\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07427541904317009\n",
      "Average test loss: 0.0024567789079414475\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07407636334498724\n",
      "Average test loss: 0.02299638547334406\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07407029024759929\n",
      "Average test loss: 0.019848672931806907\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07381131835116281\n",
      "Average test loss: 0.0024282809491786694\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07393060221274694\n",
      "Average test loss: 0.00241180864131699\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07381579616996978\n",
      "Average test loss: 0.00239635798562732\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07364158645934529\n",
      "Average test loss: 0.002415504693157143\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07342021570271916\n",
      "Average test loss: 0.002454958952963352\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07292585392130746\n",
      "Average test loss: 0.0024392909064061114\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07326187728510962\n",
      "Average test loss: 0.0028608861681487826\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07232698565059238\n",
      "Average test loss: 0.003991391710730063\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07234863645500607\n",
      "Average test loss: 0.002423670410282082\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07266997327407201\n",
      "Average test loss: 0.0026554710738774805\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07176574496428172\n",
      "Average test loss: 0.0025263846388293635\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0755936033129692\n",
      "Average test loss: 0.003316378989153438\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07136835307545132\n",
      "Average test loss: 0.0024794921626647313\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07144701367616653\n",
      "Average test loss: 0.002645066194443239\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07125380248824756\n",
      "Average test loss: 0.00380620645524727\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07148226163784663\n",
      "Average test loss: 0.0024878976529257163\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07101269877619214\n",
      "Average test loss: 0.00245173417031765\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07082512130339941\n",
      "Average test loss: 0.11733193379640579\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07088323433531656\n",
      "Average test loss: 0.002512056032816569\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07089856035841836\n",
      "Average test loss: 0.003344849275632037\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07028591810994678\n",
      "Average test loss: 0.004858779927715659\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07101457853449715\n",
      "Average test loss: 0.0026372179662187893\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07021306134263675\n",
      "Average test loss: 0.0025246340029148593\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06983508861727185\n",
      "Average test loss: 0.002763647038075659\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0699530758327908\n",
      "Average test loss: 0.0025386368479165767\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07078253883123398\n",
      "Average test loss: 0.003703343634183208\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06911268397834566\n",
      "Average test loss: 0.0025677564709136883\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07007154593865077\n",
      "Average test loss: 0.002496990203857422\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06915643372138341\n",
      "Average test loss: 0.0028166702397995525\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06963981297281054\n",
      "Average test loss: 0.00254709901019103\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06932129621505738\n",
      "Average test loss: 0.03426712089901169\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06915675432152218\n",
      "Average test loss: 0.002515449530341559\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07018037890725666\n",
      "Average test loss: 0.0025408838662422367\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06853117534849379\n",
      "Average test loss: 0.002602499110624194\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06847605195972654\n",
      "Average test loss: 0.006691280101322465\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06856726047065523\n",
      "Average test loss: 0.0024965426509992944\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0682019855512513\n",
      "Average test loss: 0.0030834405459463595\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06964441922638151\n",
      "Average test loss: 0.01015677914602889\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0679748701122072\n",
      "Average test loss: 0.003285555833743678\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06899406371514002\n",
      "Average test loss: 0.00334719725470576\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06776101931598451\n",
      "Average test loss: 0.01820890881617864\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06802825979391734\n",
      "Average test loss: 0.0026222827589760223\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06815451780954997\n",
      "Average test loss: 0.0025063559727536307\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06850545301040013\n",
      "Average test loss: 0.00383232093271282\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06844048348400328\n",
      "Average test loss: 0.002472341819355885\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0672097656859292\n",
      "Average test loss: 0.002558911017452677\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06836659387085173\n",
      "Average test loss: 0.010781898687283197\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06719895405239529\n",
      "Average test loss: 0.0026075261734012103\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06755213910341262\n",
      "Average test loss: 0.0025639930176031256\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06717459010415607\n",
      "Average test loss: 0.016720843699243332\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06747083217567867\n",
      "Average test loss: 0.002548019262030721\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06771109077996677\n",
      "Average test loss: 0.0025152673025925954\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06727824462453524\n",
      "Average test loss: 0.0026232506843904655\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06675518277287483\n",
      "Average test loss: 0.0025340383367405996\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0670229731698831\n",
      "Average test loss: 0.003783317781570885\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06675405330790414\n",
      "Average test loss: 0.0026146938347568114\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06727800909678142\n",
      "Average test loss: 0.0091576739102602\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06634674235847261\n",
      "Average test loss: 0.002615493946812219\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06659889784786437\n",
      "Average test loss: 0.002821177251636982\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06665478179189895\n",
      "Average test loss: 0.002584962591321932\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06655101004574034\n",
      "Average test loss: 0.0025893920324742796\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06630476265483433\n",
      "Average test loss: 0.0028317027325845427\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06697074100706313\n",
      "Average test loss: 0.0026298995866543716\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06649194371369149\n",
      "Average test loss: 0.00275332690257993\n",
      "Epoch 170/300\n",
      "Average training loss: 0.18659587895207935\n",
      "Average test loss: 0.0027025802843272688\n",
      "Epoch 171/300\n",
      "Average training loss: 0.11485041822327507\n",
      "Average test loss: 0.0030461316388100384\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09274283272690244\n",
      "Average test loss: 0.0024155467806590927\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08616405512226953\n",
      "Average test loss: 0.002401026808656752\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0823065837820371\n",
      "Average test loss: 0.0024660994021428956\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07912400794029235\n",
      "Average test loss: 0.002446514198349582\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07654394759072197\n",
      "Average test loss: 0.007127050559967756\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07496164387464524\n",
      "Average test loss: 0.0027858774177730084\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07313497492339877\n",
      "Average test loss: 0.002498151951779922\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0749208080139425\n",
      "Average test loss: 0.0046986621655523775\n",
      "Epoch 180/300\n",
      "Average training loss: 0.07068436067303022\n",
      "Average test loss: 0.0026846182534678116\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06918056347635057\n",
      "Average test loss: 0.0024768868101139865\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06868775311443541\n",
      "Average test loss: 0.0025428297043674523\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06846268679036034\n",
      "Average test loss: 0.002619140056686269\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06702857396999995\n",
      "Average test loss: 0.0124650074996882\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06725222931967842\n",
      "Average test loss: 0.003220675078117185\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07049989186724027\n",
      "Average test loss: 0.002572219666507509\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06612036853366428\n",
      "Average test loss: 0.0028038287177268003\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06616751349965731\n",
      "Average test loss: 0.0031630096247212756\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0664948826862706\n",
      "Average test loss: 0.005496101261427005\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06701916929086049\n",
      "Average test loss: 0.0029058004431426524\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06644286688168843\n",
      "Average test loss: 0.0026681213569309976\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06612240558862686\n",
      "Average test loss: 0.002685731664299965\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06627809548377991\n",
      "Average test loss: 0.0026190180730902486\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06589932825167974\n",
      "Average test loss: 0.0026975431131819883\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06562730797462993\n",
      "Average test loss: 0.009307818413194682\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06606049511167739\n",
      "Average test loss: 0.002729853255260322\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06565705067581601\n",
      "Average test loss: 0.0026399589112649363\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06608182584246\n",
      "Average test loss: 0.018049543830255668\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06551075934039222\n",
      "Average test loss: 0.00267526934026844\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06559469142887328\n",
      "Average test loss: 0.0026551007657415336\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06684043332272106\n",
      "Average test loss: 0.002745924027429687\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0648119419713815\n",
      "Average test loss: 0.0026893457281920645\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06649974030918546\n",
      "Average test loss: 0.005614817967638373\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06495128411716886\n",
      "Average test loss: 0.0025818949588057066\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06547523791922463\n",
      "Average test loss: 0.007315131847229268\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06504747469557656\n",
      "Average test loss: 0.1889409427046776\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06663360748357243\n",
      "Average test loss: 0.0026736817992188866\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06436246913671494\n",
      "Average test loss: 0.002909703193646338\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06513366936975055\n",
      "Average test loss: 0.002767035018859638\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06449503145284123\n",
      "Average test loss: 0.021367592437399757\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07024738630983565\n",
      "Average test loss: 0.0027906926137705643\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06447278460529116\n",
      "Average test loss: 0.004055794778383441\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0643155242005984\n",
      "Average test loss: 0.003113804518348641\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06501116991705365\n",
      "Average test loss: 0.0035280957095738916\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0643208139538765\n",
      "Average test loss: 0.0026238350603315567\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06616374668810103\n",
      "Average test loss: 0.003038632517473565\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06417674707041847\n",
      "Average test loss: 0.003815354153720869\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06479246930943595\n",
      "Average test loss: 0.0026152882389724255\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06447440696424908\n",
      "Average test loss: 0.002972343698143959\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06426680917872322\n",
      "Average test loss: 0.002837255119242602\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0648485348423322\n",
      "Average test loss: 0.0039098219055061535\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06435404737128152\n",
      "Average test loss: 0.0026031548952062927\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06416677041848501\n",
      "Average test loss: 0.002666714314578308\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06470012882020738\n",
      "Average test loss: 0.0038382475181586214\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06416229105989138\n",
      "Average test loss: 0.0035895101436310343\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06372124768628014\n",
      "Average test loss: 0.005028334421623084\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06400392484664917\n",
      "Average test loss: 0.0025443999612083036\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06416779547598626\n",
      "Average test loss: 0.0025807259616752467\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06468522667222552\n",
      "Average test loss: 0.002584099138776461\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06349609315726493\n",
      "Average test loss: 0.0025957780205127265\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06367966516812643\n",
      "Average test loss: 0.004623142189035813\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06382094515032238\n",
      "Average test loss: 0.0027366151170184214\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0677269317706426\n",
      "Average test loss: 0.0026765543673601417\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06308976354201634\n",
      "Average test loss: 0.002656194303184748\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06330484464764595\n",
      "Average test loss: 0.0026222365426106587\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06345389615164863\n",
      "Average test loss: 0.0032189960132042567\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06366052255365584\n",
      "Average test loss: 0.0028652430896957716\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06333781331115299\n",
      "Average test loss: 0.003031730839568708\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06348808776007758\n",
      "Average test loss: 0.0029527218648129037\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0651949496600363\n",
      "Average test loss: 0.0026112673042549025\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06300203990936279\n",
      "Average test loss: 0.0027014206411937873\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0652237843606207\n",
      "Average test loss: 0.002884362498919169\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06319002186920908\n",
      "Average test loss: 0.002917622480334507\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06358776400817766\n",
      "Average test loss: 0.0029337224298053317\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06324940364228354\n",
      "Average test loss: 0.006848354929851161\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0629789025121265\n",
      "Average test loss: 0.004759237348619435\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06423704120847915\n",
      "Average test loss: 0.3323794322941038\n",
      "Epoch 248/300\n",
      "Average training loss: 0.19991061582167943\n",
      "Average test loss: 0.004591404403249423\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08922739761405521\n",
      "Average test loss: 0.0031640993671284783\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08289745834138658\n",
      "Average test loss: 0.0025481405907000106\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07844826220803791\n",
      "Average test loss: 0.0024242393302006853\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07489621311426163\n",
      "Average test loss: 0.002467915280825562\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07173821236027611\n",
      "Average test loss: 0.040155583447880214\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06869150051143434\n",
      "Average test loss: 0.0025637579007695118\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0666493750611941\n",
      "Average test loss: 0.0028413086771551105\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0646635234951973\n",
      "Average test loss: 0.002703429972752929\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06920357518725925\n",
      "Average test loss: 0.002881015056744218\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06302593906720479\n",
      "Average test loss: 0.0025847782155291902\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06266548091173171\n",
      "Average test loss: 0.0034651681253065666\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06311275313297908\n",
      "Average test loss: 0.0106812278230985\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06336779780520334\n",
      "Average test loss: 0.002571095651015639\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06355744126770231\n",
      "Average test loss: 0.002701805244303412\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06295611369940969\n",
      "Average test loss: 0.023879119695888625\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06300490424368116\n",
      "Average test loss: 0.0026771771498024463\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06366801176468531\n",
      "Average test loss: 0.002616459712593092\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0630723766386509\n",
      "Average test loss: 0.0027483906615525483\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06386311385697789\n",
      "Average test loss: 0.0026360777461280427\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06284920026196374\n",
      "Average test loss: 0.0026066151371018755\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06306888004144033\n",
      "Average test loss: 0.0029115396903620825\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06274131627546417\n",
      "Average test loss: 0.003005147758871317\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06330747401714325\n",
      "Average test loss: 0.0027161300724579227\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06963430500030518\n",
      "Average test loss: 0.0025857020628949007\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06237289246585634\n",
      "Average test loss: 0.7875869865649276\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06270088536209531\n",
      "Average test loss: 0.006212529171051251\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06249070578482416\n",
      "Average test loss: 0.002749624556137456\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06320176740487417\n",
      "Average test loss: 0.0031171277610378134\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06250474703974195\n",
      "Average test loss: 0.002586814783513546\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06277876612212922\n",
      "Average test loss: 0.0029580177292227745\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06328567422760857\n",
      "Average test loss: 0.0025838055877635876\n",
      "Epoch 280/300\n",
      "Average training loss: 0.062878326912721\n",
      "Average test loss: 0.002649801227160626\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06206491282251146\n",
      "Average test loss: 0.0026964272138559156\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06435813424984614\n",
      "Average test loss: 0.026878544995354282\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06221146341827181\n",
      "Average test loss: 0.002778372708087166\n",
      "Epoch 284/300\n",
      "Average training loss: 0.062248545481099024\n",
      "Average test loss: 0.002636313637304637\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06287033526764976\n",
      "Average test loss: 0.011020580980512831\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06254781393872368\n",
      "Average test loss: 0.0034839905057516365\n",
      "Epoch 287/300\n",
      "Average training loss: 0.061929411186112296\n",
      "Average test loss: 0.004525667779975468\n",
      "Epoch 288/300\n",
      "Average training loss: 0.062396815889411504\n",
      "Average test loss: 0.003987227225676179\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0628427742520968\n",
      "Average test loss: 0.0025941252240704164\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0657752914627393\n",
      "Average test loss: 0.00260874176170263\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06232108106878069\n",
      "Average test loss: 0.003463510869277848\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06153516403834025\n",
      "Average test loss: 0.004550712366898854\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06197271139423052\n",
      "Average test loss: 0.004554774083197117\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06279425364070468\n",
      "Average test loss: 0.004112615471498834\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06280374513732062\n",
      "Average test loss: 0.002996930058942073\n",
      "Epoch 296/300\n",
      "Average training loss: 0.061921771612432265\n",
      "Average test loss: 0.0026331406638233196\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06163196461730533\n",
      "Average test loss: 0.0026240566422541934\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06762445754143927\n",
      "Average test loss: 0.0029562494543691475\n",
      "Epoch 299/300\n",
      "Average training loss: 0.061387699676884545\n",
      "Average test loss: 0.0026740146945748063\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0634824144244194\n",
      "Average test loss: 0.002603729922324419\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.52\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.44\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.70\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 23.86\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 23.96\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.07\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.20\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.28\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.35\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 24.40\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.40\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 24.46\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 24.52\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 24.56\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 24.56\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 24.56\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 24.63\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 24.59\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 24.65\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 24.68\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 24.72\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 24.73\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 24.78\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 24.79\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 24.84\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 24.86\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 24.89\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.03\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.10\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.29\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.80\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.99\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.17\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.43\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.43\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.41\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.38\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.51\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.53\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.54\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.58\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.61\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.72\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.23\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.17\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.53\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.57\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.60\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.69\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.37\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.45\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.58\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.63\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
