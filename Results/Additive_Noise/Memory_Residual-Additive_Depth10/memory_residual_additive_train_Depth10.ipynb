{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.05)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.05)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.05)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.05)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2864944607151879\n",
      "Average test loss: 0.014755847568313281\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1071389783554607\n",
      "Average test loss: 0.012208811956975195\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0868615259329478\n",
      "Average test loss: 0.014336644759608639\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07799991805023618\n",
      "Average test loss: 0.00970499063614342\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0718730349275801\n",
      "Average test loss: 0.02960692480372058\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06647217667102814\n",
      "Average test loss: 0.01014347677015596\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06480299909247292\n",
      "Average test loss: 0.016978985354304314\n",
      "Epoch 8/300\n",
      "Average training loss: 0.060945913904243046\n",
      "Average test loss: 0.010076100475672218\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05945470257600149\n",
      "Average test loss: 0.00922777377648486\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05726497694849968\n",
      "Average test loss: 0.010330139550069967\n",
      "Epoch 11/300\n",
      "Average training loss: 0.054853465802139706\n",
      "Average test loss: 0.014391901678509183\n",
      "Epoch 12/300\n",
      "Average training loss: 0.053089431040816835\n",
      "Average test loss: 0.010031330113609632\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05119339124030537\n",
      "Average test loss: 0.008368847868508764\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04966268896394306\n",
      "Average test loss: 0.016662950023180908\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04806968542933464\n",
      "Average test loss: 0.008072791007657846\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04676321166422632\n",
      "Average test loss: 0.01139953075018194\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04602023012108273\n",
      "Average test loss: 0.007866985111186902\n",
      "Epoch 18/300\n",
      "Average training loss: 0.044787181354231306\n",
      "Average test loss: 0.007405485558840964\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04464751800232463\n",
      "Average test loss: 0.008359903319014443\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04362362281150288\n",
      "Average test loss: 0.007945132160352336\n",
      "Epoch 21/300\n",
      "Average training loss: 0.042841358757681317\n",
      "Average test loss: 0.007609688484006458\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04213615904251734\n",
      "Average test loss: 0.011371485687792301\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04175032823284467\n",
      "Average test loss: 0.007445640446411239\n",
      "Epoch 24/300\n",
      "Average training loss: 0.040984904646873474\n",
      "Average test loss: 0.0077247143064936\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0404863664607207\n",
      "Average test loss: 0.007451970784200563\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04021283789972464\n",
      "Average test loss: 0.0070231289358602626\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03985942743221919\n",
      "Average test loss: 0.008604256018168397\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039476702375544445\n",
      "Average test loss: 0.00746036441044675\n",
      "Epoch 29/300\n",
      "Average training loss: 0.039134792745113375\n",
      "Average test loss: 0.09097638868788878\n",
      "Epoch 30/300\n",
      "Average training loss: 0.038735984282361136\n",
      "Average test loss: 0.008258654236793518\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03845246281557613\n",
      "Average test loss: 0.007180382769140932\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03810849185453521\n",
      "Average test loss: 0.00723915463189284\n",
      "Epoch 33/300\n",
      "Average training loss: 0.038057739476362866\n",
      "Average test loss: 0.007284352742135525\n",
      "Epoch 34/300\n",
      "Average training loss: 0.037503855998317404\n",
      "Average test loss: 0.0071693146253625556\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03727241641614172\n",
      "Average test loss: 0.00705549303152495\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03712619842092196\n",
      "Average test loss: 0.008347876658870114\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03701652453011937\n",
      "Average test loss: 0.007022651069280174\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03684964672062132\n",
      "Average test loss: 0.00696118160088857\n",
      "Epoch 39/300\n",
      "Average training loss: 0.036438534875710804\n",
      "Average test loss: 0.006952440586354997\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03632376140356064\n",
      "Average test loss: 0.007164010911766026\n",
      "Epoch 41/300\n",
      "Average training loss: 0.036065006323986584\n",
      "Average test loss: 0.0069725035308963726\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03594553611013625\n",
      "Average test loss: 0.008664620594018036\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03569983075724708\n",
      "Average test loss: 0.006925378443466293\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03558454076780213\n",
      "Average test loss: 0.009310575674805376\n",
      "Epoch 45/300\n",
      "Average training loss: 0.035387336992555196\n",
      "Average test loss: 0.006894518146084414\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03521356440583865\n",
      "Average test loss: 0.007973714862846665\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03507160775529014\n",
      "Average test loss: 0.007070500324169795\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03468539484673076\n",
      "Average test loss: 0.006925345454778936\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03462073296639654\n",
      "Average test loss: 0.00728487702500489\n",
      "Epoch 52/300\n",
      "Average training loss: 0.034535421868165335\n",
      "Average test loss: 0.006861397899273369\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03437055276499854\n",
      "Average test loss: 0.007024781549142467\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03427107393079334\n",
      "Average test loss: 0.007118788871500227\n",
      "Epoch 55/300\n",
      "Average training loss: 0.034090108543634416\n",
      "Average test loss: 0.011450008571975761\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03408964296181997\n",
      "Average test loss: 0.007147526361462143\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03389491188526154\n",
      "Average test loss: 0.0067756900042295455\n",
      "Epoch 58/300\n",
      "Average training loss: 0.033844272401597764\n",
      "Average test loss: 0.008903451214234035\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03364747492472331\n",
      "Average test loss: 0.006942105240292019\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03359779940711127\n",
      "Average test loss: 0.008811654498593675\n",
      "Epoch 61/300\n",
      "Average training loss: 0.033642904063065845\n",
      "Average test loss: 0.007182341298295392\n",
      "Epoch 62/300\n",
      "Average training loss: 0.033478512505690255\n",
      "Average test loss: 0.007276201449334621\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03336570639080472\n",
      "Average training loss: 0.032936468617783654\n",
      "Average test loss: 0.006899562591479884\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03293934808505906\n",
      "Average test loss: 0.0069665045539538065\n",
      "Epoch 69/300\n",
      "Average training loss: 0.032826601871185836\n",
      "Average test loss: 0.013824912368423408\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03281822943687439\n",
      "Average test loss: 0.0068661858100030155\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03278355782892969\n",
      "Average test loss: 0.007308937977585528\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03267277416918013\n",
      "Average test loss: 0.007429985472725498\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03252868024673727\n",
      "Average test loss: 0.008170767453809579\n",
      "Epoch 74/300\n",
      "Average training loss: 0.032561547742949594\n",
      "Average test loss: 0.007099257703456614\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03244846684734026\n",
      "Average test loss: 0.007556045078568988\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03242948110567199\n",
      "Average test loss: 0.006866438150405884\n",
      "Epoch 77/300\n",
      "Average training loss: 0.032315344459480706\n",
      "Average test loss: 0.007127401821315289\n",
      "Epoch 78/300\n",
      "Average training loss: 0.032244922151168184\n",
      "Average test loss: 0.00686202755322059\n",
      "Epoch 79/300\n",
      "Average training loss: 0.032099773335787986\n",
      "Average test loss: 0.00713938646680779\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03209277674886916\n",
      "Average test loss: 0.00729015194127957\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0321126660572158\n",
      "Average test loss: 0.008920077844626374\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03206581464409828\n",
      "Average test loss: 0.0073814137714604535\n",
      "Epoch 83/300\n",
      "Average training loss: 0.031982011715571086\n",
      "Average test loss: 0.0069852650099330475\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0318306951969862\n",
      "Average test loss: 0.007845637200607193\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03189713139004177\n",
      "Average test loss: 0.007224555669145451\n",
      "Epoch 86/300\n",
      "Average training loss: 0.031762084126472474\n",
      "Average test loss: 0.007007561772647831\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03170119102961487\n",
      "Average test loss: 0.0071024317650331395\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03176583050853676\n",
      "Average test loss: 0.007317163883397977\n",
      "Epoch 89/300\n",
      "Average training loss: 0.031617641932434506\n",
      "Average test loss: 0.00698103681868977\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03150083160069254\n",
      "Average test loss: 0.00804443022236228\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03159367963837253\n",
      "Average test loss: 0.006922728043463495\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03153612907727559\n",
      "Average test loss: 0.05818335652351379\n",
      "Epoch 93/300\n",
      "Average training loss: 0.031310486889547774\n",
      "Average test loss: 0.007642071751670705\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0313049040403631\n",
      "Average test loss: 0.00712671395101481\n",
      "Epoch 95/300\n",
      "Average training loss: 0.031240044623613357\n",
      "Average test loss: 0.009086615431639883\n",
      "Epoch 96/300\n",
      "Average training loss: 0.031128148552444244\n",
      "Average test loss: 0.01273990797996521\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0310756999436352\n",
      "Average test loss: 0.007606310741768943\n",
      "Epoch 101/300\n",
      "Average training loss: 0.031123218993345896\n",
      "Average test loss: 0.006858027627898587\n",
      "Epoch 102/300\n",
      "Average training loss: 0.031111743812759718\n",
      "Average test loss: 0.007519667220198446\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03097140499121613\n",
      "Average test loss: 0.007096132878214121\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03088726548353831\n",
      "Average test loss: 0.008399836503797107\n",
      "Epoch 105/300\n",
      "Average training loss: 0.030863486733701494\n",
      "Average test loss: 0.007065356254577637\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03088631776968638\n",
      "Average test loss: 0.032018556445837024\n",
      "Epoch 107/300\n",
      "Average training loss: 0.030850627270009784\n",
      "Average test loss: 0.007478653576638963\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03076425865292549\n",
      "Average test loss: 0.012311656734181775\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03072964525885052\n",
      "Average test loss: 0.007481229035804669\n",
      "Epoch 110/300\n",
      "Average training loss: 0.030720922182003656\n",
      "Average test loss: 0.00782179207023647\n",
      "Epoch 111/300\n",
      "Average training loss: 0.030698531033264267\n",
      "Average test loss: 0.024022388989726702\n",
      "Epoch 112/300\n",
      "Average training loss: 0.030542095495594872\n",
      "Average test loss: 0.00725623357668519\n",
      "Epoch 113/300\n",
      "Average training loss: 0.030502509401904212\n",
      "Average test loss: 0.008371379466106494\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03053509352604548\n",
      "Average test loss: 0.006961946614086628\n",
      "Epoch 115/300\n",
      "Average training loss: 0.030511610047684774\n",
      "Average test loss: 0.007171922975116306\n",
      "Epoch 116/300\n",
      "Average training loss: 0.030546490160955325\n",
      "Average test loss: 0.0077142316963937545\n",
      "Epoch 117/300\n",
      "Average training loss: 0.030505189635687404\n",
      "Average test loss: 0.0073647469087607335\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03039848957790269\n",
      "Average test loss: 0.007721697728666994\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030322623410158688\n",
      "Average test loss: 0.5216706299814913\n",
      "Epoch 120/300\n",
      "Average training loss: 0.030438570737838747\n",
      "Average test loss: 0.012988468035641644\n",
      "Epoch 121/300\n",
      "Average training loss: 0.030305525195267465\n",
      "Average test loss: 0.007870011212925116\n",
      "Epoch 122/300\n",
      "Average training loss: 0.030171597545345624\n",
      "Average test loss: 0.0073833618006772465\n",
      "Epoch 123/300\n",
      "Average training loss: 0.030306795643435584\n",
      "Average test loss: 0.009149280154870616\n",
      "Epoch 124/300\n",
      "Average training loss: 0.030233492639329698\n",
      "Average test loss: 0.007031704290045632\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03014143297738499\n",
      "Average test loss: 0.0074396308337648705\n",
      "Epoch 126/300\n",
      "Average training loss: 0.030172399904992844\n",
      "Average test loss: 0.007420228077305688\n",
      "Epoch 127/300\n",
      "Average training loss: 0.030031619510716864\n",
      "Average test loss: 0.007178279040588273\n",
      "Epoch 128/300\n",
      "Average training loss: 0.030149359467956754\n",
      "Average test loss: 0.007424755533536275\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03012094399664137\n",
      "Average test loss: 0.022760175611409875\n",
      "Epoch 130/300\n",
      "Average training loss: 0.030062140040927464\n",
      "Average test loss: 0.007321609492103259\n",
      "Epoch 131/300\n",
      "Average training loss: 0.029988390452331967\n",
      "Average test loss: 0.010755186191035642\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03001311214764913\n",
      "Average test loss: 0.007532151147309277\n",
      "Epoch 133/300\n",
      "Average training loss: 0.029887661692168978\n",
      "Average test loss: 0.007576547524995274\n",
      "Epoch 134/300\n",
      "Average training loss: 0.029984694671299723\n",
      "Average test loss: 0.007602074748526017\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02988861464791828\n",
      "Average test loss: 0.02420267519685957\n",
      "Epoch 136/300\n",
      "Average training loss: 0.029791488624281354\n",
      "Average test loss: 0.007547770724114444\n",
      "Epoch 137/300\n",
      "Average training loss: 0.029877870013316474\n",
      "Average test loss: 0.0074650045418077045\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02979402748081419\n",
      "Average test loss: 0.012290431161721547\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02971045254005326\n",
      "Average test loss: 0.011226616963744164\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03017131613360511\n",
      "Average test loss: 0.007131807333065404\n",
      "Epoch 141/300\n",
      "Average training loss: 0.029681433296865886\n",
      "Average test loss: 0.008828099758674702\n",
      "Epoch 142/300\n",
      "Average training loss: 0.029662100310126942\n",
      "Average test loss: 0.007210705490575896\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0295595642791854\n",
      "Average test loss: 0.0070802300915949875\n",
      "Epoch 144/300\n",
      "Average training loss: 0.029708706776301066\n",
      "Average test loss: 0.026716787168549166\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0295538436141279\n",
      "Average test loss: 0.007076080300741725\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02954778997103373\n",
      "Average test loss: 0.009730951191650497\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02966660095916854\n",
      "Average test loss: 0.008368107654568222\n",
      "Epoch 148/300\n",
      "Average training loss: 0.029563030847244792\n",
      "Average test loss: 0.007853813609315289\n",
      "Epoch 149/300\n",
      "Average training loss: 0.029546741033593813\n",
      "Average test loss: 0.008741883917815156\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02948377299805482\n",
      "Average test loss: 0.007578968911121289\n",
      "Epoch 151/300\n",
      "Average training loss: 0.029521548908617763\n",
      "Average test loss: 0.007567110904388958\n",
      "Epoch 152/300\n",
      "Average training loss: 0.029517896991637017\n",
      "Average test loss: 0.007352299240728219\n",
      "Epoch 153/300\n",
      "Average training loss: 0.029444987883170445\n",
      "Average test loss: 0.007299082097907861\n",
      "Epoch 154/300\n",
      "Average training loss: 0.029382780687676534\n",
      "Average test loss: 0.007238607575496038\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02938554860651493\n",
      "Average test loss: 0.007348783912344111\n",
      "Epoch 156/300\n",
      "Average training loss: 0.029341129594379\n",
      "Average test loss: 0.008224861718714237\n",
      "Epoch 157/300\n",
      "Average training loss: 0.029410060438844893\n",
      "Average test loss: 0.012370722020251884\n",
      "Epoch 158/300\n",
      "Average training loss: 0.029297970321443347\n",
      "Average test loss: 0.007901810485455725\n",
      "Epoch 159/300\n",
      "Average training loss: 0.029324346267514758\n",
      "Average test loss: 0.007403560315569242\n",
      "Epoch 160/300\n",
      "Average training loss: 0.029282991766929625\n",
      "Average test loss: 0.007141915211247073\n",
      "Epoch 161/300\n",
      "Average training loss: 0.029202478064431086\n",
      "Average test loss: 0.0071916141861842735\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029235346257686614\n",
      "Average test loss: 0.007510603932456838\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029209125833378896\n",
      "Average test loss: 0.007442915027754174\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02926313575108846\n",
      "Average test loss: 0.007287753835320473\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02916363967458407\n",
      "Average test loss: 0.0077436473166777025\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029148588725262218\n",
      "Average test loss: 0.0077903161495923995\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02917027596798208\n",
      "Average test loss: 0.061859447850121395\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02912508656581243\n",
      "Average test loss: 0.007448405851920446\n",
      "Epoch 169/300\n",
      "Average training loss: 0.029101020554701488\n",
      "Average test loss: 0.0077193217252691584\n",
      "Epoch 170/300\n",
      "Average training loss: 0.029101449792583785\n",
      "Average test loss: 0.007378113080643945\n",
      "Epoch 171/300\n",
      "Average training loss: 0.029114713930421407\n",
      "Average test loss: 0.007391424499452114\n",
      "Epoch 172/300\n",
      "Average training loss: 0.028964437829123602\n",
      "Average test loss: 0.008171826906502247\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029032871892054876\n",
      "Average test loss: 0.01740022309290038\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029008778472741446\n",
      "Average test loss: 0.0073203669786453245\n",
      "Epoch 175/300\n",
      "Average training loss: 0.029032465285725062\n",
      "Average test loss: 0.008002318001869652\n",
      "Epoch 176/300\n",
      "Average training loss: 0.029016692669855224\n",
      "Average test loss: 0.00769770756115516\n",
      "Epoch 177/300\n",
      "Average training loss: 0.028937241410215697\n",
      "Average test loss: 0.007287142848802938\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02895819698439704\n",
      "Average test loss: 0.0076027191082636515\n",
      "Epoch 179/300\n",
      "Average training loss: 0.028891464100943672\n",
      "Average test loss: 0.007366100665181875\n",
      "Epoch 180/300\n",
      "Average training loss: 0.028873499898446932\n",
      "Average test loss: 0.008116280698113971\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02890025168326166\n",
      "Average test loss: 0.007436437190820774\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02886347443362077\n",
      "Average test loss: 0.0073769558813009\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02886292925808165\n",
      "Average test loss: 0.020707305914825863\n",
      "Epoch 184/300\n",
      "Average training loss: 0.028912663885288768\n",
      "Average test loss: 0.007243836635930671\n",
      "Epoch 185/300\n",
      "Average training loss: 0.028768737349245285\n",
      "Average test loss: 0.007842817972103755\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028822713606887394\n",
      "Average test loss: 0.016515892133116723\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02870329310496648\n",
      "Average test loss: 0.00738934648119741\n",
      "Epoch 188/300\n",
      "Average training loss: 0.028878763877683216\n",
      "Average test loss: 0.009375859663718277\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02886757672495312\n",
      "Average test loss: 0.008055783947308858\n",
      "Epoch 190/300\n",
      "Average training loss: 0.028678081184625625\n",
      "Average test loss: 0.007387881122529506\n",
      "Epoch 191/300\n",
      "Average training loss: 0.028658717913760078\n",
      "Average test loss: 0.007418914709654119\n",
      "Epoch 192/300\n",
      "Average training loss: 0.028806278851297166\n",
      "Average test loss: 0.007265300708512465\n",
      "Epoch 193/300\n",
      "Average training loss: 0.028659541447957355\n",
      "Average test loss: 0.007305187304400736\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02861108279393779\n",
      "Average test loss: 0.007781146702667077\n",
      "Epoch 195/300\n",
      "Average training loss: 0.028660579472780227\n",
      "Average test loss: 0.019476276073190902\n",
      "Epoch 196/300\n",
      "Average training loss: 0.028633627037207284\n",
      "Average test loss: 0.007515847506622473\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028678683676653438\n",
      "Average test loss: 0.00736970276840859\n",
      "Epoch 198/300\n",
      "Average training loss: 0.028565961788098018\n",
      "Average test loss: 0.04877316007018089\n",
      "Epoch 199/300\n",
      "Average training loss: 0.028551365415255228\n",
      "Average test loss: 0.007452362546490298\n",
      "Epoch 200/300\n",
      "Average training loss: 0.028535328437884647\n",
      "Average test loss: 0.007357327635089557\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02860593182179663\n",
      "Average test loss: 0.007607919048931863\n",
      "Epoch 202/300\n",
      "Average training loss: 0.028650679118103452\n",
      "Average test loss: 0.008342105057504441\n",
      "Epoch 203/300\n",
      "Average training loss: 0.028501259750790067\n",
      "Average test loss: 0.007516434929437108\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02856825269261996\n",
      "Average test loss: 0.007643092644297415\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028458080861303543\n",
      "Average test loss: 0.007518406296769778\n",
      "Epoch 206/300\n",
      "Average training loss: 0.028556983977556228\n",
      "Average test loss: 0.008707401030593448\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02843417289521959\n",
      "Average test loss: 0.00739503106681837\n",
      "Epoch 208/300\n",
      "Average training loss: 0.028435406625270842\n",
      "Average test loss: 0.007908332381811408\n",
      "Epoch 209/300\n",
      "Average training loss: 0.028460240652163824\n",
      "Average test loss: 0.007613349587966998\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028433321840233273\n",
      "Average test loss: 0.007200859372814497\n",
      "Epoch 211/300\n",
      "Average training loss: 0.028399078735046917\n",
      "Average test loss: 0.01766346788737509\n",
      "Epoch 212/300\n",
      "Average training loss: 0.028336955845355987\n",
      "Average test loss: 0.00749864837113354\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02836745411488745\n",
      "Average test loss: 0.007255106865531868\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02835893559290303\n",
      "Average test loss: 0.00740531278691358\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02827963971098264\n",
      "Average test loss: 0.0077385183456871245\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02840301501750946\n",
      "Average test loss: 0.014141465226809183\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02830935567451848\n",
      "Average test loss: 0.007532146874401305\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028303368349870045\n",
      "Average test loss: 0.007715743496186204\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02826492968532774\n",
      "Average test loss: 0.00763314673387342\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028214688786202008\n",
      "Average test loss: 0.010694875439835919\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028350788452559048\n",
      "Average test loss: 0.007670995380315516\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02830343961715698\n",
      "Average test loss: 0.008165830796791448\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02819311590658294\n",
      "Average test loss: 0.00748286420893338\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02821926735838254\n",
      "Average test loss: 0.012268603013621437\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02814465703566869\n",
      "Average test loss: 0.007731782088677089\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028165875054068035\n",
      "Average test loss: 0.007693870960010423\n",
      "Epoch 227/300\n",
      "Average training loss: 0.028162277870708043\n",
      "Average test loss: 0.007892402061571678\n",
      "Epoch 228/300\n",
      "Average training loss: 0.028226531154579588\n",
      "Average test loss: 0.0073453063186672\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028196062088012695\n",
      "Average test loss: 0.007844373969568146\n",
      "Epoch 230/300\n",
      "Average training loss: 0.028114135763711398\n",
      "Average test loss: 0.007886048796276251\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02817474277979798\n",
      "Average test loss: 0.007613940060966545\n",
      "Epoch 232/300\n",
      "Average training loss: 0.028138220849964354\n",
      "Average test loss: 0.014859745462735495\n",
      "Epoch 233/300\n",
      "Average training loss: 0.028132433725727927\n",
      "Average test loss: 0.13168641688426336\n",
      "Epoch 234/300\n",
      "Average training loss: 0.028155524098210864\n",
      "Average test loss: 0.007425413144959344\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02805648766623603\n",
      "Average test loss: 0.007408365148223108\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02806601292391618\n",
      "Average test loss: 0.008053396969205803\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028056364196870062\n",
      "Average test loss: 0.007646677906314532\n",
      "Epoch 238/300\n",
      "Average training loss: 0.028047121276458104\n",
      "Average test loss: 0.0076707396879792215\n",
      "Epoch 239/300\n",
      "Average training loss: 0.028025239686171214\n",
      "Average test loss: 0.00897385593959027\n",
      "Epoch 240/300\n",
      "Average training loss: 0.028165175057119792\n",
      "Average test loss: 0.09251993706491259\n",
      "Epoch 241/300\n",
      "Average training loss: 0.028033448547124862\n",
      "Average test loss: 0.011241210353457265\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027968867351611457\n",
      "Average test loss: 0.007653505308760537\n",
      "Epoch 243/300\n",
      "Average training loss: 0.027975707600514096\n",
      "Average test loss: 0.007403010520670149\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02799774714310964\n",
      "Average test loss: 0.017451273750513793\n",
      "Epoch 245/300\n",
      "Average training loss: 0.028007958938678107\n",
      "Average test loss: 0.007390572976734903\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02799363833003574\n",
      "Average test loss: 0.007796686409248246\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02798901590042644\n",
      "Average test loss: 0.007725352197057671\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02786305065287484\n",
      "Average test loss: 0.007278773558222585\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028001003166039786\n",
      "Average test loss: 0.007374863841881354\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027902284373839695\n",
      "Average test loss: 0.008049010093427367\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02789779727988773\n",
      "Average test loss: 0.007746200850440396\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02789668244123459\n",
      "Average test loss: 0.007372540794726875\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027988148265414767\n",
      "Average test loss: 0.007392481796443463\n",
      "Epoch 254/300\n",
      "Average training loss: 0.027922692123386595\n",
      "Average test loss: 0.007436741633547677\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02794407950176133\n",
      "Average test loss: 0.007301110185061892\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027779414551125634\n",
      "Average test loss: 0.06683132389850087\n",
      "Epoch 257/300\n",
      "Average training loss: 0.027839303640855684\n",
      "Average test loss: 0.007506398511429628\n",
      "Epoch 258/300\n",
      "Average training loss: 0.027914918517072996\n",
      "Average test loss: 0.007766507195929686\n",
      "Epoch 259/300\n",
      "Average training loss: 0.027836162163151636\n",
      "Average test loss: 0.009363384111473957\n",
      "Epoch 260/300\n",
      "Average training loss: 0.027793236770563657\n",
      "Average test loss: 0.009114612674547566\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027846725165843963\n",
      "Average test loss: 0.007806846227910783\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027876455381512642\n",
      "Average test loss: 0.008663983910448021\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02779380693866147\n",
      "Average test loss: 0.007604731459998422\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02775946260823144\n",
      "Average test loss: 0.007659706095854441\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027794239926669333\n",
      "Average test loss: 0.007847751363284057\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02769188694490327\n",
      "Average test loss: 0.007451029541591803\n",
      "Epoch 267/300\n",
      "Average training loss: 0.027804992496967315\n",
      "Average test loss: 0.0076002938532167014\n",
      "Epoch 268/300\n",
      "Average training loss: 0.027703432409299743\n",
      "Average test loss: 0.007519013017416001\n",
      "Epoch 269/300\n",
      "Average training loss: 0.027799016119705307\n",
      "Average test loss: 0.00813719943828053\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027746229880385928\n",
      "Average test loss: 0.007547428747018178\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027655430517262884\n",
      "Average test loss: 0.007661940782020489\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027724217014180288\n",
      "Average test loss: 0.007386198955277602\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027651527302132713\n",
      "Average test loss: 0.007559460140764713\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027724479072623782\n",
      "Average test loss: 0.007524548379911317\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027713716651002567\n",
      "Average test loss: 0.007552459760672516\n",
      "Epoch 276/300\n",
      "Average training loss: 0.027587392398052746\n",
      "Average test loss: 0.0078070139893227156\n",
      "Epoch 277/300\n",
      "Average training loss: 0.028131692661179437\n",
      "Average test loss: 0.0072169174262219\n",
      "Epoch 278/300\n",
      "Average training loss: 0.027865416690707207\n",
      "Average test loss: 0.007983011717597643\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027619881119992997\n",
      "Average test loss: 0.007475092026094595\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027567653621236483\n",
      "Average test loss: 0.00806373871035046\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027640185869402357\n",
      "Average test loss: 0.028948571016391117\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02757328888111644\n",
      "Average test loss: 0.0074739638210998645\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02759627751674917\n",
      "Average test loss: 0.007810565091669559\n",
      "Epoch 284/300\n",
      "Average training loss: 0.027627795366777314\n",
      "Average test loss: 0.008043281314273676\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027694123845961358\n",
      "Average test loss: 0.007573911614302132\n",
      "Epoch 286/300\n",
      "Average training loss: 0.027540269929501746\n",
      "Average test loss: 0.007551713921957546\n",
      "Epoch 287/300\n",
      "Average training loss: 0.027556762715180715\n",
      "Average test loss: 0.007402469093600909\n",
      "Epoch 288/300\n",
      "Average training loss: 0.027478561740782524\n",
      "Average test loss: 0.007628950287070539\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02750508181585206\n",
      "Average test loss: 0.008513556364923715\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02759115931722853\n",
      "Average test loss: 0.008064301129016611\n",
      "Epoch 291/300\n",
      "Average training loss: 0.027490295910172994\n",
      "Average test loss: 0.007840153409789006\n",
      "Epoch 292/300\n",
      "Average training loss: 0.027488289647632176\n",
      "Average test loss: 0.009748387626475757\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02758937813507186\n",
      "Average test loss: 0.008417619965142674\n",
      "Epoch 294/300\n",
      "Average training loss: 0.027453344856699306\n",
      "Average test loss: 0.007614286476539241\n",
      "Epoch 295/300\n",
      "Average training loss: 0.027459135082032946\n",
      "Average test loss: 0.03859798332883252\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02742261782619688\n",
      "Average test loss: 0.00762533800676465\n",
      "Epoch 297/300\n",
      "Average training loss: 0.027618027024798923\n",
      "Average test loss: 0.007902481503784656\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027531431742840343\n",
      "Average test loss: 0.007761279063092338\n",
      "Epoch 299/300\n",
      "Average training loss: 0.027468904664119085\n",
      "Average test loss: 0.007720106009807852\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02743618130021625\n",
      "Average test loss: 0.008359336835642656\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2758850577804777\n",
      "Average test loss: 0.015651149191790156\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09903981300195058\n",
      "Average test loss: 0.010087005169027382\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07637942538658778\n",
      "Average test loss: 0.00948900386194388\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0657564480304718\n",
      "Average test loss: 0.0073388153612613674\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05941800660888354\n",
      "Average test loss: 0.007726397788359059\n",
      "Epoch 6/300\n",
      "Average training loss: 0.054603486441903645\n",
      "Average test loss: 0.006229968454274867\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05130823718839222\n",
      "Average test loss: 0.005904116783291101\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04888603101174037\n",
      "Average test loss: 0.006307586148381233\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0460272144443459\n",
      "Average test loss: 0.10524438040620751\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0439986017578178\n",
      "Average test loss: 0.005871450481315454\n",
      "Epoch 11/300\n",
      "Average training loss: 0.041798476103279324\n",
      "Average test loss: 0.0061538166130582495\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04069019419285986\n",
      "Average test loss: 0.005485155678043763\n",
      "Epoch 13/300\n",
      "Average training loss: 0.039100765539540186\n",
      "Average test loss: 0.007750893556409412\n",
      "Epoch 14/300\n",
      "Average training loss: 0.037968375066916145\n",
      "Average test loss: 0.005197395537048578\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03646029193533792\n",
      "Average test loss: 0.006690646075540119\n",
      "Epoch 16/300\n",
      "Average training loss: 0.035341660499572755\n",
      "Average test loss: 0.004949848939147261\n",
      "Epoch 17/300\n",
      "Average training loss: 0.033861538069115744\n",
      "Average test loss: 0.01649944199456109\n",
      "Epoch 18/300\n",
      "Average training loss: 0.033174309563305644\n",
      "Average test loss: 0.005240745868119928\n",
      "Epoch 19/300\n",
      "Average training loss: 0.032283585213952595\n",
      "Average test loss: 0.006321238344328271\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03154265978270107\n",
      "Average test loss: 0.004904951305852996\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03100614868270026\n",
      "Average test loss: 0.00488888829615381\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03057561945915222\n",
      "Average test loss: 0.0047703452648388016\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03015891367031468\n",
      "Average test loss: 0.00468368055505885\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02990505680607425\n",
      "Average test loss: 0.0047115942997237046\n",
      "Epoch 25/300\n",
      "Average training loss: 0.029316998660564423\n",
      "Average test loss: 0.004665210261733996\n",
      "Epoch 26/300\n",
      "Average training loss: 0.028986137244436477\n",
      "Average test loss: 0.004536847863346338\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02873006783756945\n",
      "Average test loss: 0.004571868512779474\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02833998812072807\n",
      "Average test loss: 0.005166557125333282\n",
      "Epoch 29/300\n",
      "Average training loss: 0.028247693225741385\n",
      "Average test loss: 0.004514213217422366\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02793363210393323\n",
      "Average test loss: 0.004453230945600403\n",
      "Epoch 31/300\n",
      "Average training loss: 0.027609413007895153\n",
      "Average test loss: 0.004551545199420717\n",
      "Epoch 32/300\n",
      "Average training loss: 0.027556115943524574\n",
      "Average test loss: 0.0044206299280954734\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02733247465391954\n",
      "Average test loss: 0.00462930007568664\n",
      "Epoch 34/300\n",
      "Average training loss: 0.027099005146159066\n",
      "Average test loss: 0.004507318216893408\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027043231240577167\n",
      "Average test loss: 0.0044357404282523525\n",
      "Epoch 36/300\n",
      "Average training loss: 0.026796592669354545\n",
      "Average test loss: 0.0044647246793740325\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026642399821016523\n",
      "Average test loss: 0.005201507318351004\n",
      "Epoch 38/300\n",
      "Average training loss: 0.026486968921290505\n",
      "Average test loss: 0.004435287826798028\n",
      "Epoch 39/300\n",
      "Average training loss: 0.026375813694463835\n",
      "Average test loss: 0.00445216603246\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026198934035168753\n",
      "Average test loss: 0.004818316136383348\n",
      "Epoch 41/300\n",
      "Average training loss: 0.026147050933705437\n",
      "Average test loss: 0.004381705747710334\n",
      "Epoch 42/300\n",
      "Average training loss: 0.026077962458133697\n",
      "Average test loss: 0.004325862908528911\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02584901736014419\n",
      "Average test loss: 0.004448240826941199\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025805580450428856\n",
      "Average test loss: 0.004533451240302788\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02571382511489921\n",
      "Average test loss: 0.004445906335694923\n",
      "Epoch 46/300\n",
      "Average training loss: 0.025633916570080652\n",
      "Average test loss: 0.007847549908897944\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025523396016822922\n",
      "Average test loss: 0.004329747992878159\n",
      "Epoch 48/300\n",
      "Average training loss: 0.025416446689102385\n",
      "Average test loss: 0.004371203967266613\n",
      "Epoch 49/300\n",
      "Average training loss: 0.025244320635994275\n",
      "Average test loss: 0.0043853792717887296\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02520783009959592\n",
      "Average test loss: 0.004481994997503029\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025205956732233366\n",
      "Average test loss: 0.0051548427144686385\n",
      "Epoch 52/300\n",
      "Average training loss: 0.025061553705069753\n",
      "Average test loss: 0.00437479941195084\n",
      "Epoch 53/300\n",
      "Average training loss: 0.024995506036612724\n",
      "Average test loss: 0.00945932035562065\n",
      "Epoch 54/300\n",
      "Average training loss: 0.024936460832754772\n",
      "Average test loss: 0.004420250640975104\n",
      "Epoch 55/300\n",
      "Average training loss: 0.024827505167987613\n",
      "Average test loss: 0.004532730573581324\n",
      "Epoch 56/300\n",
      "Average training loss: 0.024789754985107316\n",
      "Average test loss: 0.005013671608848704\n",
      "Epoch 57/300\n",
      "Average training loss: 0.024736610455645455\n",
      "Average test loss: 0.004523665501839585\n",
      "Epoch 58/300\n",
      "Average training loss: 0.024588758894138866\n",
      "Average test loss: 0.0046037756109403236\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024676928063233693\n",
      "Average test loss: 0.004385052001310719\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0244935951034228\n",
      "Average test loss: 0.0043469378153483075\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024460949318276512\n",
      "Average test loss: 0.004327407063502405\n",
      "Epoch 62/300\n",
      "Average training loss: 0.024434730069504845\n",
      "Average test loss: 0.004394074579493867\n",
      "Epoch 63/300\n",
      "Average training loss: 0.024303654281629457\n",
      "Average test loss: 0.004410117298778561\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024250727759467232\n",
      "Average test loss: 0.004560112363762326\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024236241155200535\n",
      "Average test loss: 0.004358001802116632\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02418655002117157\n",
      "Average test loss: 0.004409509116576778\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02406831106212404\n",
      "Average test loss: 0.004387478032459815\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02405814617375533\n",
      "Average test loss: 0.0044877187758684154\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02398823779821396\n",
      "Average test loss: 0.004459609874834617\n",
      "Epoch 70/300\n",
      "Average training loss: 0.023959567033582263\n",
      "Average test loss: 0.004402585238632228\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02392666293018394\n",
      "Average test loss: 0.011241521884169844\n",
      "Epoch 72/300\n",
      "Average training loss: 0.023886277111040223\n",
      "Average test loss: 0.005654161574112045\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02380953997373581\n",
      "Average test loss: 0.00449753932779034\n",
      "Epoch 74/300\n",
      "Average training loss: 0.023799981392092176\n",
      "Average test loss: 0.00451712009559075\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023715531204309727\n",
      "Average test loss: 0.004498825892185172\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02364711305830214\n",
      "Average test loss: 0.004491708525766929\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02364169313841396\n",
      "Average test loss: 0.0050177487528158555\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023597126935919126\n",
      "Average test loss: 0.004807055914567576\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02353685754372014\n",
      "Average test loss: 0.0044139952953490946\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023489222736822236\n",
      "Average test loss: 0.004305604427638981\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023491846695542336\n",
      "Average test loss: 0.0048342648103005354\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023404695726103254\n",
      "Average test loss: 0.01353766943514347\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02342647831638654\n",
      "Average test loss: 0.004365273546427489\n",
      "Epoch 84/300\n",
      "Average training loss: 0.023361901857786707\n",
      "Average test loss: 0.004550343860768609\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02333938934819566\n",
      "Average test loss: 0.004456152091630631\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02330388984746403\n",
      "Average test loss: 0.004384343841423591\n",
      "Epoch 87/300\n",
      "Average training loss: 0.023281965921322506\n",
      "Average test loss: 0.006673281775166591\n",
      "Epoch 88/300\n",
      "Average training loss: 0.023163502605424987\n",
      "Average test loss: 0.0047236567363142965\n",
      "Epoch 89/300\n",
      "Average training loss: 0.023166358841790094\n",
      "Average test loss: 0.004417380802954236\n",
      "Epoch 90/300\n",
      "Average training loss: 0.023112512136499088\n",
      "Average test loss: 0.004506511708514558\n",
      "Epoch 91/300\n",
      "Average training loss: 0.023120693114068773\n",
      "Average test loss: 0.004578636056847042\n",
      "Epoch 92/300\n",
      "Average training loss: 0.023096753413478534\n",
      "Average test loss: 0.004471343729645014\n",
      "Epoch 93/300\n",
      "Average training loss: 0.023093479149871404\n",
      "Average test loss: 0.005317057362033261\n",
      "Epoch 94/300\n",
      "Average training loss: 0.02296999138676458\n",
      "Average test loss: 0.004512993782758713\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022954149507813982\n",
      "Average test loss: 0.010465225439932611\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022971152649985418\n",
      "Average test loss: 0.005803161251462168\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022943346069918737\n",
      "Average test loss: 0.004421793868558274\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022869884732696744\n",
      "Average test loss: 0.004362265335395932\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022878713164064618\n",
      "Average test loss: 0.004394156080981096\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02277170970539252\n",
      "Average test loss: 0.005097541124042537\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02282009606063366\n",
      "Average test loss: 0.004409981161769894\n",
      "Epoch 102/300\n",
      "Average training loss: 0.022742674469947814\n",
      "Average test loss: 0.00483254542408718\n",
      "Epoch 103/300\n",
      "Average training loss: 0.022718154640661344\n",
      "Average test loss: 0.006399551003343529\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022707371910413107\n",
      "Average test loss: 0.01404405028373003\n",
      "Epoch 105/300\n",
      "Average training loss: 0.022678324206007853\n",
      "Average test loss: 0.004476605854100651\n",
      "Epoch 106/300\n",
      "Average training loss: 0.022651395118898816\n",
      "Average test loss: 0.0539383884155088\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02267061958213647\n",
      "Average test loss: 0.006994351832816998\n",
      "Epoch 108/300\n",
      "Average training loss: 0.022629833698272706\n",
      "Average test loss: 0.004415132531689273\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02255287625392278\n",
      "Average test loss: 0.004419709066963858\n",
      "Epoch 110/300\n",
      "Average training loss: 0.022518688331047695\n",
      "Average test loss: 0.005222495580712954\n",
      "Epoch 111/300\n",
      "Average training loss: 0.022581747034357655\n",
      "Average test loss: 0.004512656416330072\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0224552652777897\n",
      "Average test loss: 0.0048679059495528535\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02246015306479401\n",
      "Average test loss: 0.004385622259850303\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02244200434618526\n",
      "Average test loss: 0.004554694519481725\n",
      "Epoch 115/300\n",
      "Average training loss: 0.022448158371779655\n",
      "Average test loss: 0.00468075704574585\n",
      "Epoch 116/300\n",
      "Average training loss: 0.022390233208735785\n",
      "Average test loss: 0.004433935725854503\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02239991262886259\n",
      "Average test loss: 0.004660512262334426\n",
      "Epoch 118/300\n",
      "Average training loss: 0.022367870234780842\n",
      "Average test loss: 0.013141570762627655\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02235724868045913\n",
      "Average test loss: 0.005129049156275061\n",
      "Epoch 120/300\n",
      "Average training loss: 0.022275649807519383\n",
      "Average test loss: 0.004459578908152051\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02228454892999596\n",
      "Average test loss: 0.034255446184012626\n",
      "Epoch 122/300\n",
      "Average training loss: 0.022245563666025796\n",
      "Average test loss: 0.0046252019065949656\n",
      "Epoch 123/300\n",
      "Average training loss: 0.022223049499922327\n",
      "Average test loss: 0.004624310471117496\n",
      "Epoch 124/300\n",
      "Average training loss: 0.022208999779489307\n",
      "Average test loss: 0.005117430783808231\n",
      "Epoch 125/300\n",
      "Average training loss: 0.022174865684575506\n",
      "Average test loss: 0.004804565380844805\n",
      "Epoch 126/300\n",
      "Average training loss: 0.022063256215718058\n",
      "Average test loss: 0.004726917758997943\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02208690828581651\n",
      "Average test loss: 0.00447552055782742\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02202260277668635\n",
      "Average test loss: 0.004565470985240406\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02207111007471879\n",
      "Average test loss: 0.004654739389402999\n",
      "Epoch 134/300\n",
      "Average training loss: 0.022015524230069583\n",
      "Average test loss: 0.004541149924612708\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021989666811294026\n",
      "Average test loss: 0.004635080679009358\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02203606335653199\n",
      "Average test loss: 0.004488374882274204\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021962811271349587\n",
      "Average test loss: 0.00452664653542969\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02198087319235007\n",
      "Average test loss: 0.004511052928451035\n",
      "Epoch 139/300\n",
      "Average training loss: 0.021918920649422538\n",
      "Average test loss: 0.00468221035744581\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021860014018085266\n",
      "Average test loss: 0.004633381833218866\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02189197577453322\n",
      "Average test loss: 0.004642205499940448\n",
      "Epoch 142/300\n",
      "Average training loss: 0.021889292279879253\n",
      "Average test loss: 0.004451506894909673\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02184225341512097\n",
      "Average test loss: 0.00459010819118056\n",
      "Epoch 144/300\n",
      "Average training loss: 0.021770643171336916\n",
      "Average test loss: 0.00450283254062136\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021798632514145638\n",
      "Average test loss: 0.004743803414619632\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02178952989478906\n",
      "Average test loss: 0.004643466247038709\n",
      "Epoch 147/300\n",
      "Average training loss: 0.021802387429608237\n",
      "Average test loss: 0.004519622030978401\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02181230374508434\n",
      "Average test loss: 0.004530078901598851\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02175486370507214\n",
      "Average test loss: 0.00463615394052532\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021773346276746857\n",
      "Average test loss: 0.004467069687114821\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02172946065167586\n",
      "Average test loss: 0.004843132206135326\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021696665753920874\n",
      "Average test loss: 0.005066833542245958\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02160075739522775\n",
      "Average test loss: 0.004494696404577957\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02162460298670663\n",
      "Average test loss: 0.006564357064664364\n",
      "Epoch 159/300\n",
      "Average training loss: 0.021573413527674144\n",
      "Average test loss: 0.004732795582049423\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021683931739793882\n",
      "Average test loss: 0.004644179782105817\n",
      "Epoch 161/300\n",
      "Average training loss: 0.021543303148614034\n",
      "Average test loss: 0.004582638296816084\n",
      "Epoch 162/300\n",
      "Average training loss: 0.021568369557460148\n",
      "Average test loss: 0.004680234482718839\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021522988370723195\n",
      "Average test loss: 0.004569144435640838\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021551520723435615\n",
      "Average test loss: 0.004670210050625933\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02151886092623075\n",
      "Average test loss: 0.004826459176010556\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0214537290930748\n",
      "Average test loss: 0.004490275454603963\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021476050641801622\n",
      "Average test loss: 0.005306972293390168\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021503627156217894\n",
      "Average test loss: 0.00567059794606434\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021432808553179104\n",
      "Average test loss: 0.004888554402730531\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021422371551394464\n",
      "Average test loss: 0.004520288991224435\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021438772491282888\n",
      "Average test loss: 0.004607637922796938\n",
      "Epoch 172/300\n",
      "Average training loss: 0.021419665103157362\n",
      "Average test loss: 0.004815028814805879\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021375962382389438\n",
      "Average test loss: 0.005042330443859101\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021396938973002964\n",
      "Average test loss: 0.004527588759445482\n",
      "Epoch 175/300\n",
      "Average training loss: 0.021361510394348037\n",
      "Average test loss: 0.004531610702681873\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021330482926633622\n",
      "Average test loss: 0.004647655353984899\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021276330318715838\n",
      "Average test loss: 0.00460449819597933\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021324258539411757\n",
      "Average test loss: 0.004554331203301748\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021409052352110543\n",
      "Average test loss: 0.005083532047768434\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02126628457672066\n",
      "Average test loss: 0.005116721994967924\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02130359044174353\n",
      "Average test loss: 0.004620403484751781\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021243386747936407\n",
      "Average test loss: 0.0048449196819629935\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02130673086312082\n",
      "Average test loss: 0.00495417709234688\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02118957277801302\n",
      "Average test loss: 0.005046877915246619\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02128628676136335\n",
      "Average test loss: 0.004700212464771337\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02128464950952265\n",
      "Average test loss: 0.004689896324028571\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02123028874397278\n",
      "Average test loss: 0.00468702062095205\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02119063656528791\n",
      "Average test loss: 0.004688635814934969\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021196486173404586\n",
      "Average test loss: 0.004562556221667263\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021211401960088146\n",
      "Average test loss: 0.008060281454688973\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021217498320672247\n",
      "Average test loss: 0.004694143699482083\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021129861722389856\n",
      "Average test loss: 0.00470057686459687\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0211307242396805\n",
      "Average test loss: 0.004759811942776044\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021145894689692392\n",
      "Average test loss: 0.004793767733292448\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02112105778190825\n",
      "Average test loss: 0.0047224548148612185\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02112009678284327\n",
      "Average test loss: 0.004630057520336575\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02116066295074092\n",
      "Average test loss: 0.004718803216599756\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02108139061762227\n",
      "Average test loss: 0.004733124371204112\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021138120161162482\n",
      "Average test loss: 0.004833925772044394\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02108721855117215\n",
      "Average test loss: 0.004684816891534461\n",
      "Epoch 204/300\n",
      "Average training loss: 0.021030006176895565\n",
      "Average test loss: 0.004819365568459034\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021077001046803264\n",
      "Average test loss: 0.004708817730347315\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02101292251381609\n",
      "Average test loss: 0.004950617778011494\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02104558244347572\n",
      "Average test loss: 0.004807537064370182\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02100285431245963\n",
      "Average test loss: 0.00503479448830088\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0209264544347922\n",
      "Average test loss: 0.005280902131564087\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020980815869238643\n",
      "Average test loss: 0.00495633019755284\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02091158430112733\n",
      "Average test loss: 0.0047396602092517745\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020968938302662637\n",
      "Average test loss: 0.005100075117001931\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020957283463743\n",
      "Average test loss: 0.004922550772213274\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020938897515336674\n",
      "Average test loss: 0.0047040230097870036\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020925543963909148\n",
      "Average test loss: 0.004677481649650468\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020910442501306532\n",
      "Average test loss: 0.004827026632303993\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020931045754088294\n",
      "Average test loss: 0.0053189629308051534\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020916254421075184\n",
      "Average test loss: 0.004828514404387938\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020897068603171243\n",
      "Average test loss: 0.004705729026140438\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02089827361702919\n",
      "Average test loss: 0.004639807186192936\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02091275414162212\n",
      "Average test loss: 0.004759120564907789\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020853969593842826\n",
      "Average test loss: 0.0047794721302472885\n",
      "Epoch 223/300\n",
      "Average training loss: 0.020889237398902576\n",
      "Average test loss: 0.004663795087900427\n",
      "Epoch 224/300\n",
      "Average training loss: 0.020821922875112957\n",
      "Average test loss: 0.005085000546028217\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02083519204126464\n",
      "Average test loss: 0.004680045509193507\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020802263173792097\n",
      "Average test loss: 0.00516177469988664\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020855388780434927\n",
      "Average test loss: 0.004583162609073851\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020820414940516153\n",
      "Average test loss: 0.004723913070849246\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020819377650817237\n",
      "Average test loss: 0.004724087226308054\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020781168018778166\n",
      "Average test loss: 0.004660871658888128\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020797498981157937\n",
      "Average test loss: 0.0049015205539762975\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020826161071658134\n",
      "Average test loss: 0.004682503473841482\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020756653333703676\n",
      "Average test loss: 0.004780420853859849\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020758479366699854\n",
      "Average test loss: 0.004651788579093086\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020758373071750005\n",
      "Average test loss: 0.005243467553208272\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020756532337930466\n",
      "Average test loss: 0.004816391394370132\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02072885097563267\n",
      "Average test loss: 0.004696498176289929\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02072645302779145\n",
      "Average test loss: 0.004752625359843174\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020731945529580117\n",
      "Average test loss: 0.004725388881646924\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02071396135290464\n",
      "Average test loss: 0.004761077555517356\n",
      "Epoch 241/300\n",
      "Average training loss: 0.020723047444389926\n",
      "Average test loss: 0.004775375086814165\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02073280499709977\n",
      "Average test loss: 0.004853556492676337\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020707507045732605\n",
      "Average test loss: 0.004670136844118436\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0206751939041747\n",
      "Average test loss: 0.004872132478074895\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020714980862206883\n",
      "Average test loss: 0.004674982533686691\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020691162819663683\n",
      "Average test loss: 0.004612995364392797\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020657029362188446\n",
      "Average test loss: 0.004698324981249041\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02072578586306837\n",
      "Average test loss: 0.004640833049598668\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020648542414108913\n",
      "Average test loss: 0.004845483986453877\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02065131706992785\n",
      "Average test loss: 0.0048573059104382995\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02063970825738377\n",
      "Average test loss: 0.004761876763568984\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02060062737762928\n",
      "Average test loss: 0.004763963139305512\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020629583188229137\n",
      "Average test loss: 0.004693570009950135\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020632217886547247\n",
      "Average test loss: 0.00491518933326006\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020591700702905655\n",
      "Average test loss: 0.004811480950564146\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020588912629418903\n",
      "Average test loss: 0.004762338305099143\n",
      "Epoch 260/300\n",
      "Average training loss: 0.020566401374836764\n",
      "Average test loss: 0.004813003152401911\n",
      "Epoch 261/300\n",
      "Average training loss: 0.020533274839321772\n",
      "Average test loss: 0.004736385953716106\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020597184260686238\n",
      "Average test loss: 0.00485901660223802\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020550534759958584\n",
      "Average test loss: 0.0047491863908039195\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020559701922867032\n",
      "Average test loss: 0.00475081673471464\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02055586824980047\n",
      "Average test loss: 0.004935069838745727\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020545560378167363\n",
      "Average test loss: 0.004801910776024063\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020520994794037607\n",
      "Average test loss: 0.0048635402164525455\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0205207438584831\n",
      "Average test loss: 0.004649347099992964\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02052360485659705\n",
      "Average test loss: 0.004908927092535628\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020505046788189145\n",
      "Average test loss: 0.010699310662017928\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020510124675101705\n",
      "Average test loss: 0.004729975001265606\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020504614098204508\n",
      "Average test loss: 0.004783921228514777\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020502961304452685\n",
      "Average test loss: 0.004580548808806473\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02048555436068111\n",
      "Average test loss: 0.0047542865826851795\n",
      "Epoch 275/300\n",
      "Average training loss: 0.020429961404866644\n",
      "Average test loss: 0.004886409117529789\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020509226573838128\n",
      "Average test loss: 0.004912973834822575\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0204261282020145\n",
      "Average test loss: 0.004816564898110098\n",
      "Epoch 278/300\n",
      "Average training loss: 0.020503004264500406\n",
      "Average test loss: 0.005005295889245139\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02045514613721106\n",
      "Average test loss: 0.004788440514769819\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0204273810353544\n",
      "Average test loss: 0.004900649987161159\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020484837712513075\n",
      "Average test loss: 0.004729302629828453\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020406402981943555\n",
      "Average test loss: 0.005080152952629659\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02045819287829929\n",
      "Average test loss: 0.004864231292158365\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020397854026820924\n",
      "Average test loss: 0.004835969727486372\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02047193416953087\n",
      "Average test loss: 0.004897417848722802\n",
      "Epoch 286/300\n",
      "Average training loss: 0.020373773045010035\n",
      "Average test loss: 0.004670328855514526\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020383072353071637\n",
      "Average test loss: 0.0047763997055590155\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020407173299127154\n",
      "Average test loss: 0.004716096315532923\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02037078934080071\n",
      "Average test loss: 0.004774255615555578\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020406963143083785\n",
      "Average test loss: 0.0047367777211798565\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020404839838544526\n",
      "Average test loss: 0.004773544158786535\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020387775950961643\n",
      "Average test loss: 0.004834809432841009\n",
      "Epoch 293/300\n",
      "Average training loss: 0.020356036937899058\n",
      "Average test loss: 0.004861887103981442\n",
      "Epoch 294/300\n",
      "Average training loss: 0.020361160033278994\n",
      "Average test loss: 0.004717257783230808\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020358631938695908\n",
      "Average test loss: 0.004766045168456104\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02038837203052309\n",
      "Average test loss: 0.004862309752653043\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02037370114028454\n",
      "Average test loss: 0.004687996138715082\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02033509760432773\n",
      "Average test loss: 0.004795503367152479\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020341038917501768\n",
      "Average test loss: 0.004813587367948559\n",
      "Epoch 300/300\n",
      "Average training loss: 0.020357945659094386\n",
      "Average test loss: 0.004905825853761699\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2617402035196622\n",
      "Average test loss: 0.008416928720143105\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09313886496093537\n",
      "Average test loss: 0.006347013621694512\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07029099558459388\n",
      "Average test loss: 0.007139773845672608\n",
      "Epoch 4/300\n",
      "Average training loss: 0.058908198091718884\n",
      "Average test loss: 0.005369198740770419\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05183122568991449\n",
      "Average test loss: 0.005263771422621277\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04736382965246836\n",
      "Average test loss: 0.010854451186127132\n",
      "Epoch 7/300\n",
      "Average training loss: 0.043445341176456875\n",
      "Average test loss: 0.004604064785564939\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04104368546936247\n",
      "Average test loss: 0.0054619119829601714\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03879660023252169\n",
      "Average test loss: 0.004749826301303175\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03691454501284493\n",
      "Average test loss: 0.0052386419342623815\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03470172655582428\n",
      "Average test loss: 0.004106691798609164\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03352142547236549\n",
      "Average test loss: 0.007421804305579927\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03193424310949114\n",
      "Average test loss: 0.004051462128965391\n",
      "Epoch 14/300\n",
      "Average training loss: 0.030852047314246494\n",
      "Average test loss: 0.0039302288053764236\n",
      "Epoch 15/300\n",
      "Average training loss: 0.029522664291991126\n",
      "Average test loss: 0.003988286100327969\n",
      "Epoch 16/300\n",
      "Average training loss: 0.028637739391790495\n",
      "Average test loss: 0.0037692342593024174\n",
      "Epoch 17/300\n",
      "Average training loss: 0.027715980551309055\n",
      "Average test loss: 0.0037515550210244124\n",
      "Epoch 18/300\n",
      "Average training loss: 0.026795113268825742\n",
      "Average test loss: 0.0035431478689942095\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02627798926168018\n",
      "Average test loss: 0.003556174613121483\n",
      "Epoch 20/300\n",
      "Average training loss: 0.025500626888540056\n",
      "Average test loss: 0.0038157375185853904\n",
      "Epoch 21/300\n",
      "Average training loss: 0.025139827579259873\n",
      "Average test loss: 0.0035941151043193207\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0247308311338226\n",
      "Average test loss: 0.0037246025070134135\n",
      "Epoch 23/300\n",
      "Average training loss: 0.024311830383208062\n",
      "Average test loss: 0.006700239903810952\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02416150220566326\n",
      "Average test loss: 0.0035705559125377073\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0237386269354158\n",
      "Average test loss: 0.0033726035497254796\n",
      "Epoch 26/300\n",
      "Average training loss: 0.023455478267537223\n",
      "Average test loss: 0.003500478158187535\n",
      "Epoch 27/300\n",
      "Average training loss: 0.023199642976125083\n",
      "Average test loss: 0.0033619378325011997\n",
      "Epoch 28/300\n",
      "Average training loss: 0.022953450702958636\n",
      "Average test loss: 0.003386633694792787\n",
      "Epoch 29/300\n",
      "Average training loss: 0.022783342308468287\n",
      "Average test loss: 0.0032906904800070655\n",
      "Epoch 30/300\n",
      "Average training loss: 0.022554843640989728\n",
      "Average test loss: 0.003306903303704328\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02240874108672142\n",
      "Average test loss: 0.003388406751677394\n",
      "Epoch 32/300\n",
      "Average training loss: 0.022314549171262316\n",
      "Average test loss: 0.003597776053680314\n",
      "Epoch 33/300\n",
      "Average training loss: 0.022017857414152887\n",
      "Average test loss: 0.0035528548947638934\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021926515163646805\n",
      "Average test loss: 0.0033840564247220755\n",
      "Epoch 35/300\n",
      "Average training loss: 0.021776810901032555\n",
      "Average test loss: 0.003279404166465004\n",
      "Epoch 36/300\n",
      "Average training loss: 0.021689640757110384\n",
      "Average test loss: 0.003672991497235166\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021630373638537197\n",
      "Average test loss: 0.0032590801080481873\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02147574662996663\n",
      "Average test loss: 0.003313894276610679\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021341933763689466\n",
      "Average test loss: 0.0032678460576054122\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02131510855919785\n",
      "Average test loss: 0.0035263261441141366\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02117911214960946\n",
      "Average test loss: 0.0033968237443930574\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02106653325094117\n",
      "Average test loss: 0.0036280136298802164\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020971420332789422\n",
      "Average test loss: 0.003760171749111679\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020930188794102935\n",
      "Average test loss: 0.003219685092982319\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020829228404495453\n",
      "Average test loss: 0.003262142356692089\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020801339846518305\n",
      "Average test loss: 0.0032915710964136653\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02066017483671506\n",
      "Average test loss: 0.0032356862102945645\n",
      "Epoch 48/300\n",
      "Average training loss: 0.020610945776104925\n",
      "Average test loss: 0.00388048070958919\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020572203839818635\n",
      "Average test loss: 0.003186745865891377\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020498511633939214\n",
      "Average test loss: 0.0032469851600213182\n",
      "Epoch 51/300\n",
      "Average training loss: 0.020426801150043805\n",
      "Average test loss: 0.003254871371305651\n",
      "Epoch 52/300\n",
      "Average training loss: 0.020368027341034677\n",
      "Average test loss: 0.0041664427125619515\n",
      "Epoch 53/300\n",
      "Average training loss: 0.020326000908182727\n",
      "Average test loss: 0.003178897853112883\n",
      "Epoch 54/300\n",
      "Average training loss: 0.020203078483541807\n",
      "Average test loss: 0.0032010900270607735\n",
      "Epoch 55/300\n",
      "Average training loss: 0.020196368492311902\n",
      "Average test loss: 0.003371069722705417\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02014089920454555\n",
      "Average test loss: 0.003200270967351066\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02006186523867978\n",
      "Average test loss: 0.003955859846538967\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020052328747179774\n",
      "Average test loss: 0.003392119654971692\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019929756292038494\n",
      "Average test loss: 0.0031677634471820463\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01994198561956485\n",
      "Average test loss: 0.003227430934086442\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019920063936875926\n",
      "Average test loss: 0.004095669163184034\n",
      "Epoch 62/300\n",
      "Average training loss: 0.019875386595726012\n",
      "Average test loss: 0.003221181674963898\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01977085127764278\n",
      "Average test loss: 0.003184412399099933\n",
      "Epoch 64/300\n",
      "Average training loss: 0.019718913162748018\n",
      "Average test loss: 0.0033338675639695593\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019684564959671764\n",
      "Average test loss: 0.003179539274838236\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019686361060374313\n",
      "Average test loss: 0.0033427920364257364\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01962645838326878\n",
      "Average test loss: 0.0032822562969393202\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019554924916889933\n",
      "Average test loss: 0.0038101813319242664\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019571536304222214\n",
      "Average test loss: 0.003198204883063833\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01945306622816457\n",
      "Average test loss: 0.0035211785410841304\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019442495330340332\n",
      "Average test loss: 0.0031502167580442298\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019401548153824277\n",
      "Average test loss: 0.003216302392590377\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019367367256846693\n",
      "Average test loss: 0.0032079217239386506\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019349593336383503\n",
      "Average test loss: 0.00830334686115384\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019358276526961062\n",
      "Average test loss: 0.0032392337791000802\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019228307296832402\n",
      "Average test loss: 0.003451308804253737\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01924646688832177\n",
      "Average test loss: 0.003186507340106699\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019228415691190295\n",
      "Average test loss: 0.003222309733637505\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01916195756693681\n",
      "Average test loss: 0.00320897309254441\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019127221249871783\n",
      "Average test loss: 0.003213677126707302\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019099895619683796\n",
      "Average test loss: 0.003372453439152903\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01907226670285066\n",
      "Average test loss: 0.0033733158755219644\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019002620043026077\n",
      "Average test loss: 0.0032438996562527285\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018958637128273646\n",
      "Average test loss: 0.0031970044010215335\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018942170896463925\n",
      "Average test loss: 0.0033527632123894163\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018932344517774053\n",
      "Average test loss: 0.0032186544690695072\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018922725958956613\n",
      "Average test loss: 0.003180828447557158\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018903632392485936\n",
      "Average test loss: 0.003222670242604282\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018864047800501188\n",
      "Average test loss: 0.003171003340433041\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018803134731948376\n",
      "Average test loss: 0.0031836049418068595\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018748131288422478\n",
      "Average test loss: 0.0032955970116373564\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01875957215163443\n",
      "Average test loss: 0.003343689527776506\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018750808866487608\n",
      "Average test loss: 0.003286667453746001\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018718448479142454\n",
      "Average test loss: 0.003854338187724352\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018628765751918158\n",
      "Average test loss: 0.0032333078698979483\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01867082857588927\n",
      "Average test loss: 0.003256443720103966\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018614653252893024\n",
      "Average test loss: 0.003233988262299034\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018628813828859066\n",
      "Average test loss: 0.003870432139063875\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01861343883143531\n",
      "Average test loss: 0.0032542037481649052\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0185202474080854\n",
      "Average test loss: 0.0032801237408485676\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018527097263269954\n",
      "Average test loss: 0.003228210672322247\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018487716305587026\n",
      "Average test loss: 0.00326540634346505\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018502584133711125\n",
      "Average test loss: 0.0032556310976958936\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0184334463874499\n",
      "Average test loss: 0.0031931682779557176\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01840613996485869\n",
      "Average test loss: 0.003482596597944697\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01845775567822986\n",
      "Average test loss: 0.003438768629812532\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018432002956668535\n",
      "Average test loss: 0.0032283436137561998\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01838124349547757\n",
      "Average test loss: 0.0034447366893291475\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018351315455304253\n",
      "Average test loss: 0.003220799214930998\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01828478623264366\n",
      "Average test loss: 0.0032376459536867007\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018286027843753497\n",
      "Average test loss: 0.0041748249990244705\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018293538850214747\n",
      "Average test loss: 0.003225052235648036\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018243998542428017\n",
      "Average test loss: 0.0035592537770668667\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018263250266512234\n",
      "Average test loss: 0.0033545062388810848\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01821761913763152\n",
      "Average test loss: 0.00330134345487588\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018188136277927293\n",
      "Average test loss: 0.0033450921239952247\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018186369453039433\n",
      "Average test loss: 0.003222376422749625\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01812972774108251\n",
      "Average test loss: 0.003250144636051522\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018134150076243614\n",
      "Average test loss: 0.003316016800287697\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01808752891421318\n",
      "Average test loss: 0.0033251553459299936\n",
      "Epoch 121/300\n",
      "Average training loss: 0.018095239881012175\n",
      "Average test loss: 0.0032403226589990987\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018057694415251412\n",
      "Average test loss: 0.003364807357183761\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01806622156335248\n",
      "Average test loss: 0.0034562506791618137\n",
      "Epoch 124/300\n",
      "Average training loss: 0.018064815474881066\n",
      "Average test loss: 0.00335683288735648\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01798741939995024\n",
      "Average test loss: 0.0032932374330444467\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018003889954752394\n",
      "Average test loss: 0.0033532117541051573\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017967611557907527\n",
      "Average test loss: 0.0032762728610800373\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017975104976031515\n",
      "Average test loss: 0.0032498252271778052\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017954157748156122\n",
      "Average test loss: 0.0035101692134307493\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017961390145950846\n",
      "Average test loss: 0.003489611689415243\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017892147176795537\n",
      "Average test loss: 0.003283440554100606\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01787648297266828\n",
      "Average test loss: 0.0033265238573981656\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01786886593947808\n",
      "Average test loss: 0.0032999788903527788\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017838764975468317\n",
      "Average test loss: 0.004340009552737077\n",
      "Epoch 135/300\n",
      "Average training loss: 0.017893133198221525\n",
      "Average test loss: 0.0035859682245386972\n",
      "Epoch 136/300\n",
      "Average training loss: 0.017831806457704966\n",
      "Average test loss: 0.0033091937924424808\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01780787950588597\n",
      "Average test loss: 0.003290456920862198\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017814375086790984\n",
      "Average test loss: 0.003372867288688819\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01778701433373822\n",
      "Average test loss: 0.003358024172898796\n",
      "Epoch 140/300\n",
      "Average training loss: 0.017760061649812593\n",
      "Average test loss: 0.0036055078206376897\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01778027426203092\n",
      "Average test loss: 0.003402840970291032\n",
      "Epoch 142/300\n",
      "Average training loss: 0.017743094922767745\n",
      "Average test loss: 0.0033465881043424207\n",
      "Epoch 143/300\n",
      "Average training loss: 0.017731664069824748\n",
      "Average test loss: 0.0032682518996298314\n",
      "Epoch 144/300\n",
      "Average training loss: 0.017728153589699005\n",
      "Average test loss: 0.0033210879880934954\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017694443356659678\n",
      "Average test loss: 0.003356460093624062\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017698543394605318\n",
      "Average test loss: 0.0033407475809670156\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01766947563406494\n",
      "Average test loss: 0.003500913792393274\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017650989825526875\n",
      "Average test loss: 0.0038506873221033147\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017676440573400922\n",
      "Average test loss: 0.0052046438004407615\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01760975365506278\n",
      "Average test loss: 0.0033437137955592737\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017618310105469492\n",
      "Average test loss: 0.003518145754105515\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017584627648194632\n",
      "Average test loss: 0.0033182371449139384\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01758495937453376\n",
      "Average test loss: 0.00333378377618889\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017581180884606307\n",
      "Average test loss: 0.0038879647004521557\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017576789672176043\n",
      "Average test loss: 0.003305304992530081\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017551851493616898\n",
      "Average test loss: 0.003409414691022701\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017475187386075657\n",
      "Average test loss: 0.003411157239642408\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017508972134027218\n",
      "Average test loss: 0.003349294046560923\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017495844420459535\n",
      "Average test loss: 0.003338493238720629\n",
      "Epoch 160/300\n",
      "Average training loss: 0.017507565970222155\n",
      "Average test loss: 0.0036021553977496094\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017489956748154428\n",
      "Average test loss: 0.003487639819789264\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017488802689645026\n",
      "Average test loss: 0.0033282908582025106\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017468987717396682\n",
      "Average test loss: 0.003412258744860689\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017454684658182992\n",
      "Average test loss: 0.0043117717305819194\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017417910137110287\n",
      "Average test loss: 0.0033710748296644955\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017418910811344782\n",
      "Average test loss: 0.0033914427906274794\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017444768571191364\n",
      "Average test loss: 0.003402910106504957\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01741776574485832\n",
      "Average test loss: 0.0034350034720781776\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017439005065295433\n",
      "Average test loss: 0.0033983892293439973\n",
      "Epoch 170/300\n",
      "Average training loss: 0.017372670514716044\n",
      "Average test loss: 0.0033661260844932662\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01740282325943311\n",
      "Average test loss: 0.003394742200151086\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017348217969139417\n",
      "Average test loss: 0.0040509815547201365\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017318628655539618\n",
      "Average test loss: 0.003348452265477843\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017348429980377358\n",
      "Average test loss: 0.003413041728031304\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017320509627461435\n",
      "Average test loss: 0.0034792145929402776\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017328217215008206\n",
      "Average test loss: 0.0033609728920790884\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017273402493860987\n",
      "Average test loss: 0.0034227941151087483\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01730876855634981\n",
      "Average test loss: 0.003533761352300644\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017297086081571048\n",
      "Average test loss: 0.0034708527541822856\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01725738237798214\n",
      "Average test loss: 0.0034483973077601856\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01726856887837251\n",
      "Average test loss: 0.003488662664261129\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01730312918706073\n",
      "Average test loss: 0.00340208292255799\n",
      "Epoch 183/300\n",
      "Average training loss: 0.017210367688702214\n",
      "Average test loss: 0.0035854147480179864\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017234433402617773\n",
      "Average test loss: 0.0034602721598413254\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017215432680315442\n",
      "Average test loss: 0.003469025043046309\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017230136063363818\n",
      "Average test loss: 0.003567088761677345\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01722569595194525\n",
      "Average test loss: 0.0034944015155649846\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017187077179551126\n",
      "Average test loss: 0.0034153057943201726\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01719303518368138\n",
      "Average test loss: 0.004641839410281843\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017175278746419484\n",
      "Average test loss: 0.0034341958090662954\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017166772754655946\n",
      "Average test loss: 0.003438008591532707\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017185586947533818\n",
      "Average test loss: 0.003428567039055957\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017153918190134895\n",
      "Average test loss: 0.0034451473980314203\n",
      "Epoch 194/300\n",
      "Average training loss: 0.017168314650654794\n",
      "Average test loss: 0.0034563256460759376\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017174215063452722\n",
      "Average test loss: 0.003352689749043849\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01715469841327932\n",
      "Average test loss: 0.003383489806825916\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017133562218811776\n",
      "Average test loss: 0.0034893944710493088\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01710252419941955\n",
      "Average test loss: 0.0034145548629264037\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01708608307109939\n",
      "Average test loss: 0.003403815929260519\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017071581568982867\n",
      "Average test loss: 0.003422956619825628\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01709787588318189\n",
      "Average test loss: 0.003471247931321462\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017080496615833708\n",
      "Average test loss: 0.003377026319089863\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017066723573538993\n",
      "Average test loss: 0.0034217768179045783\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01708237666057216\n",
      "Average test loss: 0.0034776451146850984\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017074857828517753\n",
      "Average test loss: 0.0035623191733741098\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01707471553153462\n",
      "Average test loss: 0.0033347499689294234\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017054937111006842\n",
      "Average test loss: 0.004049325222770373\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017022560930914348\n",
      "Average test loss: 0.0035444049131539133\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01707085606869724\n",
      "Average test loss: 0.003441192355006933\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016974649338258638\n",
      "Average test loss: 0.0033709114065600767\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017033086692293486\n",
      "Average test loss: 0.0033859291105634635\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01700924493703577\n",
      "Average test loss: 0.0034132287179430326\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01701034123533302\n",
      "Average test loss: 0.003371590397838089\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017006727209521663\n",
      "Average test loss: 0.0036496104589766928\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017013110323084724\n",
      "Average test loss: 0.003612067725095484\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016956348947352835\n",
      "Average test loss: 0.003427844955896338\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016955875378515985\n",
      "Average test loss: 0.003470727396508058\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01696901621421178\n",
      "Average test loss: 0.0034456010572612287\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016959819530447322\n",
      "Average test loss: 0.003722335531272822\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016961150349014336\n",
      "Average test loss: 0.0035825782836311393\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016917818647291926\n",
      "Average test loss: 0.0035393872037529947\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016915278217030896\n",
      "Average test loss: 0.0035917963836756017\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016884799063205717\n",
      "Average test loss: 0.0034766595516767767\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01694026292529371\n",
      "Average test loss: 0.0035637584125830066\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01689183469530609\n",
      "Average test loss: 0.0035321871135383844\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016881491078270805\n",
      "Average test loss: 0.003419937264588144\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01698728682763047\n",
      "Average test loss: 0.003543200780120161\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016879463207390574\n",
      "Average test loss: 0.0035409586044649285\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01686138315995534\n",
      "Average test loss: 0.0035889153029355736\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016908564024501378\n",
      "Average test loss: 0.0036546258077853255\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01686398616929849\n",
      "Average test loss: 0.003462959962172641\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016902051538228987\n",
      "Average test loss: 0.0034023292880091404\n",
      "Epoch 233/300\n",
      "Average training loss: 0.016874851822853087\n",
      "Average test loss: 0.0035283847778207725\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016808972699774636\n",
      "Average test loss: 0.0036054422522170675\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016884740511576336\n",
      "Average test loss: 0.0033851222780843576\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016836398139595986\n",
      "Average test loss: 0.0034408299322757456\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016801746158964105\n",
      "Average test loss: 0.0035239176164484688\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016879414250453313\n",
      "Average test loss: 0.003630726703339153\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016794009445442095\n",
      "Average test loss: 0.0034766096812155513\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016820109941893154\n",
      "Average test loss: 0.0035688872879578006\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016792756888601514\n",
      "Average test loss: 0.0035549785958396063\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016783076191941897\n",
      "Average test loss: 0.0035211211467782656\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016795383708344564\n",
      "Average test loss: 0.0034660680720375644\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016768103033304213\n",
      "Average test loss: 0.003488254893364178\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016771339744329452\n",
      "Average test loss: 0.003923130722923412\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016776614814168877\n",
      "Average test loss: 0.003555315719296535\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01677525120973587\n",
      "Average test loss: 0.0034683910689006247\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01679921081662178\n",
      "Average test loss: 0.0034602313660499124\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016744787658254306\n",
      "Average test loss: 0.0034044949315074416\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016760562262601322\n",
      "Average test loss: 0.00497371045831177\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016750733660327064\n",
      "Average test loss: 0.003494737209338281\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016737973660230637\n",
      "Average test loss: 0.005051955748349428\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01671415922211276\n",
      "Average test loss: 0.003483494085363216\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01673669851819674\n",
      "Average test loss: 0.003513084965447585\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016738208128346338\n",
      "Average test loss: 0.003699682742771175\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01672406378057268\n",
      "Average test loss: 0.003502121774272786\n",
      "Epoch 257/300\n",
      "Average training loss: 0.016704035033782323\n",
      "Average test loss: 0.00358299798063106\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016702638167474006\n",
      "Average test loss: 0.0036194189571672015\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01669515553779072\n",
      "Average test loss: 0.003510988516940011\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016715089001589353\n",
      "Average test loss: 0.0036120326856358183\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01670160165263547\n",
      "Average test loss: 0.0035047494744261105\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01668895930879646\n",
      "Average test loss: 0.0035151533099512257\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016699610132310125\n",
      "Average test loss: 0.003561310819039742\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016666903831892545\n",
      "Average test loss: 0.003415237643652492\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01665682377749019\n",
      "Average test loss: 0.003449242317635152\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01666319536831644\n",
      "Average test loss: 0.0036745609893567032\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01664258120705684\n",
      "Average test loss: 0.003507768116477463\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016647893354296683\n",
      "Average test loss: 0.00357800275604758\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01663174888657199\n",
      "Average test loss: 0.0034428421047826606\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016682881130112542\n",
      "Average test loss: 0.0034443426442643007\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016661573691500557\n",
      "Average test loss: 0.0034431009531642\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01662275871137778\n",
      "Average test loss: 0.003525776290231281\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016636354009310406\n",
      "Average test loss: 0.003543145015835762\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01661262873891327\n",
      "Average test loss: 0.0035459960595601134\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016643378403451706\n",
      "Average test loss: 0.0034523435996638404\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01657928817636437\n",
      "Average test loss: 0.003520591772471865\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016589148153861363\n",
      "Average test loss: 0.003460879581256045\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016666447755363252\n",
      "Average test loss: 0.003438206626309289\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016589641408787832\n",
      "Average test loss: 0.0036343028381880787\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016577466135223706\n",
      "Average test loss: 0.0035327312987711696\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016598018238941827\n",
      "Average test loss: 0.003528815892421537\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016596712195210986\n",
      "Average test loss: 0.0035120614969895944\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016602276841799418\n",
      "Average test loss: 0.0034669298730376696\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016568765709797543\n",
      "Average test loss: 0.005072329788572258\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016578086568249596\n",
      "Average test loss: 0.003518681313842535\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01654331308603287\n",
      "Average test loss: 0.0036551964413374665\n",
      "Epoch 287/300\n",
      "Average training loss: 0.016545243064562478\n",
      "Average test loss: 0.003544059201454123\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016529132255249553\n",
      "Average test loss: 0.0035538407264070378\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016553696393966675\n",
      "Average test loss: 0.003434643819514248\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016568964776065613\n",
      "Average test loss: 0.003506756196005477\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016550288091103236\n",
      "Average test loss: 0.003552658708766103\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016537803380025758\n",
      "Average test loss: 0.00359939696184463\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016514125615358352\n",
      "Average test loss: 0.0036155706838601163\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01654845851659775\n",
      "Average test loss: 0.003546044441146983\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01656664083732499\n",
      "Average test loss: 0.0035321727105312875\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016507909398939875\n",
      "Average test loss: 0.0035401629824191334\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016554848275250857\n",
      "Average test loss: 0.0034866073220554327\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016510885794957478\n",
      "Average test loss: 0.00355040506604645\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01652919999923971\n",
      "Average test loss: 0.003821137832684649\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016502898005975618\n",
      "Average test loss: 0.003722197412409716\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2531694078180525\n",
      "Average test loss: 0.00894178264753686\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08708335354593065\n",
      "Average test loss: 0.009294690233137873\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06412745624449517\n",
      "Average test loss: 0.00454287407712804\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05316029617190361\n",
      "Average test loss: 0.004175125264666147\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04696826855010457\n",
      "Average test loss: 0.0043453386591540445\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0419643605351448\n",
      "Average test loss: 0.0049045425980455346\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03897054752045208\n",
      "Average test loss: 0.003752376839518547\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03578574299646749\n",
      "Average test loss: 0.00776996016005675\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03340282590190569\n",
      "Average test loss: 0.0034749736953526736\n",
      "Epoch 10/300\n",
      "Average training loss: 0.032107340039478405\n",
      "Average test loss: 0.0040474975957638685\n",
      "Epoch 11/300\n",
      "Average training loss: 0.030236377368370693\n",
      "Average test loss: 0.0034675305185632574\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02880207550360097\n",
      "Average test loss: 0.0033199029409637054\n",
      "Epoch 13/300\n",
      "Average training loss: 0.027640639134579235\n",
      "Average test loss: 0.013964038274354405\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02625494376818339\n",
      "Average test loss: 0.0031437675973607433\n",
      "Epoch 15/300\n",
      "Average training loss: 0.025315615359279845\n",
      "Average test loss: 0.0030506156565000614\n",
      "Epoch 16/300\n",
      "Average training loss: 0.024335854570070904\n",
      "Average test loss: 0.003400831764149997\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02381062073012193\n",
      "Average test loss: 0.004492047453920046\n",
      "Epoch 18/300\n",
      "Average training loss: 0.022951376168264283\n",
      "Average test loss: 0.0029035514667630196\n",
      "Epoch 19/300\n",
      "Average training loss: 0.022390518211656146\n",
      "Average test loss: 0.0028437955861704218\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021987229103843372\n",
      "Average test loss: 0.002922057993710041\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02149034693174892\n",
      "Average test loss: 0.0029632976218644117\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021226807478401395\n",
      "Average test loss: 0.003001936852104134\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02081680674685372\n",
      "Average test loss: 0.0031560116631703244\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020423347052600647\n",
      "Average test loss: 0.002818275618056456\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020214392297797732\n",
      "Average test loss: 0.0028073540747993522\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01998824834989177\n",
      "Average test loss: 0.0026938244285475877\n",
      "Epoch 27/300\n",
      "Average training loss: 0.019795936478508844\n",
      "Average test loss: 0.0026500839268167814\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01953741075264083\n",
      "Average test loss: 0.0027893860520174104\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01940197239816189\n",
      "Average test loss: 0.002846523205853171\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019332323027981654\n",
      "Average test loss: 0.0026599748343643214\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019022179486023055\n",
      "Average test loss: 0.0028985826263411177\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01892981557548046\n",
      "Average test loss: 0.00278326553116656\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018758918788697985\n",
      "Average test loss: 0.0029263607791314523\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018723083466291427\n",
      "Average test loss: 0.002783187342186769\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018569607521096865\n",
      "Average test loss: 0.0026000720305989185\n",
      "Epoch 36/300\n",
      "Average training loss: 0.018423844170239238\n",
      "Average test loss: 0.002568323851459556\n",
      "Epoch 37/300\n",
      "Average training loss: 0.018370270469122464\n",
      "Average test loss: 0.0026299178993536365\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01820774844288826\n",
      "Average test loss: 0.00339809610756735\n",
      "Epoch 39/300\n",
      "Average training loss: 0.018131579817997085\n",
      "Average test loss: 0.00257868032116029\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018040805795126492\n",
      "Average test loss: 0.0025888652962942917\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018014613870945242\n",
      "Average test loss: 0.004172851780015562\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01792141130897734\n",
      "Average test loss: 0.0025390027719032434\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017800802177853056\n",
      "Average test loss: 0.002588296030751533\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017766691833734513\n",
      "Average test loss: 0.0025515533542881407\n",
      "Epoch 45/300\n",
      "Average training loss: 0.017690219935443667\n",
      "Average test loss: 0.0026074504473557075\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017613137943877113\n",
      "Average test loss: 0.002558677614770002\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017544781593812838\n",
      "Average test loss: 0.0025564067843887543\n",
      "Epoch 48/300\n",
      "Average training loss: 0.017478589474327035\n",
      "Average test loss: 0.002662293278715677\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01739588138129976\n",
      "Average test loss: 0.002515890482399199\n",
      "Epoch 50/300\n",
      "Average training loss: 0.017364328490363228\n",
      "Average test loss: 0.0025500572628031175\n",
      "Epoch 51/300\n",
      "Average training loss: 0.017318370606336328\n",
      "Average test loss: 0.0028216092913515038\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01726969209478961\n",
      "Average test loss: 0.0025963536302248636\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017214049701889354\n",
      "Average test loss: 0.002846897397397293\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01714969185491403\n",
      "Average test loss: 0.0025405264627188446\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017078264468246035\n",
      "Average test loss: 0.0027191173895779585\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017016414602597556\n",
      "Average test loss: 0.0026297962634513777\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01700829210380713\n",
      "Average test loss: 0.0025140709440327354\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01694473214033577\n",
      "Average test loss: 0.0025197437782254486\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0169258387370242\n",
      "Average test loss: 0.0026278437498129074\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016860601019528177\n",
      "Average test loss: 0.0025239565360049405\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01681044332517518\n",
      "Average test loss: 0.0038322846665978434\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016779858364827104\n",
      "Average test loss: 0.0025183789543807506\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01673905989030997\n",
      "Average test loss: 0.002498600730362038\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016652017241550816\n",
      "Average test loss: 0.00255430687405169\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01662421469307608\n",
      "Average test loss: 0.002567172885975904\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016625551836358176\n",
      "Average test loss: 0.0025152110409819416\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016598937552836205\n",
      "Average test loss: 0.002532607152747611\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0165272280457947\n",
      "Average test loss: 0.002522070952380697\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016530332148075103\n",
      "Average test loss: 0.0026134590860456227\n",
      "Epoch 70/300\n",
      "Average training loss: 0.016448867317703036\n",
      "Average test loss: 0.0025365154902554222\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01643676405482822\n",
      "Average test loss: 0.0024994455781868763\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01638614853554302\n",
      "Average test loss: 0.0025156834257973565\n",
      "Epoch 73/300\n",
      "Average training loss: 0.01636520044505596\n",
      "Average test loss: 0.0025454769442892736\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016299449978603258\n",
      "Average test loss: 0.0025235561542212965\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0162752952211433\n",
      "Average test loss: 0.002593398104939196\n",
      "Epoch 76/300\n",
      "Average training loss: 0.016281638325916396\n",
      "Average test loss: 0.002771103189430303\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016222929523222976\n",
      "Average test loss: 0.002510149573493335\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01620797286265426\n",
      "Average test loss: 0.0025511052484313646\n",
      "Epoch 79/300\n",
      "Average training loss: 0.016187119343214564\n",
      "Average test loss: 0.0025236448384821416\n",
      "Epoch 80/300\n",
      "Average training loss: 0.016127951683269606\n",
      "Average test loss: 0.0026594397794041367\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01612227943374051\n",
      "Average test loss: 0.002617364427695672\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01608963174952401\n",
      "Average test loss: 0.0025512001775205135\n",
      "Epoch 83/300\n",
      "Average training loss: 0.016055251551171143\n",
      "Average test loss: 0.0025850242560522423\n",
      "Epoch 84/300\n",
      "Average training loss: 0.016082774442103175\n",
      "Average test loss: 0.0028346308287647034\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01602126233610842\n",
      "Average test loss: 0.0025228840094059708\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01595502520766523\n",
      "Average test loss: 0.0025233864807006384\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015959878952138955\n",
      "Average test loss: 0.005550573629016678\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01591318767931726\n",
      "Average test loss: 0.0046503067972759405\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015914106488227844\n",
      "Average test loss: 0.002565214808823334\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015905619542631838\n",
      "Average test loss: 0.0026264119096514253\n",
      "Epoch 91/300\n",
      "Average training loss: 0.015833886633316675\n",
      "Average test loss: 0.003943291558987564\n",
      "Epoch 92/300\n",
      "Average training loss: 0.015834565848112107\n",
      "Average test loss: 0.0025372777686764798\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015776138946413994\n",
      "Average test loss: 0.002675517158789767\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015765226637323698\n",
      "Average test loss: 0.0025779017553561262\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015740273541874357\n",
      "Average test loss: 0.0029263905878696176\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01577747785217232\n",
      "Average test loss: 0.0027257980830553506\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015712405156758095\n",
      "Average test loss: 0.002611231138722764\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015659269832902483\n",
      "Average test loss: 0.0030247281173037158\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015652109884553487\n",
      "Average test loss: 0.002580295951002174\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015646465541587936\n",
      "Average test loss: 0.002557514921658569\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015615207667152086\n",
      "Average test loss: 0.0028546681269589398\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01558925510942936\n",
      "Average test loss: 0.0027145438742720417\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015571407842967245\n",
      "Average test loss: 0.0025467379951021737\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015583054663406478\n",
      "Average test loss: 0.0025780642160938847\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015537327587604522\n",
      "Average test loss: 0.0028828211741314994\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0154875478512711\n",
      "Average test loss: 0.002653098072649704\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01550063916709688\n",
      "Average test loss: 0.0029704657623337376\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015497255388233397\n",
      "Average test loss: 0.00344827668596473\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015465630029638609\n",
      "Average test loss: 0.0025585705337838995\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015431640952825546\n",
      "Average test loss: 0.002948918912973669\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01543461427009768\n",
      "Average test loss: 0.0025823344214715893\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01540445297294193\n",
      "Average test loss: 0.0025659562229282326\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015404669811328252\n",
      "Average test loss: 0.002593706399202347\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015391518087850676\n",
      "Average test loss: 0.0027138594556599853\n",
      "Epoch 115/300\n",
      "Average training loss: 0.015372140111194716\n",
      "Average test loss: 0.003028803748389085\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015360908793078528\n",
      "Average test loss: 0.0025700481838236253\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01533093543516265\n",
      "Average test loss: 0.002853656752448943\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01530195800628927\n",
      "Average test loss: 0.0026320269536226986\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015281098142266273\n",
      "Average test loss: 0.0025829741743703684\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01526597566405932\n",
      "Average test loss: 0.0025594302254418532\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01525238768921958\n",
      "Average test loss: 0.002567981419256992\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015249303249021372\n",
      "Average test loss: 0.0026236004359606237\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015237439365022712\n",
      "Average test loss: 0.0026761788573736946\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015209638978044192\n",
      "Average test loss: 0.002567697861749265\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015204191620979045\n",
      "Average test loss: 0.0026320959917373126\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01517467142475976\n",
      "Average test loss: 0.00259698094199929\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01516883419950803\n",
      "Average test loss: 0.0026733872041934067\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01512725130882528\n",
      "Average test loss: 0.0025819030268531707\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015143927965727118\n",
      "Average test loss: 0.002694521031445927\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015125851917597983\n",
      "Average test loss: 0.0026259336148699124\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015126689184043142\n",
      "Average test loss: 0.0026488157235499887\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015093036820491155\n",
      "Average test loss: 0.002802062541246414\n",
      "Epoch 133/300\n",
      "Average training loss: 0.015076914742588998\n",
      "Average test loss: 0.002591887345744504\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015065055897666348\n",
      "Average test loss: 0.0027308235133273735\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015080459078152975\n",
      "Average test loss: 0.002615546798540486\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015030873632265462\n",
      "Average test loss: 0.002719217355466551\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015032976615760061\n",
      "Average test loss: 0.002682880467010869\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015017565087311798\n",
      "Average test loss: 0.002760582826617691\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014993101395666598\n",
      "Average test loss: 0.002649993838328454\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01499068916340669\n",
      "Average test loss: 0.0029993531511475643\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014977179185383849\n",
      "Average test loss: 0.0027701220663471355\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014985241502523422\n",
      "Average test loss: 0.0026298299222770665\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014932715516123506\n",
      "Average test loss: 0.002685103955368201\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014938037060201168\n",
      "Average test loss: 0.0026384083512756558\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014946198220054309\n",
      "Average test loss: 0.00259780067536566\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014954858619305823\n",
      "Average test loss: 0.0026623216486639447\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01488286946548356\n",
      "Average test loss: 0.002599831465010842\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01488691837257809\n",
      "Average test loss: 0.002638905045058992\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014861991920404964\n",
      "Average test loss: 0.002590953150557147\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014884458566705385\n",
      "Average test loss: 0.0027318752060333886\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014847893373833762\n",
      "Average test loss: 0.0027445626602404646\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01483704245256053\n",
      "Average test loss: 0.0026717206850234007\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014841565797726313\n",
      "Average test loss: 0.0028770838280518848\n",
      "Epoch 154/300\n",
      "Average training loss: 0.014822206298510233\n",
      "Average test loss: 0.00273502008513444\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01481432487236129\n",
      "Average test loss: 0.0026540651578042243\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014792289026909404\n",
      "Average test loss: 0.0026246062765518823\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014790617499914433\n",
      "Average test loss: 0.002660906037936608\n",
      "Epoch 158/300\n",
      "Average training loss: 0.014789252295262284\n",
      "Average test loss: 0.0026917585906469157\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014769453634818394\n",
      "Average test loss: 0.0026728925005429323\n",
      "Epoch 160/300\n",
      "Average training loss: 0.014776808684070905\n",
      "Average test loss: 0.0029571352675557138\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01478916299260325\n",
      "Average test loss: 0.0027499648661663133\n",
      "Epoch 162/300\n",
      "Average training loss: 0.014728115845057699\n",
      "Average test loss: 0.0026792470667925147\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014726409041219288\n",
      "Average test loss: 0.0027154517335196337\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014708114766412312\n",
      "Average test loss: 0.002645411436342531\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014689952111078633\n",
      "Average test loss: 0.002667269038243426\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014703491093383895\n",
      "Average test loss: 0.0030498268449058136\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014683751340541574\n",
      "Average test loss: 0.0026240699274672403\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014667750630941655\n",
      "Average test loss: 0.002788390105176303\n",
      "Epoch 169/300\n",
      "Average training loss: 0.014641232571668095\n",
      "Average test loss: 0.0026517096704079045\n",
      "Epoch 170/300\n",
      "Average training loss: 0.014642616402771738\n",
      "Average test loss: 0.0027678642299854093\n",
      "Epoch 171/300\n",
      "Average training loss: 0.014672313676940071\n",
      "Average test loss: 0.0026187366613497336\n",
      "Epoch 172/300\n",
      "Average training loss: 0.014628455412884553\n",
      "Average test loss: 0.002729764999407861\n",
      "Epoch 173/300\n",
      "Average training loss: 0.014630082727306419\n",
      "Average test loss: 0.00266717363624937\n",
      "Epoch 174/300\n",
      "Average training loss: 0.014600793631540404\n",
      "Average test loss: 0.00272245906583137\n",
      "Epoch 175/300\n",
      "Average training loss: 0.014619127712315983\n",
      "Average test loss: 0.002699133420570029\n",
      "Epoch 176/300\n",
      "Average training loss: 0.014589485223922465\n",
      "Average test loss: 0.0026952183935791255\n",
      "Epoch 177/300\n",
      "Average training loss: 0.014577875452737013\n",
      "Average test loss: 0.002924124592087335\n",
      "Epoch 178/300\n",
      "Average training loss: 0.014599510158101717\n",
      "Average test loss: 0.002634898484581047\n",
      "Epoch 179/300\n",
      "Average training loss: 0.014588950081004037\n",
      "Average test loss: 0.0027181979610274233\n",
      "Epoch 180/300\n",
      "Average training loss: 0.014571405373513698\n",
      "Average test loss: 0.002678934922752281\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014564975116815832\n",
      "Average test loss: 0.002677419131828679\n",
      "Epoch 182/300\n",
      "Average training loss: 0.014537009469336934\n",
      "Average test loss: 0.0027069900238679513\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014578455351293087\n",
      "Average test loss: 0.0026619771396120388\n",
      "Epoch 184/300\n",
      "Average training loss: 0.014519338861107827\n",
      "Average test loss: 0.002662064540096455\n",
      "Epoch 185/300\n",
      "Average training loss: 0.014524376985927423\n",
      "Average test loss: 0.002693809348468979\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014527466459406746\n",
      "Average test loss: 0.002757495702761743\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01452729640652736\n",
      "Average test loss: 0.002692382501024339\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01450068647828367\n",
      "Average test loss: 0.0030711737192339367\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01448349229991436\n",
      "Average test loss: 0.002667675850085086\n",
      "Epoch 190/300\n",
      "Average training loss: 0.014535716977384355\n",
      "Average test loss: 0.002761769600212574\n",
      "Epoch 191/300\n",
      "Average training loss: 0.014463989307483037\n",
      "Average test loss: 0.0029690954900450177\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014502235763602787\n",
      "Average test loss: 0.0027056574548284212\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014445300352242257\n",
      "Average test loss: 0.0027086937357154155\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014440470673143864\n",
      "Average test loss: 0.002697446294128895\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014470277571015887\n",
      "Average test loss: 0.0030502890354643267\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014463885507649845\n",
      "Average test loss: 0.0026945435559997954\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014455923120180765\n",
      "Average test loss: 0.0027227498340523905\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014445645418432023\n",
      "Average test loss: 0.0026630026627745894\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014444295510649682\n",
      "Average test loss: 0.0026458872300055294\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014415432732966211\n",
      "Average test loss: 0.00268321826784975\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014404331548346413\n",
      "Average test loss: 0.002724409685573644\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014399841902984514\n",
      "Average test loss: 0.0028655703556206493\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014400499297512902\n",
      "Average test loss: 0.0028463230542838574\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014441828117602402\n",
      "Average test loss: 0.0026348241348233487\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014401687709821595\n",
      "Average test loss: 0.0028264550568742886\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014364139686028163\n",
      "Average test loss: 0.0027302412008866667\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014393078856170178\n",
      "Average test loss: 0.002687248650317391\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014339337786038717\n",
      "Average test loss: 0.0026765737558404603\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014359106793999672\n",
      "Average test loss: 0.0027080401538146868\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0143398917954829\n",
      "Average test loss: 0.0027129714600741864\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014359949395888381\n",
      "Average test loss: 0.002731268328304092\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014338511952095562\n",
      "Average test loss: 0.0031200032343880997\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014331775474879477\n",
      "Average test loss: 0.0026782114673405888\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014352506164875295\n",
      "Average test loss: 0.002721132679325011\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014354669753048156\n",
      "Average test loss: 0.002719604844434394\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014329405249820815\n",
      "Average test loss: 0.0026771324759142265\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014304868939022223\n",
      "Average test loss: 0.002769205354981952\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014295804323421583\n",
      "Average test loss: 0.002741825484774179\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014294859081506728\n",
      "Average test loss: 0.0028470855351123547\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014292276816235648\n",
      "Average test loss: 0.002688521747166912\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014250264612336954\n",
      "Average test loss: 0.002714286756184366\n",
      "Epoch 222/300\n",
      "Average training loss: 0.014295390663047632\n",
      "Average test loss: 0.002842407887594567\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01430075064798196\n",
      "Average test loss: 0.002743765735377868\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014310294484098752\n",
      "Average test loss: 0.0026600436413039765\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014269542879528469\n",
      "Average test loss: 0.0027117072068568735\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014239956736564635\n",
      "Average test loss: 0.0027625511505951486\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01425440074586206\n",
      "Average test loss: 0.0027172333790610233\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014256264069014125\n",
      "Average test loss: 0.0027471151368485556\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014241277401645979\n",
      "Average test loss: 0.002769999595152007\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014210879777040747\n",
      "Average test loss: 0.0027730261364744767\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014229428475101788\n",
      "Average test loss: 0.0027110456532488266\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01425468778444661\n",
      "Average test loss: 0.0027079428057703706\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014237316294676727\n",
      "Average test loss: 0.002749883860970537\n",
      "Epoch 234/300\n",
      "Average training loss: 0.014186780124902725\n",
      "Average test loss: 0.003033151319043504\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014213004166881243\n",
      "Average test loss: 0.0027623257525265215\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014203000740872489\n",
      "Average test loss: 0.003060274968130721\n",
      "Epoch 237/300\n",
      "Average training loss: 0.014209918946027756\n",
      "Average test loss: 0.0026780665173298784\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014218466226425436\n",
      "Average test loss: 0.002746939436636037\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014198687103059557\n",
      "Average test loss: 0.002854374365260204\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014151711008614964\n",
      "Average test loss: 0.0027043486167159345\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014180368584063317\n",
      "Average test loss: 0.002777042399471005\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014182238221168517\n",
      "Average test loss: 0.0028490895461291076\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014157315613494979\n",
      "Average test loss: 0.0027323973238882093\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014173727500769827\n",
      "Average test loss: 0.0027036819491121503\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01415977384812302\n",
      "Average test loss: 0.0027222124110493394\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0141720155403018\n",
      "Average test loss: 0.06784720770517985\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014153543799287743\n",
      "Average test loss: 0.0027245208784523935\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014134860732489162\n",
      "Average test loss: 0.0026981467950261302\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014136996068888241\n",
      "Average test loss: 0.002987933859022127\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014164662563138537\n",
      "Average test loss: 0.0027313605716658964\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014128367898364861\n",
      "Average test loss: 0.0028185026138606998\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014107397957808441\n",
      "Average test loss: 0.0027545196196685235\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014134091014663378\n",
      "Average test loss: 0.00286473035481241\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014102053223384751\n",
      "Average test loss: 0.002828119959268305\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014097000685830911\n",
      "Average test loss: 0.003119118007520835\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014079908361865415\n",
      "Average test loss: 0.0027548685222864152\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014122233500083287\n",
      "Average test loss: 0.0027317014911936387\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014074185086621178\n",
      "Average test loss: 0.0027141758325613208\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01410134263916148\n",
      "Average test loss: 0.002745101876039472\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014090532052848074\n",
      "Average test loss: 0.0026774884071201085\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01407087450226148\n",
      "Average test loss: 0.0028184785588334003\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014074727619687716\n",
      "Average test loss: 0.0027593691860222152\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014058150332834985\n",
      "Average test loss: 0.002826592104612953\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014048976027303272\n",
      "Average test loss: 0.002728118082301484\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014067263160314826\n",
      "Average test loss: 0.002748736670033799\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014065333588255777\n",
      "Average test loss: 0.002874108674418595\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014060789355801212\n",
      "Average test loss: 0.0028063388760719036\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014034848906927638\n",
      "Average test loss: 0.00274669960524059\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014062059216201305\n",
      "Average test loss: 0.0027731749965912767\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014032550784448783\n",
      "Average test loss: 0.0027779811991171703\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014061051232119402\n",
      "Average test loss: 0.0029129715396298303\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014199857966767418\n",
      "Average test loss: 0.0030357708351479635\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013994357521335285\n",
      "Average test loss: 0.002817817750490374\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014007741298940446\n",
      "Average test loss: 0.0027584630977362393\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013992356688612036\n",
      "Average test loss: 0.0029194108216712873\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01403509965704547\n",
      "Average test loss: 0.0027776567877994644\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014013518883122338\n",
      "Average test loss: 0.002807440057086448\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0140073597629865\n",
      "Average test loss: 0.0027364505012002255\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0139964372879929\n",
      "Average test loss: 0.0027417026265627808\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014000736733277638\n",
      "Average test loss: 0.002750076000268261\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0140001450330019\n",
      "Average test loss: 0.00280972194723371\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014002284626993869\n",
      "Average test loss: 0.0028403835176593725\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014009831799401178\n",
      "Average test loss: 0.0027620780734966197\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013989278213845359\n",
      "Average test loss: 0.0027116788116594154\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013968848237560855\n",
      "Average test loss: 0.0027545135714527632\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01397200069990423\n",
      "Average test loss: 0.0027226001311921413\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01399497076537874\n",
      "Average test loss: 0.002967076769512561\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013972392254405552\n",
      "Average test loss: 0.00277435583083166\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013953166041109298\n",
      "Average test loss: 0.0027237168786426384\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01395423486828804\n",
      "Average test loss: 0.002749503982046412\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013983364714516534\n",
      "Average test loss: 0.002821304974042707\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013955861334999403\n",
      "Average test loss: 0.0029614498567663962\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013942419451144007\n",
      "Average test loss: 0.002763547848703133\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013942917007539007\n",
      "Average test loss: 0.00285773091783954\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013937728929022949\n",
      "Average test loss: 0.002782733330710067\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013931334422694312\n",
      "Average test loss: 0.002743422182276845\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013942011616296239\n",
      "Average test loss: 0.0029233530927449463\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013934400611453587\n",
      "Average test loss: 0.0027888476726495555\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013931558760503927\n",
      "Average test loss: 0.002896182676570283\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0139324215915468\n",
      "Average test loss: 0.002962874149903655\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth10/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.28\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.70\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.20\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.45\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.40\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.50\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.50\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.674432468626234\n",
      "Average test loss: 1.3374493715763092\n",
      "Epoch 2/300\n",
      "Average training loss: 2.518489230261909\n",
      "Average test loss: 0.015868108506004015\n",
      "Epoch 3/300\n",
      "Average training loss: 1.96896977297465\n",
      "Average test loss: 0.014089374615086449\n",
      "Epoch 4/300\n",
      "Average training loss: 1.5727512950897218\n",
      "Average test loss: 0.04731389160950979\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2954256403181288\n",
      "Average test loss: 0.1423328007062276\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1279429086049397\n",
      "Average test loss: 0.012878315327896012\n",
      "Epoch 7/300\n",
      "Average training loss: 0.96435912572013\n",
      "Average test loss: 0.009625299047264788\n",
      "Epoch 8/300\n",
      "Average training loss: 0.8339919569757249\n",
      "Average test loss: 0.01038435732656055\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7291216032769945\n",
      "Average test loss: 0.00906828739411301\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6458912352985806\n",
      "Average test loss: 0.008334773085183567\n",
      "Epoch 11/300\n",
      "Average training loss: 0.576086669921875\n",
      "Average test loss: 0.010837074703640408\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5155510121186574\n",
      "Average test loss: 0.008929437410500314\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4650127277109358\n",
      "Average test loss: 0.008906474807196193\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4206340222623613\n",
      "Average test loss: 0.007877896917363007\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3806799011230469\n",
      "Average test loss: 0.007681398642559846\n",
      "Epoch 16/300\n",
      "Average training loss: 0.34618313283390467\n",
      "Average test loss: 0.010108365510072973\n",
      "Epoch 17/300\n",
      "Average training loss: 0.31663375947210526\n",
      "Average test loss: 0.010168669000267982\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2881945658789741\n",
      "Average test loss: 0.0070936711529890695\n",
      "Epoch 19/300\n",
      "Average training loss: 0.26411059353086686\n",
      "Average test loss: 0.0074610638717810315\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2434726782374912\n",
      "Average test loss: 0.007021299031873544\n",
      "Epoch 21/300\n",
      "Average training loss: 0.22558607425954608\n",
      "Average test loss: 0.015350674271583558\n",
      "Epoch 22/300\n",
      "Average training loss: 0.21171092006895278\n",
      "Average test loss: 0.007698123989833726\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1985612026055654\n",
      "Average test loss: 0.2934653346836567\n",
      "Epoch 24/300\n",
      "Average training loss: 0.18898237433698442\n",
      "Average test loss: 0.006912374828424718\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1802849482562807\n",
      "Average test loss: 0.006820985276665952\n",
      "Epoch 26/300\n",
      "Average training loss: 0.17174683548344505\n",
      "Average test loss: 0.006601433280441496\n",
      "Epoch 27/300\n",
      "Average training loss: 0.16442726751168568\n",
      "Average test loss: 0.21641977526081932\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1579565088086658\n",
      "Average test loss: 0.0066385506730940605\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1537784767680698\n",
      "Average test loss: 0.007824528128736548\n",
      "Epoch 30/300\n",
      "Average training loss: 0.14962814123100704\n",
      "Average test loss: 0.008344701687080992\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1439194803237915\n",
      "Average test loss: 0.0066682557496759625\n",
      "Epoch 32/300\n",
      "Average training loss: 0.14092320843537648\n",
      "Average test loss: 0.008362727411091328\n",
      "Epoch 33/300\n",
      "Average training loss: 0.13759257287449306\n",
      "Average test loss: 0.0070086936689913275\n",
      "Epoch 34/300\n",
      "Average training loss: 0.13392564202679527\n",
      "Average test loss: 0.03256058022545444\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1312643006046613\n",
      "Average test loss: 0.007316753698719872\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12782986452844408\n",
      "Average test loss: 0.006669063019255797\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12671292390426\n",
      "Average test loss: 0.010867913284649451\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12274937309159173\n",
      "Average test loss: 1.1569154667258263\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12108388956387837\n",
      "Average test loss: 0.049723602710912626\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11907612117793825\n",
      "Average test loss: 0.20934176598654852\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11725797496239344\n",
      "Average test loss: 0.006279944207933214\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1153805966509713\n",
      "Average test loss: 0.006412921113272508\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11329593096176784\n",
      "Average test loss: 0.006308807796902126\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1121865016023318\n",
      "Average test loss: 0.0065654921183983484\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11017665495475133\n",
      "Average test loss: 6.866463434202804\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10912424823972915\n",
      "Average test loss: 0.04283860082924366\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10783151704072952\n",
      "Average test loss: 0.006777606918993924\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10705061353577508\n",
      "Average test loss: 0.11654039881626765\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10577730241749021\n",
      "Average test loss: 0.007311642578078641\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10451869070529937\n",
      "Average test loss: 304113367.05877775\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10360185886091657\n",
      "Average test loss: 0.006507017970912987\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10242944298850165\n",
      "Average test loss: 70.38371132310563\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10195431030458874\n",
      "Average test loss: 0.006360924367275503\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10167729187673992\n",
      "Average test loss: 0.006524008766644531\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10001966669824389\n",
      "Average test loss: 0.008383635823925336\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0999490643342336\n",
      "Average test loss: 1927.3136358829074\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09935428970389897\n",
      "Average test loss: 0.006525839021222459\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09797923095358743\n",
      "Average test loss: 0.006318183340132236\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09718523244394196\n",
      "Average test loss: 0.006316188825501336\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09671687891748217\n",
      "Average test loss: 0.017090400621294976\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09693262749248081\n",
      "Average test loss: 0.06784404748098717\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0953852793375651\n",
      "Average test loss: 0.006165923373152813\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09478478057516945\n",
      "Average test loss: 0.00653149802568886\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09508419262700611\n",
      "Average test loss: 0.03791109875506825\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09372616138723161\n",
      "Average test loss: 0.8061682959480418\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0933930773999956\n",
      "Average test loss: 0.48035467303792634\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09272192752361298\n",
      "Average test loss: 0.20465574520495203\n",
      "Epoch 68/300\n",
      "Average training loss: 0.13619158200422923\n",
      "Average test loss: 0.0063530284406410325\n",
      "Epoch 69/300\n",
      "Average training loss: 0.1075248462094201\n",
      "Average test loss: 0.0061979930765099\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10124476200342178\n",
      "Average test loss: 0.008249153818521236\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09726716788278686\n",
      "Average test loss: 0.006275704647103945\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09469957488112979\n",
      "Average test loss: 0.021167502356072267\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09371022798617681\n",
      "Average test loss: 0.04223078422910637\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09251790862613254\n",
      "Average test loss: 48408.516283799916\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09208509202798208\n",
      "Average test loss: 0.0063512953263190055\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09153257797824012\n",
      "Average test loss: 0.006333497712595595\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09120570398039288\n",
      "Average test loss: 0.03027681641570396\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09047348922491073\n",
      "Average test loss: 0.207263995040622\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09062019799815284\n",
      "Average test loss: 0.02146238914463255\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0905118788878123\n",
      "Average test loss: 0.007134371187537908\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08960898617241117\n",
      "Average test loss: 0.0074985596475501855\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08929540435473124\n",
      "Average test loss: 0.00637472150185042\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08915407782793044\n",
      "Average test loss: 0.0077988344993856215\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08879518979125553\n",
      "Average test loss: 84.09104965335462\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08848638282219569\n",
      "Average test loss: 0.006590097426540322\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08781911010212369\n",
      "Average test loss: 0.034902609013020995\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08767931323581271\n",
      "Average test loss: 0.006682544091509448\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08725739868481953\n",
      "Average test loss: 0.013866062677568859\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08742900522881084\n",
      "Average test loss: 0.006465828687780433\n",
      "Epoch 90/300\n",
      "Average training loss: 0.08651832934220632\n",
      "Average test loss: 0.006513380125578907\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08777863043546677\n",
      "Average test loss: 0.006560049429949787\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08595900839567185\n",
      "Average test loss: 0.006479231308731768\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08583651248613994\n",
      "Average test loss: 0.006547118542095025\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08577852910094791\n",
      "Average test loss: 0.0077182962629530165\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08516864672634336\n",
      "Average test loss: 0.02921358001894421\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08506095012029012\n",
      "Average test loss: 0.006386502135131094\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08475468962722355\n",
      "Average test loss: 0.006616293164590994\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0885594006313218\n",
      "Average test loss: 0.006422878014958567\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08524421030945248\n",
      "Average test loss: 0.008727998247163164\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08393212784661187\n",
      "Average test loss: 0.0066384167596697805\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08335478259457482\n",
      "Average test loss: 0.006593162822226683\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0835495305094454\n",
      "Average test loss: 0.006927894052945905\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08315636801719666\n",
      "Average test loss: 0.016218114330536788\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0832905972732438\n",
      "Average test loss: 0.006772551636728976\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08292279536856545\n",
      "Average test loss: 0.006554478613038857\n",
      "Epoch 106/300\n",
      "Average training loss: 0.08297087270683712\n",
      "Average test loss: 0.007981762692332268\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08266591411166721\n",
      "Average test loss: 0.09121888412369622\n",
      "Epoch 108/300\n",
      "Average training loss: 0.08281655457284716\n",
      "Average test loss: 0.014900469304372867\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0896150341100163\n",
      "Average test loss: 0.03646476185073455\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09319174163871341\n",
      "Average test loss: 0.015217459045350552\n",
      "Epoch 111/300\n",
      "Average training loss: 0.08348298221826553\n",
      "Average test loss: 0.0071529502562350696\n",
      "Epoch 112/300\n",
      "Average training loss: 0.08187399349610011\n",
      "Average test loss: 0.007291675666968028\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08132545222176446\n",
      "Average test loss: 1.0357213865088093\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08136768517229293\n",
      "Average test loss: 0.01750194735535317\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08085660375489129\n",
      "Average test loss: 1.0544694343441063\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0815222968724039\n",
      "Average test loss: 0.006741453607049253\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08042676765388912\n",
      "Average test loss: 0.006792442665331894\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08024394436015023\n",
      "Average test loss: 0.0067946838330891395\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08009462704261144\n",
      "Average test loss: 0.01813316416574849\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08021865496370527\n",
      "Average test loss: 0.006910499311155743\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0798965455558565\n",
      "Average test loss: 0.006937326454867919\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07967325964238908\n",
      "Average test loss: 0.006774934189187156\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07949848343266382\n",
      "Average test loss: 0.02489211368560791\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08046384806103177\n",
      "Average test loss: 0.006757086796893014\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07899217557244831\n",
      "Average test loss: 0.01686884059343073\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07894353969229592\n",
      "Average test loss: 0.006574581238130728\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07967812563975651\n",
      "Average test loss: 0.0068040247257384985\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07878715657525592\n",
      "Average test loss: 0.007037081970936722\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07866142842504713\n",
      "Average test loss: 0.007240349200036791\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07814773442347844\n",
      "Average test loss: 0.006821612349400917\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07818680114216274\n",
      "Average test loss: 0.006803592181454102\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07812512505716748\n",
      "Average test loss: 0.007148013303677241\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07780714551607767\n",
      "Average test loss: 0.006895735446363687\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07809029132790035\n",
      "Average test loss: 0.007136361610144377\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07733601502577464\n",
      "Average test loss: 0.006889944097234143\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0775656191209952\n",
      "Average test loss: 0.008265699168046316\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07754176133871078\n",
      "Average test loss: 0.01040704964266883\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07712457976076338\n",
      "Average test loss: 0.006792886771675613\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07693680912918514\n",
      "Average test loss: 0.007068469237536192\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07698484252227678\n",
      "Average test loss: 0.007043338973489073\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07675012329551908\n",
      "Average test loss: 0.006876289592021041\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07703298981984456\n",
      "Average test loss: 0.0069414853428800905\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07690824560986625\n",
      "Average test loss: 0.00689914746168587\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07643421107530594\n",
      "Average test loss: 0.15484858340770005\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07641289672586653\n",
      "Average test loss: 0.007477858554985788\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07630768034855524\n",
      "Average test loss: 0.007144862102551592\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07630756978856193\n",
      "Average test loss: 0.0070981991663575175\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0764543415374226\n",
      "Average test loss: 0.006904672320518229\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07631035641829173\n",
      "Average test loss: 0.01858695303897063\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07567999746070968\n",
      "Average test loss: 0.007069364140431086\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07577755152516895\n",
      "Average test loss: 0.007888887731151449\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07600815150472853\n",
      "Average test loss: 0.01114998986903164\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07556757137510511\n",
      "Average test loss: 0.006789053137931559\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07536303553316329\n",
      "Average test loss: 0.006836381588959032\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07594029953082403\n",
      "Average test loss: 0.04739154862364133\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07526605473624336\n",
      "Average test loss: 0.007679944399330351\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07516663884123166\n",
      "Average test loss: 0.007313832526405653\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07528194117546082\n",
      "Average test loss: 0.006997044024368127\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07535854541593127\n",
      "Average test loss: 0.007022030776987473\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07498777325285806\n",
      "Average test loss: 0.007162362397958835\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07511575277646383\n",
      "Average test loss: 0.00691083789906568\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07498341103394826\n",
      "Average test loss: 0.007043586387816402\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07479661055737072\n",
      "Average test loss: 0.006810841664671898\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0745857908460829\n",
      "Average test loss: 0.007018066542016135\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07446364818678962\n",
      "Average test loss: 0.006866554541306363\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07587867877880732\n",
      "Average test loss: 0.006924528769320912\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07477996182441711\n",
      "Average test loss: 0.007116992771625519\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0740368679828114\n",
      "Average test loss: 0.007406901197301017\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07429336638583077\n",
      "Average test loss: 0.007543091955284278\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0742783650888337\n",
      "Average test loss: 0.007273757778522041\n",
      "Epoch 176/300\n",
      "Average training loss: 0.07810927295353677\n",
      "Average test loss: 0.0069282490462064744\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07389637235138151\n",
      "Average test loss: 0.007228647643493282\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07358579560783174\n",
      "Average test loss: 0.007090102627459499\n",
      "Epoch 179/300\n",
      "Average training loss: 0.07371255936225256\n",
      "Average test loss: 0.007092849887079662\n",
      "Epoch 181/300\n",
      "Average training loss: 0.07388696044021183\n",
      "Average test loss: 0.00917236029356718\n",
      "Epoch 182/300\n",
      "Average training loss: 0.07398226662476858\n",
      "Average test loss: 0.00796234711756309\n",
      "Epoch 183/300\n",
      "Average training loss: 0.07350234078036415\n",
      "Average test loss: 0.007620173070165846\n",
      "Epoch 184/300\n",
      "Average training loss: 0.07363330250316195\n",
      "Average test loss: 0.0067697736716104875\n",
      "Epoch 185/300\n",
      "Average training loss: 0.07383236630095376\n",
      "Average test loss: 0.00684703994790713\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07341082974937227\n",
      "Average test loss: 0.0069732500087055895\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07339624044630262\n",
      "Average test loss: 0.018315835467643208\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07653745688994726\n",
      "Average test loss: 0.00706361682795816\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07338660780588786\n",
      "Average test loss: 0.00794281440642145\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07308687453799778\n",
      "Average test loss: 0.006961331527266237\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07292323189311557\n",
      "Average test loss: 0.007009665493335988\n",
      "Epoch 192/300\n",
      "Average training loss: 0.07304992761214574\n",
      "Average test loss: 0.008160280837780899\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07318040251069599\n",
      "Average test loss: 0.007837860722508696\n",
      "Epoch 194/300\n",
      "Average training loss: 0.073022339436743\n",
      "Average test loss: 0.00849473827249474\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07317475273211797\n",
      "Average test loss: 0.006909608080983162\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07265805199411181\n",
      "Average test loss: 0.007011604465544224\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08348829038937887\n",
      "Average test loss: 0.006896649063461356\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07690442053476969\n",
      "Average test loss: 0.006892697013086743\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07270376933283276\n",
      "Average test loss: 0.0070397963623205825\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07234637383288807\n",
      "Average test loss: 0.007114630477295982\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07198881423473358\n",
      "Average test loss: 0.0069375967602762914\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07221475801202985\n",
      "Average test loss: 0.00708234075208505\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07331590025292503\n",
      "Average test loss: 0.007159944884065125\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07213111249605815\n",
      "Average test loss: 0.0071604237697190706\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07385300330983267\n",
      "Average test loss: 0.2065526209572951\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07353131200869878\n",
      "Average test loss: 0.59344682997134\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0717373969356219\n",
      "Average test loss: 0.007251546544333299\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07172755444712109\n",
      "Average test loss: 0.0077656649806433255\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0718448775543107\n",
      "Average test loss: 0.0069941190994448135\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0718818832702107\n",
      "Average test loss: 0.007049360387855106\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07197640079259872\n",
      "Average test loss: 0.0070464742527239855\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07653108321295844\n",
      "Average test loss: 0.008436706429554356\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07298228724135293\n",
      "Average test loss: 0.006918062484719687\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07127569821808073\n",
      "Average test loss: 0.006966676437192493\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07171900698873732\n",
      "Average test loss: 0.029477545158730613\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07160326242446899\n",
      "Average test loss: 0.010643488039573034\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07158363499244054\n",
      "Average test loss: 0.007044297731584973\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07176849780811204\n",
      "Average test loss: 0.007120185553613636\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07140006219678455\n",
      "Average test loss: 0.007144584153261449\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07178343872229258\n",
      "Average test loss: 0.007025654222402308\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07113376656505796\n",
      "Average test loss: 0.00871912352906333\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0715657597316636\n",
      "Average test loss: 0.006795147930582364\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07130529352360301\n",
      "Average test loss: 0.007163362559345033\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07313316200508012\n",
      "Average test loss: 0.0074402134252919095\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07078833210468292\n",
      "Average test loss: 0.00703686419998606\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0707191132903099\n",
      "Average test loss: 0.006971120235820611\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0709127542177836\n",
      "Average test loss: 0.006963963291711278\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07088219793968731\n",
      "Average test loss: 0.007258795925726493\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07094430163171556\n",
      "Average test loss: 0.006983147632123696\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0709673610230287\n",
      "Average test loss: 0.007243305794480774\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07108075439267689\n",
      "Average test loss: 0.007158543484078513\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07412517872783873\n",
      "Average test loss: 0.007045234831256999\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07052866321139865\n",
      "Average test loss: 0.0071105219514833555\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07020036343071195\n",
      "Average test loss: 0.008260693836129374\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07092807847261429\n",
      "Average test loss: 0.00693184862492813\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07051893184582392\n",
      "Average test loss: 0.007651829802327686\n",
      "Epoch 243/300\n",
      "Average training loss: 0.14989494110478296\n",
      "Average test loss: 0.006798271158503162\n",
      "Epoch 244/300\n",
      "Average training loss: 0.1115036970310741\n",
      "Average test loss: 0.006508912439975474\n",
      "Epoch 245/300\n",
      "Average training loss: 0.10159163529343075\n",
      "Average test loss: 0.0066987599010268845\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09419776788685057\n",
      "Average test loss: 0.006727430264568991\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08768396532535552\n",
      "Average test loss: 0.006682680062535736\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08010292533040046\n",
      "Average test loss: 0.006837641666332881\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07531833271516694\n",
      "Average test loss: 0.007292769050846497\n",
      "Epoch 250/300\n",
      "Average training loss: 0.07291098177433014\n",
      "Average test loss: 0.007057978229804172\n",
      "Epoch 251/300\n",
      "Average training loss: 0.071345376710097\n",
      "Average test loss: 0.007066700364566512\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0706610454916954\n",
      "Average test loss: 0.006905999345084031\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07026812658045027\n",
      "Average test loss: 0.007073386188182566\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07014477188057369\n",
      "Average test loss: 0.007680833067331049\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07019047133790123\n",
      "Average training loss: 0.07027665838930341\n",
      "Average test loss: 0.00731506493439277\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07052077411943011\n",
      "Average test loss: 0.0071999152588347595\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07864016724295086\n",
      "Average test loss: 0.3760092965364456\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07676356858346198\n",
      "Average test loss: 0.007515911946694057\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07073695345719655\n",
      "Average test loss: 0.007978389824430148\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07014812909232246\n",
      "Average test loss: 0.00720380744834741\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07006662448909548\n",
      "Average test loss: 0.007099485043022368\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07004367061456045\n",
      "Average test loss: 0.00718543840572238\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07068472266197205\n",
      "Average test loss: 0.007028669664429294\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06990813787778219\n",
      "Average test loss: 0.006979978383415275\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06996151190996171\n",
      "Average test loss: 0.0077175067158208955\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07019750650723776\n",
      "Average test loss: 0.00843359037157562\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07133045666747623\n",
      "Average test loss: 0.0070782221779227255\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06966551723745135\n",
      "Average test loss: 0.007143239894674884\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07005422927935918\n",
      "Average test loss: 0.007023525728947586\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07001829496357176\n",
      "Average test loss: 0.014609390422287915\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06978510147333145\n",
      "Average test loss: 0.006992573525342677\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07028142071432537\n",
      "Average test loss: 0.007004632834759023\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06951285978158316\n",
      "Average test loss: 0.007323045593582922\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06981054580211639\n",
      "Average test loss: 0.007041001551681095\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06933477274576823\n",
      "Average test loss: 0.007098329028321637\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06965635393394365\n",
      "Average test loss: 0.00891802204814222\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06950098052952025\n",
      "Average test loss: 0.007119640735288461\n",
      "Epoch 281/300\n",
      "Average training loss: 0.10014865879880057\n",
      "Average test loss: 0.0076579040591087605\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07602620485093858\n",
      "Average test loss: 0.00941528950838579\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07043851171599494\n",
      "Average test loss: 0.0075318310757478074\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06887912629048029\n",
      "Average test loss: 0.007312553643352456\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07120244549380408\n",
      "Average test loss: 0.017341346222493382\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07046075797080993\n",
      "Average test loss: 0.0071279483048452275\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06883279814322789\n",
      "Average test loss: 0.006986596910903851\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06886426689227422\n",
      "Average test loss: 0.007282346280084716\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06921192974183295\n",
      "Average test loss: 0.0072327688638534815\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06896700073613062\n",
      "Average test loss: 0.012131800557176272\n",
      "Epoch 293/300\n",
      "Average training loss: 0.11167425298028522\n",
      "Average test loss: 0.006654242538743549\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0800583184030321\n",
      "Average test loss: 0.008412198524508212\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07209183251857758\n",
      "Average test loss: 0.006994132037791941\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06958442311154471\n",
      "Average test loss: 0.008886945360236698\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0685602171089914\n",
      "Average test loss: 0.007111700321237246\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06842119201686647\n",
      "Average test loss: 0.007096659276634455\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06861834836668439\n",
      "Average test loss: 0.008584772599240145\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07890565709935295\n",
      "Average test loss: 0.007351387908475266\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.092292727152507\n",
      "Average test loss: 32.966793693343796\n",
      "Epoch 2/300\n",
      "Average training loss: 2.1673032086690265\n",
      "Average test loss: 0.01174120807233784\n",
      "Epoch 3/300\n",
      "Average training loss: 1.5195207999547322\n",
      "Average test loss: 0.48160381319125495\n",
      "Epoch 4/300\n",
      "Average training loss: 0.9613712452782525\n",
      "Average test loss: 0.006815029237833288\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8091190565427144\n",
      "Average test loss: 0.006638845214413272\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6888888552453782\n",
      "Average test loss: 0.006696524623367521\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6008739460839165\n",
      "Average test loss: 0.006195031918585301\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5253517168892754\n",
      "Average test loss: 0.057961946460935804\n",
      "Epoch 10/300\n",
      "Average training loss: 0.46222108658154804\n",
      "Average test loss: 0.013256427278535234\n",
      "Epoch 11/300\n",
      "Average training loss: 0.40714819129308066\n",
      "Average test loss: 0.006737560338858101\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3609363927046458\n",
      "Average test loss: 0.005338929428201583\n",
      "Epoch 13/300\n",
      "Average training loss: 0.32123700096872115\n",
      "Average test loss: 0.005748724962274234\n",
      "Epoch 14/300\n",
      "Average training loss: 0.28725779411527846\n",
      "Average test loss: 0.005202748936497503\n",
      "Epoch 15/300\n",
      "Average training loss: 0.25917981775601706\n",
      "Average test loss: 0.005320513584133652\n",
      "Epoch 16/300\n",
      "Average training loss: 0.23480068393548328\n",
      "Average test loss: 0.005019321195781231\n",
      "Epoch 17/300\n",
      "Average training loss: 0.21473901732762654\n",
      "Average test loss: 0.005113711448179351\n",
      "Epoch 18/300\n",
      "Average training loss: 0.19828725055853527\n",
      "Average test loss: 0.0055279086441215545\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18397481320963965\n",
      "Average test loss: 0.004735059238142437\n",
      "Epoch 20/300\n",
      "Average training loss: 0.17059472844335768\n",
      "Average test loss: 0.0046425303001370695\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16003707632753583\n",
      "Average test loss: 0.029316932612823116\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1515435632334815\n",
      "Average test loss: 0.0045215623983078535\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14164993438455795\n",
      "Average test loss: 0.5950236912435956\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12184338766998715\n",
      "Average test loss: 0.004718974997599919\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11602790876229604\n",
      "Average test loss: 0.004259951810455984\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11057175623046027\n",
      "Average test loss: 0.004402037030706803\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10661711412668227\n",
      "Average test loss: 0.004269783138194018\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10308092724614673\n",
      "Average test loss: 0.015005123429000377\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09932329198386934\n",
      "Average test loss: 0.013457465530683596\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09650736031267378\n",
      "Average test loss: 0.004378834522432751\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0939880733953582\n",
      "Average test loss: 0.06537929910421371\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09157533992661371\n",
      "Average test loss: 0.004160885576779644\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08879441269901063\n",
      "Average test loss: 0.004118738590222266\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08707963479889763\n",
      "Average test loss: 0.004395925954398181\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0851196628411611\n",
      "Average test loss: 0.004212335493415594\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08293302729394701\n",
      "Average test loss: 0.004106503195025855\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08090937933656904\n",
      "Average test loss: 0.004241233953171306\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07935413604312473\n",
      "Average test loss: 0.0041512290168967516\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07791430619359016\n",
      "Average test loss: 1.1088050217198\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07702733800808588\n",
      "Average test loss: 4.07079103049967\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07550098590718375\n",
      "Average test loss: 0.007259315356612206\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07454845630460315\n",
      "Average test loss: 0.008438217787279023\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07327686572737165\n",
      "Average test loss: 0.05695679620073901\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07257213845849037\n",
      "Average test loss: 0.004511217438098457\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07172131225797865\n",
      "Average test loss: 0.00506599637410707\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07101678289307488\n",
      "Average test loss: 0.004422706072943078\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07035105972157585\n",
      "Average test loss: 0.006297382036017047\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06994258544842402\n",
      "Average test loss: 0.0040950514206455815\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06908754101726744\n",
      "Average test loss: 2.775621886652377\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06852971414062711\n",
      "Average test loss: 0.01377834023286899\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06833842034141223\n",
      "Average test loss: 0.006142986164738734\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06778392061922285\n",
      "Average test loss: 0.005148312182062202\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0742653742465708\n",
      "Average test loss: 0.004077911189239886\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07108464768860075\n",
      "Average test loss: 0.004079998267607556\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06953242325782776\n",
      "Average test loss: 0.004063767087956269\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06844803887605667\n",
      "Average test loss: 0.0041297637470480465\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06771941213475334\n",
      "Average test loss: 0.00404146641989549\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06695038799444834\n",
      "Average test loss: 0.00408194579059879\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06649631675415568\n",
      "Average test loss: 0.003999301012191508\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06616381483607822\n",
      "Average test loss: 0.00460862942536672\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0659276984002855\n",
      "Average test loss: 0.00405424562883046\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06541048345632024\n",
      "Average test loss: 0.004003435773154099\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06518842171298132\n",
      "Average test loss: 0.028089711170229646\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06486829235818652\n",
      "Average test loss: 0.005107693439142571\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06451313792996936\n",
      "Average test loss: 0.004921713682512443\n",
      "Epoch 70/300\n",
      "Average training loss: 0.064262299504545\n",
      "Average test loss: 0.004361240019814835\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0640311805539661\n",
      "Average test loss: 0.004643006248192655\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06341846345530616\n",
      "Average test loss: 0.005396310745841927\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06334035874737634\n",
      "Average test loss: 0.0044801515320109\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06283568018343713\n",
      "Average test loss: 0.0040856828157686525\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06274573361211352\n",
      "Average test loss: 0.004052765465651949\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06229442252053155\n",
      "Average training loss: 0.06186037821902169\n",
      "Average test loss: 0.005687064464514454\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0614165684249666\n",
      "Average test loss: 14.05703590607643\n",
      "Epoch 80/300\n",
      "Average training loss: 0.061462231357892354\n",
      "Average test loss: 0.009443732596106\n",
      "Epoch 81/300\n",
      "Average training loss: 0.060852501339382595\n",
      "Average test loss: 0.004295604754239321\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06094081230627166\n",
      "Average test loss: 0.004277936824080017\n",
      "Epoch 83/300\n",
      "Average training loss: 0.2104115103615655\n",
      "Average test loss: 0.004613064445141289\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10938876179854075\n",
      "Average test loss: 0.00464657953961028\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09296980595588684\n",
      "Average test loss: 0.0042273016718940605\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08597585899962319\n",
      "Average test loss: 0.004296340573579073\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08206170071495904\n",
      "Average test loss: 0.0045602569857405295\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07870427831676272\n",
      "Average test loss: 0.004062077590988742\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07639429022206201\n",
      "Average test loss: 0.005876969355675909\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0743784458041191\n",
      "Average test loss: 0.004041933494309584\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07107392638921738\n",
      "Average test loss: 0.007288400646713045\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06980530507697\n",
      "Average test loss: 0.004221564241995414\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06880086404085159\n",
      "Average test loss: 0.004132624862301681\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06781348409917619\n",
      "Average test loss: 0.005425530875722567\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06691417006651561\n",
      "Average test loss: 0.013087278515514401\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0661201476222939\n",
      "Average test loss: 0.004115270874980423\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06573595299985674\n",
      "Average test loss: 0.00432827940914366\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06514715447690751\n",
      "Average test loss: 0.004378369006431765\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0646081877152125\n",
      "Average test loss: 0.004469064550681246\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06416042782200708\n",
      "Average test loss: 0.00571767773768968\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06352473003334469\n",
      "Average test loss: 0.004099136719480157\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06320650702052646\n",
      "Average test loss: 0.004261502438535293\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06277831068634987\n",
      "Average test loss: 36.752204202757945\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06240428386794196\n",
      "Average test loss: 0.00415452798248993\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06208602314856317\n",
      "Average test loss: 0.004145979139953852\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06179961038960351\n",
      "Average test loss: 0.281115535987748\n",
      "Epoch 108/300\n",
      "Average training loss: 0.061567101856072746\n",
      "Average test loss: 0.004463169726232688\n",
      "Epoch 109/300\n",
      "Average training loss: 0.061252878096368576\n",
      "Average test loss: 0.005038929633382294\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06079274439811706\n",
      "Average test loss: 0.004198108539813094\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06073964729573991\n",
      "Average test loss: 0.004356154593742556\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06027666500873036\n",
      "Average test loss: 0.004149032617401746\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0598704202108913\n",
      "Average test loss: 0.004143932892009616\n",
      "Epoch 114/300\n",
      "Average training loss: 0.059899965484937034\n",
      "Average test loss: 0.004462925116014149\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05956219622823927\n",
      "Average test loss: 0.0057877528228693536\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05931211460961236\n",
      "Average test loss: 0.0040945120489017835\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05913928504122628\n",
      "Average test loss: 0.0041759195054570835\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0589827308886581\n",
      "Average test loss: 0.35884425736467046\n",
      "Epoch 119/300\n",
      "Average training loss: 0.058609731680817075\n",
      "Average test loss: 5.151363130715159\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05845798831515842\n",
      "Average test loss: 0.004360280823583404\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05819798919227388\n",
      "Average test loss: 0.0051890777651634485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0582628841665056\n",
      "Average test loss: 0.007161184894541899\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05789406094286177\n",
      "Average test loss: 0.004109795124166542\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05891844806406233\n",
      "Average test loss: 0.004251780369629463\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05746353209680981\n",
      "Average test loss: 0.004360121750790212\n",
      "Epoch 126/300\n",
      "Average training loss: 0.057435529126061334\n",
      "Average test loss: 0.004127066915647851\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05732580502827962\n",
      "Average test loss: 0.07637771880295542\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0569331021838718\n",
      "Average test loss: 0.004237492941319943\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05700390110413234\n",
      "Average test loss: 0.004333687350567844\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05695104825496673\n",
      "Average test loss: 0.004272864960961872\n",
      "Epoch 131/300\n",
      "Average training loss: 0.056779771318038304\n",
      "Average test loss: 0.00434169253686236\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05654773340622584\n",
      "Average test loss: 0.004317393544647429\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05635745077662998\n",
      "Average test loss: 0.005334198588712348\n",
      "Epoch 134/300\n",
      "Average training loss: 0.056468958963950475\n",
      "Average test loss: 0.004431468240502808\n",
      "Epoch 135/300\n",
      "Average training loss: 0.056131953510973186\n",
      "Average test loss: 0.004184133407970269\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05609587575329675\n",
      "Average test loss: 0.005355918269811405\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05968687613142861\n",
      "Average test loss: 3948.55414714898\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05745965162913005\n",
      "Average test loss: 0.004293402494655715\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05570213578806983\n",
      "Average test loss: 0.004205499395728111\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05550997026099099\n",
      "Average test loss: 0.04500908105737633\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05671714567475849\n",
      "Average test loss: 0.006326456622944938\n",
      "Epoch 142/300\n",
      "Average training loss: 0.055457863075865636\n",
      "Average test loss: 0.007088429389314519\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05516949453618791\n",
      "Average test loss: 0.004218834000743098\n",
      "Epoch 144/300\n",
      "Average training loss: 0.055176681571536594\n",
      "Average test loss: 0.004264877090023623\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05512062389983071\n",
      "Average test loss: 0.004303522328121794\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05503852753506767\n",
      "Average test loss: 0.004153964144488175\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05492251261406474\n",
      "Average test loss: 0.004913942188024521\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05490832589070002\n",
      "Average test loss: 0.004356158482531706\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05482581794261932\n",
      "Average test loss: 0.24203823631339602\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05489619156387117\n",
      "Average test loss: 0.004174028051396211\n",
      "Epoch 151/300\n",
      "Average training loss: 0.054573778298166065\n",
      "Average test loss: 0.004605981685635117\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05437349178393682\n",
      "Average test loss: 0.004148468545327584\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05437863228056166\n",
      "Average test loss: 0.14409622085922294\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05448101606633928\n",
      "Average test loss: 0.004683067629527715\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05414375188615587\n",
      "Average test loss: 0.004349090183360709\n",
      "Epoch 156/300\n",
      "Average training loss: 0.054127683255407545\n",
      "Average test loss: 0.004480550975849231\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05414719087216589\n",
      "Average test loss: 0.0059487532269623545\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05403259319067001\n",
      "Average test loss: 0.0042396767135295605\n",
      "Epoch 159/300\n",
      "Average training loss: 0.053869142280684575\n",
      "Average test loss: 0.004306129554079639\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05389181039730708\n",
      "Average test loss: 0.004494523027704821\n",
      "Epoch 161/300\n",
      "Average training loss: 0.053684619042608475\n",
      "Average test loss: 160.30252921560074\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05349928903579712\n",
      "Average test loss: 0.004284750530496239\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05360030750102467\n",
      "Average test loss: 0.0046822701245546345\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05350216915872362\n",
      "Average test loss: 0.00437370710944136\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05342692207627826\n",
      "Average test loss: 0.004308142555670606\n",
      "Epoch 166/300\n",
      "Average training loss: 0.053384150471952226\n",
      "Average test loss: 0.004612806344611777\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05334580570128229\n",
      "Average test loss: 0.008941863796777196\n",
      "Epoch 168/300\n",
      "Average training loss: 0.053115228689379165\n",
      "Average test loss: 0.004362906192946765\n",
      "Epoch 169/300\n",
      "Average training loss: 0.053135670175155005\n",
      "Average test loss: 0.004563651153196891\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05310851404733128\n",
      "Average test loss: 0.004215649073736535\n",
      "Epoch 171/300\n",
      "Average training loss: 0.052915641436974206\n",
      "Average test loss: 0.004453635146220525\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05327561296357049\n",
      "Average test loss: 0.0043143566683348684\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05277656082312266\n",
      "Average test loss: 0.00519427829898066\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05289472460746765\n",
      "Average test loss: 0.005246759824868706\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05517188717259301\n",
      "Average test loss: 0.004269004856132799\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05259852625926335\n",
      "Average test loss: 0.004518435393770536\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05245101369420687\n",
      "Average test loss: 0.06136702181005643\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05243498949209849\n",
      "Average test loss: 0.004484080370101663\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05251313225097126\n",
      "Average test loss: 0.004359439771415459\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05231982576847077\n",
      "Average test loss: 0.004307386810166968\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05248682341972987\n",
      "Average test loss: 0.004435708395308918\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05254640167620447\n",
      "Average test loss: 0.004336904268918766\n",
      "Epoch 183/300\n",
      "Average training loss: 0.052246460729175145\n",
      "Average test loss: 0.0047839924672411546\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05237021624048551\n",
      "Average test loss: 0.005663448009226057\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05208095437950558\n",
      "Average test loss: 0.004315540648582909\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05233211505413055\n",
      "Average test loss: 0.004440796167486244\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05208761245012283\n",
      "Average test loss: 0.005936748027801514\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05194181082977189\n",
      "Average test loss: 0.004540319852117035\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05217690363195207\n",
      "Average test loss: 0.004262802116572857\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05222525311509768\n",
      "Average test loss: 0.004975608693228828\n",
      "Epoch 191/300\n",
      "Average training loss: 0.051771620528565515\n",
      "Average test loss: 0.004391249846667051\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05183693335784806\n",
      "Average test loss: 0.004379407047397561\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05186677844656838\n",
      "Average test loss: 0.004484281278318829\n",
      "Epoch 194/300\n",
      "Average training loss: 0.051783802502685126\n",
      "Average test loss: 0.005461922696481148\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05170095417896906\n",
      "Average test loss: 0.006007170049680604\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05214244546161757\n",
      "Average test loss: 0.004422986677951283\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05141294958525234\n",
      "Average test loss: 0.00433002153183851\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05145802731977569\n",
      "Average test loss: 0.0044091455849508444\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05177382848660151\n",
      "Average test loss: 0.005794000308960676\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05147455669111676\n",
      "Average test loss: 0.004351320009678602\n",
      "Epoch 201/300\n",
      "Average training loss: 0.051313802298572325\n",
      "Average test loss: 0.009723374071634478\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05131775972909398\n",
      "Average test loss: 0.0045531143355700705\n",
      "Epoch 203/300\n",
      "Average training loss: 0.052220423761341304\n",
      "Average test loss: 0.005312951508081622\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0511718069281843\n",
      "Average test loss: 0.006451192936135663\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05123716830545002\n",
      "Average test loss: 0.004455885128428538\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05128921142551634\n",
      "Average test loss: 0.004486696156155732\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0511432206067774\n",
      "Average test loss: 0.004389548260304663\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05106396983067195\n",
      "Average test loss: 0.004709825198683474\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05126863305105103\n",
      "Average test loss: 0.006019935281740295\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0509641187455919\n",
      "Average test loss: 0.006914989468124177\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05104702841904428\n",
      "Average test loss: 0.0044838680314521\n",
      "Epoch 212/300\n",
      "Average training loss: 0.051151022957430944\n",
      "Average test loss: 0.05974782685645753\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0509323493109809\n",
      "Average test loss: 0.017498767897486688\n",
      "Epoch 214/300\n",
      "Average training loss: 0.050900957385698956\n",
      "Average test loss: 0.005006364524571432\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05104873252908389\n",
      "Average test loss: 0.008566735769725508\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05078210952546861\n",
      "Average test loss: 0.004514921113848686\n",
      "Epoch 217/300\n",
      "Average training loss: 0.050795625663465925\n",
      "Average test loss: 0.0043273049839254885\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05066900943054093\n",
      "Average test loss: 0.004416046114017566\n",
      "Epoch 219/300\n",
      "Average training loss: 0.050568432758251826\n",
      "Average test loss: 0.005083590751720799\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0509185087647703\n",
      "Average test loss: 0.005329819564190176\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0504733990629514\n",
      "Average test loss: 0.004641527268207736\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05046894836756918\n",
      "Average test loss: 0.004351013445605834\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05062963159547912\n",
      "Average test loss: 0.005192523645444049\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05046520259976387\n",
      "Average test loss: 0.004431930288258526\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05044084227747388\n",
      "Average test loss: 0.004546122532958785\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05060017286406623\n",
      "Average test loss: 0.0047355800548361405\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05043500254882707\n",
      "Average test loss: 0.02147006447509759\n",
      "Epoch 228/300\n",
      "Average training loss: 0.050252384930849076\n",
      "Average test loss: 0.0047965910354008275\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05048358563913239\n",
      "Average test loss: 0.004923803890744845\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05056711225377189\n",
      "Average test loss: 0.008459841841624842\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05003018045425415\n",
      "Average test loss: 0.005264973427686426\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05026207878192266\n",
      "Average test loss: 0.0046577954519954\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05022858022650083\n",
      "Average test loss: 0.004465558375335402\n",
      "Epoch 234/300\n",
      "Average training loss: 0.050459952821334204\n",
      "Average test loss: 0.004492451602386103\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05153015568190151\n",
      "Average test loss: 0.004447139116624991\n",
      "Epoch 236/300\n",
      "Average training loss: 0.050159869578149587\n",
      "Average test loss: 5802340.493333333\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07865934162007437\n",
      "Average test loss: 0.00414665484883719\n",
      "Epoch 238/300\n",
      "Average training loss: 0.057149552020761704\n",
      "Average test loss: 0.004289642515281836\n",
      "Epoch 239/300\n",
      "Average training loss: 0.052076659772131176\n",
      "Average test loss: 0.004400660326083501\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05032413054505984\n",
      "Average test loss: 0.004373433057839672\n",
      "Epoch 241/300\n",
      "Average training loss: 0.049705366886324355\n",
      "Average test loss: 0.004439076430681679\n",
      "Epoch 242/300\n",
      "Average training loss: 0.049619331879748235\n",
      "Average test loss: 0.005200362620668279\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04960872117678324\n",
      "Average test loss: 0.004461904259191619\n",
      "Epoch 244/300\n",
      "Average training loss: 0.049633849766519335\n",
      "Average test loss: 0.004355690997093916\n",
      "Epoch 245/300\n",
      "Average training loss: 0.049794096956650415\n",
      "Average test loss: 0.0044021484144032\n",
      "Epoch 246/300\n",
      "Average training loss: 0.049829721675978766\n",
      "Average test loss: 0.004714509180436532\n",
      "Epoch 247/300\n",
      "Average training loss: 0.049912565463119084\n",
      "Average test loss: 0.004399537077173591\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04975891139441066\n",
      "Average test loss: 0.00438987042920457\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04969745503862699\n",
      "Average test loss: 0.00463013352557189\n",
      "Epoch 250/300\n",
      "Average training loss: 0.049727575384908254\n",
      "Average test loss: 0.00451709986022777\n",
      "Epoch 251/300\n",
      "Average training loss: 0.049867590602901246\n",
      "Average test loss: 0.0044143206100496985\n",
      "Epoch 252/300\n",
      "Average training loss: 0.049697581479946774\n",
      "Average test loss: 0.008734019260439608\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05003364501065678\n",
      "Average test loss: 0.004492789190883438\n",
      "Epoch 254/300\n",
      "Average training loss: 0.050537672612402176\n",
      "Average test loss: 0.004420581708351771\n",
      "Epoch 255/300\n",
      "Average training loss: 0.049385503070222006\n",
      "Average test loss: 0.005043019550542037\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04942893678281042\n",
      "Average test loss: 0.004811967804200119\n",
      "Epoch 257/300\n",
      "Average training loss: 0.049542466935184264\n",
      "Average test loss: 0.004498441761980454\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04952350702881813\n",
      "Average test loss: 0.005161435545732578\n",
      "Epoch 259/300\n",
      "Average training loss: 0.049684080143769585\n",
      "Average test loss: 0.004571531713422802\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0493651949763298\n",
      "Average test loss: 0.004826509949233797\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04976118471225103\n",
      "Average test loss: 0.02639387750956747\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04929401682813962\n",
      "Average test loss: 0.004398370029197799\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0494808791478475\n",
      "Average test loss: 0.004882692772895098\n",
      "Epoch 264/300\n",
      "Average training loss: 0.049241509897841346\n",
      "Average test loss: 0.004634863167173333\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04995669474535518\n",
      "Average test loss: 0.004924005948834949\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04941955352160666\n",
      "Average test loss: 0.004472719589041339\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04921294786863857\n",
      "Average test loss: 0.004529327837957276\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0493298034105036\n",
      "Average test loss: 0.004641671670807732\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04932418611645699\n",
      "Average test loss: 0.005302655519296725\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04915623535050286\n",
      "Average test loss: 0.005054645885195997\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04915850541326735\n",
      "Average test loss: 0.004650079509864251\n",
      "Epoch 272/300\n",
      "Average training loss: 0.049167492740684084\n",
      "Average test loss: 0.004594790451228618\n",
      "Epoch 273/300\n",
      "Average training loss: 0.051132528020275965\n",
      "Average test loss: 0.004470956474749578\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04899052005675104\n",
      "Average test loss: 0.005079242740447323\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0490427880220943\n",
      "Average test loss: 0.004467017954008447\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04895796741048495\n",
      "Average test loss: 0.0044839367051091455\n",
      "Epoch 277/300\n",
      "Average training loss: 0.048839022523827026\n",
      "Average test loss: 0.005617696824173133\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04885260499517123\n",
      "Average test loss: 0.004525183026989301\n",
      "Epoch 281/300\n",
      "Average training loss: 0.048930626024802526\n",
      "Average test loss: 0.004511125354510215\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04902988896436161\n",
      "Average test loss: 0.004770793973571725\n",
      "Epoch 283/300\n",
      "Average training loss: 0.048962750756078297\n",
      "Average test loss: 0.00450829716026783\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04873902020520634\n",
      "Average test loss: 0.004429533539960782\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0491497159269121\n",
      "Average test loss: 0.004854092264340983\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04908815006746186\n",
      "Average test loss: 0.005830741346710258\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04868087474505107\n",
      "Average test loss: 0.005096057604584429\n",
      "Epoch 288/300\n",
      "Average training loss: 0.052646936578883065\n",
      "Average test loss: 0.004962386458491286\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04887911327348815\n",
      "Average test loss: 0.00442696299445298\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04855742142597834\n",
      "Average test loss: 0.004608633246686723\n",
      "Epoch 291/300\n",
      "Average training loss: 0.048767136656575734\n",
      "Average test loss: 0.004504028263191382\n",
      "Epoch 292/300\n",
      "Average training loss: 0.048640941828489305\n",
      "Average test loss: 0.0044784030649397105\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04881304907136493\n",
      "Average test loss: 0.004640812258753512\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04876962916056315\n",
      "Average test loss: 0.004643313503720694\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0489117185374101\n",
      "Average test loss: 0.008534515289796723\n",
      "Epoch 296/300\n",
      "Average training loss: 0.048452068252695935\n",
      "Average test loss: 0.004492083171382546\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04841617026262813\n",
      "Average test loss: 0.17469120589229795\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04894016870856285\n",
      "Average test loss: 0.004574152745720413\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04867600019110574\n",
      "Average test loss: 0.007186207818902201\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04848149336046643\n",
      "Average test loss: 0.004517186987731192\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.435311433792115\n",
      "Average test loss: 0.039085828665230006\n",
      "Epoch 2/300\n",
      "Average training loss: 2.0540174668629962\n",
      "Average test loss: 0.007154759152068032\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4641383926603528\n",
      "Average test loss: 0.008474669641090764\n",
      "Epoch 4/300\n",
      "Average training loss: 1.166914482858446\n",
      "Average test loss: 0.005419489497111903\n",
      "Epoch 5/300\n",
      "Average training loss: 0.9714668822288514\n",
      "Average test loss: 0.0051754568078451685\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8263483355840047\n",
      "Average test loss: 0.004905175068726142\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7193826264275445\n",
      "Average test loss: 0.004745353486802843\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6334046359062195\n",
      "Average test loss: 0.005311535080687867\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5618847887251113\n",
      "Average test loss: 0.004735940785871612\n",
      "Epoch 10/300\n",
      "Average training loss: 0.49919674038887024\n",
      "Average test loss: 0.004202296811880337\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4464476804468367\n",
      "Average test loss: 0.004072397413767046\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3984354977607727\n",
      "Average test loss: 0.004101981351979905\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3573593597412109\n",
      "Average test loss: 0.003907609018393689\n",
      "Epoch 14/300\n",
      "Average training loss: 0.3199010604222616\n",
      "Average test loss: 0.0038588927880757384\n",
      "Epoch 15/300\n",
      "Average training loss: 0.28722150394651624\n",
      "Average test loss: 0.00388756846015652\n",
      "Epoch 16/300\n",
      "Average training loss: 0.25757539953125846\n",
      "Average test loss: 0.0037808156408783464\n",
      "Epoch 17/300\n",
      "Average training loss: 0.23274166560173035\n",
      "Average test loss: 0.004528599249405994\n",
      "Epoch 18/300\n",
      "Average training loss: 0.21046434953477647\n",
      "Average test loss: 0.0035428346674889325\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1909600813786189\n",
      "Average test loss: 0.003591026576442851\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1746948039929072\n",
      "Average test loss: 0.003434542518729965\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16100690666834513\n",
      "Average test loss: 0.0037998352419171067\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1484383365313212\n",
      "Average test loss: 0.0046766916339596116\n",
      "Epoch 23/300\n",
      "Average training loss: 0.13709791972902086\n",
      "Average test loss: 0.00338030004584127\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12760549981064268\n",
      "Average test loss: 0.0035476823972745075\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11877547228336334\n",
      "Average test loss: 0.08190522457990382\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11161708549658457\n",
      "Average test loss: 0.0032878473653561537\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10434156291352378\n",
      "Average test loss: 0.003731653100086583\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09912116862005657\n",
      "Average test loss: 0.0032571270859075917\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09436568674114015\n",
      "Average test loss: 0.003180478816231092\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08925543034738964\n",
      "Average test loss: 0.003701402413762278\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08564814553658168\n",
      "Average test loss: 0.003195348304592901\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08209972867038515\n",
      "Average test loss: 0.0037987975283629363\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07858395146661334\n",
      "Average test loss: 0.0031446825112733576\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07669957686132856\n",
      "Average test loss: 0.022349777676165104\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07405390425523121\n",
      "Average test loss: 0.021228075539072355\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07164963434802162\n",
      "Average test loss: 0.005316971314657065\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06887192773156696\n",
      "Average test loss: 0.0034611286903835003\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06752588161494996\n",
      "Average test loss: 0.0031055538062420155\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06575282592574755\n",
      "Average test loss: 0.004186602326730887\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06429856492082278\n",
      "Average test loss: 0.023699088846850725\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06271101021104389\n",
      "Average test loss: 0.022772030661503472\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06103516764110989\n",
      "Average test loss: 0.0033749467184146244\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05983971444103453\n",
      "Average test loss: 0.003187379595720106\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05862432226869795\n",
      "Average test loss: 0.003578822017543846\n",
      "Epoch 45/300\n",
      "Average training loss: 0.057820820139514076\n",
      "Average test loss: 0.004100129107634226\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05704526988665263\n",
      "Average test loss: 0.0030998799758445886\n",
      "Epoch 47/300\n",
      "Average training loss: 0.056416739030016795\n",
      "Average test loss: 0.003002534519570569\n",
      "Epoch 48/300\n",
      "Average training loss: 0.055599642389350466\n",
      "Average test loss: 0.003174958981366621\n",
      "Epoch 49/300\n",
      "Average training loss: 0.054981771171092986\n",
      "Average test loss: 0.0030853355444139903\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05453505001465479\n",
      "Average test loss: 0.005744260594248772\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05393960697452227\n",
      "Average test loss: 0.0029834333380891215\n",
      "Epoch 52/300\n",
      "Average training loss: 0.053628220985333125\n",
      "Average test loss: 0.004499831715805663\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05349908232357767\n",
      "Average test loss: 0.0030843270375496812\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05257273436917199\n",
      "Average test loss: 0.003018740389082167\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05235790400372611\n",
      "Average test loss: 0.0031601803828444747\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05194046276476648\n",
      "Average test loss: 0.00719830288986365\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05160757093297111\n",
      "Average test loss: 1616634844.3875556\n",
      "Epoch 58/300\n",
      "Average training loss: 0.051383223550187214\n",
      "Average test loss: 0.0033601877107802365\n",
      "Epoch 59/300\n",
      "Average training loss: 0.051048136070370675\n",
      "Average test loss: 0.0032815681950499613\n",
      "Epoch 60/300\n",
      "Average training loss: 0.050624793340762454\n",
      "Average test loss: 0.0032421651230090196\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05076428685585658\n",
      "Average test loss: 0.00731946314788527\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0502496554205815\n",
      "Average test loss: 0.00299292072136369\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05002023224367036\n",
      "Average test loss: 0.027499847539390127\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04977903727690379\n",
      "Average test loss: 0.02739655294145147\n",
      "Epoch 65/300\n",
      "Average training loss: 0.049632953448428045\n",
      "Average test loss: 0.44684355788926283\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04920363239447276\n",
      "Average test loss: 0.06742355832436846\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04923742448952463\n",
      "Average test loss: 0.006812049320174588\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04872221708463298\n",
      "Average test loss: 0.0034281957948373424\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04864592290918032\n",
      "Average test loss: 32462.87361087907\n",
      "Epoch 70/300\n",
      "Average training loss: 0.048506391021940445\n",
      "Average test loss: 0.00352486970813738\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04835737429393663\n",
      "Average test loss: 0.007976827334198687\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04815041879481739\n",
      "Average test loss: 0.00435320792057448\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04800384884079297\n",
      "Average test loss: 0.05051558330986235\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04772542096177737\n",
      "Average test loss: 0.0033041167255077096\n",
      "Epoch 75/300\n",
      "Average training loss: 0.047362982802920874\n",
      "Average test loss: 0.004719633832159969\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04755116259389453\n",
      "Average test loss: 0.0029778265725407334\n",
      "Epoch 77/300\n",
      "Average training loss: 0.047165176977713906\n",
      "Average test loss: 0.0031583272454639277\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04709537695844968\n",
      "Average test loss: 0.23981046521456706\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04706452797849973\n",
      "Average test loss: 0.0031942845431880817\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04669262262516551\n",
      "Average test loss: 0.003075144607366787\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04655963655643993\n",
      "Average test loss: 0.003734406174470981\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05754105030828052\n",
      "Average test loss: 0.003626122019978033\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05128801324301296\n",
      "Average test loss: 0.0030539508455743394\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04808331778645515\n",
      "Average test loss: 0.0033967776702096067\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04689040487673547\n",
      "Average test loss: 0.0032317661678211555\n",
      "Epoch 86/300\n",
      "Average training loss: 0.046383390618695154\n",
      "Average test loss: 0.0030931882026294867\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04610716456174851\n",
      "Average test loss: 0.0030669270002593595\n",
      "Epoch 88/300\n",
      "Average training loss: 0.045924072434504824\n",
      "Average test loss: 0.0030135499791552624\n",
      "Epoch 89/300\n",
      "Average training loss: 0.045839627580510245\n",
      "Average test loss: 0.0030926292604870264\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04573837295505736\n",
      "Average test loss: 0.0034181575290858747\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04576630045308007\n",
      "Average test loss: 0.14904462192248966\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04564010290139251\n",
      "Average test loss: 0.008685385996061895\n",
      "Epoch 93/300\n",
      "Average training loss: 0.045417798257536356\n",
      "Average test loss: 0.004341952397591538\n",
      "Epoch 94/300\n",
      "Average training loss: 0.045319511542717614\n",
      "Average test loss: 0.004002846082051595\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04521702643235524\n",
      "Average test loss: 0.003415400061963333\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04517964972058932\n",
      "Average test loss: 0.003390537926099367\n",
      "Epoch 97/300\n",
      "Average training loss: 0.045553255165616674\n",
      "Average test loss: 0.003586659332944287\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04476137182116508\n",
      "Average test loss: 0.0032399983217732773\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04466494375136164\n",
      "Average test loss: 1.1179500164538623\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0445668063627349\n",
      "Average test loss: 0.016076764067014058\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04458067878749635\n",
      "Average test loss: 0.0032982310774839585\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04442313480873902\n",
      "Average test loss: 0.004515375617581109\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0443488763405217\n",
      "Average test loss: 0.0032091449753691753\n",
      "Epoch 104/300\n",
      "Average training loss: 0.044197550710704595\n",
      "Average test loss: 0.005032612581633859\n",
      "Epoch 105/300\n",
      "Average training loss: 0.044203080783287683\n",
      "Average test loss: 0.006507980729970667\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04398707432548205\n",
      "Average test loss: 0.0036536242825289567\n",
      "Epoch 107/300\n",
      "Average training loss: 0.044089186671707366\n",
      "Average test loss: 0.0031702189799398183\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04376282207171122\n",
      "Average test loss: 0.0031048225628005134\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04370769686500232\n",
      "Average test loss: 6.720667396866613\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04430982215868102\n",
      "Average test loss: 0.00344109409281777\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04348505504263772\n",
      "Average test loss: 0.004881227795448568\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04339129586021105\n",
      "Average test loss: 7.896945928298558\n",
      "Epoch 113/300\n",
      "Average training loss: 0.043508902466959425\n",
      "Average test loss: 0.0050413352195173506\n",
      "Epoch 114/300\n",
      "Average training loss: 0.043323964685201645\n",
      "Average test loss: 0.0031569414051870505\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04329382802380456\n",
      "Average test loss: 0.0031299281896402437\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04326821979549196\n",
      "Average test loss: 0.003493113252437777\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0430790606505341\n",
      "Average test loss: 0.0032253923774179486\n",
      "Epoch 118/300\n",
      "Average training loss: 0.043119904375738566\n",
      "Average test loss: 0.0032741531955285206\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04290301777919134\n",
      "Average test loss: 0.0033351624545951684\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04295949556430181\n",
      "Average test loss: 0.0031987629152006574\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04288494109610717\n",
      "Average test loss: 0.0034983477166129484\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04272683221764035\n",
      "Average test loss: 0.0030909464407919184\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04270232245988316\n",
      "Average test loss: 0.003167851513872544\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04265683338046074\n",
      "Average test loss: 0.003182786099612713\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0424173054628902\n",
      "Average test loss: 0.0032792898315108484\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04258702895707554\n",
      "Average test loss: 0.0033444793398181598\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04231636263595687\n",
      "Average test loss: 0.0032962564010587\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04253723437918557\n",
      "Average test loss: 0.003172988199111488\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04234554776880476\n",
      "Average test loss: 0.0031448412756952975\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04220301769839393\n",
      "Average test loss: 0.00321932890618013\n",
      "Epoch 131/300\n",
      "Average training loss: 0.042262913955582515\n",
      "Average test loss: 0.0033742003598146966\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04210344894064797\n",
      "Average test loss: 0.013263613164838817\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04196047139830059\n",
      "Average test loss: 0.0032053617741912605\n",
      "Epoch 134/300\n",
      "Average training loss: 0.042178228951162765\n",
      "Average test loss: 0.0033473109987874825\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0419015503625075\n",
      "Average test loss: 0.003210837282654312\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04201490331027243\n",
      "Average test loss: 0.003416004157728619\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04193097326159477\n",
      "Average test loss: 0.0033710227101627324\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04172180543674363\n",
      "Average test loss: 0.003181371766452988\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04173899327384101\n",
      "Average test loss: 0.00468943783454597\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04163747616608938\n",
      "Average test loss: 0.004076885084104207\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04170981079671118\n",
      "Average test loss: 0.003158433344008194\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04172113161947992\n",
      "Average test loss: 0.004433500015901194\n",
      "Epoch 143/300\n",
      "Average training loss: 0.041468018256955674\n",
      "Average test loss: 0.003247960469375054\n",
      "Epoch 144/300\n",
      "Average training loss: 0.041434011376566354\n",
      "Average test loss: 0.003247719258276953\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0415243485669295\n",
      "Average test loss: 0.004487917175102565\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04152889068259133\n",
      "Average test loss: 0.1344470402225852\n",
      "Epoch 147/300\n",
      "Average training loss: 0.041362412634823056\n",
      "Average test loss: 0.006648453348212772\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04125134418076939\n",
      "Average test loss: 0.0065076998414264784\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04127438357141283\n",
      "Average test loss: 0.003364405330063568\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04113803530401654\n",
      "Average test loss: 0.0039864294578631715\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04123370029528936\n",
      "Average test loss: 0.004894346024013228\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04123600564069218\n",
      "Average test loss: 0.007596504763182667\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04096550354361534\n",
      "Average test loss: 0.006116517579803864\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04153389943970574\n",
      "Average test loss: 0.0035892485483653017\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04097789469692442\n",
      "Average test loss: 0.003339876086140672\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04090712129076322\n",
      "Average test loss: 0.003332508226442668\n",
      "Epoch 157/300\n",
      "Average training loss: 0.040858239001697966\n",
      "Average test loss: 0.0033362858605881533\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04094837017191781\n",
      "Average test loss: 0.003353424572282367\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04087273075183233\n",
      "Average test loss: 0.0032948193203450903\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04095314228865835\n",
      "Average test loss: 0.003299167420508133\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04075668259130584\n",
      "Average test loss: 0.005882692511296935\n",
      "Epoch 162/300\n",
      "Average training loss: 0.040932117281688586\n",
      "Average test loss: 0.0032444014300902686\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04071323960688379\n",
      "Average test loss: 0.003247517457231879\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04074731526772181\n",
      "Average test loss: 0.005660068344324827\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04107870361374484\n",
      "Average test loss: 0.0032709248463312786\n",
      "Epoch 166/300\n",
      "Average training loss: 0.040421719243129095\n",
      "Average test loss: 0.003357548746590813\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04040935810407003\n",
      "Average test loss: 0.0042776924924304086\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04054534516400761\n",
      "Average test loss: 0.0033374974546540115\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04058781002627479\n",
      "Average test loss: 0.0033568299642453593\n",
      "Epoch 170/300\n",
      "Average training loss: 0.040432750089301\n",
      "Average test loss: 0.0033609212490005626\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04032933414313528\n",
      "Average test loss: 0.003280776993681987\n",
      "Epoch 172/300\n",
      "Average training loss: 0.040401213486989336\n",
      "Average test loss: 0.003688614897429943\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04042664061817858\n",
      "Average test loss: 0.007276770687765545\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04207861478792296\n",
      "Average test loss: 0.003246383021896084\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04039808316032092\n",
      "Average test loss: 0.003284425729057855\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0400875573919879\n",
      "Average test loss: 0.003300613649189472\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04012414388524162\n",
      "Average test loss: 0.018342992764380242\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0401917008890046\n",
      "Average test loss: 0.013018156272255712\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04028264274862078\n",
      "Average test loss: 0.003332849499045147\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04009007015493181\n",
      "Average test loss: 0.004942184583801362\n",
      "Epoch 181/300\n",
      "Average training loss: 0.040190149986081655\n",
      "Average test loss: 0.0045298617507020635\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0400262210485008\n",
      "Average test loss: 0.003346309646136231\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04004007334510485\n",
      "Average test loss: 0.003342008463624451\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04021654428044955\n",
      "Average test loss: 0.0033001920951323377\n",
      "Epoch 185/300\n",
      "Average training loss: 0.040270889421304065\n",
      "Average test loss: 0.0037507161924408543\n",
      "Epoch 186/300\n",
      "Average training loss: 0.039862111651235155\n",
      "Average test loss: 0.0034335752282705573\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03989273258878125\n",
      "Average test loss: 0.0034190567961583534\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05107576990127564\n",
      "Average test loss: 50095.84350748698\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04801792485846414\n",
      "Average test loss: 0.003185667488310072\n",
      "Epoch 190/300\n",
      "Average training loss: 0.042389561576975716\n",
      "Average test loss: 0.003252297938697868\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04073781548606025\n",
      "Average test loss: 0.005541707180647387\n",
      "Epoch 192/300\n",
      "Average training loss: 0.040118559939993755\n",
      "Average test loss: 0.003396822810379995\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03993718188007673\n",
      "Average test loss: 0.0032478854436841275\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03983051860663626\n",
      "Average test loss: 0.004509525724583202\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03999589059419102\n",
      "Average test loss: 0.003267211506764094\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0397875310877959\n",
      "Average test loss: 0.0032765445535381634\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04015885793500477\n",
      "Average test loss: 0.0035927853447695573\n",
      "Epoch 198/300\n",
      "Average training loss: 0.039806443876690335\n",
      "Average test loss: 0.0033580720803389947\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0397343187365267\n",
      "Average test loss: 0.0039057621268762483\n",
      "Epoch 200/300\n",
      "Average training loss: 0.039883175879716876\n",
      "Average test loss: 0.003353034251059095\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03964337543315358\n",
      "Average test loss: 0.003435886041364736\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03973521052797636\n",
      "Average test loss: 0.005114767312589619\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03969546811117066\n",
      "Average test loss: 0.003437209704166485\n",
      "Epoch 204/300\n",
      "Average training loss: 0.039591950204637315\n",
      "Average test loss: 0.00351810997703837\n",
      "Epoch 205/300\n",
      "Average training loss: 0.039561188510722586\n",
      "Average test loss: 0.0033859011868221894\n",
      "Epoch 206/300\n",
      "Average training loss: 0.039724337577819825\n",
      "Average test loss: 0.0037157958824601436\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03950688018401464\n",
      "Average test loss: 0.003376563556699289\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03940989558233155\n",
      "Average test loss: 0.003387255842073096\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03946670311027103\n",
      "Average test loss: 0.003443653562830554\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03962469895680745\n",
      "Average test loss: 0.009368179393311341\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03934227426846822\n",
      "Average test loss: 0.0034363607195102507\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03932747099796931\n",
      "Average test loss: 0.0034486176007323794\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03931802695658472\n",
      "Average test loss: 0.003499374562667476\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03928581066595183\n",
      "Average test loss: 0.0033418401608036625\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03917036671770944\n",
      "Average test loss: 0.005743770247118341\n",
      "Epoch 216/300\n",
      "Average training loss: 0.039243067814244165\n",
      "Average test loss: 0.0033203401561412546\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03939243460032675\n",
      "Average test loss: 0.0033251279106156694\n",
      "Epoch 218/300\n",
      "Average training loss: 0.039149516221549774\n",
      "Average test loss: 0.003519861031530632\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03909407063325246\n",
      "Average test loss: 0.0035359884363909562\n",
      "Epoch 220/300\n",
      "Average training loss: 0.039269919147094094\n",
      "Average test loss: 0.003578750540398889\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0391081597507\n",
      "Average test loss: 0.004015032705850899\n",
      "Epoch 222/300\n",
      "Average training loss: 0.039240494690007635\n",
      "Average test loss: 0.0034135316212972003\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03920276434553994\n",
      "Average test loss: 0.0033497597322695786\n",
      "Epoch 224/300\n",
      "Average training loss: 0.038930948287248614\n",
      "Average test loss: 0.003379484705833925\n",
      "Epoch 225/300\n",
      "Average training loss: 0.039149963395463096\n",
      "Average test loss: 0.0034660730941428078\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03892389678292804\n",
      "Average test loss: 0.0037898467816412447\n",
      "Epoch 227/300\n",
      "Average training loss: 0.039004691468344795\n",
      "Average test loss: 0.0034346521099408466\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03926436099410057\n",
      "Average test loss: 0.0034138220004323455\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03879836401674482\n",
      "Average test loss: 0.0034876468984617126\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03899755643804868\n",
      "Average test loss: 0.004993998747112023\n",
      "Epoch 231/300\n",
      "Average training loss: 0.039089722322093116\n",
      "Average test loss: 0.003397015537238783\n",
      "Epoch 232/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth10/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth10/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
