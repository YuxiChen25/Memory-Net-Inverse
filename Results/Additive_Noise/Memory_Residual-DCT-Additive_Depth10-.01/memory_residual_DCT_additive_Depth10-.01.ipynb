{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.01)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.01)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.01)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.01)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11785265900029077\n",
      "Average test loss: 0.004954437348577712\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02931957848618428\n",
      "Average test loss: 0.004681413814011547\n",
      "Epoch 3/300\n",
      "Average training loss: 0.025220341538389524\n",
      "Average test loss: 0.004560843965659539\n",
      "Epoch 4/300\n",
      "Average training loss: 0.023792867533034747\n",
      "Average test loss: 0.00437182481173012\n",
      "Epoch 5/300\n",
      "Average training loss: 0.023064041174120375\n",
      "Average test loss: 0.004332070234749052\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02259951438672013\n",
      "Average test loss: 0.004276753262099292\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02227611402173837\n",
      "Average test loss: 0.0042546424426966245\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02201301484803359\n",
      "Average test loss: 0.004228586548732387\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021814500571952927\n",
      "Average test loss: 0.004224428777065542\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021633235659864213\n",
      "Average test loss: 0.004189833408842484\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02148360851738188\n",
      "Average test loss: 0.004163645849459701\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02133532760043939\n",
      "Average test loss: 0.004135969253877799\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021211216097076734\n",
      "Average test loss: 0.004138208658744891\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021094990321331553\n",
      "Average test loss: 0.004107813351270226\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02098380653394593\n",
      "Average test loss: 0.004092874414597949\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020855191970864932\n",
      "Average test loss: 0.0042127640955150125\n",
      "Epoch 17/300\n",
      "Average training loss: 0.020756624857584637\n",
      "Average test loss: 0.004089049301006727\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020653936157623926\n",
      "Average test loss: 0.004041928485863738\n",
      "Epoch 19/300\n",
      "Average training loss: 0.020548938773572445\n",
      "Average test loss: 0.0040436462209456495\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020451643716957835\n",
      "Average test loss: 0.004030221491638157\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020370220821764734\n",
      "Average test loss: 0.004019972734360232\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0202817383027739\n",
      "Average test loss: 0.004011964819497532\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02022101336552037\n",
      "Average test loss: 0.004014009126772483\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02013785945210192\n",
      "Average test loss: 0.003980692935486634\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020067532257901297\n",
      "Average test loss: 0.003971436292140021\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02000443187687132\n",
      "Average test loss: 0.003975373274232778\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01995003990166717\n",
      "Average test loss: 0.003961929998050133\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019881217900249694\n",
      "Average test loss: 0.003989725520627366\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01981511549651623\n",
      "Average test loss: 0.004001255037883918\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01976956663529078\n",
      "Average test loss: 0.003949526016910871\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01971404866543081\n",
      "Average test loss: 0.003935858018903269\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01964727083179686\n",
      "Average test loss: 0.0039428847163087795\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01961070537732707\n",
      "Average test loss: 0.0039508344063328375\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019558205876913336\n",
      "Average test loss: 0.003951418284947674\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01951533291240533\n",
      "Average test loss: 0.003942755826438466\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01946746618549029\n",
      "Average test loss: 0.003955436480748985\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01941360883994235\n",
      "Average test loss: 0.003968519891301791\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019380678246418634\n",
      "Average test loss: 0.003946378304312627\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019334122570024596\n",
      "Average test loss: 0.00392781927726335\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019285693630576133\n",
      "Average test loss: 0.003931647821226054\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019236903983685705\n",
      "Average test loss: 0.003975333711753289\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01920793396896786\n",
      "Average test loss: 0.003950589549210336\n",
      "Epoch 43/300\n",
      "Average training loss: 0.019164381479223568\n",
      "Average test loss: 0.003926375234292613\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01911126569079028\n",
      "Average test loss: 0.00396359875674049\n",
      "Epoch 45/300\n",
      "Average training loss: 0.019063891329699094\n",
      "Average test loss: 0.003930197863943047\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019044348519709375\n",
      "Average test loss: 0.0039407844557944275\n",
      "Epoch 47/300\n",
      "Average training loss: 0.018983567221297157\n",
      "Average test loss: 0.003982583259128862\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01895153066350354\n",
      "Average test loss: 0.003986248037156959\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01891529406938288\n",
      "Average test loss: 0.0039491995473702746\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01887536694854498\n",
      "Average test loss: 0.003957671005692747\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0188332014547454\n",
      "Average test loss: 0.003944232775519292\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01879177974992328\n",
      "Average test loss: 0.003937275677298506\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018750791819559205\n",
      "Average test loss: 0.0039854211107724245\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018707984765370687\n",
      "Average test loss: 0.003960434464530813\n",
      "Epoch 55/300\n",
      "Average training loss: 0.018661107695764966\n",
      "Average test loss: 0.00395027430065804\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01862942399084568\n",
      "Average test loss: 0.0039670828104847\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018587180162469547\n",
      "Average test loss: 0.003964506210966242\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0185408107754257\n",
      "Average test loss: 0.00396324886340234\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01850324663850996\n",
      "Average test loss: 0.004040822380946742\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018477933627035883\n",
      "Average test loss: 0.003984947811398241\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018424353036615583\n",
      "Average test loss: 0.0040361479673948555\n",
      "Epoch 62/300\n",
      "Average training loss: 0.018403080638911988\n",
      "Average test loss: 0.003994382763695386\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01837843220598168\n",
      "Average test loss: 0.004032448084817992\n",
      "Epoch 64/300\n",
      "Average training loss: 0.018313824519515038\n",
      "Average test loss: 0.004005774285230372\n",
      "Epoch 65/300\n",
      "Average training loss: 0.018283077910542487\n",
      "Average test loss: 0.003973143989841143\n",
      "Epoch 66/300\n",
      "Average training loss: 0.018246659520599576\n",
      "Average test loss: 0.003993246386862464\n",
      "Epoch 67/300\n",
      "Average training loss: 0.018201106449796094\n",
      "Average test loss: 0.004071018024451203\n",
      "Epoch 68/300\n",
      "Average training loss: 0.018189457184738585\n",
      "Average test loss: 0.0040088532235887315\n",
      "Epoch 69/300\n",
      "Average training loss: 0.018139060659127102\n",
      "Average test loss: 0.004155521527346637\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01809023256103198\n",
      "Average test loss: 0.00400637468861209\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0180896022932397\n",
      "Average test loss: 0.004002779674198893\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01803834147006273\n",
      "Average test loss: 0.004020194048268927\n",
      "Epoch 73/300\n",
      "Average training loss: 0.018009508222341538\n",
      "Average test loss: 0.004041123939471113\n",
      "Epoch 74/300\n",
      "Average training loss: 0.017985704142186375\n",
      "Average test loss: 0.0040690167045427696\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01792686136232482\n",
      "Average test loss: 0.004000198050919506\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017900785555442174\n",
      "Average test loss: 0.004019314279572831\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017916194691426225\n",
      "Average test loss: 0.00413588408463531\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017841313127014372\n",
      "Average test loss: 0.004111012938121955\n",
      "Epoch 79/300\n",
      "Average training loss: 0.017799201839500005\n",
      "Average test loss: 0.0040492372165123625\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01777150850329134\n",
      "Average test loss: 0.004042697214417987\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017750840408106646\n",
      "Average test loss: 0.00407078960372342\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017722112400664222\n",
      "Average test loss: 0.004028611962580019\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01770961488617791\n",
      "Average test loss: 0.004028900542192989\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017665410979754395\n",
      "Average test loss: 0.004114798051201635\n",
      "Epoch 85/300\n",
      "Average training loss: 0.017637509215209218\n",
      "Average test loss: 0.004046139067659775\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01762155814303292\n",
      "Average test loss: 0.003995798363246852\n",
      "Epoch 87/300\n",
      "Average training loss: 0.017573736018604704\n",
      "Average test loss: 0.004053229983896017\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017553250899745357\n",
      "Average test loss: 0.0041194382338888115\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017522138219740656\n",
      "Average test loss: 0.004015760659343667\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017516201578080655\n",
      "Average test loss: 0.004116786113422778\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0174740582389964\n",
      "Average test loss: 0.0041472826223406525\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01743604351580143\n",
      "Average test loss: 0.00406973999655909\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017414720616406865\n",
      "Average test loss: 0.004062202851391501\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017393613999916447\n",
      "Average test loss: 0.0041354077741917634\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017380971862210166\n",
      "Average test loss: 0.004036348518398073\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017354829563862747\n",
      "Average test loss: 0.004063160886781083\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017317979388766818\n",
      "Average test loss: 0.0040712196319881416\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01729893626852168\n",
      "Average test loss: 0.004079695205307669\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017283244124717184\n",
      "Average test loss: 0.004096171689530213\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01725001983344555\n",
      "Average test loss: 0.004162757678992219\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01722817004058096\n",
      "Average test loss: 0.004122309384246667\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017218614598115287\n",
      "Average test loss: 0.004128243199032214\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01718009411295255\n",
      "Average test loss: 0.004134300961883532\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017157601899570888\n",
      "Average test loss: 0.004173359427601099\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017143112630479865\n",
      "Average test loss: 0.0041470960399342905\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01711838362779882\n",
      "Average test loss: 0.00418546232001649\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017104310321311156\n",
      "Average test loss: 0.004182612845881117\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01709294353591071\n",
      "Average test loss: 0.004108870437989632\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017058338251378802\n",
      "Average test loss: 0.004096647276025679\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01704397034479512\n",
      "Average test loss: 0.004226177418811454\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017005120876762602\n",
      "Average test loss: 0.004292745678789085\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01699567587673664\n",
      "Average test loss: 0.004060226829515563\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016973580587241386\n",
      "Average test loss: 0.004213169056922197\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016945890153447787\n",
      "Average test loss: 0.004069471132010221\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01694447525176737\n",
      "Average test loss: 0.004254792915243242\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016923014966977967\n",
      "Average test loss: 0.004207526529414786\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01690522305501832\n",
      "Average test loss: 0.0041755580881403555\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016884462397959498\n",
      "Average test loss: 0.004114117936955558\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01685512499097321\n",
      "Average test loss: 0.0041175065468996765\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016857457641926078\n",
      "Average test loss: 0.004220590468496085\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01682882984644837\n",
      "Average test loss: 0.004191612401770221\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01679584850867589\n",
      "Average test loss: 0.004289682556357649\n",
      "Epoch 123/300\n",
      "Average training loss: 0.01677828170027998\n",
      "Average test loss: 0.004200139352844821\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01676577099578248\n",
      "Average test loss: 0.004224519964514507\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016732735317614343\n",
      "Average test loss: 0.004254239254527622\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01672166906379991\n",
      "Average test loss: 0.004215813344551457\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01670387275682555\n",
      "Average test loss: 0.004248396549787786\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016689562890264725\n",
      "Average test loss: 0.004282762470344702\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01667400429149469\n",
      "Average test loss: 0.004151796088036563\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01666211934387684\n",
      "Average test loss: 0.004245153909342157\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0166439246022039\n",
      "Average test loss: 0.004123348207316465\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01664072831140624\n",
      "Average test loss: 0.004170696188592248\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016612453376253444\n",
      "Average test loss: 0.004261880290177133\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016607171373234853\n",
      "Average test loss: 0.004240420014493995\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016590058593286407\n",
      "Average test loss: 0.004222812917911344\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016575034338566993\n",
      "Average test loss: 0.004120369357781278\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01655046783056524\n",
      "Average test loss: 0.004313297400871913\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016548618611362247\n",
      "Average test loss: 0.004181335081863734\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016512174580660132\n",
      "Average test loss: 0.0041446503363549705\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01650509899440739\n",
      "Average test loss: 0.004275895283040073\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01648897962851657\n",
      "Average test loss: 0.004176216025319365\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0164825044075648\n",
      "Average test loss: 0.004184426817215151\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016463082420329254\n",
      "Average test loss: 0.0042246602612237135\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016451726469728683\n",
      "Average test loss: 0.004139600843605068\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016422218535509375\n",
      "Average test loss: 0.004310060888942745\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016449340654744043\n",
      "Average test loss: 0.004236270330225428\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01641106781529056\n",
      "Average test loss: 0.004210610824326674\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016407104811734623\n",
      "Average test loss: 0.004435893327411678\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016393892847829394\n",
      "Average test loss: 0.004245585636546215\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016367334614197412\n",
      "Average test loss: 0.004295480477934082\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016359147269692686\n",
      "Average test loss: 0.004317498305191596\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01635583206348949\n",
      "Average test loss: 0.00420763023963405\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016334470263785785\n",
      "Average test loss: 0.004164446089830663\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016320817223853534\n",
      "Average test loss: 0.00421820637356076\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016336313612759112\n",
      "Average test loss: 0.00420556918076343\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01629050101422601\n",
      "Average test loss: 0.004374785079310338\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016281428926520878\n",
      "Average test loss: 0.004332577454133167\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016275674991309642\n",
      "Average test loss: 0.004278814346426063\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016260979715320797\n",
      "Average test loss: 0.0041749327700171205\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01626108150018586\n",
      "Average test loss: 0.004333692964994245\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016242639491955438\n",
      "Average test loss: 0.004229473393203484\n",
      "Epoch 163/300\n",
      "Average training loss: 0.016238808688190248\n",
      "Average test loss: 0.004277780684745974\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01622389392223623\n",
      "Average test loss: 0.0042566560167405345\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01619810128211975\n",
      "Average test loss: 0.00423960247139136\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01618154803911845\n",
      "Average test loss: 0.004273610423422522\n",
      "Epoch 167/300\n",
      "Average training loss: 0.016189174419475927\n",
      "Average test loss: 0.004248046011974414\n",
      "Epoch 168/300\n",
      "Average training loss: 0.016169414011968506\n",
      "Average test loss: 0.004491446039328973\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016152123185495536\n",
      "Average test loss: 0.004252686427699195\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016152120692034564\n",
      "Average test loss: 0.0042098840522683326\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016091597707735168\n",
      "Average test loss: 0.004418794851957096\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016097378582590156\n",
      "Average test loss: 0.0042237853426486255\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01608281932108932\n",
      "Average test loss: 0.004306123743454615\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016060666464269163\n",
      "Average test loss: 0.0041617476791143415\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01606068121227953\n",
      "Average test loss: 0.004251668413480123\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016051262428363165\n",
      "Average test loss: 0.0042867023303276965\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016041333210964997\n",
      "Average test loss: 0.004268016130766935\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01602544122768773\n",
      "Average test loss: 0.004250498930406239\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016019602861669327\n",
      "Average test loss: 0.004282399244606495\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015996815143360033\n",
      "Average test loss: 0.004220183527717988\n",
      "Epoch 186/300\n",
      "Average training loss: 0.016000450005133948\n",
      "Average test loss: 0.004183315501858791\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01598162129107449\n",
      "Average test loss: 0.004359587411085764\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015977746394773323\n",
      "Average test loss: 0.00432576630761226\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015969508613149327\n",
      "Average test loss: 0.00448538849171665\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01592032840020127\n",
      "Average test loss: 0.004335706494748593\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015923659855292902\n",
      "Average test loss: 0.004234428928130203\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015891568274961576\n",
      "Average test loss: 0.004296402860018942\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015909576836559507\n",
      "Average test loss: 0.004262647176989251\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0158650430838267\n",
      "Average test loss: 0.004395267845752338\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015876717697415086\n",
      "Average test loss: 0.004325846224195427\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015862643987768228\n",
      "Average test loss: 0.004280696274919642\n",
      "Epoch 201/300\n",
      "Average training loss: 0.015856004652877648\n",
      "Average test loss: 0.00438526397695144\n",
      "Epoch 202/300\n",
      "Average training loss: 0.015855951198273235\n",
      "Average test loss: 0.004206359170377255\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01585714572833644\n",
      "Average test loss: 0.004346142351213429\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015831637501716615\n",
      "Average test loss: 0.004354980325533284\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015831601703332532\n",
      "Average test loss: 0.004346399452330338\n",
      "Epoch 206/300\n",
      "Average training loss: 0.015813695672485563\n",
      "Average test loss: 0.004359576802700758\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015808317656318346\n",
      "Average test loss: 0.0042811938257267075\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015795478877094055\n",
      "Average test loss: 0.004363122909433312\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015805014845397737\n",
      "Average test loss: 0.004404556061244673\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01578208304113812\n",
      "Average test loss: 0.004285929437105854\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01577935520807902\n",
      "Average test loss: 0.004304484965486659\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01575732762614886\n",
      "Average test loss: 0.004234326414763927\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0157685745101836\n",
      "Average test loss: 0.004259774720503224\n",
      "Epoch 214/300\n",
      "Average training loss: 0.015741450258427197\n",
      "Average test loss: 0.004396443364520867\n",
      "Epoch 215/300\n",
      "Average training loss: 0.015744394638472135\n",
      "Average test loss: 0.0041921924406455625\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015732329428195954\n",
      "Average test loss: 0.004207558251917362\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015729678331977793\n",
      "Average test loss: 0.0044115837187402775\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015729998893207975\n",
      "Average test loss: 0.004263550730215179\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015712997291651036\n",
      "Average test loss: 0.00448883452018102\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015701197307142947\n",
      "Average test loss: 0.004299197589150734\n",
      "Epoch 221/300\n",
      "Average training loss: 0.015709172676006952\n",
      "Average test loss: 0.004347853975991408\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01570795695318116\n",
      "Average test loss: 0.004187744672513671\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01568622816271252\n",
      "Average test loss: 0.004524703839172919\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01567497466173437\n",
      "Average test loss: 0.004384425196589695\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01567012087835206\n",
      "Average test loss: 0.004321740401287874\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01566061183479097\n",
      "Average test loss: 0.00422445034649637\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015652106324831643\n",
      "Average test loss: 0.0045178723608454065\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015647201099329523\n",
      "Average test loss: 0.004262773331668641\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015611876907447975\n",
      "Average test loss: 0.004478928219733966\n",
      "Epoch 234/300\n",
      "Average training loss: 0.015607906674345334\n",
      "Average test loss: 0.004367087787845068\n",
      "Epoch 235/300\n",
      "Average training loss: 0.015607999313208791\n",
      "Average test loss: 0.004263179081388646\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01559520741717683\n",
      "Average test loss: 0.004231396474772029\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015599425135387314\n",
      "Average test loss: 0.004337918075422446\n",
      "Epoch 238/300\n",
      "Average training loss: 0.015574494894180033\n",
      "Average test loss: 0.004269328460511234\n",
      "Epoch 239/300\n",
      "Average training loss: 0.015582859177556302\n",
      "Average test loss: 0.004339634616962737\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015574306151933141\n",
      "Average test loss: 0.004326586661032504\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015563465458651385\n",
      "Average test loss: 0.004482180808981259\n",
      "Epoch 242/300\n",
      "Average training loss: 0.015557653952803877\n",
      "Average test loss: 0.00437179304783543\n",
      "Epoch 243/300\n",
      "Average training loss: 0.015556090393000179\n",
      "Average test loss: 0.0043486980692380005\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015547221102648312\n",
      "Average test loss: 0.004306562334919969\n",
      "Epoch 245/300\n",
      "Average training loss: 0.015528409228556686\n",
      "Average test loss: 0.004239609195540349\n",
      "Epoch 246/300\n",
      "Average training loss: 0.015537683779166804\n",
      "Average test loss: 0.004306029420346021\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015529564114080536\n",
      "Average test loss: 0.004356888357549906\n",
      "Epoch 248/300\n",
      "Average training loss: 0.015508489060733053\n",
      "Average test loss: 0.004273665446788073\n",
      "Epoch 249/300\n",
      "Average training loss: 0.015520811497337288\n",
      "Average test loss: 0.004326105482048459\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015512581898934312\n",
      "Average test loss: 0.0044129085511797\n",
      "Epoch 251/300\n",
      "Average training loss: 0.015497818287875918\n",
      "Average test loss: 0.004378107646687163\n",
      "Epoch 252/300\n",
      "Average training loss: 0.015512475396196048\n",
      "Average test loss: 0.004326314443515407\n",
      "Epoch 253/300\n",
      "Average training loss: 0.015490102768772178\n",
      "Average test loss: 0.004244817813237508\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015495769664645194\n",
      "Average test loss: 0.00432457500592702\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015474477372235722\n",
      "Average test loss: 0.004369990069833067\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015462353436483276\n",
      "Average test loss: 0.004357475208325519\n",
      "Epoch 257/300\n",
      "Average training loss: 0.015458559925357501\n",
      "Average test loss: 0.0044535097426010506\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015462659096552266\n",
      "Average test loss: 0.004418300284279717\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015455844775670104\n",
      "Average test loss: 0.004372756926135884\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015455508196519481\n",
      "Average test loss: 0.004376720012476047\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015441407357652983\n",
      "Average test loss: 0.004335881632442275\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01544789107888937\n",
      "Average test loss: 0.0044732667124933664\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015429441728525691\n",
      "Average test loss: 0.0043812716754360334\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015446439531114367\n",
      "Average test loss: 0.004541461769077513\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015425598778658444\n",
      "Average test loss: 0.0045694765287141005\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015421121081544293\n",
      "Average test loss: 0.004407274271671971\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015412162822153833\n",
      "Average test loss: 0.004192178999384244\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015399589095678595\n",
      "Average test loss: 0.004309432737529277\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015385917809274462\n",
      "Average test loss: 0.004333291811454627\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015408858853081861\n",
      "Average test loss: 0.004301839996543195\n",
      "Epoch 271/300\n",
      "Average training loss: 0.015381780019236936\n",
      "Average test loss: 0.004268217454560929\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015375881795254018\n",
      "Average test loss: 0.004415474832885795\n",
      "Epoch 273/300\n",
      "Average training loss: 0.015368535223934386\n",
      "Average test loss: 0.004315968226848377\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015368168075879415\n",
      "Average test loss: 0.0043322367904086905\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015344973932537767\n",
      "Average test loss: 0.004266093932920032\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015364915955397818\n",
      "Average test loss: 0.0044274097027050124\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015349839482870367\n",
      "Average test loss: 0.004275935154408216\n",
      "Epoch 278/300\n",
      "Average training loss: 0.015351272753543324\n",
      "Average test loss: 0.004401280741310782\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01534122435665793\n",
      "Average test loss: 0.00429254215500421\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01533779699769285\n",
      "Average test loss: 0.00437771839565701\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015335950076580047\n",
      "Average test loss: 0.004422848906781939\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015341043021115991\n",
      "Average test loss: 0.00436565639389058\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015326336379680368\n",
      "Average test loss: 0.004370987864418162\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015308134628666772\n",
      "Average test loss: 0.0044781169783737924\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015308888868325286\n",
      "Average test loss: 0.004374778589440717\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01530125942081213\n",
      "Average test loss: 0.004358064584227072\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015310272910528713\n",
      "Average test loss: 0.00433705738455885\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015283561830719312\n",
      "Average test loss: 0.004376146026369598\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015287954394188192\n",
      "Average test loss: 0.004428087982452578\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015309255448480446\n",
      "Average test loss: 0.004447701578338941\n",
      "Epoch 291/300\n",
      "Average training loss: 0.015294462415907118\n",
      "Average test loss: 0.004392211825483375\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01527212262319194\n",
      "Average test loss: 0.004524987939331267\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01527728114525477\n",
      "Average test loss: 0.00439550312442912\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015260070367819734\n",
      "Average test loss: 0.0043028778516583975\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01527631235205465\n",
      "Average test loss: 0.004366072543172373\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0152546930188934\n",
      "Average test loss: 0.0043670263443556095\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015254948513375388\n",
      "Average test loss: 0.0042816364636851685\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015264179883731736\n",
      "Average test loss: 0.004372568012111716\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015251378207570977\n",
      "Average test loss: 0.004329601681480805\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01524120729499393\n",
      "Average test loss: 0.004294907840175761\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.12103203705284331\n",
      "Average test loss: 0.004800073224637244\n",
      "Epoch 2/300\n",
      "Average training loss: 0.027319430463843875\n",
      "Average test loss: 0.00421877937267224\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02273240688939889\n",
      "Average test loss: 0.004090960905783706\n",
      "Epoch 4/300\n",
      "Average training loss: 0.021197215201126204\n",
      "Average test loss: 0.0038379692708452543\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020405191984441545\n",
      "Average test loss: 0.0037767037066320577\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01991024028344287\n",
      "Average test loss: 0.0036632080045011306\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01954332528097762\n",
      "Average test loss: 0.0036576272389954993\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019249187351928817\n",
      "Average test loss: 0.0035598861070142854\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01898131074508031\n",
      "Average test loss: 0.0035494328513741495\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01875347984333833\n",
      "Average test loss: 0.0035245421955155004\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01853948535770178\n",
      "Average test loss: 0.0034264893490407204\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01833172815044721\n",
      "Average test loss: 0.0033807559913645187\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01813209820455975\n",
      "Average test loss: 0.0033789722884280814\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01795674487286144\n",
      "Average test loss: 0.003380367111414671\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01776387824614843\n",
      "Average test loss: 0.0033081375325305596\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01758538659579224\n",
      "Average test loss: 0.003264560157226192\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01740918106916878\n",
      "Average test loss: 0.0032607412764595616\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017256043256570895\n",
      "Average test loss: 0.0033671749401837587\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017073321767979197\n",
      "Average test loss: 0.003251051510166791\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01692118077394035\n",
      "Average test loss: 0.0031763031736223232\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01675434113293886\n",
      "Average test loss: 0.0032004641658729977\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01661290704541736\n",
      "Average test loss: 0.003140674701788359\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016466042036811512\n",
      "Average test loss: 0.0031293519275883835\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01633003321207232\n",
      "Average test loss: 0.003106752932485607\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016188444925679103\n",
      "Average test loss: 0.003108388874058922\n",
      "Epoch 26/300\n",
      "Average training loss: 0.016072124251061016\n",
      "Average test loss: 0.003070964315906167\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01593945470535093\n",
      "Average test loss: 0.00308048770038618\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01585613715979788\n",
      "Average test loss: 0.0030423848593814505\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015740109871659014\n",
      "Average test loss: 0.00306028511437277\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015624728814595276\n",
      "Average test loss: 0.0030393225194679367\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015527356708215343\n",
      "Average test loss: 0.003009361823192901\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015446508353783025\n",
      "Average test loss: 0.0030316078749795753\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01534760561502642\n",
      "Average test loss: 0.003043689142912626\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015270873242782222\n",
      "Average test loss: 0.0030027215542892615\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015190779443416331\n",
      "Average test loss: 0.0029904779543479284\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015098710184295972\n",
      "Average test loss: 0.0029873046040948892\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015023590072989463\n",
      "Average test loss: 0.003069169894274738\n",
      "Epoch 38/300\n",
      "Average training loss: 0.014935402563048733\n",
      "Average test loss: 0.0030733408720956907\n",
      "Epoch 39/300\n",
      "Average training loss: 0.014871707897219392\n",
      "Average test loss: 0.0029714453601174884\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014780826621585422\n",
      "Average test loss: 0.0030072011467483306\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014724465099473795\n",
      "Average test loss: 0.003086106916061706\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014653860649300946\n",
      "Average test loss: 0.002982465300295088\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014590408119062582\n",
      "Average test loss: 0.0029802187790887225\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014511450047294299\n",
      "Average test loss: 0.002998854332913955\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014443911047445404\n",
      "Average test loss: 0.0030337778909338845\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014380821014444033\n",
      "Average test loss: 0.0032076429826103977\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014325194101366732\n",
      "Average test loss: 0.0029927248688828615\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014262615030838383\n",
      "Average test loss: 0.003076660150455104\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014195829223013587\n",
      "Average test loss: 0.003097191075897879\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01412574591860175\n",
      "Average test loss: 0.0031810492765572337\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014071147060228719\n",
      "Average test loss: 0.0029665510511646667\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014005492431422075\n",
      "Average test loss: 0.00302768793505513\n",
      "Epoch 53/300\n",
      "Average training loss: 0.013962510425183507\n",
      "Average test loss: 0.00307326117840906\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013912099111411306\n",
      "Average test loss: 0.0029795562093042666\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013869341738522053\n",
      "Average test loss: 0.002970242954997553\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013794254699514973\n",
      "Average test loss: 0.0029912955450514954\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013740138814681106\n",
      "Average test loss: 0.0029998891684744093\n",
      "Epoch 58/300\n",
      "Average training loss: 0.013677317218648063\n",
      "Average test loss: 0.003080487657752302\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013673942817582024\n",
      "Average test loss: 0.003062082764175203\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013605090825094117\n",
      "Average test loss: 0.0031023203935474156\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013541439811388651\n",
      "Average test loss: 0.002999801135932406\n",
      "Epoch 62/300\n",
      "Average training loss: 0.013502378824684354\n",
      "Average test loss: 0.003188851878667871\n",
      "Epoch 63/300\n",
      "Average training loss: 0.013465928520593378\n",
      "Average test loss: 0.0030417309306148025\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013429691096974743\n",
      "Average test loss: 0.00302403393925892\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013389954790472985\n",
      "Average test loss: 0.003027314322690169\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013350902662509018\n",
      "Average test loss: 0.0030645138654443954\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013304598802493678\n",
      "Average test loss: 0.0030951575332631666\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013286015081736777\n",
      "Average test loss: 0.0031157812546524738\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013216727879312303\n",
      "Average test loss: 0.00301947598449058\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01318509728461504\n",
      "Average test loss: 0.0031727567234387\n",
      "Epoch 71/300\n",
      "Average training loss: 0.013170238408777448\n",
      "Average test loss: 0.003073706936505106\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013120014266835319\n",
      "Average test loss: 0.0032624993856168456\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013098141680161159\n",
      "Average test loss: 0.0033491571673916446\n",
      "Epoch 74/300\n",
      "Average training loss: 0.013058452550735738\n",
      "Average test loss: 0.0030494770805040996\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01303237990124358\n",
      "Average test loss: 0.003125849456112418\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012992108037074407\n",
      "Average test loss: 0.0030968993993269072\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012958896450698375\n",
      "Average test loss: 0.003143728485123979\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01290964085691505\n",
      "Average test loss: 0.0031512807495892047\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012902352832257748\n",
      "Average test loss: 0.003223518433049321\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012899872253338496\n",
      "Average test loss: 0.003080248372215364\n",
      "Epoch 81/300\n",
      "Average training loss: 0.012850100404686399\n",
      "Average test loss: 0.0032322389293048116\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012806576435764631\n",
      "Average test loss: 0.0032080413442518974\n",
      "Epoch 83/300\n",
      "Average training loss: 0.012782478897107973\n",
      "Average test loss: 0.0030486350216799313\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012781105822987027\n",
      "Average test loss: 0.003238264527792732\n",
      "Epoch 85/300\n",
      "Average training loss: 0.012728221881720755\n",
      "Average test loss: 0.003217903031450179\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012755493557287587\n",
      "Average test loss: 0.0030682146487136684\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012702664251956675\n",
      "Average test loss: 0.0031024511095343365\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012648624618020323\n",
      "Average test loss: 0.0030555253751162028\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012638571872479385\n",
      "Average test loss: 0.0033382532820105553\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012618919431335395\n",
      "Average test loss: 0.003349039858827988\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01262032791889376\n",
      "Average test loss: 0.0031910071799324618\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012569063949088256\n",
      "Average test loss: 0.0031360950014657444\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01254266190694438\n",
      "Average test loss: 0.00309698218645321\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012519407737172313\n",
      "Average test loss: 0.003433707955396838\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012509396980206172\n",
      "Average test loss: 0.0031447046955840456\n",
      "Epoch 96/300\n",
      "Average training loss: 0.012486691295272774\n",
      "Average test loss: 0.0032640722658899097\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012461779490113258\n",
      "Average test loss: 0.003214078021546205\n",
      "Epoch 98/300\n",
      "Average training loss: 0.012462747821377382\n",
      "Average test loss: 0.0030684128577510516\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01244085926645332\n",
      "Average test loss: 0.0030907542473740048\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012403341306580438\n",
      "Average test loss: 0.0031895019163688024\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012373081832296318\n",
      "Average test loss: 0.0031878150426265265\n",
      "Epoch 102/300\n",
      "Average training loss: 0.012350567596654097\n",
      "Average test loss: 0.003133947458325161\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012359037370317513\n",
      "Average test loss: 0.003212921836103002\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01231797561960088\n",
      "Average test loss: 0.003333920743316412\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012313676029443742\n",
      "Average test loss: 0.0031760608082016307\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012290832308431466\n",
      "Average test loss: 0.0031139697869204813\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012298017264240318\n",
      "Average test loss: 0.0031695161109997166\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01227873350514306\n",
      "Average test loss: 0.0031609579130179354\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012246020096871589\n",
      "Average test loss: 0.00320848017392887\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012205629800756772\n",
      "Average test loss: 0.003197373163782888\n",
      "Epoch 111/300\n",
      "Average training loss: 0.012209386915796334\n",
      "Average test loss: 0.0032005080290966564\n",
      "Epoch 112/300\n",
      "Average training loss: 0.012191244098875257\n",
      "Average test loss: 0.0032613512331412897\n",
      "Epoch 113/300\n",
      "Average training loss: 0.012185770607656903\n",
      "Average test loss: 0.0031850888673216103\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01216360343247652\n",
      "Average test loss: 0.0032381301507767705\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012154509493046337\n",
      "Average test loss: 0.0033510356272260346\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012134256714748012\n",
      "Average test loss: 0.003285699447823895\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012120878984530768\n",
      "Average test loss: 0.0032544017576922974\n",
      "Epoch 118/300\n",
      "Average training loss: 0.012107169657945633\n",
      "Average test loss: 0.0032306290645566253\n",
      "Epoch 119/300\n",
      "Average training loss: 0.012106762695643637\n",
      "Average test loss: 0.0033045761483824917\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012062424779766136\n",
      "Average test loss: 0.0031685884861896436\n",
      "Epoch 121/300\n",
      "Average training loss: 0.012055502499971124\n",
      "Average test loss: 0.003172147635784414\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012049927719765239\n",
      "Average test loss: 0.0032191701063679326\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012029936578538682\n",
      "Average test loss: 0.0032046911811663046\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012025255866348743\n",
      "Average test loss: 0.003210089581501153\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011989257786009047\n",
      "Average test loss: 0.003256380637900697\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011989362769656712\n",
      "Average test loss: 0.0031956182892123857\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012005258535345396\n",
      "Average test loss: 0.0032458432937661806\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01195386980391211\n",
      "Average test loss: 0.0031648673684232763\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011958570490280786\n",
      "Average test loss: 0.0032172612367818753\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011936081326670117\n",
      "Average test loss: 0.003213942227057285\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011929774715668626\n",
      "Average test loss: 0.0033866122174594135\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011910418066713545\n",
      "Average test loss: 0.0032446849625557663\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011924652306569947\n",
      "Average test loss: 0.0032208910128101705\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01188358409040504\n",
      "Average test loss: 0.0033195446208119394\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011874531650708781\n",
      "Average test loss: 0.0033388637805150615\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011896325803465313\n",
      "Average test loss: 0.0031544395555845564\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011871449836426311\n",
      "Average test loss: 0.003147591310656733\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011865213378436035\n",
      "Average test loss: 0.0031927669905126095\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011855994627707535\n",
      "Average test loss: 0.0033068375755101443\n",
      "Epoch 140/300\n",
      "Average training loss: 0.011814583285815186\n",
      "Average test loss: 0.0035363402420447933\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011822366796020004\n",
      "Average test loss: 0.003238953164468209\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011826532817549175\n",
      "Average test loss: 0.003236194072291255\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011811287979284922\n",
      "Average test loss: 0.0032231127543167937\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011777857038709852\n",
      "Average test loss: 0.003235840945608086\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01175681167261468\n",
      "Average test loss: 0.003295961041417387\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011751461270782682\n",
      "Average test loss: 0.003245546513133579\n",
      "Epoch 147/300\n",
      "Average training loss: 0.011749437765942679\n",
      "Average test loss: 0.0032082980233762\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01174397838777966\n",
      "Average test loss: 0.003285169765353203\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011751978137012986\n",
      "Average test loss: 0.003265506943480836\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011722135476768017\n",
      "Average test loss: 0.0032072231243881915\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011724493294126458\n",
      "Average test loss: 0.0034041445712662405\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011730183906025357\n",
      "Average test loss: 0.003374058808717463\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011683738140596284\n",
      "Average test loss: 0.003303430411964655\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011674598511722353\n",
      "Average test loss: 0.003321028740041786\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01168704955610964\n",
      "Average test loss: 0.0031577467028465534\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011689928588353925\n",
      "Average test loss: 0.003298996903002262\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011651552437080277\n",
      "Average test loss: 0.003251682407119208\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01166385010133187\n",
      "Average test loss: 0.003293016954842541\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011643538840942913\n",
      "Average test loss: 0.0032867693660987747\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011636351577937604\n",
      "Average test loss: 0.003280781074323588\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011606510493490431\n",
      "Average test loss: 0.0032554267843564353\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011615616981354025\n",
      "Average test loss: 0.003249788053954641\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01162188623431656\n",
      "Average test loss: 0.003246596777604686\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011591915700170728\n",
      "Average test loss: 0.0033068741595165597\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01158773551798529\n",
      "Average test loss: 0.003237989713334375\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011579660034841961\n",
      "Average test loss: 0.003278480961297949\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011564120828277536\n",
      "Average test loss: 0.003347645092962517\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011583518953786957\n",
      "Average test loss: 0.003407110319265889\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011547877993848588\n",
      "Average test loss: 0.0033138868887391355\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01155254528340366\n",
      "Average test loss: 0.0032084630218644937\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011546286449664168\n",
      "Average test loss: 0.003367348531468047\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011515803077982532\n",
      "Average test loss: 0.0033932458615551393\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01154544542522894\n",
      "Average test loss: 0.0035320843623744117\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011526427217655712\n",
      "Average test loss: 0.003227767667836613\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011511656103862657\n",
      "Average test loss: 0.0032697150111198423\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011502063703205851\n",
      "Average test loss: 0.0032883167564868926\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011480046618315908\n",
      "Average test loss: 0.0033943200825403133\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011484248384833336\n",
      "Average test loss: 0.003306015804513461\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01151399690657854\n",
      "Average test loss: 0.003412704036053684\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011477814581659106\n",
      "Average test loss: 0.0032525871319489348\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01145741714619928\n",
      "Average test loss: 0.003305280624785357\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011465000124027331\n",
      "Average test loss: 0.003390165861075123\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011454615674912929\n",
      "Average test loss: 0.003351300165057182\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01143623753802644\n",
      "Average test loss: 0.0033971858736541534\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011462778902716107\n",
      "Average test loss: 0.003324309680610895\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011436259261435933\n",
      "Average test loss: 0.0032119689848687915\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011449010113047229\n",
      "Average test loss: 0.003303844386090835\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011423401760558288\n",
      "Average test loss: 0.0035949053975443046\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011412473142147064\n",
      "Average test loss: 0.003222602790014611\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011402292354239358\n",
      "Average test loss: 0.003435948789326681\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011400079275998804\n",
      "Average test loss: 0.003320038711445199\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011390353249178992\n",
      "Average test loss: 0.0033759349739799897\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011365929108526972\n",
      "Average test loss: 0.0034583265516493055\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011398566681477759\n",
      "Average test loss: 0.00333519647974107\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0113939987288581\n",
      "Average test loss: 0.0033296440825280214\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011356914619604747\n",
      "Average test loss: 0.0033167074678672683\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011373494066298008\n",
      "Average test loss: 0.003193750626511044\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01135703826447328\n",
      "Average test loss: 0.003331296222905318\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011350041583180428\n",
      "Average test loss: 0.003242093694396317\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011332565239734119\n",
      "Average test loss: 0.0035389275021023223\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011352226263946956\n",
      "Average test loss: 0.0032812802454249726\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01133644231946932\n",
      "Average test loss: 0.0034588962364941835\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011315313337577715\n",
      "Average test loss: 0.0032901805515090626\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011314706629349126\n",
      "Average test loss: 0.0033160246786040565\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011329236729277504\n",
      "Average test loss: 0.003221138326451182\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01131838757213619\n",
      "Average test loss: 0.0033990073922193715\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01130606016682254\n",
      "Average test loss: 0.0033460563094251687\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011304860559602578\n",
      "Average test loss: 0.0033904649201366636\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011296954502662023\n",
      "Average test loss: 0.0033932317714724276\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011293772807551755\n",
      "Average test loss: 0.003243497751239273\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011275320317182275\n",
      "Average test loss: 0.0032996837434669337\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011283694629867872\n",
      "Average test loss: 0.0033606669356425604\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011264822190006574\n",
      "Average test loss: 0.003339917314549287\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011260821269618141\n",
      "Average test loss: 0.0034023471031751898\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011259662121534347\n",
      "Average test loss: 0.003332369611495071\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011251981247630384\n",
      "Average test loss: 0.003394655771139595\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011245598081085417\n",
      "Average test loss: 0.0033658250119123195\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01125001482748323\n",
      "Average test loss: 0.0035944744607226717\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011239815157320764\n",
      "Average test loss: 0.0033342966846086914\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011228006604644987\n",
      "Average test loss: 0.003327875201072958\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011224120350347625\n",
      "Average test loss: 0.003403143857502275\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011214127371708552\n",
      "Average test loss: 0.0032960436658726797\n",
      "Epoch 223/300\n",
      "Average training loss: 0.011225338026053376\n",
      "Average test loss: 0.003221984105391635\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011201648796598117\n",
      "Average test loss: 0.0035927606974211004\n",
      "Epoch 225/300\n",
      "Average training loss: 0.011204879379934734\n",
      "Average test loss: 0.0033919338629994954\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011210077287422286\n",
      "Average test loss: 0.003621925298538473\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011212211841510401\n",
      "Average test loss: 0.0033236084176848334\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011182191104938587\n",
      "Average test loss: 0.0033098836559802293\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011200362590452035\n",
      "Average test loss: 0.003244050458901458\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011176519196066591\n",
      "Average test loss: 0.0035031756108833685\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011197467780775494\n",
      "Average test loss: 0.0035007221903651953\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011180636227958732\n",
      "Average test loss: 0.003304652584095796\n",
      "Epoch 233/300\n",
      "Average training loss: 0.011179092428750462\n",
      "Average test loss: 0.003438000967519151\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011150699715647432\n",
      "Average test loss: 0.0033933589332219628\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011146227753824657\n",
      "Average test loss: 0.0033517154653867087\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01116348165853156\n",
      "Average test loss: 0.0034480724684480164\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011146012768977219\n",
      "Average test loss: 0.0032894355696108605\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01113679262664583\n",
      "Average test loss: 0.0034079138468950986\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011125550922420289\n",
      "Average test loss: 0.0034334461784197223\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011142218948238426\n",
      "Average test loss: 0.0034555082987580035\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011151334908273485\n",
      "Average test loss: 0.0034226994156423543\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011140414175887903\n",
      "Average test loss: 0.003339568086589376\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011126466064817375\n",
      "Average test loss: 0.0034292152867548996\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011099741094642216\n",
      "Average test loss: 0.003473597602504823\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011139922398659918\n",
      "Average test loss: 0.0032588810672362647\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011104308809671138\n",
      "Average test loss: 0.003273746233433485\n",
      "Epoch 247/300\n",
      "Average training loss: 0.011096968457930617\n",
      "Average test loss: 0.003448869624485572\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011092447131872176\n",
      "Average test loss: 0.0033496832804133497\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011100591851605309\n",
      "Average test loss: 0.0035214162026014593\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011093935585684247\n",
      "Average test loss: 0.003479960195099314\n",
      "Epoch 251/300\n",
      "Average training loss: 0.011069503492779202\n",
      "Average test loss: 0.003435313033560912\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0110819434142775\n",
      "Average test loss: 0.0033779066652059554\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011058002075387372\n",
      "Average test loss: 0.0036425224079026115\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011069006878468726\n",
      "Average test loss: 0.0034258807458811334\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011087222109238307\n",
      "Average test loss: 0.003360842100862\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011071296249412828\n",
      "Average test loss: 0.003443621741193864\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01105242267002662\n",
      "Average test loss: 0.0034174554716381762\n",
      "Epoch 258/300\n",
      "Average training loss: 0.011061806783907942\n",
      "Average test loss: 0.003301281192857358\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011055351241595216\n",
      "Average test loss: 0.003460529676328103\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01104790948662493\n",
      "Average test loss: 0.003386643973593083\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011031703101264105\n",
      "Average test loss: 0.0033547596182260248\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011043167888290352\n",
      "Average test loss: 0.0034269808969563907\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0110304672461417\n",
      "Average test loss: 0.003467888673560487\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011061451901992161\n",
      "Average test loss: 0.0034218809482538036\n",
      "Epoch 265/300\n",
      "Average training loss: 0.011043273522622056\n",
      "Average test loss: 0.0034736203402280807\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011043942344685396\n",
      "Average test loss: 0.003512438539105157\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011013943379124006\n",
      "Average test loss: 0.0033881379769494136\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01100695725861523\n",
      "Average test loss: 0.0033642631713300944\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011009857827590572\n",
      "Average test loss: 0.0033658766009741358\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011016690282358063\n",
      "Average test loss: 0.0034219427183270453\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01099874775691165\n",
      "Average test loss: 0.003404404610602392\n",
      "Epoch 272/300\n",
      "Average training loss: 0.011005691783295737\n",
      "Average test loss: 0.003511312726057238\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010991380118661456\n",
      "Average test loss: 0.003314344248837895\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01100132878911164\n",
      "Average test loss: 0.003531418980202741\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01096920917265945\n",
      "Average test loss: 0.003423348585764567\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010993014463947879\n",
      "Average test loss: 0.003471425921966632\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010973490811056561\n",
      "Average test loss: 0.0033571268833345836\n",
      "Epoch 278/300\n",
      "Average training loss: 0.010998606610629293\n",
      "Average test loss: 0.0033209236562252043\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010989990576273865\n",
      "Average test loss: 0.0034727175475822553\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010962732945051458\n",
      "Average test loss: 0.003372894628180398\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010986179169681337\n",
      "Average test loss: 0.003320041925128963\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010973195285432868\n",
      "Average test loss: 0.0033575292281392546\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010961769034465154\n",
      "Average test loss: 0.003351061601191759\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010963320665889316\n",
      "Average test loss: 0.003407115687512689\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010973999589681625\n",
      "Average test loss: 0.003462244837648339\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010952303032908174\n",
      "Average test loss: 0.003297178856614563\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010934482475949659\n",
      "Average test loss: 0.0032856173529807065\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010927668737040626\n",
      "Average test loss: 0.0034568758153667055\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010933064721524716\n",
      "Average test loss: 0.0033093823810211487\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010935403219527668\n",
      "Average test loss: 0.0033328873639305433\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010934431960185368\n",
      "Average test loss: 0.003229340924984879\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010926044180161423\n",
      "Average test loss: 0.003398941116200553\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010932602920466\n",
      "Average test loss: 0.0034984359714306064\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010912774049573474\n",
      "Average test loss: 0.003433276361061467\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010916896410286427\n",
      "Average test loss: 0.0033928958773612977\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010924477484491137\n",
      "Average test loss: 0.003433698543657859\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01090726222925716\n",
      "Average test loss: 0.0034104753072477048\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01092630168547233\n",
      "Average test loss: 0.0033745724716120295\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010922775990433163\n",
      "Average test loss: 0.0035760692175891666\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010918861274090079\n",
      "Average test loss: 0.0034576451145112513\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10530678538646963\n",
      "Average test loss: 0.004673718240732948\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02309319759077496\n",
      "Average test loss: 0.0035913870077994136\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019504587625463803\n",
      "Average test loss: 0.0033400969221774077\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018203647235201466\n",
      "Average test loss: 0.003201054609277182\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017500394373304314\n",
      "Average test loss: 0.0030988273279120523\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017017610487010743\n",
      "Average test loss: 0.0030493381975425613\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01662561565720373\n",
      "Average test loss: 0.0029236032359509006\n",
      "Epoch 8/300\n",
      "Average training loss: 0.016298124093976286\n",
      "Average test loss: 0.0028092038739058705\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01599389051232073\n",
      "Average test loss: 0.002745607941928837\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015716429017484188\n",
      "Average test loss: 0.0027878653893454205\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015457273808618386\n",
      "Average test loss: 0.002666059535410669\n",
      "Epoch 12/300\n",
      "Average training loss: 0.015215513057178921\n",
      "Average test loss: 0.002629791832011607\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014983947837518321\n",
      "Average test loss: 0.0026050072335120703\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014756192137797674\n",
      "Average test loss: 0.0025735427219834594\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014531355910003185\n",
      "Average test loss: 0.0025589961794515452\n",
      "Epoch 16/300\n",
      "Average training loss: 0.014311831850144598\n",
      "Average test loss: 0.0024720395747572185\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0141005104581515\n",
      "Average test loss: 0.0024889689148921107\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013889762232700983\n",
      "Average test loss: 0.0024194539334211084\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013680618837475777\n",
      "Average test loss: 0.0024025397135151757\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013493470773100853\n",
      "Average test loss: 0.0024353176018016205\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013291858535673884\n",
      "Average test loss: 0.002417671515295903\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013088661142521434\n",
      "Average test loss: 0.002453775309233202\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012948069697452916\n",
      "Average test loss: 0.002685839971113536\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012785566329128212\n",
      "Average test loss: 0.0023194119744002818\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012643027399149206\n",
      "Average test loss: 0.0022918834936701588\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012509671015044053\n",
      "Average test loss: 0.002278324180489613\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01239225220017963\n",
      "Average test loss: 0.0022593618348861733\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012266641610198551\n",
      "Average test loss: 0.0022377503510150644\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012167355733613173\n",
      "Average test loss: 0.002227876024010281\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012067403235369258\n",
      "Average test loss: 0.0022099178261641\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011635914733840359\n",
      "Average test loss: 0.002192503126338124\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011552252605557442\n",
      "Average test loss: 0.002197451344794697\n",
      "Epoch 37/300\n",
      "Average training loss: 0.011508628544708093\n",
      "Average test loss: 0.002187893019575212\n",
      "Epoch 38/300\n",
      "Average training loss: 0.011435210230449836\n",
      "Average test loss: 0.002208026548019714\n",
      "Epoch 39/300\n",
      "Average training loss: 0.011343668292793962\n",
      "Average test loss: 0.0021773432593585715\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01127921178109116\n",
      "Average test loss: 0.0022287066291189856\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011221691531439622\n",
      "Average test loss: 0.002226411356383728\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011163187428481049\n",
      "Average test loss: 0.002276942112379604\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01112029991712835\n",
      "Average test loss: 0.00215996318600244\n",
      "Epoch 44/300\n",
      "Average training loss: 0.011053048549840848\n",
      "Average test loss: 0.002175779915932152\n",
      "Epoch 45/300\n",
      "Average training loss: 0.010989308763295413\n",
      "Average test loss: 0.002230973919853568\n",
      "Epoch 46/300\n",
      "Average training loss: 0.010938519693083234\n",
      "Average test loss: 0.002172338381306165\n",
      "Epoch 47/300\n",
      "Average training loss: 0.010877716093427605\n",
      "Average test loss: 0.002180211103003886\n",
      "Epoch 48/300\n",
      "Average training loss: 0.010819580876164966\n",
      "Average test loss: 0.0021798665730489624\n",
      "Epoch 49/300\n",
      "Average training loss: 0.010781269776324431\n",
      "Average test loss: 0.0021848116962032184\n",
      "Epoch 50/300\n",
      "Average training loss: 0.010734549117585023\n",
      "Average test loss: 0.0022793194744735955\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010687098412877984\n",
      "Average test loss: 0.002243573648441169\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010633093324800332\n",
      "Average test loss: 0.002169369725510478\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01058568323817518\n",
      "Average test loss: 0.002243417560847269\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010547440943618615\n",
      "Average test loss: 0.0022032667977942362\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0105076188792785\n",
      "Average test loss: 0.0022344838110730054\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01045803652703762\n",
      "Average test loss: 0.002227886812140544\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010403772781292597\n",
      "Average test loss: 0.0022928474330239828\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01037335112111436\n",
      "Average test loss: 0.002243044694057769\n",
      "Epoch 59/300\n",
      "Average training loss: 0.010324939933088091\n",
      "Average test loss: 0.0021842353088367314\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010308527623199754\n",
      "Average test loss: 0.002264530802869962\n",
      "Epoch 61/300\n",
      "Average training loss: 0.010269201534489791\n",
      "Average test loss: 0.0022086478480034403\n",
      "Epoch 62/300\n",
      "Average training loss: 0.010235705023010572\n",
      "Average test loss: 0.002195812215200729\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010180573505659898\n",
      "Average test loss: 0.002184454141805569\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009996823066638575\n",
      "Average test loss: 0.002205204777005646\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009950105053683121\n",
      "Average test loss: 0.0022695786216192777\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009934072225872012\n",
      "Average test loss: 0.002299138966533873\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009925206413699521\n",
      "Average test loss: 0.002228730622265074\n",
      "Epoch 73/300\n",
      "Average training loss: 0.009870551677627695\n",
      "Average test loss: 0.002288965067929692\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00986268938332796\n",
      "Average test loss: 0.0022777244908114276\n",
      "Epoch 75/300\n",
      "Average training loss: 0.009831852056913906\n",
      "Average test loss: 0.0023528726081260376\n",
      "Epoch 76/300\n",
      "Average training loss: 0.009824575727184613\n",
      "Average test loss: 0.0022566778280047905\n",
      "Epoch 77/300\n",
      "Average training loss: 0.009780518131123649\n",
      "Average test loss: 0.002196903654063741\n",
      "Epoch 78/300\n",
      "Average training loss: 0.009740495519919528\n",
      "Average test loss: 0.0022296628469808235\n",
      "Epoch 79/300\n",
      "Average training loss: 0.009735115077760485\n",
      "Average test loss: 0.0022346911798748707\n",
      "Epoch 80/300\n",
      "Average training loss: 0.009723637573421002\n",
      "Average test loss: 0.002273999711084697\n",
      "Epoch 81/300\n",
      "Average training loss: 0.00968829460359282\n",
      "Average test loss: 0.002357186980545521\n",
      "Epoch 82/300\n",
      "Average training loss: 0.00959760130279594\n",
      "Average test loss: 0.002268097516977125\n",
      "Epoch 86/300\n",
      "Average training loss: 0.009590321590916977\n",
      "Average test loss: 0.002273868082712094\n",
      "Epoch 87/300\n",
      "Average training loss: 0.009573335684008068\n",
      "Average test loss: 0.0022838117815554143\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009555225568099155\n",
      "Average test loss: 0.002249131311559015\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009537252582609653\n",
      "Average test loss: 0.002284041264404853\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009502422959026363\n",
      "Average test loss: 0.002335456874532004\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009486947516600291\n",
      "Average test loss: 0.002238131937156949\n",
      "Epoch 92/300\n",
      "Average training loss: 0.00946643934895595\n",
      "Average test loss: 0.0023754154187109735\n",
      "Epoch 93/300\n",
      "Average training loss: 0.009455492986573113\n",
      "Average test loss: 0.002324748090157906\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009443072339726818\n",
      "Average test loss: 0.0024234770790984234\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009413806634644667\n",
      "Average test loss: 0.0023439392876914806\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009414070235772265\n",
      "Average test loss: 0.002287380226370361\n",
      "Epoch 97/300\n",
      "Average training loss: 0.009403420012030336\n",
      "Average test loss: 0.002301882510383924\n",
      "Epoch 98/300\n",
      "Average training loss: 0.009390805411669943\n",
      "Average test loss: 0.0023792582003192768\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009365024254553848\n",
      "Average test loss: 0.002372287131018109\n",
      "Epoch 100/300\n",
      "Average training loss: 0.009348540429439809\n",
      "Average test loss: 0.002322997074988153\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009325182351387209\n",
      "Average test loss: 0.002310650918322305\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009319422931306892\n",
      "Average test loss: 0.0023010841336929136\n",
      "Epoch 103/300\n",
      "Average training loss: 0.009305580178482665\n",
      "Average test loss: 0.002348951916417314\n",
      "Epoch 104/300\n",
      "Average training loss: 0.009289303749799728\n",
      "Average test loss: 0.0023238670121257504\n",
      "Epoch 105/300\n",
      "Average training loss: 0.009267427364985148\n",
      "Average test loss: 0.0022960114932308595\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009278894192642636\n",
      "Average test loss: 0.0023278535327149763\n",
      "Epoch 107/300\n",
      "Average training loss: 0.009244835565487544\n",
      "Average test loss: 0.0023623830816811987\n",
      "Epoch 108/300\n",
      "Average training loss: 0.009239720248513751\n",
      "Average test loss: 0.002295532075067361\n",
      "Epoch 109/300\n",
      "Average training loss: 0.009224159801999728\n",
      "Average test loss: 0.0023133005854777166\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009214894679271512\n",
      "Average test loss: 0.002324573482076327\n",
      "Epoch 111/300\n",
      "Average training loss: 0.009204820540216234\n",
      "Average test loss: 0.0023746295923160183\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009175134545399083\n",
      "Average test loss: 0.002326128601717452\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009162088567597999\n",
      "Average test loss: 0.002456840773216552\n",
      "Epoch 114/300\n",
      "Average training loss: 0.009184382241633203\n",
      "Average test loss: 0.002377617288173901\n",
      "Epoch 115/300\n",
      "Average training loss: 0.00915511778742075\n",
      "Average test loss: 0.0023703856692752906\n",
      "Epoch 116/300\n",
      "Average training loss: 0.009134463334249126\n",
      "Average test loss: 0.0023129360412971842\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009154043383068508\n",
      "Average test loss: 0.00231589086415867\n",
      "Epoch 118/300\n",
      "Average training loss: 0.009103678272002274\n",
      "Average test loss: 0.002361177637966143\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009104549320207702\n",
      "Average test loss: 0.0023827884716706143\n",
      "Epoch 120/300\n",
      "Average training loss: 0.00908644762966368\n",
      "Average test loss: 0.002322334935888648\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009035033884975644\n",
      "Average test loss: 0.002321404020819399\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009030205049448544\n",
      "Average test loss: 0.002454222644575768\n",
      "Epoch 127/300\n",
      "Average training loss: 0.00902102431241009\n",
      "Average test loss: 0.0023577194857514567\n",
      "Epoch 128/300\n",
      "Average training loss: 0.00902735871159368\n",
      "Average test loss: 0.002337148348490397\n",
      "Epoch 129/300\n",
      "Average training loss: 0.008996847285578648\n",
      "Average test loss: 0.0023520721608979835\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00900316159096029\n",
      "Average test loss: 0.002361102714203298\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008986930796669589\n",
      "Average test loss: 0.0023634425261989237\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008969541627913713\n",
      "Average test loss: 0.0024203302080018653\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008974271388517486\n",
      "Average test loss: 0.0023887938589064613\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008955374074892865\n",
      "Average test loss: 0.0023998641735977597\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008942899112900098\n",
      "Average test loss: 0.0023286331798881292\n",
      "Epoch 136/300\n",
      "Average training loss: 0.00894117788142628\n",
      "Average test loss: 0.00233640795511504\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008936942875799206\n",
      "Average test loss: 0.00241352588766151\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008940610972129637\n",
      "Average test loss: 0.0024016635006086696\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008924032651715808\n",
      "Average test loss: 0.002463291867532664\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008910992078483105\n",
      "Average test loss: 0.002417575167492032\n",
      "Epoch 141/300\n",
      "Average training loss: 0.008921471708350712\n",
      "Average test loss: 0.002438886766632398\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00888045338085956\n",
      "Average test loss: 0.0024405786662052074\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008900145361820856\n",
      "Average test loss: 0.0023379156647457015\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008877726305690076\n",
      "Average test loss: 0.0024145994343691404\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008858613039884302\n",
      "Average test loss: 0.002434851537562079\n",
      "Epoch 146/300\n",
      "Average training loss: 0.00887091248896387\n",
      "Average test loss: 0.0023665598724037407\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008854062222358253\n",
      "Average test loss: 0.0025360460496611065\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008831194252603584\n",
      "Average test loss: 0.0024412486593549452\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008813766879340013\n",
      "Average test loss: 0.0023231353337566057\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008816331047150823\n",
      "Average test loss: 0.0024818805331985154\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008813017084366746\n",
      "Average test loss: 0.0025406259298324583\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008798467156787714\n",
      "Average test loss: 0.00241982897847063\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008807371268669765\n",
      "Average test loss: 0.0023942637424916027\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008771002137826549\n",
      "Average test loss: 0.002403410286953052\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008787066950566238\n",
      "Average test loss: 0.0024447281505498623\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008756330659819974\n",
      "Average test loss: 0.002377581894294255\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008792734891176223\n",
      "Average test loss: 0.00248630200429923\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008753780710200469\n",
      "Average test loss: 0.002583000173792243\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008737633056110805\n",
      "Average test loss: 0.0023539741509076622\n",
      "Epoch 163/300\n",
      "Average training loss: 0.00874466483708885\n",
      "Average test loss: 0.002504050486307177\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008744564717842473\n",
      "Average test loss: 0.0024011933700078063\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008729525946080684\n",
      "Average test loss: 0.002406216863956716\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008728643430603875\n",
      "Average test loss: 0.002439710149541497\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008726708590984344\n",
      "Average test loss: 0.002397552761870126\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008715633226765527\n",
      "Average test loss: 0.002695815146693753\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008728502440783712\n",
      "Average test loss: 0.002447551023422016\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008700650387340123\n",
      "Average test loss: 0.0023924334459006786\n",
      "Epoch 171/300\n",
      "Average training loss: 0.00870639145705435\n",
      "Average test loss: 0.002464868066004581\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008691569799350367\n",
      "Average test loss: 0.002367392648011446\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008671739254560735\n",
      "Average test loss: 0.0024283370696422126\n",
      "Epoch 174/300\n",
      "Average training loss: 0.00867761206626892\n",
      "Average test loss: 0.002414100418902106\n",
      "Epoch 175/300\n",
      "Average training loss: 0.008668111783762773\n",
      "Average test loss: 0.0025148091800510884\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008681183069944382\n",
      "Average test loss: 0.0023863119101151823\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008666435710257953\n",
      "Average test loss: 0.0024232822516933082\n",
      "Epoch 178/300\n",
      "Average training loss: 0.00866674818429682\n",
      "Average test loss: 0.0024953119541621873\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008669008166425757\n",
      "Average test loss: 0.0024376703361049293\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00864337850155102\n",
      "Average test loss: 0.0024601144389145902\n",
      "Epoch 181/300\n",
      "Average training loss: 0.00865519127332502\n",
      "Average test loss: 0.002433897560876277\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008659232693413893\n",
      "Average test loss: 0.0024533829929100142\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008641271129664448\n",
      "Average test loss: 0.0025450267068420846\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008621897207780017\n",
      "Average test loss: 0.002517606917561756\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008628853056993749\n",
      "Average test loss: 0.0024080881068689955\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00861054905421204\n",
      "Average test loss: 0.0026363741273267402\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008609680835157632\n",
      "Average test loss: 0.002467423477830986\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008595947679546145\n",
      "Average test loss: 0.002440510519573258\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008612455007930597\n",
      "Average test loss: 0.002464527645872699\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008579346844719516\n",
      "Average test loss: 0.00244011759902868\n",
      "Epoch 194/300\n",
      "Average training loss: 0.00857943452273806\n",
      "Average test loss: 0.0024965096182293363\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008573693600793679\n",
      "Average test loss: 0.002450665292640527\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008568091647492514\n",
      "Average test loss: 0.0024384858553401297\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008563038194759025\n",
      "Average test loss: 0.0025689906291663646\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008567899978823132\n",
      "Average test loss: 0.00242968756519258\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008549378041177988\n",
      "Average test loss: 0.0024448279610110655\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008555992131845819\n",
      "Average test loss: 0.002525408556788332\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008561440916524993\n",
      "Average test loss: 0.0024340690376443994\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008558593784769376\n",
      "Average test loss: 0.0023761715388132465\n",
      "Epoch 203/300\n",
      "Average training loss: 0.008539337250921462\n",
      "Average test loss: 0.0025303620446680323\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008543583017256525\n",
      "Average test loss: 0.0024879617368181546\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0085258988307582\n",
      "Average test loss: 0.00253854123244269\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008540633156895638\n",
      "Average test loss: 0.0023980511269635625\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008551702957186434\n",
      "Average test loss: 0.002525748720806506\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008504983701639705\n",
      "Average test loss: 0.002500841809436679\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008515058958695995\n",
      "Average test loss: 0.0025106421827028194\n",
      "Epoch 210/300\n",
      "Average training loss: 0.00851466598196162\n",
      "Average test loss: 0.002467946805473831\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008523457278807958\n",
      "Average test loss: 0.0023963532061833472\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008512417543265555\n",
      "Average test loss: 0.002484873888393243\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008500019415385193\n",
      "Average test loss: 0.00243605815163917\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0085168653163645\n",
      "Average test loss: 0.0025423166683564585\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008495317377150058\n",
      "Average test loss: 0.002500879816090067\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008485328941709466\n",
      "Average test loss: 0.002468916037223405\n",
      "Epoch 217/300\n",
      "Average training loss: 0.008477211866941716\n",
      "Average test loss: 0.002550459594051871\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008483767516083188\n",
      "Average test loss: 0.002529509813214342\n",
      "Epoch 219/300\n",
      "Average training loss: 0.00848227278350128\n",
      "Average test loss: 0.002548240019318958\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008478021396117077\n",
      "Average test loss: 0.0025590386458983023\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008472654056631857\n",
      "Average test loss: 0.002432885755474369\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00846526065427396\n",
      "Average test loss: 0.002627616446258293\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00846824402610461\n",
      "Average test loss: 0.002450651806261804\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00847403978308042\n",
      "Average test loss: 0.0025452833332949214\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008454410718960895\n",
      "Average test loss: 0.002540124039372636\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008446597896516323\n",
      "Average test loss: 0.0025182188366436294\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008455890551623371\n",
      "Average test loss: 0.002491660482353634\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00843578573813041\n",
      "Average test loss: 0.002499038135115471\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008440506484773423\n",
      "Average test loss: 0.0024655422922223807\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008445281267166137\n",
      "Average test loss: 0.0025192096676263545\n",
      "Epoch 231/300\n",
      "Average training loss: 0.00843741340107388\n",
      "Average test loss: 0.0024727470696800286\n",
      "Epoch 232/300\n",
      "Average training loss: 0.00842393382721477\n",
      "Average test loss: 0.002402104185997612\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008431543994281027\n",
      "Average test loss: 0.002478821563637919\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0084275112433566\n",
      "Average test loss: 0.002440459960657689\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00843111885835727\n",
      "Average test loss: 0.002446100816027158\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008413441364136007\n",
      "Average test loss: 0.002428143045140637\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00841265207860205\n",
      "Average test loss: 0.00251685181632638\n",
      "Epoch 238/300\n",
      "Average training loss: 0.008409421869450146\n",
      "Average test loss: 0.002495212577904264\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008414065420627594\n",
      "Average test loss: 0.0025186058690564498\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008399533244470755\n",
      "Average test loss: 0.002474749381136563\n",
      "Epoch 241/300\n",
      "Average training loss: 0.00840339227186309\n",
      "Average test loss: 0.002500614546239376\n",
      "Epoch 242/300\n",
      "Average training loss: 0.00839957951878508\n",
      "Average test loss: 0.002479211773723364\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0083876204772128\n",
      "Average test loss: 0.0024532361228743363\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008382654482291805\n",
      "Average test loss: 0.002543802385528882\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008383203166640468\n",
      "Average test loss: 0.0025500876096387704\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008383909412142303\n",
      "Average test loss: 0.00251362725161016\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008388255773319139\n",
      "Average test loss: 0.0024288480310804316\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008389156637920274\n",
      "Average test loss: 0.002475075201648805\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008378928117454052\n",
      "Average test loss: 0.0025313804716699654\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008363105136487218\n",
      "Average test loss: 0.0024931791577902105\n",
      "Epoch 251/300\n",
      "Average training loss: 0.00836676660262876\n",
      "Average test loss: 0.0025617669050892193\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008368077587336303\n",
      "Average test loss: 0.0025032665222469302\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008374841093603107\n",
      "Average test loss: 0.0024252705580244463\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008353381259573831\n",
      "Average test loss: 0.002540792614014612\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008356701978378825\n",
      "Average test loss: 0.0025094849854293798\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008361325541718138\n",
      "Average test loss: 0.0025223847916349767\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008353055060737663\n",
      "Average test loss: 0.0024520197455874747\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008360033291081587\n",
      "Average test loss: 0.002535961609954635\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008351663356439935\n",
      "Average test loss: 0.002589970851213568\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008344462066060966\n",
      "Average test loss: 0.002638140681717131\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008341489479359653\n",
      "Average test loss: 0.002497660644041995\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008345776124133003\n",
      "Average test loss: 0.0024699838055918613\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008335486195981503\n",
      "Average test loss: 0.0024564171954989432\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008334089963800377\n",
      "Average test loss: 0.002486966716332568\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008327666466434796\n",
      "Average test loss: 0.0025272692365364896\n",
      "Epoch 266/300\n",
      "Average training loss: 0.00831757997473081\n",
      "Average test loss: 0.0024779899685333172\n",
      "Epoch 267/300\n",
      "Average training loss: 0.00831658392565118\n",
      "Average test loss: 0.0025402304149336283\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008341537032276391\n",
      "Average test loss: 0.0025555067715338537\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008305093387762705\n",
      "Average test loss: 0.002490110060820977\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008330468431529072\n",
      "Average test loss: 0.0024954974099786744\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008311077192425728\n",
      "Average test loss: 0.002472802061173651\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008316651796301205\n",
      "Average test loss: 0.002445590615272522\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008306409146222804\n",
      "Average test loss: 0.0025688727806425756\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008311108571787676\n",
      "Average test loss: 0.0026187814807312358\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008306960672967963\n",
      "Average test loss: 0.0025249249952741795\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008289172668837838\n",
      "Average test loss: 0.0025589117421251206\n",
      "Epoch 277/300\n",
      "Average training loss: 0.00828241311510404\n",
      "Average test loss: 0.0024665512030737267\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008290999791688389\n",
      "Average test loss: 0.0024675837140530348\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008295894980430603\n",
      "Average test loss: 0.0026542313652527\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008290388998058108\n",
      "Average test loss: 0.0025300911300712163\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008293508804092804\n",
      "Average test loss: 0.002556841861870554\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008292299448615975\n",
      "Average test loss: 0.0026171661506717405\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008271136456893551\n",
      "Average test loss: 0.002452651306365927\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008264501761231157\n",
      "Average test loss: 0.0024608451852367983\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008282410514023569\n",
      "Average test loss: 0.0025189867168664932\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008278949602610535\n",
      "Average test loss: 0.002538751269173291\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008266797381970618\n",
      "Average test loss: 0.0025213229596200916\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008264557951854335\n",
      "Average test loss: 0.002496976451948285\n",
      "Epoch 289/300\n",
      "Average training loss: 0.00827298857520024\n",
      "Average test loss: 0.00250955342542794\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008262192874319023\n",
      "Average test loss: 0.0024878548298858934\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008256915666162967\n",
      "Average test loss: 0.002536392789127098\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008254089501168993\n",
      "Average test loss: 0.0024883823757991196\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00825281046082576\n",
      "Average test loss: 0.002536288950178358\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008256997602681318\n",
      "Average test loss: 0.0024713469084041814\n",
      "Epoch 295/300\n",
      "Average training loss: 0.00825564712161819\n",
      "Average test loss: 0.002489926171799501\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008252641589277321\n",
      "Average test loss: 0.0025956365159816213\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008242053512483836\n",
      "Average test loss: 0.0025376319049133195\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008247897406419118\n",
      "Average test loss: 0.002490699573109547\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008239194756580723\n",
      "Average test loss: 0.002566807671346598\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008225671546326743\n",
      "Average test loss: 0.00258076909598377\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.10670727861589856\n",
      "Average test loss: 0.003624757506988115\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0212905440264278\n",
      "Average test loss: 0.003135985186530484\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01690202890833219\n",
      "Average test loss: 0.0026961271692481307\n",
      "Epoch 4/300\n",
      "Average training loss: 0.015414160827795665\n",
      "Average test loss: 0.0025347257544183067\n",
      "Epoch 5/300\n",
      "Average training loss: 0.014618857579926649\n",
      "Average test loss: 0.002501305551785562\n",
      "Epoch 6/300\n",
      "Average training loss: 0.014068738417492973\n",
      "Average test loss: 0.0023850034773349764\n",
      "Epoch 7/300\n",
      "Average training loss: 0.013643849578168657\n",
      "Average test loss: 0.0022738688079019386\n",
      "Epoch 8/300\n",
      "Average training loss: 0.013275054542554749\n",
      "Average test loss: 0.002223550060350034\n",
      "Epoch 9/300\n",
      "Average training loss: 0.012953582044276926\n",
      "Average test loss: 0.0021056267516687513\n",
      "Epoch 10/300\n",
      "Average training loss: 0.012680099376373821\n",
      "Average test loss: 0.002084031090968185\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01240824575142728\n",
      "Average test loss: 0.002008727092709806\n",
      "Epoch 12/300\n",
      "Average training loss: 0.012157614236076673\n",
      "Average test loss: 0.0020493842135700913\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011912304000722037\n",
      "Average test loss: 0.001995887496198217\n",
      "Epoch 14/300\n",
      "Average training loss: 0.011672366427878538\n",
      "Average test loss: 0.0018873431023417248\n",
      "Epoch 15/300\n",
      "Average training loss: 0.011436046007606719\n",
      "Average test loss: 0.0018667164264867704\n",
      "Epoch 16/300\n",
      "Average training loss: 0.011201758572624789\n",
      "Average test loss: 0.0018097515865746472\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010989313723726406\n",
      "Average test loss: 0.0021279550045728685\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010785748016917044\n",
      "Average test loss: 0.0017826040366457569\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010590366691764858\n",
      "Average test loss: 0.00174399118963629\n",
      "Epoch 20/300\n",
      "Average training loss: 0.010384703132841322\n",
      "Average test loss: 0.001739021522199942\n",
      "Epoch 21/300\n",
      "Average training loss: 0.0102181510358221\n",
      "Average test loss: 0.0018225753227662708\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010042828852103816\n",
      "Average test loss: 0.0016930840973638827\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009886509846482011\n",
      "Average test loss: 0.001764638141832418\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009734746445384291\n",
      "Average test loss: 0.0016427327673882246\n",
      "Epoch 25/300\n",
      "Average training loss: 0.009601127807878786\n",
      "Average test loss: 0.0016776411006641056\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009485123981204298\n",
      "Average test loss: 0.0016151792720581093\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009361722872489028\n",
      "Average test loss: 0.0016159788859594199\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009250840566638443\n",
      "Average test loss: 0.001599358338655697\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009162055891421106\n",
      "Average test loss: 0.0016061614295467734\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009080161856280433\n",
      "Average test loss: 0.0015944193680253293\n",
      "Epoch 31/300\n",
      "Average training loss: 0.008984234740336735\n",
      "Average test loss: 0.0015730115186630023\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008909144865141974\n",
      "Average test loss: 0.001555024328848554\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008846534465750058\n",
      "Average test loss: 0.001566213322389457\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008782734039756986\n",
      "Average test loss: 0.0015648365443986323\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008716266432570086\n",
      "Average test loss: 0.001545036863328682\n",
      "Epoch 36/300\n",
      "Average training loss: 0.00864655440673232\n",
      "Average test loss: 0.0015298523938076364\n",
      "Epoch 37/300\n",
      "Average training loss: 0.008591848844455349\n",
      "Average test loss: 0.00155643201371034\n",
      "Epoch 38/300\n",
      "Average training loss: 0.00854662719120582\n",
      "Average test loss: 0.0017427356445954905\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008482686532040437\n",
      "Average test loss: 0.0015285114266185298\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008441454199453195\n",
      "Average test loss: 0.0015794563091670473\n",
      "Epoch 41/300\n",
      "Average training loss: 0.00842094192529718\n",
      "Average test loss: 0.001695504083091186\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008329972806076209\n",
      "Average test loss: 0.001539529047595958\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008310706069072088\n",
      "Average test loss: 0.0015674312653847866\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0082631944819457\n",
      "Average test loss: 0.0015372451154722108\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00819971606383721\n",
      "Average test loss: 0.0015695961048412654\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00817111093012823\n",
      "Average test loss: 0.0015352514610729284\n",
      "Epoch 47/300\n",
      "Average training loss: 0.00812670408272081\n",
      "Average test loss: 0.0015244357314788634\n",
      "Epoch 48/300\n",
      "Average training loss: 0.008101458703478177\n",
      "Average test loss: 0.001551099800815185\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008040384895271726\n",
      "Average test loss: 0.0015295819291431044\n",
      "Epoch 50/300\n",
      "Average training loss: 0.00801874213044842\n",
      "Average test loss: 0.001577409054359628\n",
      "Epoch 51/300\n",
      "Average training loss: 0.007976243448754151\n",
      "Average test loss: 0.0016190995593658752\n",
      "Epoch 52/300\n",
      "Average training loss: 0.007942702654749155\n",
      "Average test loss: 0.001550963722065919\n",
      "Epoch 53/300\n",
      "Average training loss: 0.007906255630569325\n",
      "Average test loss: 0.0015311689807309045\n",
      "Epoch 54/300\n",
      "Average training loss: 0.007868627830098073\n",
      "Average test loss: 0.0015337691545072528\n",
      "Epoch 55/300\n",
      "Average training loss: 0.007839541633923848\n",
      "Average test loss: 0.0015391607669492562\n",
      "Epoch 56/300\n",
      "Average training loss: 0.007805519714123673\n",
      "Average test loss: 0.0015669863868711723\n",
      "Epoch 57/300\n",
      "Average training loss: 0.007784070293936464\n",
      "Average test loss: 0.0015654929549329811\n",
      "Epoch 58/300\n",
      "Average training loss: 0.007761116632570823\n",
      "Average test loss: 0.0015512781600571341\n",
      "Epoch 59/300\n",
      "Average training loss: 0.007714370641029544\n",
      "Average test loss: 0.001611900651310053\n",
      "Epoch 60/300\n",
      "Average training loss: 0.007693739566124148\n",
      "Average test loss: 0.0015218435240288576\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0076559834002206725\n",
      "Average test loss: 0.0023920604183028144\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0076340015671319435\n",
      "Average test loss: 0.0015521656678368647\n",
      "Epoch 63/300\n",
      "Average training loss: 0.007605405782245928\n",
      "Average test loss: 0.001565902243471808\n",
      "Epoch 64/300\n",
      "Average training loss: 0.00758324414326085\n",
      "Average test loss: 0.0015884216997979416\n",
      "Epoch 65/300\n",
      "Average training loss: 0.007552873977356486\n",
      "Average test loss: 0.0015587172996666696\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0075319408650199576\n",
      "Average test loss: 0.0015608459552749992\n",
      "Epoch 67/300\n",
      "Average training loss: 0.007494611226850086\n",
      "Average test loss: 0.0015626770780525273\n",
      "Epoch 68/300\n",
      "Average training loss: 0.007508902366790506\n",
      "Average test loss: 0.001578996861146556\n",
      "Epoch 69/300\n",
      "Average training loss: 0.007464242764645153\n",
      "Average test loss: 0.0015587604708141752\n",
      "Epoch 70/300\n",
      "Average training loss: 0.007426097743213176\n",
      "Average test loss: 0.001616148491907451\n",
      "Epoch 71/300\n",
      "Average training loss: 0.007404051434248686\n",
      "Average test loss: 0.001551450135703716\n",
      "Epoch 72/300\n",
      "Average training loss: 0.007399758407225211\n",
      "Average test loss: 0.0015707701321484315\n",
      "Epoch 73/300\n",
      "Average training loss: 0.007385295760300424\n",
      "Average test loss: 0.0015841415314417745\n",
      "Epoch 74/300\n",
      "Average training loss: 0.00734441900667217\n",
      "Average test loss: 0.001600545637516512\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0073382900299297435\n",
      "Average test loss: 0.0015981994222642646\n",
      "Epoch 76/300\n",
      "Average training loss: 0.007331410143110487\n",
      "Average test loss: 0.0015993240270763636\n",
      "Epoch 77/300\n",
      "Average training loss: 0.007301383499470022\n",
      "Average test loss: 0.0016009324007771081\n",
      "Epoch 78/300\n",
      "Average training loss: 0.007271281894296408\n",
      "Average test loss: 0.001668120224132306\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0072539954682191215\n",
      "Average test loss: 0.0016432909485366608\n",
      "Epoch 80/300\n",
      "Average training loss: 0.007232959193901883\n",
      "Average test loss: 0.0016183917992230918\n",
      "Epoch 81/300\n",
      "Average training loss: 0.007233690966334608\n",
      "Average test loss: 0.0016730356186421382\n",
      "Epoch 82/300\n",
      "Average training loss: 0.007199243079457018\n",
      "Average test loss: 0.0016221895929839877\n",
      "Epoch 83/300\n",
      "Average training loss: 0.007196035712957382\n",
      "Average test loss: 0.0016365666894449127\n",
      "Epoch 84/300\n",
      "Average training loss: 0.007177450937529405\n",
      "Average test loss: 0.0016497642846984995\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0071680165276759205\n",
      "Average test loss: 0.0016051181162604028\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0071409422192308634\n",
      "Average test loss: 0.0016099690710090929\n",
      "Epoch 87/300\n",
      "Average training loss: 0.007120434346298376\n",
      "Average test loss: 0.0016760905252562628\n",
      "Epoch 88/300\n",
      "Average training loss: 0.007107345822370714\n",
      "Average test loss: 0.0016263564408549832\n",
      "Epoch 89/300\n",
      "Average training loss: 0.007112018581893709\n",
      "Average test loss: 0.0016226421271761258\n",
      "Epoch 90/300\n",
      "Average training loss: 0.007081329950441917\n",
      "Average test loss: 0.0016322340833333632\n",
      "Epoch 91/300\n",
      "Average training loss: 0.007060807928442955\n",
      "Average test loss: 0.0015722734947792357\n",
      "Epoch 92/300\n",
      "Average training loss: 0.007055758312344551\n",
      "Average test loss: 0.0015937809675104087\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007044372862825791\n",
      "Average test loss: 0.0016585973786811033\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007033334628368418\n",
      "Average test loss: 0.0016803858259485827\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007013372956464688\n",
      "Average test loss: 0.001670636675837967\n",
      "Epoch 96/300\n",
      "Average training loss: 0.00700612435158756\n",
      "Average test loss: 0.0016147890094046792\n",
      "Epoch 97/300\n",
      "Average training loss: 0.006999221221854289\n",
      "Average test loss: 0.0016564592897064156\n",
      "Epoch 98/300\n",
      "Average training loss: 0.006988199017114109\n",
      "Average test loss: 0.0016892187187655104\n",
      "Epoch 99/300\n",
      "Average training loss: 0.006966036840031544\n",
      "Average test loss: 0.0016013942557490534\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0069496359945171405\n",
      "Average test loss: 0.0016347056210248006\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0069415111090574\n",
      "Average test loss: 0.0016506471294495794\n",
      "Epoch 102/300\n",
      "Average training loss: 0.006938759642342726\n",
      "Average test loss: 0.0017183308170901405\n",
      "Epoch 103/300\n",
      "Average training loss: 0.006917905127008756\n",
      "Average test loss: 0.0016136952434769934\n",
      "Epoch 104/300\n",
      "Average training loss: 0.006909895755764511\n",
      "Average test loss: 0.001605103296848635\n",
      "Epoch 105/300\n",
      "Average training loss: 0.006898252043045229\n",
      "Average test loss: 0.001657151579235991\n",
      "Epoch 106/300\n",
      "Average training loss: 0.006884188300619523\n",
      "Average test loss: 0.0016360721331503657\n",
      "Epoch 107/300\n",
      "Average training loss: 0.006892519964526097\n",
      "Average test loss: 0.0016422363783543308\n",
      "Epoch 108/300\n",
      "Average training loss: 0.006873906485322449\n",
      "Average test loss: 0.0017792294239625335\n",
      "Epoch 109/300\n",
      "Average training loss: 0.006857974646819962\n",
      "Average test loss: 0.0016492843725201157\n",
      "Epoch 110/300\n",
      "Average training loss: 0.006855588796652026\n",
      "Average test loss: 0.0022207573821975126\n",
      "Epoch 111/300\n",
      "Average training loss: 0.00684801722317934\n",
      "Average test loss: 0.0017023462847185632\n",
      "Epoch 112/300\n",
      "Average training loss: 0.006835557792335749\n",
      "Average test loss: 0.0017210234751303991\n",
      "Epoch 113/300\n",
      "Average training loss: 0.00682720367403494\n",
      "Average test loss: 0.0016706364285200834\n",
      "Epoch 114/300\n",
      "Average training loss: 0.006804418286929528\n",
      "Average test loss: 0.0017301798554965192\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0068102579617665875\n",
      "Average test loss: 0.0016554706208407879\n",
      "Epoch 116/300\n",
      "Average training loss: 0.006801289526952637\n",
      "Average test loss: 0.0017072063765178123\n",
      "Epoch 117/300\n",
      "Average training loss: 0.006782079845666885\n",
      "Average test loss: 0.0016742123334358137\n",
      "Epoch 118/300\n",
      "Average training loss: 0.006776803549793031\n",
      "Average test loss: 0.0016592755170745982\n",
      "Epoch 119/300\n",
      "Average training loss: 0.006761040720260805\n",
      "Average test loss: 0.0017291812798422245\n",
      "Epoch 120/300\n",
      "Average training loss: 0.006765669757293331\n",
      "Average test loss: 0.001632718920086821\n",
      "Epoch 121/300\n",
      "Average training loss: 0.006761419869131512\n",
      "Average test loss: 0.0016869605864501661\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0067437258242732945\n",
      "Average test loss: 0.0017179428265533515\n",
      "Epoch 123/300\n",
      "Average training loss: 0.006738629337400198\n",
      "Average test loss: 0.0017357335373138388\n",
      "Epoch 124/300\n",
      "Average training loss: 0.006742239904900392\n",
      "Average test loss: 0.0016739790400283204\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0067284506513840625\n",
      "Average test loss: 0.0016890106389505996\n",
      "Epoch 126/300\n",
      "Average training loss: 0.006722995141314136\n",
      "Average test loss: 0.0017206548555857605\n",
      "Epoch 127/300\n",
      "Average training loss: 0.006706655709279908\n",
      "Average test loss: 0.001682018256539272\n",
      "Epoch 128/300\n",
      "Average training loss: 0.00669926777192288\n",
      "Average test loss: 0.0016546535064569778\n",
      "Epoch 129/300\n",
      "Average training loss: 0.006704310954031017\n",
      "Average test loss: 0.001724500751019352\n",
      "Epoch 130/300\n",
      "Average training loss: 0.006700457021180126\n",
      "Average test loss: 0.0016932830342815982\n",
      "Epoch 131/300\n",
      "Average training loss: 0.006682061369634337\n",
      "Average test loss: 0.001666139755398035\n",
      "Epoch 132/300\n",
      "Average training loss: 0.00667651987199982\n",
      "Average test loss: 0.001668821417933537\n",
      "Epoch 133/300\n",
      "Average training loss: 0.006667611432158284\n",
      "Average test loss: 0.001696813527080748\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0066583897848096155\n",
      "Average test loss: 0.0017750463160789674\n",
      "Epoch 135/300\n",
      "Average training loss: 0.006662945508129067\n",
      "Average test loss: 0.0017031825217418374\n",
      "Epoch 136/300\n",
      "Average training loss: 0.006651825556738509\n",
      "Average test loss: 0.0017156439456674788\n",
      "Epoch 137/300\n",
      "Average training loss: 0.006649259977042675\n",
      "Average test loss: 0.0017823160678769152\n",
      "Epoch 138/300\n",
      "Average training loss: 0.006635896459635761\n",
      "Average test loss: 0.0017068064831611183\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0066259653953214485\n",
      "Average test loss: 0.0017981283966865805\n",
      "Epoch 140/300\n",
      "Average training loss: 0.006625495370891359\n",
      "Average test loss: 0.001764518605131242\n",
      "Epoch 141/300\n",
      "Average training loss: 0.006617806857658757\n",
      "Average test loss: 0.00170400899876323\n",
      "Epoch 142/300\n",
      "Average training loss: 0.006611669496115711\n",
      "Average test loss: 0.0017250321181491017\n",
      "Epoch 143/300\n",
      "Average training loss: 0.006609826271318727\n",
      "Average test loss: 0.0017346241688355804\n",
      "Epoch 144/300\n",
      "Average training loss: 0.006602425895424353\n",
      "Average test loss: 0.001771150364437037\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0066027198731899266\n",
      "Average test loss: 0.001657435755762789\n",
      "Epoch 146/300\n",
      "Average training loss: 0.006602326300409105\n",
      "Average test loss: 0.0016893941393742958\n",
      "Epoch 147/300\n",
      "Average training loss: 0.006586614246169726\n",
      "Average test loss: 0.0016915940743767552\n",
      "Epoch 148/300\n",
      "Average training loss: 0.006585773638139169\n",
      "Average test loss: 0.0016983318134314485\n",
      "Epoch 149/300\n",
      "Average training loss: 0.006570487489302953\n",
      "Average test loss: 0.0016874292193808488\n",
      "Epoch 150/300\n",
      "Average training loss: 0.006574239722556538\n",
      "Average test loss: 0.0017376213796022866\n",
      "Epoch 151/300\n",
      "Average training loss: 0.006567059374103943\n",
      "Average test loss: 0.0017407537932611174\n",
      "Epoch 152/300\n",
      "Average training loss: 0.00656311296713021\n",
      "Average test loss: 0.0017144546899944544\n",
      "Epoch 153/300\n",
      "Average training loss: 0.006545524929132726\n",
      "Average test loss: 0.0017649559368275933\n",
      "Epoch 154/300\n",
      "Average training loss: 0.006554056858436929\n",
      "Average test loss: 0.001731392578325338\n",
      "Epoch 155/300\n",
      "Average training loss: 0.006543635827385717\n",
      "Average test loss: 0.0016954430989507172\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0065415775974591575\n",
      "Average test loss: 0.0017212057553438677\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0065352496107419335\n",
      "Average test loss: 0.001718927720354663\n",
      "Epoch 158/300\n",
      "Average training loss: 0.006519820012980037\n",
      "Average test loss: 0.0018074461072683334\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0065241208256532745\n",
      "Average test loss: 0.001769585742511683\n",
      "Epoch 160/300\n",
      "Average training loss: 0.006521181183142794\n",
      "Average test loss: 0.001730380219873041\n",
      "Epoch 161/300\n",
      "Average training loss: 0.006515824869275093\n",
      "Average test loss: 0.0017380026223965817\n",
      "Epoch 162/300\n",
      "Average training loss: 0.006515321880578994\n",
      "Average test loss: 0.001723943644668907\n",
      "Epoch 163/300\n",
      "Average training loss: 0.006512663388003906\n",
      "Average test loss: 0.001729637175384495\n",
      "Epoch 164/300\n",
      "Average training loss: 0.006493976024703847\n",
      "Average test loss: 0.0017178355555257036\n",
      "Epoch 165/300\n",
      "Average training loss: 0.006487775322463777\n",
      "Average test loss: 0.0018086489490750763\n",
      "Epoch 166/300\n",
      "Average training loss: 0.006488887755821148\n",
      "Average test loss: 0.001740130599691636\n",
      "Epoch 167/300\n",
      "Average training loss: 0.006494894786427418\n",
      "Average test loss: 0.001684966857958999\n",
      "Epoch 168/300\n",
      "Average training loss: 0.006480557134168016\n",
      "Average test loss: 0.0018345060849355326\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0064947509078515904\n",
      "Average test loss: 0.001735838265882598\n",
      "Epoch 170/300\n",
      "Average training loss: 0.006467176862061024\n",
      "Average test loss: 0.0017133397998081313\n",
      "Epoch 171/300\n",
      "Average training loss: 0.006463751650518841\n",
      "Average test loss: 0.0017554429361803664\n",
      "Epoch 172/300\n",
      "Average training loss: 0.006466459104998244\n",
      "Average test loss: 0.0017524336512304015\n",
      "Epoch 173/300\n",
      "Average training loss: 0.006463824156257842\n",
      "Average test loss: 0.0017812317423522473\n",
      "Epoch 174/300\n",
      "Average training loss: 0.006450519683046474\n",
      "Average test loss: 0.0017570677393426498\n",
      "Epoch 175/300\n",
      "Average training loss: 0.006454171994080146\n",
      "Average test loss: 0.0017888851961534885\n",
      "Epoch 176/300\n",
      "Average training loss: 0.006449259721570544\n",
      "Average test loss: 0.0017151254283057319\n",
      "Epoch 177/300\n",
      "Average training loss: 0.006457230625053247\n",
      "Average test loss: 0.0017082638735365537\n",
      "Epoch 178/300\n",
      "Average training loss: 0.006441338837146759\n",
      "Average test loss: 0.0017556610602057641\n",
      "Epoch 179/300\n",
      "Average training loss: 0.006433711484902435\n",
      "Average test loss: 0.0017314607012603018\n",
      "Epoch 180/300\n",
      "Average training loss: 0.006437128061635627\n",
      "Average test loss: 0.0017530534185676112\n",
      "Epoch 181/300\n",
      "Average training loss: 0.006436014144370953\n",
      "Average test loss: 0.00172492516040802\n",
      "Epoch 182/300\n",
      "Average training loss: 0.006422614258196619\n",
      "Average test loss: 0.0017154259321590264\n",
      "Epoch 183/300\n",
      "Average training loss: 0.006429402264455954\n",
      "Average test loss: 0.0017237181624190675\n",
      "Epoch 184/300\n",
      "Average training loss: 0.006419847976416349\n",
      "Average test loss: 0.0017307244878676203\n",
      "Epoch 185/300\n",
      "Average training loss: 0.006430814734763569\n",
      "Average test loss: 0.0017737091972182195\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00640685351855225\n",
      "Average test loss: 0.0017693738059865105\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0064044951361914475\n",
      "Average test loss: 0.001776906256770922\n",
      "Epoch 188/300\n",
      "Average training loss: 0.006396147637317578\n",
      "Average test loss: 0.0017177418262387316\n",
      "Epoch 189/300\n",
      "Average training loss: 0.006412962189979023\n",
      "Average test loss: 0.001757116960775521\n",
      "Epoch 190/300\n",
      "Average training loss: 0.006390266958624125\n",
      "Average test loss: 0.0018311642840918567\n",
      "Epoch 191/300\n",
      "Average training loss: 0.006400377677132686\n",
      "Average test loss: 0.0017455585990101099\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0063965589875976244\n",
      "Average test loss: 0.0017182170142316156\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0063825672128134305\n",
      "Average test loss: 0.0017489428331868516\n",
      "Epoch 194/300\n",
      "Average training loss: 0.006382809684922297\n",
      "Average test loss: 0.0017634202260524035\n",
      "Epoch 195/300\n",
      "Average training loss: 0.0063846919342047635\n",
      "Average test loss: 0.0017362636241854894\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0063857491778002845\n",
      "Average test loss: 0.0017734559668848911\n",
      "Epoch 197/300\n",
      "Average training loss: 0.006378616168681118\n",
      "Average test loss: 0.0017489436524402765\n",
      "Epoch 198/300\n",
      "Average training loss: 0.006366757606466611\n",
      "Average test loss: 0.0017718156031850312\n",
      "Epoch 199/300\n",
      "Average training loss: 0.006378498102227847\n",
      "Average test loss: 0.0017612730368143981\n",
      "Epoch 200/300\n",
      "Average training loss: 0.006370017180012332\n",
      "Average test loss: 0.0017316905789905124\n",
      "Epoch 201/300\n",
      "Average training loss: 0.00636275093547172\n",
      "Average test loss: 0.0017817693618126214\n",
      "Epoch 202/300\n",
      "Average training loss: 0.006356283730516831\n",
      "Average test loss: 0.0018296023566896718\n",
      "Epoch 203/300\n",
      "Average training loss: 0.006353714944587813\n",
      "Average test loss: 0.0017284518788672155\n",
      "Epoch 204/300\n",
      "Average training loss: 0.006375268197721906\n",
      "Average test loss: 0.001725607653044992\n",
      "Epoch 205/300\n",
      "Average training loss: 0.006359301050090128\n",
      "Average test loss: 0.0017047090224093862\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0063517329792181654\n",
      "Average test loss: 0.001757481693290174\n",
      "Epoch 207/300\n",
      "Average training loss: 0.006340988839666049\n",
      "Average test loss: 0.0017606153811017672\n",
      "Epoch 208/300\n",
      "Average training loss: 0.006327826110025247\n",
      "Average test loss: 0.0017776080339940057\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0063336520302626825\n",
      "Average test loss: 0.0017606470822874043\n",
      "Epoch 210/300\n",
      "Average training loss: 0.006339029092755582\n",
      "Average test loss: 0.001838598507249521\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0063354757585459285\n",
      "Average test loss: 0.0017691685628766815\n",
      "Epoch 212/300\n",
      "Average training loss: 0.006329222489562299\n",
      "Average test loss: 0.0017413501730188727\n",
      "Epoch 213/300\n",
      "Average training loss: 0.006322157609793875\n",
      "Average test loss: 0.0017176713380548689\n",
      "Epoch 214/300\n",
      "Average training loss: 0.006330792042116324\n",
      "Average test loss: 0.0017857684288173914\n",
      "Epoch 215/300\n",
      "Average training loss: 0.006328092538648181\n",
      "Average test loss: 0.0017060595638015204\n",
      "Epoch 216/300\n",
      "Average training loss: 0.006319910744412078\n",
      "Average test loss: 0.0017788022310576506\n",
      "Epoch 217/300\n",
      "Average training loss: 0.006316532928082678\n",
      "Average test loss: 0.0017700366883849105\n",
      "Epoch 218/300\n",
      "Average training loss: 0.006318245961848232\n",
      "Average test loss: 0.0017813540695028173\n",
      "Epoch 219/300\n",
      "Average training loss: 0.006317358515742753\n",
      "Average test loss: 0.0017524327509519126\n",
      "Epoch 220/300\n",
      "Average training loss: 0.006301090548849768\n",
      "Average test loss: 0.0017426415496609277\n",
      "Epoch 221/300\n",
      "Average training loss: 0.006300860457950168\n",
      "Average test loss: 0.0017709866112305059\n",
      "Epoch 222/300\n",
      "Average training loss: 0.006302817896836334\n",
      "Average test loss: 0.0017243817910138103\n",
      "Epoch 223/300\n",
      "Average training loss: 0.006300125310404433\n",
      "Average test loss: 0.0017561770271923808\n",
      "Epoch 224/300\n",
      "Average training loss: 0.006308545593586233\n",
      "Average test loss: 0.001755713338446286\n",
      "Epoch 225/300\n",
      "Average training loss: 0.006299194581806659\n",
      "Average test loss: 0.001733240906149149\n",
      "Epoch 226/300\n",
      "Average training loss: 0.006288797965480221\n",
      "Average test loss: 0.0018099652944753568\n",
      "Epoch 227/300\n",
      "Average training loss: 0.006291038900613785\n",
      "Average test loss: 0.0018001060823185577\n",
      "Epoch 228/300\n",
      "Average training loss: 0.006278588227513764\n",
      "Average test loss: 0.0017673217979156308\n",
      "Epoch 229/300\n",
      "Average training loss: 0.006280829631206062\n",
      "Average test loss: 0.0017983496634082661\n",
      "Epoch 230/300\n",
      "Average training loss: 0.006285011204580466\n",
      "Average test loss: 0.0017714416136344275\n",
      "Epoch 231/300\n",
      "Average training loss: 0.006269059766083955\n",
      "Average test loss: 0.0017265101479780342\n",
      "Epoch 232/300\n",
      "Average training loss: 0.006281597973157962\n",
      "Average test loss: 0.00177089134686523\n",
      "Epoch 233/300\n",
      "Average training loss: 0.006295175855772363\n",
      "Average test loss: 0.0017532337008871967\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0062687036502692434\n",
      "Average test loss: 0.0017827109230889213\n",
      "Epoch 235/300\n",
      "Average training loss: 0.006272291485220194\n",
      "Average test loss: 0.0017888803701433871\n",
      "Epoch 236/300\n",
      "Average training loss: 0.00626577151276999\n",
      "Average test loss: 0.0018032311087267266\n",
      "Epoch 237/300\n",
      "Average training loss: 0.006264515420628919\n",
      "Average test loss: 0.001763436933979392\n",
      "Epoch 238/300\n",
      "Average training loss: 0.006264429745574792\n",
      "Average test loss: 0.001804985309433606\n",
      "Epoch 239/300\n",
      "Average training loss: 0.00626362491481834\n",
      "Average test loss: 0.0018249009707942605\n",
      "Epoch 240/300\n",
      "Average training loss: 0.006249661450998651\n",
      "Average test loss: 0.001729568648772935\n",
      "Epoch 241/300\n",
      "Average training loss: 0.006249901676757468\n",
      "Average test loss: 0.0017853273221602043\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0062590365525749\n",
      "Average test loss: 0.0018673719535581767\n",
      "Epoch 243/300\n",
      "Average training loss: 0.006248357083648444\n",
      "Average test loss: 0.0017977800778009826\n",
      "Epoch 244/300\n",
      "Average training loss: 0.006254486110475329\n",
      "Average test loss: 0.0017529211431327792\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0062474755566153265\n",
      "Average test loss: 0.0017629217547881935\n",
      "Epoch 246/300\n",
      "Average training loss: 0.006239848676241107\n",
      "Average test loss: 0.0021345620072550243\n",
      "Epoch 247/300\n",
      "Average training loss: 0.006252617239952087\n",
      "Average test loss: 0.0017753842088083427\n",
      "Epoch 248/300\n",
      "Average training loss: 0.006239070461442073\n",
      "Average test loss: 0.0017214782376670176\n",
      "Epoch 249/300\n",
      "Average training loss: 0.006235435000310341\n",
      "Average test loss: 0.0018316429232557614\n",
      "Epoch 250/300\n",
      "Average training loss: 0.00623373076857792\n",
      "Average test loss: 0.0017428533660454883\n",
      "Epoch 251/300\n",
      "Average training loss: 0.006237844010194143\n",
      "Average test loss: 0.0017898810983945925\n",
      "Epoch 252/300\n",
      "Average training loss: 0.006228239863283105\n",
      "Average test loss: 0.0017872271438439688\n",
      "Epoch 253/300\n",
      "Average training loss: 0.006224533864607413\n",
      "Average test loss: 0.0018269957427142396\n",
      "Epoch 254/300\n",
      "Average training loss: 0.006220849535531468\n",
      "Average test loss: 0.001834306514201065\n",
      "Epoch 255/300\n",
      "Average training loss: 0.006218508414096303\n",
      "Average test loss: 0.0017764264419674874\n",
      "Epoch 256/300\n",
      "Average training loss: 0.006222117990255356\n",
      "Average test loss: 0.0017980009597829646\n",
      "Epoch 257/300\n",
      "Average training loss: 0.006214300634960333\n",
      "Average test loss: 0.001827325130191942\n",
      "Epoch 258/300\n",
      "Average training loss: 0.006226277916381757\n",
      "Average test loss: 0.0018476727704207102\n",
      "Epoch 259/300\n",
      "Average training loss: 0.006218124219112926\n",
      "Average test loss: 0.0018231821581721306\n",
      "Epoch 260/300\n",
      "Average training loss: 0.006202711702220969\n",
      "Average test loss: 0.001753913797231184\n",
      "Epoch 261/300\n",
      "Average training loss: 0.006208930689427588\n",
      "Average test loss: 0.00183673412580457\n",
      "Epoch 262/300\n",
      "Average training loss: 0.006201292736662758\n",
      "Average test loss: 0.0017747559746106466\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0062055059236784775\n",
      "Average test loss: 0.0017835609767482513\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00621037669107318\n",
      "Average test loss: 0.001790332294586632\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0061994916664229495\n",
      "Average test loss: 0.001810372692429357\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0062080268946786725\n",
      "Average test loss: 0.0017164016193192866\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0061920568642930855\n",
      "Average test loss: 0.0018599715588821304\n",
      "Epoch 268/300\n",
      "Average training loss: 0.006195496604674392\n",
      "Average test loss: 0.001811852056791799\n",
      "Epoch 269/300\n",
      "Average training loss: 0.006197763640226589\n",
      "Average test loss: 0.0017698496471469601\n",
      "Epoch 270/300\n",
      "Average training loss: 0.006186127706948254\n",
      "Average test loss: 0.0018237353201127715\n",
      "Epoch 271/300\n",
      "Average training loss: 0.006204437824173106\n",
      "Average test loss: 0.0018282663229232034\n",
      "Epoch 272/300\n",
      "Average training loss: 0.006188898519923289\n",
      "Average test loss: 0.0018249250105064777\n",
      "Epoch 273/300\n",
      "Average training loss: 0.006184656350149049\n",
      "Average test loss: 0.001787383552847637\n",
      "Epoch 274/300\n",
      "Average training loss: 0.006179671092993683\n",
      "Average test loss: 0.0017441093701248368\n",
      "Epoch 275/300\n",
      "Average training loss: 0.006176556049121751\n",
      "Average test loss: 0.0018051064495618145\n",
      "Epoch 276/300\n",
      "Average training loss: 0.006191721955521239\n",
      "Average test loss: 0.0017465794055412213\n",
      "Epoch 277/300\n",
      "Average training loss: 0.006186402168952757\n",
      "Average test loss: 0.0017855397507341372\n",
      "Epoch 278/300\n",
      "Average training loss: 0.006172356448653672\n",
      "Average test loss: 0.0018167047895905045\n",
      "Epoch 279/300\n",
      "Average training loss: 0.006172599758125014\n",
      "Average test loss: 0.001808635929806365\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0061792860668566495\n",
      "Average test loss: 0.001759106943797734\n",
      "Epoch 281/300\n",
      "Average training loss: 0.006168953012261126\n",
      "Average test loss: 0.001871926033972866\n",
      "Epoch 282/300\n",
      "Average training loss: 0.006173881370988157\n",
      "Average test loss: 0.001840735008319219\n",
      "Epoch 283/300\n",
      "Average training loss: 0.006169115447749694\n",
      "Average test loss: 0.0018410416278574202\n",
      "Epoch 284/300\n",
      "Average training loss: 0.00617128457998236\n",
      "Average test loss: 0.0017248997163648407\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0061806108057498935\n",
      "Average test loss: 0.0018464329211662213\n",
      "Epoch 286/300\n",
      "Average training loss: 0.006162295788112614\n",
      "Average test loss: 0.0017724875406258636\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0061639819691578545\n",
      "Average test loss: 0.0017754165898594592\n",
      "Epoch 288/300\n",
      "Average training loss: 0.006166387850625648\n",
      "Average test loss: 0.0018305467340267367\n",
      "Epoch 289/300\n",
      "Average training loss: 0.006158736445837551\n",
      "Average test loss: 0.0017362542809504602\n",
      "Epoch 290/300\n",
      "Average training loss: 0.006161340378638771\n",
      "Average test loss: 0.0017817479853207867\n",
      "Epoch 291/300\n",
      "Average training loss: 0.006151952613559034\n",
      "Average test loss: 0.001764394523575902\n",
      "Epoch 292/300\n",
      "Average training loss: 0.006159859556290838\n",
      "Average test loss: 0.0018431490119546652\n",
      "Epoch 293/300\n",
      "Average training loss: 0.006148325251208411\n",
      "Average test loss: 0.0017708421580286489\n",
      "Epoch 294/300\n",
      "Average training loss: 0.006140115690314108\n",
      "Average test loss: 0.001796488072309229\n",
      "Epoch 295/300\n",
      "Average training loss: 0.006151487413793802\n",
      "Average test loss: 0.0018287673520131244\n",
      "Epoch 296/300\n",
      "Average training loss: 0.006148887580053674\n",
      "Average test loss: 0.0017618854670888849\n",
      "Epoch 297/300\n",
      "Average training loss: 0.006148891642275784\n",
      "Average test loss: 0.001855416111078941\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00613221084450682\n",
      "Average test loss: 0.0018694015372958449\n",
      "Epoch 299/300\n",
      "Average training loss: 0.006142917279981904\n",
      "Average test loss: 0.0017972702802055412\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0061297494057152005\n",
      "Average test loss: 0.0018356626447186702\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth10-.01/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.75\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.53\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.773451455116272\n",
      "Average test loss: 0.006869489257120424\n",
      "Epoch 2/300\n",
      "Average training loss: 1.1604972919887966\n",
      "Average test loss: 0.005163340801994006\n",
      "Epoch 3/300\n",
      "Average training loss: 0.7078636231952243\n",
      "Average test loss: 0.004676200392759508\n",
      "Epoch 4/300\n",
      "Average training loss: 0.4886129412651062\n",
      "Average test loss: 0.006565967711723513\n",
      "Epoch 5/300\n",
      "Average training loss: 0.36064615421824986\n",
      "Average test loss: 0.004556926319789555\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2844872238371107\n",
      "Average test loss: 0.004412614259041018\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2329515784051683\n",
      "Average test loss: 0.0043372479201191005\n",
      "Epoch 8/300\n",
      "Average training loss: 0.19571667271190218\n",
      "Average test loss: 0.004286951872002748\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1672169076734119\n",
      "Average test loss: 0.004262007998095618\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1457490440607071\n",
      "Average test loss: 0.004223275291423003\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12889688963360257\n",
      "Average test loss: 0.004278653124968211\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11554791729980045\n",
      "Average test loss: 0.004218349889748626\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1049647441705068\n",
      "Average test loss: 0.004171172186732292\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09649814817640516\n",
      "Average test loss: 0.004151589108424054\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08947686412599351\n",
      "Average test loss: 0.00412311524980598\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0840199505156941\n",
      "Average test loss: 0.004121097988138596\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07957460960414675\n",
      "Average test loss: 0.010540169310238627\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07605001344283421\n",
      "Average test loss: 0.004490046398921145\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07316627615027957\n",
      "Average test loss: 0.004069742154743937\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0709533047940996\n",
      "Average test loss: 0.004065633266336388\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06908309639824761\n",
      "Average test loss: 0.004040240338072181\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06768060767319467\n",
      "Average test loss: 0.004079996291134092\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0664196285770999\n",
      "Average test loss: 0.004076898006929292\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06548080519504017\n",
      "Average test loss: 0.00405769452949365\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06464059134324392\n",
      "Average test loss: 0.004013253435906436\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0639933266573482\n",
      "Average test loss: 0.003995476025674078\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0633817733824253\n",
      "Average test loss: 0.004040670022989312\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06288369463218584\n",
      "Average test loss: 0.003972215619973011\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06245816779136658\n",
      "Average test loss: 0.003984054391168885\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06213259193632338\n",
      "Average test loss: 0.0039729342737959495\n",
      "Epoch 31/300\n",
      "Average training loss: 0.061713591628604465\n",
      "Average test loss: 0.0039679422436489\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06151569457186593\n",
      "Average test loss: 0.0039512552904586\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06120697193013297\n",
      "Average test loss: 0.003953951634052727\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06091012374891175\n",
      "Average test loss: 0.0039345258377078505\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06063764988713794\n",
      "Average test loss: 0.00398506189427442\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06047039836645127\n",
      "Average test loss: 0.0039474605350858635\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06017675527599123\n",
      "Average test loss: 0.0039242577327208384\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06004028237528271\n",
      "Average test loss: 0.003951767185082038\n",
      "Epoch 39/300\n",
      "Average training loss: 0.059698890762196644\n",
      "Average test loss: 0.003943353915587067\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05951373561223348\n",
      "Average test loss: 0.003916393087555965\n",
      "Epoch 41/300\n",
      "Average training loss: 0.059271759506728916\n",
      "Average test loss: 0.003905493489570088\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05911063782705201\n",
      "Average test loss: 0.003925282414381703\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05902968856361177\n",
      "Average test loss: 0.003932537775486708\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05879156161016888\n",
      "Average test loss: 0.00400151378992531\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05854769709706306\n",
      "Average test loss: 0.0039031823157436316\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05841531564791997\n",
      "Average test loss: 0.00389915073559516\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05823095851474338\n",
      "Average test loss: 0.003924359566635556\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05798986509773466\n",
      "Average test loss: 0.00394247701122529\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05787255210677783\n",
      "Average test loss: 0.003959900298880206\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05769985401961539\n",
      "Average test loss: 0.003929573795033826\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05758058627777629\n",
      "Average test loss: 0.0039019892832471265\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05735576026638349\n",
      "Average test loss: 0.003914882792573836\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0573459816939301\n",
      "Average test loss: 0.0038892430228491624\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05710027324822214\n",
      "Average test loss: 0.003917817268106673\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05685451492336061\n",
      "Average test loss: 0.003923172386570109\n",
      "Epoch 56/300\n",
      "Average training loss: 0.056731846574279994\n",
      "Average test loss: 0.00394302287677096\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05658615482515759\n",
      "Average test loss: 0.003955279559724861\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05635720959968037\n",
      "Average test loss: 0.0039021128138734236\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05624578153921498\n",
      "Average test loss: 0.00396123578813341\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05605264005396101\n",
      "Average test loss: 0.0039055343959480526\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05596397837334209\n",
      "Average test loss: 0.003948844977757997\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05574163950483004\n",
      "Average test loss: 0.003887806322839525\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05557168734073639\n",
      "Average test loss: 0.003928157678701812\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05554398043950399\n",
      "Average test loss: 0.003913742031488154\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05529758844441838\n",
      "Average test loss: 0.0038873065151274204\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05513057289355331\n",
      "Average test loss: 0.004017947797560029\n",
      "Epoch 67/300\n",
      "Average training loss: 0.055008054312732486\n",
      "Average test loss: 0.00393124430357582\n",
      "Epoch 68/300\n",
      "Average training loss: 0.054794601410627364\n",
      "Average test loss: 0.003966962417173717\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05468144484029876\n",
      "Average test loss: 0.0039086696449667216\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05466705679893494\n",
      "Average test loss: 0.00397724334233337\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05438747254345152\n",
      "Average test loss: 0.003996091700262493\n",
      "Epoch 72/300\n",
      "Average training loss: 0.054187306380934185\n",
      "Average test loss: 0.003960627350956201\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05412410494685173\n",
      "Average test loss: 0.004013986308541563\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05392727306154039\n",
      "Average test loss: 0.004142590019438002\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05383635880549749\n",
      "Average test loss: 0.003964758397804366\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05363950232333607\n",
      "Average test loss: 0.003948917370703485\n",
      "Epoch 77/300\n",
      "Average training loss: 0.053532803538772794\n",
      "Average test loss: 0.0039762686383393074\n",
      "Epoch 78/300\n",
      "Average training loss: 0.053308309134509825\n",
      "Average test loss: 0.004010607380833891\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05324147851599587\n",
      "Average test loss: 0.003995531191221542\n",
      "Epoch 80/300\n",
      "Average training loss: 0.053072617881827884\n",
      "Average test loss: 0.003986505853633086\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05293384329809083\n",
      "Average test loss: 0.004038508427639803\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05276269739203983\n",
      "Average test loss: 0.003974668838911587\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05264758652448654\n",
      "Average test loss: 0.0039467183525363604\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05253175162606769\n",
      "Average test loss: 0.004129617653787136\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05238239344954491\n",
      "Average test loss: 0.00396070064086881\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05221875425179799\n",
      "Average test loss: 0.004013935310559141\n",
      "Epoch 87/300\n",
      "Average training loss: 0.052160460571448006\n",
      "Average test loss: 0.0040019400181869665\n",
      "Epoch 88/300\n",
      "Average training loss: 0.052020796264211334\n",
      "Average test loss: 0.00409240940068331\n",
      "Epoch 89/300\n",
      "Average training loss: 0.051876625734898776\n",
      "Average test loss: 0.004063250626333886\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05177414279182752\n",
      "Average test loss: 0.004027836692002085\n",
      "Epoch 91/300\n",
      "Average training loss: 0.051721144917938444\n",
      "Average test loss: 0.004096221160971456\n",
      "Epoch 92/300\n",
      "Average training loss: 0.051503461205297044\n",
      "Average test loss: 0.0040375383537676595\n",
      "Epoch 93/300\n",
      "Average training loss: 0.051421553151475057\n",
      "Average test loss: 0.004182792130029864\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05125321946210332\n",
      "Average test loss: 0.004105284334884749\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05112350474463569\n",
      "Average test loss: 0.004101925445099672\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05101606845855713\n",
      "Average test loss: 0.0040203365853263276\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0509347689251105\n",
      "Average test loss: 0.0041364864522798195\n",
      "Epoch 98/300\n",
      "Average training loss: 0.050791257838408155\n",
      "Average test loss: 0.004040802089290486\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05067258758677377\n",
      "Average test loss: 0.004155559890386131\n",
      "Epoch 100/300\n",
      "Average training loss: 0.050579319530063206\n",
      "Average test loss: 0.004017321926852067\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05047515989674462\n",
      "Average test loss: 0.004012111541297701\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05034758317304982\n",
      "Average test loss: 0.0040674727174143\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05026774567696783\n",
      "Average test loss: 0.0041926932798491585\n",
      "Epoch 104/300\n",
      "Average training loss: 0.050165788617399\n",
      "Average test loss: 0.004143253530272179\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05000121107697487\n",
      "Average test loss: 0.00403226967031757\n",
      "Epoch 106/300\n",
      "Average training loss: 0.049988737606339984\n",
      "Average test loss: 0.0040376134216785434\n",
      "Epoch 107/300\n",
      "Average training loss: 0.049826467507415345\n",
      "Average test loss: 0.0040315801830341415\n",
      "Epoch 108/300\n",
      "Average training loss: 0.049881420648760263\n",
      "Average test loss: 0.0040968652770130174\n",
      "Epoch 109/300\n",
      "Average training loss: 0.049738792131344474\n",
      "Average test loss: 0.004139727014220423\n",
      "Epoch 110/300\n",
      "Average training loss: 0.049479766974846524\n",
      "Average test loss: 0.004114093866199255\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04937096248401536\n",
      "Average test loss: 0.004134146582335234\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04930602278643184\n",
      "Average test loss: 0.004220211540659269\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04921398566497697\n",
      "Average test loss: 0.004066259184852242\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04916728970739576\n",
      "Average test loss: 0.004060981312145789\n",
      "Epoch 115/300\n",
      "Average training loss: 0.049044481684764225\n",
      "Average test loss: 0.004083250605397755\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04900374193323983\n",
      "Average test loss: 0.0041509958710521455\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04887397148874071\n",
      "Average test loss: 0.004106272206124332\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0487535682088799\n",
      "Average test loss: 0.0041720318910148406\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04873788990908199\n",
      "Average test loss: 0.0041056308512472444\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04860414679514037\n",
      "Average test loss: 0.004193358384900623\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04848808368378215\n",
      "Average test loss: 0.004164524048566818\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04847439858648512\n",
      "Average test loss: 0.004065867829653952\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04838177994887034\n",
      "Average test loss: 0.004182493520279725\n",
      "Epoch 124/300\n",
      "Average training loss: 0.048213601287868287\n",
      "Average test loss: 0.004063974557237493\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04819570679134793\n",
      "Average test loss: 0.00419374687017666\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04810388818383217\n",
      "Average test loss: 0.0043304748692446285\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04798061336080233\n",
      "Average test loss: 0.004170676383707259\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04804272331794103\n",
      "Average test loss: 0.0041350033577117655\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04786524410380257\n",
      "Average test loss: 0.004214728265586827\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04777264059914483\n",
      "Average test loss: 0.004194828408459823\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04773373395204544\n",
      "Average test loss: 0.0042000146156383885\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0477172801858849\n",
      "Average test loss: 0.004269893444246716\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0475475832356347\n",
      "Average test loss: 0.004184874989920193\n",
      "Epoch 134/300\n",
      "Average training loss: 0.047469818611939746\n",
      "Average test loss: 0.0042419133672697675\n",
      "Epoch 135/300\n",
      "Average training loss: 0.047468674722645016\n",
      "Average test loss: 0.004200844490279754\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04731326847275098\n",
      "Average test loss: 0.004210877080758413\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04721727587779363\n",
      "Average test loss: 0.004135334347271257\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04723359310626984\n",
      "Average test loss: 0.0041983088068664074\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04716019804610146\n",
      "Average test loss: 0.004264146637171507\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04723266634013918\n",
      "Average test loss: 0.004169700954937273\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04699761133392652\n",
      "Average test loss: 0.0042389710230959785\n",
      "Epoch 142/300\n",
      "Average training loss: 0.046838215344482\n",
      "Average test loss: 0.004301154598179791\n",
      "Epoch 143/300\n",
      "Average training loss: 0.046797326415777205\n",
      "Average test loss: 0.004171294985546006\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04674882911311255\n",
      "Average test loss: 0.004228395141454206\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0466826366285483\n",
      "Average test loss: 0.004168950896296236\n",
      "Epoch 146/300\n",
      "Average training loss: 0.046663407391972014\n",
      "Average test loss: 0.004257626068674855\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04653684167398347\n",
      "Average test loss: 0.004218841593712568\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0465480697453022\n",
      "Average test loss: 0.004220360043562121\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04643434774213367\n",
      "Average test loss: 0.004202335239698489\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04637008454236719\n",
      "Average test loss: 0.004240177949269613\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04634071561031872\n",
      "Average test loss: 0.004166153719027837\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04634013158414099\n",
      "Average test loss: 0.004189026263438993\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04624027129014333\n",
      "Average test loss: 0.0041731248626278505\n",
      "Epoch 154/300\n",
      "Average training loss: 0.046084392057524785\n",
      "Average test loss: 0.0042643630796422565\n",
      "Epoch 155/300\n",
      "Average training loss: 0.046050677726666135\n",
      "Average test loss: 0.004304202329367399\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04605756221877204\n",
      "Average test loss: 0.004209806617349386\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0459504883521133\n",
      "Average test loss: 0.0042159248420761694\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04588446160488659\n",
      "Average test loss: 0.004152652526067363\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04583029508590698\n",
      "Average test loss: 0.004136116361659434\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0457798407541381\n",
      "Average test loss: 0.004321362998336554\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04573479914002948\n",
      "Average test loss: 0.004255436454175247\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04563370135923227\n",
      "Average test loss: 0.004194136483387814\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04563176555766\n",
      "Average test loss: 0.004321871065845092\n",
      "Epoch 164/300\n",
      "Average training loss: 0.045627514637178845\n",
      "Average test loss: 0.004343461133953598\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04547578697403272\n",
      "Average test loss: 0.004237269452669554\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0455081439614296\n",
      "Average test loss: 0.00430919659944872\n",
      "Epoch 167/300\n",
      "Average training loss: 0.045434082143836554\n",
      "Average test loss: 0.00431118283338017\n",
      "Epoch 168/300\n",
      "Average training loss: 0.045313549356328114\n",
      "Average test loss: 0.00432175969498025\n",
      "Epoch 169/300\n",
      "Average training loss: 0.045262346333927575\n",
      "Average test loss: 0.004287499543900291\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04527150118019846\n",
      "Average test loss: 0.004264872445828385\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04517950794763035\n",
      "Average test loss: 0.00437613489644395\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04517627000146442\n",
      "Average test loss: 0.004136308896872732\n",
      "Epoch 173/300\n",
      "Average training loss: 0.045056022483441566\n",
      "Average test loss: 0.004251180541184213\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04509667843911383\n",
      "Average test loss: 0.004242805587748686\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04491899019479752\n",
      "Average test loss: 0.004317174899909231\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04488185081879298\n",
      "Average test loss: 0.004278499778360129\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04488240265846252\n",
      "Average test loss: 0.004318408213555813\n",
      "Epoch 178/300\n",
      "Average training loss: 0.044951194908883836\n",
      "Average test loss: 0.004256967964685625\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04478146939807468\n",
      "Average test loss: 0.004238503699915277\n",
      "Epoch 180/300\n",
      "Average training loss: 0.044737153473827576\n",
      "Average test loss: 0.004234090444114473\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04471061978075239\n",
      "Average test loss: 0.004300072056551774\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04472811982035637\n",
      "Average test loss: 0.004321126813689868\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04462350705597136\n",
      "Average test loss: 0.004400731301969952\n",
      "Epoch 184/300\n",
      "Average training loss: 0.044553193026118806\n",
      "Average test loss: 0.004246894104820159\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04449497236477004\n",
      "Average test loss: 0.004197428991397222\n",
      "Epoch 186/300\n",
      "Average training loss: 0.044458645708031125\n",
      "Average test loss: 0.004339825249794457\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04445089646180471\n",
      "Average test loss: 0.004237016846322351\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04433016148209572\n",
      "Average test loss: 0.004329239555944999\n",
      "Epoch 189/300\n",
      "Average training loss: 0.044296459959612955\n",
      "Average test loss: 0.004186123438179493\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04429447297586335\n",
      "Average test loss: 0.004228088872714175\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04425207207931413\n",
      "Average test loss: 0.004224918424255318\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04416957905888558\n",
      "Average test loss: 0.004189876868493027\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04417638621727626\n",
      "Average test loss: 0.004272281828232937\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04419786228073968\n",
      "Average test loss: 0.004425175454260574\n",
      "Epoch 195/300\n",
      "Average training loss: 0.044059426724910734\n",
      "Average test loss: 0.004202244902236594\n",
      "Epoch 196/300\n",
      "Average training loss: 0.043962885008917915\n",
      "Average test loss: 0.004181531053450372\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0439847159186999\n",
      "Average test loss: 0.004509483906957839\n",
      "Epoch 198/300\n",
      "Average training loss: 0.043915028168095484\n",
      "Average test loss: 0.004225687957472271\n",
      "Epoch 199/300\n",
      "Average training loss: 0.043995786047644085\n",
      "Average test loss: 0.0042602609507739544\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04394289096196492\n",
      "Average test loss: 0.004323012109431956\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04383542266156938\n",
      "Average test loss: 0.0043237383373909526\n",
      "Epoch 202/300\n",
      "Average training loss: 0.043828414004709985\n",
      "Average test loss: 0.004213989092244043\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0437282237874137\n",
      "Average test loss: 0.004267509450722072\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04375241331259409\n",
      "Average test loss: 0.004327647108584643\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04364384259449111\n",
      "Average test loss: 0.004440098808457454\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04359699841340383\n",
      "Average test loss: 0.004375286032756169\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04346642563078139\n",
      "Average test loss: 0.00431170147160689\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04354671528604295\n",
      "Average test loss: 0.004295416349338161\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04353816878795624\n",
      "Average test loss: 0.004261228465578622\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04345955883463224\n",
      "Average test loss: 0.004231150490542253\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04342879183093707\n",
      "Average test loss: 0.004301043956850966\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0433516806628969\n",
      "Average test loss: 0.004289009655101431\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04333885532286432\n",
      "Average test loss: 0.004296163178566429\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04332008119093047\n",
      "Average test loss: 0.004275453318324354\n",
      "Epoch 215/300\n",
      "Average training loss: 0.043309673855702085\n",
      "Average test loss: 0.004169938901232349\n",
      "Epoch 216/300\n",
      "Average training loss: 0.043233502712514665\n",
      "Average test loss: 0.004251644383495052\n",
      "Epoch 217/300\n",
      "Average training loss: 0.043161289948556156\n",
      "Average test loss: 0.0042259336200853185\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04319457796878285\n",
      "Average test loss: 0.0042461332020660245\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04311995198991564\n",
      "Average test loss: 0.004430571630804075\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0431663601398468\n",
      "Average test loss: 0.004321679653392898\n",
      "Epoch 221/300\n",
      "Average training loss: 0.043118820975224174\n",
      "Average test loss: 0.004469417134920756\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0430006123913659\n",
      "Average test loss: 0.0042655981911553275\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04297852288848824\n",
      "Average test loss: 0.004279776853612727\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0429765909049246\n",
      "Average test loss: 0.004281570906854338\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04297481033205986\n",
      "Average test loss: 0.004220429236069322\n",
      "Epoch 226/300\n",
      "Average training loss: 0.042890816489855446\n",
      "Average test loss: 0.004353395883821779\n",
      "Epoch 227/300\n",
      "Average training loss: 0.042741905354791214\n",
      "Average test loss: 0.00430536292741696\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0427193603515625\n",
      "Average test loss: 0.004300777987059619\n",
      "Epoch 230/300\n",
      "Average training loss: 0.042741944001780614\n",
      "Average test loss: 0.00432229391609629\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04266000176800622\n",
      "Average test loss: 0.004735141709860828\n",
      "Epoch 234/300\n",
      "Average training loss: 0.042672675165865155\n",
      "Average test loss: 0.004464217792161637\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04256221026182175\n",
      "Average test loss: 0.004497083891597059\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04253088337182999\n",
      "Average test loss: 0.004222241588350799\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04249372678663996\n",
      "Average test loss: 0.0042523861171470745\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04245719244745043\n",
      "Average test loss: 0.004292238775019845\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04245028970970048\n",
      "Average test loss: 0.004421886129718688\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04239647921588686\n",
      "Average test loss: 0.004307604066613647\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04236201128363609\n",
      "Average test loss: 0.0043747508724530535\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04233260748783747\n",
      "Average test loss: 0.004200843896302912\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0423079842031002\n",
      "Average test loss: 0.004666122332836191\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04223557091752688\n",
      "Average test loss: 0.004368888417465819\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04222014362282223\n",
      "Average test loss: 0.004288102015439007\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04220303824543953\n",
      "Average test loss: 0.004374845319944951\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04210853656464153\n",
      "Average test loss: 0.004391963691347175\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04210169989367326\n",
      "Average test loss: 0.004386867509327001\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04215900655918651\n",
      "Average test loss: 0.004391033646754093\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04207024122609033\n",
      "Average test loss: 0.004138902087178495\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04204808043440183\n",
      "Average test loss: 0.004413228554858102\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04204324296447966\n",
      "Average test loss: 0.004315330725991063\n",
      "Epoch 254/300\n",
      "Average training loss: 0.042002852509419125\n",
      "Average test loss: 0.004275531108180682\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04187727260258463\n",
      "Average test loss: 0.004345255134006341\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04197246977024608\n",
      "Average test loss: 0.0044443346777309975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04193563260303603\n",
      "Average test loss: 0.004281686392923196\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04187144493063291\n",
      "Average test loss: 0.00427350987204247\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04182491858137979\n",
      "Average test loss: 0.004283055164126887\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04184542125794623\n",
      "Average test loss: 0.004352003513938851\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04180059766107135\n",
      "Average test loss: 0.00437074855921997\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04178187895814578\n",
      "Average test loss: 0.004470275988802314\n",
      "Epoch 263/300\n",
      "Average training loss: 0.041732585502995384\n",
      "Average test loss: 0.004485221227837934\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0417191840145323\n",
      "Average test loss: 0.004160646754834387\n",
      "Epoch 265/300\n",
      "Average training loss: 0.041636939068635304\n",
      "Average test loss: 0.004339189313766029\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04167224822607305\n",
      "Average test loss: 0.004322902700967259\n",
      "Epoch 267/300\n",
      "Average training loss: 0.041626609391636316\n",
      "Average test loss: 0.0042622085987693735\n",
      "Epoch 268/300\n",
      "Average training loss: 0.041654525760147304\n",
      "Average test loss: 0.004444043597413434\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04162327210936281\n",
      "Average test loss: 0.004647931795981195\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04156326603558328\n",
      "Average test loss: 0.004239498822639386\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04148561996221543\n",
      "Average test loss: 0.0043947070675591626\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04155609855386946\n",
      "Average test loss: 0.004374850649593605\n",
      "Epoch 273/300\n",
      "Average training loss: 0.041526642481486\n",
      "Average test loss: 0.0043984852764341565\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04144568903909789\n",
      "Average test loss: 0.00434764964464638\n",
      "Epoch 275/300\n",
      "Average training loss: 0.041425308065281975\n",
      "Average test loss: 0.0042860262294610344\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04150448918011453\n",
      "Average test loss: 0.004343220462815629\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04134267927540673\n",
      "Average test loss: 0.004286810649765862\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04126344411571821\n",
      "Average test loss: 0.004403013394110733\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04131105404926671\n",
      "Average test loss: 0.004349599456207619\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0413043055832386\n",
      "Average test loss: 0.004388533214314116\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0412821935945087\n",
      "Average test loss: 0.004294451736017234\n",
      "Epoch 282/300\n",
      "Average training loss: 0.041317729579077825\n",
      "Average test loss: 0.004236745707483755\n",
      "Epoch 283/300\n",
      "Average training loss: 0.041186103436681956\n",
      "Average test loss: 0.004390340772353941\n",
      "Epoch 284/300\n",
      "Average training loss: 0.041192845546536974\n",
      "Average test loss: 0.004344926585753759\n",
      "Epoch 286/300\n",
      "Average training loss: 0.041163974699046875\n",
      "Average test loss: 0.004393719540288051\n",
      "Epoch 287/300\n",
      "Average training loss: 0.041204657822847365\n",
      "Average test loss: 0.004331111788335774\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04116466914282905\n",
      "Average test loss: 0.004308100181528263\n",
      "Epoch 289/300\n",
      "Average training loss: 0.041063002245293724\n",
      "Average test loss: 0.004275694048239125\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0410235713687208\n",
      "Average test loss: 0.0043659199869467155\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04101863197816743\n",
      "Average test loss: 0.00432027903985646\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04102427062061098\n",
      "Average test loss: 0.004435325021959013\n",
      "Epoch 293/300\n",
      "Average training loss: 0.040975443336698746\n",
      "Average test loss: 0.004340840137667126\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04099025368028217\n",
      "Average test loss: 0.004309454175126222\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04093534120255046\n",
      "Average test loss: 0.004414457981371217\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04097189519802729\n",
      "Average test loss: 0.004345511715031333\n",
      "Epoch 297/300\n",
      "Average training loss: 0.040905675768852234\n",
      "Average test loss: 0.0041999092023405765\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04085678154561255\n",
      "Average test loss: 0.004442010710636775\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04083405637741089\n",
      "Average test loss: 0.0044619154934253955\n",
      "Epoch 300/300\n",
      "Average training loss: 3.7943568293253582\n",
      "Average test loss: 0.06320902239986592\n",
      "Epoch 2/300\n",
      "Average training loss: 1.5039281364017063\n",
      "Average test loss: 0.00484436337918871\n",
      "Epoch 3/300\n",
      "Average training loss: 1.0442966194152832\n",
      "Average test loss: 0.006084671013885074\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7743979847696092\n",
      "Average test loss: 0.004031840674372183\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5822710167831845\n",
      "Average test loss: 0.003937530535790656\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4606228028403388\n",
      "Average test loss: 0.0038485920127067303\n",
      "Epoch 7/300\n",
      "Average training loss: 0.37591611912515427\n",
      "Average test loss: 0.0038027751998354993\n",
      "Epoch 8/300\n",
      "Average training loss: 0.3151165774928199\n",
      "Average test loss: 0.003676606208913856\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2673436451223162\n",
      "Average test loss: 0.00400709932256076\n",
      "Epoch 10/300\n",
      "Average training loss: 0.22953448213471306\n",
      "Average test loss: 0.003592173925290505\n",
      "Epoch 11/300\n",
      "Average training loss: 0.19808432369761997\n",
      "Average test loss: 0.0035366810946207907\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14950125716792212\n",
      "Average test loss: 0.003430292951977915\n",
      "Epoch 14/300\n",
      "Average training loss: 0.13075809126430088\n",
      "Average test loss: 0.003408260480190317\n",
      "Epoch 15/300\n",
      "Average training loss: 0.11578378536966112\n",
      "Average test loss: 0.003392060352075431\n",
      "Epoch 16/300\n",
      "Average training loss: 0.10342671152618196\n",
      "Average test loss: 0.0033363911197003392\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08501403125458293\n",
      "Average test loss: 0.0032436948735266925\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07840488033824497\n",
      "Average test loss: 0.0032117217911614314\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07294076122177971\n",
      "Average test loss: 0.0031824773914284174\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06511286772290865\n",
      "Average test loss: 0.003107198499557045\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06217608341905806\n",
      "Average test loss: 0.0032491142751855983\n",
      "Epoch 24/300\n",
      "Average training loss: 0.059854047718975276\n",
      "Average test loss: 0.0030775968929131828\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05801411667797301\n",
      "Average test loss: 0.0030529413535777066\n",
      "Epoch 26/300\n",
      "Average training loss: 0.056546297672722075\n",
      "Average test loss: 0.0030384947280916903\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05516165928045909\n",
      "Average test loss: 0.003011981786953078\n",
      "Epoch 28/300\n",
      "Average training loss: 0.054072338501612346\n",
      "Average test loss: 0.0030244625301824677\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05318413427472114\n",
      "Average test loss: 0.0030131742590003546\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05242965964476268\n",
      "Average test loss: 0.0029989343794683617\n",
      "Epoch 31/300\n",
      "Average training loss: 0.051738073252969315\n",
      "Average test loss: 0.0029998087520814606\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05111103469795651\n",
      "Average test loss: 0.00298445588350296\n",
      "Epoch 33/300\n",
      "Average training loss: 0.05066201913025644\n",
      "Average test loss: 0.0029620253391977815\n",
      "Epoch 34/300\n",
      "Average training loss: 0.050046664582358465\n",
      "Average test loss: 0.002931418968571557\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0495191628171338\n",
      "Average test loss: 0.002928871493579613\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04863698094420963\n",
      "Average test loss: 0.002921853092809518\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04823185908132129\n",
      "Average test loss: 0.0029148237770423293\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04784151410725382\n",
      "Average test loss: 0.002957613972740041\n",
      "Epoch 40/300\n",
      "Average training loss: 0.047422798732916514\n",
      "Average test loss: 0.002952538407718142\n",
      "Epoch 41/300\n",
      "Average training loss: 0.047003107244769735\n",
      "Average test loss: 0.002985838515063127\n",
      "Epoch 42/300\n",
      "Average training loss: 0.046654322266578674\n",
      "Average test loss: 0.0028909727684739562\n",
      "Epoch 43/300\n",
      "Average training loss: 0.046369909303055866\n",
      "Average test loss: 0.0029038500438133877\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04592642625172933\n",
      "Average test loss: 0.0029036941250993147\n",
      "Epoch 45/300\n",
      "Average training loss: 0.045587273114257386\n",
      "Average test loss: 0.0029052704179452524\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04532775663998392\n",
      "Average test loss: 0.0029634943993555175\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04503838578528828\n",
      "Average test loss: 0.0029541211190323036\n",
      "Epoch 48/300\n",
      "Average training loss: 0.044742206378115545\n",
      "Average test loss: 0.0028789843798925478\n",
      "Epoch 49/300\n",
      "Average training loss: 0.044371606330076856\n",
      "Average test loss: 0.002918739677924249\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04405512572659386\n",
      "Average test loss: 0.0030111363950288956\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04338703400890032\n",
      "Average test loss: 0.002892388856245412\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04314477733605438\n",
      "Average test loss: 0.0029645445524818367\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04287851542234421\n",
      "Average test loss: 0.0029647757332358095\n",
      "Epoch 55/300\n",
      "Average training loss: 0.042570650415288076\n",
      "Average test loss: 0.002901071832515299\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04233131169610553\n",
      "Average test loss: 0.002859199034050107\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04202940697471301\n",
      "Average test loss: 0.002891602138678233\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04177157771587372\n",
      "Average test loss: 0.002846077487079634\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04159761641257339\n",
      "Average test loss: 0.0028652902053048213\n",
      "Epoch 60/300\n",
      "Average training loss: 0.041249572647942434\n",
      "Average test loss: 0.002876326702121231\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04099260076549318\n",
      "Average test loss: 0.002946979805206259\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0407459262907505\n",
      "Average test loss: 0.002914544683570663\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04054218729005919\n",
      "Average test loss: 0.002890980157587263\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04028907746738858\n",
      "Average test loss: 0.002954439113744431\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04016369469298257\n",
      "Average test loss: 0.002957650235750609\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03983534536262353\n",
      "Average test loss: 0.0030281723791526424\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03947546543346511\n",
      "Average test loss: 0.0029563335441052914\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0393394217689832\n",
      "Average test loss: 0.00292531933542341\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03906722154551082\n",
      "Average test loss: 0.0029064180329442025\n",
      "Epoch 71/300\n",
      "Average training loss: 0.038897495521439444\n",
      "Average test loss: 0.002989427499887016\n",
      "Epoch 72/300\n",
      "Average training loss: 0.038615347461567986\n",
      "Average test loss: 0.002947112895341383\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03849439616004626\n",
      "Average test loss: 0.0029577923773063553\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03829813205367989\n",
      "Average test loss: 0.002992284129270249\n",
      "Epoch 75/300\n",
      "Average training loss: 0.038216778175698386\n",
      "Average test loss: 0.002914073948541449\n",
      "Epoch 76/300\n",
      "Average training loss: 0.037997167146868176\n",
      "Average test loss: 0.0029193794986026155\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03780353386534585\n",
      "Average test loss: 0.0030958605251378483\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03767025407486492\n",
      "Average test loss: 0.002991867573414412\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03749387292232778\n",
      "Average test loss: 0.002991113399051958\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03716202783915731\n",
      "Average test loss: 0.002923095680980219\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03695970136258337\n",
      "Average test loss: 0.0029411307724399698\n",
      "Epoch 84/300\n",
      "Average training loss: 0.036806802345646754\n",
      "Average test loss: 0.0030096861279259126\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03667709642814265\n",
      "Average test loss: 0.0029994453572564656\n",
      "Epoch 86/300\n",
      "Average training loss: 0.036554712274008326\n",
      "Average test loss: 0.0030207591263784303\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03649292740722497\n",
      "Average test loss: 0.003263769136948718\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03632526003321012\n",
      "Average test loss: 0.002967824552829067\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03612548450628916\n",
      "Average test loss: 0.0031527170818299055\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03611150170697106\n",
      "Average test loss: 0.002950659317481849\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03600046962002913\n",
      "Average test loss: 0.003029667421554526\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03585266678697533\n",
      "Average test loss: 0.0030682856099059183\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03567429173986117\n",
      "Average test loss: 0.003021183766838577\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03552547519405683\n",
      "Average test loss: 0.0030969742654512328\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03547083739108509\n",
      "Average test loss: 0.003059023684097661\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03530009727511141\n",
      "Average test loss: 0.0030345390933669276\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03528459749619166\n",
      "Average test loss: 0.0029527429064942733\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03514559267626868\n",
      "Average test loss: 0.003077430970138974\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03502405577235752\n",
      "Average test loss: 0.003007393289771345\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0349810315767924\n",
      "Average test loss: 0.00304071498165528\n",
      "Epoch 102/300\n",
      "Average training loss: 0.034878495673338575\n",
      "Average test loss: 0.0030157936414082847\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03475493348141511\n",
      "Average test loss: 0.0032201753545345532\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03470446309778426\n",
      "Average test loss: 0.0029950530937769347\n",
      "Epoch 105/300\n",
      "Average training loss: 0.034587787485784956\n",
      "Average test loss: 0.0030954972048186594\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03445131021572484\n",
      "Average test loss: 0.0030668902221239274\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034423999312851165\n",
      "Average test loss: 0.003086603841640883\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03430105262001355\n",
      "Average test loss: 0.0031175469499495293\n",
      "Epoch 110/300\n",
      "Average training loss: 0.034139431216650536\n",
      "Average test loss: 0.003038291014317009\n",
      "Epoch 111/300\n",
      "Average training loss: 0.034045282602310184\n",
      "Average test loss: 0.0031835839150266515\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03397429664267434\n",
      "Average test loss: 0.003091422006487846\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03391058185365465\n",
      "Average test loss: 0.0030223625351985294\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03380320046345393\n",
      "Average test loss: 0.0031866938939525023\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0336834925495916\n",
      "Average test loss: 0.0030498837795522476\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03368056359555986\n",
      "Average test loss: 0.0030983387823734017\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03359757050540712\n",
      "Average test loss: 0.0032730006396563516\n",
      "Epoch 119/300\n",
      "Average training loss: 0.033578748461272984\n",
      "Average test loss: 0.003012614951158563\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03351133621566826\n",
      "Average test loss: 0.0031150364157640272\n",
      "Epoch 121/300\n",
      "Average training loss: 0.033354955779181586\n",
      "Average test loss: 0.00307612675966488\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0333069378832976\n",
      "Average test loss: 0.0032071006678872638\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03333005569709672\n",
      "Average test loss: 0.0030425687469542028\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03314374181131522\n",
      "Average test loss: 0.003158057851406435\n",
      "Epoch 126/300\n",
      "Average training loss: 0.033066303006476824\n",
      "Average test loss: 0.0030463508980141745\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03318533536460665\n",
      "Average test loss: 0.0030740956076317364\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03291987095276515\n",
      "Average test loss: 0.003024203872929017\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03292228110631307\n",
      "Average test loss: 0.0031667680251929496\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03286798827515708\n",
      "Average test loss: 0.0030476045296010043\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03277273178431723\n",
      "Average test loss: 0.003153650370322996\n",
      "Epoch 132/300\n",
      "Average training loss: 0.032753633479277296\n",
      "Average test loss: 0.003074812502082851\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0325994821406073\n",
      "Average test loss: 0.0030306962985131476\n",
      "Epoch 134/300\n",
      "Average training loss: 0.032621122664875456\n",
      "Average test loss: 0.0029870065428937477\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03258626440829701\n",
      "Average test loss: 0.003079955487822493\n",
      "Epoch 136/300\n",
      "Average training loss: 0.032509819813900526\n",
      "Average test loss: 0.003138622990912861\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03253954735563861\n",
      "Average test loss: 0.003174805572670367\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03238773561351829\n",
      "Average test loss: 0.003112803058491813\n",
      "Epoch 140/300\n",
      "Average training loss: 0.032310305751032296\n",
      "Average test loss: 0.0031254164785560635\n",
      "Epoch 141/300\n",
      "Average training loss: 0.032265657951434455\n",
      "Average test loss: 0.003209338759796487\n",
      "Epoch 142/300\n",
      "Average training loss: 0.032184045063124764\n",
      "Average test loss: 0.0031255147920714486\n",
      "Epoch 143/300\n",
      "Average training loss: 0.032076989319589404\n",
      "Average test loss: 0.003099595170468092\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03210722695125474\n",
      "Average test loss: 0.0031486536270628374\n",
      "Epoch 145/300\n",
      "Average training loss: 0.032003423237138326\n",
      "Average test loss: 0.00302554273729523\n",
      "Epoch 146/300\n",
      "Average training loss: 0.032037411289082635\n",
      "Average test loss: 0.003072639839930667\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03196809444493717\n",
      "Average test loss: 0.0031204207452634972\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03196948576304648\n",
      "Average test loss: 0.003098951412571801\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03183407131665283\n",
      "Average test loss: 0.0030972133667932617\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031836782140864266\n",
      "Average test loss: 0.003099833152567347\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03182817805144522\n",
      "Average test loss: 0.0031428780681971046\n",
      "Epoch 152/300\n",
      "Average training loss: 0.031744626608159805\n",
      "Average test loss: 0.0030965897486441665\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031848184269335535\n",
      "Average test loss: 0.0031684847273346453\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03158962119287915\n",
      "Average test loss: 0.003187738332276543\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03163380260599984\n",
      "Average test loss: 0.0032194383152657083\n",
      "Epoch 157/300\n",
      "Average training loss: 0.031455059130986535\n",
      "Average test loss: 0.003116260985119475\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03147329331437747\n",
      "Average test loss: 0.003093908450048831\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03144184296164248\n",
      "Average test loss: 0.003162121102834741\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03131759018699328\n",
      "Average test loss: 0.003093361595645547\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031321140012807315\n",
      "Average test loss: 0.0031665063286200165\n",
      "Epoch 163/300\n",
      "Average training loss: 0.031321496191951965\n",
      "Average test loss: 0.0032519470057967635\n",
      "Epoch 164/300\n",
      "Average training loss: 0.031238521413670645\n",
      "Average test loss: 0.003151751225193342\n",
      "Epoch 165/300\n",
      "Average training loss: 0.031236767273810175\n",
      "Average test loss: 0.003142542629606194\n",
      "Epoch 166/300\n",
      "Average training loss: 0.031122604043947327\n",
      "Average test loss: 0.003200321873442994\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0312745725893312\n",
      "Average test loss: 0.0032490224296020135\n",
      "Epoch 168/300\n",
      "Average training loss: 0.031150999946726692\n",
      "Average test loss: 0.003199038437464171\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03099609825346205\n",
      "Average test loss: 0.0031108555797901417\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030995356096161737\n",
      "Average test loss: 0.003122894929928912\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03086269287102752\n",
      "Average test loss: 0.003162771810260084\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030883618531955613\n",
      "Average test loss: 0.003074922143171231\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030832287751966052\n",
      "Average test loss: 0.0031284675161457726\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030834345098998812\n",
      "Average test loss: 0.0032379999628497497\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030762859745158088\n",
      "Average test loss: 0.003176437289867964\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030769655663106178\n",
      "Average test loss: 0.0032329272298763194\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030693729278114108\n",
      "Average test loss: 0.0031959407582051224\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03071839229265849\n",
      "Average test loss: 0.003192411837478479\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030642811841434903\n",
      "Average test loss: 0.003331068097303311\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03063867524266243\n",
      "Average test loss: 0.0032159115235424703\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030561308431956504\n",
      "Average test loss: 0.0031442889033092393\n",
      "Epoch 184/300\n",
      "Average training loss: 0.030525808624095388\n",
      "Average test loss: 0.003175010168717967\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03054257590075334\n",
      "Average test loss: 0.0031691603304611314\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03052545352776845\n",
      "Average test loss: 0.0031441048458218574\n",
      "Epoch 187/300\n",
      "Average training loss: 0.030481847938564087\n",
      "Average test loss: 0.0031949951748053234\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03040363372862339\n",
      "Average test loss: 0.003260921943725811\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03042173586289088\n",
      "Average test loss: 0.003112178632989526\n",
      "Epoch 190/300\n",
      "Average training loss: 0.030410739719867706\n",
      "Average test loss: 0.0031818723188092313\n",
      "Epoch 191/300\n",
      "Average training loss: 0.030278874528076913\n",
      "Average test loss: 0.0031834089695993396\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030267297118902206\n",
      "Average test loss: 0.003227456802916196\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030260127478175695\n",
      "Average test loss: 0.003201709835065736\n",
      "Epoch 194/300\n",
      "Average training loss: 0.030237949955794548\n",
      "Average test loss: 0.00315347735873527\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03027073551548852\n",
      "Average test loss: 0.0031858762510948713\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03016457759340604\n",
      "Average test loss: 0.0032060420360002255\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030163313994805017\n",
      "Average test loss: 0.003126976886867649\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030170654144552018\n",
      "Average test loss: 0.0032457171173559296\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030095220797591738\n",
      "Average test loss: 0.003092130303589834\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03011312524146504\n",
      "Average test loss: 0.0032037209690444998\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03009026722609997\n",
      "Average test loss: 0.003205782886594534\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029985112748212285\n",
      "Average test loss: 0.0031365045085549356\n",
      "Epoch 203/300\n",
      "Average training loss: 0.029945018059677548\n",
      "Average test loss: 0.0032378322947770356\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02995429993338055\n",
      "Average test loss: 0.0031875783676902454\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029979215463002522\n",
      "Average test loss: 0.0031070789587166574\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029931038142906295\n",
      "Average test loss: 0.0031659598797559737\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02983040791749954\n",
      "Average test loss: 0.0031980009735044506\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029868610511223474\n",
      "Average test loss: 0.003135185413269533\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02988055302699407\n",
      "Average test loss: 0.0031778736917508973\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029819617677066063\n",
      "Average test loss: 0.0032620213193198045\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029772992412249247\n",
      "Average test loss: 0.0031595081318583755\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029733237465222676\n",
      "Average test loss: 0.0032356966100633142\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02977908621231715\n",
      "Average test loss: 0.0032575411804848247\n",
      "Epoch 214/300\n",
      "Average training loss: 0.029654718751708668\n",
      "Average test loss: 0.0031837454839713044\n",
      "Epoch 215/300\n",
      "Average training loss: 0.029677822631266382\n",
      "Average test loss: 0.0032301311985486083\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02966725417971611\n",
      "Average test loss: 0.0031772867091414\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02964132149517536\n",
      "Average test loss: 0.0030838668971425957\n",
      "Epoch 218/300\n",
      "Average training loss: 0.029531735113925404\n",
      "Average test loss: 0.0032475785240530966\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02955938539571232\n",
      "Average test loss: 0.0032176332858701545\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02953831257091628\n",
      "Average test loss: 0.0032047736624048816\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029484927215509944\n",
      "Average test loss: 0.0033028434928920533\n",
      "Epoch 222/300\n",
      "Average training loss: 0.029516097760862774\n",
      "Average test loss: 0.003113351130651103\n",
      "Epoch 223/300\n",
      "Average training loss: 0.029517932012677193\n",
      "Average test loss: 0.0032217649136566454\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02943427782257398\n",
      "Average test loss: 0.0032690297789457774\n",
      "Epoch 225/300\n",
      "Average training loss: 0.029436492264270784\n",
      "Average test loss: 0.0031587408408522604\n",
      "Epoch 226/300\n",
      "Average training loss: 0.029457720435327954\n",
      "Average test loss: 0.0031476725766228304\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029460118883185917\n",
      "Average test loss: 0.003139602961846524\n",
      "Epoch 228/300\n",
      "Average training loss: 0.029326532365547286\n",
      "Average test loss: 0.003189465121469564\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029366217411226695\n",
      "Average test loss: 0.0032889744010236527\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029353195337785614\n",
      "Average test loss: 0.003104555119656854\n",
      "Epoch 231/300\n",
      "Average training loss: 0.029316210336155363\n",
      "Average test loss: 0.003217380422891842\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02932856819199191\n",
      "Average test loss: 0.0032737616192963386\n",
      "Epoch 233/300\n",
      "Average training loss: 0.029249230860008135\n",
      "Average test loss: 0.0032328434363007546\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029266202671660316\n",
      "Average test loss: 0.003168323595697681\n",
      "Epoch 235/300\n",
      "Average training loss: 0.029169567187627157\n",
      "Average test loss: 0.0032357956026163367\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02915601939956347\n",
      "Average test loss: 0.0032071788700090515\n",
      "Epoch 237/300\n",
      "Average training loss: 0.029155766427516938\n",
      "Average test loss: 0.0032909275382343264\n",
      "Epoch 238/300\n",
      "Average training loss: 0.029167442618144884\n",
      "Average test loss: 0.003224460232175059\n",
      "Epoch 239/300\n",
      "Average training loss: 0.029146689592136277\n",
      "Average test loss: 0.0033019745347814427\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02910503534310394\n",
      "Average test loss: 0.0032673430202735795\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02908773832519849\n",
      "Average test loss: 0.003190997363999486\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029077469365464317\n",
      "Average test loss: 0.0034800419143090643\n",
      "Epoch 243/300\n",
      "Average training loss: 0.029052616314755544\n",
      "Average test loss: 0.003127932580601838\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02900839733084043\n",
      "Average test loss: 0.003187118078271548\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02905563503669368\n",
      "Average test loss: 0.003191229339895977\n",
      "Epoch 246/300\n",
      "Average training loss: 0.028935577144225438\n",
      "Average test loss: 0.003276029347959492\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02893644776609209\n",
      "Average test loss: 0.0031586381730933986\n",
      "Epoch 248/300\n",
      "Average training loss: 0.028942572759257424\n",
      "Average test loss: 0.0032199970059510735\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028855693134996627\n",
      "Average test loss: 0.003163704612188869\n",
      "Epoch 250/300\n",
      "Average training loss: 0.028834515084822973\n",
      "Average test loss: 0.0032512008185601898\n",
      "Epoch 251/300\n",
      "Average training loss: 0.028861556654175124\n",
      "Average test loss: 0.0032170365285128354\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02884167799519168\n",
      "Average test loss: 0.003177349725531207\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02885525113509761\n",
      "Average test loss: 0.0032849125375764236\n",
      "Epoch 254/300\n",
      "Average training loss: 0.028888107621007495\n",
      "Average test loss: 0.003231578404497769\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028821108725335862\n",
      "Average test loss: 0.0032662933232883613\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02876330680979623\n",
      "Average test loss: 0.0032729699802067544\n",
      "Epoch 257/300\n",
      "Average training loss: 0.028728802556792894\n",
      "Average test loss: 0.003202542171503107\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02874832199513912\n",
      "Average test loss: 0.0031697101824813417\n",
      "Epoch 259/300\n",
      "Average training loss: 0.028683498231901063\n",
      "Average test loss: 0.0032826931037836606\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0287237209164434\n",
      "Average test loss: 0.0032892802183826764\n",
      "Epoch 261/300\n",
      "Average training loss: 0.028748661347561412\n",
      "Average test loss: 0.0032490820346607104\n",
      "Epoch 262/300\n",
      "Average training loss: 0.028692181065678595\n",
      "Average test loss: 0.0032617710375537476\n",
      "Epoch 263/300\n",
      "Average training loss: 0.028637698175178635\n",
      "Average test loss: 0.003329474524077442\n",
      "Epoch 264/300\n",
      "Average training loss: 0.028619369758499994\n",
      "Average test loss: 0.0031739904714955225\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02862913102739387\n",
      "Average test loss: 0.0032555196699168946\n",
      "Epoch 266/300\n",
      "Average training loss: 0.028676566148797673\n",
      "Average test loss: 0.0032091482252710394\n",
      "Epoch 267/300\n",
      "Average training loss: 0.028587256105409728\n",
      "Average test loss: 0.0031785772041314176\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02859756007624997\n",
      "Average test loss: 0.0031611722194486193\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02853717949986458\n",
      "Average test loss: 0.0032436815448519257\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028528158128261566\n",
      "Average test loss: 0.0033942248467355965\n",
      "Epoch 271/300\n",
      "Average training loss: 0.028500429602132903\n",
      "Average test loss: 0.0033071583989593718\n",
      "Epoch 272/300\n",
      "Average training loss: 0.028441531934671933\n",
      "Average test loss: 0.003212705246690247\n",
      "Epoch 273/300\n",
      "Average training loss: 0.028505023398333124\n",
      "Average test loss: 0.003231980126972\n",
      "Epoch 274/300\n",
      "Average training loss: 0.028540309036771457\n",
      "Average test loss: 0.0032616501831346087\n",
      "Epoch 275/300\n",
      "Average training loss: 0.028414773262209363\n",
      "Average test loss: 0.003204092773919304\n",
      "Epoch 276/300\n",
      "Average training loss: 0.028411974430084228\n",
      "Average test loss: 0.0032537480348514185\n",
      "Epoch 277/300\n",
      "Average training loss: 0.028407162010669708\n",
      "Average test loss: 0.0033242374654445382\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028379749089479447\n",
      "Average test loss: 0.0032905285126633116\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02840475210381879\n",
      "Average test loss: 0.003190957969468501\n",
      "Epoch 280/300\n",
      "Average training loss: 0.028347951046294634\n",
      "Average test loss: 0.003302306118524737\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02829413900938299\n",
      "Average test loss: 0.0032812521107908753\n",
      "Epoch 282/300\n",
      "Average training loss: 0.028319946308930716\n",
      "Average test loss: 0.0032363236103620796\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02835365793440077\n",
      "Average test loss: 0.003220877146969239\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02831242998275492\n",
      "Average test loss: 0.003279539614915848\n",
      "Epoch 285/300\n",
      "Average training loss: 0.028272056059704887\n",
      "Average test loss: 0.0031982496519469554\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02822728474934896\n",
      "Average test loss: 0.003223703796871834\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02827683870328797\n",
      "Average test loss: 0.0032044695358102522\n",
      "Epoch 288/300\n",
      "Average training loss: 0.028176170599129465\n",
      "Average test loss: 0.0033129936858183806\n",
      "Epoch 289/300\n",
      "Average training loss: 0.028193867436713642\n",
      "Average test loss: 0.003232264370554023\n",
      "Epoch 290/300\n",
      "Average training loss: 0.028172795110278658\n",
      "Average test loss: 0.003260577232059505\n",
      "Epoch 291/300\n",
      "Average training loss: 0.028228538662195206\n",
      "Average test loss: 0.0032700499654230142\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02814544911848174\n",
      "Average test loss: 0.003254838894224829\n",
      "Epoch 293/300\n",
      "Average training loss: 0.028151139330532815\n",
      "Average test loss: 0.0032143866107281712\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02813246273663309\n",
      "Average test loss: 0.003174542336414258\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02811905943353971\n",
      "Average test loss: 0.0032624748030470477\n",
      "Epoch 296/300\n",
      "Average training loss: 0.028071563879648844\n",
      "Average test loss: 0.0032451718768311873\n",
      "Epoch 297/300\n",
      "Average training loss: 0.028063789856102733\n",
      "Average test loss: 0.0032965847719460726\n",
      "Epoch 298/300\n",
      "Average training loss: 0.028075036823749542\n",
      "Average test loss: 0.0031954043077097997\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028039915637837518\n",
      "Average test loss: 0.003301440844933192\n",
      "Epoch 300/300\n",
      "Average training loss: 0.028043420435653794\n",
      "Average test loss: 0.003173839407869511\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.279588976012336\n",
      "Average test loss: 0.6157953209810787\n",
      "Epoch 2/300\n",
      "Average training loss: 1.2151244832144843\n",
      "Average test loss: 0.003923846196383238\n",
      "Epoch 3/300\n",
      "Average training loss: 0.7748024012247722\n",
      "Average test loss: 0.0037824147500925592\n",
      "Epoch 4/300\n",
      "Average training loss: 0.5501259539392259\n",
      "Average test loss: 0.00342175725247297\n",
      "Epoch 5/300\n",
      "Average training loss: 0.41962928777270847\n",
      "Average test loss: 0.003297359916071097\n",
      "Epoch 6/300\n",
      "Average training loss: 0.33207520325978596\n",
      "Average test loss: 0.003183267290186551\n",
      "Epoch 7/300\n",
      "Average training loss: 0.2701118651363585\n",
      "Average test loss: 0.0030705528371036054\n",
      "Epoch 8/300\n",
      "Average training loss: 0.22372904687457615\n",
      "Average test loss: 0.005627696227282286\n",
      "Epoch 9/300\n",
      "Average training loss: 0.18829897161324818\n",
      "Average test loss: 0.003044724537887507\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15919166541099547\n",
      "Average test loss: 0.0028421752059625254\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13692796730995177\n",
      "Average test loss: 0.0029447811198317344\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11848647644122441\n",
      "Average test loss: 0.004403270052952899\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10400128269195556\n",
      "Average test loss: 0.0026416120930678314\n",
      "Epoch 14/300\n",
      "Average training loss: 0.092304641339514\n",
      "Average test loss: 0.0026002712815793023\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08254591785536872\n",
      "Average test loss: 0.0025637874288691417\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07458150794108709\n",
      "Average test loss: 0.0024911118804787596\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0682994349333975\n",
      "Average test loss: 0.00251859519071877\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06303518444961971\n",
      "Average test loss: 0.0024229719766105215\n",
      "Epoch 19/300\n",
      "Average training loss: 0.058787247598171236\n",
      "Average test loss: 0.0024246568222426705\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05534544383486112\n",
      "Average test loss: 0.002358130506550272\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05247292557358742\n",
      "Average test loss: 0.0024157354217022656\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05018262782361772\n",
      "Average test loss: 0.002343561624487241\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04821059088740084\n",
      "Average test loss: 0.0022890393001337847\n",
      "Epoch 24/300\n",
      "Average training loss: 0.046590883142418334\n",
      "Average test loss: 0.0022404201776824065\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04522871884041362\n",
      "Average test loss: 0.0022292107387135427\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04414865973591804\n",
      "Average test loss: 0.002242534265646504\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04318893660439385\n",
      "Average test loss: 0.0022301313766381805\n",
      "Epoch 28/300\n",
      "Average training loss: 0.042385583660668795\n",
      "Average test loss: 0.0022061731373477314\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04167570157514678\n",
      "Average test loss: 0.0021982709241824016\n",
      "Epoch 30/300\n",
      "Average training loss: 0.040964990155564415\n",
      "Average test loss: 0.0022578679766092034\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04037838039133284\n",
      "Average test loss: 0.00215344391297549\n",
      "Epoch 32/300\n",
      "Average training loss: 0.039256659428278606\n",
      "Average test loss: 0.002160925751965907\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0386864269706938\n",
      "Average test loss: 0.002142318875218431\n",
      "Epoch 35/300\n",
      "Average training loss: 0.038288408352269064\n",
      "Average test loss: 0.00213730112152795\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03780699040492376\n",
      "Average test loss: 0.0021105215160383117\n",
      "Epoch 37/300\n",
      "Average training loss: 0.037271788030862806\n",
      "Average test loss: 0.0021218328231738673\n",
      "Epoch 38/300\n",
      "Average training loss: 0.036911443637477025\n",
      "Average test loss: 0.002127486697708567\n",
      "Epoch 39/300\n",
      "Average training loss: 0.036536436387234264\n",
      "Average test loss: 0.0020960409943428303\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03605700824989213\n",
      "Average test loss: 0.0020873248654728134\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03560254768033822\n",
      "Average test loss: 0.0021013411446991893\n",
      "Epoch 42/300\n",
      "Average training loss: 0.035245150064428646\n",
      "Average test loss: 0.0021037992459411424\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03489686847395367\n",
      "Average test loss: 0.0021209394915236365\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03454088602794541\n",
      "Average test loss: 0.0021259230139354865\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03413125019272169\n",
      "Average test loss: 0.0021300569858608972\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03380698717468315\n",
      "Average test loss: 0.002068219980224967\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03376390261616972\n",
      "Average test loss: 0.0020632417572455275\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0331918354978164\n",
      "Average test loss: 0.002123295867194732\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03284785030947791\n",
      "Average test loss: 0.002067229977498452\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03282398715615273\n",
      "Average test loss: 0.002080914131883118\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03228728514744176\n",
      "Average test loss: 0.0021058283888010514\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03199498250087102\n",
      "Average test loss: 0.0021125326173173056\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03167584786481328\n",
      "Average test loss: 0.0021076623778790234\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03143884735637241\n",
      "Average test loss: 0.0020843646015144056\n",
      "Epoch 55/300\n",
      "Average training loss: 0.031199149650004174\n",
      "Average test loss: 0.002086458053646816\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03087861474023925\n",
      "Average test loss: 0.0020563325275563533\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0307316547996468\n",
      "Average test loss: 0.0020912477746605875\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03046717890103658\n",
      "Average test loss: 0.0020813413127842876\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030185161377820704\n",
      "Average test loss: 0.0021318919009839497\n",
      "Epoch 60/300\n",
      "Average training loss: 0.02989409736295541\n",
      "Average test loss: 0.0020804019131594235\n",
      "Epoch 61/300\n",
      "Average training loss: 0.029749139216211105\n",
      "Average test loss: 0.002083570020687249\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02966773769342237\n",
      "Average test loss: 0.002133365906981958\n",
      "Epoch 63/300\n",
      "Average training loss: 0.029339164767000412\n",
      "Average test loss: 0.002086272711141242\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029082980344692866\n",
      "Average test loss: 0.002156633478692836\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0288584546364016\n",
      "Average test loss: 0.0021251245436982977\n",
      "Epoch 66/300\n",
      "Average training loss: 0.028730532970693376\n",
      "Average test loss: 0.00212850117093573\n",
      "Epoch 67/300\n",
      "Average training loss: 0.028498764961957933\n",
      "Average test loss: 0.0022828013234668307\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02830839587830835\n",
      "Average test loss: 0.0021898434224228063\n",
      "Epoch 69/300\n",
      "Average training loss: 0.028174259449044864\n",
      "Average test loss: 0.0021284646052453253\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02798501647180981\n",
      "Average test loss: 0.0020964496484440235\n",
      "Epoch 71/300\n",
      "Average training loss: 0.027849918784366715\n",
      "Average test loss: 0.002139816711242828\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02777560756769445\n",
      "Average test loss: 0.002113380906689498\n",
      "Epoch 73/300\n",
      "Average training loss: 0.027536633772982492\n",
      "Average test loss: 0.0022190051765905486\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027459298570950825\n",
      "Average test loss: 0.00212220528587285\n",
      "Epoch 75/300\n",
      "Average training loss: 0.027181830894615917\n",
      "Average test loss: 0.002128402622313135\n",
      "Epoch 76/300\n",
      "Average training loss: 0.027123424213793542\n",
      "Average test loss: 0.0021143822831412155\n",
      "Epoch 77/300\n",
      "Average training loss: 0.027026672005653382\n",
      "Average test loss: 0.0021248555251707634\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02691187828613652\n",
      "Average test loss: 0.002169594129961398\n",
      "Epoch 79/300\n",
      "Average training loss: 0.026691474970844058\n",
      "Average test loss: 0.00213176628947258\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02651934761967924\n",
      "Average test loss: 0.0021946153924283056\n",
      "Epoch 81/300\n",
      "Average training loss: 0.026517287800709406\n",
      "Average test loss: 0.0021340867986695635\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026377358146839672\n",
      "Average test loss: 0.0021118402897587255\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0262485880272256\n",
      "Average test loss: 0.0022573890027900536\n",
      "Epoch 84/300\n",
      "Average training loss: 0.026132551749547324\n",
      "Average test loss: 0.0022407774349881542\n",
      "Epoch 85/300\n",
      "Average training loss: 0.025996792030003334\n",
      "Average test loss: 0.002372805640515354\n",
      "Epoch 86/300\n",
      "Average training loss: 0.025993008211255074\n",
      "Average test loss: 0.002137893687933683\n",
      "Epoch 87/300\n",
      "Average training loss: 0.025833104393548436\n",
      "Average test loss: 0.0022374650671457253\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025698846345146496\n",
      "Average test loss: 0.0021080196770942874\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02563030378189352\n",
      "Average test loss: 0.002106623365647263\n",
      "Epoch 90/300\n",
      "Average training loss: 0.025533249042100376\n",
      "Average test loss: 0.0021699881537093056\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025475382606188455\n",
      "Average test loss: 0.0021693583112210035\n",
      "Epoch 92/300\n",
      "Average training loss: 0.025372855911652246\n",
      "Average test loss: 0.0022621864530568323\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02524050814244482\n",
      "Average test loss: 0.002218617679240803\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025196737959980963\n",
      "Average test loss: 0.0022716137015571197\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02509597704642349\n",
      "Average test loss: 0.0022023600505457984\n",
      "Epoch 96/300\n",
      "Average training loss: 0.025052622927559748\n",
      "Average test loss: 0.002164430887955758\n",
      "Epoch 97/300\n",
      "Average training loss: 0.025002734158602026\n",
      "Average test loss: 0.002174622555573781\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024952124352256456\n",
      "Average test loss: 0.0022977953049250774\n",
      "Epoch 99/300\n",
      "Average training loss: 0.024767371932665507\n",
      "Average test loss: 0.0021751629056202043\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02472592132455773\n",
      "Average test loss: 0.002252480440048708\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02464171857304043\n",
      "Average test loss: 0.002216516747656796\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02463710520995988\n",
      "Average test loss: 0.00221163651585165\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02456731515791681\n",
      "Average test loss: 0.002161020322806305\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024461896095010968\n",
      "Average test loss: 0.0022631639774060913\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024394243866205215\n",
      "Average test loss: 0.0022822630109472406\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02430024431894223\n",
      "Average test loss: 0.002246658281630112\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024294698673817845\n",
      "Average test loss: 0.002313580932509568\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024179456773731442\n",
      "Average test loss: 0.00222304918203089\n",
      "Epoch 109/300\n",
      "Average training loss: 0.024183116821779146\n",
      "Average test loss: 0.0022188960399685633\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024097943123843935\n",
      "Average test loss: 0.0022564478535205125\n",
      "Epoch 111/300\n",
      "Average training loss: 0.024001076829102304\n",
      "Average test loss: 0.0021533160177576874\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02394720554186238\n",
      "Average test loss: 0.0022851260900497434\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023924973633554246\n",
      "Average test loss: 0.0022653014750944243\n",
      "Epoch 114/300\n",
      "Average training loss: 0.023855500044094192\n",
      "Average test loss: 0.002302717957852615\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02381517246696684\n",
      "Average test loss: 0.002168017361933986\n",
      "Epoch 116/300\n",
      "Average training loss: 0.023776302309499845\n",
      "Average test loss: 0.0022295213480376533\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02370586488975419\n",
      "Average test loss: 0.0023008932651331026\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02365384321080314\n",
      "Average test loss: 0.0024436185364094046\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02357400426434146\n",
      "Average test loss: 0.002217954171821475\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02354214441114002\n",
      "Average test loss: 0.002276215211177866\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023533858916825717\n",
      "Average test loss: 0.0022559673140446347\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0234652999970648\n",
      "Average test loss: 0.002201975469270514\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023390837978985574\n",
      "Average test loss: 0.00223724566358659\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023340946555137635\n",
      "Average test loss: 0.0022884163750956457\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02328431794544061\n",
      "Average test loss: 0.0023021286591473553\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02330902299284935\n",
      "Average test loss: 0.0022670754530570575\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02320072541468673\n",
      "Average test loss: 0.0022578675330926974\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02314127070042822\n",
      "Average test loss: 0.0022580801174044607\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0231803906576501\n",
      "Average test loss: 0.0022321118168118926\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02306433542735047\n",
      "Average test loss: 0.002287155608750052\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023040832416878806\n",
      "Average test loss: 0.0022898464983122217\n",
      "Epoch 132/300\n",
      "Average training loss: 0.022998284002145132\n",
      "Average test loss: 0.0022550434799244005\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0229749054097467\n",
      "Average test loss: 0.0022358124612106216\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02289346521596114\n",
      "Average test loss: 0.002292386152678066\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02293335680332449\n",
      "Average test loss: 0.0022414075099966594\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02287578772670693\n",
      "Average test loss: 0.002399905329570174\n",
      "Epoch 137/300\n",
      "Average training loss: 0.022862072659863366\n",
      "Average test loss: 0.00221566485054791\n",
      "Epoch 138/300\n",
      "Average training loss: 0.022771826419565412\n",
      "Average test loss: 0.002234818028079139\n",
      "Epoch 139/300\n",
      "Average training loss: 0.022750469611750708\n",
      "Average test loss: 0.0022669229195970628\n",
      "Epoch 140/300\n",
      "Average training loss: 0.022697168411480055\n",
      "Average test loss: 0.0023293207043574918\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02266608431107468\n",
      "Average test loss: 0.0022335388347920443\n",
      "Epoch 142/300\n",
      "Average training loss: 0.022641552628742324\n",
      "Average test loss: 0.002337211013875074\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022590502424372567\n",
      "Average test loss: 0.002242276543337438\n",
      "Epoch 144/300\n",
      "Average training loss: 0.022563506422771346\n",
      "Average test loss: 0.002243216660287645\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022575700463520157\n",
      "Average test loss: 0.0022534918058663604\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02248961947361628\n",
      "Average test loss: 0.0022642814686728847\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02250832918120755\n",
      "Average test loss: 0.0023120471395345196\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022441601566142506\n",
      "Average test loss: 0.0022358288827041784\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02241167873309718\n",
      "Average test loss: 0.0022748165549710394\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02238892034937938\n",
      "Average test loss: 0.0022427290845662357\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02233647051619159\n",
      "Average test loss: 0.002424000510428515\n",
      "Epoch 152/300\n",
      "Average training loss: 0.022321756367882094\n",
      "Average test loss: 0.0022971960126111904\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02225065002259281\n",
      "Average test loss: 0.002315437287092209\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02225096984124846\n",
      "Average test loss: 0.002295610246558984\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02222944889134831\n",
      "Average test loss: 0.0023074242384690377\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0221725076271428\n",
      "Average test loss: 0.0022272140571019715\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022191078239017063\n",
      "Average test loss: 0.0023478802636058794\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02215540064043469\n",
      "Average test loss: 0.002335675950369073\n",
      "Epoch 159/300\n",
      "Average training loss: 0.022105740484264162\n",
      "Average test loss: 0.002225969939182202\n",
      "Epoch 160/300\n",
      "Average training loss: 0.022049959253933694\n",
      "Average test loss: 0.002292194799106154\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02200999376343356\n",
      "Average test loss: 0.002300379183143377\n",
      "Epoch 162/300\n",
      "Average training loss: 0.022049977945784727\n",
      "Average test loss: 0.002311914848577645\n",
      "Epoch 163/300\n",
      "Average training loss: 0.021985769675837624\n",
      "Average test loss: 0.0022794692895064753\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021956589993503358\n",
      "Average test loss: 0.002324784387110008\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021911837165554365\n",
      "Average test loss: 0.0023182730846520927\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02187261040508747\n",
      "Average test loss: 0.002300590527140432\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021855833719174066\n",
      "Average test loss: 0.00234180759307411\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02183541941973898\n",
      "Average test loss: 0.0023343337890174656\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02184064834813277\n",
      "Average test loss: 0.002296289866997136\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021802509177062248\n",
      "Average test loss: 0.002317808497697115\n",
      "Epoch 171/300\n",
      "Average training loss: 0.021787636094623142\n",
      "Average test loss: 0.00232528844786187\n",
      "Epoch 172/300\n",
      "Average training loss: 0.021777761648098626\n",
      "Average test loss: 0.002245995833641953\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021724367194705538\n",
      "Average test loss: 0.002368748210370541\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021669715757171314\n",
      "Average test loss: 0.0023294777841203745\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02176313097609414\n",
      "Average test loss: 0.0023135034733762342\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021670623757772976\n",
      "Average test loss: 0.002346908002057009\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021618365633818838\n",
      "Average test loss: 0.002315848936016361\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021592947021126747\n",
      "Average test loss: 0.002317962853031026\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021618236776855256\n",
      "Average test loss: 0.002289085826733046\n",
      "Epoch 180/300\n",
      "Average training loss: 0.021572347202234797\n",
      "Average test loss: 0.002369640883886152\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021508402216765615\n",
      "Average test loss: 0.0023923902050074605\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021502397791379028\n",
      "Average test loss: 0.0023919484031697112\n",
      "Epoch 183/300\n",
      "Average training loss: 0.021499503360854256\n",
      "Average test loss: 0.002345509629489647\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02148316049410237\n",
      "Average test loss: 0.00228143732674006\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021455354806449677\n",
      "Average test loss: 0.002273900010726518\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021412080517245663\n",
      "Average test loss: 0.00227734000608325\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021394889388647344\n",
      "Average test loss: 0.002283193161090215\n",
      "Epoch 188/300\n",
      "Average training loss: 0.021379538868864376\n",
      "Average test loss: 0.0023375671058893203\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021370829090476037\n",
      "Average test loss: 0.002294840021058917\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021355080970459513\n",
      "Average test loss: 0.0023152634773610367\n",
      "Epoch 191/300\n",
      "Average training loss: 0.021285042961438495\n",
      "Average test loss: 0.002306423778231773\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0213180916061004\n",
      "Average test loss: 0.0023369259128554\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021317270123296313\n",
      "Average test loss: 0.0023489265566070876\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021303816616535187\n",
      "Average test loss: 0.0023445168605281245\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02122910837166839\n",
      "Average test loss: 0.002274514655686087\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021184721731477313\n",
      "Average test loss: 0.00230665339405338\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02121059070361985\n",
      "Average test loss: 0.002353235510695312\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021198758161730235\n",
      "Average test loss: 0.0023271904749174914\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021157161687811215\n",
      "Average test loss: 0.0023040375552243657\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02113262782825364\n",
      "Average test loss: 0.002342338201072481\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02109683591624101\n",
      "Average test loss: 0.0023476505325072342\n",
      "Epoch 202/300\n",
      "Average training loss: 0.021118147000670433\n",
      "Average test loss: 0.0023432376163287296\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021092253290944628\n",
      "Average test loss: 0.002301771330750651\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02110688537855943\n",
      "Average test loss: 0.0023318629612525303\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021031155560165642\n",
      "Average test loss: 0.0023529664162132476\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02103630753358205\n",
      "Average test loss: 0.0024170629017882875\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02101877524620957\n",
      "Average test loss: 0.0024042518210286895\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020943943194217152\n",
      "Average test loss: 0.0023353721822301546\n",
      "Epoch 209/300\n",
      "Average training loss: 0.020979536328050826\n",
      "Average test loss: 0.0023307554407252205\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02095458025071356\n",
      "Average test loss: 0.002325740469309191\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02104799400601122\n",
      "Average test loss: 0.00244064209693008\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020945325965682667\n",
      "Average test loss: 0.0024199250501890975\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02093887767692407\n",
      "Average test loss: 0.00238597638780872\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020835895761847495\n",
      "Average test loss: 0.0023236722410139112\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020854985430008834\n",
      "Average test loss: 0.0024045472116106087\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020826050851080153\n",
      "Average test loss: 0.0023406230044654675\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0208465013752381\n",
      "Average test loss: 0.0023354849305210843\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02082855898141861\n",
      "Average test loss: 0.002291063497670823\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020797679071625073\n",
      "Average test loss: 0.002454230320950349\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020791895386245516\n",
      "Average test loss: 0.0024095337241888046\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02077249663074811\n",
      "Average test loss: 0.0023454229245479736\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020803873401549128\n",
      "Average test loss: 0.0023502146444386906\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02070912307831976\n",
      "Average test loss: 0.0024073574340177906\n",
      "Epoch 224/300\n",
      "Average training loss: 0.020694284913440546\n",
      "Average test loss: 0.0023337851578576698\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02072001539170742\n",
      "Average test loss: 0.0023468374961780176\n",
      "Epoch 226/300\n",
      "Average training loss: 0.020663314811057515\n",
      "Average test loss: 0.002328381697336833\n",
      "Epoch 227/300\n",
      "Average training loss: 0.020730190989043978\n",
      "Average test loss: 0.0022924310591899686\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02071331972214911\n",
      "Average test loss: 0.0022868460873659287\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020635595296820006\n",
      "Average test loss: 0.0023949366567863357\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02065641060305966\n",
      "Average test loss: 0.0024654472126728957\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020614011420143977\n",
      "Average test loss: 0.0023694091023256383\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020589160109559693\n",
      "Average test loss: 0.0023297728266980914\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020575871609979205\n",
      "Average test loss: 0.002351921726846033\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020574080848859415\n",
      "Average test loss: 0.0023879635619620484\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020558675285842685\n",
      "Average test loss: 0.002454235779328479\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02050369591265917\n",
      "Average test loss: 0.0023710559736937285\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02052369015581078\n",
      "Average test loss: 0.002395012544364565\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02051219728257921\n",
      "Average test loss: 0.00238914003657798\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020516801052623324\n",
      "Average test loss: 0.0022824928276240827\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020452840798430974\n",
      "Average test loss: 0.0023007729864782758\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02045985073182318\n",
      "Average test loss: 0.002355230788182881\n",
      "Epoch 242/300\n",
      "Average training loss: 0.020489882323477003\n",
      "Average test loss: 0.002389960950654414\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020367952997485797\n",
      "Average test loss: 0.0023265172844338746\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020432090488572915\n",
      "Average test loss: 0.0024476391707236567\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02038168608976735\n",
      "Average test loss: 0.0023223312865528795\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02038867411679692\n",
      "Average test loss: 0.002421783251894845\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02039539877904786\n",
      "Average test loss: 0.002404237920211421\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020366182445651956\n",
      "Average test loss: 0.002382988220701615\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02038120938837528\n",
      "Average test loss: 0.0023622308559715748\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02033283410800828\n",
      "Average test loss: 0.0023691271711140873\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02036619918379519\n",
      "Average test loss: 0.002420624273725682\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02029021942946646\n",
      "Average test loss: 0.002393020967228545\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020304798972275523\n",
      "Average test loss: 0.0024099523114661375\n",
      "Epoch 254/300\n",
      "Average training loss: 0.020277461565203138\n",
      "Average test loss: 0.0023668653793219065\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02025089420295424\n",
      "Average test loss: 0.002414586210830344\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02023934435016579\n",
      "Average test loss: 0.0026424736538901926\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020327596990598572\n",
      "Average test loss: 0.002288299831251303\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020216903165810637\n",
      "Average test loss: 0.0024466797353492843\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020178079798817635\n",
      "Average test loss: 0.002405853211051888\n",
      "Epoch 260/300\n",
      "Average training loss: 0.020238223358988762\n",
      "Average test loss: 0.002341907546338108\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02021697430478202\n",
      "Average test loss: 0.0023660499043762686\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02022553729845418\n",
      "Average test loss: 0.0023705662284046413\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020182515046662755\n",
      "Average test loss: 0.0023404772707985508\n",
      "Epoch 264/300\n",
      "Average training loss: 0.020162931034962337\n",
      "Average test loss: 0.002396343929279182\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02017207298345036\n",
      "Average test loss: 0.002409995515520374\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02011695359316137\n",
      "Average test loss: 0.002387875545562969\n",
      "Epoch 267/300\n",
      "Average training loss: 0.020112156450748445\n",
      "Average test loss: 0.0023524907316184707\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020124297065867318\n",
      "Average test loss: 0.0024222739179515177\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020080989695257612\n",
      "Average test loss: 0.0023952814845575226\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020112589582800867\n",
      "Average test loss: 0.002378114372284876\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02010906952785121\n",
      "Average test loss: 0.0023923430610448123\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02004587019979954\n",
      "Average test loss: 0.0023358507878664466\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02005258580380016\n",
      "Average test loss: 0.0023538061374177533\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020026523859964476\n",
      "Average test loss: 0.0024273308583845697\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02006241518424617\n",
      "Average test loss: 0.0023428671072340673\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02000029148409764\n",
      "Average test loss: 0.002358781057720383\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020053359157509273\n",
      "Average test loss: 0.002392661182830731\n",
      "Epoch 278/300\n",
      "Average training loss: 0.019983052228887875\n",
      "Average test loss: 0.0024012257715480194\n",
      "Epoch 279/300\n",
      "Average training loss: 0.020029736149642204\n",
      "Average test loss: 0.0023656798681865134\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019925380672845574\n",
      "Average test loss: 0.0024029667367123893\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01996347725060251\n",
      "Average test loss: 0.002427477809910973\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01995632529921002\n",
      "Average test loss: 0.0023339584747122393\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01992923640873697\n",
      "Average test loss: 0.0024151914838908446\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019962417173716756\n",
      "Average test loss: 0.0025093982515649663\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01992510389453835\n",
      "Average test loss: 0.0024517861914096608\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0198798173599773\n",
      "Average test loss: 0.0023923792418920333\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019912645151217778\n",
      "Average test loss: 0.0023804296971195273\n",
      "Epoch 288/300\n",
      "Average training loss: 0.019889086744851535\n",
      "Average test loss: 0.0023067314752067128\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019898410563667614\n",
      "Average test loss: 0.0024091160868604977\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01985186682641506\n",
      "Average test loss: 0.0023582724668085573\n",
      "Epoch 291/300\n",
      "Average training loss: 0.019858644131157132\n",
      "Average test loss: 0.0023636000280578932\n",
      "Epoch 292/300\n",
      "Average training loss: 0.019881842414538064\n",
      "Average test loss: 0.0023912771087553765\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019822700975669755\n",
      "Average test loss: 0.002446128014785548\n",
      "Epoch 294/300\n",
      "Average training loss: 0.019795601011150413\n",
      "Average test loss: 0.0023600687688837448\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0198463658110963\n",
      "Average test loss: 0.0023884881821771464\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019789953785638015\n",
      "Average test loss: 0.002394332801302274\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019800678976707987\n",
      "Average test loss: 0.00243359259609133\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019776538252830506\n",
      "Average test loss: 0.0023654150190866655\n",
      "Epoch 299/300\n",
      "Average training loss: 0.019745223821037345\n",
      "Average test loss: 0.0024179546535015107\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01978860531002283\n",
      "Average test loss: 0.0024569246157382925\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.738170508172777\n",
      "Average test loss: 0.004749629087332222\n",
      "Epoch 2/300\n",
      "Average training loss: 1.23212294377221\n",
      "Average test loss: 0.004679870619128148\n",
      "Epoch 3/300\n",
      "Average training loss: 0.884095764848921\n",
      "Average test loss: 0.0032481222711503504\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6836041537920634\n",
      "Average test loss: 0.0028350654397573736\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5586010728147295\n",
      "Average test loss: 0.0027199995981322393\n",
      "Epoch 6/300\n",
      "Average training loss: 0.46293935566478306\n",
      "Average test loss: 0.0027137872154513994\n",
      "Epoch 7/300\n",
      "Average training loss: 0.38701293852594165\n",
      "Average test loss: 0.0025508309294366175\n",
      "Epoch 8/300\n",
      "Average training loss: 0.326611097600725\n",
      "Average test loss: 0.002406815139369832\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2789015308221181\n",
      "Average test loss: 0.0022797406771116785\n",
      "Epoch 10/300\n",
      "Average training loss: 0.23762051637967427\n",
      "Average test loss: 0.0022701973074840177\n",
      "Epoch 11/300\n",
      "Average training loss: 0.20469212427404193\n",
      "Average test loss: 0.002199916137382388\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1762684625917011\n",
      "Average test loss: 0.0022404916170570585\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1517816068927447\n",
      "Average test loss: 0.002048261483406855\n",
      "Epoch 14/300\n",
      "Average training loss: 0.13033110549052557\n",
      "Average test loss: 0.001970694072751535\n",
      "Epoch 15/300\n",
      "Average training loss: 0.11226623506678475\n",
      "Average test loss: 0.001915779144813617\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09713038247161442\n",
      "Average test loss: 0.0020141868870705367\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08441746496491961\n",
      "Average test loss: 0.001841289686349531\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07401546882258521\n",
      "Average test loss: 0.0018085393940822946\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06549539680282275\n",
      "Average test loss: 0.0017649125704127882\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05870832708809111\n",
      "Average test loss: 0.0017300660943405496\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05321620699928867\n",
      "Average test loss: 0.0017392585666643249\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04877158786522018\n",
      "Average test loss: 0.0016944941538903449\n",
      "Epoch 23/300\n",
      "Average training loss: 0.045223852270179325\n",
      "Average test loss: 0.001675954933795664\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04241954123973846\n",
      "Average test loss: 0.0016519633607111044\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04012711494167646\n",
      "Average test loss: 0.0016627689208835363\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03830104842782021\n",
      "Average test loss: 0.0016041565366710225\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03678638241357274\n",
      "Average test loss: 0.0015656412293513617\n",
      "Epoch 28/300\n",
      "Average training loss: 0.035513560684190856\n",
      "Average test loss: 0.0015583304413076904\n",
      "Epoch 29/300\n",
      "Average training loss: 0.034465563986036514\n",
      "Average test loss: 0.0015532838625626432\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03350705892344316\n",
      "Average test loss: 0.0015451374685184823\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03269122054345078\n",
      "Average test loss: 0.0016686557366823156\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03199038341310289\n",
      "Average test loss: 0.0015426850800092022\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03133384682734807\n",
      "Average test loss: 0.0015298417202817896\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03073254292872217\n",
      "Average test loss: 0.001518294841878944\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03014695511261622\n",
      "Average test loss: 0.0015947323606556488\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02964755869905154\n",
      "Average test loss: 0.0014879561765119434\n",
      "Epoch 37/300\n",
      "Average training loss: 0.029171743396255705\n",
      "Average test loss: 0.0014781516038088335\n",
      "Epoch 38/300\n",
      "Average training loss: 0.028731530328591666\n",
      "Average test loss: 0.0014695353257573314\n",
      "Epoch 39/300\n",
      "Average training loss: 0.028269032327665224\n",
      "Average test loss: 0.0014773966748681333\n",
      "Epoch 40/300\n",
      "Average training loss: 0.027820930189556545\n",
      "Average test loss: 0.0014716411257783572\n",
      "Epoch 41/300\n",
      "Average training loss: 0.027478577567471397\n",
      "Average test loss: 0.0014551012180745602\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02705643848412567\n",
      "Average test loss: 0.0014590786105642716\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02665625624027517\n",
      "Average test loss: 0.001468376212546395\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02628358442750242\n",
      "Average test loss: 0.0014788711468378703\n",
      "Epoch 45/300\n",
      "Average training loss: 0.025944366024600134\n",
      "Average test loss: 0.0015277802632707688\n",
      "Epoch 46/300\n",
      "Average training loss: 0.025604534041550425\n",
      "Average test loss: 0.0015211987872090604\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025221941512491967\n",
      "Average test loss: 0.0015568298380821944\n",
      "Epoch 48/300\n",
      "Average training loss: 0.024951859475837813\n",
      "Average test loss: 0.001438301754080587\n",
      "Epoch 49/300\n",
      "Average training loss: 0.024651391900248\n",
      "Average test loss: 0.0015180707708415057\n",
      "Epoch 50/300\n",
      "Average training loss: 0.024355020536316767\n",
      "Average test loss: 0.0014879303792905477\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02412052857047982\n",
      "Average test loss: 0.0014382452925460207\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02386283368534512\n",
      "Average test loss: 0.001447803164832294\n",
      "Epoch 53/300\n",
      "Average training loss: 0.023518263551923962\n",
      "Average test loss: 0.0014397126723908716\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023363241255283357\n",
      "Average test loss: 0.0014353319715915455\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02305895836651325\n",
      "Average test loss: 0.001445590037231644\n",
      "Epoch 56/300\n",
      "Average training loss: 0.022801382328073185\n",
      "Average test loss: 0.0014472999087431365\n",
      "Epoch 57/300\n",
      "Average training loss: 0.022586182597610686\n",
      "Average test loss: 0.001434141500853002\n",
      "Epoch 58/300\n",
      "Average training loss: 0.022368113464779325\n",
      "Average test loss: 0.0014735016843510998\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0221710873130295\n",
      "Average test loss: 0.0014443407491263416\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021960415323575337\n",
      "Average test loss: 0.0014413264869815774\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021835654265350765\n",
      "Average test loss: 0.0014521322621860438\n",
      "Epoch 62/300\n",
      "Average training loss: 0.021551446600920624\n",
      "Average test loss: 0.0014684781153789825\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02138980854054292\n",
      "Average test loss: 0.0014612201186828316\n",
      "Epoch 64/300\n",
      "Average training loss: 0.021232532522744603\n",
      "Average test loss: 0.0014480072745225495\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02110166030956639\n",
      "Average test loss: 0.0015344421660734547\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020906083246072134\n",
      "Average test loss: 0.0014775677702079217\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0209331549902757\n",
      "Average test loss: 0.0014853037438458867\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020652047018210093\n",
      "Average test loss: 0.0014410799022557008\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0203875808748934\n",
      "Average test loss: 0.001461329600566791\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020307274316747982\n",
      "Average test loss: 0.0015321049191471604\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019946288022730085\n",
      "Average test loss: 0.0015661886188512047\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01978035711083147\n",
      "Average test loss: 0.0014808695045196347\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019691290746132534\n",
      "Average test loss: 0.0014978083735331894\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019523162325223286\n",
      "Average test loss: 0.0014659818388625151\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019486815290318597\n",
      "Average test loss: 0.001482146634409825\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019257994023462136\n",
      "Average test loss: 0.0015894072306238943\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01919006200134754\n",
      "Average test loss: 0.001485835308002101\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019084977260894247\n",
      "Average test loss: 0.0014514815682131383\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018976727417773672\n",
      "Average test loss: 0.0015169695624046855\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01897797574268447\n",
      "Average test loss: 0.0014923824503396948\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018789400807685323\n",
      "Average test loss: 0.0015272972556865877\n",
      "Epoch 84/300\n",
      "Average training loss: 0.018720361108581225\n",
      "Average test loss: 0.0015052754400918882\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018509150417314636\n",
      "Average test loss: 0.0015513142108813755\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018534522683256203\n",
      "Average test loss: 0.001505436841553698\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018481553305354382\n",
      "Average test loss: 0.001522161065083411\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018315337618192037\n",
      "Average test loss: 0.0015185398645699024\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01819675177998013\n",
      "Average test loss: 0.0015222995169460773\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018170264811151558\n",
      "Average test loss: 0.0014945413489929505\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018082704751027957\n",
      "Average test loss: 0.0015554198919691972\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018009562558597987\n",
      "Average test loss: 0.0015772455361568265\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017918781416283714\n",
      "Average test loss: 0.0015435484289709064\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017846777699059912\n",
      "Average test loss: 0.0015171694720370903\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0178390895947814\n",
      "Average test loss: 0.001556345948845976\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01768686574863063\n",
      "Average test loss: 0.0015486888278068768\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017612122586203947\n",
      "Average test loss: 0.0015489291499058404\n",
      "Epoch 100/300\n",
      "Average training loss: 0.017574485192696252\n",
      "Average test loss: 0.0015327360165408915\n",
      "Epoch 101/300\n",
      "Average training loss: 0.017527849400209055\n",
      "Average test loss: 0.0015117779335834913\n",
      "Epoch 102/300\n",
      "Average training loss: 0.017480593245890404\n",
      "Average test loss: 0.0016117498547666602\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01742437241309219\n",
      "Average test loss: 0.0015495718786906864\n",
      "Epoch 104/300\n",
      "Average training loss: 0.017326470591127872\n",
      "Average test loss: 0.0015468454069147506\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017297660237385167\n",
      "Average test loss: 0.0015579084161048134\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01724243316054344\n",
      "Average test loss: 0.0016123128982467785\n",
      "Epoch 107/300\n",
      "Average training loss: 0.017175590894288488\n",
      "Average test loss: 0.0015683761357019345\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017112491683827506\n",
      "Average test loss: 0.0015686722659609385\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017077875326904985\n",
      "Average test loss: 0.001573410944081843\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017003145469559562\n",
      "Average test loss: 0.0015369236380275754\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016951341711812548\n",
      "Average test loss: 0.0015629637899498144\n",
      "Epoch 113/300\n",
      "Average training loss: 0.016927694391045304\n",
      "Average test loss: 0.001600534674194124\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01694189104106691\n",
      "Average test loss: 0.0016341229926587807\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016878951756490602\n",
      "Average test loss: 0.0015396348213156065\n",
      "Epoch 116/300\n",
      "Average training loss: 0.016795944459736346\n",
      "Average test loss: 0.0015808070587615172\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01679316493951612\n",
      "Average test loss: 0.0015593049159894387\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016752631877859435\n",
      "Average test loss: 0.0015723088193270896\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01668420385983255\n",
      "Average test loss: 0.0016023881858628656\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016624710147579512\n",
      "Average test loss: 0.0016199140653221143\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01661157224741247\n",
      "Average test loss: 0.0016257314723398951\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016595503331058557\n",
      "Average test loss: 0.0016015306204143498\n",
      "Epoch 124/300\n",
      "Average training loss: 0.016490118676589595\n",
      "Average test loss: 0.0016137030939054157\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016483515850371784\n",
      "Average test loss: 0.0015642952198783556\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016437381685607964\n",
      "Average test loss: 0.0015716945864260197\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016434126005404524\n",
      "Average test loss: 0.0015475485570107896\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016396016061306\n",
      "Average test loss: 0.00163578075543046\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016363375638094212\n",
      "Average test loss: 0.0015505184260093503\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016343431908223363\n",
      "Average test loss: 0.0015834896687625182\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016265315282675955\n",
      "Average test loss: 0.0016448694051553806\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016250336430139012\n",
      "Average test loss: 0.0016190670726613865\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016242893025279044\n",
      "Average test loss: 0.0016179062955909305\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016180526400605837\n",
      "Average test loss: 0.0016587120962018769\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016111017405986784\n",
      "Average test loss: 0.0016100245208686425\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016116496175527573\n",
      "Average test loss: 0.0015819118432700635\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01607898556937774\n",
      "Average test loss: 0.001585329090555509\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016076454799208377\n",
      "Average test loss: 0.00164927686088615\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0160269465678268\n",
      "Average test loss: 0.0016254151672538783\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01598730609814326\n",
      "Average test loss: 0.0016005688393488527\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01600820503466659\n",
      "Average test loss: 0.0016971061198661724\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015957528738511934\n",
      "Average test loss: 0.001574243397348457\n",
      "Epoch 144/300\n",
      "Average training loss: 0.015952058366603322\n",
      "Average test loss: 0.0015956508941534493\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015872307433850236\n",
      "Average test loss: 0.001696011144667864\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015847335144877434\n",
      "Average test loss: 0.0015850683736304443\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015917184265123473\n",
      "Average test loss: 0.0016986984830970565\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015762545675867132\n",
      "Average test loss: 0.0016624538702890276\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01579139228661855\n",
      "Average test loss: 0.0017180541645632021\n",
      "Epoch 152/300\n",
      "Average training loss: 0.015723390117287635\n",
      "Average test loss: 0.0016161908949207928\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015692398544814853\n",
      "Average test loss: 0.0016052814099109835\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015673306266466777\n",
      "Average test loss: 0.0015901980592558781\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015634016333354844\n",
      "Average test loss: 0.0016394722039500872\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01564500046769778\n",
      "Average test loss: 0.0016470132391485904\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01566240483439631\n",
      "Average test loss: 0.0015906605494932996\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01562116439971659\n",
      "Average test loss: 0.001636876193082167\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015554008177585073\n",
      "Average test loss: 0.0015936545403674245\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015546551390654511\n",
      "Average test loss: 0.0016329489344110092\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015538378245300717\n",
      "Average test loss: 0.0016350700587241186\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015512005097336239\n",
      "Average test loss: 0.0015874672446192966\n",
      "Epoch 164/300\n",
      "Average training loss: 0.015496142635742824\n",
      "Average test loss: 0.001617442662310269\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01546497548619906\n",
      "Average test loss: 0.0017017286923817463\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015465158679419094\n",
      "Average test loss: 0.0015972585931627286\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015440875031881861\n",
      "Average test loss: 0.0016327765975147487\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015401711919241482\n",
      "Average test loss: 0.0015927535730103652\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01535322081297636\n",
      "Average test loss: 0.0016898565757502285\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01538736135760943\n",
      "Average test loss: 0.0017941286933297912\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015367590456373162\n",
      "Average test loss: 0.0016459658783343103\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01535996131102244\n",
      "Average test loss: 0.0016703760384892425\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01531958550049199\n",
      "Average test loss: 0.0015963813783600926\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015328046293722258\n",
      "Average test loss: 0.0017021106539501084\n",
      "Epoch 175/300\n",
      "Average training loss: 0.015287695690989494\n",
      "Average test loss: 0.0016626492650765512\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015270286137859027\n",
      "Average test loss: 0.0016531103799740474\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015234676218695111\n",
      "Average test loss: 0.0016361894568221436\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015238628872566754\n",
      "Average test loss: 0.0016332796290516854\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01525051044093238\n",
      "Average test loss: 0.0016443989461080895\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015213510535657405\n",
      "Average test loss: 0.0016049341473521458\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015202735749383767\n",
      "Average test loss: 0.0016535515336112843\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015160329242547354\n",
      "Average test loss: 0.0017266987882968452\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01514629009200467\n",
      "Average test loss: 0.0016317580074278846\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015102214577297369\n",
      "Average test loss: 0.001624939776957035\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015097032361560398\n",
      "Average test loss: 0.0016230975531248582\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015077568297584852\n",
      "Average test loss: 0.0016406079383256534\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015037124988933405\n",
      "Average test loss: 0.0016379955180196297\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01506041686485211\n",
      "Average test loss: 0.001697284661233425\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015026541840698984\n",
      "Average test loss: 0.001624045934424632\n",
      "Epoch 192/300\n",
      "Average training loss: 0.014998910800450379\n",
      "Average test loss: 0.0016639760960307387\n",
      "Epoch 193/300\n",
      "Average training loss: 0.015014171108603477\n",
      "Average test loss: 0.0016355534926470784\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014968556590378284\n",
      "Average test loss: 0.00162945579426984\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014986681586338414\n",
      "Average test loss: 0.0016309947232819265\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014956145524150796\n",
      "Average test loss: 0.001641730010509491\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014894812542531225\n",
      "Average test loss: 0.0016615134525216288\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014883471760484906\n",
      "Average test loss: 0.0016578565653827455\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014901379013227092\n",
      "Average test loss: 0.0016337953579301635\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014889711268246173\n",
      "Average test loss: 0.001706328917708662\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014899695477551884\n",
      "Average test loss: 0.0016125592256171835\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014853718764252132\n",
      "Average test loss: 0.0016442099989184902\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014835880436003208\n",
      "Average test loss: 0.0016277834095267786\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014850313242938784\n",
      "Average test loss: 0.0016617183320017325\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014860619666675727\n",
      "Average test loss: 0.0016605809874211748\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014801793966856268\n",
      "Average test loss: 0.0016756923695405324\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014794912503825294\n",
      "Average test loss: 0.0016195623308627141\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01475510083552864\n",
      "Average test loss: 0.0016448531111495363\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014742918687562149\n",
      "Average test loss: 0.0016675507984538044\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014767010520729753\n",
      "Average test loss: 0.0016596824877067574\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014729030229979092\n",
      "Average test loss: 0.001725462675301565\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014675814173287815\n",
      "Average test loss: 0.0016614318995012178\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014712975843913026\n",
      "Average test loss: 0.0016371955282779202\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014671101245615218\n",
      "Average test loss: 0.0016972533247123162\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014674293094211154\n",
      "Average test loss: 0.0016866068411618472\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01465042219393783\n",
      "Average test loss: 0.0016732879847081172\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014660419844918782\n",
      "Average test loss: 0.0016481610180603134\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014659566877616777\n",
      "Average test loss: 0.0017059286012728182\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014621087337533633\n",
      "Average test loss: 0.0016792923499726587\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014613233235147264\n",
      "Average test loss: 0.0016460806832959256\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014608448528581196\n",
      "Average test loss: 0.0016730086253955961\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014598177015781403\n",
      "Average test loss: 0.0016715684237165584\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014575734042459064\n",
      "Average test loss: 0.0017117711960648496\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014542520131501887\n",
      "Average test loss: 0.0016638366584148672\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014588889441556402\n",
      "Average test loss: 0.0016249064885907702\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01454254849255085\n",
      "Average test loss: 0.0017345053334203032\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014539245133598645\n",
      "Average test loss: 0.0017019336563017632\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014521629304521614\n",
      "Average test loss: 0.00166830033229457\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014511544066170851\n",
      "Average test loss: 0.0017084945249888632\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01449473448511627\n",
      "Average test loss: 0.0017055878672334883\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01451857126918104\n",
      "Average test loss: 0.001638572330897053\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014451549082166618\n",
      "Average test loss: 0.001696699403433336\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014484532356262207\n",
      "Average test loss: 0.0017382054618663258\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014444631828202142\n",
      "Average test loss: 0.0016586794139196476\n",
      "Epoch 240/300\n",
      "Average training loss: 0.014435144479076067\n",
      "Average test loss: 0.0016498875167841713\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014413326669898299\n",
      "Average test loss: 0.0016537429292996725\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014444022124840153\n",
      "Average test loss: 0.0017106442094987466\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014393845386803151\n",
      "Average test loss: 0.0017631129677303963\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014411679323348733\n",
      "Average test loss: 0.001764213845754663\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014393287455042203\n",
      "Average test loss: 0.0016801204745554262\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014376043647527694\n",
      "Average test loss: 0.001707987888612681\n",
      "Epoch 247/300\n",
      "Average training loss: 0.014390965731607542\n",
      "Average test loss: 0.00167795786789308\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014360137447714806\n",
      "Average test loss: 0.0016559164642045896\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014363762652708424\n",
      "Average test loss: 0.001670056174405747\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014389033749699593\n",
      "Average test loss: 0.0016823669476434587\n",
      "Epoch 251/300\n",
      "Average training loss: 0.014366036882003148\n",
      "Average test loss: 0.0016869048124386204\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014307819548580382\n",
      "Average test loss: 0.0017068087371687095\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014343809052473969\n",
      "Average test loss: 0.0016606841567489835\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014299176437987221\n",
      "Average test loss: 0.0016786567854384581\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014279712200164794\n",
      "Average test loss: 0.0017095090596833163\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014296691635830534\n",
      "Average test loss: 0.0017393843016276756\n",
      "Epoch 257/300\n",
      "Average training loss: 0.014289793182578352\n",
      "Average test loss: 0.0017020771530353361\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014278705505861177\n",
      "Average test loss: 0.001694391418972777\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0142782215376695\n",
      "Average test loss: 0.0016648513798912367\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014280615029235682\n",
      "Average test loss: 0.0016823432747171158\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014233958525790109\n",
      "Average test loss: 0.0016508366246190335\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014245736757914225\n",
      "Average test loss: 0.0017370454476525385\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014226186475820012\n",
      "Average test loss: 0.0017014166307118203\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014217355012893677\n",
      "Average test loss: 0.0016445410299218363\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014211869970791869\n",
      "Average test loss: 0.0017293952166413267\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014199507161147065\n",
      "Average test loss: 0.0017093267499779662\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014196091210676564\n",
      "Average test loss: 0.0016785112222035725\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014187417031990158\n",
      "Average test loss: 0.0016562124526956015\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014167263572414716\n",
      "Average test loss: 0.0016788559442179071\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014210507091548709\n",
      "Average test loss: 0.0016674736787875494\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014136030505100886\n",
      "Average test loss: 0.0016692110216762457\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014145981876386536\n",
      "Average test loss: 0.001800733668108781\n",
      "Epoch 273/300\n",
      "Average training loss: 0.014136117308504052\n",
      "Average test loss: 0.0016761366758081648\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014135253695978059\n",
      "Average test loss: 0.0016950493931977286\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014102163487010532\n",
      "Average test loss: 0.001685697946128332\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014095152891344494\n",
      "Average test loss: 0.0017346958533550302\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01411887618071503\n",
      "Average test loss: 0.0017117124170892767\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014097084912988875\n",
      "Average test loss: 0.001676982841350966\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014105829487244289\n",
      "Average test loss: 0.0016978254138181608\n",
      "Epoch 280/300\n",
      "Average training loss: 0.014124351345002652\n",
      "Average test loss: 0.0016672513807813327\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014093002206749387\n",
      "Average test loss: 0.0017165440685219236\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014089555310706298\n",
      "Average test loss: 0.0017685435228049756\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01405165359377861\n",
      "Average test loss: 0.0016703735916978784\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014042797627548377\n",
      "Average test loss: 0.0017731527160439226\n",
      "Epoch 285/300\n",
      "Average training loss: 0.014050388794806268\n",
      "Average test loss: 0.0016743668381952576\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01406717930485805\n",
      "Average test loss: 0.0017490696923600302\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014049487637976806\n",
      "Average test loss: 0.0017848826135612196\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014035522832638687\n",
      "Average test loss: 0.001724236409800748\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014038724984559749\n",
      "Average test loss: 0.001676755574428373\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014048789556655618\n",
      "Average test loss: 0.0017322225531356203\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013983020741078588\n",
      "Average test loss: 0.0016927111701418955\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013991055588755342\n",
      "Average test loss: 0.0017694671441697413\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014013590406212542\n",
      "Average test loss: 0.0017019675127747985\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013977369470728768\n",
      "Average test loss: 0.001803376002651122\n",
      "Epoch 295/300\n",
      "Average training loss: 0.014009400107794338\n",
      "Average test loss: 0.0017036911650664276\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013959247746401364\n",
      "Average test loss: 0.0017713382021627493\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014002644508249229\n",
      "Average test loss: 0.0016720938111344974\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01399040635344055\n",
      "Average test loss: 0.0017292655654665497\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013950636135207282\n",
      "Average test loss: 0.0016825590370119446\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01394497068060769\n",
      "Average test loss: 0.0017203635221554174\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth10-.01/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.91\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.32\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.42\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.92\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.22\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.56\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.04\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.15\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.40\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.57\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.80\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.08\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.47\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.64\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.90\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.90\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 39.606005771213106\n",
      "Average test loss: 0.007108760703768995\n",
      "Epoch 2/300\n",
      "Average training loss: 22.45621371120877\n",
      "Average test loss: 0.005840778502325217\n",
      "Epoch 3/300\n",
      "Average training loss: 15.816619618733723\n",
      "Average test loss: 0.018506852484411664\n",
      "Epoch 4/300\n",
      "Average training loss: 12.801394744873047\n",
      "Average test loss: 0.037704650295277434\n",
      "Epoch 5/300\n",
      "Average training loss: 10.104846038818359\n",
      "Average test loss: 0.0079679030896061\n",
      "Epoch 6/300\n",
      "Average training loss: 8.506429909176296\n",
      "Average test loss: 0.0684815733200974\n",
      "Epoch 7/300\n",
      "Average training loss: 6.0074363526238335\n",
      "Average test loss: 0.030728775133689244\n",
      "Epoch 8/300\n",
      "Average training loss: 5.152192259046767\n",
      "Average test loss: 0.0051202226179755395\n",
      "Epoch 9/300\n",
      "Average training loss: 4.274924631542629\n",
      "Average test loss: 0.005433514830138948\n",
      "Epoch 10/300\n",
      "Average training loss: 3.8888884370591907\n",
      "Average test loss: 0.016365285464458996\n",
      "Epoch 11/300\n",
      "Average training loss: 3.3900552531348334\n",
      "Average test loss: 0.004266249892612298\n",
      "Epoch 12/300\n",
      "Average training loss: 2.9602439846462674\n",
      "Average test loss: 0.004221834209230211\n",
      "Epoch 13/300\n",
      "Average training loss: 2.306207492934333\n",
      "Average test loss: 0.004217292408148447\n",
      "Epoch 14/300\n",
      "Average training loss: 1.89997005059984\n",
      "Average test loss: 0.005017643535716666\n",
      "Epoch 15/300\n",
      "Average training loss: 1.649850715637207\n",
      "Average test loss: 0.12333581427733104\n",
      "Epoch 16/300\n",
      "Average training loss: 1.277900216208564\n",
      "Average test loss: 0.01478311228296823\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0103591385417514\n",
      "Average test loss: 0.0908559406010641\n",
      "Epoch 18/300\n",
      "Average training loss: 0.8685076207054986\n",
      "Average test loss: 0.09105090781052907\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7286465131971571\n",
      "Average test loss: 0.004119386639859941\n",
      "Epoch 20/300\n",
      "Average training loss: 0.5965410188568963\n",
      "Average test loss: 0.00806907055982285\n",
      "Epoch 21/300\n",
      "Average training loss: 0.4752308178477817\n",
      "Average test loss: 0.040360765150023833\n",
      "Epoch 22/300\n",
      "Average training loss: 0.39340928859180874\n",
      "Average test loss: 0.004106352767182721\n",
      "Epoch 23/300\n",
      "Average training loss: 0.33888869513405695\n",
      "Average test loss: 0.00409411869591309\n",
      "Epoch 24/300\n",
      "Average training loss: 0.25918116241031225\n",
      "Average test loss: 0.00400734105395774\n",
      "Epoch 26/300\n",
      "Average training loss: 0.23173431419001686\n",
      "Average test loss: 0.07591921036607689\n",
      "Epoch 27/300\n",
      "Average training loss: 0.21121584861808354\n",
      "Average test loss: 0.010781621447867817\n",
      "Epoch 28/300\n",
      "Average training loss: 0.18142056744628482\n",
      "Average test loss: 0.18848870615495575\n",
      "Epoch 30/300\n",
      "Average training loss: 0.16215245548884075\n",
      "Average test loss: 0.003971035223040316\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15638054160277048\n",
      "Average training loss: 0.15091317458285225\n",
      "Average test loss: 0.003950301749838723\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1449357540210088\n",
      "Average test loss: 0.003946520306583908\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1398957565493054\n",
      "Average test loss: 0.003952648197197252\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13658322526348962\n",
      "Average test loss: 0.003989347412768338\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13390202793810102\n",
      "Average test loss: 0.004851060911806093\n",
      "Epoch 38/300\n",
      "Average test loss: 0.003927145044422812\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1294729550878207\n",
      "Average test loss: 0.00395617876905534\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12788238675064512\n",
      "Average test loss: 0.0039051740479966006\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12659949356979794\n",
      "Average test loss: 0.003919536022676362\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12545231421788533\n",
      "Average test loss: 0.003942778353268901\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1246269437538253\n",
      "Average test loss: 0.0038940343153145577\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12357512440284094\n",
      "Average test loss: 0.003909083825639552\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12259130803743998\n",
      "Average test loss: 0.03078746392329534\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12196688347392612\n",
      "Average test loss: 0.003941756295040249\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12120061782333585\n",
      "Average test loss: 0.003949714088191589\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12054136100080279\n",
      "Average test loss: 0.0039023269882632627\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1199329765703943\n",
      "Average test loss: 0.003932117578883966\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11945295419295629\n",
      "Average test loss: 0.00394302302164336\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11896405086252425\n",
      "Average test loss: 0.0038940017749038006\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11842703592114978\n",
      "Average test loss: 0.003903356126613087\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11797807255718443\n",
      "Average test loss: 0.0039740584457500115\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11750437249077691\n",
      "Average test loss: 0.003933577273868852\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11717590673102272\n",
      "Average test loss: 0.0039218363165855405\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11670919184552299\n",
      "Average test loss: 0.00390449586407178\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11584770594702827\n",
      "Average test loss: 0.00427302509587672\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11566988595989015\n",
      "Average test loss: 0.004401280840237935\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11568930181529787\n",
      "Average test loss: 0.003963509633930193\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11502217643790776\n",
      "Average test loss: 0.003928012154996395\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11467853093147277\n",
      "Average test loss: 0.0039952301730712255\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11425106359852685\n",
      "Average test loss: 0.0039480954396227995\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1138260916935073\n",
      "Average test loss: 0.0038805217151012686\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11352593242459827\n",
      "Average test loss: 0.003951893920699756\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11312031183640162\n",
      "Average test loss: 0.003965619750527872\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11274371887577904\n",
      "Average test loss: 0.00401090456214216\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11256894029511345\n",
      "Average test loss: 0.003956967040275534\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11193551045656204\n",
      "Average test loss: 0.004005807127700084\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11159658821423848\n",
      "Average test loss: 0.003919421656264199\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11138860812452105\n",
      "Average test loss: 0.0039008617661893366\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11100791122515996\n",
      "Average test loss: 0.004030034244060517\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11057905569341447\n",
      "Average test loss: 0.003964448813555969\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11015670667754279\n",
      "Average test loss: 0.003928658913820982\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10984605830245547\n",
      "Average test loss: 0.004018021456069417\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10910078488455878\n",
      "Average test loss: 0.003964398789323038\n",
      "Epoch 78/300\n",
      "Average training loss: 0.1089714091817538\n",
      "Average test loss: 0.004020126164580385\n",
      "Epoch 79/300\n",
      "Average training loss: 0.10840273771683374\n",
      "Average test loss: 0.003940995208919048\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10783129251003265\n",
      "Average test loss: 0.004049591519559423\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10750279778242111\n",
      "Average test loss: 0.003961944774000181\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10714390797085233\n",
      "Average test loss: 0.00390275354973144\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10635842417346107\n",
      "Average test loss: 0.004100912452571922\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10610735616419051\n",
      "Average test loss: 0.003998959579815467\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10566664930184683\n",
      "Average test loss: 0.004046440773126152\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10534865476687749\n",
      "Average test loss: 0.003942055189775096\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10487095705005858\n",
      "Average test loss: 0.004097100378324588\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10453523396783405\n",
      "Average test loss: 0.0040411802327467335\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10415984375609291\n",
      "Average test loss: 0.004007987913158205\n",
      "Epoch 91/300\n",
      "Average training loss: 0.1037635156446033\n",
      "Average test loss: 0.004009679391980171\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10371064556307263\n",
      "Average test loss: 0.003996148184355762\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10298426709572474\n",
      "Average test loss: 0.004091166736972001\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10268816890981462\n",
      "Average test loss: 0.00402601299352116\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10245263176494175\n",
      "Average test loss: 0.003969289197482997\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10197771026690801\n",
      "Average test loss: 0.003999842345921529\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1014223627448082\n",
      "Average test loss: 0.004018078179823028\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10108565313948525\n",
      "Average test loss: 0.004069736835443311\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10085668694972992\n",
      "Average test loss: 0.004041976665871011\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10181479937500423\n",
      "Average test loss: 0.004137245023623109\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10045711692174275\n",
      "Average test loss: 0.004043319536993901\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0996785012880961\n",
      "Average test loss: 0.004065752551373508\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09942031211323209\n",
      "Average test loss: 0.004127430540819963\n",
      "Epoch 105/300\n",
      "Average training loss: 0.09898711989323299\n",
      "Average test loss: 0.004126181494444609\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09910542694396443\n",
      "Average test loss: 0.0041380217460294565\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09877877622842789\n",
      "Average test loss: 0.004251001717729701\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09816384075085322\n",
      "Average test loss: 0.0040340515627629225\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09796651118993759\n",
      "Average test loss: 0.004070969255848063\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09760541592703925\n",
      "Average test loss: 0.004228843508495225\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09742644594775306\n",
      "Average test loss: 0.0041727871667179795\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09708836880988544\n",
      "Average test loss: 0.004249415670418077\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0969934664302402\n",
      "Average test loss: 0.004188724544934101\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09660662184158961\n",
      "Average test loss: 0.004211324913634194\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09604951464467579\n",
      "Average test loss: 0.004186451186736425\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09590181314282947\n",
      "Average test loss: 0.004151629419376452\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09548377805948258\n",
      "Average test loss: 0.004221630713591973\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0956701885594262\n",
      "Average test loss: 0.004051526720325152\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09502311857541403\n",
      "Average test loss: 0.004246004765853286\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09459854447841644\n",
      "Average test loss: 0.0043080599481860795\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0942120289935006\n",
      "Average test loss: 0.004154915932565928\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0939234954714775\n",
      "Average test loss: 0.004275220637934075\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09378420612547132\n",
      "Average test loss: 0.0041169878745244606\n",
      "Epoch 126/300\n",
      "Average training loss: 0.09351960091458426\n",
      "Average test loss: 0.004125840623759561\n",
      "Epoch 127/300\n",
      "Average training loss: 0.09321659537818697\n",
      "Average test loss: 0.004134049076173041\n",
      "Epoch 128/300\n",
      "Average training loss: 0.09272323355409834\n",
      "Average test loss: 0.004328941788524389\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09274311580260594\n",
      "Average test loss: 0.004216942259834872\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09252010972632302\n",
      "Average test loss: 0.0041465841901178165\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09215274260441463\n",
      "Average test loss: 0.0042796039237744275\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09179128703806136\n",
      "Average test loss: 0.004086474346617857\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09163784523804983\n",
      "Average test loss: 0.004097104104028808\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09152002525991863\n",
      "Average test loss: 0.004351824737671349\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09096955056322946\n",
      "Average test loss: 0.004290449999272823\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09070856102969911\n",
      "Average test loss: 0.004324520891325342\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09073213669988844\n",
      "Average test loss: 0.004432492582334412\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09057547389136421\n",
      "Average test loss: 0.004220934287541442\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09025242931312985\n",
      "Average test loss: 0.004261553417684303\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09030851225058238\n",
      "Average test loss: 0.0042560067218210965\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08982000252273348\n",
      "Average test loss: 0.004207916588419014\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08962745518154568\n",
      "Average test loss: 0.004276977416541841\n",
      "Epoch 145/300\n",
      "Average training loss: 0.08946434537569682\n",
      "Average test loss: 0.004099683719583683\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0890490563246939\n",
      "Average test loss: 0.004237619071577986\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08896529443396463\n",
      "Average test loss: 0.0042382806301530865\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08895068047444026\n",
      "Average test loss: 0.0041672281989206875\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08847821766138077\n",
      "Average test loss: 0.004236595329311159\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08823552025689019\n",
      "Average test loss: 0.0041247091222968366\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08839852221806845\n",
      "Average test loss: 0.00424909586004085\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08797607989443673\n",
      "Average test loss: 0.00416699996093909\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08778439353571997\n",
      "Average test loss: 0.0042680788609302705\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08760822233888838\n",
      "Average test loss: 0.004174154526036647\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0875549375779099\n",
      "Average test loss: 0.0041850424359242125\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08719673169321485\n",
      "Average test loss: 0.004121937524527311\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08711111271381378\n",
      "Average test loss: 0.004237044644438558\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08669662995470895\n",
      "Average test loss: 0.004195269127272898\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0865669709444046\n",
      "Average test loss: 0.004219017351253165\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08649337036742104\n",
      "Average test loss: 0.004419135047536757\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08623021015856001\n",
      "Average test loss: 0.004271974366158247\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08639568013615079\n",
      "Average test loss: 0.004191075319838193\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08601568912466367\n",
      "Average test loss: 0.004206228685254852\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0857203480137719\n",
      "Average test loss: 0.004325960793842872\n",
      "Epoch 168/300\n",
      "Average training loss: 0.08546672326988644\n",
      "Average test loss: 0.004305087183912595\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08542186625798544\n",
      "Average test loss: 0.004570672461556064\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08539699360397127\n",
      "Average test loss: 0.004164219417091873\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0852072451280223\n",
      "Average test loss: 0.004522507730664479\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08482429117626614\n",
      "Average test loss: 0.004446553126598398\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08474444957905346\n",
      "Average test loss: 0.004188012201752927\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08465876256426176\n",
      "Average test loss: 0.004308211375441816\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08459948426485062\n",
      "Average test loss: 0.004317170280549261\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08421770904461542\n",
      "Average test loss: 0.004229029257471362\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08424233096175723\n",
      "Average test loss: 0.00435421176503102\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08413490845097436\n",
      "Average test loss: 0.004128155513356129\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08375229693783653\n",
      "Average test loss: 0.004365824495545692\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08369400469462077\n",
      "Average test loss: 0.004269938523156776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08354081256522072\n",
      "Average test loss: 0.004264981736325555\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08360775683323542\n",
      "Average test loss: 0.004345193043765095\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08318187045719888\n",
      "Average test loss: 0.004232830645102594\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08305188963148329\n",
      "Average test loss: 0.004243291149329808\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08302883236275779\n",
      "Average test loss: 0.004232887054690057\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08285881491502126\n",
      "Average test loss: 0.004275203078157372\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08259807478057013\n",
      "Average test loss: 0.004227981124901109\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0823532093597783\n",
      "Average test loss: 0.004173043308986558\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0822970778743426\n",
      "Average test loss: 0.004224062562816673\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08226734475957023\n",
      "Average test loss: 0.0042471181628190805\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08222903640402687\n",
      "Average test loss: 0.004135610894403524\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08195783687962425\n",
      "Average test loss: 0.004412427221321397\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08186472058296204\n",
      "Average test loss: 0.0042699727057996725\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08157302969363\n",
      "Average test loss: 0.004305128607071108\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08145468205213546\n",
      "Average test loss: 0.004302161219633288\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08155099747247166\n",
      "Average test loss: 0.004285408839169476\n",
      "Epoch 200/300\n",
      "Average test loss: 0.004302347887721327\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08130560174915526\n",
      "Average test loss: 0.004577547238932715\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08116039227114784\n",
      "Average test loss: 0.004305361705521742\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08089697700076633\n",
      "Average test loss: 0.004278237702118026\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08083405029111439\n",
      "Average test loss: 0.004303999076286952\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08057975181606081\n",
      "Average test loss: 0.004154681879406174\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0807847157716751\n",
      "Average training loss: 0.08055266721381081\n",
      "Average test loss: 0.004418077713085546\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08034088287088606\n",
      "Average test loss: 0.004304788097325298\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0803001267115275\n",
      "Average test loss: 0.004208939517537753\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08011998207701578\n",
      "Average test loss: 0.004174186875836716\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08016828642951118\n",
      "Average test loss: 0.004307093276745743\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08057599741220474\n",
      "Average test loss: 0.00435301681732138\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0799240686363644\n",
      "Average test loss: 0.004522241033820642\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0795398063659668\n",
      "Average test loss: 0.004441321524480978\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0796455887887213\n",
      "Average test loss: 0.0042946165677987865\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0794193267888493\n",
      "Average test loss: 0.004229328468028042\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07923345349232355\n",
      "Average test loss: 0.004340492036193609\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07922144922945235\n",
      "Average test loss: 0.004236913765056266\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07901489259137047\n",
      "Average test loss: 0.004291354234019915\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07899060163564152\n",
      "Average test loss: 0.004326304458909564\n",
      "Epoch 222/300\n",
      "Average training loss: 0.0789752197795444\n",
      "Average test loss: 0.004357267117748658\n",
      "Epoch 223/300\n",
      "Average training loss: 0.07880352260006798\n",
      "Average test loss: 0.004292020227139195\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07877313756611612\n",
      "Average test loss: 0.004355057833509313\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07870447593927384\n",
      "Average test loss: 0.004290627177390787\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07855429298679034\n",
      "Average test loss: 0.004229834530502558\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0783378442923228\n",
      "Average test loss: 0.0042451346601463024\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07848877660433451\n",
      "Average test loss: 0.004395560247616635\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07826444123850929\n",
      "Average test loss: 0.004300320732303791\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07804831763770845\n",
      "Average test loss: 0.004286376553277175\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07809570900930299\n",
      "Average test loss: 0.004572898173911704\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07786483787496885\n",
      "Average test loss: 0.004228904076955384\n",
      "Epoch 233/300\n",
      "Average training loss: 0.07791066844595804\n",
      "Average test loss: 0.004501546024034421\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07788282206323412\n",
      "Average test loss: 0.004304642905170719\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07766217116514841\n",
      "Average test loss: 0.0042238300099141065\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07762979583607779\n",
      "Average test loss: 0.004320624482300546\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07762924292352465\n",
      "Average test loss: 0.004300370350894001\n",
      "Epoch 238/300\n",
      "Average training loss: 0.07734274315834046\n",
      "Average test loss: 0.004377154260873794\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07742398208379746\n",
      "Average test loss: 0.004290588256799513\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07719609326124191\n",
      "Average test loss: 0.0041640207349426216\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07700377380185656\n",
      "Average test loss: 0.004378410768177774\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07696230827437507\n",
      "Average test loss: 0.004311773747412695\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07763131207889981\n",
      "Average test loss: 0.004350080628775888\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07706876675287883\n",
      "Average test loss: 0.0042838169996523194\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07658718115091324\n",
      "Average test loss: 0.004318750984345874\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07669542372888989\n",
      "Average test loss: 0.004337504751566384\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07658622271484798\n",
      "Average test loss: 0.004424135492700669\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07643410134646628\n",
      "Average test loss: 0.004312027821524276\n",
      "Epoch 249/300\n",
      "Average training loss: 0.07663961029383871\n",
      "Average test loss: 0.004349013968060414\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0764591234061453\n",
      "Average test loss: 0.004241048992507987\n",
      "Epoch 251/300\n",
      "Average training loss: 0.07618058712614907\n",
      "Average test loss: 0.004368023319790761\n",
      "Epoch 252/300\n",
      "Average training loss: 0.07661116964287228\n",
      "Average test loss: 0.004300892905228668\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07615800076060825\n",
      "Average test loss: 0.004364281995428933\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07596560117933485\n",
      "Average test loss: 0.004302161592162318\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07595088981919819\n",
      "Average test loss: 0.0043052130250467195\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0759699315097597\n",
      "Average test loss: 0.004366245665277044\n",
      "Epoch 257/300\n",
      "Average training loss: 0.07586322487062878\n",
      "Average test loss: 0.004226886206409998\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07580271132124795\n",
      "Average test loss: 0.004254435237497091\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07577780369917551\n",
      "Average test loss: 0.00437194460671809\n",
      "Epoch 260/300\n",
      "Average training loss: 0.07609048851993348\n",
      "Average test loss: 0.0043169802154103916\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07532864179876116\n",
      "Average test loss: 0.004268185000866651\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07547010038296381\n",
      "Average test loss: 0.004388614582518736\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07536856654617521\n",
      "Average test loss: 0.004381439014855358\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07535513658656014\n",
      "Average test loss: 0.004276417489474019\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07512182874149746\n",
      "Average test loss: 0.0042543299574818874\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07531099265813827\n",
      "Average test loss: 0.004304377880775266\n",
      "Epoch 267/300\n",
      "Average training loss: 0.07506327990690867\n",
      "Average test loss: 0.004251592659287983\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07476288128561444\n",
      "Average test loss: 0.004219487611204386\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07487560752034188\n",
      "Average test loss: 0.004372392141570648\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07486032187607554\n",
      "Average test loss: 0.004428591604448027\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07478145353661643\n",
      "Average test loss: 0.004545022292269601\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07473682751258215\n",
      "Average test loss: 0.004304756245265404\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07460776261488597\n",
      "Average test loss: 0.004285074530376329\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07462130558490754\n",
      "Average test loss: 0.0044038941304509836\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07435575644837486\n",
      "Average test loss: 0.004269352696835995\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07445169944895638\n",
      "Average test loss: 0.004475364479753706\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07425112312369876\n",
      "Average test loss: 0.004288428635232979\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07447041367822223\n",
      "Average test loss: 0.004610366329550743\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07415444968144098\n",
      "Average test loss: 0.004231656183385187\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0740593101978302\n",
      "Average test loss: 0.004316152373121844\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0744474850959248\n",
      "Average test loss: 0.004274585308300124\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07432127610842387\n",
      "Average test loss: 0.004342702892091539\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07399688242541419\n",
      "Average test loss: 0.004332533420787917\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07366206718815697\n",
      "Average test loss: 0.004366648902081781\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07377449458175235\n",
      "Average test loss: 0.00425113561625282\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07377881070640352\n",
      "Average test loss: 0.0042387760414017575\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07374764566951328\n",
      "Average test loss: 0.004237677163961861\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07350611367490556\n",
      "Average test loss: 0.0043760674212955765\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07345231159528097\n",
      "Average test loss: 0.004292375763257345\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07349661710527208\n",
      "Average test loss: 0.0042760597959988645\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07327669864892959\n",
      "Average test loss: 0.004351322183592452\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07335475468635559\n",
      "Average test loss: 0.004466500275457899\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07329828787512249\n",
      "Average test loss: 0.004288383523623149\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0731625971727901\n",
      "Average test loss: 0.004315860981742541\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07298050467173259\n",
      "Average test loss: 0.00422037318141924\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07301950020922555\n",
      "Average test loss: 0.004384116315179401\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07318506832917532\n",
      "Average test loss: 0.004347547275945544\n",
      "Epoch 298/300\n",
      "Average training loss: 0.07285556770033307\n",
      "Average test loss: 0.0042628279518749975\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0728278406129943\n",
      "Average test loss: 0.004236715604033735\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07281643750601345\n",
      "Average test loss: 0.004400595128743185\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 28.93022142537435\n",
      "Average test loss: 0.08297554416375028\n",
      "Epoch 2/300\n",
      "Average training loss: 13.924911660936143\n",
      "Average test loss: 0.00587811436669694\n",
      "Epoch 3/300\n",
      "Average training loss: 11.287353747049968\n",
      "Average test loss: 0.008363261702987883\n",
      "Epoch 4/300\n",
      "Average training loss: 8.577249237060547\n",
      "Average test loss: 0.004402155997024642\n",
      "Epoch 5/300\n",
      "Average training loss: 6.511346451653375\n",
      "Average test loss: 0.00406511328700516\n",
      "Epoch 6/300\n",
      "Average training loss: 6.0744346974690755\n",
      "Average test loss: 0.004059014225999514\n",
      "Epoch 7/300\n",
      "Average training loss: 4.858509904225667\n",
      "Average test loss: 0.0039206267055124045\n",
      "Epoch 8/300\n",
      "Average training loss: 4.182582381354438\n",
      "Average test loss: 1.4124687744517708\n",
      "Epoch 9/300\n",
      "Average training loss: 3.6425885592566596\n",
      "Average test loss: 99.56536245783667\n",
      "Epoch 10/300\n",
      "Average training loss: 3.92013187789917\n",
      "Average test loss: 0.00413881583015124\n",
      "Epoch 11/300\n",
      "Average training loss: 3.464969161139594\n",
      "Average test loss: 1.7700307353064417\n",
      "Epoch 12/300\n",
      "Average training loss: 2.7570357456207275\n",
      "Average test loss: 0.030519160639080736\n",
      "Epoch 13/300\n",
      "Average training loss: 2.4718125110202367\n",
      "Average test loss: 2.5236654867722343\n",
      "Epoch 14/300\n",
      "Average training loss: 2.193541115442912\n",
      "Average test loss: 0.00375133116543293\n",
      "Epoch 15/300\n",
      "Average training loss: 1.933865917523702\n",
      "Average test loss: 1.8544487385611153\n",
      "Epoch 16/300\n",
      "Average training loss: 1.7208380402459038\n",
      "Average test loss: 0.0820900585129857\n",
      "Epoch 17/300\n",
      "Average training loss: 1.5024394534428915\n",
      "Average test loss: 15.58005171885093\n",
      "Epoch 18/300\n",
      "Average training loss: 1.367813243760003\n",
      "Average test loss: 1145.8439804226557\n",
      "Epoch 19/300\n",
      "Average training loss: 1.2003657319810654\n",
      "Average test loss: 0.04118179986543125\n",
      "Epoch 20/300\n",
      "Average training loss: 1.0766150471369424\n",
      "Average test loss: 0.0032103457757168344\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9639106226497226\n",
      "Average test loss: 0.00691927576272024\n",
      "Epoch 22/300\n",
      "Average training loss: 0.8586746014489068\n",
      "Average test loss: 0.0031787761838899717\n",
      "Epoch 23/300\n",
      "Average training loss: 0.7703844629923503\n",
      "Average test loss: 0.0031266234070062636\n",
      "Epoch 24/300\n",
      "Average training loss: 0.6880433387756347\n",
      "Average test loss: 0.0030981391198519202\n",
      "Epoch 25/300\n",
      "Average training loss: 0.6114008285734388\n",
      "Average test loss: 0.003076551310511099\n",
      "Epoch 26/300\n",
      "Average training loss: 0.5416725097232394\n",
      "Average test loss: 0.0030847458442052203\n",
      "Epoch 27/300\n",
      "Average training loss: 0.47882528760698106\n",
      "Average test loss: 0.0030263464817156392\n",
      "Epoch 28/300\n",
      "Average training loss: 0.4252562826209598\n",
      "Average test loss: 0.0030032541807740926\n",
      "Epoch 29/300\n",
      "Average training loss: 0.37839391250080534\n",
      "Average test loss: 0.0030226918492052292\n",
      "Epoch 30/300\n",
      "Average training loss: 0.3378796706199646\n",
      "Average test loss: 0.0029950206180413565\n",
      "Epoch 31/300\n",
      "Average training loss: 0.30209733276897005\n",
      "Average test loss: 0.002981494662972788\n",
      "Epoch 32/300\n",
      "Average training loss: 0.27132176438967387\n",
      "Average test loss: 0.0029556145499356917\n",
      "Epoch 33/300\n",
      "Average training loss: 0.24366636196772257\n",
      "Average test loss: 0.0029469260291920766\n",
      "Epoch 34/300\n",
      "Average training loss: 0.21987676548957824\n",
      "Average test loss: 0.0029502556624097956\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1981077419784334\n",
      "Average test loss: 0.0029325749892741442\n",
      "Epoch 36/300\n",
      "Average training loss: 0.17955625462532043\n",
      "Average test loss: 0.00294714285660949\n",
      "Epoch 37/300\n",
      "Average training loss: 0.163543455613984\n",
      "Average test loss: 0.002933421772180332\n",
      "Epoch 38/300\n",
      "Average training loss: 0.14978737433751424\n",
      "Average test loss: 0.002954059587377641\n",
      "Epoch 39/300\n",
      "Average training loss: 0.13881277237998116\n",
      "Average test loss: 0.0029534091711458234\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1296191094186571\n",
      "Average test loss: 0.0029492683412714136\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1221200414299965\n",
      "Average test loss: 0.002896378146691455\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11623188706901338\n",
      "Average test loss: 0.0029398395895130105\n",
      "Epoch 43/300\n",
      "Average training loss: 0.11146454379955928\n",
      "Average test loss: 0.0029112943108710976\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10754155037800471\n",
      "Average test loss: 0.0029768267058663898\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10450144375695122\n",
      "Average test loss: 0.002934116712357435\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10191846582624647\n",
      "Average test loss: 0.0028711313973698352\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09966207084390852\n",
      "Average test loss: 0.0029272987929483255\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0979607099228435\n",
      "Average test loss: 0.0029014084172538585\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0964128879474269\n",
      "Average test loss: 0.0028573298677802084\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09504151754909092\n",
      "Average test loss: 0.0028890516917324726\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09397643627723058\n",
      "Average test loss: 0.0029799643409334953\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09311121736632454\n",
      "Average test loss: 0.002858811615034938\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09207153536876042\n",
      "Average test loss: 0.0029653816661900944\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09098549075921376\n",
      "Average test loss: 0.0029521772687633833\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09033157914214664\n",
      "Average test loss: 0.002883616221861707\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08951588771078321\n",
      "Average test loss: 0.002841867302233974\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0885452064540651\n",
      "Average test loss: 0.002983897183918291\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08803264478842418\n",
      "Average test loss: 0.0028672159928828478\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08751712432834838\n",
      "Average test loss: 0.002877318743409382\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08663358087672128\n",
      "Average test loss: 0.0028625800150136153\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08600605481863022\n",
      "Average test loss: 0.002917577763191528\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08540502057804002\n",
      "Average test loss: 0.0028409152670452993\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08457389305697548\n",
      "Average test loss: 0.003029918274531762\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08431834177176158\n",
      "Average test loss: 0.002972310583624575\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08332756253745821\n",
      "Average test loss: 0.0028896771265814703\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08280340379476547\n",
      "Average test loss: 0.0028885164318813218\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0823295847111278\n",
      "Average test loss: 0.0029162667122566038\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08166910413569874\n",
      "Average test loss: 0.003029744445449776\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08104672457112207\n",
      "Average test loss: 0.0029255240354686975\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0804508730173111\n",
      "Average test loss: 0.0029780958864010043\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08020490222507053\n",
      "Average test loss: 0.0028660413415895567\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0799079967406061\n",
      "Average test loss: 0.002891116646842824\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07888509691754977\n",
      "Average test loss: 0.0029620883696609072\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07845421650674608\n",
      "Average test loss: 0.0029240528578973477\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07813130243619283\n",
      "Average test loss: 0.0029912128657516505\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07845957429210344\n",
      "Average test loss: 0.0029934578893913164\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07724208502637016\n",
      "Average test loss: 0.0029293896998796197\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07666824317640729\n",
      "Average test loss: 0.0030224349072410003\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07619020742840237\n",
      "Average test loss: 0.00293878571378688\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07572710784607463\n",
      "Average test loss: 0.002893056305953198\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0755084368818336\n",
      "Average test loss: 0.002963590251488818\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07522854689094756\n",
      "Average test loss: 0.002927487441028158\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07482243833608097\n",
      "Average test loss: 0.0029736446098734934\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07413680339521832\n",
      "Average test loss: 0.003003437336327301\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0738163087103102\n",
      "Average test loss: 0.0029753134089211624\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07338137018018298\n",
      "Average test loss: 0.002926363271764583\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07320246090160476\n",
      "Average test loss: 0.00294329864014354\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07260907517539131\n",
      "Average test loss: 0.0030732370543604098\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0724914832909902\n",
      "Average test loss: 0.002919112078948981\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07230972503291236\n",
      "Average test loss: 0.003026847817003727\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07165240314271715\n",
      "Average test loss: 0.0029727774545964267\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07139527604315016\n",
      "Average test loss: 0.002900692297352685\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07113990303542879\n",
      "Average test loss: 0.0029453981154494817\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07067060311635336\n",
      "Average test loss: 0.0029606860435257357\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0705026236904992\n",
      "Average test loss: 0.0029934574047931367\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0700770463347435\n",
      "Average test loss: 0.002989627902292543\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07012592143813769\n",
      "Average test loss: 0.003082057752336065\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06951308878262838\n",
      "Average test loss: 0.0029454473573714496\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06919992929697037\n",
      "Average test loss: 0.002998534813730253\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06902360658513175\n",
      "Average test loss: 0.0030194677259359095\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06871874388721254\n",
      "Average test loss: 0.003072942252788279\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06870910662578211\n",
      "Average test loss: 0.0030521423005395467\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06854869277278582\n",
      "Average test loss: 0.0030958259548577996\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0682926890651385\n",
      "Average test loss: 0.0030561227009942133\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06767266897360484\n",
      "Average test loss: 0.0030447204475187594\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06733033502764171\n",
      "Average test loss: 0.003121665241610673\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06721363096767001\n",
      "Average test loss: 0.0029986251687837973\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06695564944876566\n",
      "Average test loss: 0.002984778987036811\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06687893436021275\n",
      "Average test loss: 0.003047325026450886\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06650280524955855\n",
      "Average test loss: 0.0029740132457680172\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06629288917448786\n",
      "Average test loss: 0.0031186169602183834\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06621316831641727\n",
      "Average test loss: 0.003092097235740059\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06585925738016764\n",
      "Average test loss: 0.0030698687173426153\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06577648399273554\n",
      "Average test loss: 0.0029550184804118342\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06529381088746919\n",
      "Average test loss: 0.0030718453959044482\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06537167756093873\n",
      "Average test loss: 0.0030197703109847173\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06519727902611097\n",
      "Average test loss: 0.003098214822096957\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06484397928582297\n",
      "Average test loss: 0.0030326451069364944\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06458530601196819\n",
      "Average test loss: 0.003175637745608886\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06440475278430514\n",
      "Average test loss: 0.003055626341659162\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06424228435423639\n",
      "Average test loss: 0.0031056062575015755\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06439904839131567\n",
      "Average test loss: 0.0031176251019868584\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06385304611590173\n",
      "Average test loss: 0.0031073172316989964\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06356830656528473\n",
      "Average test loss: 0.003095371333675252\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06347299495008257\n",
      "Average test loss: 0.003207817057354583\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06348494674099817\n",
      "Average test loss: 0.0030723845810732907\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06319266208012898\n",
      "Average training loss: 0.0628463310930464\n",
      "Average test loss: 0.003088637485686276\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06287981468770239\n",
      "Average test loss: 0.0031649950734443134\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06245574504799313\n",
      "Average test loss: 0.003078727785497904\n",
      "Epoch 132/300\n",
      "Average training loss: 0.062422224584552974\n",
      "Average test loss: 0.0031098483846419387\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0622755196624332\n",
      "Average test loss: 0.003107515707405077\n",
      "Epoch 134/300\n",
      "Average training loss: 0.061944137298398545\n",
      "Average training loss: 0.061745797177155816\n",
      "Average test loss: 0.0031239075304733384\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06168895665142271\n",
      "Average test loss: 0.0031252347592057453\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06164000231689877\n",
      "Average test loss: 0.003113533461259471\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06144225403997633\n",
      "Average test loss: 0.0030838305935677554\n",
      "Epoch 139/300\n",
      "Average training loss: 0.06129675598608123\n",
      "Average test loss: 0.0031496069323685436\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06095504869686233\n",
      "Average test loss: 0.003058687843071918\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06124666211009026\n",
      "Average test loss: 0.0030682029380566544\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06092959571215841\n",
      "Average test loss: 0.0031450698673725127\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06076051288180881\n",
      "Average test loss: 0.0031082373737461037\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06046863727105988\n",
      "Average test loss: 0.003075478156821595\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06046041089296341\n",
      "Average test loss: 0.0031568276015006835\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06035809110270606\n",
      "Average test loss: 0.0031320570001585617\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06010457015037537\n",
      "Average test loss: 0.002962965042847726\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06026887381076813\n",
      "Average test loss: 0.003114156602157487\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05989051500293944\n",
      "Average test loss: 0.0031558143958035443\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05982239361272918\n",
      "Average test loss: 0.003132048381285535\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05958797932002279\n",
      "Average test loss: 0.0030883111722974314\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05940961153308551\n",
      "Average test loss: 0.003143752099118299\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05948845672607422\n",
      "Average test loss: 0.0032151137774603235\n",
      "Epoch 154/300\n",
      "Average training loss: 0.059425070769257016\n",
      "Average test loss: 0.0030501102848599355\n",
      "Epoch 155/300\n",
      "Average training loss: 0.059173193845483994\n",
      "Average test loss: 0.003175044897943735\n",
      "Epoch 156/300\n",
      "Average training loss: 0.059323891424470476\n",
      "Average test loss: 0.0030689351989163292\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05909881995783912\n",
      "Average test loss: 0.0031393273112674553\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05892522812220785\n",
      "Average test loss: 0.003190557061591082\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05866931556661924\n",
      "Average test loss: 0.0030409734009040727\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05853695349229707\n",
      "Average test loss: 0.003088293776330021\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05836157091458639\n",
      "Average test loss: 0.003190809054921071\n",
      "Epoch 162/300\n",
      "Average training loss: 0.058631396157874\n",
      "Average test loss: 0.0030787265725019906\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05838235552774535\n",
      "Average test loss: 0.00305797141417861\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05828194239404466\n",
      "Average test loss: 0.0030817183032631873\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05791804003384378\n",
      "Average test loss: 0.003090540558927589\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05783001407649782\n",
      "Average test loss: 0.0031359235242837003\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05774861276812024\n",
      "Average test loss: 0.0031903504302932158\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05772302879558669\n",
      "Average test loss: 0.0031526879146695137\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05751260441210535\n",
      "Average test loss: 0.003122038117506438\n",
      "Epoch 170/300\n",
      "Average training loss: 0.057422136071655486\n",
      "Average test loss: 0.0031073314816587503\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05758458952109019\n",
      "Average test loss: 0.0030372603514956104\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05736987739139133\n",
      "Average test loss: 0.0030923526438160075\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05730312289132012\n",
      "Average test loss: 0.0031686617312952875\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05701855137613084\n",
      "Average test loss: 0.0031633689916796153\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05718493518895573\n",
      "Average test loss: 0.0030548856428100003\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05691619422203965\n",
      "Average test loss: 0.0032143584456708696\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05673349729511473\n",
      "Average test loss: 0.003100993073027995\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0565826104150878\n",
      "Average test loss: 0.003145645461976528\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05662129397855865\n",
      "Average test loss: 0.0031217958983033897\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05665037613444858\n",
      "Average test loss: 0.0031373123683863216\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0564703202744325\n",
      "Average test loss: 0.0031166076461474103\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05649734170238177\n",
      "Average test loss: 0.003152557267496983\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05609935392936071\n",
      "Average test loss: 0.0031143526480429703\n",
      "Epoch 184/300\n",
      "Average training loss: 0.056161250859498975\n",
      "Average test loss: 0.003096727299400502\n",
      "Epoch 185/300\n",
      "Average training loss: 0.056036529665191966\n",
      "Average test loss: 0.003143847797686855\n",
      "Epoch 186/300\n",
      "Average training loss: 0.056177220669057636\n",
      "Average test loss: 0.003143893890082836\n",
      "Epoch 187/300\n",
      "Average training loss: 0.055855274243487255\n",
      "Average test loss: 0.0032159788829998837\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05592842874262068\n",
      "Average test loss: 0.0031341260170771016\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05593631429142422\n",
      "Average test loss: 0.0031151514624555908\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05559496265980932\n",
      "Average test loss: 0.0031965457225839296\n",
      "Epoch 191/300\n",
      "Average training loss: 0.055662045660946104\n",
      "Average test loss: 0.0031482514360298714\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05546149598889881\n",
      "Average test loss: 0.003277825048400296\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05542094593577915\n",
      "Average test loss: 0.0030657834505869283\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05581697705056932\n",
      "Average test loss: 0.0031261889040470124\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05528930564721425\n",
      "Average test loss: 0.003143515597614977\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05516748845908377\n",
      "Average test loss: 0.0031289770152005884\n",
      "Epoch 197/300\n",
      "Average training loss: 0.054989616917239297\n",
      "Average test loss: 0.0031426578954690033\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05495740918649567\n",
      "Average test loss: 0.0031968766200459664\n",
      "Epoch 199/300\n",
      "Average training loss: 0.054914923714266886\n",
      "Average test loss: 0.003121378234898051\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05483939501974318\n",
      "Average test loss: 0.0032883059438318014\n",
      "Epoch 201/300\n",
      "Average training loss: 0.054770048158036336\n",
      "Average test loss: 0.0031220456111348337\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05480047381255362\n",
      "Average test loss: 0.0031600615688496167\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05474439508716265\n",
      "Average test loss: 0.0032561640184786585\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05448793338735898\n",
      "Average test loss: 0.0032596173261602718\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05443037035067876\n",
      "Average test loss: 0.0032224090821627114\n",
      "Epoch 206/300\n",
      "Average training loss: 0.054449636121590934\n",
      "Average test loss: 0.003173030342285832\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05437809512847\n",
      "Average test loss: 0.003259920407086611\n",
      "Epoch 208/300\n",
      "Average training loss: 0.054223185668389004\n",
      "Average test loss: 0.0031126419518970783\n",
      "Epoch 209/300\n",
      "Average training loss: 0.054213850600851904\n",
      "Average test loss: 0.0031401840994755427\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0541428751581245\n",
      "Average test loss: 0.003203663007252746\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05403578333059947\n",
      "Average test loss: 0.003162875775868694\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05398484854400158\n",
      "Average test loss: 0.0031095172129571437\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0541160398821036\n",
      "Average test loss: 0.003167138332087133\n",
      "Epoch 214/300\n",
      "Average training loss: 0.053911099834574594\n",
      "Average test loss: 0.003135035932271017\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05377731043762631\n",
      "Average test loss: 0.00315662038533224\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05357915684249666\n",
      "Average test loss: 0.0031625324580818416\n",
      "Epoch 217/300\n",
      "Average training loss: 0.053571194529533385\n",
      "Average test loss: 0.003214480275909106\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05352164508899053\n",
      "Average test loss: 0.00319064959573249\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05344194400310516\n",
      "Average test loss: 0.003197029633447528\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05359516556064288\n",
      "Average test loss: 0.003199824295731054\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05357359207338757\n",
      "Average test loss: 0.003224194000164668\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05342504539092382\n",
      "Average test loss: 0.0032045711148530245\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05323078898257679\n",
      "Average test loss: 0.0031399944230086274\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05317625628246202\n",
      "Average test loss: 0.0031505521357887323\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05312860291534\n",
      "Average test loss: 0.0031502664140943023\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05305326252513461\n",
      "Average test loss: 0.0031034500901069907\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05304399817519718\n",
      "Average test loss: 0.003131462765754097\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0529610326571597\n",
      "Average test loss: 0.003223542078087727\n",
      "Epoch 229/300\n",
      "Average training loss: 0.052817469163073436\n",
      "Average test loss: 0.0031438646163377496\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05279950698216756\n",
      "Average test loss: 0.0032694503656691973\n",
      "Epoch 231/300\n",
      "Average training loss: 0.052694412499666214\n",
      "Average test loss: 0.003213734501765834\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05272605282730526\n",
      "Average test loss: 0.003279764330221547\n",
      "Epoch 233/300\n",
      "Average training loss: 0.052635029004679784\n",
      "Average test loss: 0.0032278888198650547\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05263177712427245\n",
      "Average test loss: 0.0031267423468331495\n",
      "Epoch 235/300\n",
      "Average training loss: 0.052616901252004834\n",
      "Average test loss: 0.0032171825001844102\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05254058525794082\n",
      "Average test loss: 0.0032303195668177474\n",
      "Epoch 237/300\n",
      "Average training loss: 0.052343217389451135\n",
      "Average test loss: 0.0033920127318965063\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0524857463505533\n",
      "Average test loss: 0.003212839493321048\n",
      "Epoch 239/300\n",
      "Average training loss: 0.052197965310679544\n",
      "Average test loss: 0.0031428701658215786\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05217924269702699\n",
      "Average test loss: 0.0031792955353028246\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05228057502375709\n",
      "Average test loss: 0.003292054273808996\n",
      "Epoch 242/300\n",
      "Average training loss: 0.052372909943262734\n",
      "Average test loss: 0.0032613009789751635\n",
      "Epoch 243/300\n",
      "Average training loss: 0.052060532598031894\n",
      "Average test loss: 0.0031953577351652913\n",
      "Epoch 244/300\n",
      "Average training loss: 0.052051703916655645\n",
      "Average test loss: 0.00323542498693698\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05184452615512742\n",
      "Average test loss: 0.003201411661381523\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05187980093227493\n",
      "Average test loss: 0.0031648619992451534\n",
      "Epoch 247/300\n",
      "Average training loss: 0.051773421458072136\n",
      "Average test loss: 0.003299357229947216\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05175866483648618\n",
      "Average test loss: 0.003157530022992028\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05183442249894142\n",
      "Average test loss: 0.0032578812870714398\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05171533029940393\n",
      "Average test loss: 0.003188208595953054\n",
      "Epoch 251/300\n",
      "Average training loss: 0.051606933019227454\n",
      "Average test loss: 0.0032603494433893097\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05164379714594947\n",
      "Average test loss: 0.0032308396357629034\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05164810485972299\n",
      "Average test loss: 0.003186877995315525\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05143630784418848\n",
      "Average test loss: 0.0031934560723602774\n",
      "Epoch 255/300\n",
      "Average training loss: 0.051407350010342065\n",
      "Average test loss: 0.0032596668758326105\n",
      "Epoch 256/300\n",
      "Average training loss: 0.051420198967059454\n",
      "Average test loss: 0.0031832477044728066\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05129794148935212\n",
      "Average test loss: 0.003263903373438451\n",
      "Epoch 258/300\n",
      "Average training loss: 0.051363818056053584\n",
      "Average test loss: 0.0033141739250471193\n",
      "Epoch 259/300\n",
      "Average training loss: 0.051302416649129656\n",
      "Average test loss: 0.0032658996573752827\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0511223713391357\n",
      "Average test loss: 0.0032536981236189603\n",
      "Epoch 261/300\n",
      "Average training loss: 0.051049157831403943\n",
      "Average test loss: 0.003228894187965327\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05102099894152747\n",
      "Average test loss: 0.0032408555804027453\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0510447567668226\n",
      "Average test loss: 0.003308148901495669\n",
      "Epoch 265/300\n",
      "Average training loss: 0.050859901474581824\n",
      "Average test loss: 0.0033843269497156143\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05077493135796653\n",
      "Average test loss: 0.003193272831125392\n",
      "Epoch 269/300\n",
      "Average training loss: 0.050731489698092144\n",
      "Average test loss: 0.003318264616653323\n",
      "Epoch 270/300\n",
      "Average training loss: 0.050741044716702564\n",
      "Average test loss: 0.0032250129938539533\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05078381216526032\n",
      "Average test loss: 0.003126075594789452\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0507232918507523\n",
      "Average test loss: 0.0031349659810463587\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05046020711130566\n",
      "Average test loss: 0.003291887996097406\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05042712370554606\n",
      "Average test loss: 0.0032734362880388894\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05055229897631539\n",
      "Average test loss: 0.003175034723762009\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05056062965591748\n",
      "Average test loss: 0.003314557431679633\n",
      "Epoch 279/300\n",
      "Average training loss: 0.050414252499739326\n",
      "Average test loss: 0.0032233053193324143\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05014818286564615\n",
      "Average test loss: 0.003198721813658873\n",
      "Epoch 281/300\n",
      "Average test loss: 0.0032250158107943005\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0500871275862058\n",
      "Average test loss: 0.003191629896354344\n",
      "Epoch 284/300\n",
      "Average training loss: 0.050008654591110015\n",
      "Average test loss: 0.0033082510858981145\n",
      "Epoch 285/300\n",
      "Average training loss: 0.050025450776020684\n",
      "Average test loss: 0.003209672215498156\n",
      "Epoch 286/300\n",
      "Average training loss: 0.049868728342983455\n",
      "Average test loss: 0.0032850733204848235\n",
      "Epoch 288/300\n",
      "Average training loss: 0.049972994267940524\n",
      "Average test loss: 0.003259987197402451\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04989800257815255\n",
      "Average test loss: 0.0032176244881831936\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04979641636874941\n",
      "Average test loss: 0.003202193663765987\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04965366651945644\n",
      "Average test loss: 0.003232059444818232\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04980268359846539\n",
      "Average test loss: 0.0032326784214625755\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04967681976159414\n",
      "Average test loss: 0.0031785947425911824\n",
      "Epoch 294/300\n",
      "Average training loss: 0.049613661421669855\n",
      "Average test loss: 0.0031531370505690575\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04959595250752237\n",
      "Average test loss: 0.003190600251572\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04960537661612034\n",
      "Average test loss: 0.003240240266132686\n",
      "Epoch 298/300\n",
      "Average training loss: 0.049452742639515136\n",
      "Average test loss: 0.003305563478420178\n",
      "Epoch 299/300\n",
      "Average training loss: 0.049324361910422644\n",
      "Average test loss: 0.0032671963988492887\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04960303542017937\n",
      "Average test loss: 0.0032086592517379256\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 36.64225708177354\n",
      "Average test loss: 1571.6284393689268\n",
      "Epoch 2/300\n",
      "Average training loss: 12.436308096991645\n",
      "Average test loss: 5.627403020486236\n",
      "Epoch 4/300\n",
      "Average training loss: 10.416880654229058\n",
      "Average test loss: 0.9386057676457696\n",
      "Epoch 5/300\n",
      "Average training loss: 9.07868037753635\n",
      "Average test loss: 0.09654090599798494\n",
      "Epoch 6/300\n",
      "Average training loss: 7.332325272878011\n",
      "Average test loss: 0.040041597933404977\n",
      "Epoch 7/300\n",
      "Average training loss: 6.055938079833984\n",
      "Average test loss: 0.5384473672890001\n",
      "Epoch 8/300\n",
      "Average training loss: 5.823198632558187\n",
      "Average test loss: 0.003198580057463712\n",
      "Epoch 9/300\n",
      "Average training loss: 3.617316891564263\n",
      "Average test loss: 0.24010756836759134\n",
      "Epoch 11/300\n",
      "Average training loss: 2.9888482996622723\n",
      "Average test loss: 0.0226647682475547\n",
      "Epoch 12/300\n",
      "Average training loss: 2.9701535968780517\n",
      "Average test loss: 0.003072253640534149\n",
      "Epoch 13/300\n",
      "Average training loss: 2.504898248884413\n",
      "Average test loss: 0.004140860688355234\n",
      "Epoch 14/300\n",
      "Average training loss: 2.0266373313268025\n",
      "Average test loss: 0.09487233136697776\n",
      "Epoch 15/300\n",
      "Average training loss: 1.7644342601564196\n",
      "Average test loss: 0.003152963228937652\n",
      "Epoch 16/300\n",
      "Average training loss: 1.5654538788265653\n",
      "Average test loss: 0.29105320276402763\n",
      "Epoch 17/300\n",
      "Average training loss: 1.2124383104112413\n",
      "Average test loss: 0.1970045880569766\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9892249585787455\n",
      "Average test loss: 0.7309108127266583\n",
      "Epoch 19/300\n",
      "Average training loss: 0.8061199181344774\n",
      "Average test loss: 0.015543646240192983\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6690877302487691\n",
      "Average test loss: 459.5764244823986\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5612923110855951\n",
      "Average test loss: 0.04198068577713437\n",
      "Epoch 22/300\n",
      "Average training loss: 0.47397813958591883\n",
      "Average test loss: 0.007363461028163632\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4037231477101644\n",
      "Average test loss: 1.1324407536337773\n",
      "Epoch 24/300\n",
      "Average training loss: 0.34357090634769866\n",
      "Average test loss: 14.505224070265061\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2939077041943868\n",
      "Average test loss: 20120.281065863714\n",
      "Epoch 26/300\n",
      "Average training loss: 0.25434966178735097\n",
      "Average test loss: 0.002262098827295833\n",
      "Epoch 27/300\n",
      "Average training loss: 0.22143132752842373\n",
      "Average test loss: 0.0033220816695441802\n",
      "Epoch 28/300\n",
      "Average training loss: 0.19532283385594687\n",
      "Average test loss: 0.014847829169697232\n",
      "Epoch 29/300\n",
      "Average training loss: 0.17432269587781693\n",
      "Average test loss: 0.0027960893671131796\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1570632719596227\n",
      "Average test loss: 0.002207031953872906\n",
      "Epoch 31/300\n",
      "Average training loss: 0.14330839105447132\n",
      "Average test loss: 0.002244238802128368\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1315943925778071\n",
      "Average test loss: 0.0021742385648604898\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12242329205407036\n",
      "Average test loss: 0.0021774595501936143\n",
      "Epoch 34/300\n",
      "Average training loss: 0.11439373298486073\n",
      "Average test loss: 0.0022487964338312547\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10244063055515289\n",
      "Average test loss: 0.002251809402162002\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09759377891487546\n",
      "Average test loss: 0.0021495726311372386\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09365365843640433\n",
      "Average test loss: 0.0021121356294800837\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09030547878477309\n",
      "Average test loss: 0.002091442814303769\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08738691604799695\n",
      "Average test loss: 0.002152760373842385\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08496959232621723\n",
      "Average test loss: 0.0020695161001963746\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08284277637137306\n",
      "Average test loss: 0.0021229216433647606\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0810785154501597\n",
      "Average test loss: 0.0020787839401099416\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07931423838602172\n",
      "Average test loss: 0.002059357040251295\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07775089222523901\n",
      "Average test loss: 0.0021325682513415813\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0763509597910775\n",
      "Average test loss: 0.002094153174199164\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07541165018081665\n",
      "Average test loss: 0.002094102491107252\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07413308445612589\n",
      "Average test loss: 0.0020597654674202204\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0730847853091028\n",
      "Average test loss: 0.0020608182756437195\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07194508493608899\n",
      "Average test loss: 0.002075049104169011\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07095348079337015\n",
      "Average test loss: 0.0020531986615517075\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07021834649973445\n",
      "Average test loss: 0.002105179554575847\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0693837675816483\n",
      "Average test loss: 0.0020382338023434083\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06833974825011359\n",
      "Average test loss: 0.0020609873938891624\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0678088694214821\n",
      "Average test loss: 0.0020888002053317096\n",
      "Epoch 56/300\n",
      "Average training loss: 0.066037395854791\n",
      "Average test loss: 0.0020649395250818795\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06545856270525191\n",
      "Average test loss: 0.0020555669836079082\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06454976330531968\n",
      "Average test loss: 0.0020304498011246325\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06397940040296979\n",
      "Average test loss: 0.0021275250740970175\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06315960023800532\n",
      "Average test loss: 0.0021378521679176225\n",
      "Epoch 62/300\n",
      "Average training loss: 0.062458546539147695\n",
      "Average test loss: 0.002048897271975875\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06240540246168772\n",
      "Average test loss: 0.0021212607336541017\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06118084136313862\n",
      "Average test loss: 0.0020614385091596178\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0605915763411257\n",
      "Average test loss: 0.002117717708978388\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06020779151717822\n",
      "Average test loss: 0.0020849793298790852\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05957820466491911\n",
      "Average test loss: 0.0020641202809703017\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05886500450637606\n",
      "Average test loss: 0.0021700546154752374\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05849711826443672\n",
      "Average test loss: 0.0020989667938815224\n",
      "Epoch 70/300\n",
      "Average training loss: 0.057769118001063664\n",
      "Average test loss: 0.002138408325612545\n",
      "Epoch 71/300\n",
      "Average training loss: 0.057371974527835844\n",
      "Average test loss: 0.002195117218005988\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05672386023733351\n",
      "Average test loss: 0.0021161876490546597\n",
      "Epoch 73/300\n",
      "Average training loss: 0.055647684150271944\n",
      "Average test loss: 0.0021009021327934333\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05556605740719371\n",
      "Average test loss: 0.002097542338176734\n",
      "Epoch 76/300\n",
      "Average training loss: 0.054755787505043875\n",
      "Average test loss: 0.0020684809476758043\n",
      "Epoch 77/300\n",
      "Average training loss: 0.054617546147770354\n",
      "Average test loss: 0.0021085874065756797\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05427927408615748\n",
      "Average test loss: 0.0021457281549357708\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0537993789712588\n",
      "Average test loss: 0.002121968060317967\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05269853353831503\n",
      "Average test loss: 0.002106611378594405\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05227712062332365\n",
      "Average test loss: 0.0021164913535532026\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05197768139839172\n",
      "Average test loss: 0.0022832419930232896\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05177794166074859\n",
      "Average test loss: 0.002075493375253346\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05160235277811686\n",
      "Average test loss: 0.002110475973950492\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0510028712981277\n",
      "Average test loss: 0.00213910838154455\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05055941950943735\n",
      "Average test loss: 0.0021883857902139426\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05008987817168236\n",
      "Average test loss: 0.002167241512797773\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04962373020582729\n",
      "Average test loss: 0.0021278044283390045\n",
      "Epoch 91/300\n",
      "Average training loss: 0.049549965060419504\n",
      "Average test loss: 0.002212580472230911\n",
      "Epoch 92/300\n",
      "Average training loss: 0.049295933473441335\n",
      "Average test loss: 0.0021915562372240753\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0489254181848632\n",
      "Average test loss: 0.002229678387236264\n",
      "Epoch 94/300\n",
      "Average training loss: 0.048762053155236774\n",
      "Average test loss: 0.0021606305345065065\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04849096867110994\n",
      "Average test loss: 0.0021960152570779123\n",
      "Epoch 96/300\n",
      "Average training loss: 0.0481865466468864\n",
      "Average test loss: 0.00218038892135438\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04788598779837291\n",
      "Average test loss: 0.002177180632121033\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04741561997930209\n",
      "Average test loss: 0.002172322092577815\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04726961198780272\n",
      "Average test loss: 0.0021982247545901273\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04716826286911965\n",
      "Average test loss: 0.002174689171421859\n",
      "Epoch 102/300\n",
      "Average training loss: 0.046984971470303005\n",
      "Average test loss: 0.002150647688553565\n",
      "Epoch 103/300\n",
      "Average training loss: 0.046876782188812895\n",
      "Average test loss: 0.002192853880011373\n",
      "Epoch 104/300\n",
      "Average training loss: 0.046260902017354964\n",
      "Average test loss: 0.0021684847575508884\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04600173977017403\n",
      "Average test loss: 0.002199436969641182\n",
      "Epoch 107/300\n",
      "Average training loss: 0.045953203697999316\n",
      "Average test loss: 0.002147062152210209\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04582903837164243\n",
      "Average test loss: 0.0021744229160249235\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04559790441062715\n",
      "Average test loss: 0.0021714259723408353\n",
      "Epoch 110/300\n",
      "Average training loss: 0.045399800164832006\n",
      "Average test loss: 0.0021863092506925266\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04526112177968025\n",
      "Average test loss: 0.002126923620286915\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04488786075678137\n",
      "Average test loss: 0.0022199746400324836\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04466433784696791\n",
      "Average test loss: 0.002220085414333476\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04455926591820187\n",
      "Average test loss: 0.0022139107920229437\n",
      "Epoch 116/300\n",
      "Average training loss: 0.044440333657794526\n",
      "Average test loss: 0.002195651854491896\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04434832703404956\n",
      "Average test loss: 0.002241152847496172\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04415041287408935\n",
      "Average test loss: 0.002190478417608473\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0438829662717051\n",
      "Average test loss: 0.0021805785730895067\n",
      "Epoch 121/300\n",
      "Average training loss: 0.043791122496128085\n",
      "Average test loss: 0.0021874836603593497\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04361054266161389\n",
      "Average test loss: 0.0022465481753978465\n",
      "Epoch 123/300\n",
      "Average training loss: 0.043511164834101994\n",
      "Average test loss: 0.002241935020105706\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04326626308759054\n",
      "Average test loss: 0.002205525331199169\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04316185388300154\n",
      "Average test loss: 0.002122348563124736\n",
      "Epoch 126/300\n",
      "Average training loss: 0.043109113107124966\n",
      "Average test loss: 0.0022136567323986027\n",
      "Epoch 127/300\n",
      "Average training loss: 0.042867654777235455\n",
      "Average test loss: 0.002272071591888865\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04280300713909997\n",
      "Average test loss: 0.0022295911044089333\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04274372851848602\n",
      "Average test loss: 0.0022670720265143447\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04259803080889914\n",
      "Average test loss: 0.0023115716701787377\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04253280857536528\n",
      "Average test loss: 0.0022895972398627134\n",
      "Epoch 132/300\n",
      "Average training loss: 0.042402894973754886\n",
      "Average test loss: 0.0024041150537216\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0421655050681697\n",
      "Average test loss: 0.0022252086696421935\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04192251626153787\n",
      "Average test loss: 0.002254279103750984\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04177068351705869\n",
      "Average test loss: 0.002228422966889209\n",
      "Epoch 137/300\n",
      "Average training loss: 0.041880048675669565\n",
      "Average test loss: 0.002262972036169635\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04174846166703436\n",
      "Average test loss: 0.0022810685945053896\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0415592434571849\n",
      "Average test loss: 0.002253252838427822\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04157385569148594\n",
      "Average test loss: 0.002222006540538536\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04129879744185342\n",
      "Average test loss: 0.002295830058554808\n",
      "Epoch 143/300\n",
      "Average training loss: 0.041138770308759474\n",
      "Average test loss: 0.0021859115382863417\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04113523241215282\n",
      "Average test loss: 0.0022331052672945793\n",
      "Epoch 145/300\n",
      "Average training loss: 0.040967612660593455\n",
      "Average test loss: 0.002239308645327886\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0409371976421939\n",
      "Average test loss: 0.00227261175111764\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0407921547508902\n",
      "Average test loss: 0.0022317257458344102\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04080540319946077\n",
      "Average test loss: 0.002280701974717279\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04055292683177524\n",
      "Average test loss: 0.002302499926338593\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04053951012591521\n",
      "Average test loss: 0.0023351128828815286\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04047598958015442\n",
      "Average test loss: 0.0022926925118598672\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04030056346125073\n",
      "Average test loss: 0.002312087828707364\n",
      "Epoch 154/300\n",
      "Average training loss: 0.040042374428775576\n",
      "Average test loss: 0.0021999083133414387\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04009211100141207\n",
      "Average test loss: 0.002272763375399841\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03987567834556103\n",
      "Average test loss: 0.0022562089106068013\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03985707224905491\n",
      "Average test loss: 0.0022803261338008775\n",
      "Epoch 160/300\n",
      "Average training loss: 0.039876782167288995\n",
      "Average test loss: 0.0023467801625116002\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03960644317501121\n",
      "Average test loss: 0.0023126228226141798\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03952636959817674\n",
      "Average test loss: 0.0023703051906906894\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03957594536410438\n",
      "Average test loss: 0.002273834471280376\n",
      "Epoch 164/300\n",
      "Average training loss: 0.039433368974261813\n",
      "Average test loss: 0.002288958598756128\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03926363357901573\n",
      "Average test loss: 0.002271661509397543\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03931676382819811\n",
      "Average test loss: 0.0022513416419840523\n",
      "Epoch 167/300\n",
      "Average training loss: 0.039240794973240956\n",
      "Average test loss: 0.002245548598571784\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03929684912165006\n",
      "Average test loss: 0.0022872688229092293\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03915889663497607\n",
      "Average test loss: 0.00232136106532481\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0389300159547064\n",
      "Average test loss: 0.0023046240175349847\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03898039022419188\n",
      "Average test loss: 0.0023502967103073993\n",
      "Epoch 172/300\n",
      "Average training loss: 0.038829722884628506\n",
      "Average test loss: 0.0022998799124939575\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03885862926973237\n",
      "Average test loss: 0.002330139208171103\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03875939190718863\n",
      "Average test loss: 0.0022229132286997306\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03858752706978056\n",
      "Average test loss: 0.0022504857875820664\n",
      "Epoch 176/300\n",
      "Average training loss: 0.038577996561924614\n",
      "Average test loss: 0.0023683504420850014\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03858898952272203\n",
      "Average test loss: 0.0022593538771486944\n",
      "Epoch 178/300\n",
      "Average training loss: 0.038633773712648284\n",
      "Average test loss: 0.002314250947907567\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0383635715842247\n",
      "Average test loss: 0.0022765045863472754\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03834170212679439\n",
      "Average test loss: 0.002256870604430636\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03833189797070291\n",
      "Average test loss: 0.002267174300013317\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03820920483602418\n",
      "Average test loss: 0.002284907085200151\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03827734112739563\n",
      "Average test loss: 0.0023416153398445913\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03811407593223784\n",
      "Average test loss: 0.002306521624740627\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03808178111082978\n",
      "Average test loss: 0.0023110540985233252\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03813397585021125\n",
      "Average test loss: 0.002208139149679078\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03795844272772471\n",
      "Average test loss: 0.002257331202841467\n",
      "Epoch 188/300\n",
      "Average training loss: 0.038030877672963675\n",
      "Average test loss: 0.0022207791031234794\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03811620193719864\n",
      "Average test loss: 0.0023187400989441408\n",
      "Epoch 190/300\n",
      "Average training loss: 0.037794536944892675\n",
      "Average test loss: 0.002296087802077333\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03772401054369079\n",
      "Average test loss: 0.00226874754174302\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03763025607996517\n",
      "Average test loss: 0.002377295102510187\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03763065921101305\n",
      "Average test loss: 0.002278967596693999\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03750801828669177\n",
      "Average test loss: 0.002348671172021164\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03750950437453058\n",
      "Average test loss: 0.0022362112205268608\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03750174653198984\n",
      "Average test loss: 0.002293106469946603\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0373264899916119\n",
      "Average test loss: 0.0022993331737816334\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03741102021932602\n",
      "Average test loss: 0.0023804002253131735\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03726613685488701\n",
      "Average test loss: 0.002288955853631099\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03724296286039882\n",
      "Average test loss: 0.0022821398691998587\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03733816330300437\n",
      "Average test loss: 0.0022291237437683676\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0371730718927251\n",
      "Average test loss: 0.002344611373833484\n",
      "Epoch 203/300\n",
      "Average training loss: 0.037042508436573876\n",
      "Average test loss: 0.0023465907314999235\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03713072275122007\n",
      "Average test loss: 0.00238003757264879\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03705844673183229\n",
      "Average test loss: 0.0022995386258181597\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03702142987648646\n",
      "Average test loss: 0.0023360622689748804\n",
      "Epoch 207/300\n",
      "Average training loss: 0.036952526830964616\n",
      "Average test loss: 0.0022993032609423003\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03676499331328604\n",
      "Average test loss: 0.002307419754564762\n",
      "Epoch 209/300\n",
      "Average training loss: 0.036733293622732165\n",
      "Average test loss: 0.002298973476100299\n",
      "Epoch 210/300\n",
      "Average training loss: 0.036751942273643284\n",
      "Average test loss: 0.0023195916870608927\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0367446516222424\n",
      "Average test loss: 0.002367017791916927\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03668724947836664\n",
      "Average test loss: 0.002313031459848086\n",
      "Epoch 213/300\n",
      "Average training loss: 0.036577223479747775\n",
      "Average test loss: 0.002321032466987769\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0365978724790944\n",
      "Average test loss: 0.002284308604378667\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03652039142449697\n",
      "Average test loss: 0.0022692893149538172\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0365354163646698\n",
      "Average test loss: 0.0023343772045854067\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03647504205008348\n",
      "Average test loss: 0.0022982519974725116\n",
      "Epoch 218/300\n",
      "Average training loss: 0.036430678096082476\n",
      "Average test loss: 0.002308836843507985\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03634165867169698\n",
      "Average test loss: 0.0022547338302764627\n",
      "Epoch 220/300\n",
      "Average training loss: 0.036258066091272564\n",
      "Average test loss: 0.002341424889345136\n",
      "Epoch 221/300\n",
      "Average training loss: 0.036278800563679804\n",
      "Average test loss: 0.002266947562288907\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03629673178825114\n",
      "Average test loss: 0.002292677770368755\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03619684508442879\n",
      "Average test loss: 0.002326053661501242\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03616473664840063\n",
      "Average test loss: 0.0023390042204409836\n",
      "Epoch 225/300\n",
      "Average training loss: 0.036057558480236264\n",
      "Average test loss: 0.0023464788436475727\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03606737840837902\n",
      "Average test loss: 0.002398251818286048\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03603819262650278\n",
      "Average test loss: 0.002330942741698689\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03596381073196729\n",
      "Average test loss: 0.002450801047599978\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0360795306066672\n",
      "Average test loss: 0.0023372794636007813\n",
      "Epoch 230/300\n",
      "Average training loss: 0.035834533449676305\n",
      "Average test loss: 0.0023225020182629425\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03573468437128597\n",
      "Average test loss: 0.0023025124561455515\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03583104598853323\n",
      "Average test loss: 0.002399584260251787\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03575231617026859\n",
      "Average test loss: 0.0023831192327456343\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03576747200555271\n",
      "Average test loss: 0.0022870199241571956\n",
      "Epoch 235/300\n",
      "Average training loss: 0.035686317637562755\n",
      "Average test loss: 0.0023336965903225874\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03573655646873845\n",
      "Average test loss: 0.0023352945268981986\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0356865915523635\n",
      "Average test loss: 0.0023565719141107467\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03559019349846575\n",
      "Average test loss: 0.00234206618140969\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035530941608879305\n",
      "Average test loss: 0.0023241012365453773\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03549821501804723\n",
      "Average test loss: 0.0023621764512111744\n",
      "Epoch 241/300\n",
      "Average training loss: 0.035410828255944785\n",
      "Average test loss: 0.0023889079358842638\n",
      "Epoch 242/300\n",
      "Average training loss: 0.035464871125088795\n",
      "Average test loss: 0.002329905901828574\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03554102467828327\n",
      "Average test loss: 0.002278080864188572\n",
      "Epoch 244/300\n",
      "Average training loss: 0.035311771137846844\n",
      "Average test loss: 0.0022847925376974876\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03525384117166201\n",
      "Average test loss: 0.002423933972915014\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03528011827667554\n",
      "Average test loss: 0.0023169113610767658\n",
      "Epoch 247/300\n",
      "Average training loss: 0.035243252706196576\n",
      "Average test loss: 0.0023906922665321165\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03532924241158698\n",
      "Average test loss: 0.0023632863668931856\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03524043978254\n",
      "Average test loss: 0.002390562929626968\n",
      "Epoch 250/300\n",
      "Average training loss: 0.035212719235155315\n",
      "Average test loss: 0.0023375233254498908\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0352312536239624\n",
      "Average test loss: 0.002355754356831312\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03510835453536775\n",
      "Average test loss: 0.002464528364025884\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03503104093008571\n",
      "Average test loss: 0.002398340443149209\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03509121538533105\n",
      "Average test loss: 0.0024411696833040977\n",
      "Epoch 255/300\n",
      "Average training loss: 0.035012248780992294\n",
      "Average test loss: 0.0023743966195939317\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03497844913270738\n",
      "Average test loss: 0.002372118078586128\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03488327342271805\n",
      "Average test loss: 0.002280776826561325\n",
      "Epoch 258/300\n",
      "Average training loss: 0.034942797280020185\n",
      "Average test loss: 0.0023410863706635103\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03483460592892435\n",
      "Average test loss: 0.002341652785324388\n",
      "Epoch 260/300\n",
      "Average training loss: 0.034837238056792154\n",
      "Average test loss: 0.0023102418497308263\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03500078080760108\n",
      "Average test loss: 0.0023159058396187092\n",
      "Epoch 262/300\n",
      "Average training loss: 0.034824457267920175\n",
      "Average test loss: 0.0023963653043740326\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034699346353610355\n",
      "Average test loss: 0.0023396573923528193\n",
      "Epoch 264/300\n",
      "Average training loss: 0.034662552846802606\n",
      "Average test loss: 0.002394276518271201\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03466259242428674\n",
      "Average test loss: 0.0025011077218999463\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03470092465811306\n",
      "Average test loss: 0.002346552207341625\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034743067353963854\n",
      "Average test loss: 0.0022939968630671503\n",
      "Epoch 268/300\n",
      "Average training loss: 0.034572602005468474\n",
      "Average test loss: 0.00233218558339609\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03457709522379769\n",
      "Average test loss: 0.0023275847766134473\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03444160869717598\n",
      "Average test loss: 0.002352663290583425\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03449878745112154\n",
      "Average test loss: 0.0023238115627318623\n",
      "Epoch 272/300\n",
      "Average training loss: 0.034446141395303935\n",
      "Average test loss: 0.002364381872634921\n",
      "Epoch 273/300\n",
      "Average training loss: 0.034357259414262244\n",
      "Average test loss: 0.0023689518566760753\n",
      "Epoch 274/300\n",
      "Average training loss: 0.034443640713890396\n",
      "Average test loss: 0.0023162046492927603\n",
      "Epoch 275/300\n",
      "Average training loss: 0.034375492725107405\n",
      "Average test loss: 0.00235157686451243\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03434726082119677\n",
      "Average test loss: 0.0023478012331244018\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03439133165942298\n",
      "Average test loss: 0.0023569796834554935\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03427809612452984\n",
      "Average test loss: 0.002320135244892703\n",
      "Epoch 279/300\n",
      "Average training loss: 0.034151432611876066\n",
      "Average test loss: 0.002452722887715532\n",
      "Epoch 281/300\n",
      "Average training loss: 0.034169661664300494\n",
      "Average test loss: 0.002340202437299821\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03416814627250036\n",
      "Average test loss: 0.002362335985836883\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03424673364890946\n",
      "Average test loss: 0.0024101756904274223\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03414010566804144\n",
      "Average test loss: 0.0023383930453823674\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03406670643223657\n",
      "Average test loss: 0.0023137069521471858\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03396059490740299\n",
      "Average test loss: 0.002367241040493051\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0338518931766351\n",
      "Average test loss: 0.0023789221565756534\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03403120201494959\n",
      "Average test loss: 0.0024047571892539662\n",
      "Epoch 291/300\n",
      "Average training loss: 0.033895327281620764\n",
      "Average test loss: 0.0023550856140338713\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03394678416020341\n",
      "Average test loss: 0.002295573789936801\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03381669420169459\n",
      "Average test loss: 0.002333117939014402\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03380004306965404\n",
      "Average test loss: 0.002309984945795602\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03380590950118171\n",
      "Average test loss: 0.0023593227579775783\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03378072887162367\n",
      "Average test loss: 0.0023843217425876194\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03369910685552491\n",
      "Average test loss: 0.002342356477346685\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03370893135666847\n",
      "Average test loss: 0.0023486803186436494\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03361980531944169\n",
      "Average test loss: 0.002421199474690689\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 23.503873595343695\n",
      "Average test loss: 163559.03835655487\n",
      "Epoch 2/300\n",
      "Average training loss: 11.400388029310438\n",
      "Average test loss: 0.004369289136595196\n",
      "Epoch 4/300\n",
      "Average training loss: 10.398875307718912\n",
      "Average test loss: 0.0030812282382200163\n",
      "Epoch 5/300\n",
      "Average training loss: 8.396827254825169\n",
      "Average test loss: 0.051750480173362624\n",
      "Epoch 6/300\n",
      "Average training loss: 6.930635690053304\n",
      "Average test loss: 0.005214662395624651\n",
      "Epoch 8/300\n",
      "Average training loss: 5.203708745320638\n",
      "Average test loss: 0.00896137810829613\n",
      "Epoch 9/300\n",
      "Average training loss: 5.043461959203085\n",
      "Average test loss: 0.005057542472870814\n",
      "Epoch 10/300\n",
      "Average training loss: 4.2460898331536185\n",
      "Average test loss: 0.002372783048285378\n",
      "Epoch 11/300\n",
      "Average training loss: 3.885105639139811\n",
      "Average test loss: 3.2977086412426497\n",
      "Epoch 12/300\n",
      "Average training loss: 3.0306709541744654\n",
      "Average test loss: 0.0176660085854431\n",
      "Epoch 14/300\n",
      "Average training loss: 2.7281924154493544\n",
      "Average test loss: 0.006591772325647374\n",
      "Epoch 15/300\n",
      "Average training loss: 2.3848704295688203\n",
      "Average test loss: 0.0021625925526022913\n",
      "Epoch 16/300\n",
      "Average training loss: 2.1393899235195586\n",
      "Average test loss: 0.0019174382870809899\n",
      "Epoch 17/300\n",
      "Average training loss: 1.888789218266805\n",
      "Average test loss: 0.01830617429398828\n",
      "Epoch 18/300\n",
      "Average training loss: 1.7097115851508247\n",
      "Average test loss: 0.022924559724827608\n",
      "Epoch 19/300\n",
      "Average training loss: 1.316536030769348\n",
      "Average test loss: 0.0018695647252930535\n",
      "Epoch 21/300\n",
      "Average training loss: 1.1339315487543742\n",
      "Average test loss: 0.0017461145182864533\n",
      "Epoch 22/300\n",
      "Average training loss: 0.9758008850945367\n",
      "Average test loss: 0.001706118378891713\n",
      "Epoch 23/300\n",
      "Average training loss: 0.8477435794936287\n",
      "Average test loss: 0.0017138892925447889\n",
      "Epoch 24/300\n",
      "Average training loss: 0.7331384121047126\n",
      "Average test loss: 0.0016843951084754532\n",
      "Epoch 25/300\n",
      "Average training loss: 0.634901002989875\n",
      "Average test loss: 0.0016386551932535238\n",
      "Epoch 26/300\n",
      "Average training loss: 0.46873224433263144\n",
      "Average test loss: 0.0016106280681884124\n",
      "Epoch 28/300\n",
      "Average training loss: 0.39943943678008187\n",
      "Average test loss: 0.0015937937619164586\n",
      "Epoch 29/300\n",
      "Average training loss: 0.34328564707438153\n",
      "Average test loss: 0.0015793863245182568\n",
      "Epoch 30/300\n",
      "Average training loss: 0.29798953829871283\n",
      "Average test loss: 0.0015648807705276543\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2586271367867788\n",
      "Average test loss: 0.0015414829246906772\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1975864408546024\n",
      "Average test loss: 0.0015357962291066844\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1736531685060925\n",
      "Average test loss: 0.0015092994509678748\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15318937260574764\n",
      "Average test loss: 0.0015014169021095667\n",
      "Epoch 36/300\n",
      "Average training loss: 0.13642268652386136\n",
      "Average test loss: 0.0014884148261820277\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12223504695627424\n",
      "Average test loss: 0.0015076201874762774\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11021897646453646\n",
      "Average test loss: 0.0014787890882127816\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09201115399599075\n",
      "Average test loss: 0.0014947838089946243\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0854084819091691\n",
      "Average test loss: 0.001460208604319228\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07953456872039372\n",
      "Average test loss: 0.0030409532394260167\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07472389855649736\n",
      "Average test loss: 0.0014623562617020476\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07082748810781372\n",
      "Average test loss: 0.0014454372118020223\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06754733045895894\n",
      "Average test loss: 0.0014540834471893807\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06249862296713723\n",
      "Average test loss: 0.001432261964927117\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06045765138996972\n",
      "Average test loss: 0.0014395761489868165\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05884544598725107\n",
      "Average test loss: 0.0014474551652351188\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05739851121438874\n",
      "Average test loss: 0.0014403656034006013\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05612198709448179\n",
      "Average test loss: 0.0014629766903300253\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05507657970322503\n",
      "Average test loss: 0.0015296604917902086\n",
      "Epoch 53/300\n",
      "Average training loss: 0.054070852782991195\n",
      "Average test loss: 0.001444454447987179\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05299764412972662\n",
      "Average test loss: 0.0014638995809170107\n",
      "Epoch 55/300\n",
      "Average training loss: 0.052118325508303115\n",
      "Average test loss: 0.001444373555481434\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05136570421192381\n",
      "Average test loss: 0.001437207783675856\n",
      "Epoch 57/300\n",
      "Average training loss: 0.050619370496935316\n",
      "Average test loss: 0.0014517824291251599\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04975727952188916\n",
      "Average test loss: 0.001431693867676788\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04912914286388291\n",
      "Average test loss: 0.001489130730740726\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04848613722787963\n",
      "Average test loss: 0.0014648387774618135\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04752480664849281\n",
      "Average test loss: 0.0014506359638956686\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04714288472466999\n",
      "Average test loss: 0.0014348508784961368\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04639687849415673\n",
      "Average test loss: 0.0014760324095065395\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04601329840885268\n",
      "Average test loss: 0.0014437610414913958\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04548548395766152\n",
      "Average test loss: 0.0015700169558533364\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04455283940500683\n",
      "Average test loss: 0.0014478826013704141\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04392381063103676\n",
      "Average test loss: 0.0014828642111064658\n",
      "Epoch 68/300\n",
      "Average training loss: 0.043486432671546935\n",
      "Average test loss: 0.0014816695985694725\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04293041439519988\n",
      "Average test loss: 0.0015036534751868911\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04241239872574806\n",
      "Average test loss: 0.0014544034621988733\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04189200382100211\n",
      "Average test loss: 0.001486816199703349\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04189786798424191\n",
      "Average test loss: 0.001532995611222254\n",
      "Epoch 73/300\n",
      "Average training loss: 0.041443713873624805\n",
      "Average test loss: 0.001447320665853719\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04065550653802024\n",
      "Average test loss: 0.0014907732254101171\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04018197732667128\n",
      "Average test loss: 0.0014691339541847508\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03974135882821348\n",
      "Average test loss: 0.001546277111603154\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03941636821462048\n",
      "Average test loss: 0.0014389995660425887\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03890715100367864\n",
      "Average test loss: 0.0015287669602160653\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03866288378669156\n",
      "Average test loss: 0.0015476736738863918\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03826263557374478\n",
      "Average test loss: 0.0015173448509433203\n",
      "Epoch 81/300\n",
      "Average training loss: 0.037935215794377856\n",
      "Average test loss: 0.0014790860367938877\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03770646417968803\n",
      "Average test loss: 0.0015310943354335095\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03736488741305139\n",
      "Average test loss: 0.0014624215631435316\n",
      "Epoch 84/300\n",
      "Average training loss: 0.036877148868309124\n",
      "Average test loss: 0.0015111344680190087\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03673481613728735\n",
      "Average test loss: 0.0015396033015309108\n",
      "Epoch 86/300\n",
      "Average training loss: 0.036553696566157874\n",
      "Average test loss: 0.0014785388067571654\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0360017037888368\n",
      "Average test loss: 0.00148632545179377\n",
      "Epoch 88/300\n",
      "Average training loss: 0.035771143207947415\n",
      "Average test loss: 0.0014739811582904724\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03567251916064156\n",
      "Average test loss: 0.001449247812748783\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03537899922662311\n",
      "Average test loss: 0.001500965587898261\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0349921621647146\n",
      "Average test loss: 0.0014988396467847957\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03484865111278163\n",
      "Average test loss: 0.0015109771060653858\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03490691248410278\n",
      "Average test loss: 0.001500972551604112\n",
      "Epoch 94/300\n",
      "Average training loss: 0.034558611096607315\n",
      "Average test loss: 0.0015919621291880805\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03403170150021712\n",
      "Average test loss: 0.0015753145741505756\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03390780572593212\n",
      "Average test loss: 0.0015377835952159432\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03384177797204918\n",
      "Average test loss: 0.0015499943496348958\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033795448849598564\n",
      "Average test loss: 0.001512351014237437\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0334294493496418\n",
      "Average test loss: 0.001511893303754429\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03320139343539874\n",
      "Average test loss: 0.001515266321392523\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03298413419061237\n",
      "Average test loss: 0.0015191968602335287\n",
      "Epoch 102/300\n",
      "Average training loss: 0.032899394025405246\n",
      "Average test loss: 0.0015491652342801293\n",
      "Epoch 103/300\n",
      "Average training loss: 0.032806640475988386\n",
      "Average test loss: 0.0014967537633039886\n",
      "Epoch 104/300\n",
      "Average training loss: 0.032519194030099445\n",
      "Average test loss: 0.0015213704859423968\n",
      "Epoch 105/300\n",
      "Average training loss: 0.033593410043252836\n",
      "Average test loss: 0.0015653822732468446\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03215752651790778\n",
      "Average test loss: 0.0015172927798703312\n",
      "Epoch 107/300\n",
      "Average training loss: 0.031971520220239955\n",
      "Average test loss: 0.0015147905598084132\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03201071952945656\n",
      "Average test loss: 0.0015177385188225243\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03180240430268976\n",
      "Average test loss: 0.0015470531104753414\n",
      "Epoch 110/300\n",
      "Average training loss: 0.031797316835986245\n",
      "Average test loss: 0.001541939980453915\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03145504685242971\n",
      "Average test loss: 0.001534168031377097\n",
      "Epoch 112/300\n",
      "Average training loss: 0.031406065531902845\n",
      "Average test loss: 0.001586877213480572\n",
      "Epoch 113/300\n",
      "Average training loss: 0.031214690276318128\n",
      "Average test loss: 0.0015413746210849947\n",
      "Epoch 114/300\n",
      "Average training loss: 0.031088392289148437\n",
      "Average test loss: 0.0017454246394336223\n",
      "Epoch 115/300\n",
      "Average training loss: 0.031062520747383436\n",
      "Average test loss: 0.001570615645394557\n",
      "Epoch 116/300\n",
      "Average training loss: 0.030852341862188444\n",
      "Average test loss: 0.001592632822278473\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03080849763419893\n",
      "Average test loss: 0.0015681175781517393\n",
      "Epoch 118/300\n",
      "Average training loss: 0.030641739971107907\n",
      "Average test loss: 0.0015463720977616808\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030568696015410953\n",
      "Average test loss: 0.00152984786985649\n",
      "Epoch 120/300\n",
      "Average training loss: 0.030360582104987568\n",
      "Average training loss: 0.030318378852473366\n",
      "Average training loss: 0.03018767324421141\n",
      "Average test loss: 0.0015909271830072005\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0300761263254616\n",
      "Average test loss: 0.0015811240231204365\n",
      "Epoch 124/300\n",
      "Average training loss: 0.030094187671939533\n",
      "Average test loss: 0.0015803479318403535\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02971627153125074\n",
      "Average test loss: 0.0015372027402950658\n",
      "Epoch 126/300\n",
      "Average training loss: 0.029848444339301852\n",
      "Average test loss: 0.001581170156908532\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02971879265209039\n",
      "Average test loss: 0.0016122987124448022\n",
      "Epoch 128/300\n",
      "Average training loss: 0.029566552648941674\n",
      "Average test loss: 0.0015478654943613542\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02936033833358023\n",
      "Average test loss: 0.0015803776435140107\n",
      "Epoch 131/300\n",
      "Average training loss: 0.029320399656891823\n",
      "Average test loss: 0.002150095810389353\n",
      "Epoch 132/300\n",
      "Average training loss: 0.029177569690677856\n",
      "Average test loss: 0.00155032958333484\n",
      "Epoch 133/300\n",
      "Average training loss: 0.029141816589567396\n",
      "Average test loss: 0.0016449008936372895\n",
      "Epoch 134/300\n",
      "Average training loss: 0.029046498831775455\n",
      "Average test loss: 0.001608716271093322\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028980166082580885\n",
      "Average test loss: 0.0015791733453257217\n",
      "Epoch 136/300\n",
      "Average training loss: 0.028910805607835453\n",
      "Average test loss: 0.0015779973320249054\n",
      "Epoch 137/300\n",
      "Average training loss: 0.028764164600107407\n",
      "Average test loss: 0.0015803720506115092\n",
      "Epoch 138/300\n",
      "Average training loss: 0.028699003098739517\n",
      "Average test loss: 0.001556198452702827\n",
      "Epoch 139/300\n",
      "Average training loss: 0.028589036880267992\n",
      "Average test loss: 0.001623880028207269\n",
      "Epoch 140/300\n",
      "Average training loss: 0.02857538323601087\n",
      "Average test loss: 0.0015817122254003252\n",
      "Epoch 141/300\n",
      "Average training loss: 0.028462416605816946\n",
      "Average test loss: 0.0016010279373990166\n",
      "Epoch 142/300\n",
      "Average training loss: 0.028428902937306297\n",
      "Average test loss: 0.0015877794544729921\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02835802663034863\n",
      "Average test loss: 0.001574473638087511\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0282455318785376\n",
      "Average test loss: 0.0015983722008143862\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028102837031086285\n",
      "Average test loss: 0.0019309852638592323\n",
      "Epoch 146/300\n",
      "Average training loss: 0.028115628633234236\n",
      "Average test loss: 0.0016057095669416918\n",
      "Epoch 147/300\n",
      "Average training loss: 0.028026589984695118\n",
      "Average test loss: 0.0015924153397273687\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02792472867998812\n",
      "Average test loss: 0.0015955139794904326\n",
      "Epoch 149/300\n",
      "Average training loss: 0.027911433612306913\n",
      "Average test loss: 0.0016342198064343797\n",
      "Epoch 150/300\n",
      "Average training loss: 0.027876267429855134\n",
      "Average test loss: 0.002134242579754856\n",
      "Epoch 151/300\n",
      "Average training loss: 0.027635051634576586\n",
      "Average test loss: 0.0016623590023567278\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02769725765287876\n",
      "Average test loss: 0.0016395762237823672\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02766316585573885\n",
      "Average test loss: 0.0015839949193307095\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027557495849000083\n",
      "Average test loss: 0.0015889034496827258\n",
      "Epoch 155/300\n",
      "Average training loss: 0.027451746217078632\n",
      "Average test loss: 0.0016420653279249867\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02754303822086917\n",
      "Average test loss: 0.0016436774048229886\n",
      "Epoch 157/300\n",
      "Average training loss: 0.027310907145341238\n",
      "Average test loss: 0.0015797993246879842\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027258330980936687\n",
      "Average test loss: 0.001605980608612299\n",
      "Epoch 160/300\n",
      "Average training loss: 0.027146131272117298\n",
      "Average test loss: 0.0015922408241571652\n",
      "Epoch 161/300\n",
      "Average training loss: 0.027169798905650775\n",
      "Average test loss: 0.0016226438854096664\n",
      "Epoch 162/300\n",
      "Average training loss: 0.027069566435284086\n",
      "Average test loss: 0.0015859659769468838\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027107539614041647\n",
      "Average test loss: 0.002704619624548488\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027014355435967446\n",
      "Average test loss: 0.0016160885403967565\n",
      "Epoch 165/300\n",
      "Average training loss: 0.026969166237446995\n",
      "Average test loss: 0.0015828889768777622\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026870007544755936\n",
      "Average test loss: 0.0019279157059888044\n",
      "Epoch 167/300\n",
      "Average training loss: 0.026837770043147934\n",
      "Average test loss: 0.0015962681172208653\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026718812377916443\n",
      "Average test loss: 0.0016061890565065874\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0267247966080904\n",
      "Average test loss: 0.0016805118895653222\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02663569781680902\n",
      "Average test loss: 0.0016032700354440344\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026761181326376067\n",
      "Average test loss: 0.0016111040939059522\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0266687620083491\n",
      "Average test loss: 0.0015621801828965545\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02652820821437571\n",
      "Average test loss: 0.0015973882944219642\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026474018818802302\n",
      "Average test loss: 0.0016197812700023254\n",
      "Epoch 175/300\n",
      "Average training loss: 0.026426883281932937\n",
      "Average test loss: 0.0015773051179324587\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026361996079484622\n",
      "Average test loss: 0.0015815147317738997\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026333019122481347\n",
      "Average test loss: 0.0016705513636892041\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0263598440653748\n",
      "Average test loss: 0.0016374540933304363\n",
      "Epoch 179/300\n",
      "Average training loss: 0.026286459503902328\n",
      "Average test loss: 0.0016814420032832357\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026208520475361083\n",
      "Average test loss: 0.0016346577265196377\n",
      "Epoch 181/300\n",
      "Average training loss: 0.026108405356605847\n",
      "Average test loss: 0.001642975891733335\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02613943095339669\n",
      "Average test loss: 0.0016617909624344772\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02607393011616336\n",
      "Average test loss: 0.0016495474997080034\n",
      "Epoch 184/300\n",
      "Average training loss: 0.026030982067187628\n",
      "Average test loss: 0.0016468129757170875\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026048933477865324\n",
      "Average test loss: 0.0016247367498775324\n",
      "Epoch 186/300\n",
      "Average training loss: 0.025911914742655223\n",
      "Average test loss: 0.001604053390522798\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02596486658685737\n",
      "Average test loss: 0.0015963160546703472\n",
      "Epoch 188/300\n",
      "Average training loss: 0.025832802960442172\n",
      "Average test loss: 0.0016664892019083103\n",
      "Epoch 189/300\n",
      "Average training loss: 0.025793608723415268\n",
      "Average test loss: 0.001599543339572847\n",
      "Epoch 190/300\n",
      "Average training loss: 0.025789492522676784\n",
      "Average test loss: 0.001702966394006378\n",
      "Epoch 191/300\n",
      "Average training loss: 0.025749995860788556\n",
      "Average test loss: 0.0016339241485628818\n",
      "Epoch 192/300\n",
      "Average training loss: 0.025681735401352247\n",
      "Average test loss: 0.0016632087481104666\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02567780951824453\n",
      "Average test loss: 0.001604313747646908\n",
      "Epoch 194/300\n",
      "Average training loss: 0.025555945242444675\n",
      "Average test loss: 0.0015814757386429443\n",
      "Epoch 195/300\n",
      "Average training loss: 0.025618382331397797\n",
      "Average test loss: 0.0016050919857290057\n",
      "Epoch 196/300\n",
      "Average training loss: 0.025519638266828324\n",
      "Average test loss: 0.001622116905119684\n",
      "Epoch 197/300\n",
      "Average training loss: 0.025529237606459195\n",
      "Average test loss: 0.001635599942670928\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02541940225329664\n",
      "Average test loss: 0.0016123102930270963\n",
      "Epoch 199/300\n",
      "Average training loss: 0.025443111098474927\n",
      "Average test loss: 0.0016284770729641119\n",
      "Epoch 200/300\n",
      "Average training loss: 0.025399154775672488\n",
      "Average test loss: 0.001598412544052634\n",
      "Epoch 201/300\n",
      "Average training loss: 0.025318037206927935\n",
      "Average test loss: 0.0016445257632682721\n",
      "Epoch 202/300\n",
      "Average training loss: 0.025358055737283496\n",
      "Average test loss: 0.0016804447589028213\n",
      "Epoch 203/300\n",
      "Average training loss: 0.025276498946878646\n",
      "Average test loss: 0.001649369251707362\n",
      "Epoch 204/300\n",
      "Average training loss: 0.025241496720247798\n",
      "Average test loss: 0.0016290999080778824\n",
      "Epoch 205/300\n",
      "Average training loss: 0.025292499951190418\n",
      "Average test loss: 0.0016074174924029245\n",
      "Epoch 206/300\n",
      "Average training loss: 0.025215404520432156\n",
      "Average test loss: 0.0016391107850811549\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02512342747880353\n",
      "Average test loss: 0.0015961546016236147\n",
      "Epoch 208/300\n",
      "Average training loss: 0.025110255156954128\n",
      "Average test loss: 0.0016302163665079408\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02509228461318546\n",
      "Average test loss: 0.00167712611425668\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025099719580676822\n",
      "Average test loss: 0.0016449557374128038\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02504384387864007\n",
      "Average test loss: 0.0016533658018128739\n",
      "Epoch 212/300\n",
      "Average training loss: 0.024977393105626106\n",
      "Average test loss: 0.0016538132500524322\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02496113223499722\n",
      "Average test loss: 0.0016348833749070763\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02486244912445545\n",
      "Average test loss: 0.0016082725701853633\n",
      "Epoch 216/300\n",
      "Average training loss: 0.024836552586820392\n",
      "Average test loss: 0.0015819515006409751\n",
      "Epoch 217/300\n",
      "Average training loss: 0.024837634581658576\n",
      "Average test loss: 0.0016273753607852592\n",
      "Epoch 218/300\n",
      "Average training loss: 0.024844708376460606\n",
      "Average test loss: 0.0016744343143784337\n",
      "Epoch 219/300\n",
      "Average training loss: 0.024744998824265268\n",
      "Average test loss: 0.0016004519400497279\n",
      "Epoch 220/300\n",
      "Average training loss: 0.024764501298467318\n",
      "Average test loss: 0.0017020798417118688\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02473045917848746\n",
      "Average test loss: 0.0016380050823920304\n",
      "Epoch 222/300\n",
      "Average training loss: 0.024601070529884764\n",
      "Average test loss: 0.001639252661416928\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02472172564930386\n",
      "Average test loss: 0.0016229537156307035\n",
      "Epoch 225/300\n",
      "Average training loss: 0.024658414428432782\n",
      "Average test loss: 0.001755006775777373\n",
      "Epoch 226/300\n",
      "Average training loss: 0.024564197255505456\n",
      "Average test loss: 0.001697899070257942\n",
      "Epoch 227/300\n",
      "Average training loss: 0.024509128951364093\n",
      "Average test loss: 0.0017212079322586457\n",
      "Epoch 228/300\n",
      "Average training loss: 0.024549519654777315\n",
      "Average training loss: 0.024452562330497637\n",
      "Average test loss: 0.0016373397020830048\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02449500157766872\n",
      "Average test loss: 0.0016568566914647817\n",
      "Epoch 231/300\n",
      "Average training loss: 0.024418680219186677\n",
      "Average test loss: 0.0016459519038390781\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024400088629788822\n",
      "Average test loss: 0.0016238236223450966\n",
      "Epoch 233/300\n",
      "Average training loss: 0.024396178079975975\n",
      "Average test loss: 0.0016582873024874264\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0244176799721188\n",
      "Average test loss: 0.00165084833258556\n",
      "Epoch 235/300\n",
      "Average training loss: 0.024259835624032552\n",
      "Average training loss: 0.024255031223098435\n",
      "Average test loss: 0.001619323333673593\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02435408737924364\n",
      "Average test loss: 0.0016728060817759898\n",
      "Epoch 238/300\n",
      "Average training loss: 0.024227354428834384\n",
      "Average test loss: 0.0016803269390430716\n",
      "Epoch 239/300\n",
      "Average training loss: 0.024235786452889443\n",
      "Average test loss: 0.0016284855090909535\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02420133200618956\n",
      "Average test loss: 0.001626605404747857\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02410226710471842\n",
      "Average test loss: 0.0016636878742008574\n",
      "Epoch 242/300\n",
      "Average training loss: 0.024158310979604723\n",
      "Average test loss: 0.0016587810513253012\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02417163981497288\n",
      "Average test loss: 0.001759005479618079\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02409235079586506\n",
      "Average test loss: 0.0016729735990779267\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02398040916191207\n",
      "Average test loss: 0.0016895101063160432\n",
      "Epoch 247/300\n",
      "Average training loss: 0.024022847563028336\n",
      "Average test loss: 0.0016356877701150046\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023937432433168093\n",
      "Average test loss: 0.0016961614610627293\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023917889825171894\n",
      "Average test loss: 0.0016361558510818415\n",
      "Epoch 251/300\n",
      "Average training loss: 0.023852100167009567\n",
      "Average test loss: 0.0016635869275778532\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02400260520643658\n",
      "Average test loss: 0.0016531704968462389\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023884730231430795\n",
      "Average test loss: 0.0016801858519514402\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02390985283917851\n",
      "Average test loss: 0.0016732189133763314\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02379367393420802\n",
      "Average test loss: 0.0016877155779964393\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023799034347136816\n",
      "Average test loss: 0.0016733683265952601\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023786726165148948\n",
      "Average test loss: 0.00173038746509701\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02369057499203417\n",
      "Average test loss: 0.0016960364930952588\n",
      "Epoch 259/300\n",
      "Average training loss: 0.023801615580916403\n",
      "Average test loss: 0.0016590573996719386\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023655320760276582\n",
      "Average test loss: 0.001625067519955337\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02372914301024543\n",
      "Average test loss: 0.0017050067497831251\n",
      "Epoch 262/300\n",
      "Average training loss: 0.023644465878605844\n",
      "Average test loss: 0.0016300323669695192\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02361041341556443\n",
      "Average test loss: 0.0016352406030313836\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023629702303144668\n",
      "Average test loss: 0.001654164206650522\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02360946616033713\n",
      "Average test loss: 0.001628840947523713\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023490755452050104\n",
      "Average test loss: 0.0016733726248559025\n",
      "Epoch 268/300\n",
      "Average training loss: 0.023482468691137102\n",
      "Average test loss: 0.0016379853203478787\n",
      "Epoch 269/300\n",
      "Average training loss: 0.023607703069845835\n",
      "Average test loss: 0.0016524997006894815\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02357104249133004\n",
      "Average test loss: 0.0016134924534708262\n",
      "Epoch 271/300\n",
      "Average training loss: 0.023433684789472158\n",
      "Average test loss: 0.001629919853164918\n",
      "Epoch 273/300\n",
      "Average training loss: 0.023391018521454598\n",
      "Average test loss: 0.001685159042943269\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023443997350003985\n",
      "Average test loss: 0.001672434613108635\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023411440208554266\n",
      "Average test loss: 0.0016504799759843283\n",
      "Epoch 276/300\n",
      "Average training loss: 0.023351227906015185\n",
      "Average test loss: 0.001651247270198332\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023385641935798857\n",
      "Average training loss: 0.023279352337121962\n",
      "Average test loss: 0.0016875429117224282\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023389005134503045\n",
      "Average test loss: 0.001663889056796001\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02327782152261999\n",
      "Average test loss: 0.0016790633675538831\n",
      "Epoch 281/300\n",
      "Average training loss: 0.023276605778270298\n",
      "Average test loss: 0.0016528313606977462\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023273908481001852\n",
      "Average test loss: 0.0017120325416326522\n",
      "Epoch 283/300\n",
      "Average training loss: 0.023236235292421446\n",
      "Average test loss: 0.001647615231366621\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02313521038989226\n",
      "Average test loss: 0.0016632182730051378\n",
      "Epoch 286/300\n",
      "Average training loss: 0.023172088738116953\n",
      "Average test loss: 0.0017247519168174929\n",
      "Epoch 287/300\n",
      "Average training loss: 0.023129600433839693\n",
      "Average test loss: 0.0016571129816066888\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02320131276216772\n",
      "Average test loss: 0.0016826540815333526\n",
      "Epoch 290/300\n",
      "Average training loss: 0.023087873796621958\n",
      "Average test loss: 0.0016957755888708764\n",
      "Epoch 291/300\n",
      "Average training loss: 0.023083355309234724\n",
      "Average test loss: 0.0016962613119847244\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02304417898837063\n",
      "Average test loss: 0.0016761012430199318\n",
      "Epoch 293/300\n",
      "Average test loss: 0.0016985166352242232\n",
      "Epoch 294/300\n",
      "Average training loss: 0.023046079178651174\n",
      "Average test loss: 0.0016777635124615496\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02301264626946714\n",
      "Average test loss: 0.0017076355534502202\n",
      "Epoch 296/300\n",
      "Average training loss: 0.023030561374293435\n",
      "Average test loss: 0.0016372946167571678\n",
      "Epoch 297/300\n",
      "Average training loss: 0.022990959644317625\n",
      "Average test loss: 0.0016505331417752637\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02299035286737813\n",
      "Average test loss: 0.001726949829297761\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022899325577749145\n",
      "Average test loss: 0.001663497521231572\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02291211201912827\n",
      "Average test loss: 0.0016827354985806677\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth10-.01/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.91\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.70\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.80\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.38\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.00\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.69\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.55\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.16\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.20\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.50\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.89\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.63\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.72\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.96\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.25\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.30\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.31\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.41\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.44\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.49\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.59\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.66\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.71\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.83\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.95\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.07\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
