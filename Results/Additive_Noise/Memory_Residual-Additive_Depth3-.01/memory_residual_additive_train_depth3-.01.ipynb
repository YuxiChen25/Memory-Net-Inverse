{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.01)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.01)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.01)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.01)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.24352523883846072\n",
      "Average test loss: 0.011312417642109924\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06594387014706929\n",
      "Average test loss: 0.009837653728822867\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05952252914177047\n",
      "Average test loss: 0.009221684822605715\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05597749098141988\n",
      "Average test loss: 0.009063741586688491\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05431898597213957\n",
      "Average test loss: 0.01001914536456267\n",
      "Epoch 6/300\n",
      "Average training loss: 0.052557628197802436\n",
      "Average test loss: 0.008771701109078196\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05080292812983195\n",
      "Average test loss: 0.008849304373065631\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04971153234110938\n",
      "Average test loss: 0.008515717866520087\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04872854452000724\n",
      "Average test loss: 0.008272285001145468\n",
      "Epoch 10/300\n",
      "Average training loss: 0.047995075249009665\n",
      "Average test loss: 0.008426337146096759\n",
      "Epoch 11/300\n",
      "Average training loss: 0.047293759336074194\n",
      "Average test loss: 0.00809645063843992\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04666765071948369\n",
      "Average test loss: 0.0088820740448104\n",
      "Epoch 13/300\n",
      "Average training loss: 0.045970152106550005\n",
      "Average test loss: 0.0077754997735222185\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04545914547642072\n",
      "Average test loss: 0.007775471759339174\n",
      "Epoch 15/300\n",
      "Average training loss: 0.044829346898529264\n",
      "Average test loss: 0.007887469047473537\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04434902349445555\n",
      "Average test loss: 0.007556973989225096\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04383720193306605\n",
      "Average test loss: 0.007381530130902926\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04339809710118506\n",
      "Average test loss: 0.0075587192649642625\n",
      "Epoch 19/300\n",
      "Average training loss: 0.043082879447274736\n",
      "Average test loss: 0.007210347478588422\n",
      "Epoch 20/300\n",
      "Average training loss: 0.042648265798886614\n",
      "Average test loss: 0.007275557086699538\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04230218138628536\n",
      "Average test loss: 0.0071417027190327645\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04197848885258039\n",
      "Average test loss: 0.007051649528659052\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04158257300986184\n",
      "Average test loss: 0.006949964335809152\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04139538565940327\n",
      "Average test loss: 0.007394663833495643\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04098981884949737\n",
      "Average test loss: 0.006909043706125683\n",
      "Epoch 26/300\n",
      "Average training loss: 0.040850223859151207\n",
      "Average test loss: 0.006857466518878937\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04053914513852861\n",
      "Average test loss: 0.006986338992499643\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04027523942126168\n",
      "Average test loss: 0.006759051308035851\n",
      "Epoch 29/300\n",
      "Average training loss: 0.040045639521545835\n",
      "Average test loss: 0.006738892258041435\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03985658692651325\n",
      "Average test loss: 0.006908725445883141\n",
      "Epoch 31/300\n",
      "Average training loss: 0.039683567103412414\n",
      "Average test loss: 0.00664676392554409\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03942848978771104\n",
      "Average test loss: 0.006627873579661051\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03937939598494106\n",
      "Average test loss: 0.00663791706909736\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03905797554718123\n",
      "Average test loss: 0.006544518462899659\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03893264737394121\n",
      "Average test loss: 0.006873177227460676\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03878679572542509\n",
      "Average test loss: 0.006477195181780391\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03860060520635711\n",
      "Average test loss: 0.006594882982472579\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03846776224838363\n",
      "Average test loss: 0.006722821486079031\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03840026534265942\n",
      "Average test loss: 0.0065144141018390654\n",
      "Epoch 40/300\n",
      "Average training loss: 0.038224628193510905\n",
      "Average test loss: 0.006387299442456828\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03820173401633899\n",
      "Average test loss: 0.006391778626375728\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03794833399189843\n",
      "Average test loss: 0.006419219160245525\n",
      "Epoch 43/300\n",
      "Average training loss: 0.037843541158570186\n",
      "Average test loss: 0.0063176484968927174\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03774938226408429\n",
      "Average test loss: 0.006277109362598923\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03768239513039589\n",
      "Average test loss: 0.006335458620968792\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03748663863208559\n",
      "Average test loss: 0.006231155262225204\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03770094308257103\n",
      "Average test loss: 0.006257339475469457\n",
      "Epoch 48/300\n",
      "Average training loss: 0.037322327251235646\n",
      "Average test loss: 0.0062948452066630125\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03725945888625251\n",
      "Average test loss: 0.006179578026135763\n",
      "Epoch 50/300\n",
      "Average training loss: 0.037112545013427735\n",
      "Average test loss: 0.006199747473415401\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03703223698503441\n",
      "Average test loss: 0.0061691633247666884\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0370380302104685\n",
      "Average test loss: 0.0062118605532579954\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03691682204935286\n",
      "Average test loss: 0.006169648345973756\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03682923170261913\n",
      "Average test loss: 0.006142778069608741\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03670351777474085\n",
      "Average test loss: 0.006238014279968208\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03671903694669405\n",
      "Average test loss: 0.0061217272281646725\n",
      "Epoch 57/300\n",
      "Average training loss: 0.036621917443143\n",
      "Average test loss: 0.0060939635754459435\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03653862647546662\n",
      "Average test loss: 0.0061174832462436626\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03648995130923059\n",
      "Average test loss: 0.006135407737559742\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0363909564183818\n",
      "Average test loss: 0.006163942485219903\n",
      "Epoch 61/300\n",
      "Average training loss: 0.036329110062784616\n",
      "Average test loss: 0.006123588264816337\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03627807828783989\n",
      "Average test loss: 0.006150360696017742\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0362589935362339\n",
      "Average test loss: 0.006115121875372198\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03617599426044358\n",
      "Average test loss: 0.006162796081354221\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03610832307073805\n",
      "Average test loss: 0.006135143653386169\n",
      "Epoch 66/300\n",
      "Average training loss: 0.036117636809746424\n",
      "Average test loss: 0.006360681621564759\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036050836241907545\n",
      "Average test loss: 0.006067188442995151\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03601758498615689\n",
      "Average test loss: 0.006253371083074146\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03635307932231161\n",
      "Average test loss: 0.006086262979027298\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03586664185590214\n",
      "Average test loss: 0.006084609764317671\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03579922784864902\n",
      "Average test loss: 0.006414875467204385\n",
      "Epoch 72/300\n",
      "Average training loss: 0.035798684148324864\n",
      "Average test loss: 0.006192516213489903\n",
      "Epoch 73/300\n",
      "Average training loss: 0.035816537936528524\n",
      "Average test loss: 0.006196805917968353\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03578745544287894\n",
      "Average test loss: 0.006193212416023016\n",
      "Epoch 75/300\n",
      "Average training loss: 0.035820105979839964\n",
      "Average test loss: 0.006050430648028851\n",
      "Epoch 76/300\n",
      "Average training loss: 0.035675114477674165\n",
      "Average test loss: 0.006001084434903331\n",
      "Epoch 77/300\n",
      "Average training loss: 0.035656110720502\n",
      "Average test loss: 0.0061536105726328165\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03564748364355829\n",
      "Average test loss: 0.005979478997695777\n",
      "Epoch 79/300\n",
      "Average training loss: 0.035569576018386416\n",
      "Average test loss: 0.006028800947591662\n",
      "Epoch 80/300\n",
      "Average training loss: 0.035533775604433486\n",
      "Average test loss: 0.006279884211719036\n",
      "Epoch 81/300\n",
      "Average training loss: 0.035567298236820435\n",
      "Average test loss: 0.006137467392202881\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03548540213207404\n",
      "Average test loss: 0.006084410478671392\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03544237474600474\n",
      "Average test loss: 0.006056874780605237\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03544043231010437\n",
      "Average test loss: 0.006451615439934863\n",
      "Epoch 85/300\n",
      "Average training loss: 0.035441153840886225\n",
      "Average test loss: 0.00595230983156297\n",
      "Epoch 86/300\n",
      "Average training loss: 0.035381389064921276\n",
      "Average test loss: 0.005980120040890244\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03536062939961751\n",
      "Average test loss: 0.005971980866872602\n",
      "Epoch 88/300\n",
      "Average training loss: 0.035306083897749585\n",
      "Average test loss: 0.006093604657385085\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03529912958211369\n",
      "Average test loss: 0.006014453060511086\n",
      "Epoch 90/300\n",
      "Average training loss: 0.035262437838647104\n",
      "Average test loss: 0.005977507623533408\n",
      "Epoch 91/300\n",
      "Average training loss: 0.035191363599565297\n",
      "Average test loss: 0.006026568538612789\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03517295097808043\n",
      "Average test loss: 0.0059258642763727245\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03521428594324324\n",
      "Average test loss: 0.005993509526054064\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0351996380355623\n",
      "Average test loss: 0.006555184186539716\n",
      "Epoch 95/300\n",
      "Average training loss: 0.035121538668870925\n",
      "Average test loss: 0.005973714123169581\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03510056918197208\n",
      "Average test loss: 0.005944478143834406\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03505155910054843\n",
      "Average test loss: 0.006146874063958725\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03511670288774702\n",
      "Average test loss: 0.006318384064154492\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03512650231189198\n",
      "Average test loss: 0.005935699711243312\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03497959214780066\n",
      "Average test loss: 0.006068201994730366\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03497458243370056\n",
      "Average test loss: 0.0061303872457808915\n",
      "Epoch 102/300\n",
      "Average training loss: 0.034974765219622186\n",
      "Average test loss: 0.006090949363178677\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03496438334219985\n",
      "Average test loss: 0.006391557729906506\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03494359539614783\n",
      "Average test loss: 0.005957196493529611\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03492694007356961\n",
      "Average test loss: 0.006108630913827154\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03485859962966707\n",
      "Average test loss: 0.006037200117276775\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034878174897697234\n",
      "Average test loss: 0.006142952993719114\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03490565932459301\n",
      "Average test loss: 0.005958558184405168\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03482525387903054\n",
      "Average test loss: 0.00612991143990722\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03484675328599082\n",
      "Average test loss: 0.00592351559135649\n",
      "Epoch 111/300\n",
      "Average training loss: 0.034855039053493075\n",
      "Average test loss: 0.006827879507922464\n",
      "Epoch 112/300\n",
      "Average training loss: 0.034737168583605024\n",
      "Average test loss: 0.0059712910072671046\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03478021876679527\n",
      "Average test loss: 0.0059649371885591084\n",
      "Epoch 114/300\n",
      "Average training loss: 0.034726017975144914\n",
      "Average test loss: 0.006562380750560098\n",
      "Epoch 115/300\n",
      "Average training loss: 0.034693190319670573\n",
      "Average test loss: 0.0060000839999152554\n",
      "Epoch 116/300\n",
      "Average training loss: 0.034719360765483646\n",
      "Average test loss: 0.0060119034049825534\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03466634248693784\n",
      "Average test loss: 0.00602638691291213\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03471486382020844\n",
      "Average test loss: 0.006504081489311324\n",
      "Epoch 119/300\n",
      "Average training loss: 0.034648485958576204\n",
      "Average test loss: 0.005990853542668952\n",
      "Epoch 120/300\n",
      "Average training loss: 0.034587902704874676\n",
      "Average test loss: 0.005945214464846585\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03461665993597773\n",
      "Average test loss: 0.006020732942968607\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03460721100370089\n",
      "Average test loss: 0.006013715631845925\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03457335943645901\n",
      "Average test loss: 0.006077191166993645\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0345514247632689\n",
      "Average test loss: 0.0060740487434797815\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03456905675596661\n",
      "Average test loss: 0.0060499207000765534\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03461492454343372\n",
      "Average test loss: 0.006015990292446481\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03453557150231468\n",
      "Average test loss: 0.006095484414862262\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03450212839245796\n",
      "Average test loss: 0.00596383489916722\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03453778092728721\n",
      "Average test loss: 0.005964127706984679\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03447063345710436\n",
      "Average test loss: 0.005936318605310387\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03445936847395367\n",
      "Average test loss: 0.005967449828568432\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03444673248463207\n",
      "Average test loss: 0.005915878881182936\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03443874605331156\n",
      "Average test loss: 0.005868471628261937\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03444989056388537\n",
      "Average test loss: 0.005967086845801936\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03444751897785399\n",
      "Average test loss: 0.0060949071389105585\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03438157162401411\n",
      "Average test loss: 0.006041258497784535\n",
      "Epoch 137/300\n",
      "Average training loss: 0.034467734694480896\n",
      "Average test loss: 0.0059175558557940855\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03431825069420868\n",
      "Average test loss: 0.005904229936086469\n",
      "Epoch 139/300\n",
      "Average training loss: 0.034361009534862305\n",
      "Average test loss: 0.006056329658461941\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03434134522080422\n",
      "Average test loss: 0.006494401735150152\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0344301178322898\n",
      "Average test loss: 0.0061843076629771124\n",
      "Epoch 142/300\n",
      "Average training loss: 0.034260551821854376\n",
      "Average test loss: 0.0058928862594895895\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03431500993834601\n",
      "Average test loss: 0.00599137446615431\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0343000893857744\n",
      "Average test loss: 0.00591947302263644\n",
      "Epoch 145/300\n",
      "Average training loss: 0.034238038318024744\n",
      "Average test loss: 0.00586708850827482\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03423320246073935\n",
      "Average test loss: 0.00603147988849216\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03428306938211123\n",
      "Average test loss: 0.005957465905282232\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03425151908232106\n",
      "Average test loss: 0.006012297649350431\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03420792655812369\n",
      "Average test loss: 0.005968614260562592\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0341676962143845\n",
      "Average test loss: 0.005973064278148942\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03421581822633743\n",
      "Average test loss: 0.006097656136171685\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03417784168819586\n",
      "Average test loss: 0.005893278249435955\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03414042465223206\n",
      "Average test loss: 0.005936929500351349\n",
      "Epoch 154/300\n",
      "Average training loss: 0.034171962135367925\n",
      "Average test loss: 0.005869617421593931\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03414704437388314\n",
      "Average test loss: 0.006102982559137874\n",
      "Epoch 156/300\n",
      "Average training loss: 0.034153581715292404\n",
      "Average test loss: 0.005931165945612722\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03412520519395669\n",
      "Average test loss: 0.00624027776469787\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03411377786265479\n",
      "Average test loss: 0.005874715543041626\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03412750119302008\n",
      "Average test loss: 0.005877939174572627\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03409505735503303\n",
      "Average test loss: 0.005947605494823721\n",
      "Epoch 161/300\n",
      "Average training loss: 0.034080393238200085\n",
      "Average test loss: 0.006271365371429258\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03404919316371282\n",
      "Average test loss: 0.0060854762726359896\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03406659978131453\n",
      "Average test loss: 0.006025379590690136\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03404434268010987\n",
      "Average test loss: 0.0059883210700419215\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03402425198091401\n",
      "Average test loss: 0.9951648202472263\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0343230124215285\n",
      "Average test loss: 0.005936770492750737\n",
      "Epoch 167/300\n",
      "Average training loss: 0.033994965179098974\n",
      "Average test loss: 0.005883567394067844\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03398800893624623\n",
      "Average test loss: 0.005891844705575042\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03393766828378041\n",
      "Average test loss: 0.0058865663446486\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0339487751275301\n",
      "Average test loss: 0.005896008277932803\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03392477194302612\n",
      "Average test loss: 0.0061735679888063\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0339549489054415\n",
      "Average test loss: 0.005907734755012724\n",
      "Epoch 173/300\n",
      "Average training loss: 0.033976126359568704\n",
      "Average test loss: 0.0061639198768470025\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03391217044658131\n",
      "Average test loss: 0.005993808350422316\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03392911719944742\n",
      "Average test loss: 0.005895505030535989\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03392373742825455\n",
      "Average test loss: 0.006006741221580241\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03387829149597221\n",
      "Average test loss: 0.005947707258164883\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0339332991109954\n",
      "Average test loss: 0.005999677324874534\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03386267327268919\n",
      "Average test loss: 0.005895692218508985\n",
      "Epoch 180/300\n",
      "Average training loss: 0.033846735353271165\n",
      "Average test loss: 0.005961429777244727\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03382919362352954\n",
      "Average test loss: 0.0059016475998279125\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03387674851881133\n",
      "Average test loss: 0.005909208334154553\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03386152222752571\n",
      "Average test loss: 0.006001933047754897\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03388839044173558\n",
      "Average test loss: 0.006044362829791175\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03380845926370886\n",
      "Average test loss: 0.006041436880413029\n",
      "Epoch 186/300\n",
      "Average training loss: 0.033786257561710144\n",
      "Average test loss: 0.005913728934609228\n",
      "Epoch 187/300\n",
      "Average training loss: 0.033812747014893424\n",
      "Average test loss: 0.005917615479065312\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03377457233270009\n",
      "Average test loss: 0.005971782499303421\n",
      "Epoch 189/300\n",
      "Average training loss: 0.033771021478705934\n",
      "Average test loss: 0.005880433903800116\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03377380271918244\n",
      "Average test loss: 0.005903375872307354\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03377429158157772\n",
      "Average test loss: 0.005885643197016584\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03374029888047112\n",
      "Average test loss: 0.005870391687171327\n",
      "Epoch 193/300\n",
      "Average training loss: 0.033778969195153975\n",
      "Average test loss: 0.005963956824607319\n",
      "Epoch 194/300\n",
      "Average training loss: 0.033734929935799705\n",
      "Average test loss: 0.005944110077288416\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03373949152231216\n",
      "Average test loss: 0.005987658886031972\n",
      "Epoch 196/300\n",
      "Average training loss: 0.033722390125195185\n",
      "Average test loss: 0.00781448277624117\n",
      "Epoch 197/300\n",
      "Average training loss: 0.033725575781530806\n",
      "Average test loss: 0.005925386317074299\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03373524925443861\n",
      "Average test loss: 0.005959392808791664\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03367384638720088\n",
      "Average test loss: 0.005962649221221606\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03371095058321953\n",
      "Average test loss: 0.005943450133005778\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03366743792593479\n",
      "Average test loss: 0.005933711683998505\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03373458786308765\n",
      "Average test loss: 0.005927853174507618\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03366144845883052\n",
      "Average test loss: 0.0059420352780984505\n",
      "Epoch 204/300\n",
      "Average training loss: 0.033651494345731205\n",
      "Average test loss: 0.005954277407791879\n",
      "Epoch 205/300\n",
      "Average training loss: 0.033660448736614654\n",
      "Average test loss: 0.005894495476451185\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03362533820006582\n",
      "Average test loss: 0.006270557148175107\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03373279233607981\n",
      "Average test loss: 0.005893210752142801\n",
      "Epoch 208/300\n",
      "Average training loss: 0.033602208077907564\n",
      "Average test loss: 0.005974852629833751\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03365601067576143\n",
      "Average test loss: 0.006245939634740353\n",
      "Epoch 210/300\n",
      "Average training loss: 0.033645118216673535\n",
      "Average test loss: 0.005906844476444854\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03355706106291877\n",
      "Average training loss: 0.03361494755744934\n",
      "Average test loss: 0.005945660358501805\n",
      "Epoch 213/300\n",
      "Average training loss: 0.033568912832273375\n",
      "Average test loss: 0.005903480276879337\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03356612583663728\n",
      "Average test loss: 0.005950672709693511\n",
      "Epoch 215/300\n",
      "Average training loss: 0.033566990673542026\n",
      "Average test loss: 0.005950092451853885\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03358312145041095\n",
      "Average test loss: 0.005909166735079553\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03360161333945062\n",
      "Average test loss: 0.006050239931792021\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03349246590667301\n",
      "Average test loss: 0.005902296599414614\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03357158765031232\n",
      "Average test loss: 0.005898351697458161\n",
      "Epoch 220/300\n",
      "Average training loss: 0.033494494007693394\n",
      "Average test loss: 0.006478322887172302\n",
      "Epoch 221/300\n",
      "Average training loss: 0.033471123493380016\n",
      "Average test loss: 0.006089649779929055\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03348831237687005\n",
      "Average test loss: 0.00595149222181903\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03350819441345003\n",
      "Average test loss: 0.0059210601701504655\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03348037918408712\n",
      "Average test loss: 0.005967157049311532\n",
      "Epoch 225/300\n",
      "Average training loss: 0.033522171792056826\n",
      "Average test loss: 0.005967156230575509\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03346081615156597\n",
      "Average test loss: 0.006065406235969729\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03343404917253388\n",
      "Average test loss: 0.005952733461227682\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03338311439752579\n",
      "Average test loss: 0.005933340751876434\n",
      "Epoch 241/300\n",
      "Average training loss: 0.033369524816672005\n",
      "Average test loss: 0.005884826954454183\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03334770791729291\n",
      "Average test loss: 0.00594397247210145\n",
      "Epoch 243/300\n",
      "Average training loss: 0.033445083581739005\n",
      "Average test loss: 0.00604862551846438\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03339030667973889\n",
      "Average test loss: 0.005902981662501892\n",
      "Epoch 245/300\n",
      "Average training loss: 0.033295304497083025\n",
      "Average test loss: 0.005977854706346989\n",
      "Epoch 246/300\n",
      "Average training loss: 0.033350647843546334\n",
      "Average test loss: 0.005892975901977884\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0333538723455535\n",
      "Average test loss: 0.005884690305425061\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03331905660364363\n",
      "Average test loss: 0.005891065733300315\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0333135597176022\n",
      "Average test loss: 0.005982853872494565\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03331232228544023\n",
      "Average test loss: 0.005897999270094765\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03330039463771714\n",
      "Average test loss: 0.005898598135552473\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03331323524978426\n",
      "Average test loss: 0.0058900384054415755\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03327206861310535\n",
      "Average test loss: 0.005922660330931345\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03331163805226485\n",
      "Average test loss: 0.005943348586145375\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03326514394084613\n",
      "Average test loss: 0.005885376272930039\n",
      "Epoch 256/300\n",
      "Average training loss: 0.033225398330224885\n",
      "Average test loss: 0.0059108997401263975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03386070555117395\n",
      "Average test loss: 0.006024124788327349\n",
      "Epoch 258/300\n",
      "Average training loss: 0.033284063696861266\n",
      "Average test loss: 0.005918954929543866\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03323777862058745\n",
      "Average test loss: 0.005990987847662634\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03320864778094822\n",
      "Average test loss: 0.0060606944587909516\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03319940369327863\n",
      "Average test loss: 0.006023955062031746\n",
      "Epoch 262/300\n",
      "Average training loss: 0.033208363476726746\n",
      "Average test loss: 0.005933532292230262\n",
      "Epoch 263/300\n",
      "Average training loss: 0.033218129143118856\n",
      "Average test loss: 0.006041331033325857\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03323234359092182\n",
      "Average test loss: 0.005946658136943976\n",
      "Epoch 265/300\n",
      "Average training loss: 0.033218055185344486\n",
      "Average test loss: 0.005908641781244013\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03321624966296885\n",
      "Average test loss: 0.0074356859636803465\n",
      "Epoch 267/300\n",
      "Average training loss: 0.033180090874433514\n",
      "Average test loss: 0.005940959374523825\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03321991464495659\n",
      "Average test loss: 0.006006950747635629\n",
      "Epoch 269/300\n",
      "Average training loss: 0.033145523044798106\n",
      "Average test loss: 0.005960247081187036\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03320600048700968\n",
      "Average test loss: 0.005964856574932734\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03314975161353747\n",
      "Average test loss: 0.005939515092720588\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03316246267822054\n",
      "Average test loss: 0.005930788608474864\n",
      "Epoch 273/300\n",
      "Average training loss: 0.033189211861954795\n",
      "Average test loss: 0.006011168868177467\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03316450277136432\n",
      "Average test loss: 0.005965172191460927\n",
      "Epoch 275/300\n",
      "Average training loss: 0.033081199958920476\n",
      "Average test loss: 0.005951634724934896\n",
      "Epoch 287/300\n",
      "Average training loss: 0.033072089334328966\n",
      "Average test loss: 0.006021253758006626\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03307223326133357\n",
      "Average test loss: 0.006019698825147417\n",
      "Epoch 289/300\n",
      "Average training loss: 0.033116914222637815\n",
      "Average test loss: 0.00601026308702098\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03308351909452015\n",
      "Average test loss: 0.006008971662157111\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03307068190640873\n",
      "Average test loss: 0.005965416321737899\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03308596350418197\n",
      "Average test loss: 0.006036121049275001\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03306771216293176\n",
      "Average test loss: 0.006091144909254379\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03301659079392751\n",
      "Average test loss: 0.005923953594432937\n",
      "Epoch 295/300\n",
      "Average training loss: 0.033046674377388424\n",
      "Average test loss: 0.006055615526934465\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03304512658715248\n",
      "Average test loss: 0.006023867138971885\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03301533255312178\n",
      "Average test loss: 0.006063907539678944\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03303660465280215\n",
      "Average test loss: 0.00599211358361774\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0330238311936458\n",
      "Average test loss: 0.00604889830450217\n",
      "Epoch 300/300\n",
      "Average training loss: 0.032992537081241606\n",
      "Average test loss: 0.005935931253764365\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.20950081700086592\n",
      "Average test loss: 0.007858717307862308\n",
      "Epoch 2/300\n",
      "Average training loss: 0.05353294947081142\n",
      "Average test loss: 0.006914663555721442\n",
      "Epoch 3/300\n",
      "Average training loss: 0.046962306443187926\n",
      "Average test loss: 0.006440387500656976\n",
      "Epoch 4/300\n",
      "Average training loss: 0.043988147921032374\n",
      "Average test loss: 0.006827505714777443\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04085973567763964\n",
      "Average test loss: 0.0060738056918813125\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03943407114677959\n",
      "Average test loss: 0.005738728421429793\n",
      "Epoch 7/300\n",
      "Average training loss: 0.037638183212942544\n",
      "Average test loss: 0.0057970227888888785\n",
      "Epoch 8/300\n",
      "Average training loss: 0.036424411439233355\n",
      "Average test loss: 0.005562906570732593\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0354162323905362\n",
      "Average test loss: 0.005749321217338244\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03447726239760717\n",
      "Average test loss: 0.005327539389746057\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03365331079231368\n",
      "Average test loss: 0.005311344917035765\n",
      "Epoch 12/300\n",
      "Average training loss: 0.033035772245791226\n",
      "Average test loss: 0.005379528412802352\n",
      "Epoch 13/300\n",
      "Average training loss: 0.032475492747293576\n",
      "Average test loss: 0.005126885613633527\n",
      "Epoch 14/300\n",
      "Average training loss: 0.031804092668824724\n",
      "Average test loss: 0.004775162721880608\n",
      "Epoch 15/300\n",
      "Average training loss: 0.031266983623305954\n",
      "Average test loss: 0.004774530000984669\n",
      "Epoch 16/300\n",
      "Average training loss: 0.030827676291267078\n",
      "Average test loss: 0.0047119486737582416\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030397990488343768\n",
      "Average test loss: 0.004611800935947233\n",
      "Epoch 18/300\n",
      "Average training loss: 0.029985543607009783\n",
      "Average test loss: 0.004597394552081823\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02971988641553455\n",
      "Average test loss: 0.004675846202092038\n",
      "Epoch 20/300\n",
      "Average training loss: 0.029294057428836822\n",
      "Average test loss: 0.004537049125880003\n",
      "Epoch 21/300\n",
      "Average training loss: 0.028968164642651877\n",
      "Average test loss: 0.0043188239155958095\n",
      "Epoch 22/300\n",
      "Average training loss: 0.028699265340963998\n",
      "Average test loss: 0.004287171454065376\n",
      "Epoch 23/300\n",
      "Average training loss: 0.028472256907158426\n",
      "Average test loss: 0.00424858734715316\n",
      "Epoch 24/300\n",
      "Average training loss: 0.028164941703279812\n",
      "Average test loss: 0.00439741134395202\n",
      "Epoch 25/300\n",
      "Average training loss: 0.027957240376207563\n",
      "Average test loss: 0.004115318800426192\n",
      "Epoch 26/300\n",
      "Average training loss: 0.027706855641471014\n",
      "Average test loss: 0.004083118530818158\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02757838314606084\n",
      "Average test loss: 0.004137250512424443\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0273333923270305\n",
      "Average test loss: 0.004055265728177296\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02711301146613227\n",
      "Average test loss: 0.003990902103276716\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02700182451804479\n",
      "Average test loss: 0.00424917679404219\n",
      "Epoch 31/300\n",
      "Average training loss: 0.026817557530270684\n",
      "Average test loss: 0.003928987698836459\n",
      "Epoch 32/300\n",
      "Average training loss: 0.026706630915403366\n",
      "Average test loss: 0.004012243430647585\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02657409818470478\n",
      "Average test loss: 0.0038820724352780314\n",
      "Epoch 34/300\n",
      "Average training loss: 0.026427966243690915\n",
      "Average test loss: 0.003814230476816495\n",
      "Epoch 35/300\n",
      "Average training loss: 0.026303744736644957\n",
      "Average test loss: 0.003895345073400272\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02621322086122301\n",
      "Average test loss: 0.0038246761121683653\n",
      "Epoch 37/300\n",
      "Average training loss: 0.026078883137967853\n",
      "Average test loss: 0.0038438568328403766\n",
      "Epoch 38/300\n",
      "Average training loss: 0.025984863239857884\n",
      "Average test loss: 0.003753359949216247\n",
      "Epoch 39/300\n",
      "Average training loss: 0.025907631458507645\n",
      "Average test loss: 0.003768679503765371\n",
      "Epoch 40/300\n",
      "Average training loss: 0.025805114860335987\n",
      "Average test loss: 0.003756797010699908\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02577529598772526\n",
      "Average test loss: 0.003850296852075391\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02564801160742839\n",
      "Average test loss: 0.003686094369739294\n",
      "Epoch 43/300\n",
      "Average training loss: 0.025610084861516953\n",
      "Average test loss: 0.0037331584391908515\n",
      "Epoch 44/300\n",
      "Average training loss: 0.025535615535246\n",
      "Average test loss: 0.003742363820059432\n",
      "Epoch 45/300\n",
      "Average training loss: 0.025428007003333832\n",
      "Average test loss: 0.0036711519178416994\n",
      "Epoch 46/300\n",
      "Average training loss: 0.025428804288307825\n",
      "Average test loss: 0.003724285138150056\n",
      "Epoch 47/300\n",
      "Average training loss: 0.025377672324577966\n",
      "Average test loss: 0.0036614616202811402\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02531974171764321\n",
      "Average test loss: 0.0036786268728060856\n",
      "Epoch 49/300\n",
      "Average training loss: 0.025227359597881634\n",
      "Average test loss: 0.003823339585835735\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02524246246781614\n",
      "Average test loss: 0.0036109754468003907\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025127057146694925\n",
      "Average test loss: 0.0038050624248054292\n",
      "Epoch 52/300\n",
      "Average training loss: 0.025115098625421522\n",
      "Average test loss: 0.0037514074283341568\n",
      "Epoch 53/300\n",
      "Average training loss: 0.025068806070420478\n",
      "Average test loss: 0.0036478787625415458\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025028194475505087\n",
      "Average test loss: 0.0036047980168627367\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02496731550163693\n",
      "Average test loss: 0.0036320430878549813\n",
      "Epoch 56/300\n",
      "Average training loss: 0.024925734819637405\n",
      "Average test loss: 0.00362337647502621\n",
      "Epoch 57/300\n",
      "Average training loss: 0.024920829426911144\n",
      "Average test loss: 0.0038452916911078825\n",
      "Epoch 58/300\n",
      "Average training loss: 0.024887909152441556\n",
      "Average test loss: 0.0037216491194234954\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02485418307615651\n",
      "Average test loss: 0.003592048647710019\n",
      "Epoch 60/300\n",
      "Average training loss: 0.024809972494840623\n",
      "Average test loss: 0.003780568066570494\n",
      "Epoch 61/300\n",
      "Average training loss: 0.024780292203028995\n",
      "Average test loss: 0.0036863423260963626\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02473490544822481\n",
      "Average test loss: 0.0036380700357258318\n",
      "Epoch 63/300\n",
      "Average training loss: 0.024703198704454633\n",
      "Average test loss: 0.003577576080130206\n",
      "Epoch 64/300\n",
      "Average training loss: 0.024700434330436918\n",
      "Average test loss: 0.0035735533330589534\n",
      "Epoch 65/300\n",
      "Average training loss: 0.024623213625616498\n",
      "Average test loss: 0.0035487725685040155\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02467172558274534\n",
      "Average test loss: 0.02173454877237479\n",
      "Epoch 67/300\n",
      "Average training loss: 0.024911989125940535\n",
      "Average test loss: 0.003580185643914673\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024578782324989636\n",
      "Average test loss: 0.0035578167554405\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0244959030598402\n",
      "Average test loss: 0.003571658191581567\n",
      "Epoch 70/300\n",
      "Average training loss: 0.024515752267506387\n",
      "Average test loss: 0.0035628541494823166\n",
      "Epoch 71/300\n",
      "Average training loss: 0.024475344151258467\n",
      "Average test loss: 0.0035566440023895767\n",
      "Epoch 72/300\n",
      "Average training loss: 0.024456577888793414\n",
      "Average test loss: 0.0035772013600087827\n",
      "Epoch 73/300\n",
      "Average training loss: 0.024209739630421\n",
      "Average test loss: 0.003495184336685472\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02420129141708215\n",
      "Average test loss: 0.0035703211265305677\n",
      "Epoch 86/300\n",
      "Average training loss: 0.024172177440590328\n",
      "Average test loss: 0.003593951259429256\n",
      "Epoch 87/300\n",
      "Average training loss: 0.024232724010944368\n",
      "Average test loss: 0.003522831950750616\n",
      "Epoch 88/300\n",
      "Average training loss: 0.024175437005857628\n",
      "Average test loss: 0.003627342190593481\n",
      "Epoch 89/300\n",
      "Average training loss: 0.024117685919006666\n",
      "Average test loss: 0.0035633118293351596\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02412898450758722\n",
      "Average test loss: 0.0035653327657944626\n",
      "Epoch 91/300\n",
      "Average training loss: 0.024185622327857547\n",
      "Average test loss: 0.0035229675749109853\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02405538794729445\n",
      "Average test loss: 0.00357569859839148\n",
      "Epoch 93/300\n",
      "Average training loss: 0.024059374852312937\n",
      "Average test loss: 0.0036621094768246017\n",
      "Epoch 94/300\n",
      "Average training loss: 0.024024765301081868\n",
      "Average test loss: 0.00349914970745643\n",
      "Epoch 95/300\n",
      "Average training loss: 0.024024188285072644\n",
      "Average test loss: 0.0036137590931935443\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02401599880390697\n",
      "Average test loss: 0.003521628390169806\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02401317764321963\n",
      "Average test loss: 0.0035742375937600932\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02397381357020802\n",
      "Average test loss: 0.00357827284373343\n",
      "Epoch 99/300\n",
      "Average training loss: 0.023962251358562045\n",
      "Average test loss: 0.004411659011824264\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0239536733287904\n",
      "Average test loss: 0.003504715394228697\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02394867768800921\n",
      "Average test loss: 0.003735729428215159\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02392666618857119\n",
      "Average test loss: 0.0034951359511663518\n",
      "Epoch 103/300\n",
      "Average training loss: 0.023924644155634774\n",
      "Average test loss: 0.0035324427052918408\n",
      "Epoch 104/300\n",
      "Average training loss: 0.023904152338703473\n",
      "Average test loss: 0.004393122956570652\n",
      "Epoch 105/300\n",
      "Average training loss: 0.023934432187014156\n",
      "Average test loss: 0.003525981537376841\n",
      "Epoch 106/300\n",
      "Average training loss: 0.023878374099731446\n",
      "Average test loss: 0.0035447329154445067\n",
      "Epoch 107/300\n",
      "Average training loss: 0.023863148131304315\n",
      "Average test loss: 0.003482090091332793\n",
      "Epoch 108/300\n",
      "Average training loss: 0.023851826379696527\n",
      "Average test loss: 0.0035444164127111434\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02386702378094196\n",
      "Average test loss: 0.003514618061482906\n",
      "Epoch 110/300\n",
      "Average training loss: 0.023821772269076773\n",
      "Average test loss: 0.003528570589919885\n",
      "Epoch 111/300\n",
      "Average training loss: 0.023808329676588375\n",
      "Average test loss: 0.003506322934395737\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02381881619741519\n",
      "Average test loss: 0.003534546147618029\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023784480184316635\n",
      "Average test loss: 0.003501328109867043\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02377841269804372\n",
      "Average test loss: 0.0035167057543165152\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02376092429127958\n",
      "Average test loss: 0.0035246675035191907\n",
      "Epoch 116/300\n",
      "Average training loss: 0.023770634909470876\n",
      "Average test loss: 0.0035208379274441134\n",
      "Epoch 117/300\n",
      "Average training loss: 0.023753090913097064\n",
      "Average test loss: 0.0035035605681025318\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02371806578338146\n",
      "Average test loss: 0.00357508817439278\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023747290924191474\n",
      "Average test loss: 0.003495383765548468\n",
      "Epoch 120/300\n",
      "Average training loss: 0.023694874462154177\n",
      "Average test loss: 0.0035715160167051688\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02369943011138174\n",
      "Average test loss: 0.0034746860036005576\n",
      "Epoch 122/300\n",
      "Average training loss: 0.023679225294126405\n",
      "Average test loss: 0.0035407028583188853\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023666472342279223\n",
      "Average test loss: 0.0034714382901373836\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02367037766178449\n",
      "Average test loss: 0.0035236589403616057\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023659437252415552\n",
      "Average test loss: 0.0035198199438552062\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02365143018298679\n",
      "Average test loss: 0.0034907593770573536\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02364388666053613\n",
      "Average test loss: 0.003721658645818631\n",
      "Epoch 128/300\n",
      "Average training loss: 0.3010031288365523\n",
      "Average test loss: 0.0060574154564076\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04792181859413783\n",
      "Average test loss: 0.0046822145771649145\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03880204158027967\n",
      "Average test loss: 0.00430209049437609\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03493997628324562\n",
      "Average test loss: 0.003984003602216641\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03256647011803256\n",
      "Average test loss: 0.00391289957654145\n",
      "Epoch 133/300\n",
      "Average training loss: 0.030920855172806315\n",
      "Average test loss: 0.003884142010576195\n",
      "Epoch 134/300\n",
      "Average training loss: 0.029636803766091666\n",
      "Average test loss: 0.00387170357339912\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028733204015427165\n",
      "Average test loss: 0.003677840132680204\n",
      "Epoch 136/300\n",
      "Average training loss: 0.027930228314465948\n",
      "Average test loss: 0.0036409571208059787\n",
      "Epoch 137/300\n",
      "Average training loss: 0.027333990385135013\n",
      "Average test loss: 0.0035715721760772997\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02678956727186839\n",
      "Average test loss: 0.003650378734080328\n",
      "Epoch 139/300\n",
      "Average training loss: 0.026290966199504006\n",
      "Average test loss: 0.003763780199819141\n",
      "Epoch 140/300\n",
      "Average training loss: 0.025889135774638916\n",
      "Average test loss: 0.003628550608538919\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025504167334900962\n",
      "Average test loss: 0.0041556004811492235\n",
      "Epoch 142/300\n",
      "Average training loss: 0.025182796279589335\n",
      "Average test loss: 0.0035445101604693464\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024904825477136506\n",
      "Average test loss: 0.0035304245421042043\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024672584576739206\n",
      "Average test loss: 0.003579896259638998\n",
      "Epoch 145/300\n",
      "Average training loss: 0.024466323662135338\n",
      "Average test loss: 0.003504103105722202\n",
      "Epoch 146/300\n",
      "Average training loss: 0.024298558778232997\n",
      "Average test loss: 0.003498393588181999\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024149417991439503\n",
      "Average test loss: 0.003529313290698661\n",
      "Epoch 148/300\n",
      "Average training loss: 0.02402514537672202\n",
      "Average test loss: 0.003876431135667695\n",
      "Epoch 149/300\n",
      "Average training loss: 0.023934792826573053\n",
      "Average test loss: 0.0035012209030489128\n",
      "Epoch 150/300\n",
      "Average training loss: 0.023839976941545805\n",
      "Average test loss: 0.0035702149907333985\n",
      "Epoch 151/300\n",
      "Average training loss: 0.023784725210732882\n",
      "Average test loss: 0.003544867841526866\n",
      "Epoch 152/300\n",
      "Average training loss: 0.023747802052232956\n",
      "Average test loss: 0.0034932925233410466\n",
      "Epoch 153/300\n",
      "Average training loss: 0.023708927671114603\n",
      "Average test loss: 0.0034875512203822534\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02369416211048762\n",
      "Average test loss: 0.004935492451820109\n",
      "Epoch 155/300\n",
      "Average training loss: 0.023701170636547936\n",
      "Average test loss: 0.0034725990963892805\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02365283492207527\n",
      "Average test loss: 0.003492347076535225\n",
      "Epoch 157/300\n",
      "Average training loss: 0.023664328695999253\n",
      "Average test loss: 0.0035860381037410762\n",
      "Epoch 158/300\n",
      "Average training loss: 0.023633762475517062\n",
      "Average test loss: 0.003496176661302646\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02359878342350324\n",
      "Average test loss: 0.003549812475012408\n",
      "Epoch 160/300\n",
      "Average training loss: 0.023600429774986373\n",
      "Average test loss: 0.0035161732534567517\n",
      "Epoch 161/300\n",
      "Average training loss: 0.023573495452602705\n",
      "Average test loss: 0.0035308754230952925\n",
      "Epoch 162/300\n",
      "Average training loss: 0.023576233327388764\n",
      "Average test loss: 0.0035328180994838474\n",
      "Epoch 163/300\n",
      "Average training loss: 0.023570817384454937\n",
      "Average test loss: 0.003466454430586762\n",
      "Epoch 164/300\n",
      "Average training loss: 0.023542793351742957\n",
      "Average test loss: 0.003473043998910321\n",
      "Epoch 165/300\n",
      "Average training loss: 0.023534623619582917\n",
      "Average test loss: 0.003476961561685635\n",
      "Epoch 166/300\n",
      "Average training loss: 0.023524060954650244\n",
      "Average test loss: 0.0035050193334205284\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0234982495241695\n",
      "Average test loss: 0.0034781299262411063\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02351521598133776\n",
      "Average test loss: 0.003481168595660064\n",
      "Epoch 169/300\n",
      "Average training loss: 0.023499416273501186\n",
      "Average test loss: 0.00348309204623931\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02348582977056503\n",
      "Average test loss: 0.0035067022821555537\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02347253402074178\n",
      "Average test loss: 0.0035179685693648125\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0234635797623131\n",
      "Average test loss: 0.004259863513211409\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023498964115977287\n",
      "Average test loss: 0.00346399943696128\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02343245588739713\n",
      "Average test loss: 0.003493717413602604\n",
      "Epoch 175/300\n",
      "Average training loss: 0.023426713965005345\n",
      "Average test loss: 0.0034878060397588546\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023427346537510555\n",
      "Average test loss: 0.0035515203811228276\n",
      "Epoch 177/300\n",
      "Average training loss: 0.023404160203205213\n",
      "Average test loss: 0.003489742711186409\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02340035261048211\n",
      "Average test loss: 0.0035029090576701692\n",
      "Epoch 179/300\n",
      "Average training loss: 0.023385602633158364\n",
      "Average test loss: 0.00357066584771706\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023380067568686273\n",
      "Average test loss: 0.003508243637987309\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023384209584858682\n",
      "Average test loss: 0.0034712101556360724\n",
      "Epoch 182/300\n",
      "Average training loss: 0.023370111705528364\n",
      "Average test loss: 0.0034995348209308254\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02335329786522521\n",
      "Average test loss: 0.0035341636306709715\n",
      "Epoch 184/300\n",
      "Average training loss: 0.023345863193273544\n",
      "Average test loss: 0.003459298519624604\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023354974451992246\n",
      "Average test loss: 0.003507673749493228\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023352955624461175\n",
      "Average test loss: 0.0034699262094994385\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02332028990983963\n",
      "Average test loss: 0.0035073936794781024\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02330364030599594\n",
      "Average test loss: 0.0034847711357805463\n",
      "Epoch 189/300\n",
      "Average training loss: 0.023320191318790116\n",
      "Average test loss: 0.003486849551399549\n",
      "Epoch 190/300\n",
      "Average training loss: 0.023321414220664236\n",
      "Average test loss: 0.0034844048079103233\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02329964669048786\n",
      "Average test loss: 0.0035030011878245406\n",
      "Epoch 192/300\n",
      "Average training loss: 0.023285432752635744\n",
      "Average test loss: 0.003577910629618499\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02327434117264218\n",
      "Average test loss: 0.0034684468859599695\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023279959940248066\n",
      "Average test loss: 0.0035387382432818413\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023300947868161732\n",
      "Average test loss: 0.0034724118502603638\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02325961330201891\n",
      "Average test loss: 0.0035283814618984857\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023233360913064743\n",
      "Average test loss: 0.0035047607748872702\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02325648876859082\n",
      "Average test loss: 0.0035984141402360466\n",
      "Epoch 199/300\n",
      "Average training loss: 0.023248681928548547\n",
      "Average test loss: 0.0034932998242891496\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02321034755309423\n",
      "Average test loss: 0.003474640679028299\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02320344520277447\n",
      "Average test loss: 0.003469738073647022\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023234627448850208\n",
      "Average test loss: 0.003548570024263528\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02321022333204746\n",
      "Average test loss: 0.0034709313720878627\n",
      "Epoch 204/300\n",
      "Average training loss: 0.023211857999364535\n",
      "Average test loss: 0.003478299290355709\n",
      "Epoch 205/300\n",
      "Average training loss: 0.023203788010610474\n",
      "Average test loss: 0.0035265800106442638\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023204302041067017\n",
      "Average test loss: 0.003490830294167002\n",
      "Epoch 207/300\n",
      "Average training loss: 0.023197098438938458\n",
      "Average test loss: 0.0035556312039908434\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023182128677765527\n",
      "Average test loss: 0.0034884242705172964\n",
      "Epoch 209/300\n",
      "Average training loss: 0.023160687713159454\n",
      "Average test loss: 0.0034918828507264454\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023187058497634198\n",
      "Average test loss: 0.0034938969885309535\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023150866554843056\n",
      "Average test loss: 0.0034612904048214356\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023154382973909378\n",
      "Average test loss: 0.003485772334660093\n",
      "Epoch 213/300\n",
      "Average training loss: 0.023130427007873853\n",
      "Average test loss: 0.0035174302400814163\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023137658761607276\n",
      "Average test loss: 0.003492081464578708\n",
      "Epoch 215/300\n",
      "Average training loss: 0.023130740008420414\n",
      "Average test loss: 0.0034703014745480483\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023121290418836805\n",
      "Average test loss: 0.0034617473582426706\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023124040954642825\n",
      "Average test loss: 0.0035004452599419486\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02312209413283401\n",
      "Average test loss: 0.003491807523700926\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02312779753572411\n",
      "Average test loss: 0.003671973226384984\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023090009450912476\n",
      "Average test loss: 0.003503419116553333\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023080951881077554\n",
      "Average test loss: 0.0035329046455315418\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02310047886520624\n",
      "Average test loss: 0.00350688522350457\n",
      "Epoch 223/300\n",
      "Average training loss: 0.023099021158284612\n",
      "Average test loss: 0.0035230159140709374\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02308838272177511\n",
      "Average test loss: 0.0036806258211533227\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023072216414743\n",
      "Average test loss: 0.003512634365922875\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023090610292222764\n",
      "Average test loss: 0.0035641986351046297\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023047234584887822\n",
      "Average test loss: 0.003461100012477901\n",
      "Epoch 228/300\n",
      "Average training loss: 0.023050154806839095\n",
      "Average test loss: 0.003522241946309805\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023066121733850902\n",
      "Average test loss: 0.00351354403504067\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023126025661826134\n",
      "Average test loss: 0.0035653116860323484\n",
      "Epoch 231/300\n",
      "Average training loss: 0.023038266531295246\n",
      "Average test loss: 0.0034515451319101784\n",
      "Epoch 232/300\n",
      "Average training loss: 0.023025981263981924\n",
      "Average test loss: 0.003485939991349975\n",
      "Epoch 233/300\n",
      "Average training loss: 0.023044656948910818\n",
      "Average test loss: 0.0035483834250933594\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023058958651291\n",
      "Average test loss: 0.003531027904815144\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02301875029504299\n",
      "Average test loss: 0.0035279374693830807\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023028949088520476\n",
      "Average test loss: 0.003510805609118607\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02302684653798739\n",
      "Average test loss: 0.0034962468329403137\n",
      "Epoch 238/300\n",
      "Average training loss: 0.022992460432979797\n",
      "Average test loss: 0.003529707523683707\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023004373788833618\n",
      "Average test loss: 0.003500049469371637\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0229904149737623\n",
      "Average test loss: 0.0034620233447187477\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02300131847626633\n",
      "Average test loss: 0.0035072302596850526\n",
      "Epoch 242/300\n",
      "Average training loss: 0.023005561042163107\n",
      "Average test loss: 0.0034608115497976543\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023000541052056685\n",
      "Average test loss: 0.003582178719755676\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023002669586075677\n",
      "Average test loss: 0.003473990839181675\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022958869631091755\n",
      "Average test loss: 0.003464130212035444\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02297190656264623\n",
      "Average test loss: 0.0036669723596423863\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02294968907866213\n",
      "Average test loss: 0.0036354663831492267\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02294143619471126\n",
      "Average test loss: 0.0037047310829576518\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02299894997974237\n",
      "Average test loss: 0.00346451209547619\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022933528893523746\n",
      "Average test loss: 0.0034594684085912175\n",
      "Epoch 251/300\n",
      "Average training loss: 0.022959900274044938\n",
      "Average test loss: 0.0036124050493041675\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0229504234790802\n",
      "Average test loss: 0.003494332826592856\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02292119963798258\n",
      "Average test loss: 0.0034923740942031144\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02292299518817001\n",
      "Average test loss: 0.0035754817724227904\n",
      "Epoch 255/300\n",
      "Average training loss: 0.02294288809100787\n",
      "Average test loss: 0.0044785544317629605\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02291526472237375\n",
      "Average test loss: 0.0035010682598998148\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022921038096977604\n",
      "Average test loss: 0.003507836766127083\n",
      "Epoch 258/300\n",
      "Average training loss: 0.022913757963312995\n",
      "Average test loss: 0.003579542485376199\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02290184391869439\n",
      "Average test loss: 0.003466696786383788\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022896321167548497\n",
      "Average test loss: 0.003472319266448418\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022890136569738387\n",
      "Average test loss: 0.0034580939739114708\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022876711318890252\n",
      "Average test loss: 0.003553325592643685\n",
      "Epoch 263/300\n",
      "Average training loss: 0.022902707248926162\n",
      "Average test loss: 0.0035004817934499847\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022882664382457733\n",
      "Average test loss: 0.0034630037289526726\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02288793810705344\n",
      "Average test loss: 0.003459067350667384\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0228641343894932\n",
      "Average test loss: 0.003489874589567383\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02286375493473477\n",
      "Average test loss: 0.003599438488483429\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02287861060599486\n",
      "Average test loss: 0.003715138448195325\n",
      "Epoch 269/300\n",
      "Average training loss: 0.022846135114630062\n",
      "Average test loss: 0.004787522043618891\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022888494125670858\n",
      "Average test loss: 0.0035260188488496673\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022840799387958313\n",
      "Average test loss: 0.0035206294442630477\n",
      "Epoch 272/300\n",
      "Average training loss: 0.022866029901636973\n",
      "Average test loss: 0.003495302604718341\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02284858005742232\n",
      "Average test loss: 0.0034948762340678108\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02283111016286744\n",
      "Average test loss: 0.003459959963957469\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022830158634318247\n",
      "Average test loss: 0.003543809465236134\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022844466583596335\n",
      "Average test loss: 0.0034825110592775874\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022833672902650304\n",
      "Average test loss: 0.0034848256140119498\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02283477292954922\n",
      "Average test loss: 0.0034796112910326985\n",
      "Epoch 279/300\n",
      "Average training loss: 0.022808689259820516\n",
      "Average test loss: 0.003486431018759807\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022816537520951696\n",
      "Average test loss: 0.0035445498016973337\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022816158596012326\n",
      "Average test loss: 0.003470307479095128\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022809085117446053\n",
      "Average test loss: 0.003480325933959749\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022827573483188947\n",
      "Average test loss: 0.0035498591173026297\n",
      "Epoch 284/300\n",
      "Average training loss: 0.022801525665654077\n",
      "Average test loss: 0.003464072762264146\n",
      "Epoch 285/300\n",
      "Average training loss: 0.022818568309148152\n",
      "Average test loss: 0.003521227096517881\n",
      "Epoch 286/300\n",
      "Average training loss: 0.022781821340322493\n",
      "Average test loss: 0.0034996372991138035\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02280558599034945\n",
      "Average test loss: 0.003499636273831129\n",
      "Epoch 288/300\n",
      "Average training loss: 0.022785686845580737\n",
      "Average test loss: 0.00347528708125982\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022791314490967326\n",
      "Average test loss: 0.0034838737948901122\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022779320526454185\n",
      "Average test loss: 0.0035023774802684783\n",
      "Epoch 291/300\n",
      "Average training loss: 0.022767594213287036\n",
      "Average test loss: 0.003481193359941244\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02278222687708007\n",
      "Average test loss: 0.0035134330035911667\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022767289638519286\n",
      "Average test loss: 0.0035191907938569786\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02276534157163567\n",
      "Average test loss: 0.003487107643754118\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02278576116760572\n",
      "Average test loss: 0.0034885915002475184\n",
      "Epoch 296/300\n",
      "Average training loss: 0.0227482929693328\n",
      "Average test loss: 0.003479581607091758\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02276880718105369\n",
      "Average test loss: 0.003584001288852758\n",
      "Epoch 298/300\n",
      "Average training loss: 0.022735898681812815\n",
      "Average test loss: 0.0035799665260646076\n",
      "Epoch 299/300\n",
      "Average training loss: 0.022737093552947046\n",
      "Average test loss: 0.0035038219249496857\n",
      "Epoch 300/300\n",
      "Average training loss: 0.022785408816403813\n",
      "Average test loss: 0.003541854522294468\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.18999665205015076\n",
      "Average test loss: 0.005923831903686126\n",
      "Epoch 2/300\n",
      "Average training loss: 0.044592980338467494\n",
      "Average test loss: 0.005426647720237573\n",
      "Epoch 3/300\n",
      "Average training loss: 0.0392067709532049\n",
      "Average test loss: 0.005055135262923108\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03664082482126024\n",
      "Average test loss: 0.004763335537993246\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03435109531879425\n",
      "Average test loss: 0.004943616114556789\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03268334548175335\n",
      "Average test loss: 0.00518676210525963\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03138454992406898\n",
      "Average test loss: 0.004472977131605148\n",
      "Epoch 8/300\n",
      "Average training loss: 0.030139322284195157\n",
      "Average test loss: 0.004259336096545061\n",
      "Epoch 9/300\n",
      "Average training loss: 0.029165144705110127\n",
      "Average test loss: 0.003913844226135148\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028199199105302494\n",
      "Average test loss: 0.003783106906960408\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027328220890627967\n",
      "Average test loss: 0.003813724554454287\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02660146899521351\n",
      "Average test loss: 0.0037784563224348756\n",
      "Epoch 13/300\n",
      "Average training loss: 0.025973530378606585\n",
      "Average test loss: 0.0035916769148574934\n",
      "Epoch 14/300\n",
      "Average training loss: 0.025454893009530175\n",
      "Average test loss: 0.003557855515844292\n",
      "Epoch 15/300\n",
      "Average training loss: 0.024872481553090943\n",
      "Average test loss: 0.0034087875398496788\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02445917650229401\n",
      "Average test loss: 0.0033965027003238597\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024097614689005745\n",
      "Average test loss: 0.003608941631598605\n",
      "Epoch 18/300\n",
      "Average training loss: 0.0237084226542049\n",
      "Average test loss: 0.0031145939951141674\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02332852835290962\n",
      "Average test loss: 0.00309034922077424\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022988190836376612\n",
      "Average test loss: 0.003040494821344813\n",
      "Epoch 21/300\n",
      "Average training loss: 0.022741568335228494\n",
      "Average test loss: 0.0032000838702337608\n",
      "Epoch 22/300\n",
      "Average training loss: 0.022537180660499468\n",
      "Average test loss: 0.0028970053647127416\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02226169496609105\n",
      "Average test loss: 0.0030613070072399245\n",
      "Epoch 24/300\n",
      "Average training loss: 0.022066498852438398\n",
      "Average test loss: 0.002858095149613089\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02189916994008753\n",
      "Average test loss: 0.0028706729219605525\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021749708427323235\n",
      "Average test loss: 0.0027556590427541073\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021571064740419386\n",
      "Average test loss: 0.0027881751832448775\n",
      "Epoch 28/300\n",
      "Average training loss: 0.021400012811024983\n",
      "Average test loss: 0.0027267940594918197\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021280107940236728\n",
      "Average test loss: 0.002696576907299459\n",
      "Epoch 30/300\n",
      "Average training loss: 0.021196841739945942\n",
      "Average test loss: 0.0026877694537656175\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021027695602840846\n",
      "Average test loss: 0.00267647583824065\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020934329407082664\n",
      "Average test loss: 0.0026824804184337457\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020878814516796008\n",
      "Average test loss: 0.002627203108328912\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02077926583091418\n",
      "Average test loss: 0.002623242238743438\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020704310592677858\n",
      "Average test loss: 0.0027049002038935822\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02057601626879639\n",
      "Average test loss: 0.002641219375344614\n",
      "Epoch 37/300\n",
      "Average training loss: 0.020535763434237905\n",
      "Average test loss: 0.0028736362687001625\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020447608118255932\n",
      "Average test loss: 0.002589125483193331\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020382699160112275\n",
      "Average test loss: 0.0025966491817186275\n",
      "Epoch 40/300\n",
      "Average training loss: 0.020332273897197512\n",
      "Average test loss: 0.0025955671154790456\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02028269226352374\n",
      "Average test loss: 0.0025283385775983335\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020187739729881287\n",
      "Average test loss: 0.0025224681617692112\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02019154596825441\n",
      "Average test loss: 0.002504841326011552\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020122878670692443\n",
      "Average test loss: 0.0025179107412695886\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020071462194124857\n",
      "Average test loss: 0.0025372500905974046\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020043635896510547\n",
      "Average test loss: 0.002526125414090024\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019977176333467166\n",
      "Average test loss: 0.0026159435531331434\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0199354787170887\n",
      "Average test loss: 0.0024770182956837944\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019893096743358506\n",
      "Average test loss: 0.002603192942216992\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01984256429473559\n",
      "Average test loss: 0.00256888552610245\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019818230840894912\n",
      "Average test loss: 0.002471421365005275\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019790326731072533\n",
      "Average test loss: 0.0024541860044830373\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019768951912721\n",
      "Average test loss: 0.002481709751610955\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019693943427668677\n",
      "Average test loss: 0.0024791923862778477\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019692578348848556\n",
      "Average test loss: 0.002433509575203061\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019627342575126223\n",
      "Average test loss: 0.0024559852671292093\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0196366101950407\n",
      "Average test loss: 0.0024342930814665225\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0195731509377559\n",
      "Average test loss: 0.0024775275708072714\n",
      "Epoch 59/300\n",
      "Average training loss: 0.019551653259330327\n",
      "Average test loss: 0.0024568875656566687\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019556109795967738\n",
      "Average test loss: 0.002407420946078168\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019659116107556555\n",
      "Average test loss: 0.0024362724433756536\n",
      "Epoch 62/300\n",
      "Average training loss: 0.019480995173255602\n",
      "Average test loss: 0.0024733101398580603\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019451528350512188\n",
      "Average test loss: 0.002657360510279735\n",
      "Epoch 64/300\n",
      "Average training loss: 0.019456355104843774\n",
      "Average test loss: 0.002415220358926389\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01939431862698661\n",
      "Average test loss: 0.0025283621590998437\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019393127653333876\n",
      "Average test loss: 0.002433464923356142\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01936693060149749\n",
      "Average test loss: 0.00241540062531001\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019348325211140844\n",
      "Average test loss: 0.002455584475874073\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019311300413476097\n",
      "Average test loss: 0.0024735198430716992\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019304147832923467\n",
      "Average test loss: 0.002465837343285481\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019335434564285808\n",
      "Average test loss: 0.002591498872058259\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019243853857119877\n",
      "Average test loss: 0.0024030740429750747\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019251319500307243\n",
      "Average test loss: 0.002452579399570823\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019213022723793983\n",
      "Average test loss: 0.002407259212806821\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019196310369504824\n",
      "Average test loss: 0.0023831119235191082\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019178583752777843\n",
      "Average test loss: 0.002436027908697724\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019161340008179345\n",
      "Average test loss: 0.0024523749813023543\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019161598621143236\n",
      "Average test loss: 0.0024246772153096066\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019132932636472914\n",
      "Average test loss: 0.00239329574112263\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0191157017764118\n",
      "Average test loss: 0.002432362478847305\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019100192313392956\n",
      "Average test loss: 0.002606042438497146\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019081243108544085\n",
      "Average test loss: 0.0023925405697276195\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01906356326242288\n",
      "Average test loss: 0.0050193973953525225\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019064328379101225\n",
      "Average test loss: 0.002435363192111254\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019029384568333627\n",
      "Average test loss: 0.0024263471360835764\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018999358307156298\n",
      "Average test loss: 0.002384345414944821\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019120011839601728\n",
      "Average test loss: 0.002419702227951752\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018962551954719756\n",
      "Average test loss: 0.0023954014347659217\n",
      "Epoch 89/300\n",
      "Average training loss: 0.018963544509477084\n",
      "Average test loss: 0.002387281025035514\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018949348390102386\n",
      "Average test loss: 0.0023658632948580717\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018928554776642056\n",
      "Average test loss: 0.0023711618687957525\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018935723026593528\n",
      "Average test loss: 0.0024064098857343196\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018916975011428198\n",
      "Average test loss: 0.002478416552560197\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018923434522416856\n",
      "Average test loss: 0.0024667721767392425\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018887807664771876\n",
      "Average test loss: 0.002388223099004891\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01889563478363885\n",
      "Average test loss: 0.0024378154021170405\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0188573818537924\n",
      "Average test loss: 0.0023897809764991205\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01883945147196452\n",
      "Average test loss: 0.0023667810753815704\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018844558046095902\n",
      "Average test loss: 0.002441170412219233\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018831683090991445\n",
      "Average test loss: 0.002377823731965489\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018827927341063817\n",
      "Average test loss: 0.002373819493792123\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01881024894449446\n",
      "Average test loss: 0.0024119619582262304\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018772717081838183\n",
      "Average test loss: 0.002390511364986499\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018771179283658665\n",
      "Average test loss: 0.002478352591395378\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01878921926352713\n",
      "Average test loss: 0.0023820698513752885\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018763252468572723\n",
      "Average test loss: 0.002414051648643282\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0187498067766428\n",
      "Average test loss: 0.0023660967828085023\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018738468358914056\n",
      "Average test loss: 0.0023916945124251975\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018727204943696656\n",
      "Average test loss: 0.0024030602000032863\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01872674899217155\n",
      "Average test loss: 0.002400351619140969\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018700439744525486\n",
      "Average test loss: 0.0024320942256599665\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018683620346917045\n",
      "Average test loss: 0.0023620975888851616\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018700807789961495\n",
      "Average test loss: 0.019891097635030747\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018692390383117728\n",
      "Average test loss: 0.002430488891278704\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018673561660779846\n",
      "Average test loss: 0.002370972944216596\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01866773493339618\n",
      "Average test loss: 0.0023892650662197005\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01864658500916428\n",
      "Average test loss: 0.0024068391720453897\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018625565285483998\n",
      "Average test loss: 0.0023707881229412223\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018623406832416853\n",
      "Average test loss: 0.0023684392323096593\n",
      "Epoch 120/300\n",
      "Average training loss: 0.018612111616465782\n",
      "Average test loss: 0.0023807895438124737\n",
      "Epoch 121/300\n",
      "Average training loss: 0.018626368383566537\n",
      "Average test loss: 0.00245931050243477\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01861229939593209\n",
      "Average test loss: 0.002534851818242007\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018653326276275847\n",
      "Average test loss: 0.0024039171173547706\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01859432931741079\n",
      "Average test loss: 0.002362275259051886\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01856060763200124\n",
      "Average test loss: 0.0023583995203177136\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018561325170927578\n",
      "Average test loss: 0.0023708787334875926\n",
      "Epoch 127/300\n",
      "Average training loss: 0.018552355010476376\n",
      "Average test loss: 0.0023535743767602577\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018562966749899917\n",
      "Average test loss: 0.002363847873484095\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018545369589494334\n",
      "Average test loss: 0.0023655920647498636\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018531924780872133\n",
      "Average test loss: 0.002360594170375003\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018516061004665164\n",
      "Average test loss: 0.002373331790583001\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01851991420487563\n",
      "Average test loss: 0.00247719749642743\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01851203292939398\n",
      "Average test loss: 0.0023825613249921136\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018505187986625565\n",
      "Average test loss: 0.0024083363896028865\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018487229037615988\n",
      "Average test loss: 0.0023622276364929145\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01847360405491458\n",
      "Average test loss: 0.0023426855573844578\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018470200879706276\n",
      "Average test loss: 0.0023457766517789827\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01847587384449111\n",
      "Average test loss: 0.0024146784163183634\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01845211534698804\n",
      "Average test loss: 0.0024570888090464805\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018452638857894475\n",
      "Average test loss: 0.0024328597852339346\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018436326740516557\n",
      "Average test loss: 0.0024150754845597678\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018447465534011522\n",
      "Average test loss: 0.0023779867181761396\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018430021794305907\n",
      "Average test loss: 0.0023515613486783373\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018418571323156357\n",
      "Average test loss: 0.002392635847338372\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018409175649285316\n",
      "Average test loss: 0.0023759076928512915\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01841001292069753\n",
      "Average test loss: 0.0023388181452949843\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018410733891857996\n",
      "Average test loss: 0.0023719021553794545\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018396882833706007\n",
      "Average test loss: 0.002340544900753432\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018403291559881633\n",
      "Average test loss: 0.002354168716404173\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01838154101371765\n",
      "Average test loss: 0.0023696962597055567\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018369530651304457\n",
      "Average test loss: 0.002616699065495696\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01837247407105234\n",
      "Average test loss: 0.0023780584511243633\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018362381887104776\n",
      "Average test loss: 0.002425581805821922\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018359276935458184\n",
      "Average test loss: 0.0023431763710040185\n",
      "Epoch 155/300\n",
      "Average training loss: 0.018349068058033786\n",
      "Average test loss: 0.002373942627881964\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0183146890691585\n",
      "Average test loss: 0.0023687140378687117\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018355525800751317\n",
      "Average test loss: 0.0023684228263381453\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018315942664941152\n",
      "Average test loss: 0.0023766376812838844\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01829762764275074\n",
      "Average test loss: 0.0023792883176356554\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018308140627211995\n",
      "Average test loss: 0.0024132030056789517\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018313608025511106\n",
      "Average test loss: 0.0023634950588974687\n",
      "Epoch 162/300\n",
      "Average training loss: 0.018301321585973102\n",
      "Average test loss: 0.0023899955958541896\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01829419756763511\n",
      "Average test loss: 0.0023534962139609788\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018302769912613762\n",
      "Average test loss: 0.002414596527814865\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018280546303424572\n",
      "Average test loss: 0.002424251857317156\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018263623116744888\n",
      "Average test loss: 0.0024197479288818107\n",
      "Epoch 167/300\n",
      "Average training loss: 0.018274320584204463\n",
      "Average test loss: 0.0025790276711599694\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0182691696120633\n",
      "Average test loss: 0.0023589322644596298\n",
      "Epoch 169/300\n",
      "Average training loss: 0.018270925424993038\n",
      "Average test loss: 0.002390879226434562\n",
      "Epoch 170/300\n",
      "Average training loss: 0.018246811600195038\n",
      "Average test loss: 0.0023607519993351566\n",
      "Epoch 171/300\n",
      "Average training loss: 0.018252679058247144\n",
      "Average test loss: 0.0023704944847979478\n",
      "Epoch 172/300\n",
      "Average training loss: 0.018245599855979285\n",
      "Average test loss: 0.002806783798047238\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018230889171361925\n",
      "Average test loss: 0.0023441976504400375\n",
      "Epoch 174/300\n",
      "Average training loss: 0.018234606196482976\n",
      "Average test loss: 0.002409317448735237\n",
      "Epoch 175/300\n",
      "Average training loss: 0.018230999772747357\n",
      "Average test loss: 0.002431125198594398\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01822609774188863\n",
      "Average test loss: 0.0023802806494964495\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01821443330910471\n",
      "Average test loss: 0.0023507841150793764\n",
      "Epoch 178/300\n",
      "Average training loss: 0.018214712724089623\n",
      "Average test loss: 0.002359983920223183\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01819601548049185\n",
      "Average test loss: 0.010662851495875253\n",
      "Epoch 180/300\n",
      "Average training loss: 0.018203464897142516\n",
      "Average test loss: 0.0023713596142414545\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01820991641448604\n",
      "Average test loss: 0.0024926379856963954\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01819581067065398\n",
      "Average test loss: 0.0023795667371402183\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01818528244064914\n",
      "Average test loss: 0.0023449700228455996\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01818414878514078\n",
      "Average test loss: 0.002385344767322143\n",
      "Epoch 185/300\n",
      "Average training loss: 0.018167551004224354\n",
      "Average test loss: 0.002345555445179343\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01816716926958826\n",
      "Average test loss: 0.0023479722265361083\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01820273221532504\n",
      "Average test loss: 0.0023574529830366372\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018167158080471885\n",
      "Average test loss: 0.0023957731243636872\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01815608828349246\n",
      "Average test loss: 0.0024229577359639936\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018151611745357513\n",
      "Average test loss: 0.002349388410233789\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018142148695058292\n",
      "Average test loss: 0.0024394466876983645\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018137257821030088\n",
      "Average test loss: 0.002361773049251901\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018149070537752575\n",
      "Average test loss: 0.0023851194220284623\n",
      "Epoch 194/300\n",
      "Average training loss: 0.018142729227741558\n",
      "Average test loss: 0.0023555903720359006\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01813251598013772\n",
      "Average test loss: 0.002361226534470916\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01812595981856187\n",
      "Average test loss: 0.0024428412026415267\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018112260116471185\n",
      "Average test loss: 0.002362775823722283\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0181306932899687\n",
      "Average test loss: 0.002475431844385134\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018111281764176156\n",
      "Average test loss: 0.002346654700529244\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018098485093977715\n",
      "Average test loss: 0.0023509773483706845\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01811136547062132\n",
      "Average test loss: 0.0023456486269003815\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018101538653175035\n",
      "Average test loss: 0.0023621426741075183\n",
      "Epoch 203/300\n",
      "Average training loss: 0.018087134851349725\n",
      "Average test loss: 0.002372531374812954\n",
      "Epoch 204/300\n",
      "Average training loss: 0.018080520649751027\n",
      "Average test loss: 0.0023506540983087485\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018070476055145265\n",
      "Average test loss: 0.002400328495953646\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01807656907207436\n",
      "Average test loss: 0.002348751175424291\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01807278922282987\n",
      "Average test loss: 0.0024331113214914996\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018079869167672263\n",
      "Average test loss: 0.0023563847781883345\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018066460463735792\n",
      "Average test loss: 0.002351853658962581\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018066677439543936\n",
      "Average test loss: 0.0023724633148974844\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01805062556101216\n",
      "Average test loss: 0.0023617792388217315\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018053749731845325\n",
      "Average test loss: 0.0023726208188260597\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0180390305303865\n",
      "Average test loss: 0.0023642459987766214\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018065947817431555\n",
      "Average test loss: 0.0023752354914322493\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018048887652655443\n",
      "Average test loss: 0.0023825945308845902\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018031408888598284\n",
      "Average test loss: 0.0024256412780119313\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01805033494697677\n",
      "Average test loss: 0.0023787950784381895\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018034490009148915\n",
      "Average test loss: 0.002419467553910282\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018009380529324215\n",
      "Average test loss: 0.0023853159700002935\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01801344869368606\n",
      "Average test loss: 0.0024260315503925084\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018024190973904397\n",
      "Average test loss: 0.002714183179351191\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018011278837919236\n",
      "Average test loss: 0.0024377464161564905\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018023666745258703\n",
      "Average test loss: 0.0024481350373890664\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017996692784958416\n",
      "Average test loss: 0.0024382112344933883\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018000318619940017\n",
      "Average test loss: 0.002350078137488001\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018000092537866698\n",
      "Average test loss: 0.0023711309416426553\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01798778353052007\n",
      "Average test loss: 0.002345854305144813\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017990825661354595\n",
      "Average test loss: 0.0023630323720475036\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017982531032628483\n",
      "Average test loss: 0.002392938309763041\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017973161731329228\n",
      "Average test loss: 0.002543001232461797\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017970393225550652\n",
      "Average test loss: 0.0023492171180744966\n",
      "Epoch 232/300\n",
      "Average training loss: 0.017972083353334004\n",
      "Average test loss: 0.004333663841916456\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01798748973177539\n",
      "Average test loss: 0.0023591162264347074\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017979387957188817\n",
      "Average test loss: 0.002409785886605581\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01795629479487737\n",
      "Average test loss: 0.0024572887307860785\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017972250158588093\n",
      "Average test loss: 0.0065735060887204275\n",
      "Epoch 237/300\n",
      "Average training loss: 0.018021000153488582\n",
      "Average test loss: 0.002421247920435336\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017933962548772494\n",
      "Average test loss: 0.0023990321771966087\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01794721336166064\n",
      "Average test loss: 0.002407227590887083\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017955906086497837\n",
      "Average test loss: 0.002374715314557155\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01793298636873563\n",
      "Average test loss: 0.0023684449545625184\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017943091252611742\n",
      "Average test loss: 0.0023822811209700174\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017929104040066403\n",
      "Average test loss: 0.0023394208645655047\n",
      "Epoch 244/300\n",
      "Average training loss: 0.017942892896632354\n",
      "Average test loss: 0.002373898135808607\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01792351417409049\n",
      "Average test loss: 0.002355079921686815\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017926432895991537\n",
      "Average test loss: 0.0023968812951611147\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01792936160498195\n",
      "Average test loss: 0.002377847296289272\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01791712204284138\n",
      "Average test loss: 0.0023635198747118313\n",
      "Epoch 249/300\n",
      "Average training loss: 0.017911607169442706\n",
      "Average test loss: 0.0024209210889206993\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017929373322261705\n",
      "Average test loss: 0.002500931440335181\n",
      "Epoch 251/300\n",
      "Average training loss: 0.017916986115276815\n",
      "Average test loss: 0.0024001234399361744\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017925184797909523\n",
      "Average test loss: 0.002416180042343007\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01790489618314637\n",
      "Average test loss: 0.0023521168205059236\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01789985678758886\n",
      "Average test loss: 0.0023555586704363425\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017900615496767892\n",
      "Average test loss: 0.0023630905350049336\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017892043153444927\n",
      "Average test loss: 0.002393556860379047\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01790943948096699\n",
      "Average test loss: 0.00236594289365328\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017880398212207688\n",
      "Average test loss: 0.0023695040438324213\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01788572036061022\n",
      "Average test loss: 0.002356414873359932\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017885111677977773\n",
      "Average test loss: 0.002358043279395335\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017878501078320876\n",
      "Average test loss: 0.0023653637305315997\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017882840991848045\n",
      "Average test loss: 0.0024161037668171854\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01789433704316616\n",
      "Average test loss: 0.0023973930266996224\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017849567972951466\n",
      "Average test loss: 0.002344925537912382\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017854008505741755\n",
      "Average test loss: 0.002410812207601137\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0179784334252278\n",
      "Average test loss: 0.0023509207881159252\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017855000411470732\n",
      "Average test loss: 0.0023551355394431286\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017849580705165864\n",
      "Average test loss: 0.0023406714209251935\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017849169476164713\n",
      "Average test loss: 0.0024446733275221453\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017856978813393248\n",
      "Average test loss: 0.0024438919261511828\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01784789399471548\n",
      "Average test loss: 0.0023556611922880013\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01784130083521207\n",
      "Average test loss: 0.002359588643329011\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01783422828382916\n",
      "Average test loss: 0.0025048551769513224\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01783917839659585\n",
      "Average test loss: 0.0023697856100690032\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01784005923072497\n",
      "Average test loss: 0.0023942565044595136\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01785031658079889\n",
      "Average test loss: 0.0024340006889154516\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017838882704575856\n",
      "Average test loss: 0.0024988106379492414\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017825266388555368\n",
      "Average test loss: 0.002400490593786041\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017811815116140578\n",
      "Average test loss: 0.0023582521656321156\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017819336626264785\n",
      "Average test loss: 0.0024220240720444256\n",
      "Epoch 281/300\n",
      "Average training loss: 0.017825058167179424\n",
      "Average test loss: 0.0023456313972257904\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01781444596002499\n",
      "Average test loss: 0.0025406280706326166\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017810467561086018\n",
      "Average test loss: 0.002407725873713692\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0178226876805226\n",
      "Average test loss: 0.002348949942530857\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01780730053451326\n",
      "Average test loss: 0.0023548298380855056\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01779913357231352\n",
      "Average test loss: 0.0023826589923765925\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01779894748164548\n",
      "Average test loss: 0.0023900068358828626\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01779545787225167\n",
      "Average test loss: 0.0024034538753330706\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01781022599339485\n",
      "Average test loss: 0.002383007735841804\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017790076858467525\n",
      "Average test loss: 0.002371295906913777\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017777803603145813\n",
      "Average test loss: 0.0023601507072647412\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0177984211097161\n",
      "Average test loss: 0.002413135204671158\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017793950461679035\n",
      "Average test loss: 0.00247301602943076\n",
      "Epoch 294/300\n",
      "Average training loss: 0.017782648843195702\n",
      "Average test loss: 0.0023891485890166625\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017777466769847605\n",
      "Average test loss: 0.0023727551833209065\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017768931176927353\n",
      "Average test loss: 0.0023382244482636452\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017775049817230968\n",
      "Average test loss: 0.0023688229823278055\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017783919903967115\n",
      "Average test loss: 0.002413803411233756\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01775576172851854\n",
      "Average test loss: 0.0024195467304024432\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017771521005365585\n",
      "Average test loss: 0.0023657709966517157\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.16631157383653852\n",
      "Average test loss: 0.004984819436652793\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03842372780044873\n",
      "Average test loss: 0.004837073083553049\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03401774138874478\n",
      "Average test loss: 0.004633408577077919\n",
      "Epoch 4/300\n",
      "Average training loss: 0.031015800389978622\n",
      "Average test loss: 0.00392322875580026\n",
      "Epoch 5/300\n",
      "Average training loss: 0.029293757188651296\n",
      "Average test loss: 0.0035645245009412367\n",
      "Epoch 6/300\n",
      "Average training loss: 0.027371662148171\n",
      "Average test loss: 0.0034099419942746558\n",
      "Epoch 7/300\n",
      "Average training loss: 0.026360211870736547\n",
      "Average test loss: 0.0033876643269840213\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02515976790090402\n",
      "Average test loss: 0.0030804102717795307\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02437764970627096\n",
      "Average test loss: 0.0029179472461756733\n",
      "Epoch 10/300\n",
      "Average training loss: 0.023502323219345676\n",
      "Average test loss: 0.0034383522292806045\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02270158538884587\n",
      "Average test loss: 0.002833733210340142\n",
      "Epoch 12/300\n",
      "Average training loss: 0.022076031603746944\n",
      "Average test loss: 0.0026321144935985406\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021467718275884787\n",
      "Average test loss: 0.0026110329383777246\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02108800976143943\n",
      "Average test loss: 0.0025713210726777715\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020575842577550147\n",
      "Average test loss: 0.002528503571740455\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020184653386473655\n",
      "Average test loss: 0.002492757974399461\n",
      "Epoch 17/300\n",
      "Average training loss: 0.019849779809514682\n",
      "Average test loss: 0.002609290547668934\n",
      "Epoch 18/300\n",
      "Average training loss: 0.019580098713437716\n",
      "Average test loss: 0.002280536023899913\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01927687298092577\n",
      "Average test loss: 0.002222600881010294\n",
      "Epoch 20/300\n",
      "Average training loss: 0.019002182324727376\n",
      "Average test loss: 0.002262386712556084\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018790610944231352\n",
      "Average test loss: 0.0022070539568861324\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018614452484581205\n",
      "Average test loss: 0.002164660310993592\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01838791311449475\n",
      "Average test loss: 0.00208086201786581\n",
      "Epoch 24/300\n",
      "Average training loss: 0.018263401900728542\n",
      "Average test loss: 0.0021754258304006522\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01811529444489214\n",
      "Average test loss: 0.0020174240371626283\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017961481489241125\n",
      "Average test loss: 0.0020225988905876874\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017879975111120276\n",
      "Average test loss: 0.002041865626970927\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0177347638193104\n",
      "Average test loss: 0.001977718253309528\n",
      "Epoch 29/300\n",
      "Average training loss: 0.017656816595130496\n",
      "Average test loss: 0.0020273768063634634\n",
      "Epoch 30/300\n",
      "Average training loss: 0.017506648206048542\n",
      "Average test loss: 0.0019709656681451532\n",
      "Epoch 31/300\n",
      "Average training loss: 0.017462946236133575\n",
      "Average test loss: 0.001969918027934101\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017381241522729397\n",
      "Average test loss: 0.0019555861110695533\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01725627219180266\n",
      "Average test loss: 0.0019080343053986628\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017208682492375373\n",
      "Average test loss: 0.001913027332888709\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017140155759122637\n",
      "Average test loss: 0.0018870924362498853\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017078244272205563\n",
      "Average test loss: 0.0018847383587724633\n",
      "Epoch 37/300\n",
      "Average training loss: 0.017018812448614175\n",
      "Average test loss: 0.002020382474383546\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016937854511870278\n",
      "Average test loss: 0.0018813211903389957\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016916417742768922\n",
      "Average test loss: 0.0018442591437035137\n",
      "Epoch 40/300\n",
      "Average training loss: 0.016839568702711\n",
      "Average test loss: 0.001830914576848348\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01679397591948509\n",
      "Average test loss: 0.0018272957561744584\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016765019001232253\n",
      "Average test loss: 0.0018485272131446334\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01673598715662956\n",
      "Average test loss: 0.0035372641494290694\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01666629660874605\n",
      "Average test loss: 0.0018223380022164848\n",
      "Epoch 45/300\n",
      "Average training loss: 0.016635349614752663\n",
      "Average test loss: 0.0018250451288703414\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016572896850605805\n",
      "Average test loss: 0.0018049423624244\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01654822895427545\n",
      "Average test loss: 0.0018302569923301538\n",
      "Epoch 48/300\n",
      "Average training loss: 0.016521233672897023\n",
      "Average test loss: 0.0017976999260071252\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016477569065160222\n",
      "Average test loss: 0.0018610920117547115\n",
      "Epoch 50/300\n",
      "Average training loss: 0.016449376882778272\n",
      "Average test loss: 0.0018397842820526825\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01643974990149339\n",
      "Average test loss: 0.0018106303286428253\n",
      "Epoch 52/300\n",
      "Average training loss: 0.016399356467028458\n",
      "Average test loss: 0.0017782756790725722\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01635059718704886\n",
      "Average test loss: 0.0017951053243854807\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0163175061278873\n",
      "Average test loss: 0.00176807710631854\n",
      "Epoch 55/300\n",
      "Average training loss: 0.016301946371793746\n",
      "Average test loss: 0.0018539941634775864\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01625826969411638\n",
      "Average test loss: 0.001785408802744415\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01624800687117709\n",
      "Average test loss: 0.0017995304202453957\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01623161436782943\n",
      "Average test loss: 0.0017728790507341424\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01620645044909583\n",
      "Average test loss: 0.0017729383774308695\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016169707485371165\n",
      "Average test loss: 0.0017911692356897724\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016158789285355146\n",
      "Average test loss: 0.0017509681500701441\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01612561949259705\n",
      "Average test loss: 0.0018033323196901215\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016134509248865975\n",
      "Average test loss: 0.0017720428699006636\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016090060074296263\n",
      "Average test loss: 0.0018052291263722711\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016066435534093114\n",
      "Average test loss: 0.001769806189669503\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01603753842247857\n",
      "Average test loss: 0.001757801909931004\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01603846407930056\n",
      "Average test loss: 0.0017508019283413888\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016034682292077277\n",
      "Average test loss: 0.0017565666064620017\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015989847073952356\n",
      "Average test loss: 0.001753186527122226\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01594835360513793\n",
      "Average test loss: 0.001740779055075513\n",
      "Epoch 71/300\n",
      "Average training loss: 0.015957758687436582\n",
      "Average test loss: 0.0017604615906667378\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015940699027644264\n",
      "Average test loss: 0.001768863240774307\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015934983987775115\n",
      "Average test loss: 0.0018005253987179862\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015907718297508026\n",
      "Average test loss: 0.0017349180947575304\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01589167355828815\n",
      "Average test loss: 0.0017371636729480492\n",
      "Epoch 76/300\n",
      "Average training loss: 0.015874596205850442\n",
      "Average test loss: 0.0017660272512584924\n",
      "Epoch 77/300\n",
      "Average training loss: 0.015855865888297557\n",
      "Average test loss: 0.001752799710776243\n",
      "Epoch 78/300\n",
      "Average training loss: 0.015838542070653702\n",
      "Average test loss: 0.003139063603969084\n",
      "Epoch 79/300\n",
      "Average training loss: 0.015837856512930657\n",
      "Average test loss: 0.0017330548172402713\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015799141405357254\n",
      "Average test loss: 0.0017436945566700565\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01579921691616376\n",
      "Average test loss: 0.0017802948426041338\n",
      "Epoch 82/300\n",
      "Average training loss: 0.015786945732103454\n",
      "Average test loss: 0.0017744345924713546\n",
      "Epoch 83/300\n",
      "Average training loss: 0.015767105384005442\n",
      "Average test loss: 0.0017387333299136824\n",
      "Epoch 84/300\n",
      "Average training loss: 0.015762532911366887\n",
      "Average test loss: 0.0017267497457149957\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01574690534422795\n",
      "Average test loss: 0.0017367700814372963\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01571757571730349\n",
      "Average test loss: 0.0017200765495912896\n",
      "Epoch 87/300\n",
      "Average training loss: 0.015718859771887462\n",
      "Average test loss: 0.0017494824466605982\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01569755480190118\n",
      "Average test loss: 0.0017747381875912347\n",
      "Epoch 89/300\n",
      "Average training loss: 0.015710469710330167\n",
      "Average test loss: 0.001736028015717036\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015667533286743696\n",
      "Average test loss: 0.0017792116170749067\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0156614904354016\n",
      "Average test loss: 0.0017488483304364815\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01566464840537972\n",
      "Average test loss: 0.0017340087234559987\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015638394917050998\n",
      "Average test loss: 0.0017217347220414214\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015633365765213965\n",
      "Average test loss: 0.001729250502720889\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015621742476191785\n",
      "Average test loss: 0.0017433849970499675\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015609155193799072\n",
      "Average test loss: 0.0017241305518481468\n",
      "Epoch 97/300\n",
      "Average training loss: 0.015608191570474042\n",
      "Average test loss: 0.0017639005376646915\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015598163586523798\n",
      "Average test loss: 0.0017173509962028928\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015568624218304952\n",
      "Average test loss: 0.001765434294111199\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015565544525782267\n",
      "Average test loss: 0.00176192614218841\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01555527780453364\n",
      "Average test loss: 0.0017238933874501121\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015556842103600503\n",
      "Average test loss: 0.0017381323714637095\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015551921529902353\n",
      "Average test loss: 0.001746777681944271\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015538929850690894\n",
      "Average test loss: 0.0017231650723972254\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015528505866726239\n",
      "Average test loss: 0.01459953103793992\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015542486207352744\n",
      "Average test loss: 0.0017221115411569675\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015513323047922717\n",
      "Average test loss: 0.001717741007813149\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015484179124236106\n",
      "Average test loss: 0.0017114957111059792\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015484928585588932\n",
      "Average test loss: 0.0017005869773113066\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01548377154684729\n",
      "Average test loss: 0.0017782005450377861\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015469470706250932\n",
      "Average test loss: 0.0017302368198417956\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015461615179147986\n",
      "Average test loss: 0.0017106294849266609\n",
      "Epoch 113/300\n",
      "Average training loss: 0.015453427024185657\n",
      "Average test loss: 0.0017473341912134653\n",
      "Epoch 114/300\n",
      "Average training loss: 0.015445222143497733\n",
      "Average test loss: 0.0017518127800689803\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01543833585249053\n",
      "Average test loss: 0.0017167724382339251\n",
      "Epoch 116/300\n",
      "Average training loss: 0.015441324065956805\n",
      "Average test loss: 0.001731779570173886\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015427301108009285\n",
      "Average test loss: 0.0017281513198589284\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015406010485357708\n",
      "Average test loss: 0.0017621178291738033\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015404735516342851\n",
      "Average test loss: 0.0017305996662212743\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015390458051529195\n",
      "Average test loss: 0.0017074410250513918\n",
      "Epoch 121/300\n",
      "Average training loss: 0.015375601509379016\n",
      "Average test loss: 0.0017703613142172495\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015380343005061149\n",
      "Average test loss: 0.0018339074170216918\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015370988390511937\n",
      "Average test loss: 0.0017301902269116706\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01536424455543359\n",
      "Average test loss: 0.0018344692152200473\n",
      "Epoch 125/300\n",
      "Average training loss: 0.015363384867707889\n",
      "Average test loss: 0.02574505055944125\n",
      "Epoch 126/300\n",
      "Average training loss: 0.015715401714046798\n",
      "Average test loss: 0.0017319871753247247\n",
      "Epoch 127/300\n",
      "Average training loss: 0.015311219337913726\n",
      "Average test loss: 0.0017877012511922253\n",
      "Epoch 128/300\n",
      "Average training loss: 0.015312244699233108\n",
      "Average test loss: 0.0017383303361841373\n",
      "Epoch 129/300\n",
      "Average training loss: 0.015337777208950784\n",
      "Average test loss: 0.0017139080942918856\n",
      "Epoch 130/300\n",
      "Average training loss: 0.015316598999831411\n",
      "Average test loss: 0.0017033893749531773\n",
      "Epoch 131/300\n",
      "Average training loss: 0.015302496898505423\n",
      "Average test loss: 0.0017374703851011065\n",
      "Epoch 132/300\n",
      "Average training loss: 0.015324538644817141\n",
      "Average test loss: 0.0017039186861366033\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01529809694737196\n",
      "Average test loss: 0.0017571499314573075\n",
      "Epoch 134/300\n",
      "Average training loss: 0.015303918424579833\n",
      "Average test loss: 0.0017267475765612391\n",
      "Epoch 135/300\n",
      "Average training loss: 0.015286568018297354\n",
      "Average test loss: 0.001728685830719769\n",
      "Epoch 136/300\n",
      "Average training loss: 0.015270306978788641\n",
      "Average test loss: 0.0017089041229854855\n",
      "Epoch 137/300\n",
      "Average training loss: 0.015282674655318261\n",
      "Average test loss: 0.00170829237314562\n",
      "Epoch 138/300\n",
      "Average training loss: 0.015275829080906179\n",
      "Average test loss: 0.0017352711665961477\n",
      "Epoch 139/300\n",
      "Average training loss: 0.015274076961808734\n",
      "Average test loss: 0.001703250984971722\n",
      "Epoch 140/300\n",
      "Average training loss: 0.015249669493900405\n",
      "Average test loss: 0.0017495498957319391\n",
      "Epoch 141/300\n",
      "Average training loss: 0.015260066818859842\n",
      "Average test loss: 0.0017991401066796647\n",
      "Epoch 142/300\n",
      "Average training loss: 0.015236298791236348\n",
      "Average test loss: 0.0017292646538052295\n",
      "Epoch 143/300\n",
      "Average training loss: 0.015257377735442586\n",
      "Average test loss: 0.0017581713021629385\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01522745180543926\n",
      "Average test loss: 0.0017375070026351346\n",
      "Epoch 145/300\n",
      "Average training loss: 0.015229725084371037\n",
      "Average test loss: 0.0017959585255011917\n",
      "Epoch 146/300\n",
      "Average training loss: 0.015223618782228894\n",
      "Average test loss: 0.0017502428989650475\n",
      "Epoch 147/300\n",
      "Average training loss: 0.015240448993113305\n",
      "Average test loss: 0.0017122729987733894\n",
      "Epoch 148/300\n",
      "Average training loss: 0.015197624954912397\n",
      "Average test loss: 0.0020447153351787063\n",
      "Epoch 149/300\n",
      "Average training loss: 0.015221465448538462\n",
      "Average test loss: 0.0017502170310666164\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0152298314422369\n",
      "Average test loss: 0.0017191637410885758\n",
      "Epoch 151/300\n",
      "Average training loss: 0.015195819113817479\n",
      "Average test loss: 0.0017793961837887764\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01518710369616747\n",
      "Average test loss: 0.0017526761040919357\n",
      "Epoch 153/300\n",
      "Average training loss: 0.015181852254602644\n",
      "Average test loss: 0.0017101799978150262\n",
      "Epoch 154/300\n",
      "Average training loss: 0.015174353015091685\n",
      "Average test loss: 0.0017143724558668005\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015167564878861109\n",
      "Average test loss: 0.0017195774088096289\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015162212116850747\n",
      "Average test loss: 0.001764015884242124\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01517001858436399\n",
      "Average test loss: 0.0017106826431635353\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015165127183000247\n",
      "Average test loss: 0.0017558385842583245\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01516777625017696\n",
      "Average test loss: 0.0017463381501535574\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015161789019902547\n",
      "Average test loss: 0.0017889374593893688\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015136365059349273\n",
      "Average test loss: 0.0017407927609359225\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01512420477386978\n",
      "Average test loss: 0.0016964917418857415\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015134556733071803\n",
      "Average test loss: 0.0017249405095353723\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01512054745025105\n",
      "Average test loss: 0.0016939667087669174\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015113071079055468\n",
      "Average test loss: 0.0016978965613784062\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015126972066031561\n",
      "Average test loss: 0.00171382795398434\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015103770290811856\n",
      "Average test loss: 0.0017142929079838926\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015118480460511313\n",
      "Average test loss: 0.001726449843806525\n",
      "Epoch 169/300\n",
      "Average training loss: 0.015105639042125809\n",
      "Average test loss: 0.0017132525679965814\n",
      "Epoch 170/300\n",
      "Average training loss: 0.015100582115352154\n",
      "Average test loss: 0.0017085286852800185\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015071456239455275\n",
      "Average test loss: 0.0020539796418613857\n",
      "Epoch 172/300\n",
      "Average training loss: 0.015096135531034734\n",
      "Average test loss: 0.00169885666285538\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015081608379880587\n",
      "Average test loss: 0.001751315664086077\n",
      "Epoch 174/300\n",
      "Average training loss: 0.015080660743017992\n",
      "Average test loss: 0.0016884305817592474\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01508268507487244\n",
      "Average test loss: 0.0017118313890985316\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015074304489625824\n",
      "Average test loss: 0.0017139546270999644\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01506520128912396\n",
      "Average test loss: 0.0017042051456454728\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015050442591309547\n",
      "Average test loss: 0.0016930404586924447\n",
      "Epoch 179/300\n",
      "Average training loss: 0.015030929332806005\n",
      "Average test loss: 0.0016999943486104409\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015058304788337813\n",
      "Average test loss: 0.001724499948012332\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015050164163112641\n",
      "Average test loss: 0.0017287831217464474\n",
      "Epoch 182/300\n",
      "Average training loss: 0.015047160047623846\n",
      "Average test loss: 0.0017274573533278372\n",
      "Epoch 183/300\n",
      "Average training loss: 0.015030320865412554\n",
      "Average test loss: 0.0017387971495174698\n",
      "Epoch 184/300\n",
      "Average training loss: 0.015059487654931016\n",
      "Average test loss: 0.0017024648435827758\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015020289379689428\n",
      "Average test loss: 0.0017266992549929353\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015040602101220026\n",
      "Average test loss: 0.0017266938406974078\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015032616595427196\n",
      "Average test loss: 0.001733607326530748\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015015990377300316\n",
      "Average test loss: 0.0019176474990737107\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01501606470015314\n",
      "Average test loss: 0.0019254888554828035\n",
      "Epoch 190/300\n",
      "Average training loss: 0.015014016228417556\n",
      "Average test loss: 0.0017451071812150378\n",
      "Epoch 191/300\n",
      "Average training loss: 0.015003877084288332\n",
      "Average test loss: 0.0017466567965845267\n",
      "Epoch 192/300\n",
      "Average training loss: 0.015008620949255095\n",
      "Average test loss: 0.0018305643341607518\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014985053398542935\n",
      "Average test loss: 0.0017064689049083326\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01499260520355569\n",
      "Average test loss: 0.0017429224578663707\n",
      "Epoch 195/300\n",
      "Average training loss: 0.015019962151017453\n",
      "Average test loss: 0.001720370759980546\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014979394567509493\n",
      "Average test loss: 0.001747208559885621\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015002528365287517\n",
      "Average test loss: 0.0017390457891548674\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014967537252439394\n",
      "Average test loss: 0.0017161571541801095\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014967233113116688\n",
      "Average test loss: 0.001717229169069065\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014971913844347\n",
      "Average test loss: 0.0017465218554975257\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014965744942426681\n",
      "Average test loss: 0.0016970929599677523\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014973064025243123\n",
      "Average test loss: 0.0017028680745926168\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014962119979990854\n",
      "Average test loss: 0.0017179983555235796\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01495555074678527\n",
      "Average test loss: 0.0017042724672291015\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01496270197795497\n",
      "Average test loss: 0.0017087933221417997\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014938851893775994\n",
      "Average test loss: 0.0017405519808332125\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01495120136357016\n",
      "Average test loss: 0.0017072372265780965\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014935594463513956\n",
      "Average test loss: 0.0017158148689195513\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01494727333386739\n",
      "Average test loss: 0.005138315500484572\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014925689936512046\n",
      "Average test loss: 0.0017109968430466121\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014939549947778383\n",
      "Average test loss: 0.0017393897264781925\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01492265877293216\n",
      "Average test loss: 0.0017235114704817534\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014914243780076504\n",
      "Average test loss: 0.0018526275518039863\n",
      "Epoch 214/300\n",
      "Average training loss: 0.014917800759275754\n",
      "Average test loss: 0.0017854232310007016\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014909635314510929\n",
      "Average test loss: 0.0017284303141964807\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014919238003591696\n",
      "Average test loss: 0.014699225210481219\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0149249069781767\n",
      "Average test loss: 0.0017004736005845997\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01491251993427674\n",
      "Average test loss: 0.0017922349618747831\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014903489735391405\n",
      "Average test loss: 0.0017064343752960364\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014890461357931296\n",
      "Average test loss: 0.0018401332803898388\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014897846529881159\n",
      "Average test loss: 0.0017070761521657308\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01488659354382091\n",
      "Average test loss: 0.001762954289921456\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014876157136427032\n",
      "Average test loss: 0.0017049366690011487\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01489407797323333\n",
      "Average test loss: 0.0017525003157142136\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014877228154904313\n",
      "Average test loss: 0.0017450005841544932\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01487858906471067\n",
      "Average test loss: 0.001696626517507765\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014881344171861807\n",
      "Average test loss: 0.0017287967459609112\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014874028144611253\n",
      "Average test loss: 0.0017114138406597906\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014871204454037877\n",
      "Average test loss: 0.0017312824248025816\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014877209961414338\n",
      "Average test loss: 0.0017261156111748682\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014853182282712724\n",
      "Average test loss: 0.001720873474660847\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01487117902851767\n",
      "Average test loss: 0.0020055585727095605\n",
      "Epoch 233/300\n",
      "Average training loss: 0.014862154091397921\n",
      "Average test loss: 0.001727565163332555\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01484105068196853\n",
      "Average test loss: 0.0017205170672904286\n",
      "Epoch 235/300\n",
      "Average training loss: 0.014846812070243888\n",
      "Average test loss: 0.001709712964275645\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014857921620210011\n",
      "Average test loss: 0.0017541148679123984\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01484317088044352\n",
      "Average test loss: 0.0017676693065505889\n",
      "Epoch 238/300\n",
      "Average training loss: 0.014844643394152323\n",
      "Average test loss: 0.0016987884005324708\n",
      "Epoch 239/300\n",
      "Average training loss: 0.014844998034338157\n",
      "Average test loss: 0.0017152311898147067\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01484118377086189\n",
      "Average test loss: 0.0017077623657468293\n",
      "Epoch 241/300\n",
      "Average training loss: 0.014833354671796162\n",
      "Average test loss: 0.0018268333185050222\n",
      "Epoch 242/300\n",
      "Average training loss: 0.014829795530272855\n",
      "Average test loss: 0.0017224288168880674\n",
      "Epoch 243/300\n",
      "Average training loss: 0.014825915796889199\n",
      "Average test loss: 0.0017123190137661166\n",
      "Epoch 244/300\n",
      "Average training loss: 0.014821475002500747\n",
      "Average test loss: 0.0016968194766797953\n",
      "Epoch 245/300\n",
      "Average training loss: 0.014804702815910181\n",
      "Average test loss: 0.0017114641422198878\n",
      "Epoch 246/300\n",
      "Average training loss: 0.014820500727328989\n",
      "Average test loss: 0.0017579288995928234\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01481889180921846\n",
      "Average test loss: 0.0017416145897780856\n",
      "Epoch 248/300\n",
      "Average training loss: 0.014817575130197738\n",
      "Average test loss: 0.001897466232586238\n",
      "Epoch 249/300\n",
      "Average training loss: 0.014801531755261952\n",
      "Average test loss: 0.0017204186062638959\n",
      "Epoch 250/300\n",
      "Average training loss: 0.014818569289313422\n",
      "Average test loss: 0.0017176659900902046\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01480627956489722\n",
      "Average test loss: 0.0016966185795350207\n",
      "Epoch 252/300\n",
      "Average training loss: 0.014803045506278674\n",
      "Average test loss: 0.0017410966074094176\n",
      "Epoch 253/300\n",
      "Average training loss: 0.014802418225341374\n",
      "Average test loss: 0.0017887735524111325\n",
      "Epoch 254/300\n",
      "Average training loss: 0.014793076134390301\n",
      "Average test loss: 0.0017561196195375588\n",
      "Epoch 255/300\n",
      "Average training loss: 0.014793015141454008\n",
      "Average test loss: 0.0017255595647212533\n",
      "Epoch 256/300\n",
      "Average training loss: 0.014793922411898771\n",
      "Average test loss: 0.0017125264892561569\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01479438370714585\n",
      "Average test loss: 0.0017044905270967219\n",
      "Epoch 258/300\n",
      "Average training loss: 0.014780756369233131\n",
      "Average test loss: 0.001713930633229514\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01478383621159527\n",
      "Average test loss: 0.0017214372668208347\n",
      "Epoch 260/300\n",
      "Average training loss: 0.014782045755949285\n",
      "Average test loss: 0.001762630527632104\n",
      "Epoch 261/300\n",
      "Average training loss: 0.014785415233837234\n",
      "Average test loss: 0.0016974233529116545\n",
      "Epoch 262/300\n",
      "Average training loss: 0.014760987146861023\n",
      "Average test loss: 0.0017143883963839875\n",
      "Epoch 263/300\n",
      "Average training loss: 0.014769789223041799\n",
      "Average test loss: 0.001696128664020863\n",
      "Epoch 264/300\n",
      "Average training loss: 0.014793530127240552\n",
      "Average test loss: 0.001699810257802407\n",
      "Epoch 265/300\n",
      "Average training loss: 0.014770377034114466\n",
      "Average test loss: 0.0017454957344258824\n",
      "Epoch 266/300\n",
      "Average training loss: 0.014753786737720171\n",
      "Average test loss: 0.001697412851266563\n",
      "Epoch 267/300\n",
      "Average training loss: 0.014770896362761656\n",
      "Average test loss: 0.001705430929445558\n",
      "Epoch 268/300\n",
      "Average training loss: 0.014755582524670494\n",
      "Average test loss: 0.0017994709244618812\n",
      "Epoch 269/300\n",
      "Average training loss: 0.014755494069721964\n",
      "Average test loss: 0.0017061229265398449\n",
      "Epoch 270/300\n",
      "Average training loss: 0.014751316967937682\n",
      "Average test loss: 0.0017046839416854911\n",
      "Epoch 271/300\n",
      "Average training loss: 0.014744254281951322\n",
      "Average test loss: 0.0017259295478256212\n",
      "Epoch 272/300\n",
      "Average training loss: 0.014742366749379369\n",
      "Average test loss: 0.0016989606021799975\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01475770919687218\n",
      "Average test loss: 0.001741569899643461\n",
      "Epoch 274/300\n",
      "Average training loss: 0.014741704045070542\n",
      "Average test loss: 0.0017002919407354462\n",
      "Epoch 275/300\n",
      "Average training loss: 0.014735542323854234\n",
      "Average test loss: 0.0017178133587456412\n",
      "Epoch 276/300\n",
      "Average training loss: 0.014727530933916568\n",
      "Average test loss: 0.001715298009208507\n",
      "Epoch 277/300\n",
      "Average training loss: 0.014732601031661033\n",
      "Average test loss: 0.001721268809090058\n",
      "Epoch 278/300\n",
      "Average training loss: 0.014729616829090648\n",
      "Average test loss: 0.001709621125418279\n",
      "Epoch 279/300\n",
      "Average training loss: 0.014743175445331467\n",
      "Average test loss: 0.001733172326710903\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01473011787318521\n",
      "Average test loss: 0.0017154605129940641\n",
      "Epoch 281/300\n",
      "Average training loss: 0.014716695174574851\n",
      "Average test loss: 0.0017092961743474006\n",
      "Epoch 282/300\n",
      "Average training loss: 0.014726132653653622\n",
      "Average test loss: 0.0017439780569531852\n",
      "Epoch 283/300\n",
      "Average training loss: 0.014736575812929207\n",
      "Average test loss: 0.0017144230837002397\n",
      "Epoch 284/300\n",
      "Average training loss: 0.014721509277820587\n",
      "Average test loss: 0.0017732383570530348\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01470962444610066\n",
      "Average test loss: 0.0017123877940078576\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01471423501190212\n",
      "Average test loss: 0.001695961689059105\n",
      "Epoch 287/300\n",
      "Average training loss: 0.014725251619186665\n",
      "Average test loss: 0.0017314666977359188\n",
      "Epoch 288/300\n",
      "Average training loss: 0.014705609858863884\n",
      "Average test loss: 0.0017143650131507052\n",
      "Epoch 289/300\n",
      "Average training loss: 0.014704824849963188\n",
      "Average test loss: 0.0017446635219578943\n",
      "Epoch 290/300\n",
      "Average training loss: 0.014710772916674613\n",
      "Average test loss: 0.0017373622848341863\n",
      "Epoch 291/300\n",
      "Average training loss: 0.014693897798657417\n",
      "Average test loss: 0.0017513964052001635\n",
      "Epoch 292/300\n",
      "Average training loss: 0.014702192681531112\n",
      "Average test loss: 0.0017460538205794163\n",
      "Epoch 293/300\n",
      "Average training loss: 0.014700003584225973\n",
      "Average test loss: 0.001752920948796802\n",
      "Epoch 294/300\n",
      "Average training loss: 0.014692348635031117\n",
      "Average test loss: 0.0017574764638104373\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01469902593228552\n",
      "Average test loss: 0.001695255986932251\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01469373383704159\n",
      "Average test loss: 0.0017143870820808742\n",
      "Epoch 297/300\n",
      "Average training loss: 0.014693360630008909\n",
      "Average test loss: 0.001703080414276984\n",
      "Epoch 298/300\n",
      "Average training loss: 0.014685746509167883\n",
      "Average test loss: 0.0017152862989654144\n",
      "Epoch 299/300\n",
      "Average training loss: 0.014682604997522301\n",
      "Average test loss: 0.0017756695998832584\n",
      "Epoch 300/300\n",
      "Average training loss: 0.014675419428282314\n",
      "Average test loss: 0.0017122156825951404\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth3-.01/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.24\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.05\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.04\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.77\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.74\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.61\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.3951824896600513\n",
      "Average test loss: 0.01321575334171454\n",
      "Epoch 2/300\n",
      "Average training loss: 0.3154480259153578\n",
      "Average test loss: 0.010011888128187922\n",
      "Epoch 3/300\n",
      "Average training loss: 0.23307043857044643\n",
      "Average test loss: 0.009560091531111134\n",
      "Epoch 4/300\n",
      "Average training loss: 0.20012644267082214\n",
      "Average test loss: 0.008690088629722596\n",
      "Epoch 5/300\n",
      "Average training loss: 0.1818781664106581\n",
      "Average test loss: 0.008399052269756794\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1698397096792857\n",
      "Average test loss: 0.009098480845491091\n",
      "Epoch 7/300\n",
      "Average training loss: 0.16109314845667946\n",
      "Average test loss: 0.008018135566678313\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1549092503918542\n",
      "Average test loss: 0.008542229372594091\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14806197877724966\n",
      "Average test loss: 0.007512800840040048\n",
      "Epoch 10/300\n",
      "Average training loss: 0.14304465646213954\n",
      "Average test loss: 0.007418435414218241\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13889284133911134\n",
      "Average test loss: 0.0072469731453392245\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13506691200203366\n",
      "Average test loss: 0.007002823614411884\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13134832694795395\n",
      "Average test loss: 0.007097547639989191\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12783859510554207\n",
      "Average test loss: 0.007357506258620156\n",
      "Epoch 15/300\n",
      "Average training loss: 0.12520834891001384\n",
      "Average test loss: 0.00785259592326151\n",
      "Epoch 16/300\n",
      "Average training loss: 0.12322843432426453\n",
      "Average test loss: 0.006663767447488175\n",
      "Epoch 17/300\n",
      "Average training loss: 0.12062140582667456\n",
      "Average test loss: 0.006490225090748734\n",
      "Epoch 18/300\n",
      "Average training loss: 0.11822892935408487\n",
      "Average test loss: 0.006909738721119033\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11641857381661733\n",
      "Average test loss: 0.006477004651394155\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11479370223813587\n",
      "Average test loss: 0.00645918393217855\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11302789958980348\n",
      "Average test loss: 0.006473638788693481\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1116717167629136\n",
      "Average test loss: 0.006166301955779394\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11024709969758988\n",
      "Average test loss: 0.006171206544670794\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10905648047394223\n",
      "Average test loss: 0.006104053866118193\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10792058046658834\n",
      "Average test loss: 0.006268324504295985\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10660307094123628\n",
      "Average test loss: 0.006066985865847932\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1060044858124521\n",
      "Average test loss: 0.005899101327690813\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10491483549939261\n",
      "Average test loss: 0.005863293159753084\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10371415860785378\n",
      "Average test loss: 0.006302523375799259\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10309485190444523\n",
      "Average test loss: 0.0059004639225701495\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10218977336088816\n",
      "Average test loss: 0.0058594094125760925\n",
      "Epoch 32/300\n",
      "Average training loss: 0.101479708009296\n",
      "Average test loss: 0.005721168379402823\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10120501203669442\n",
      "Average test loss: 0.005771610140386555\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10014741043249765\n",
      "Average test loss: 0.00878351223303212\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09957972696754668\n",
      "Average test loss: 0.005698183907402886\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09905765340725581\n",
      "Average test loss: 0.005707534961816338\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09847615408235126\n",
      "Average test loss: 0.005913616722656621\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09789699761735068\n",
      "Average test loss: 0.0056064286819762655\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0974540443552865\n",
      "Average test loss: 0.005600787647275461\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09736817916234335\n",
      "Average test loss: 0.005585297464082638\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09688027852773666\n",
      "Average test loss: 0.005728640384972095\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0961424970626831\n",
      "Average test loss: 0.005913266171597772\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09590714709626304\n",
      "Average test loss: 0.007548431151443057\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09548824863963656\n",
      "Average test loss: 0.005507665294739935\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09504503830273946\n",
      "Average test loss: 0.005748736895206902\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09470872120724784\n",
      "Average test loss: 0.005515710844761795\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09427346744139989\n",
      "Average test loss: 0.005742339416096608\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09414043682813644\n",
      "Average test loss: 0.005581164671108127\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0936607085466385\n",
      "Average test loss: 0.005716094887091054\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09338574695587158\n",
      "Average test loss: 0.0064560258603758285\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09502274388406012\n",
      "Average test loss: 0.005951250250554747\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0930412896209293\n",
      "Average test loss: 0.005884776968095038\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09241783206330405\n",
      "Average test loss: 0.005499111374219259\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09229713009463417\n",
      "Average test loss: 0.005676230541740854\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09210576666726006\n",
      "Average test loss: 0.005428935358093845\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09196151740021176\n",
      "Average test loss: 0.005493808777795897\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09176658343606525\n",
      "Average test loss: 0.0054693578862481645\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09163315573334693\n",
      "Average test loss: 0.005427407370259364\n",
      "Epoch 59/300\n",
      "Average training loss: 0.091171070502864\n",
      "Average test loss: 0.005929380949172709\n",
      "Epoch 60/300\n",
      "Average training loss: 0.091144695520401\n",
      "Average test loss: 0.005998886958592468\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0906493290066719\n",
      "Average test loss: 0.005449081893596384\n",
      "Epoch 62/300\n",
      "Average training loss: 12.910207419501411\n",
      "Average test loss: 0.21569003036287096\n",
      "Epoch 67/300\n",
      "Average training loss: 11.7967474577162\n",
      "Average test loss: 0.2164038784371482\n",
      "Epoch 68/300\n",
      "Average training loss: 11.197626325819227\n",
      "Average test loss: 0.16438125975926718\n",
      "Epoch 69/300\n",
      "Average training loss: 10.78114897664388\n",
      "Average test loss: 0.07757511180639266\n",
      "Epoch 70/300\n",
      "Average training loss: 10.412260753207736\n",
      "Average test loss: 0.06688169167439142\n",
      "Epoch 71/300\n",
      "Average training loss: 10.129061108907063\n",
      "Average test loss: 0.040347745564248826\n",
      "Epoch 72/300\n",
      "Average training loss: 9.790145950317383\n",
      "Average test loss: 0.04025399351451132\n",
      "Epoch 73/300\n",
      "Average training loss: 9.491475643581815\n",
      "Average test loss: 0.04067492942677604\n",
      "Epoch 74/300\n",
      "Average training loss: 9.203864145914714\n",
      "Average test loss: 0.03386431732773781\n",
      "Epoch 75/300\n",
      "Average training loss: 8.877882669236925\n",
      "Average test loss: 0.027768116614884802\n",
      "Epoch 76/300\n",
      "Average training loss: 8.571080773247612\n",
      "Average test loss: 0.05197503929668003\n",
      "Epoch 77/300\n",
      "Average training loss: 8.33660925886366\n",
      "Average test loss: 0.03687344549430741\n",
      "Epoch 78/300\n",
      "Average training loss: 8.08485755581326\n",
      "Average test loss: 0.058653261254231134\n",
      "Epoch 79/300\n",
      "Average training loss: 7.751017246246338\n",
      "Average test loss: 0.01702350411315759\n",
      "Epoch 80/300\n",
      "Average training loss: 7.490354744381375\n",
      "Average test loss: 0.055298899966809485\n",
      "Epoch 81/300\n",
      "Average training loss: 7.075251332600912\n",
      "Average test loss: 0.018819190316730076\n",
      "Epoch 82/300\n",
      "Average training loss: 6.605771025763618\n",
      "Average test loss: 0.013254629882673423\n",
      "Epoch 83/300\n",
      "Average training loss: 6.013286894056532\n",
      "Average test loss: 0.059411733575993116\n",
      "Epoch 84/300\n",
      "Average training loss: 5.541083052741157\n",
      "Average test loss: 1.0666540614896352\n",
      "Epoch 85/300\n",
      "Average training loss: 5.015990154266357\n",
      "Average test loss: 0.04221615122920937\n",
      "Epoch 86/300\n",
      "Average training loss: 4.481738052368164\n",
      "Average test loss: 0.03026814141538408\n",
      "Epoch 87/300\n",
      "Average training loss: 4.087824474334717\n",
      "Average test loss: 0.0227071267830001\n",
      "Epoch 88/300\n",
      "Average training loss: 3.8188280211554635\n",
      "Average test loss: 0.01632725254446268\n",
      "Epoch 89/300\n",
      "Average training loss: 3.5807440670861137\n",
      "Average test loss: 0.014557637235356701\n",
      "Epoch 90/300\n",
      "Average training loss: 3.347023734198676\n",
      "Average test loss: 0.014676621658934488\n",
      "Epoch 91/300\n",
      "Average training loss: 2.1566299713982477\n",
      "Average test loss: 0.007185960502260261\n",
      "Epoch 96/300\n",
      "Average training loss: 1.9262522592544555\n",
      "Average test loss: 0.0070767289309038054\n",
      "Epoch 97/300\n",
      "Average training loss: 199704.59657135094\n",
      "Average test loss: 1.592472890747918\n",
      "Epoch 98/300\n",
      "Average training loss: 11.74610314771864\n",
      "Average test loss: 0.248633984208107\n",
      "Epoch 99/300\n",
      "Average training loss: 10.924823813544378\n",
      "Average test loss: 0.11574573556582134\n",
      "Epoch 100/300\n",
      "Average training loss: 10.348588852776421\n",
      "Average test loss: 0.12079318393601311\n",
      "Epoch 101/300\n",
      "Average training loss: 9.974198212517633\n",
      "Average test loss: 0.09488000834650463\n",
      "Epoch 102/300\n",
      "Average training loss: 9.562525561862522\n",
      "Average test loss: 0.07057659568389257\n",
      "Epoch 103/300\n",
      "Average training loss: 9.230120692782933\n",
      "Average test loss: 0.04794964699281586\n",
      "Epoch 104/300\n",
      "Average training loss: 8.853498140970865\n",
      "Average test loss: 0.06703137370612887\n",
      "Epoch 105/300\n",
      "Average training loss: 8.5753467886183\n",
      "Average test loss: 0.04283249864644474\n",
      "Epoch 106/300\n",
      "Average training loss: 8.317906633165148\n",
      "Average test loss: 0.03751862200763491\n",
      "Epoch 107/300\n",
      "Average training loss: 7.943598383161757\n",
      "Average test loss: 0.03158522328734398\n",
      "Epoch 108/300\n",
      "Average training loss: 7.706803309122721\n",
      "Average test loss: 0.02347240044342147\n",
      "Epoch 109/300\n",
      "Average training loss: 7.44873912217882\n",
      "Average test loss: 0.022080659755402143\n",
      "Epoch 110/300\n",
      "Average training loss: 7.145718978457981\n",
      "Average test loss: 0.019011343704329598\n",
      "Epoch 111/300\n",
      "Average training loss: 6.892660532633464\n",
      "Average test loss: 0.02219639471669992\n",
      "Epoch 112/300\n",
      "Average training loss: 6.607611416286892\n",
      "Average test loss: 0.01652941679292255\n",
      "Epoch 113/300\n",
      "Average training loss: 6.389726030561659\n",
      "Average test loss: 0.013895133434070481\n",
      "Epoch 114/300\n",
      "Average training loss: 6.103767523447672\n",
      "Average test loss: 0.014787604091068109\n",
      "Epoch 115/300\n",
      "Average training loss: 5.790821752760145\n",
      "Average test loss: 0.013311062947743469\n",
      "Epoch 116/300\n",
      "Average training loss: 5.522422013600667\n",
      "Average test loss: 0.012059636604454782\n",
      "Epoch 117/300\n",
      "Average training loss: 5.224254928588867\n",
      "Average test loss: 0.010830616815222634\n",
      "Epoch 118/300\n",
      "Average training loss: 4.873966543409559\n",
      "Average test loss: 0.011193220126132171\n",
      "Epoch 119/300\n",
      "Average training loss: 4.5253023456997346\n",
      "Average test loss: 0.014509532244669067\n",
      "Epoch 120/300\n",
      "Average training loss: 4.1422232998742\n",
      "Average test loss: 0.010435044428540602\n",
      "Epoch 121/300\n",
      "Average training loss: 3.779209341684977\n",
      "Average test loss: 0.009327948849234316\n",
      "Epoch 122/300\n",
      "Average training loss: 3.4257906419965956\n",
      "Average test loss: 0.010376827688680755\n",
      "Epoch 123/300\n",
      "Average training loss: 3.1027232110765244\n",
      "Average test loss: 0.010349449423452218\n",
      "Epoch 124/300\n",
      "Average training loss: 2.7966576997968886\n",
      "Average test loss: 0.009338091882566612\n",
      "Epoch 125/300\n",
      "Average training loss: 2.5028835998111303\n",
      "Average test loss: 0.00819647212823232\n",
      "Epoch 126/300\n",
      "Average training loss: 2.2269894841512046\n",
      "Average test loss: 0.008088425206641357\n",
      "Epoch 127/300\n",
      "Average training loss: 1.9589522558848063\n",
      "Average test loss: 0.009431732378072209\n",
      "Epoch 128/300\n",
      "Average training loss: 1.7237909377415974\n",
      "Average test loss: 0.007592186028758685\n",
      "Epoch 129/300\n",
      "Average training loss: 1.5227667690912883\n",
      "Average test loss: 0.008034249963031875\n",
      "Epoch 130/300\n",
      "Average training loss: 1.3183895915349324\n",
      "Average test loss: 0.007270064888728989\n",
      "Epoch 131/300\n",
      "Average training loss: 1.144068976190355\n",
      "Average test loss: 0.0068349120178156425\n",
      "Epoch 132/300\n",
      "Average training loss: 0.98955248286989\n",
      "Average test loss: 0.006851527897848023\n",
      "Epoch 133/300\n",
      "Average training loss: 0.8473049288855659\n",
      "Average test loss: 0.0065759772629373605\n",
      "Epoch 134/300\n",
      "Average training loss: 0.7222653466330634\n",
      "Average test loss: 0.006699040194352468\n",
      "Epoch 135/300\n",
      "Average training loss: 0.6119500171873304\n",
      "Average test loss: 0.006441795494821337\n",
      "Epoch 136/300\n",
      "Average training loss: 0.5125224069489374\n",
      "Average test loss: 0.006223665163748794\n",
      "Epoch 137/300\n",
      "Average training loss: 0.43394084646966724\n",
      "Average test loss: 0.006274018995463848\n",
      "Epoch 138/300\n",
      "Average training loss: 0.36663032274776036\n",
      "Average test loss: 0.006138983702080117\n",
      "Epoch 139/300\n",
      "Average training loss: 0.3151694385210673\n",
      "Average test loss: 0.006068529509007931\n",
      "Epoch 140/300\n",
      "Average training loss: 0.27255313200420805\n",
      "Average test loss: 0.006368376742220587\n",
      "Epoch 141/300\n",
      "Average training loss: 15.902564147101508\n",
      "Average test loss: 0.0893909933898184\n",
      "Epoch 145/300\n",
      "Average training loss: 15.308225980970594\n",
      "Average test loss: 0.09760567212767071\n",
      "Epoch 146/300\n",
      "Average training loss: 14.902144938151041\n",
      "Average test loss: 0.09033895313739776\n",
      "Epoch 147/300\n",
      "Average training loss: 14.383209424336751\n",
      "Average test loss: 0.03811985554628902\n",
      "Epoch 148/300\n",
      "Average training loss: 14.184040067884657\n",
      "Average test loss: 0.04596585782368978\n",
      "Epoch 149/300\n",
      "Average training loss: 13.912544714185927\n",
      "Average test loss: 0.048678313788440494\n",
      "Epoch 150/300\n",
      "Average training loss: 13.576520334031846\n",
      "Average test loss: 0.03500984780655967\n",
      "Epoch 151/300\n",
      "Average training loss: 13.176896615770127\n",
      "Average test loss: 0.05134309073289235\n",
      "Epoch 152/300\n",
      "Average training loss: 12.883634836832682\n",
      "Average test loss: 0.03661796861886978\n",
      "Epoch 153/300\n",
      "Average training loss: 12.661526897854275\n",
      "Average test loss: 0.035516463614172405\n",
      "Epoch 154/300\n",
      "Average training loss: 12.379634802924262\n",
      "Average test loss: 0.020254387271073128\n",
      "Epoch 155/300\n",
      "Average training loss: 11.815228370666503\n",
      "Average test loss: 0.018946763624747596\n",
      "Epoch 156/300\n",
      "Average training loss: 11.411822083367241\n",
      "Average test loss: 0.0151625364439355\n",
      "Epoch 157/300\n",
      "Average training loss: 10.964841623942057\n",
      "Average test loss: 0.018001652149690524\n",
      "Epoch 158/300\n",
      "Average training loss: 10.352179524739583\n",
      "Average test loss: 0.018160517651173803\n",
      "Epoch 159/300\n",
      "Average training loss: 9.77249205101861\n",
      "Average test loss: 0.012833556546105278\n",
      "Epoch 160/300\n",
      "Average training loss: 9.01188478088379\n",
      "Average test loss: 0.014602456190519864\n",
      "Epoch 161/300\n",
      "Average training loss: 8.449205373975966\n",
      "Average test loss: 0.011556942349506749\n",
      "Epoch 162/300\n",
      "Average training loss: 7.985008816189236\n",
      "Average test loss: 0.011688971189161141\n",
      "Epoch 163/300\n",
      "Average training loss: 7.5371047092013885\n",
      "Average test loss: 0.009896838327248891\n",
      "Epoch 164/300\n",
      "Average training loss: 7.050011800130208\n",
      "Average test loss: 0.010866670014129744\n",
      "Epoch 165/300\n",
      "Average training loss: 6.487220759497749\n",
      "Average test loss: 0.009789219657580058\n",
      "Epoch 166/300\n",
      "Average training loss: 4.171637747446696\n",
      "Average test loss: 0.0219410759376155\n",
      "Epoch 169/300\n",
      "Average training loss: 3.616552198410034\n",
      "Average test loss: 0.00818480445485976\n",
      "Epoch 170/300\n",
      "Average training loss: 3.0629942200978597\n",
      "Average test loss: 0.008347898594621156\n",
      "Epoch 171/300\n",
      "Average training loss: 2.523052148607042\n",
      "Average test loss: 0.008743654992845323\n",
      "Epoch 172/300\n",
      "Average training loss: 2.0929155700471664\n",
      "Average test loss: 0.008117861439784367\n",
      "Epoch 173/300\n",
      "Average training loss: 1.7228114007314046\n",
      "Average test loss: 0.007971331745800045\n",
      "Epoch 174/300\n",
      "Average training loss: 1.2928622586992051\n",
      "Average test loss: 0.009592583545380168\n",
      "Epoch 175/300\n",
      "Average training loss: 1.0876936779022217\n",
      "Average test loss: 0.006703921630978584\n",
      "Epoch 176/300\n",
      "Average training loss: 0.9297868162790934\n",
      "Average test loss: 0.006815340044597784\n",
      "Epoch 177/300\n",
      "Average training loss: 0.779446492407057\n",
      "Average test loss: 0.006324532909525765\n",
      "Epoch 178/300\n",
      "Average training loss: 0.6477254639731513\n",
      "Average test loss: 0.006364339643882381\n",
      "Epoch 179/300\n",
      "Average training loss: 0.5363050892353057\n",
      "Average test loss: 0.006232553323937787\n",
      "Epoch 180/300\n",
      "Average training loss: 0.45055745670530534\n",
      "Average test loss: 0.008028053213324811\n",
      "Epoch 181/300\n",
      "Average training loss: 0.3784946665763855\n",
      "Average test loss: 0.006324382726516989\n",
      "Epoch 182/300\n",
      "Average training loss: 0.3266047839588589\n",
      "Average test loss: 0.007574623835583528\n",
      "Epoch 183/300\n",
      "Average training loss: 0.2812698589828279\n",
      "Average test loss: 0.00601537641717328\n",
      "Epoch 184/300\n",
      "Average training loss: 0.2499145011107127\n",
      "Average test loss: 0.006010898116562102\n",
      "Epoch 185/300\n",
      "Average training loss: 0.22326316950056288\n",
      "Average test loss: 0.011116622019145223\n",
      "Epoch 186/300\n",
      "Average training loss: 0.20263716491063435\n",
      "Average test loss: 0.005886761007209619\n",
      "Epoch 187/300\n",
      "Average training loss: 0.186287607855267\n",
      "Average test loss: 0.0058310552247696455\n",
      "Epoch 188/300\n",
      "Average training loss: 0.17259833677609762\n",
      "Average test loss: 0.005833907966812451\n",
      "Epoch 189/300\n",
      "Average training loss: 0.16065538052717845\n",
      "Average test loss: 0.0057649207123451765\n",
      "Epoch 190/300\n",
      "Average training loss: 0.15073725177182093\n",
      "Average test loss: 0.005865656877971357\n",
      "Epoch 191/300\n",
      "Average training loss: 0.14227784815099503\n",
      "Average test loss: 0.005744023739877674\n",
      "Epoch 192/300\n",
      "Average training loss: 0.1350378071202172\n",
      "Average test loss: 0.005929495223694377\n",
      "Epoch 193/300\n",
      "Average training loss: 0.12821282042397394\n",
      "Average test loss: 0.005692355739573637\n",
      "Epoch 194/300\n",
      "Average training loss: 0.12187865730788973\n",
      "Average test loss: 0.005877513716204299\n",
      "Epoch 195/300\n",
      "Average training loss: 0.11612770642174615\n",
      "Average test loss: 0.005647406467960941\n",
      "Epoch 196/300\n",
      "Average training loss: 0.1125482564634747\n",
      "Average test loss: 0.005544263449394041\n",
      "Epoch 197/300\n",
      "Average training loss: 0.10870423079861535\n",
      "Average test loss: 0.005514913910379012\n",
      "Epoch 198/300\n",
      "Average training loss: 0.10027332207229403\n",
      "Average test loss: 0.005717321873539024\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09893033821715248\n",
      "Average test loss: 0.005493444807413552\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09726294191678365\n",
      "Average test loss: 0.005452686054839028\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09614153782526652\n",
      "Average test loss: 0.005539447915843792\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09506725021865632\n",
      "Average test loss: 0.005445177391585377\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09420253117879232\n",
      "Average test loss: 0.00560115877000822\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09352986962927712\n",
      "Average test loss: 0.00618263275300463\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09312963840696546\n",
      "Average test loss: 0.00542665693598489\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0924366680516137\n",
      "Average test loss: 0.005378907482243247\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0918874938554234\n",
      "Average test loss: 0.005390983061658011\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09136033427053028\n",
      "Average test loss: 0.005484035298021303\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09301931480566661\n",
      "Average test loss: 0.005370114715562926\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0907495691312684\n",
      "Average test loss: 0.005416300700770484\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09059221343861686\n",
      "Average test loss: 0.005397016713188754\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09045573059055541\n",
      "Average test loss: 0.005448296279542976\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0905201935172081\n",
      "Average test loss: 0.005593542966577741\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09002166048354572\n",
      "Average test loss: 0.0053325807708832955\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08989076997174157\n",
      "Average test loss: 0.005376067736910449\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08963338793648613\n",
      "Average test loss: 0.0053780462054742705\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08975662753979365\n",
      "Average test loss: 0.00543806764897373\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08949582489993838\n",
      "Average test loss: 0.006193141620606184\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08912625934018029\n",
      "Average test loss: 0.0053828156693942015\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08917241317696041\n",
      "Average test loss: 0.005400589177178012\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08888433953126272\n",
      "Average test loss: 0.0055410606215397515\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0886168774565061\n",
      "Average test loss: 0.005411750830709934\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08853298091226154\n",
      "Average test loss: 0.005529973916709423\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0885228905081749\n",
      "Average test loss: 0.005407905163864295\n",
      "Epoch 228/300\n",
      "Average training loss: 0.088426985528734\n",
      "Average test loss: 0.0053372922961910565\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09216026440593932\n",
      "Average test loss: 0.005706017934613758\n",
      "Epoch 230/300\n",
      "Average training loss: 279735.64422388916\n",
      "Average test loss: 1.151839785443412\n",
      "Epoch 231/300\n",
      "Average training loss: 19.98085083685981\n",
      "Average test loss: 0.8031309096018473\n",
      "Epoch 232/300\n",
      "Average training loss: 17.613371526082357\n",
      "Average test loss: 6.552685028817918\n",
      "Epoch 233/300\n",
      "Average training loss: 16.205095102945965\n",
      "Average test loss: 0.1131382491323683\n",
      "Epoch 234/300\n",
      "Average training loss: 15.12304556104872\n",
      "Average test loss: 0.114800239443779\n",
      "Epoch 235/300\n",
      "Average training loss: 14.176148756239149\n",
      "Average test loss: 0.08962223142385482\n",
      "Epoch 236/300\n",
      "Average training loss: 13.291915957132975\n",
      "Average test loss: 0.06936015331414011\n",
      "Epoch 237/300\n",
      "Average training loss: 12.469154072231717\n",
      "Average test loss: 0.06460726507173645\n",
      "Epoch 238/300\n",
      "Average training loss: 11.625041375901963\n",
      "Average test loss: 0.04042920106649399\n",
      "Epoch 239/300\n",
      "Average training loss: 10.790034389919704\n",
      "Average test loss: 0.031077939179208543\n",
      "Epoch 240/300\n",
      "Average training loss: 9.96185408698188\n",
      "Average test loss: 0.03544756966167026\n",
      "Epoch 241/300\n",
      "Average training loss: 9.179206177605524\n",
      "Average test loss: 0.04162895308931669\n",
      "Epoch 242/300\n",
      "Average training loss: 8.458809475368923\n",
      "Average test loss: 0.026802933115098213\n",
      "Epoch 243/300\n",
      "Average training loss: 7.856384919060601\n",
      "Average test loss: 0.022041990659303134\n",
      "Epoch 244/300\n",
      "Average training loss: 7.341579806009928\n",
      "Average test loss: 0.017007091596722604\n",
      "Epoch 245/300\n",
      "Average training loss: 6.894408289167616\n",
      "Average test loss: 0.014108724125557475\n",
      "Epoch 246/300\n",
      "Average training loss: 6.512850997924804\n",
      "Average test loss: 0.016212390564382076\n",
      "Epoch 247/300\n",
      "Average training loss: 6.1899531970553925\n",
      "Average test loss: 0.012528225496411324\n",
      "Epoch 248/300\n",
      "Average training loss: 5.89324009068807\n",
      "Average test loss: 0.011964377858572536\n",
      "Epoch 249/300\n",
      "Average training loss: 5.604127741919624\n",
      "Average test loss: 0.010789729135731856\n",
      "Epoch 250/300\n",
      "Average training loss: 5.313232867770725\n",
      "Average test loss: 0.010250907401243846\n",
      "Epoch 251/300\n",
      "Average training loss: 5.0209053586324055\n",
      "Average test loss: 0.010227648230890432\n",
      "Epoch 252/300\n",
      "Average training loss: 4.72226684612698\n",
      "Average test loss: 0.01032386037045055\n",
      "Epoch 253/300\n",
      "Average training loss: 4.423495288848877\n",
      "Average test loss: 0.009903176960845788\n",
      "Epoch 254/300\n",
      "Average training loss: 4.138792977227105\n",
      "Average test loss: 0.010779478404257033\n",
      "Epoch 255/300\n",
      "Average test loss: 0.009056360085805256\n",
      "Epoch 256/300\n",
      "Average training loss: 3.5963886551327175\n",
      "Average test loss: 0.008274643969618611\n",
      "Epoch 257/300\n",
      "Average training loss: 3.3298617447747123\n",
      "Average test loss: 0.008149872634973791\n",
      "Epoch 258/300\n",
      "Average training loss: 3.0688429046207\n",
      "Average test loss: 0.007829420532617305\n",
      "Epoch 259/300\n",
      "Average training loss: 2.8040617595248754\n",
      "Average test loss: 0.009236331558889813\n",
      "Epoch 260/300\n",
      "Average training loss: 2.5417489785088434\n",
      "Average test loss: 0.00732173819343249\n",
      "Epoch 261/300\n",
      "Average training loss: 2.293999442630344\n",
      "Average test loss: 0.007164198631627692\n",
      "Epoch 262/300\n",
      "Average training loss: 2.0616468658447267\n",
      "Average test loss: 0.006997220271163516\n",
      "Epoch 263/300\n",
      "Average training loss: 1.84196139409807\n",
      "Average test loss: 0.007105164045261013\n",
      "Epoch 264/300\n",
      "Average training loss: 1.6329360450108845\n",
      "Average test loss: 0.007110596194035477\n",
      "Epoch 265/300\n",
      "Average training loss: 1.4385552826987373\n",
      "Average test loss: 0.006535991407930851\n",
      "Epoch 266/300\n",
      "Average training loss: 1.2625433271196154\n",
      "Average test loss: 0.006406460458205806\n",
      "Epoch 267/300\n",
      "Average training loss: 1.1064602129194472\n",
      "Average test loss: 0.006504269993553559\n",
      "Epoch 268/300\n",
      "Average training loss: 0.9663983371522692\n",
      "Average test loss: 0.006576291728764772\n",
      "Epoch 269/300\n",
      "Average training loss: 0.8420765240457323\n",
      "Average test loss: 0.0061673851783076925\n",
      "Epoch 270/300\n",
      "Average training loss: 0.7294664347966512\n",
      "Average test loss: 0.007043008115970426\n",
      "Epoch 271/300\n",
      "Average training loss: 0.6291670941246881\n",
      "Average test loss: 0.005949391255776088\n",
      "Epoch 272/300\n",
      "Average training loss: 0.5357498762077756\n",
      "Average test loss: 0.005945719035135375\n",
      "Epoch 273/300\n",
      "Average training loss: 0.4462202587922414\n",
      "Average test loss: 0.007064558210886187\n",
      "Epoch 274/300\n",
      "Average training loss: 0.36641728109783595\n",
      "Average test loss: 0.005724267346163591\n",
      "Epoch 275/300\n",
      "Average training loss: 0.2990117602613237\n",
      "Average test loss: 0.005705383989132113\n",
      "Epoch 276/300\n",
      "Average training loss: 0.24492927702267964\n",
      "Average test loss: 0.005636277878863944\n",
      "Epoch 277/300\n",
      "Average training loss: 0.20167954047520956\n",
      "Average test loss: 0.005727872399406301\n",
      "Epoch 278/300\n",
      "Average training loss: 0.17004198303487567\n",
      "Average test loss: 0.005559546335703797\n",
      "Epoch 279/300\n",
      "Average training loss: 0.14749799757533602\n",
      "Average test loss: 0.005580517146529423\n",
      "Epoch 280/300\n",
      "Average training loss: 0.1333381710184945\n",
      "Average test loss: 0.005490954944656955\n",
      "Epoch 281/300\n",
      "Average training loss: 0.12497209338347118\n",
      "Average test loss: 0.005596257588101758\n",
      "Epoch 282/300\n",
      "Average training loss: 0.11860930911037657\n",
      "Average test loss: 0.005461303470035394\n",
      "Epoch 283/300\n",
      "Average training loss: 0.11370611734522713\n",
      "Average test loss: 0.005512098949816492\n",
      "Epoch 284/300\n",
      "Average training loss: 0.10925131593810188\n",
      "Average test loss: 0.005494687706645992\n",
      "Epoch 285/300\n",
      "Average training loss: 0.10577991387579176\n",
      "Average test loss: 0.0054893208870457275\n",
      "Epoch 286/300\n",
      "Average training loss: 0.10263138710790211\n",
      "Average test loss: 0.0060828187200758195\n",
      "Epoch 287/300\n",
      "Average training loss: 0.10009222067064709\n",
      "Average test loss: 0.005581726962907447\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09830148397551643\n",
      "Average test loss: 0.005398382792456282\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09683109954330656\n",
      "Average test loss: 0.005395628141446246\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09543635804123349\n",
      "Average test loss: 0.00534606126571695\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09442156053251691\n",
      "Average test loss: 0.005391212730358044\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09319478502207332\n",
      "Average test loss: 0.005389767083028952\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09224361697170469\n",
      "Average test loss: 0.0053772561475634575\n",
      "Epoch 294/300\n",
      "Average training loss: 0.09133789824777179\n",
      "Average test loss: 0.0053956570509407255\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09224218632115258\n",
      "Average test loss: 0.005427843546701802\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09034400730000602\n",
      "Average test loss: 0.005365869037393067\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0897530468834771\n",
      "Average test loss: 0.005357184404921201\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08947602644893858\n",
      "Average test loss: 0.005549393281340599\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08923018714454439\n",
      "Average test loss: 0.006539274646176233\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0890770705739657\n",
      "Average test loss: 0.0055104843282865155\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.876357272942861\n",
      "Average test loss: 0.008556086305114959\n",
      "Epoch 2/300\n",
      "Average training loss: 0.22277759370538924\n",
      "Average test loss: 0.006516919109142489\n",
      "Epoch 3/300\n",
      "Average training loss: 0.16421749595801036\n",
      "Average test loss: 0.006211892525355021\n",
      "Epoch 4/300\n",
      "Average training loss: 0.140958817826377\n",
      "Average test loss: 0.00557308016717434\n",
      "Epoch 5/300\n",
      "Average training loss: 0.127218813445833\n",
      "Average test loss: 0.005532029873381058\n",
      "Epoch 6/300\n",
      "Average training loss: 0.11840398478507995\n",
      "Average test loss: 0.005555601552956634\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11178040024307039\n",
      "Average test loss: 0.005139211511860291\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10713629874918196\n",
      "Average test loss: 0.004922871583865749\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1029305686685774\n",
      "Average test loss: 0.004939544066372845\n",
      "Epoch 10/300\n",
      "Average training loss: 0.09894645360443327\n",
      "Average test loss: 0.006184129203359286\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0950188757777214\n",
      "Average test loss: 0.004867760637568103\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09146194380852911\n",
      "Average test loss: 0.004957307858392596\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0885320692062378\n",
      "Average test loss: 0.00595963169924087\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08504658383462164\n",
      "Average test loss: 0.004154986647268136\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08282756841182709\n",
      "Average test loss: 0.004251372121688393\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08028940649165048\n",
      "Average test loss: 0.004014369327368008\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07795208167036374\n",
      "Average test loss: 0.0038835567709886365\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07619056471188863\n",
      "Average test loss: 0.0040277627758267855\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07418269311057196\n",
      "Average test loss: 0.006849090412259102\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07267808254559835\n",
      "Average test loss: 0.0038648592877305217\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07152245053317811\n",
      "Average test loss: 0.0037737490087747572\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07015401246481472\n",
      "Average test loss: 0.00441048813611269\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06918175765872002\n",
      "Average test loss: 0.00362404440715909\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0679903960691558\n",
      "Average test loss: 0.0036708805699729257\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06715955170326762\n",
      "Average test loss: 0.0036635605076120958\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06632045313715934\n",
      "Average test loss: 0.003665328561431832\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06539385169744491\n",
      "Average test loss: 0.003412698750694593\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06487449144323666\n",
      "Average test loss: 0.0034249042342934344\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06413568852345149\n",
      "Average test loss: 0.0038079778800408047\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06351486647129059\n",
      "Average test loss: 0.0033644017128066886\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06288997636238734\n",
      "Average test loss: 0.0037675482887360784\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06247155072291692\n",
      "Average test loss: 0.0033742933053937225\n",
      "Epoch 33/300\n",
      "Average training loss: 0.062040981296035976\n",
      "Average test loss: 0.0033351739342841837\n",
      "Epoch 34/300\n",
      "Average training loss: 0.061740987545914124\n",
      "Average test loss: 0.003488017712202337\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06123447556296984\n",
      "Average test loss: 0.0032495380506540337\n",
      "Epoch 36/300\n",
      "Average training loss: 0.060757172091139684\n",
      "Average test loss: 0.003300150569528341\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06045179244544771\n",
      "Average test loss: 0.0032618666564424833\n",
      "Epoch 38/300\n",
      "Average training loss: 0.060189311467938954\n",
      "Average test loss: 0.003276941252458427\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05980853737725152\n",
      "Average test loss: 0.0032627726529414454\n",
      "Epoch 40/300\n",
      "Average training loss: 0.059589238968160416\n",
      "Average test loss: 0.003430313827056024\n",
      "Epoch 41/300\n",
      "Average training loss: 0.059330998198853596\n",
      "Average test loss: 0.003286955585496293\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0590821090373728\n",
      "Average test loss: 0.0031728608995262118\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06532755506038666\n",
      "Average test loss: 0.0034106531526065533\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06400941025548511\n",
      "Average test loss: 0.0033402711349642938\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06045094094011519\n",
      "Average test loss: 0.003187917706039217\n",
      "Epoch 46/300\n",
      "Average training loss: 0.05912167519330978\n",
      "Average test loss: 0.0032713753049158386\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05867334570818477\n",
      "Average test loss: 0.003181215880231725\n",
      "Epoch 48/300\n",
      "Average training loss: 0.058529297259118825\n",
      "Average test loss: 0.003682680294745498\n",
      "Epoch 49/300\n",
      "Average training loss: 0.058456265085273316\n",
      "Average test loss: 0.00329458937028216\n",
      "Epoch 50/300\n",
      "Average training loss: 0.058274569498168095\n",
      "Average test loss: 0.0032265175444384417\n",
      "Epoch 51/300\n",
      "Average training loss: 0.058248420344458686\n",
      "Average test loss: 0.003124871927003066\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0958508597711722\n",
      "Average test loss: 0.0034086927260375687\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06420349296927452\n",
      "Average test loss: 0.00326041265256289\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06198732154899173\n",
      "Average test loss: 0.0032492711859651736\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06002925165163146\n",
      "Average test loss: 0.0032108035834713116\n",
      "Epoch 56/300\n",
      "Average training loss: 0.059168672458993064\n",
      "Average test loss: 0.003197753901489907\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0586606796350744\n",
      "Average test loss: 0.003242605589123236\n",
      "Epoch 58/300\n",
      "Average training loss: 0.058292102161380976\n",
      "Average test loss: 0.0031661304210623105\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05800957139995363\n",
      "Average test loss: 0.0031733937139312427\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05786662517322434\n",
      "Average test loss: 0.0032051664946807757\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05786463736494382\n",
      "Average test loss: 0.003135458912493454\n",
      "Epoch 62/300\n",
      "Average training loss: 0.057499166220426556\n",
      "Average test loss: 0.003171135232059492\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05747784593039089\n",
      "Average test loss: 0.0031845998656418587\n",
      "Epoch 64/300\n",
      "Average training loss: 0.057327895690997444\n",
      "Average test loss: 0.0032013802917467225\n",
      "Epoch 65/300\n",
      "Average training loss: 0.056977063278357186\n",
      "Average test loss: 0.003560210418783956\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05703776592678494\n",
      "Average test loss: 0.0030924307393531006\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0571481823556953\n",
      "Average test loss: 0.003180848300870922\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05681150273150868\n",
      "Average test loss: 0.0031421209221912756\n",
      "Epoch 69/300\n",
      "Average training loss: 0.056520080727007656\n",
      "Average test loss: 0.00315223376515011\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05655847293800778\n",
      "Average test loss: 0.0032448092090586822\n",
      "Epoch 71/300\n",
      "Average training loss: 0.056287841657797495\n",
      "Average test loss: 0.006765401174624761\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05629442983203464\n",
      "Average test loss: 0.003142324355327421\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05636575501163801\n",
      "Average test loss: 0.03032411183582412\n",
      "Epoch 74/300\n",
      "Average training loss: 0.056268510394626194\n",
      "Average test loss: 0.003111245746827788\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05767485038439433\n",
      "Average test loss: 0.003107940659340885\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05628153747651312\n",
      "Average test loss: 0.0032261128880911403\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05548998964826266\n",
      "Average test loss: 0.0030693170494503447\n",
      "Epoch 78/300\n",
      "Average training loss: 0.055411839379204644\n",
      "Average test loss: 0.003256255034253829\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05545821307765113\n",
      "Average test loss: 0.0030771290829612148\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05532059068149991\n",
      "Average test loss: 0.003435074649958147\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05525453746649954\n",
      "Average test loss: 0.0031139880218025713\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05527542424533102\n",
      "Average test loss: 0.0031110190531859795\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05508229413297441\n",
      "Average test loss: 0.0030611772591041192\n",
      "Epoch 84/300\n",
      "Average training loss: 0.055028434852759045\n",
      "Average test loss: 0.0031210557870152925\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05494363008936246\n",
      "Average test loss: 0.0031343687627878453\n",
      "Epoch 86/300\n",
      "Average training loss: 0.054823519534534876\n",
      "Average test loss: 0.0031257323198434377\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0554952202141285\n",
      "Average test loss: 0.0032411196844445334\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05662124130792088\n",
      "Average test loss: 0.0031994309812370275\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05544914059837659\n",
      "Average test loss: 0.0030660452948262293\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05451701301998562\n",
      "Average test loss: 0.003075237192730937\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05429044203294648\n",
      "Average test loss: 0.003140511100076967\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05485873866081238\n",
      "Average test loss: 0.004956682686590486\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05441145566105843\n",
      "Average test loss: 0.003064396113778154\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05421951941649119\n",
      "Average test loss: 0.016265533549918068\n",
      "Epoch 95/300\n",
      "Average training loss: 0.054202035228411355\n",
      "Average test loss: 0.0030784538843565516\n",
      "Epoch 96/300\n",
      "Average training loss: 0.054179527931743195\n",
      "Average test loss: 0.0030953133362862798\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05423363185922305\n",
      "Average test loss: 0.003112882679535283\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05389655905630854\n",
      "Average test loss: 0.003064521191848649\n",
      "Epoch 99/300\n",
      "Average training loss: 0.053988250623146695\n",
      "Average test loss: 0.0030819113451159663\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05379878772960769\n",
      "Average test loss: 0.00308798848465085\n",
      "Epoch 101/300\n",
      "Average training loss: 0.053887669059965344\n",
      "Average test loss: 0.003143191997582714\n",
      "Epoch 102/300\n",
      "Average training loss: 0.053904080821408164\n",
      "Average test loss: 0.003078898026090529\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05360579777095053\n",
      "Average test loss: 0.00314604444305102\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05374190338783794\n",
      "Average test loss: 0.0030925187232593693\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05348185032937262\n",
      "Average test loss: 0.003111799024252428\n",
      "Epoch 106/300\n",
      "Average training loss: 0.053439757972955707\n",
      "Average test loss: 0.003106148614651627\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05347022193339136\n",
      "Average test loss: 0.0030873492662277487\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05390670007136133\n",
      "Average test loss: 0.0030535438353609706\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05333345193664233\n",
      "Average test loss: 0.012558395640717613\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05322409835788939\n",
      "Average test loss: 0.0030812184135946964\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05315912228160434\n",
      "Average test loss: 0.008925736938085821\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05330379860930973\n",
      "Average test loss: 0.0031060734651982783\n",
      "Epoch 113/300\n",
      "Average training loss: 0.053126755575338996\n",
      "Average test loss: 0.0031232360775272053\n",
      "Epoch 114/300\n",
      "Average training loss: 0.053083034412728415\n",
      "Average test loss: 0.0031080423051284417\n",
      "Epoch 115/300\n",
      "Average training loss: 0.052932877490917844\n",
      "Average test loss: 0.0031626220577292974\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05279620529545678\n",
      "Average test loss: 0.003067228984501627\n",
      "Epoch 117/300\n",
      "Average training loss: 0.052858008242315714\n",
      "Average test loss: 0.0030975119525359735\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05312842558158769\n",
      "Average test loss: 0.0030309439160757594\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05312855924500359\n",
      "Average test loss: 0.0030264293938461278\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05296081681383981\n",
      "Average test loss: 0.003244793784701162\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05320702553457684\n",
      "Average test loss: 0.00412969051880969\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05286076389087571\n",
      "Average test loss: 0.0030346418008622197\n",
      "Epoch 123/300\n",
      "Average training loss: 0.052606537729501725\n",
      "Average test loss: 0.0031343192876213126\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05248284197184774\n",
      "Average test loss: 0.0030687951005788313\n",
      "Epoch 125/300\n",
      "Average training loss: 0.052452744071682295\n",
      "Average test loss: 0.003393504124134779\n",
      "Epoch 126/300\n",
      "Average training loss: 0.052532040254937275\n",
      "Average test loss: 0.0030254845925503306\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05237602560718854\n",
      "Average test loss: 0.003059220408813821\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05226194458206495\n",
      "Average test loss: 0.003064823028528028\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05250181700123681\n",
      "Average test loss: 0.0031145806800987985\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05218677567111121\n",
      "Average test loss: 0.003112277598844634\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05231846083535088\n",
      "Average test loss: 0.0030834580844061243\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05235294367869695\n",
      "Average test loss: 0.0031105293192797237\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05232365824447738\n",
      "Average test loss: 0.0030609527625557447\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0521409991118643\n",
      "Average test loss: 0.0030521075384070478\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05202942288915316\n",
      "Average test loss: 0.0035223401054326033\n",
      "Epoch 136/300\n",
      "Average training loss: 0.0520287290778425\n",
      "Average test loss: 0.003083268854870564\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0520241078502602\n",
      "Average test loss: 0.003047356042597029\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05186892125010491\n",
      "Average test loss: 0.0031344687146031194\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0519289587173197\n",
      "Average test loss: 0.003101639066512386\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05240108219782511\n",
      "Average test loss: 0.003107234011300736\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0520480283929242\n",
      "Average test loss: 0.0030834608028332392\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05170699852042728\n",
      "Average test loss: 0.0032510628507783014\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05218869834476047\n",
      "Average test loss: 0.0030931366781393687\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05174170559975836\n",
      "Average test loss: 0.0037901095975604324\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05184373633729087\n",
      "Average test loss: 0.0031474677924480704\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05155726981163025\n",
      "Average test loss: 0.0030790637642559076\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05165400348769294\n",
      "Average test loss: 0.00331606102320883\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05161725185645951\n",
      "Average test loss: 0.0030759789097226328\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05156506117516094\n",
      "Average test loss: 0.003096293029271894\n",
      "Epoch 150/300\n",
      "Average training loss: 0.051455137540896735\n",
      "Average test loss: 0.0030788843844913776\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05159826334317525\n",
      "Average test loss: 0.0030832271457960207\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05153232250942124\n",
      "Average test loss: 0.003072124804059664\n",
      "Epoch 153/300\n",
      "Average training loss: 0.051449989408254626\n",
      "Average test loss: 0.0032093828676475417\n",
      "Epoch 154/300\n",
      "Average training loss: 0.051323874887492925\n",
      "Average test loss: 0.0030948903500619862\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05138818470305866\n",
      "Average test loss: 0.0031912554912269116\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05128105890088611\n",
      "Average test loss: 0.003487647441112333\n",
      "Epoch 157/300\n",
      "Average training loss: 0.051240731570455764\n",
      "Average test loss: 0.0031091742714246112\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05118031748467022\n",
      "Average test loss: 0.00303675823741489\n",
      "Epoch 159/300\n",
      "Average training loss: 0.051638767288790806\n",
      "Average test loss: 0.0031124029669703708\n",
      "Epoch 160/300\n",
      "Average training loss: 0.051274555093712273\n",
      "Average test loss: 0.003134697435009811\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0512756657799085\n",
      "Average test loss: 0.003090627420279715\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05141140608655082\n",
      "Average test loss: 0.0031558984199331866\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05094693392515182\n",
      "Average test loss: 0.0031421981884373557\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05107393979032834\n",
      "Average test loss: 0.003112645004565517\n",
      "Epoch 165/300\n",
      "Average training loss: 0.051213650521304874\n",
      "Average test loss: 0.003098246628832486\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05086694203151597\n",
      "Average test loss: 0.003099361234654983\n",
      "Epoch 167/300\n",
      "Average training loss: 0.050830475721094344\n",
      "Average test loss: 0.0032003285682035815\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05104156067967415\n",
      "Average test loss: 0.003255425018361873\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05088530783024099\n",
      "Average test loss: 0.003089736035714547\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05076764436231719\n",
      "Average test loss: 0.0033188430132965246\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0511189087231954\n",
      "Average test loss: 0.003051598771992657\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05085644306739171\n",
      "Average test loss: 0.004684311163922151\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05112052203880416\n",
      "Average test loss: 0.003096491070257293\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05073902326491144\n",
      "Average test loss: 0.003180526062432263\n",
      "Epoch 175/300\n",
      "Average training loss: 0.050666630148887636\n",
      "Average test loss: 0.0032350246562725966\n",
      "Epoch 176/300\n",
      "Average training loss: 0.050692298935519325\n",
      "Average test loss: 0.0032134929231057564\n",
      "Epoch 177/300\n",
      "Average training loss: 0.050587731328275466\n",
      "Average test loss: 0.0030585028061436282\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05064054683513112\n",
      "Average test loss: 0.0031587886156307326\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05057091755833891\n",
      "Average test loss: 0.00325114794779155\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05098954728908009\n",
      "Average test loss: 0.003139083848231369\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05084241739908854\n",
      "Average test loss: 0.0031358842675884564\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0505898969537682\n",
      "Average test loss: 0.0030568211585697202\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05051539901561207\n",
      "Average test loss: 0.0030947643489473396\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05034070818457338\n",
      "Average test loss: 0.003087378730169601\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05033300046622753\n",
      "Average test loss: 0.0030954553257260056\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05065292680925793\n",
      "Average test loss: 0.0032705367103012073\n",
      "Epoch 187/300\n",
      "Average training loss: 0.050579906913969255\n",
      "Average test loss: 0.0031731612214611635\n",
      "Epoch 188/300\n",
      "Average training loss: 0.050525652309258776\n",
      "Average test loss: 0.0030393047394851845\n",
      "Epoch 189/300\n",
      "Average training loss: 0.050406283103757436\n",
      "Average test loss: 0.0031214575943433577\n",
      "Epoch 190/300\n",
      "Average training loss: 0.0504966910580794\n",
      "Average test loss: 0.0031099573137859503\n",
      "Epoch 191/300\n",
      "Average training loss: 0.050179941485325494\n",
      "Average test loss: 0.0032027480031053225\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05020899083216985\n",
      "Average test loss: 0.0032199233488904105\n",
      "Epoch 193/300\n",
      "Average training loss: 0.050350054608450996\n",
      "Average test loss: 0.0031273332019853923\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05043084204859204\n",
      "Average test loss: 0.0032205954736305607\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05060878761940532\n",
      "Average test loss: 0.0033949783601694636\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05027777580089039\n",
      "Average test loss: 0.003107982913653056\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05016400168339411\n",
      "Average test loss: 0.003103086922938625\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05002993100219303\n",
      "Average test loss: 0.003164710650841395\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05002408520711793\n",
      "Average test loss: 0.003087716694921255\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05006137906180488\n",
      "Average test loss: 0.0030907884026981063\n",
      "Epoch 201/300\n",
      "Average training loss: 0.050131557156642276\n",
      "Average test loss: 0.0031690143740011585\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0499672539167934\n",
      "Average test loss: 0.003208522107348674\n",
      "Epoch 203/300\n",
      "Average training loss: 0.050322933309608034\n",
      "Average test loss: 0.003158049001875851\n",
      "Epoch 204/300\n",
      "Average training loss: 0.050226080238819124\n",
      "Average test loss: 0.003164369007779492\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0500568227701717\n",
      "Average test loss: 0.003069487196703752\n",
      "Epoch 206/300\n",
      "Average training loss: 0.050083557936880324\n",
      "Average test loss: 0.003142838361983498\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0499872765176826\n",
      "Average test loss: 0.0031092293893711436\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04980296763446596\n",
      "Average test loss: 0.0032022559110903076\n",
      "Epoch 209/300\n",
      "Average training loss: 0.050064222968286935\n",
      "Average test loss: 0.003231045167686211\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04999028233024809\n",
      "Average test loss: 0.0031253573048031992\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04977520196305381\n",
      "Average test loss: 0.0042804912221100595\n",
      "Epoch 212/300\n",
      "Average training loss: 0.050013221704297596\n",
      "Average test loss: 0.0031237809115813838\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05070732689566083\n",
      "Average test loss: 0.0030751557474335035\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04983571508526802\n",
      "Average test loss: 0.0030932701737102536\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04996629718608327\n",
      "Average test loss: 0.0041871541535688775\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04975650385022164\n",
      "Average test loss: 0.0033050820171419116\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04981928054160542\n",
      "Average test loss: 0.0030827378073914184\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05008538833260536\n",
      "Average test loss: 0.0030697600031271578\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04962326144509845\n",
      "Average test loss: 0.0030851796031412153\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04961106896731589\n",
      "Average test loss: 0.00307428713184264\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04966445500817564\n",
      "Average test loss: 0.003183174550740255\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04956982501513428\n",
      "Average test loss: 0.0031330251095609534\n",
      "Epoch 223/300\n",
      "Average training loss: 0.049583094666401546\n",
      "Average test loss: 0.004211752535568343\n",
      "Epoch 224/300\n",
      "Average training loss: 0.049746198160780804\n",
      "Average test loss: 0.010693971331748698\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04967174529367023\n",
      "Average test loss: 0.0031700334136063853\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04956385648912854\n",
      "Average test loss: 0.008430977491868866\n",
      "Epoch 227/300\n",
      "Average training loss: 0.049673581004142764\n",
      "Average test loss: 0.0034202053675221073\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04958588514725367\n",
      "Average test loss: 0.0030709408128427136\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04938614222407341\n",
      "Average test loss: 0.0033080233188552987\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04952571726507611\n",
      "Average test loss: 0.003226858222029275\n",
      "Epoch 231/300\n",
      "Average training loss: 0.049430359068844056\n",
      "Average test loss: 0.003123845021964775\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04936715125706461\n",
      "Average test loss: 0.003118022407715519\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04946026091112031\n",
      "Average test loss: 0.0032737650326970552\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04948124317659272\n",
      "Average test loss: 0.0031968445082505544\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04931834251681964\n",
      "Average test loss: 0.0033323683153010076\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04975257177154223\n",
      "Average test loss: 0.0031263130309267177\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05155593296554353\n",
      "Average test loss: 0.0030792426301373375\n",
      "Epoch 238/300\n",
      "Average training loss: 0.049258732401662404\n",
      "Average test loss: 0.0031052030202829177\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04925662156277233\n",
      "Average test loss: 0.0031356234148972565\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04916261495484246\n",
      "Average test loss: 0.0031084353930006425\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04920320161183675\n",
      "Average test loss: 0.003685122465921773\n",
      "Epoch 242/300\n",
      "Average training loss: 0.049563216944535575\n",
      "Average test loss: 0.0031534787269516125\n",
      "Epoch 243/300\n",
      "Average training loss: 0.049352280186282266\n",
      "Average test loss: 0.0031490888992945355\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04929852631025844\n",
      "Average test loss: 0.0031237581873105632\n",
      "Epoch 245/300\n",
      "Average training loss: 0.049149160153335994\n",
      "Average test loss: 0.0031568956650379633\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04921653406487571\n",
      "Average test loss: 0.003945376346922583\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04925024684270223\n",
      "Average test loss: 0.009360877208204733\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04956859865784645\n",
      "Average test loss: 0.0031049545105132793\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04906287308865123\n",
      "Average test loss: 0.003164851263165474\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04920746476451556\n",
      "Average test loss: 0.0031400562123292024\n",
      "Epoch 251/300\n",
      "Average training loss: 0.049146050542593\n",
      "Average test loss: 0.0032815696721275648\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04923119075761901\n",
      "Average test loss: 0.003121711523168617\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04923694363236427\n",
      "Average test loss: 0.0032110143297662337\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0492325519753827\n",
      "Average test loss: 0.0033161646036638155\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04900726969043414\n",
      "Average test loss: 0.0031093106553372407\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04913101440336969\n",
      "Average test loss: 0.0031324507294015753\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04910715646545092\n",
      "Average test loss: 0.003099124648504787\n",
      "Epoch 258/300\n",
      "Average training loss: 0.048917053553793166\n",
      "Average test loss: 0.0030998450873626605\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04900620647933748\n",
      "Average test loss: 0.003208786640730169\n",
      "Epoch 260/300\n",
      "Average training loss: 0.049031742870807644\n",
      "Average test loss: 0.0034853234622213573\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04906105575958888\n",
      "Average test loss: 0.0031465798459119265\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04915559075607194\n",
      "Average test loss: 0.0031181204677042033\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04902023600207435\n",
      "Average test loss: 0.0031299571808841493\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04889448560277621\n",
      "Average test loss: 0.00347367202801009\n",
      "Epoch 265/300\n",
      "Average training loss: 0.049107350260019306\n",
      "Average test loss: 0.0032183428887898723\n",
      "Epoch 266/300\n",
      "Average training loss: 0.048844694601164924\n",
      "Average test loss: 0.0036499134653972254\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04896244490146637\n",
      "Average test loss: 0.003182957037869427\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04900452828076151\n",
      "Average test loss: 0.0033359665193905433\n",
      "Epoch 269/300\n",
      "Average training loss: 0.048820413407352235\n",
      "Average test loss: 0.0031426752644280595\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04872824550668398\n",
      "Average test loss: 0.003138952014760839\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04881346684032016\n",
      "Average test loss: 0.003141215591174033\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04898727233873473\n",
      "Average test loss: 0.003220730868478616\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04890775245096948\n",
      "Average test loss: 0.0034646640910456577\n",
      "Epoch 274/300\n",
      "Average training loss: 0.049056534475750396\n",
      "Average test loss: 0.0031735180692954194\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04879600091775258\n",
      "Average test loss: 0.0031834170350597962\n",
      "Epoch 276/300\n",
      "Average training loss: 0.048936544653442174\n",
      "Average test loss: 0.00314949456602335\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04875830482443174\n",
      "Average test loss: 0.0032159261640368236\n",
      "Epoch 278/300\n",
      "Average training loss: 0.048775597757763335\n",
      "Average test loss: 0.003138101766506831\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04871385078959995\n",
      "Average test loss: 0.0031224097369445696\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04867423381739192\n",
      "Average test loss: 0.0031830424691239993\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04870551961991522\n",
      "Average test loss: 0.004937817834317684\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04881095050772031\n",
      "Average test loss: 0.0031539701757331688\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04896142745349142\n",
      "Average test loss: 0.0032853647865768937\n",
      "Epoch 284/300\n",
      "Average training loss: 0.048789082364903556\n",
      "Average test loss: 0.003253345848578546\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04852542187107934\n",
      "Average test loss: 0.0032016522536675134\n",
      "Epoch 286/300\n",
      "Average training loss: 0.048516440424654216\n",
      "Average test loss: 0.013322896694971456\n",
      "Epoch 287/300\n",
      "Average training loss: 0.048617706722683375\n",
      "Average test loss: 0.0031583353305856387\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04874261351426443\n",
      "Average test loss: 0.003264620961621404\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04852189803123474\n",
      "Average test loss: 0.007686076885296239\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04860727489656872\n",
      "Average test loss: 0.003282582853196396\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04854667635758718\n",
      "Average test loss: 0.0032285934045082993\n",
      "Epoch 292/300\n",
      "Average training loss: 0.048728793803188536\n",
      "Average test loss: 0.0032700613405969407\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04885968520575099\n",
      "Average test loss: 0.00312357764908423\n",
      "Epoch 294/300\n",
      "Average training loss: 0.048626748969157536\n",
      "Average test loss: 0.003137853056192398\n",
      "Epoch 295/300\n",
      "Average training loss: 0.048465573284361095\n",
      "Average test loss: 0.003162316095083952\n",
      "Epoch 296/300\n",
      "Average training loss: 0.048634067647986945\n",
      "Average test loss: 0.003215962910403808\n",
      "Epoch 297/300\n",
      "Average training loss: 0.048857344534662034\n",
      "Average test loss: 0.0034871305274880595\n",
      "Epoch 298/300\n",
      "Average training loss: 0.048628183470831976\n",
      "Average test loss: 0.0031613236926496028\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04846716913912032\n",
      "Average test loss: 0.0031368610637469425\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04856920555233955\n",
      "Average test loss: 0.003152273309520549\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6032250481976402\n",
      "Average test loss: 0.00669275696327289\n",
      "Epoch 2/300\n",
      "Average training loss: 0.17275353093942006\n",
      "Average test loss: 0.005147900469601154\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12750467716323005\n",
      "Average test loss: 0.0050227861611379515\n",
      "Epoch 4/300\n",
      "Average training loss: 0.10947873969872793\n",
      "Average test loss: 0.004257127167863978\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09908750619490941\n",
      "Average test loss: 0.004495536154343022\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09234219042460123\n",
      "Average test loss: 0.004007431930551926\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08603910689883762\n",
      "Average test loss: 0.003878156476964553\n",
      "Epoch 8/300\n",
      "Average training loss: 0.08208655696445041\n",
      "Average test loss: 0.0037192228175699713\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07833286489380731\n",
      "Average test loss: 0.0035718230423000125\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07495901985300912\n",
      "Average test loss: 0.0035363891766303115\n",
      "Epoch 11/300\n",
      "Average training loss: 0.07168655558427176\n",
      "Average test loss: 0.0032488720640540124\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06909656204779943\n",
      "Average test loss: 0.004966313429590728\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06649264918102159\n",
      "Average test loss: 0.0031092229493790203\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06386516558461719\n",
      "Average test loss: 0.002973869582845105\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06137674695915646\n",
      "Average test loss: 0.0029655610614766677\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05943998025192155\n",
      "Average test loss: 0.0028272124932458005\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05750983615054025\n",
      "Average test loss: 0.0027886079719497098\n",
      "Epoch 18/300\n",
      "Average training loss: 0.055704302092393236\n",
      "Average test loss: 0.0030080576236877175\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05422792891661326\n",
      "Average test loss: 0.002624731682654884\n",
      "Epoch 20/300\n",
      "Average training loss: 0.052971330165863036\n",
      "Average test loss: 0.0024819476621018516\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05166988238361147\n",
      "Average test loss: 0.0025588624365627767\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05059722176856465\n",
      "Average test loss: 0.002483723113520278\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04965888393918673\n",
      "Average test loss: 0.0024428746570936506\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04897462471657329\n",
      "Average test loss: 0.002426618964307838\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04829115089111858\n",
      "Average test loss: 0.0023689297614619136\n",
      "Epoch 26/300\n",
      "Average training loss: 0.047645675351222355\n",
      "Average test loss: 0.002306039885307352\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04704110469255182\n",
      "Average test loss: 0.002338904473309716\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04658476917942365\n",
      "Average test loss: 0.0022590862835446994\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04586879693799549\n",
      "Average test loss: 0.0025044647777039144\n",
      "Epoch 30/300\n",
      "Average training loss: 0.045783850166532726\n",
      "Average test loss: 0.00238793811119265\n",
      "Epoch 31/300\n",
      "Average training loss: 0.045805874430470996\n",
      "Average test loss: 0.002443555226756467\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04565912720229891\n",
      "Average test loss: 0.002232668928595053\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04460496444834603\n",
      "Average test loss: 0.0022185830302639967\n",
      "Epoch 34/300\n",
      "Average training loss: 0.044407581034633846\n",
      "Average test loss: 0.0022344555442945823\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04412672900491291\n",
      "Average test loss: 0.00224495195162793\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04392418525947465\n",
      "Average test loss: 0.00217044146421055\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04367962013847298\n",
      "Average test loss: 0.002192692250220312\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04344870322611597\n",
      "Average test loss: 0.0026006824280031854\n",
      "Epoch 39/300\n",
      "Average training loss: 0.043157320297426645\n",
      "Average test loss: 0.002231517183697886\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04303210648563173\n",
      "Average test loss: 0.0022001165379252697\n",
      "Epoch 41/300\n",
      "Average training loss: 0.042863890045219\n",
      "Average test loss: 0.0021356956026413374\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04246435925033357\n",
      "Average test loss: 0.002219476718869474\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04244009704060025\n",
      "Average test loss: 0.0021972709584774245\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04222708698113759\n",
      "Average test loss: 0.002172922709542844\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04205155727929539\n",
      "Average test loss: 0.002118035373484923\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04193787086009979\n",
      "Average test loss: 0.0021079657847682637\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04193340882990095\n",
      "Average test loss: 0.002110218715440068\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04138137534923023\n",
      "Average test loss: 0.0021171963415626024\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04124197847644488\n",
      "Average test loss: 0.0022613947779561083\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04124373681677712\n",
      "Average test loss: 0.002147788078834613\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04132469705906179\n",
      "Average test loss: 0.002059621292269892\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04391723465753926\n",
      "Average test loss: 0.0021537395537727408\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04153114156921705\n",
      "Average test loss: 0.0020809848119194307\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04111046554313766\n",
      "Average test loss: 0.002046566316444013\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04080567651987076\n",
      "Average test loss: 0.0020476778948472604\n",
      "Epoch 56/300\n",
      "Average training loss: 0.040823265711466473\n",
      "Average test loss: 0.002290024904534221\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04073664749330944\n",
      "Average test loss: 0.00208675019774172\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04050478266676267\n",
      "Average test loss: 0.002033494268440538\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04040448840790325\n",
      "Average test loss: 0.0020471175199167596\n",
      "Epoch 60/300\n",
      "Average training loss: 0.040443012264039783\n",
      "Average test loss: 0.002152144506159756\n",
      "Epoch 61/300\n",
      "Average training loss: 0.040332215838962134\n",
      "Average test loss: 0.0020682552033621402\n",
      "Epoch 62/300\n",
      "Average training loss: 0.040100126574436824\n",
      "Average test loss: 0.0020384801533073187\n",
      "Epoch 63/300\n",
      "Average training loss: 0.040200705723630055\n",
      "Average test loss: 0.0020555845065456297\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03992600079708629\n",
      "Average test loss: 0.002039933701977134\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03993619488345252\n",
      "Average test loss: 0.0031919812311728794\n",
      "Epoch 66/300\n",
      "Average training loss: 0.039872597444388605\n",
      "Average test loss: 0.0021153535099907054\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0397768882115682\n",
      "Average test loss: 0.0020956125533622173\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03987735998630524\n",
      "Average test loss: 0.0020492365304380657\n",
      "Epoch 69/300\n",
      "Average training loss: 0.039927860544787516\n",
      "Average test loss: 0.002025120391924348\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04005999943282869\n",
      "Average test loss: 0.002016088912056552\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0394000454015202\n",
      "Average test loss: 0.0020106366768272385\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03936509041653739\n",
      "Average test loss: 0.0019930956084281207\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03934197592073017\n",
      "Average test loss: 0.0021562837781384588\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03938033172322644\n",
      "Average test loss: 0.0020398286148491833\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03925649474726783\n",
      "Average test loss: 0.0019901941435204614\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03921874109572834\n",
      "Average test loss: 0.00282847780879173\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03911046449012227\n",
      "Average test loss: 0.002439298607615961\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039027824915117686\n",
      "Average test loss: 0.002026623406033549\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03910375356674194\n",
      "Average test loss: 0.002188219776791003\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03907687635885344\n",
      "Average test loss: 0.002050547070387337\n",
      "Epoch 81/300\n",
      "Average training loss: 0.038796483475301\n",
      "Average test loss: 0.001997357265195913\n",
      "Epoch 82/300\n",
      "Average training loss: 0.038894388750195505\n",
      "Average test loss: 0.0020404113443154427\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03909853437542915\n",
      "Average test loss: 0.0022455867876609165\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03879099803169568\n",
      "Average test loss: 0.0019909046362671587\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0389995979120334\n",
      "Average test loss: 0.0020722595364269284\n",
      "Epoch 86/300\n",
      "Average training loss: 0.038905392550759844\n",
      "Average test loss: 0.0020026159848396978\n",
      "Epoch 87/300\n",
      "Average training loss: 0.038667995567123095\n",
      "Average test loss: 0.001999125062332799\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03855259549617767\n",
      "Average test loss: 0.002040773297970494\n",
      "Epoch 89/300\n",
      "Average training loss: 0.038372009575366975\n",
      "Average test loss: 0.0020101157885251773\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03857879828578896\n",
      "Average test loss: 0.002033402727606396\n",
      "Epoch 91/300\n",
      "Average training loss: 0.038371184837487006\n",
      "Average test loss: 0.001989659608859155\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03836081689430608\n",
      "Average test loss: 0.0019844883198125497\n",
      "Epoch 93/300\n",
      "Average training loss: 0.038595275670289995\n",
      "Average test loss: 0.002145962596870959\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03843686404824257\n",
      "Average test loss: 0.0020071872470693456\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03823603267802132\n",
      "Average test loss: 0.00203033454136716\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03817655234866672\n",
      "Average test loss: 0.0020093210705866417\n",
      "Epoch 97/300\n",
      "Average training loss: 0.038160475658045874\n",
      "Average test loss: 0.0019787955210130246\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03813812051879035\n",
      "Average test loss: 0.0020994211965137057\n",
      "Epoch 99/300\n",
      "Average training loss: 0.038099812497695285\n",
      "Average test loss: 0.0019775523922095696\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0381042222281297\n",
      "Average test loss: 0.0019953679150591292\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03800362186630567\n",
      "Average test loss: 0.0020021317430461445\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03802363627486759\n",
      "Average test loss: 0.0020652119440750945\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03848578984207577\n",
      "Average test loss: 0.0020235630300723846\n",
      "Epoch 104/300\n",
      "Average training loss: 0.038051465680201846\n",
      "Average test loss: 0.0020142397201723524\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03791110515594483\n",
      "Average test loss: 0.0020104244146496057\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03803556675712268\n",
      "Average test loss: 0.002033720190015932\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03781873205304146\n",
      "Average test loss: 0.00213744679848767\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03794520943363507\n",
      "Average test loss: 0.009792886532429192\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03775894612736172\n",
      "Average test loss: 0.0019777522846642468\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0376703073018127\n",
      "Average test loss: 0.0020081020125912296\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03766739255189896\n",
      "Average test loss: 0.0020222737581158676\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0378581900662846\n",
      "Average test loss: 0.0019794888108347853\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03764624743660291\n",
      "Average test loss: 0.002115491255496939\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03753520535429319\n",
      "Average test loss: 0.00210050002961523\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03746816612614526\n",
      "Average test loss: 0.0020120821487572458\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03763740943041113\n",
      "Average test loss: 0.002004249229406317\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03743074000709587\n",
      "Average test loss: 0.0020184863698151376\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0374296155207687\n",
      "Average test loss: 0.0020048739692817134\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03747672860158814\n",
      "Average test loss: 0.002011631412431598\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03771132715211974\n",
      "Average test loss: 0.002050488575258189\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03759256463580661\n",
      "Average test loss: 0.002481084947784742\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03734638376699553\n",
      "Average test loss: 0.002054413476337989\n",
      "Epoch 123/300\n",
      "Average training loss: 0.037420100692245696\n",
      "Average test loss: 0.001987235526036885\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03739761162135336\n",
      "Average test loss: 0.31653239311112297\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03725462108519342\n",
      "Average test loss: 0.0019944787215855388\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03721199490626653\n",
      "Average test loss: 0.0028298182336406576\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03720572044286463\n",
      "Average test loss: 0.0019832785266141097\n",
      "Epoch 128/300\n",
      "Average training loss: 0.037085278491179145\n",
      "Average test loss: 0.0020044786834882366\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03736731460193793\n",
      "Average test loss: 0.002134251432493329\n",
      "Epoch 130/300\n",
      "Average training loss: 0.037230063700013694\n",
      "Average test loss: 0.0026587836471282772\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03707288193040424\n",
      "Average test loss: 0.00209860239074462\n",
      "Epoch 132/300\n",
      "Average training loss: 0.037113713643617104\n",
      "Average test loss: 0.002198092508957618\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03713294653097789\n",
      "Average test loss: 0.04752096831964122\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03726917667852508\n",
      "Average test loss: 0.0019905443395384485\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03697278356552124\n",
      "Average test loss: 0.005464548630019029\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03697130050924089\n",
      "Average test loss: 0.001978317052539852\n",
      "Epoch 137/300\n",
      "Average training loss: 0.037013291670216455\n",
      "Average test loss: 0.0021284157765201397\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03693896057870653\n",
      "Average test loss: 0.0019916629113463894\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03703629501660665\n",
      "Average test loss: 0.002046777969847123\n",
      "Epoch 140/300\n",
      "Average training loss: 0.036915405015150705\n",
      "Average test loss: 0.00204847948708468\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0368799193335904\n",
      "Average test loss: 0.002012127590883109\n",
      "Epoch 142/300\n",
      "Average training loss: 0.037282684829499986\n",
      "Average test loss: 0.0019652760435516638\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03674970454970996\n",
      "Average test loss: 0.001987720525202652\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03672497633430693\n",
      "Average test loss: 0.001984990592321588\n",
      "Epoch 145/300\n",
      "Average training loss: 0.036764061245653366\n",
      "Average test loss: 0.001998752568124069\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03700150716304779\n",
      "Average test loss: 0.002060833384179407\n",
      "Epoch 147/300\n",
      "Average training loss: 0.036679210566812094\n",
      "Average test loss: 0.0020101209721631475\n",
      "Epoch 148/300\n",
      "Average training loss: 0.036859665082560646\n",
      "Average test loss: 0.0019836531670557127\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03690613000922733\n",
      "Average test loss: 0.001988832280660669\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03657259363598294\n",
      "Average test loss: 0.002002384472431408\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03658789501918687\n",
      "Average test loss: 0.0021391334591640367\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03680588374204106\n",
      "Average test loss: 0.001995089908440908\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03664607901705636\n",
      "Average test loss: 0.0019990800650169453\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03650025827354855\n",
      "Average test loss: 0.00559190939915263\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03665345891316732\n",
      "Average test loss: 0.006737128973835044\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03666022591458427\n",
      "Average test loss: 0.002038114197552204\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03642916155854861\n",
      "Average test loss: 0.002061389461159706\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03643525528411071\n",
      "Average test loss: 0.0020137934068010913\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0365092364963558\n",
      "Average test loss: 0.00202663097385731\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03658906517095036\n",
      "Average test loss: 0.001993384476647609\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03654622574150562\n",
      "Average test loss: 0.001994279817574554\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03638024343715774\n",
      "Average test loss: 0.0020475492640915846\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03655132766564687\n",
      "Average test loss: 0.002112726921836535\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03646396252512932\n",
      "Average test loss: 0.002010376143372721\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03627477157447073\n",
      "Average test loss: 0.0020185480736609964\n",
      "Epoch 166/300\n",
      "Average training loss: 0.036357256101237405\n",
      "Average test loss: 0.002002336793372201\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03643773963385158\n",
      "Average test loss: 0.001998241703957319\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03653428075710932\n",
      "Average test loss: 0.0019724421610848772\n",
      "Epoch 169/300\n",
      "Average training loss: 0.036418809422188336\n",
      "Average test loss: 0.0020251657139095996\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03635011879603068\n",
      "Average test loss: 0.0022394876229680245\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0362451663547092\n",
      "Average test loss: 0.002030678909892837\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03633064675662253\n",
      "Average test loss: 0.0020318119682164655\n",
      "Epoch 173/300\n",
      "Average training loss: 0.036260376817650264\n",
      "Average test loss: 0.0020347487618111904\n",
      "Epoch 174/300\n",
      "Average training loss: 0.036354296659429866\n",
      "Average test loss: 0.0019912711664413414\n",
      "Epoch 175/300\n",
      "Average training loss: 0.036163383344809216\n",
      "Average test loss: 0.0020356994637598593\n",
      "Epoch 176/300\n",
      "Average training loss: 0.036226373218827775\n",
      "Average test loss: 0.0020632624125315083\n",
      "Epoch 177/300\n",
      "Average training loss: 0.036233187975154985\n",
      "Average test loss: 0.0021714837590439454\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03612717034750514\n",
      "Average test loss: 0.0020209313035011293\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0362338356408808\n",
      "Average test loss: 0.0022493029773856203\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03608904174632496\n",
      "Average test loss: 0.00215227253196968\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03603671032687028\n",
      "Average test loss: 0.0020908146924856637\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03619679163561927\n",
      "Average test loss: 0.0020200351675351462\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0364882220129172\n",
      "Average test loss: 0.0020233181002032425\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03609446024894714\n",
      "Average test loss: 0.0019866291044486895\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0360842002067301\n",
      "Average test loss: 0.002038281901429097\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03603300014800496\n",
      "Average test loss: 0.002001036865429746\n",
      "Epoch 187/300\n",
      "Average training loss: 0.036128435421321124\n",
      "Average test loss: 0.002312675267044041\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03616109208928214\n",
      "Average test loss: 0.0019914716826751827\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03590666661659876\n",
      "Average test loss: 0.0020174500768383344\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03594692052072949\n",
      "Average test loss: 0.0021169922289748988\n",
      "Epoch 191/300\n",
      "Average training loss: 0.035992240183883244\n",
      "Average test loss: 0.002184280261914763\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03587460830973254\n",
      "Average test loss: 0.002062737134285271\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0358657376749648\n",
      "Average test loss: 0.0029219876589874428\n",
      "Epoch 194/300\n",
      "Average training loss: 0.035918744603792824\n",
      "Average test loss: 0.0020913321673870088\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03582491313417752\n",
      "Average test loss: 0.002013056276159154\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03591219744086266\n",
      "Average test loss: 0.002033552721246249\n",
      "Epoch 197/300\n",
      "Average training loss: 0.035887288702858816\n",
      "Average test loss: 0.0021444528349157835\n",
      "Epoch 198/300\n",
      "Average training loss: 0.036030465533336005\n",
      "Average test loss: 0.0020457593417829937\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03593019011616707\n",
      "Average test loss: 0.004692212084722188\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03578610177503692\n",
      "Average test loss: 0.0020837604289667476\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03581883079475827\n",
      "Average test loss: 0.002041839280165732\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03656343907780117\n",
      "Average test loss: 0.002501637847162783\n",
      "Epoch 203/300\n",
      "Average training loss: 0.035714270941085285\n",
      "Average test loss: 0.0020509500125836994\n",
      "Epoch 204/300\n",
      "Average training loss: 0.035640426324473486\n",
      "Average test loss: 0.002048055060518285\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03564655505783028\n",
      "Average test loss: 0.0020474263580722942\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03567480902208222\n",
      "Average test loss: 0.002015421289536688\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03572971341345045\n",
      "Average test loss: 0.002036120258478655\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03584052010708385\n",
      "Average test loss: 0.002389801635303431\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03584885959161652\n",
      "Average test loss: 0.001998156395430366\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03561539657910665\n",
      "Average test loss: 0.0021633093675805463\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0356072896172603\n",
      "Average test loss: 0.002102399157670637\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03574931065572633\n",
      "Average test loss: 0.0020866385652787154\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03560988576047951\n",
      "Average test loss: 0.0022829845945040385\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03563635361029042\n",
      "Average test loss: 0.0024881988236059747\n",
      "Epoch 215/300\n",
      "Average training loss: 0.035552528076701694\n",
      "Average test loss: 0.0020231981474078363\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03579075681169828\n",
      "Average test loss: 0.002050755464368396\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03562173700663779\n",
      "Average test loss: 0.0020144252746055525\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03554478954937723\n",
      "Average test loss: 0.0021089634417245785\n",
      "Epoch 219/300\n",
      "Average training loss: 0.035525270611047745\n",
      "Average test loss: 0.0021326885341356196\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03553595021035936\n",
      "Average test loss: 0.0020587592845161757\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03562342922720644\n",
      "Average test loss: 0.0021246866199912296\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03585182774894767\n",
      "Average test loss: 0.00201509266698526\n",
      "Epoch 223/300\n",
      "Average training loss: 0.035657895581589806\n",
      "Average test loss: 0.0020765661266114975\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03556003227498796\n",
      "Average test loss: 0.0022683135302116473\n",
      "Epoch 225/300\n",
      "Average training loss: 0.035427665435605576\n",
      "Average test loss: 0.018090637675589986\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03562901631991069\n",
      "Average test loss: 0.0020520613077614045\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03546526402731737\n",
      "Average test loss: 0.002051412910223007\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0353680187397533\n",
      "Average test loss: 0.0020353900297648375\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03540614691376686\n",
      "Average test loss: 0.00205409937331246\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03536411603126261\n",
      "Average test loss: 0.002272804380704959\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03547861310508516\n",
      "Average test loss: 0.0020063116872269244\n",
      "Epoch 232/300\n",
      "Average training loss: 0.035967328899436525\n",
      "Average test loss: 0.002119190612600909\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03537268579337332\n",
      "Average test loss: 0.002130786745188137\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03541581639316347\n",
      "Average test loss: 0.0020326933800760244\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03528846803473102\n",
      "Average test loss: 0.002062201286252174\n",
      "Epoch 236/300\n",
      "Average training loss: 0.035372254638208284\n",
      "Average test loss: 0.002091057697414524\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03529469428128666\n",
      "Average test loss: 0.0020506873175294865\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03529440956976679\n",
      "Average test loss: 0.002096613487228751\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035369265008303855\n",
      "Average test loss: 0.002241411821295818\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03551578393578529\n",
      "Average test loss: 0.00213048369396064\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03540349279840787\n",
      "Average test loss: 0.0023215404199436306\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03517224259343412\n",
      "Average test loss: 0.002071356785690619\n",
      "Epoch 243/300\n",
      "Average training loss: 0.035231243928273516\n",
      "Average test loss: 0.0020522299228856963\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03554811632964346\n",
      "Average test loss: 0.0021183375285731424\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03518182294898563\n",
      "Average test loss: 0.002184348373984297\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03531387173136075\n",
      "Average test loss: 0.0023073964131375153\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03525212741560406\n",
      "Average test loss: 0.002275647554960516\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03522340447703997\n",
      "Average test loss: 0.0020084958923980594\n",
      "Epoch 249/300\n",
      "Average training loss: 0.035152252616153824\n",
      "Average test loss: 0.002050291761755943\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03519549150268237\n",
      "Average test loss: 0.0021069145043277077\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03524249484141668\n",
      "Average test loss: 0.002345870385153426\n",
      "Epoch 252/300\n",
      "Average training loss: 0.035202453369895616\n",
      "Average test loss: 0.00431189468378822\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03530649842487441\n",
      "Average test loss: 0.0021512959166947337\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03541371057430903\n",
      "Average test loss: 0.002036534313422938\n",
      "Epoch 255/300\n",
      "Average training loss: 0.035068413810597526\n",
      "Average test loss: 0.0020788895798226196\n",
      "Epoch 256/300\n",
      "Average training loss: 0.035244002964761524\n",
      "Average test loss: 0.0022482873557342423\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03496830049984985\n",
      "Average test loss: 0.002046943456865847\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03512676977448993\n",
      "Average test loss: 0.002021035667922762\n",
      "Epoch 259/300\n",
      "Average training loss: 0.035125545710325244\n",
      "Average test loss: 0.002050628189721869\n",
      "Epoch 260/300\n",
      "Average training loss: 0.035083072188827724\n",
      "Average test loss: 0.0021583856134158043\n",
      "Epoch 261/300\n",
      "Average training loss: 0.035062460048331155\n",
      "Average test loss: 0.002044290481135249\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03524494766857889\n",
      "Average test loss: 0.002045632125602828\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03513568054305183\n",
      "Average test loss: 0.0020642704566319783\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0350944636993938\n",
      "Average test loss: 0.0021118961788920893\n",
      "Epoch 265/300\n",
      "Average training loss: 0.035087199565437106\n",
      "Average test loss: 0.0024915553271356557\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03517032707399792\n",
      "Average test loss: 0.0020259724052415955\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03498015164335569\n",
      "Average test loss: 0.002191102523356676\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03516514383090867\n",
      "Average test loss: 0.0020238811255743104\n",
      "Epoch 269/300\n",
      "Average training loss: 0.035074386691053706\n",
      "Average test loss: 0.0021059001406861675\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03497329294350412\n",
      "Average test loss: 0.0020261081211889785\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03503445585237609\n",
      "Average test loss: 0.0032331154307143556\n",
      "Epoch 272/300\n",
      "Average training loss: 0.034933595488468805\n",
      "Average test loss: 0.0020417705175156393\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03492012134028805\n",
      "Average test loss: 0.0021030436102300884\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03510306414299541\n",
      "Average test loss: 0.002076984019536111\n",
      "Epoch 275/300\n",
      "Average training loss: 0.035075590117110145\n",
      "Average test loss: 0.0020600233763042423\n",
      "Epoch 276/300\n",
      "Average training loss: 0.034864625245332716\n",
      "Average test loss: 0.07914605757914897\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0349905626906289\n",
      "Average test loss: 0.002053821987162034\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03502327099442482\n",
      "Average test loss: 0.005612455441306035\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03495189084609349\n",
      "Average test loss: 0.0021615871373150083\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03504292183783319\n",
      "Average test loss: 0.002194493825236956\n",
      "Epoch 281/300\n",
      "Average training loss: 0.034878771225611366\n",
      "Average test loss: 0.0020306841058449613\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03480086504750782\n",
      "Average test loss: 0.0020335176523981822\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03486539313528273\n",
      "Average test loss: 0.004542683530598879\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03505607172018952\n",
      "Average test loss: 0.0023054689131677152\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03476710817880101\n",
      "Average test loss: 0.0020475995286057392\n",
      "Epoch 286/300\n",
      "Average training loss: 0.034878085391389\n",
      "Average test loss: 0.0020979779880079957\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03487150204512808\n",
      "Average test loss: 0.0020740163891265788\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03485485147767597\n",
      "Average test loss: 0.0020543535604245133\n",
      "Epoch 289/300\n",
      "Average training loss: 0.034985629475779004\n",
      "Average test loss: 0.0021367669484267634\n",
      "Epoch 290/300\n",
      "Average training loss: 0.035162805845340095\n",
      "Average test loss: 0.0020497878367702164\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03470311292509238\n",
      "Average test loss: 0.0021347825954564745\n",
      "Epoch 292/300\n",
      "Average training loss: 0.034747682723734116\n",
      "Average test loss: 0.002079493432202273\n",
      "Epoch 293/300\n",
      "Average training loss: 0.034781020580066574\n",
      "Average test loss: 0.0021149485922522014\n",
      "Epoch 294/300\n",
      "Average training loss: 0.034803544425302084\n",
      "Average test loss: 0.0021619971856060957\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03490803114242024\n",
      "Average test loss: 0.0020452452078461645\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03576414343714714\n",
      "Average test loss: 0.0020375053271030385\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03477614837553766\n",
      "Average test loss: 0.0021173958829086687\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03469012705485026\n",
      "Average test loss: 0.002374688507575128\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03460186117225223\n",
      "Average test loss: 0.002076989493643244\n",
      "Epoch 300/300\n",
      "Average training loss: 0.034708306060896976\n",
      "Average test loss: 0.0021252862142605916\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5267628313965267\n",
      "Average test loss: 0.005783518036620484\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16869917384783428\n",
      "Average test loss: 0.00474990557713641\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11893901562028461\n",
      "Average test loss: 0.003833876241826349\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09939754413896137\n",
      "Average test loss: 0.004211513879812426\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08824976845582326\n",
      "Average test loss: 0.0034261857519547146\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08097643905878067\n",
      "Average test loss: 0.0033051885072555806\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07553752700487773\n",
      "Average test loss: 0.0030055180014007623\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07161382475826475\n",
      "Average test loss: 0.0031106386967003343\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06849938553240564\n",
      "Average test loss: 0.0033205893809596697\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06525604436132643\n",
      "Average test loss: 0.002675761115219858\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06267531589998139\n",
      "Average test loss: 0.003192157730046246\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06121005669236183\n",
      "Average test loss: 0.0027759932010538047\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05885325828856892\n",
      "Average test loss: 0.002561061298267709\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05677309099170897\n",
      "Average test loss: 0.002786952043366101\n",
      "Epoch 15/300\n",
      "Average training loss: 0.054671010547214086\n",
      "Average test loss: 0.0026476043700757955\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05291362925039397\n",
      "Average test loss: 0.002966707575134933\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05156025785870022\n",
      "Average test loss: 0.003091425447621279\n",
      "Epoch 18/300\n",
      "Average training loss: 0.050019261515802806\n",
      "Average test loss: 0.002388129602910744\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0476828299164772\n",
      "Average test loss: 0.002183936160368224\n",
      "Epoch 20/300\n",
      "Average training loss: 0.046718705371022226\n",
      "Average test loss: 0.002102382074731092\n",
      "Epoch 21/300\n",
      "Average training loss: 0.045264934460322065\n",
      "Average test loss: 0.0024840874115212097\n",
      "Epoch 22/300\n",
      "Average training loss: 0.044209948708613715\n",
      "Average test loss: 0.0020843727646602524\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04331954084171189\n",
      "Average test loss: 0.002208271968799333\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04223451962735918\n",
      "Average test loss: 0.0019388566913290156\n",
      "Epoch 25/300\n",
      "Average training loss: 0.041243561529450946\n",
      "Average test loss: 0.0018424904768665632\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04040740508337816\n",
      "Average test loss: 0.0017558162046803368\n",
      "Epoch 27/300\n",
      "Average training loss: 0.039654664629035524\n",
      "Average test loss: 0.001860242059868243\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03910066812899378\n",
      "Average test loss: 0.0017954478421145015\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03836078337000476\n",
      "Average test loss: 0.001909093639658143\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03775095361140039\n",
      "Average test loss: 0.0017662524070797695\n",
      "Epoch 31/300\n",
      "Average training loss: 0.037363635058204336\n",
      "Average test loss: 0.0017235659655804435\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03674913556873798\n",
      "Average test loss: 0.0016351696950280003\n",
      "Epoch 33/300\n",
      "Average training loss: 0.036206000569793916\n",
      "Average test loss: 0.0016596271238393254\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03579300390680631\n",
      "Average test loss: 0.0015922606787644328\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03532352791229884\n",
      "Average test loss: 0.0015893875492943657\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03494689545035362\n",
      "Average test loss: 0.0016264031890572772\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03462856307294634\n",
      "Average test loss: 0.0015464962888509036\n",
      "Epoch 38/300\n",
      "Average training loss: 0.034369603239827684\n",
      "Average test loss: 0.0015397581826481555\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0340230065882206\n",
      "Average test loss: 0.0015842564228094286\n",
      "Epoch 40/300\n",
      "Average training loss: 0.033747838318347934\n",
      "Average test loss: 0.001712310270820227\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03353492072555754\n",
      "Average test loss: 0.001567404145737075\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03336597743299272\n",
      "Average test loss: 0.0014939956404268742\n",
      "Epoch 43/300\n",
      "Average training loss: 0.033112900669376055\n",
      "Average test loss: 0.0015951717473670012\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03285297409362263\n",
      "Average test loss: 0.0015041203959327606\n",
      "Epoch 45/300\n",
      "Average training loss: 0.032800217052300774\n",
      "Average test loss: 0.0017232244312763215\n",
      "Epoch 46/300\n",
      "Average training loss: 0.032561325642797685\n",
      "Average test loss: 0.001488732371582753\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03250604771243201\n",
      "Average test loss: 0.002327112621938189\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03226489147875044\n",
      "Average test loss: 0.0014696916038584378\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03210019508335325\n",
      "Average test loss: 0.0014943257827932635\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03190830878416697\n",
      "Average test loss: 0.0014509116198039716\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03185754077302085\n",
      "Average test loss: 0.001470146359047956\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03172155857748456\n",
      "Average test loss: 0.0015108688184991479\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0316500680959887\n",
      "Average test loss: 0.0014679426373913884\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03187872871756554\n",
      "Average test loss: 0.0014410987212840054\n",
      "Epoch 55/300\n",
      "Average training loss: 0.031768244119154083\n",
      "Average test loss: 0.0015673371085690127\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03126293735868401\n",
      "Average test loss: 0.0014389916563199627\n",
      "Epoch 57/300\n",
      "Average training loss: 0.031195767889420193\n",
      "Average test loss: 0.0014126475445098347\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03128325957887702\n",
      "Average test loss: 0.001437338606764873\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03102027071515719\n",
      "Average test loss: 0.0015601516218028135\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030959836608833738\n",
      "Average test loss: 0.0017670778818428517\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030956699735588497\n",
      "Average test loss: 0.0015572860936323802\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03084508553478453\n",
      "Average test loss: 0.0015359922758200102\n",
      "Epoch 63/300\n",
      "Average training loss: 0.030895129380954636\n",
      "Average test loss: 0.0016248232865085205\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03074533515340752\n",
      "Average test loss: 0.0014795937470884787\n",
      "Epoch 65/300\n",
      "Average training loss: 0.030656604647636414\n",
      "Average test loss: 0.0014230685520710217\n",
      "Epoch 66/300\n",
      "Average training loss: 0.030626588268412483\n",
      "Average test loss: 0.0014316524994663067\n",
      "Epoch 67/300\n",
      "Average training loss: 0.030505220649970903\n",
      "Average test loss: 0.001414278709122704\n",
      "Epoch 68/300\n",
      "Average training loss: 0.030449392891592448\n",
      "Average test loss: 0.0013896968395759662\n",
      "Epoch 69/300\n",
      "Average training loss: 0.030346804668505988\n",
      "Average test loss: 0.0018264673792032732\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03040426837735706\n",
      "Average test loss: 0.0013940082096701694\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0303616169244051\n",
      "Average test loss: 0.0014075914085325267\n",
      "Epoch 72/300\n",
      "Average training loss: 0.030290404561493133\n",
      "Average test loss: 0.001680296456027362\n",
      "Epoch 73/300\n",
      "Average training loss: 0.030307591936654516\n",
      "Average test loss: 0.0015461596444042193\n",
      "Epoch 74/300\n",
      "Average training loss: 0.030034686255786154\n",
      "Average test loss: 0.0013726768113879694\n",
      "Epoch 75/300\n",
      "Average training loss: 0.030043705390559304\n",
      "Average test loss: 0.0014166590684714416\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03011949060195022\n",
      "Average test loss: 0.0014022654995529188\n",
      "Epoch 77/300\n",
      "Average training loss: 0.030465359446075226\n",
      "Average test loss: 0.0014128728983923793\n",
      "Epoch 78/300\n",
      "Average training loss: 0.029931428391072486\n",
      "Average test loss: 0.0013891648445278407\n",
      "Epoch 79/300\n",
      "Average training loss: 0.029810493144724105\n",
      "Average test loss: 0.0014195721394692858\n",
      "Epoch 80/300\n",
      "Average training loss: 0.029914596165219944\n",
      "Average test loss: 0.0015363397719338537\n",
      "Epoch 81/300\n",
      "Average training loss: 0.029813420847058295\n",
      "Average test loss: 0.0014142573734538422\n",
      "Epoch 82/300\n",
      "Average training loss: 0.029905127169357407\n",
      "Average test loss: 0.0014184305394689242\n",
      "Epoch 83/300\n",
      "Average training loss: 0.029772990119126107\n",
      "Average test loss: 0.0019812054675486354\n",
      "Epoch 84/300\n",
      "Average training loss: 0.030089973939789665\n",
      "Average test loss: 0.0013941839572249187\n",
      "Epoch 85/300\n",
      "Average training loss: 0.030012020107772614\n",
      "Average test loss: 0.001596559358967675\n",
      "Epoch 86/300\n",
      "Average training loss: 0.029552915081381796\n",
      "Average test loss: 0.0013771418601067529\n",
      "Epoch 87/300\n",
      "Average training loss: 0.029527103521757657\n",
      "Average test loss: 0.0014152886553977927\n",
      "Epoch 88/300\n",
      "Average training loss: 0.029556230303313998\n",
      "Average test loss: 0.0013910463069462115\n",
      "Epoch 89/300\n",
      "Average training loss: 0.029484464385443263\n",
      "Average test loss: 0.0018875042241480616\n",
      "Epoch 90/300\n",
      "Average training loss: 0.030372698303725985\n",
      "Average test loss: 0.0013699367959052323\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02936962717274825\n",
      "Average test loss: 0.001396349518456393\n",
      "Epoch 92/300\n",
      "Average training loss: 0.029338997395502196\n",
      "Average test loss: 0.0013708298217194777\n",
      "Epoch 93/300\n",
      "Average training loss: 0.029315373447206285\n",
      "Average test loss: 0.0013679663149846924\n",
      "Epoch 94/300\n",
      "Average training loss: 0.029356692920128505\n",
      "Average test loss: 0.0015576615195928348\n",
      "Epoch 95/300\n",
      "Average training loss: 0.029342989297376738\n",
      "Average test loss: 0.0014330170545727014\n",
      "Epoch 96/300\n",
      "Average training loss: 0.029200999319553375\n",
      "Average test loss: 0.0013808227385290796\n",
      "Epoch 97/300\n",
      "Average training loss: 0.029240140517552693\n",
      "Average test loss: 0.0019953433490461775\n",
      "Epoch 98/300\n",
      "Average training loss: 0.029234601732757357\n",
      "Average test loss: 0.0013646859171696835\n",
      "Epoch 99/300\n",
      "Average training loss: 0.029373116569386588\n",
      "Average test loss: 0.0013673510526617369\n",
      "Epoch 100/300\n",
      "Average training loss: 0.029540532047549882\n",
      "Average test loss: 0.0013967971338166131\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02918418157266246\n",
      "Average test loss: 0.00136543537179629\n",
      "Epoch 102/300\n",
      "Average training loss: 0.029267330947849484\n",
      "Average test loss: 0.001366503067418105\n",
      "Epoch 103/300\n",
      "Average training loss: 0.029002495711048445\n",
      "Average test loss: 0.0014393524246083366\n",
      "Epoch 104/300\n",
      "Average training loss: 0.028982313245534897\n",
      "Average test loss: 0.00137374461752673\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02896591847969426\n",
      "Average test loss: 0.001911694968947106\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02894407372176647\n",
      "Average test loss: 0.0013512779192792045\n",
      "Epoch 107/300\n",
      "Average training loss: 0.029173135042190552\n",
      "Average test loss: 0.0018423651806596252\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02919766029715538\n",
      "Average test loss: 0.003490543966078096\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029178997851080364\n",
      "Average test loss: 0.0013576344101586275\n",
      "Epoch 110/300\n",
      "Average training loss: 0.028784169965320162\n",
      "Average test loss: 0.001405051613950895\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02888554003669156\n",
      "Average test loss: 0.0014218284725728962\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02881235725680987\n",
      "Average test loss: 0.0014301294162869453\n",
      "Epoch 113/300\n",
      "Average training loss: 0.028765119137035475\n",
      "Average test loss: 0.001809307507239282\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028785486698150634\n",
      "Average test loss: 0.001366121067148116\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02878213330109914\n",
      "Average test loss: 0.0013665641562806234\n",
      "Epoch 116/300\n",
      "Average training loss: 0.028851660544673603\n",
      "Average test loss: 0.0013999501175971495\n",
      "Epoch 117/300\n",
      "Average training loss: 0.028661125025815432\n",
      "Average test loss: 0.001371816970511443\n",
      "Epoch 118/300\n",
      "Average training loss: 0.028696937024593354\n",
      "Average test loss: 0.001427567113811771\n",
      "Epoch 119/300\n",
      "Average training loss: 0.028837150120072895\n",
      "Average test loss: 0.001454510354850855\n",
      "Epoch 120/300\n",
      "Average training loss: 0.028730242672893737\n",
      "Average test loss: 0.0013971880028645198\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02857772402630912\n",
      "Average test loss: 0.0013858607317217522\n",
      "Epoch 122/300\n",
      "Average training loss: 0.028565637722611427\n",
      "Average test loss: 0.0013561506030253239\n",
      "Epoch 123/300\n",
      "Average training loss: 0.028609187313252026\n",
      "Average test loss: 0.0014979509443251624\n",
      "Epoch 124/300\n",
      "Average training loss: 0.028493636652827264\n",
      "Average test loss: 0.0013690347985054056\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028694341588351463\n",
      "Average test loss: 0.0013599399768023028\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02863229910367065\n",
      "Average test loss: 0.0017104001789250308\n",
      "Epoch 127/300\n",
      "Average training loss: 0.028649110808968543\n",
      "Average test loss: 0.001365802598806719\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02845981142255995\n",
      "Average test loss: 0.010884384607068366\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02843954016930527\n",
      "Average test loss: 0.0015136524544407924\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02836492156982422\n",
      "Average test loss: 0.0013760677344269222\n",
      "Epoch 131/300\n",
      "Average training loss: 0.028446521474255457\n",
      "Average test loss: 0.0013755450336676504\n",
      "Epoch 132/300\n",
      "Average training loss: 0.028368581763572164\n",
      "Average test loss: 0.0013705152577927543\n",
      "Epoch 133/300\n",
      "Average training loss: 0.028444577475388845\n",
      "Average test loss: 0.0013830281372906434\n",
      "Epoch 134/300\n",
      "Average training loss: 0.028368299696180557\n",
      "Average test loss: 0.001428186351278176\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028322666459613375\n",
      "Average test loss: 0.0013935809305144682\n",
      "Epoch 136/300\n",
      "Average training loss: 0.028567210516995855\n",
      "Average test loss: 0.0014147460949089793\n",
      "Epoch 137/300\n",
      "Average training loss: 0.028394559654924604\n",
      "Average test loss: 0.00144328862728758\n",
      "Epoch 138/300\n",
      "Average training loss: 0.028279764650596513\n",
      "Average test loss: 0.0013794319888369906\n",
      "Epoch 139/300\n",
      "Average training loss: 0.028227398186922072\n",
      "Average test loss: 0.0014799932149859767\n",
      "Epoch 140/300\n",
      "Average training loss: 0.028744242760870193\n",
      "Average test loss: 0.001505716424021456\n",
      "Epoch 141/300\n",
      "Average training loss: 0.028204254466626378\n",
      "Average test loss: 0.001393796794426938\n",
      "Epoch 142/300\n",
      "Average training loss: 0.028229074055949845\n",
      "Average test loss: 0.0013682381933968928\n",
      "Epoch 143/300\n",
      "Average training loss: 0.028215192740162213\n",
      "Average test loss: 0.0022599376190660728\n",
      "Epoch 144/300\n",
      "Average training loss: 0.028155369372003608\n",
      "Average test loss: 0.0013842810260959798\n",
      "Epoch 145/300\n",
      "Average training loss: 0.028206286617451243\n",
      "Average test loss: 0.0014494746061455871\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02834559078183439\n",
      "Average test loss: 0.0013945573246520427\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02822061647143629\n",
      "Average test loss: 0.0013718884239594142\n",
      "Epoch 148/300\n",
      "Average training loss: 0.028165462952521113\n",
      "Average test loss: 0.0013997056250356966\n",
      "Epoch 149/300\n",
      "Average training loss: 0.028053655640946494\n",
      "Average test loss: 0.005201049246721798\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02818062719371584\n",
      "Average test loss: 0.03547317670782407\n",
      "Epoch 151/300\n",
      "Average training loss: 0.028086444343129794\n",
      "Average test loss: 0.006953108058000605\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02802444733182589\n",
      "Average test loss: 0.0013608420432234805\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02855895456009441\n",
      "Average test loss: 0.0013926994567736984\n",
      "Epoch 154/300\n",
      "Average training loss: 0.028090801848305598\n",
      "Average test loss: 0.0013947769761499431\n",
      "Epoch 155/300\n",
      "Average training loss: 0.028020540045367345\n",
      "Average test loss: 0.0013708087189329996\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0279901469366418\n",
      "Average test loss: 0.0013904277578824097\n",
      "Epoch 157/300\n",
      "Average training loss: 0.027974106036954455\n",
      "Average test loss: 0.001501143135647807\n",
      "Epoch 158/300\n",
      "Average training loss: 0.028048097260296345\n",
      "Average test loss: 0.0013966464328890045\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02785760553015603\n",
      "Average test loss: 0.0013901809485008319\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02790854399568505\n",
      "Average test loss: 0.0014027464680580629\n",
      "Epoch 161/300\n",
      "Average training loss: 0.028050948313540883\n",
      "Average test loss: 0.0013712161235097382\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02788868428601159\n",
      "Average test loss: 0.0013832707921456959\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027875328918298085\n",
      "Average test loss: 0.0014215086317724652\n",
      "Epoch 164/300\n",
      "Average training loss: 0.027935193326738144\n",
      "Average test loss: 0.0013723419262096285\n",
      "Epoch 165/300\n",
      "Average training loss: 0.027785288666685424\n",
      "Average test loss: 0.0013883783244010475\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02784520298904843\n",
      "Average test loss: 0.0014222025047573778\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0279234415706661\n",
      "Average test loss: 0.0013969020795904928\n",
      "Epoch 168/300\n",
      "Average training loss: 0.027974849982394114\n",
      "Average test loss: 0.001385865127098643\n",
      "Epoch 169/300\n",
      "Average training loss: 0.027866288888785576\n",
      "Average test loss: 0.0013972164873137243\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02783733225034343\n",
      "Average test loss: 0.0014237770896611943\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0277595597124762\n",
      "Average test loss: 0.0013739129467349913\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02768993886642986\n",
      "Average test loss: 0.0013652315903455018\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0282275710536374\n",
      "Average test loss: 0.0014430829206895498\n",
      "Epoch 174/300\n",
      "Average training loss: 0.027710304589735137\n",
      "Average test loss: 0.0014274342216344343\n",
      "Epoch 175/300\n",
      "Average training loss: 0.027727239814069535\n",
      "Average test loss: 0.001374464157089177\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02778162547449271\n",
      "Average test loss: 0.001384050053647823\n",
      "Epoch 177/300\n",
      "Average training loss: 0.027782859315474827\n",
      "Average test loss: 0.0013965014573186637\n",
      "Epoch 178/300\n",
      "Average training loss: 0.027675340892540083\n",
      "Average test loss: 0.00148930158579929\n",
      "Epoch 179/300\n",
      "Average training loss: 0.027678327860103713\n",
      "Average test loss: 0.0013700965899560186\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02806037114891741\n",
      "Average test loss: 0.0014802147700554795\n",
      "Epoch 181/300\n",
      "Average training loss: 0.027731490262680583\n",
      "Average test loss: 0.0014171962830134563\n",
      "Epoch 182/300\n",
      "Average training loss: 0.027599157934387525\n",
      "Average test loss: 4.678441034740872\n",
      "Epoch 183/300\n",
      "Average training loss: 0.027740385784043205\n",
      "Average test loss: 0.0013788492317414946\n",
      "Epoch 184/300\n",
      "Average training loss: 0.027646500857339964\n",
      "Average test loss: 0.0013902334196286069\n",
      "Epoch 185/300\n",
      "Average training loss: 0.027578990006612406\n",
      "Average test loss: 0.0014559570499178436\n",
      "Epoch 186/300\n",
      "Average training loss: 0.027643130898475646\n",
      "Average test loss: 0.001385069046066039\n",
      "Epoch 187/300\n",
      "Average training loss: 0.028142926772435507\n",
      "Average test loss: 0.0013643868039362132\n",
      "Epoch 188/300\n",
      "Average training loss: 0.027431466763218244\n",
      "Average test loss: 0.0015939662153315213\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02754645728237099\n",
      "Average test loss: 0.0014546113217042552\n",
      "Epoch 190/300\n",
      "Average training loss: 0.027498111918568612\n",
      "Average test loss: 0.0013755145476510126\n",
      "Epoch 191/300\n",
      "Average training loss: 0.027469956836766667\n",
      "Average test loss: 0.0013893954820103115\n",
      "Epoch 192/300\n",
      "Average training loss: 0.027530719192491636\n",
      "Average test loss: 0.0014469160338242849\n",
      "Epoch 193/300\n",
      "Average training loss: 0.027473641821079784\n",
      "Average test loss: 0.0013805614609478248\n",
      "Epoch 194/300\n",
      "Average training loss: 0.027800913085540134\n",
      "Average test loss: 0.0015202375061603055\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02762717280205753\n",
      "Average test loss: 0.001455911814338631\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02750422572924031\n",
      "Average test loss: 0.0013836653557502561\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02755875966946284\n",
      "Average test loss: 0.001473660914848248\n",
      "Epoch 198/300\n",
      "Average training loss: 0.027354125844107735\n",
      "Average test loss: 0.0013951296291003625\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02755765646364954\n",
      "Average test loss: 0.0014066546767846579\n",
      "Epoch 200/300\n",
      "Average training loss: 0.027356119193964534\n",
      "Average test loss: 0.001412894723833435\n",
      "Epoch 201/300\n",
      "Average training loss: 0.027416207909584045\n",
      "Average test loss: 0.0018868718939936823\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02747042812903722\n",
      "Average test loss: 0.0014128813466264141\n",
      "Epoch 203/300\n",
      "Average training loss: 0.027594517586959733\n",
      "Average test loss: 0.0013880647990542154\n",
      "Epoch 204/300\n",
      "Average training loss: 0.027332120064232083\n",
      "Average test loss: 0.021888950262632635\n",
      "Epoch 205/300\n",
      "Average training loss: 0.027330276567074988\n",
      "Average test loss: 0.001396847410955363\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02736378488772445\n",
      "Average test loss: 0.0015707035416530238\n",
      "Epoch 207/300\n",
      "Average training loss: 0.027314168723093137\n",
      "Average test loss: 0.0019606615259415575\n",
      "Epoch 208/300\n",
      "Average training loss: 0.027373732030391694\n",
      "Average test loss: 0.0016779848603117797\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0279335741152366\n",
      "Average test loss: 0.0014389085183954902\n",
      "Epoch 210/300\n",
      "Average training loss: 0.027300645026895734\n",
      "Average test loss: 0.0014042276519661149\n",
      "Epoch 211/300\n",
      "Average training loss: 0.027372165388531156\n",
      "Average test loss: 0.0015239744459589323\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02723183991677231\n",
      "Average test loss: 14385.234521050348\n",
      "Epoch 213/300\n",
      "Average training loss: 0.027263539950052897\n",
      "Average test loss: 0.0016896531531173321\n",
      "Epoch 214/300\n",
      "Average training loss: 0.027298504885700015\n",
      "Average test loss: 0.0014422527683070965\n",
      "Epoch 215/300\n",
      "Average training loss: 0.027378166461984318\n",
      "Average test loss: 0.001408323385235336\n",
      "Epoch 216/300\n",
      "Average training loss: 0.027245028456879986\n",
      "Average test loss: 0.0014137523266383343\n",
      "Epoch 217/300\n",
      "Average training loss: 0.027346107641855875\n",
      "Average test loss: 0.0014045425982524952\n",
      "Epoch 218/300\n",
      "Average training loss: 0.027209964260458946\n",
      "Average test loss: 0.001401880368590355\n",
      "Epoch 219/300\n",
      "Average training loss: 0.027490974504086708\n",
      "Average test loss: 0.0013946507738696204\n",
      "Epoch 220/300\n",
      "Average training loss: 0.027182473051879142\n",
      "Average test loss: 0.001383659119096895\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02719892926679717\n",
      "Average test loss: 0.0013863341998205418\n",
      "Epoch 222/300\n",
      "Average training loss: 0.027175333393944633\n",
      "Average test loss: 0.0014635614686542087\n",
      "Epoch 223/300\n",
      "Average training loss: 0.027232817828655245\n",
      "Average test loss: 0.0014065554354650278\n",
      "Epoch 224/300\n",
      "Average training loss: 0.027225926738646294\n",
      "Average test loss: 0.0014130650501077374\n",
      "Epoch 225/300\n",
      "Average training loss: 0.027186735139952765\n",
      "Average test loss: 0.0013840142362233664\n",
      "Epoch 226/300\n",
      "Average training loss: 0.027232232951455645\n",
      "Average test loss: 0.0014115370210881035\n",
      "Epoch 227/300\n",
      "Average training loss: 0.027180727791455056\n",
      "Average test loss: 0.001404123255982995\n",
      "Epoch 228/300\n",
      "Average training loss: 0.027075691282749177\n",
      "Average test loss: 0.0014196582635243734\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02723340811332067\n",
      "Average test loss: 0.0016325695221829745\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02710222188797262\n",
      "Average test loss: 0.0014109808389200932\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02722999760011832\n",
      "Average test loss: 0.0014261154723871085\n",
      "Epoch 232/300\n",
      "Average training loss: 0.027093669374783833\n",
      "Average test loss: 0.0014563768489493263\n",
      "Epoch 233/300\n",
      "Average training loss: 0.027136354678206975\n",
      "Average test loss: 0.001602703233456446\n",
      "Epoch 234/300\n",
      "Average training loss: 0.027109526144133672\n",
      "Average test loss: 0.0014114248940928114\n",
      "Epoch 235/300\n",
      "Average training loss: 0.027199075726999176\n",
      "Average test loss: 0.001403241609585368\n",
      "Epoch 236/300\n",
      "Average training loss: 0.027021381790439288\n",
      "Average test loss: 0.10521165192334188\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02730097192360295\n",
      "Average test loss: 0.001393913849670854\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02702252856393655\n",
      "Average test loss: 0.001425563745200634\n",
      "Epoch 239/300\n",
      "Average training loss: 0.027081987496879366\n",
      "Average test loss: 0.1675660395556026\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0270615241030852\n",
      "Average test loss: 0.0014076850625375907\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02703611380358537\n",
      "Average test loss: 0.0013895104441584813\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02705901657210456\n",
      "Average test loss: 0.0018086221356772715\n",
      "Epoch 243/300\n",
      "Average training loss: 0.026929753553536204\n",
      "Average test loss: 0.0014261741137339009\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026968277812004088\n",
      "Average test loss: 0.0014378853160887956\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0269848582678371\n",
      "Average test loss: 0.0019248268091016346\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027754647756616273\n",
      "Average test loss: 0.0014125020204422375\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0269189656558964\n",
      "Average test loss: 0.0014072512529997362\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02691928566330009\n",
      "Average test loss: 0.0014950129398041301\n",
      "Epoch 253/300\n",
      "Average training loss: 0.026986339631179967\n",
      "Average test loss: 0.0016625409457418654\n",
      "Epoch 254/300\n",
      "Average training loss: 0.026937585189938544\n",
      "Average test loss: 0.0014494562122142978\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027122248530387878\n",
      "Average test loss: 0.0030919541826264726\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02691414375934336\n",
      "Average test loss: 0.0014268330999960502\n",
      "Epoch 260/300\n",
      "Average training loss: 0.027345015479458702\n",
      "Average test loss: 0.001522899395165344\n",
      "Epoch 261/300\n",
      "Average training loss: 0.026858493632740444\n",
      "Average test loss: 0.0014081700128606624\n",
      "Epoch 262/300\n",
      "Average training loss: 0.026808785383900007\n",
      "Average test loss: 0.0013990272334259417\n",
      "Epoch 263/300\n",
      "Average training loss: 0.026763016757037905\n",
      "Average test loss: 0.001431593806275891\n",
      "Epoch 264/300\n",
      "Average training loss: 0.026849867863787544\n",
      "Average test loss: 0.0014073772652902538\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02683776818215847\n",
      "Average test loss: 0.005994278035333587\n",
      "Epoch 266/300\n",
      "Average training loss: 0.026816269440783396\n",
      "Average test loss: 0.002442737823507438\n",
      "Epoch 267/300\n",
      "Average training loss: 0.026942161666022407\n",
      "Average test loss: 0.0014893356120834749\n",
      "Epoch 268/300\n",
      "Average training loss: 0.026763922813865873\n",
      "Average test loss: 0.0014195338191671503\n",
      "Epoch 269/300\n",
      "Average training loss: 0.026897376835346223\n",
      "Average test loss: 0.0017981281327083707\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02680034677022033\n",
      "Average test loss: 0.0014648607935135564\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027162566678391564\n",
      "Average test loss: 0.002703879256836242\n",
      "Epoch 272/300\n",
      "Average training loss: 0.026711018189787866\n",
      "Average test loss: 0.0015050295883168776\n",
      "Epoch 273/300\n",
      "Average training loss: 0.026787402257323265\n",
      "Average test loss: 0.0014274514253872137\n",
      "Epoch 274/300\n",
      "Average training loss: 0.026760876645644506\n",
      "Average test loss: 0.001475346158672538\n",
      "Epoch 275/300\n",
      "Average training loss: 0.026974290470282238\n",
      "Average test loss: 0.001417556511445178\n",
      "Epoch 276/300\n",
      "Average training loss: 0.026768678898612657\n",
      "Average test loss: 0.001543623690907326\n",
      "Epoch 277/300\n",
      "Average training loss: 0.026757281455728744\n",
      "Average test loss: 0.0019179261746919817\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02672880069580343\n",
      "Average test loss: 0.0014130285988665288\n",
      "Epoch 279/300\n",
      "Average training loss: 0.026820747815900377\n",
      "Average test loss: 0.001423939171868066\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026995015487074853\n",
      "Average test loss: 0.0014129054615687993\n",
      "Epoch 281/300\n",
      "Average training loss: 0.026808747167388597\n",
      "Average test loss: 0.001419709986851861\n",
      "Epoch 282/300\n",
      "Average training loss: 0.026724062316947513\n",
      "Average test loss: 7.487623594707913\n",
      "Epoch 283/300\n",
      "Average training loss: 0.026757659054464766\n",
      "Average test loss: 0.0014324483045687279\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026698679842882687\n",
      "Average test loss: 0.0014455514829605817\n",
      "Epoch 285/300\n",
      "Average training loss: 0.026717392807205517\n",
      "Average test loss: 0.0014167535136350327\n",
      "Epoch 286/300\n",
      "Average training loss: 0.026703951640261545\n",
      "Average test loss: 0.0014062559780561262\n",
      "Epoch 287/300\n",
      "Average training loss: 0.026715861234400007\n",
      "Average test loss: 0.0014645642135292293\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026707032173871995\n",
      "Average test loss: 0.001414757344043917\n",
      "Epoch 289/300\n",
      "Average training loss: 0.026600194661153688\n",
      "Average test loss: 0.001423772372615834\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02671106756395764\n",
      "Average test loss: 0.0014203693928817908\n",
      "Epoch 294/300\n",
      "Average training loss: 0.026599375918507577\n",
      "Average test loss: 0.0014116101092141536\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02675376575357384\n",
      "Average test loss: 0.0021622428785388666\n",
      "Epoch 296/300\n",
      "Average training loss: 0.026628881328635747\n",
      "Average test loss: 0.0014795608853714333\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02663739411864016\n",
      "Average test loss: 0.008238251229127249\n",
      "Epoch 298/300\n",
      "Average training loss: 0.026670920471350352\n",
      "Average test loss: 0.0014326726497254439\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02656997808151775\n",
      "Average test loss: 0.001397240107982523\n",
      "Epoch 300/300\n",
      "Average training loss: 0.026625383620460828\n",
      "Average test loss: 0.0015078937026361625\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth3-.01/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.94\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.43\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.07\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.39\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.56\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.80\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.42\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.63\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.68\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.66\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.43\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.82\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.60\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.64\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.40\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.38\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.19\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.76\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.86\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.784444282107883\n",
      "Average test loss: 0.01412928224603335\n",
      "Epoch 2/300\n",
      "Average training loss: 3.05816351975335\n",
      "Average test loss: 0.010887865845527913\n",
      "Epoch 3/300\n",
      "Average training loss: 2.138427090220981\n",
      "Average test loss: 0.010027557996412119\n",
      "Epoch 4/300\n",
      "Average training loss: 1.592377396053738\n",
      "Average test loss: 0.010320795993424125\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2269281118181017\n",
      "Average test loss: 0.00903927997002999\n",
      "Epoch 6/300\n",
      "Average training loss: 0.9284771104918585\n",
      "Average test loss: 0.008536077882680628\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7243667730225457\n",
      "Average test loss: 0.008089195192688041\n",
      "Epoch 8/300\n",
      "Average training loss: 0.5951408725844489\n",
      "Average test loss: 0.0076583137934406595\n",
      "Epoch 9/300\n",
      "Average training loss: 0.5086848589579265\n",
      "Average test loss: 0.0078066546875569555\n",
      "Epoch 10/300\n",
      "Average training loss: 0.4472273894680871\n",
      "Average test loss: 0.007543494718770186\n",
      "Epoch 11/300\n",
      "Average training loss: 0.4040586223072476\n",
      "Average test loss: 0.008201351302779384\n",
      "Epoch 12/300\n",
      "Average training loss: 0.374163916349411\n",
      "Average test loss: 0.007999846037063333\n",
      "Epoch 13/300\n",
      "Average training loss: 0.34984567099147373\n",
      "Average test loss: 0.007465146584643258\n",
      "Epoch 14/300\n",
      "Average training loss: 0.33222997930314807\n",
      "Average test loss: 0.007184193957183096\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3174241485330794\n",
      "Average test loss: 0.007781044866475794\n",
      "Epoch 16/300\n",
      "Average training loss: 0.30740772828790874\n",
      "Average test loss: 0.0065203304083810915\n",
      "Epoch 17/300\n",
      "Average training loss: 0.29583757939603594\n",
      "Average test loss: 0.007219993279212051\n",
      "Epoch 18/300\n",
      "Average training loss: 0.28688247826364305\n",
      "Average test loss: 0.006908599081966612\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2763084361155828\n",
      "Average test loss: 0.006901088976611694\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2694546114338769\n",
      "Average test loss: 0.008807874978416496\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2599953342013889\n",
      "Average test loss: 0.006269271093110243\n",
      "Epoch 22/300\n",
      "Average training loss: 0.25234988515906864\n",
      "Average test loss: 0.0060403550536268285\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2460914194981257\n",
      "Average test loss: 0.006515093402316173\n",
      "Epoch 24/300\n",
      "Average training loss: 0.24136763180626764\n",
      "Average test loss: 0.006132464985880587\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2353412214252684\n",
      "Average test loss: 0.005944836411211226\n",
      "Epoch 26/300\n",
      "Average training loss: 0.23048597394095527\n",
      "Average test loss: 0.007274889058123032\n",
      "Epoch 27/300\n",
      "Average training loss: 0.22517797475390963\n",
      "Average test loss: 0.005897881455719471\n",
      "Epoch 28/300\n",
      "Average training loss: 0.22085538409815894\n",
      "Average test loss: 0.005803587107608716\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2183428097963333\n",
      "Average test loss: 0.005685649007972744\n",
      "Epoch 30/300\n",
      "Average training loss: 0.21391192192501493\n",
      "Average test loss: 0.005753046623120706\n",
      "Epoch 31/300\n",
      "Average training loss: 0.21000993275642396\n",
      "Average test loss: 0.006607433607594834\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2082452122370402\n",
      "Average test loss: 0.006296894737415844\n",
      "Epoch 33/300\n",
      "Average training loss: 0.20520796236726974\n",
      "Average test loss: 0.005818947642213769\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2026296366320716\n",
      "Average test loss: 0.005617121346708801\n",
      "Epoch 35/300\n",
      "Average training loss: 0.20004103799661\n",
      "Average test loss: 0.005582261289159457\n",
      "Epoch 36/300\n",
      "Average training loss: 0.19819948131508297\n",
      "Average test loss: 0.005764012923257219\n",
      "Epoch 37/300\n",
      "Average training loss: 0.19753961707486048\n",
      "Average test loss: 0.005597267310652468\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1961613338920805\n",
      "Average test loss: 0.005646821029070351\n",
      "Epoch 39/300\n",
      "Average training loss: 0.19299736660056643\n",
      "Average test loss: 0.005923955322967635\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1917905567487081\n",
      "Average test loss: 0.005795774691220787\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1900265661345588\n",
      "Average test loss: 0.005554985346479548\n",
      "Epoch 42/300\n",
      "Average training loss: 0.18878307665718927\n",
      "Average test loss: 0.005605624650915464\n",
      "Epoch 43/300\n",
      "Average training loss: 0.18807326204246944\n",
      "Average test loss: 0.005761642450673713\n",
      "Epoch 44/300\n",
      "Average training loss: 0.18401502933767105\n",
      "Average test loss: 0.0061535717265473475\n",
      "Epoch 47/300\n",
      "Average training loss: 0.18285141617059708\n",
      "Average test loss: 0.005418029298798905\n",
      "Epoch 48/300\n",
      "Average training loss: 0.18236481744713254\n",
      "Average test loss: 0.00566707760343949\n",
      "Epoch 49/300\n",
      "Average training loss: 0.18160808854632907\n",
      "Average test loss: 0.005401361893655525\n",
      "Epoch 50/300\n",
      "Average training loss: 0.18061486032274035\n",
      "Average test loss: 0.00546430882335537\n",
      "Epoch 51/300\n",
      "Average training loss: 0.17985234910911985\n",
      "Average test loss: 0.006636811474131213\n",
      "Epoch 52/300\n",
      "Average training loss: 0.17969543973604837\n",
      "Average test loss: 0.005810660836390323\n",
      "Epoch 53/300\n",
      "Average training loss: 0.17811720555358462\n",
      "Average test loss: 0.0053859663580854734\n",
      "Epoch 54/300\n",
      "Average training loss: 0.17742375612258912\n",
      "Average test loss: 0.005441123673071464\n",
      "Epoch 55/300\n",
      "Average training loss: 0.17779765968852573\n",
      "Average test loss: 0.005349189721047878\n",
      "Epoch 56/300\n",
      "Average training loss: 0.17660884300867716\n",
      "Average test loss: 7.1511303249994915\n",
      "Epoch 57/300\n",
      "Average training loss: 0.17492102931605444\n",
      "Average test loss: 0.005345929236254758\n",
      "Epoch 58/300\n",
      "Average training loss: 0.17461073381370967\n",
      "Average test loss: 130.60141299099394\n",
      "Epoch 59/300\n",
      "Average training loss: 0.17453798345724741\n",
      "Average test loss: 0.005225208722882801\n",
      "Epoch 60/300\n",
      "Average training loss: 0.173981237411499\n",
      "Average test loss: 0.005248523757689529\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1753925257258945\n",
      "Average test loss: 0.009683714889196886\n",
      "Epoch 62/300\n",
      "Average training loss: 0.17239734750323826\n",
      "Average test loss: 0.005489232123312023\n",
      "Epoch 63/300\n",
      "Average training loss: 0.173254776040713\n",
      "Average test loss: 0.005247464642342594\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1737324496905009\n",
      "Average test loss: 0.005348570734262466\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1720631237294939\n",
      "Average test loss: 0.005421536838014921\n",
      "Epoch 67/300\n",
      "Average training loss: 0.17086544308397505\n",
      "Average test loss: 0.0053155163129170735\n",
      "Epoch 68/300\n",
      "Average training loss: 0.16964182731840347\n",
      "Average test loss: 0.00517187764412827\n",
      "Epoch 69/300\n",
      "Average training loss: 0.16875839834743075\n",
      "Average test loss: 0.017075520381331442\n",
      "Epoch 70/300\n",
      "Average training loss: 0.16896078079276614\n",
      "Average test loss: 0.02577137116756704\n",
      "Epoch 71/300\n",
      "Average training loss: 0.16963733279705048\n",
      "Average test loss: 0.005344785281767447\n",
      "Epoch 72/300\n",
      "Average training loss: 0.16931652275721232\n",
      "Average test loss: 0.0054748714673850275\n",
      "Epoch 73/300\n",
      "Average training loss: 0.16727235535780588\n",
      "Average test loss: 0.005376565198103587\n",
      "Epoch 74/300\n",
      "Average training loss: 0.16812708140744104\n",
      "Average test loss: 0.005360157562212812\n",
      "Epoch 75/300\n",
      "Average training loss: 0.16682465563880072\n",
      "Average test loss: 0.0052264938387605875\n",
      "Epoch 76/300\n",
      "Average training loss: 0.16591358853711022\n",
      "Average test loss: 0.006912625778466463\n",
      "Epoch 77/300\n",
      "Average training loss: 0.16604242861270904\n",
      "Average test loss: 0.00540949380149444\n",
      "Epoch 78/300\n",
      "Average training loss: 0.16666748550203112\n",
      "Average test loss: 0.005196276978486114\n",
      "Epoch 79/300\n",
      "Average training loss: 0.16536762514379288\n",
      "Average test loss: 0.006021439802729421\n",
      "Epoch 80/300\n",
      "Average training loss: 0.16467065428362954\n",
      "Average test loss: 0.00610319910902116\n",
      "Epoch 81/300\n",
      "Average training loss: 0.1644719341993332\n",
      "Average test loss: 0.0060007941629737615\n",
      "Epoch 83/300\n",
      "Average training loss: 0.1636147957642873\n",
      "Average test loss: 0.0053632116098370815\n",
      "Epoch 84/300\n",
      "Average training loss: 151586236415.27795\n",
      "Average test loss: 18.90166677347819\n",
      "Epoch 85/300\n",
      "Average training loss: 257.10676840549047\n",
      "Average test loss: 18.31350622897678\n",
      "Epoch 86/300\n",
      "Average training loss: 231.3893855658637\n",
      "Average test loss: 3.225034708658854\n",
      "Epoch 87/300\n",
      "Average training loss: 210.97838750542536\n",
      "Average test loss: 6.4595540987650555\n",
      "Epoch 88/300\n",
      "Average training loss: 183.42493273925783\n",
      "Average test loss: 3.1342080290052627\n",
      "Epoch 90/300\n",
      "Average training loss: 174.195117702908\n",
      "Average test loss: 1.0124987778133816\n",
      "Epoch 91/300\n",
      "Average training loss: 166.48694436306423\n",
      "Average test loss: 1.1512047978507147\n",
      "Epoch 92/300\n",
      "Average training loss: 161.11755406358506\n",
      "Average test loss: 1.4175918148888482\n",
      "Epoch 93/300\n",
      "Average training loss: 149.60055124240452\n",
      "Average test loss: 0.8043697730700174\n",
      "Epoch 95/300\n",
      "Average training loss: 146.09334232584635\n",
      "Average test loss: 0.7616515878041585\n",
      "Epoch 96/300\n",
      "Average training loss: 142.03401689995658\n",
      "Average test loss: 0.7055958651966519\n",
      "Epoch 97/300\n",
      "Average training loss: 139.98565661621095\n",
      "Average test loss: 0.715014217376709\n",
      "Epoch 98/300\n",
      "Average training loss: 136.38111184353298\n",
      "Average test loss: 0.5490585675769382\n",
      "Epoch 99/300\n",
      "Average training loss: 132.76281575520832\n",
      "Average test loss: 0.5574382088979085\n",
      "Epoch 100/300\n",
      "Average training loss: 129.06664735243055\n",
      "Average test loss: 0.39948058729701574\n",
      "Epoch 101/300\n",
      "Average training loss: 125.75360188802084\n",
      "Average test loss: 0.3484597727457682\n",
      "Epoch 102/300\n",
      "Average training loss: 121.70462007649739\n",
      "Average test loss: 0.2776182127263811\n",
      "Epoch 103/300\n",
      "Average training loss: 118.68719682481554\n",
      "Average test loss: 0.2056843796306186\n",
      "Epoch 104/300\n",
      "Average training loss: 114.42093975830078\n",
      "Average test loss: 0.145369161023034\n",
      "Epoch 105/300\n",
      "Average training loss: 110.11371610514323\n",
      "Average test loss: 0.11464809940258662\n",
      "Epoch 106/300\n",
      "Average training loss: 104.19033319091797\n",
      "Average test loss: 0.09669043392605252\n",
      "Epoch 107/300\n",
      "Average training loss: 99.49489185926649\n",
      "Average test loss: 0.08519846034712261\n",
      "Epoch 108/300\n",
      "Average training loss: 93.62605763075086\n",
      "Average test loss: 0.06330070792966419\n",
      "Epoch 109/300\n",
      "Average training loss: 87.35363928222657\n",
      "Average test loss: 0.05802052515745163\n",
      "Epoch 110/300\n",
      "Average training loss: 81.57001406521267\n",
      "Average test loss: 0.058573518262969124\n",
      "Epoch 111/300\n",
      "Average training loss: 76.33952033148871\n",
      "Average test loss: 0.047644495477279024\n",
      "Epoch 112/300\n",
      "Average training loss: 62.32614525689019\n",
      "Average test loss: 0.025062815139691035\n",
      "Epoch 115/300\n",
      "Average training loss: 57.77343906656901\n",
      "Average test loss: 0.021547924247052933\n",
      "Epoch 116/300\n",
      "Average training loss: 54.07627079942491\n",
      "Average test loss: 0.03972029709484842\n",
      "Epoch 117/300\n",
      "Average training loss: 50.25217309570313\n",
      "Average test loss: 0.01698139975633886\n",
      "Epoch 118/300\n",
      "Average training loss: 46.01192453342014\n",
      "Average test loss: 0.016531953593095144\n",
      "Epoch 119/300\n",
      "Average training loss: 42.48506823391384\n",
      "Average test loss: 0.01482317071656386\n",
      "Epoch 120/300\n",
      "Average training loss: 38.72589705403646\n",
      "Average test loss: 0.012036255784332752\n",
      "Epoch 121/300\n",
      "Average training loss: 35.258102411905924\n",
      "Average test loss: 0.018736627439657847\n",
      "Epoch 122/300\n",
      "Average training loss: 32.17269170803494\n",
      "Average test loss: 0.010669447833465205\n",
      "Epoch 123/300\n",
      "Average training loss: 29.508489920722113\n",
      "Average test loss: 0.010888187052475082\n",
      "Epoch 124/300\n",
      "Average training loss: 27.317453808254665\n",
      "Average test loss: 0.010426678525076971\n",
      "Epoch 125/300\n",
      "Average training loss: 25.1782339392768\n",
      "Average test loss: 0.010976483527157042\n",
      "Epoch 126/300\n",
      "Average training loss: 23.438545167711045\n",
      "Average test loss: 0.009460870466712448\n",
      "Epoch 127/300\n",
      "Average training loss: 21.878707950168184\n",
      "Average test loss: 0.009232922618587812\n",
      "Epoch 128/300\n",
      "Average training loss: 20.515502768622504\n",
      "Average test loss: 0.008652854331251647\n",
      "Epoch 129/300\n",
      "Average training loss: 19.433525953504773\n",
      "Average test loss: 0.008360705138908492\n",
      "Epoch 130/300\n",
      "Average training loss: 18.46117197502984\n",
      "Average test loss: 0.008029292711781131\n",
      "Epoch 131/300\n",
      "Average training loss: 17.69232699754503\n",
      "Average test loss: 0.0079914161844386\n",
      "Epoch 132/300\n",
      "Average test loss: 0.007994229083259901\n",
      "Epoch 134/300\n",
      "Average training loss: 15.539973097059463\n",
      "Average test loss: 0.007284779166595803\n",
      "Epoch 135/300\n",
      "Average training loss: 14.814907636006673\n",
      "Average test loss: 0.007229860995378759\n",
      "Epoch 136/300\n",
      "Average training loss: 14.12640661875407\n",
      "Average test loss: 0.0069603715340296425\n",
      "Epoch 137/300\n",
      "Average training loss: 13.476996325174968\n",
      "Average test loss: 0.006923248500873645\n",
      "Epoch 138/300\n",
      "Average training loss: 12.85040763092041\n",
      "Average test loss: 0.007223784880505668\n",
      "Epoch 139/300\n",
      "Average training loss: 12.286285558912489\n",
      "Average test loss: 0.006828870930605464\n",
      "Epoch 140/300\n",
      "Average training loss: 11.779992604573568\n",
      "Average test loss: 0.006706780703531371\n",
      "Epoch 141/300\n",
      "Average training loss: 11.357729889763727\n",
      "Average test loss: 0.00645766431838274\n",
      "Epoch 142/300\n",
      "Average training loss: 10.90982298956977\n",
      "Average test loss: 0.00652366094580955\n",
      "Epoch 143/300\n",
      "Average training loss: 10.473588931613499\n",
      "Average test loss: 0.006353513393965032\n",
      "Epoch 144/300\n",
      "Average training loss: 10.05124772813585\n",
      "Average test loss: 0.006281205328802267\n",
      "Epoch 145/300\n",
      "Average training loss: 9.640307367960611\n",
      "Average test loss: 0.006480574910839399\n",
      "Epoch 146/300\n",
      "Average training loss: 9.239344791836208\n",
      "Average test loss: 0.006105671779149108\n",
      "Epoch 147/300\n",
      "Average training loss: 8.840385706583659\n",
      "Average test loss: 0.0062487606190972855\n",
      "Epoch 148/300\n",
      "Average training loss: 8.422893634372286\n",
      "Average test loss: 0.006118520220534669\n",
      "Epoch 149/300\n",
      "Average training loss: 8.012025344000921\n",
      "Average test loss: 0.005999203879800108\n",
      "Epoch 150/300\n",
      "Average training loss: 7.635275010426839\n",
      "Average test loss: 0.0060915019735693935\n",
      "Epoch 151/300\n",
      "Average training loss: 7.256688997480604\n",
      "Average test loss: 0.005815274868160486\n",
      "Epoch 152/300\n",
      "Average training loss: 6.179106746249729\n",
      "Average test loss: 0.005967115454789665\n",
      "Epoch 155/300\n",
      "Average training loss: 5.834571570502387\n",
      "Average test loss: 0.006559364258415169\n",
      "Epoch 156/300\n",
      "Average training loss: 5.491519535064698\n",
      "Average test loss: 0.005744266699999571\n",
      "Epoch 157/300\n",
      "Average training loss: 5.151236666361491\n",
      "Average test loss: 0.00578726934393247\n",
      "Epoch 158/300\n",
      "Average training loss: 4.793793861813016\n",
      "Average test loss: 0.005662364052401649\n",
      "Epoch 159/300\n",
      "Average training loss: 4.428994920518663\n",
      "Average test loss: 0.005675368375248379\n",
      "Epoch 160/300\n",
      "Average training loss: 4.045620969348484\n",
      "Average test loss: 0.006928242167250977\n",
      "Epoch 161/300\n",
      "Average training loss: 3.6291832540300155\n",
      "Average test loss: 0.005556756639232238\n",
      "Epoch 162/300\n",
      "Average training loss: 3.19004639922248\n",
      "Average test loss: 0.005535084556374285\n",
      "Epoch 163/300\n",
      "Average training loss: 2.81755430899726\n",
      "Average test loss: 0.0054470987166795465\n",
      "Epoch 164/300\n",
      "Average training loss: 2.4850352848900688\n",
      "Average test loss: 0.005597463411175543\n",
      "Epoch 165/300\n",
      "Average training loss: 2.1942819046444364\n",
      "Average test loss: 0.0054488324796160064\n",
      "Epoch 166/300\n",
      "Average training loss: 1.9360855234993828\n",
      "Average test loss: 0.005364351692299048\n",
      "Epoch 167/300\n",
      "Average training loss: 1.6936011839972602\n",
      "Average test loss: 0.0054063325809935725\n",
      "Epoch 168/300\n",
      "Average training loss: 1.4738635410732692\n",
      "Average test loss: 0.005483248778929313\n",
      "Epoch 169/300\n",
      "Average training loss: 1.2821185245513915\n",
      "Average test loss: 0.005332594929469956\n",
      "Epoch 170/300\n",
      "Average training loss: 0.8356292864481608\n",
      "Average test loss: 0.005553212570647398\n",
      "Epoch 173/300\n",
      "Average training loss: 0.7064072499805026\n",
      "Average test loss: 0.0052494601301021045\n",
      "Epoch 174/300\n",
      "Average training loss: 0.5914628858036465\n",
      "Average test loss: 0.005434569535983934\n",
      "Epoch 175/300\n",
      "Average training loss: 0.4941172908147176\n",
      "Average test loss: 0.005307504867927896\n",
      "Epoch 176/300\n",
      "Average training loss: 0.41742665995491873\n",
      "Average test loss: 0.0052795227234148315\n",
      "Epoch 177/300\n",
      "Average training loss: 0.3565943466027578\n",
      "Average test loss: 0.005245933631228076\n",
      "Epoch 178/300\n",
      "Average training loss: 0.31054645750257703\n",
      "Average test loss: 0.0060475200124912795\n",
      "Epoch 179/300\n",
      "Average training loss: 0.27633141758706836\n",
      "Average test loss: 0.005331962770058049\n",
      "Epoch 180/300\n",
      "Average training loss: 0.2513696517679426\n",
      "Average test loss: 0.005265149898827076\n",
      "Epoch 181/300\n",
      "Average training loss: 0.23343981069988676\n",
      "Average test loss: 0.005312240050070816\n",
      "Epoch 182/300\n",
      "Average training loss: 0.22133450837930044\n",
      "Average test loss: 0.005352980764996675\n",
      "Epoch 183/300\n",
      "Average training loss: 0.21276162485281627\n",
      "Average test loss: 0.005199245174311929\n",
      "Epoch 184/300\n",
      "Average training loss: 0.2060216662618849\n",
      "Average test loss: 0.005239873343457779\n",
      "Epoch 185/300\n",
      "Average training loss: 0.2004103942049874\n",
      "Average test loss: 0.005740574610730013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.19455292847421435\n",
      "Average test loss: 0.005173964824941423\n",
      "Epoch 187/300\n",
      "Average training loss: 0.18695126437478596\n",
      "Average test loss: 0.005151192587696844\n",
      "Epoch 189/300\n",
      "Average training loss: 0.18241679179668427\n",
      "Average test loss: 0.005137453503078884\n",
      "Epoch 190/300\n",
      "Average training loss: 0.17982040202617644\n",
      "Average test loss: 0.005223939321935177\n",
      "Epoch 191/300\n",
      "Average training loss: 0.1780749751329422\n",
      "Average test loss: 0.005279633749276399\n",
      "Epoch 192/300\n",
      "Average training loss: 0.17611218780941434\n",
      "Average test loss: 0.005228205545081033\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1743847797446781\n",
      "Average test loss: 0.005148173772626453\n",
      "Epoch 194/300\n",
      "Average training loss: 0.17337586385673948\n",
      "Average test loss: 0.005202687006029817\n",
      "Epoch 195/300\n",
      "Average training loss: 0.17153699851036072\n",
      "Average test loss: 0.005268132747047478\n",
      "Epoch 196/300\n",
      "Average training loss: 0.17073895789517296\n",
      "Average test loss: 0.005211069198325277\n",
      "Epoch 197/300\n",
      "Average training loss: 0.16890500815709433\n",
      "Average test loss: 0.00531659268339475\n",
      "Epoch 198/300\n",
      "Average training loss: 0.16791195562150743\n",
      "Average test loss: 0.005185919771591822\n",
      "Epoch 199/300\n",
      "Average training loss: 0.16679624160130818\n",
      "Average test loss: 0.005558498179333077\n",
      "Epoch 200/300\n",
      "Average training loss: 0.16709411593278248\n",
      "Average test loss: 0.0054887216091156\n",
      "Epoch 201/300\n",
      "Average training loss: 0.16582534782091776\n",
      "Average test loss: 0.005156905118376016\n",
      "Epoch 202/300\n",
      "Average training loss: 0.16604882644282448\n",
      "Average test loss: 0.00521197632037931\n",
      "Epoch 203/300\n",
      "Average training loss: 0.16393399120701685\n",
      "Average test loss: 0.005154847333828609\n",
      "Epoch 204/300\n",
      "Average training loss: 0.16449154301484425\n",
      "Average test loss: 0.005209255841871103\n",
      "Epoch 205/300\n",
      "Average training loss: 0.16321051381693946\n",
      "Average test loss: 0.005193187811722358\n",
      "Epoch 206/300\n",
      "Average training loss: 0.16242184718449912\n",
      "Average test loss: 0.00518980703006188\n",
      "Epoch 209/300\n",
      "Average training loss: 0.1623891001674864\n",
      "Average test loss: 0.005501871003872818\n",
      "Epoch 210/300\n",
      "Average training loss: 0.16106182486481135\n",
      "Average test loss: 0.005302399098045296\n",
      "Epoch 213/300\n",
      "Average training loss: 0.1621869892279307\n",
      "Average test loss: 0.005205207328415579\n",
      "Epoch 214/300\n",
      "Average training loss: 0.1605609179602729\n",
      "Average test loss: 0.005237318494253689\n",
      "Epoch 215/300\n",
      "Average training loss: 0.16034887276755438\n",
      "Average test loss: 0.005212483942922619\n",
      "Epoch 216/300\n",
      "Average training loss: 45093066701.74564\n",
      "Average test loss: 281.56641197109224\n",
      "Epoch 217/300\n",
      "Average training loss: 12.943342179192436\n",
      "Average test loss: 9357.66564505789\n",
      "Epoch 218/300\n",
      "Average training loss: 12.749992550320096\n",
      "Average test loss: 4949.152718870812\n",
      "Epoch 219/300\n",
      "Average training loss: 12.57571171145969\n",
      "Average test loss: 497.8295860673057\n",
      "Epoch 220/300\n",
      "Average training loss: 12.475149527655708\n",
      "Average test loss: 1.349487738556332\n",
      "Epoch 221/300\n",
      "Average training loss: 12.373806821187337\n",
      "Average test loss: 1915.4263376462725\n",
      "Epoch 222/300\n",
      "Average training loss: 12.304938098483616\n",
      "Average test loss: 386027.07479296875\n",
      "Epoch 223/300\n",
      "Average training loss: 12.216833920796713\n",
      "Average test loss: 28.255687231063842\n",
      "Epoch 224/300\n",
      "Average training loss: 12.131934757656522\n",
      "Average test loss: 6.851593222233984\n",
      "Epoch 225/300\n",
      "Average training loss: 12.081974487304688\n",
      "Average test loss: 66.8608984497918\n",
      "Epoch 226/300\n",
      "Average training loss: 12.00466776784261\n",
      "Average test loss: 7736.914924268616\n",
      "Epoch 227/300\n",
      "Average training loss: 11.870475621541342\n",
      "Average test loss: 929.2461578983201\n",
      "Epoch 229/300\n",
      "Average training loss: 11.757468063354493\n",
      "Average test loss: 3683.7700676946574\n",
      "Epoch 230/300\n",
      "Average training loss: 11.694471906026205\n",
      "Average test loss: 39095.37017220709\n",
      "Epoch 231/300\n",
      "Average training loss: 11.597419346279569\n",
      "Average test loss: 473.5006478289498\n",
      "Epoch 232/300\n",
      "Average training loss: 11.478718315972221\n",
      "Average test loss: 8498.914948526022\n",
      "Epoch 233/300\n",
      "Average training loss: 11.331175656636557\n",
      "Average test loss: 19.124159312142265\n",
      "Epoch 234/300\n",
      "Average training loss: 11.22271619245741\n",
      "Average test loss: 717649.1336117925\n",
      "Epoch 235/300\n",
      "Average training loss: 11.066493212382\n",
      "Average test loss: 6862.761767367813\n",
      "Epoch 236/300\n",
      "Average training loss: 10.909694875929091\n",
      "Average test loss: 14797.261430393723\n",
      "Epoch 237/300\n",
      "Average training loss: 10.77322878519694\n",
      "Average test loss: 2649.312958366712\n",
      "Epoch 238/300\n",
      "Average training loss: 10.663599044799804\n",
      "Average test loss: 33400.18995227469\n",
      "Epoch 239/300\n",
      "Average training loss: 10.4935648888482\n",
      "Average test loss: 3759.1333972320954\n",
      "Epoch 240/300\n",
      "Average training loss: 10.378242343478732\n",
      "Average test loss: 0.2672063700805108\n",
      "Epoch 241/300\n",
      "Average training loss: 10.202056938171387\n",
      "Average test loss: 1015235.2672766753\n",
      "Epoch 242/300\n",
      "Average training loss: 10.072466719733344\n",
      "Average test loss: 0.12073883467581537\n",
      "Epoch 243/300\n",
      "Average training loss: 9.898798089769151\n",
      "Average test loss: 52.746727091222176\n",
      "Epoch 244/300\n",
      "Average training loss: 9.774993819342718\n",
      "Average test loss: 22.912899379058018\n",
      "Epoch 245/300\n",
      "Average test loss: 14000.282939445913\n",
      "Epoch 248/300\n",
      "Average training loss: 9.133979944864908\n",
      "Average test loss: 0.20166419736213154\n",
      "Epoch 249/300\n",
      "Average training loss: 8.953685065375433\n",
      "Average test loss: 0.4313898829031322\n",
      "Epoch 250/300\n",
      "Average training loss: 8.751215949164497\n",
      "Average test loss: 246.4354247405496\n",
      "Epoch 251/300\n",
      "Average training loss: 8.57777783203125\n",
      "Average test loss: 5.14800463517838\n",
      "Epoch 252/300\n",
      "Average training loss: 8.419845110575357\n",
      "Average test loss: 0.007841225012722943\n",
      "Epoch 253/300\n",
      "Average training loss: 8.234743337843152\n",
      "Average test loss: 20.789394306335183\n",
      "Epoch 254/300\n",
      "Average training loss: 8.048455752054851\n",
      "Average test loss: 0.0490434775625666\n",
      "Epoch 255/300\n",
      "Average training loss: 7.844000418768989\n",
      "Average test loss: 0.7655616092847454\n",
      "Epoch 256/300\n",
      "Average training loss: 7.595531401316324\n",
      "Average test loss: 0.00801696724196275\n",
      "Epoch 257/300\n",
      "Average training loss: 7.375591406504313\n",
      "Average test loss: 144.92666037686666\n",
      "Epoch 258/300\n",
      "Average training loss: 7.117479734208849\n",
      "Average test loss: 0.08305574800239669\n",
      "Epoch 259/300\n",
      "Average training loss: 6.879095937516954\n",
      "Average test loss: 0.3343744178083208\n",
      "Epoch 260/300\n",
      "Average training loss: 6.646580480787489\n",
      "Average test loss: 0.007078815786788861\n",
      "Epoch 261/300\n",
      "Average training loss: 6.373984402550591\n",
      "Average test loss: 0.0107285020314157\n",
      "Epoch 262/300\n",
      "Average training loss: 6.101976785024007\n",
      "Average test loss: 0.10440062390226457\n",
      "Epoch 263/300\n",
      "Average training loss: 5.833997085995144\n",
      "Average test loss: 0.006671835015631384\n",
      "Epoch 264/300\n",
      "Average training loss: 5.517105338202582\n",
      "Average test loss: 0.006371014580544498\n",
      "Epoch 265/300\n",
      "Average training loss: 5.222581403944227\n",
      "Average test loss: 0.015921600717637274\n",
      "Epoch 266/300\n",
      "Average training loss: 4.460164593166775\n",
      "Average test loss: 0.03376930313102073\n",
      "Epoch 269/300\n",
      "Average training loss: 4.220811762703789\n",
      "Average test loss: 0.07772889057464069\n",
      "Epoch 270/300\n",
      "Average training loss: 3.6250841772291396\n",
      "Average test loss: 0.006141860526055098\n",
      "Epoch 273/300\n",
      "Average training loss: 3.451755293316311\n",
      "Average test loss: 0.22609864730305143\n",
      "Epoch 274/300\n",
      "Average training loss: 3.2808832013871934\n",
      "Average test loss: 7.478491367124849\n",
      "Epoch 275/300\n",
      "Average training loss: 3.1313747929467097\n",
      "Average test loss: 2.6623686035983263\n",
      "Epoch 276/300\n",
      "Average training loss: 2.983420669131809\n",
      "Average test loss: 0.005752534665995174\n",
      "Epoch 277/300\n",
      "Average training loss: 2.845641162448459\n",
      "Average test loss: 0.0057005947513712775\n",
      "Epoch 278/300\n",
      "Average training loss: 2.710303900612725\n",
      "Average test loss: 0.009011579073551629\n",
      "Epoch 279/300\n",
      "Average training loss: 2.5675263017018635\n",
      "Average test loss: 0.007607189186745219\n",
      "Epoch 280/300\n",
      "Average training loss: 2.4262743894788956\n",
      "Average test loss: 0.005637490176906189\n",
      "Epoch 281/300\n",
      "Average training loss: 2.3062082104153103\n",
      "Average test loss: 0.005458732180711296\n",
      "Epoch 282/300\n",
      "Average training loss: 2.198353935453627\n",
      "Average test loss: 0.005465413766602675\n",
      "Epoch 283/300\n",
      "Average training loss: 2.0933397612041897\n",
      "Average test loss: 0.005513342346168227\n",
      "Epoch 284/300\n",
      "Average training loss: 1.9918087793986003\n",
      "Average test loss: 0.005464113138409124\n",
      "Epoch 285/300\n",
      "Average training loss: 1.8864393586052788\n",
      "Average test loss: 0.005590497883243693\n",
      "Epoch 286/300\n",
      "Average training loss: 1.7776205252541437\n",
      "Average training loss: 1.559696917851766\n",
      "Average test loss: 0.005295928943488333\n",
      "Epoch 289/300\n",
      "Average training loss: 1.4648009957207573\n",
      "Average test loss: 0.005336437051908838\n",
      "Epoch 290/300\n",
      "Average training loss: 1.3718103367487589\n",
      "Average test loss: 0.0052931418360935316\n",
      "Epoch 291/300\n",
      "Average training loss: 1.2798616925345527\n",
      "Average test loss: 0.005275092712293069\n",
      "Epoch 292/300\n",
      "Average training loss: 1.1942811298370362\n",
      "Average test loss: 0.005268255123661624\n",
      "Epoch 293/300\n",
      "Average training loss: 1.1066586971812777\n",
      "Average test loss: 0.0052771968816717465\n",
      "Epoch 294/300\n",
      "Average training loss: 1.0189414650069342\n",
      "Average test loss: 0.005206471594464448\n",
      "Epoch 295/300\n",
      "Average training loss: 0.9272471435864766\n",
      "Average test loss: 0.005200723933469918\n",
      "Epoch 296/300\n",
      "Average training loss: 0.8325987646314833\n",
      "Average test loss: 0.0051550893547634286\n",
      "Epoch 297/300\n",
      "Average training loss: 0.7370793564054701\n",
      "Average test loss: 0.005160147258390983\n",
      "Epoch 298/300\n",
      "Average training loss: 0.6462075532277425\n",
      "Average test loss: 0.005208640674336089\n",
      "Epoch 299/300\n",
      "Average training loss: 0.5671756317350599\n",
      "Average test loss: 0.005163195505324337\n",
      "Epoch 300/300\n",
      "Average training loss: 0.4997281156645881\n",
      "Average test loss: 0.005185278401192692\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.51125136672126\n",
      "Average test loss: 0.009549411280287636\n",
      "Epoch 2/300\n",
      "Average training loss: 2.0247835778130425\n",
      "Average test loss: 0.007148577613963021\n",
      "Epoch 3/300\n",
      "Average training loss: 1.181432604153951\n",
      "Average test loss: 0.006421902952922715\n",
      "Epoch 4/300\n",
      "Average training loss: 0.783821754137675\n",
      "Average test loss: 0.006129201542172167\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5731359855069055\n",
      "Average test loss: 0.005893249299791124\n",
      "Epoch 6/300\n",
      "Average training loss: 0.4521554240120782\n",
      "Average test loss: 0.006527038643343581\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3769286001258426\n",
      "Average test loss: 0.005579991712752316\n",
      "Epoch 8/300\n",
      "Average training loss: 0.32587513873312207\n",
      "Average test loss: 0.0054844645245207685\n",
      "Epoch 9/300\n",
      "Average training loss: 0.28993802597787643\n",
      "Average test loss: 0.005169362380686733\n",
      "Epoch 10/300\n",
      "Average training loss: 0.26238783215151895\n",
      "Average test loss: 0.004883597059796254\n",
      "Epoch 11/300\n",
      "Average training loss: 0.24274986596902212\n",
      "Average test loss: 0.004920826039794418\n",
      "Epoch 12/300\n",
      "Average training loss: 0.22760818179448444\n",
      "Average test loss: 0.004639722437494331\n",
      "Epoch 13/300\n",
      "Average training loss: 0.2155416128370497\n",
      "Average test loss: 0.004426259208884504\n",
      "Epoch 14/300\n",
      "Average training loss: 0.20541656659709082\n",
      "Average test loss: 0.005053848297231727\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1965270225074556\n",
      "Average test loss: 0.004304251791702377\n",
      "Epoch 16/300\n",
      "Average training loss: 0.19004352482159934\n",
      "Average test loss: 0.004544791868991322\n",
      "Epoch 17/300\n",
      "Average training loss: 0.18380198921097649\n",
      "Average test loss: 0.004884524962968297\n",
      "Epoch 18/300\n",
      "Average training loss: 0.17738011565473344\n",
      "Average test loss: 0.006687697862999307\n",
      "Epoch 19/300\n",
      "Average training loss: 0.17361546403831907\n",
      "Average test loss: 0.0051500715898970765\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16652950406736797\n",
      "Average test loss: 0.003716797839850187\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16052216739124722\n",
      "Average test loss: 0.0038842708634005654\n",
      "Epoch 22/300\n",
      "Average training loss: 0.15717158026165431\n",
      "Average test loss: 0.004088392936521106\n",
      "Epoch 23/300\n",
      "Average training loss: 0.15236643942197164\n",
      "Average test loss: 0.0036213547270745037\n",
      "Epoch 24/300\n",
      "Average training loss: 0.14800994273026785\n",
      "Average test loss: 0.004033726053933303\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14405529369248285\n",
      "Average test loss: 0.0035686757295495934\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13972167755497827\n",
      "Average test loss: 0.004650897052552965\n",
      "Epoch 27/300\n",
      "Average training loss: 0.13690499665339786\n",
      "Average test loss: 0.003737431027409103\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13327667323085998\n",
      "Average test loss: 0.004011274794116617\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1305045973857244\n",
      "Average test loss: 0.003483120141343938\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12866147809558445\n",
      "Average test loss: 0.003377719214806954\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1256512797077497\n",
      "Average test loss: 0.003291679645991988\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1238642254670461\n",
      "Average test loss: 0.003641294964071777\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12283751662572225\n",
      "Average test loss: 0.00329164538346231\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12078720562325583\n",
      "Average test loss: 0.0032871836334880857\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11850943705770704\n",
      "Average test loss: 0.0032203661118530564\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1172432335085339\n",
      "Average test loss: 0.0032024577593223916\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11600839313533572\n",
      "Average test loss: 0.0032824073851936393\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11502423547373877\n",
      "Average test loss: 0.004922248786522283\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11384413472149107\n",
      "Average test loss: 0.003233475605232848\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11317682239744398\n",
      "Average test loss: 0.0032321020909067658\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11191198933124542\n",
      "Average test loss: 0.004608466695580218\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11081711073716481\n",
      "Average test loss: 0.0035608982344468433\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1111530286139912\n",
      "Average test loss: 0.0031447867929107615\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10906434967782762\n",
      "Average test loss: 0.0034208395996441444\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11047881170776155\n",
      "Average test loss: 0.003554023868507809\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10846290099620819\n",
      "Average test loss: 0.02354841780165831\n",
      "Epoch 47/300\n",
      "Average training loss: 1.1442051592469216\n",
      "Average test loss: 0.1120438714913196\n",
      "Epoch 48/300\n",
      "Average training loss: 2.628442970275879\n",
      "Average test loss: 0.004169626859741079\n",
      "Epoch 49/300\n",
      "Average training loss: 1.1290064992374844\n",
      "Average test loss: 0.0038724962911672062\n",
      "Epoch 50/300\n",
      "Average training loss: 0.6962961230807834\n",
      "Average test loss: 0.00364258529452814\n",
      "Epoch 51/300\n",
      "Average training loss: 0.497608516799079\n",
      "Average test loss: 0.003643958889361885\n",
      "Epoch 52/300\n",
      "Average training loss: 0.37686455252435475\n",
      "Average test loss: 0.0034431686049534215\n",
      "Epoch 53/300\n",
      "Average training loss: 0.29701376419597203\n",
      "Average test loss: 0.003457794439461496\n",
      "Epoch 54/300\n",
      "Average training loss: 0.24256847531265682\n",
      "Average test loss: 0.003346829389428927\n",
      "Epoch 55/300\n",
      "Average training loss: 0.20461524883906046\n",
      "Average test loss: 0.0032996556972050006\n",
      "Epoch 56/300\n",
      "Average training loss: 0.17707272289858925\n",
      "Average test loss: 0.0032848737680663664\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1604695530070199\n",
      "Average test loss: 0.003219580681166715\n",
      "Epoch 58/300\n",
      "Average training loss: 0.15086657840675777\n",
      "Average test loss: 0.003325947672335638\n",
      "Epoch 59/300\n",
      "Average training loss: 0.14409036535686917\n",
      "Average test loss: 0.003223361039741172\n",
      "Epoch 60/300\n",
      "Average training loss: 0.138664858367708\n",
      "Average test loss: 0.0032600807891123825\n",
      "Epoch 61/300\n",
      "Average training loss: 0.13433904638555313\n",
      "Average test loss: 0.0031312867394751973\n",
      "Epoch 62/300\n",
      "Average training loss: 0.13080407750606537\n",
      "Average test loss: 0.003156525131728914\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12785046710570652\n",
      "Average test loss: 0.003306113914690084\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12521504137251113\n",
      "Average test loss: 0.0031263564040677413\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12280706614255905\n",
      "Average test loss: 0.003134995253342721\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12074047470092773\n",
      "Average test loss: 0.0031565640109280746\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11918581180440055\n",
      "Average test loss: 0.0031164001350601515\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11895266451438267\n",
      "Average test loss: 0.0032498359742263953\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11600223086939918\n",
      "Average test loss: 0.0033561291450427636\n",
      "Epoch 70/300\n",
      "Average training loss: 0.1149417731364568\n",
      "Average test loss: 0.003097404379811552\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11390184923675326\n",
      "Average test loss: 0.003245130191039708\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11252667946947946\n",
      "Average test loss: 0.0033208407405763865\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11307796647482449\n",
      "Average test loss: 0.0033342479523271323\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11147809783617656\n",
      "Average test loss: 0.0032851479016244413\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1102873743640052\n",
      "Average test loss: 0.003293804054458936\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10993033558792538\n",
      "Average test loss: 0.003069006332506736\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11013234632545048\n",
      "Average test loss: 0.0031038144657181367\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10881790945927303\n",
      "Average test loss: 0.003110280734797319\n",
      "Epoch 79/300\n",
      "Average training loss: 0.12636485076612897\n",
      "Average test loss: 0.0030979493545989197\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11283996508518855\n",
      "Average test loss: 0.0030214394368231296\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11009207167228063\n",
      "Average test loss: 0.0030559441051963305\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10850142698817783\n",
      "Average test loss: 0.0031529366477496094\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10742783260345459\n",
      "Average test loss: 0.003204583688121703\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10821336071358786\n",
      "Average test loss: 0.003015276672525538\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10618734218676885\n",
      "Average test loss: 0.0032102374527603387\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10627421256899834\n",
      "Average test loss: 0.003060941360476944\n",
      "Epoch 87/300\n",
      "Average training loss: 0.105373093465964\n",
      "Average test loss: 0.003006240660014252\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10488033073478274\n",
      "Average test loss: 0.0033215472085608377\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10477517106135686\n",
      "Average test loss: 0.003136188550334838\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10498706309000651\n",
      "Average test loss: 0.0031125643946644335\n",
      "Epoch 91/300\n",
      "Average training loss: 0.103845946556992\n",
      "Average test loss: 0.003008800280280411\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10353516546885172\n",
      "Average test loss: 0.0030177883305069473\n",
      "Epoch 93/300\n",
      "Average training loss: 0.10349058535363939\n",
      "Average test loss: 0.003194928070116374\n",
      "Epoch 94/300\n",
      "Average training loss: 0.1050652018851704\n",
      "Average test loss: 0.0033755503464490176\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10215505952967538\n",
      "Average test loss: 0.0029943301673564645\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10194115369849735\n",
      "Average test loss: 0.003042519146680004\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10252447773350609\n",
      "Average test loss: 0.0038198342306746376\n",
      "Epoch 98/300\n",
      "Average training loss: 0.1013850583102968\n",
      "Average test loss: 0.0029445178334911665\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11878554689884185\n",
      "Average test loss: 0.00305350316543546\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11714588129520416\n",
      "Average test loss: 0.003040898024621937\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10434277283483082\n",
      "Average test loss: 0.002950448857827319\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10224885822004742\n",
      "Average test loss: 0.003087861661902732\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10146781850523419\n",
      "Average test loss: 0.00316716057765815\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10110660735766093\n",
      "Average test loss: 0.0029893440006093846\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10046957855092155\n",
      "Average test loss: 0.003118432161294752\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10239169659217198\n",
      "Average test loss: 0.0031172398020409874\n",
      "Epoch 107/300\n",
      "Average training loss: 0.1005335405535168\n",
      "Average test loss: 0.0030227689991394677\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09986991161770291\n",
      "Average test loss: 0.01230058221767346\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09999030248324077\n",
      "Average test loss: 0.002968541179680162\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09972384968068865\n",
      "Average test loss: 0.0030061177300910154\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09935864710145527\n",
      "Average test loss: 0.0033933746417363483\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09964032249980502\n",
      "Average test loss: 0.0029827595038546457\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09876983012755712\n",
      "Average test loss: 0.009700261060562399\n",
      "Epoch 114/300\n",
      "Average training loss: 0.1006892215344641\n",
      "Average test loss: 0.0031080116447475222\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09818813862734371\n",
      "Average test loss: 0.002985040625764264\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0986446182595359\n",
      "Average test loss: 2771.93790234375\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09833589064412647\n",
      "Average test loss: 0.0029902330639047757\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09785790604352951\n",
      "Average test loss: 0.002951705471509033\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09803512799739837\n",
      "Average test loss: 0.003035280563765102\n",
      "Epoch 120/300\n",
      "Average training loss: 0.09777965427769555\n",
      "Average test loss: 0.003118288216698501\n",
      "Epoch 121/300\n",
      "Average training loss: 0.09705419149001439\n",
      "Average test loss: 0.0031379444717119136\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09687989495860205\n",
      "Average test loss: 0.003154245361685753\n",
      "Epoch 123/300\n",
      "Average training loss: 0.09683624280823602\n",
      "Average test loss: 0.0030491038254565664\n",
      "Epoch 124/300\n",
      "Average training loss: 0.09654201192326016\n",
      "Average test loss: 0.0034658403475251462\n",
      "Epoch 125/300\n",
      "Average training loss: 0.09633330527279112\n",
      "Average test loss: 0.003017166869611376\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10248054463995827\n",
      "Average test loss: 0.0035220560063090587\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10726732747422324\n",
      "Average test loss: 0.0031757619985275797\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10176579180690977\n",
      "Average test loss: 0.004936000978160235\n",
      "Epoch 129/300\n",
      "Average training loss: 0.09906870014799966\n",
      "Average test loss: 0.0030101385814034274\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09681281543440289\n",
      "Average test loss: 0.0029656295573545826\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09794295669926538\n",
      "Average test loss: 0.002933798101213243\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0957898242407375\n",
      "Average test loss: 0.003092373240325186\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09563455216752158\n",
      "Average test loss: 0.003894388169257177\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09551401161485248\n",
      "Average test loss: 0.003168644575816062\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09524010707934698\n",
      "Average test loss: 0.003001094661756522\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09594059467315674\n",
      "Average test loss: 0.0029730416968878773\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09528140493896273\n",
      "Average test loss: 0.002993948221206665\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09492915023035474\n",
      "Average test loss: 0.0030555821638554334\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0949769910838869\n",
      "Average test loss: 0.0030342381708323954\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09425604006316926\n",
      "Average test loss: 0.0033958940042389764\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0943065921664238\n",
      "Average test loss: 0.039625322030650244\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09431245492895445\n",
      "Average test loss: 0.0030497545160146225\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09466357254650858\n",
      "Average test loss: 0.003020441286886732\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09385801781548395\n",
      "Average test loss: 0.0029930474727104107\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09420181975099776\n",
      "Average test loss: 0.0030750201230661735\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0935092927283711\n",
      "Average test loss: 0.002942070939267675\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09346965160634782\n",
      "Average test loss: 0.0029980607099003263\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09314773102601369\n",
      "Average test loss: 0.002995907757224308\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09416379984882142\n",
      "Average test loss: 0.0031284763461185824\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09288476425409317\n",
      "Average test loss: 0.0030106039378378128\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09334984484646056\n",
      "Average test loss: 0.0029732228072567118\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09266522132688099\n",
      "Average test loss: 0.0031451057261890837\n",
      "Epoch 153/300\n",
      "Average training loss: 0.1020502110256089\n",
      "Average test loss: 0.0037241202890872954\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10137967819637722\n",
      "Average test loss: 0.0029863037448376417\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09357863287793265\n",
      "Average test loss: 0.0029424622375518084\n",
      "Epoch 156/300\n",
      "Average training loss: 0.1150128038989173\n",
      "Average test loss: 0.007794832598004077\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10546911742952135\n",
      "Average test loss: 0.004084474864933226\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09614178797271516\n",
      "Average test loss: 0.002937761322491699\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09382664141390058\n",
      "Average test loss: 0.0029715162358350225\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09325742223527696\n",
      "Average test loss: 0.002975443848512239\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09231240230136448\n",
      "Average test loss: 0.0029896048878630004\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09224985624684227\n",
      "Average test loss: 0.0029684904350174798\n",
      "Epoch 163/300\n",
      "Average training loss: 0.092359241452482\n",
      "Average test loss: 0.002996736537044247\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09197526748312844\n",
      "Average test loss: 0.00297918575650288\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09286659893724654\n",
      "Average test loss: 0.002951419626052181\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09252273489369287\n",
      "Average test loss: 0.003306261560155286\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0919690193798807\n",
      "Average test loss: 0.0029873633668240575\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09210902080933253\n",
      "Average test loss: 0.0030162974616719618\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09162116600407494\n",
      "Average test loss: 0.0029894012990925047\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09170625585317611\n",
      "Average test loss: 0.0029976620854188997\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0925176005297237\n",
      "Average test loss: 0.002956854424956772\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09111579200956557\n",
      "Average test loss: 0.02109187211965521\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09125272461440828\n",
      "Average test loss: 0.0032961602591805987\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0913352854847908\n",
      "Average test loss: 0.004167327082405488\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09088436390956244\n",
      "Average test loss: 0.004594880453828308\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09068565122286479\n",
      "Average test loss: 0.0029920562940339247\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09082618268330891\n",
      "Average test loss: 0.0034880636208173303\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09055903411573833\n",
      "Average test loss: 0.003257725890725851\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09163659730884764\n",
      "Average test loss: 0.0029909467404294343\n",
      "Epoch 180/300\n",
      "Average training loss: 0.10681807361708746\n",
      "Average test loss: 0.0029866971710903775\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09320846006274223\n",
      "Average test loss: 0.0030063820174998706\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09109595916006301\n",
      "Average test loss: 0.00294987796805799\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09012196089824041\n",
      "Average test loss: 0.0031661561698549325\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08980257554848989\n",
      "Average test loss: 0.002971492966430055\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08957648570007748\n",
      "Average test loss: 0.003097471829917696\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09024471470382478\n",
      "Average test loss: 0.0029748919653809734\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08966485199663374\n",
      "Average test loss: 0.0029985951150043144\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08981870310836368\n",
      "Average test loss: 0.003071894726612502\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09012791063388188\n",
      "Average test loss: 0.0030791624347782797\n",
      "Epoch 190/300\n",
      "Average training loss: 0.090088908539878\n",
      "Average test loss: 0.0030316492209417952\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08959600432051552\n",
      "Average test loss: 0.0029760369078980554\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08920951693587834\n",
      "Average test loss: 0.004395673486507601\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08979897805717256\n",
      "Average test loss: 0.0030525718660404286\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08889517797364128\n",
      "Average test loss: 0.0031909600103067026\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08983491311470668\n",
      "Average test loss: 0.003090313879773021\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08944967023531596\n",
      "Average test loss: 0.023726968694064352\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08936556910143958\n",
      "Average test loss: 0.0030189636424183845\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08989603838655684\n",
      "Average test loss: 0.0030886011699007617\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08884196268187629\n",
      "Average test loss: 0.0030394624537891813\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08882229571210014\n",
      "Average test loss: 0.003506127441715863\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08827764109108183\n",
      "Average test loss: 0.0030925773113138146\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08829477215475506\n",
      "Average test loss: 0.00328115774359968\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08850818632708655\n",
      "Average test loss: 0.007405369741221269\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08783500193225013\n",
      "Average test loss: 0.0037015867543717224\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08863577477468385\n",
      "Average test loss: 0.003122170113854938\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08792418042818705\n",
      "Average test loss: 0.003104547046124935\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08824881247679392\n",
      "Average test loss: 0.003388584994814462\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08800850062900119\n",
      "Average test loss: 0.003005610198403398\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08747327927748362\n",
      "Average test loss: 0.0030555437819825278\n",
      "Epoch 212/300\n",
      "Average training loss: 0.087727057437102\n",
      "Average test loss: 0.003128829937428236\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0881591213080618\n",
      "Average test loss: 0.003238792996025748\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08755547243356705\n",
      "Average test loss: 0.003132506991840071\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08742811852693558\n",
      "Average test loss: 0.0034699069352613554\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08743164061837726\n",
      "Average test loss: 0.0030822942550811504\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08777517371376356\n",
      "Average test loss: 0.003889985599037674\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08690328467554516\n",
      "Average test loss: 0.003155177869937486\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08728509047958585\n",
      "Average test loss: 0.0031569433696568014\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08700170138147142\n",
      "Average test loss: 0.0030962159865432315\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08683962703413434\n",
      "Average test loss: 0.003187357691013151\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08694699280791812\n",
      "Average test loss: 0.0030873951709104908\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0867004181113508\n",
      "Average test loss: 0.0032117338973200985\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10990283309751087\n",
      "Average test loss: 0.0030117905092322166\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08761099549796846\n",
      "Average test loss: 0.0030616314245594874\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08624340539508396\n",
      "Average test loss: 0.003014881414257818\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08637555583318074\n",
      "Average test loss: 0.0030856344488759836\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0861009479827351\n",
      "Average test loss: 0.0030330796823319463\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08629079369703929\n",
      "Average test loss: 0.0030514844736705222\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08618758398294449\n",
      "Average test loss: 0.0032655834323830073\n",
      "Epoch 231/300\n",
      "Average training loss: 0.086789888381958\n",
      "Average test loss: 0.0031538048268606265\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08704798451397154\n",
      "Average test loss: 0.003785126170764367\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08608809394306607\n",
      "Average test loss: 0.0031028372301823565\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08618978224198023\n",
      "Average test loss: 0.0032029126797699266\n",
      "Epoch 235/300\n",
      "Average training loss: 0.086437612010373\n",
      "Average test loss: 0.003025306225029959\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08757619913750225\n",
      "Average test loss: 0.003140082300744123\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08620914778444502\n",
      "Average test loss: 0.003139209950135814\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08798363333940507\n",
      "Average test loss: 0.003317484758173426\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08635626345210605\n",
      "Average test loss: 0.003030440128718813\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08546027312676112\n",
      "Average test loss: 0.003660965847886271\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08598981432120005\n",
      "Average test loss: 0.003054955826451381\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0854386609726482\n",
      "Average test loss: 0.0031191771440207956\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08591851923863093\n",
      "Average test loss: 0.003190776489261124\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0856121359202597\n",
      "Average test loss: 0.0030648780595511197\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08576632610294554\n",
      "Average test loss: 0.0030280403467930024\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0856454535457823\n",
      "Average test loss: 0.00317590534604258\n",
      "Epoch 247/300\n",
      "Average training loss: 0.085564034514957\n",
      "Average test loss: 0.003127079385850165\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08600924045509763\n",
      "Average test loss: 0.0030715666337766583\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08499183777968089\n",
      "Average test loss: 0.0031829580534249546\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08540686506032943\n",
      "Average test loss: 0.0031206927002511093\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08624567807383007\n",
      "Average test loss: 0.0031608186564925644\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08662406579653421\n",
      "Average test loss: 0.0031062854605002537\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08709366350703769\n",
      "Average test loss: 0.0030371915658728944\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08520429479413562\n",
      "Average test loss: 0.0030940494181381333\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08558725993500815\n",
      "Average test loss: 0.0039426266472372745\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0850175877213478\n",
      "Average test loss: 0.004512586772855785\n",
      "Epoch 257/300\n",
      "Average training loss: 0.085293382184373\n",
      "Average test loss: 0.0031197217241343525\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08440253857109281\n",
      "Average test loss: 0.014012234267261293\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08593490806884235\n",
      "Average test loss: 0.003167927801195118\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08503190482987298\n",
      "Average test loss: 0.003267831153753731\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08505830624368456\n",
      "Average test loss: 0.0032252061255276204\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08469758964247173\n",
      "Average test loss: 0.003138966534493698\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08426421797275543\n",
      "Average test loss: 0.003632687865446011\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08484293636348512\n",
      "Average test loss: 0.0030939968764368032\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08442031383514405\n",
      "Average test loss: 0.003530744976053635\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08505247951216167\n",
      "Average test loss: 0.0031239202924900583\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08434935694932938\n",
      "Average test loss: 0.003160859655174944\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08420697254604763\n",
      "Average test loss: 0.003669108103753792\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08470890602138308\n",
      "Average test loss: 0.003056934927486711\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08445592854420345\n",
      "Average test loss: 0.0030731560784495538\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0846152538590961\n",
      "Average test loss: 0.0031266289767291813\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08498699700832367\n",
      "Average test loss: 0.0032247221575429042\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0842469541894065\n",
      "Average test loss: 0.0031054019559588696\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08443867809904947\n",
      "Average test loss: 0.003093562088906765\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08664353776640363\n",
      "Average test loss: 0.003132208729783694\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08426943741904365\n",
      "Average test loss: 0.0031410561982128356\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08408867800898022\n",
      "Average test loss: 0.003115927392202947\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08370659891102049\n",
      "Average test loss: 0.003124702232786351\n",
      "Epoch 279/300\n",
      "Average training loss: 0.3214294830494457\n",
      "Average test loss: 0.0031311215547223884\n",
      "Epoch 280/300\n",
      "Average training loss: 0.12323427615563075\n",
      "Average test loss: 0.0030660464066184227\n",
      "Epoch 281/300\n",
      "Average training loss: 0.10847531370984183\n",
      "Average test loss: 0.002996116902679205\n",
      "Epoch 282/300\n",
      "Average training loss: 0.1017682857910792\n",
      "Average test loss: 0.003001369631331828\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09736031967401504\n",
      "Average test loss: 0.003216440932721727\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09419512101014456\n",
      "Average test loss: 0.0030224580561949146\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09200232732958263\n",
      "Average test loss: 0.003124745931683315\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0902151412334707\n",
      "Average test loss: 0.0030538508821692733\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08878009685542848\n",
      "Average test loss: 0.0034424731756250064\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08756765272219975\n",
      "Average test loss: 0.003110285210940573\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08683051666948531\n",
      "Average test loss: 0.0030482928469363186\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08617410095532735\n",
      "Average test loss: 0.003017743460834026\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08590319579177433\n",
      "Average test loss: 4614.30074314944\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08561766626437504\n",
      "Average test loss: 0.0030940093385676544\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08535532901684444\n",
      "Average test loss: 0.003162087053474453\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08536808604664273\n",
      "Average test loss: 0.0035216940773857962\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08483715450101428\n",
      "Average test loss: 0.003251187718576855\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08463583599858814\n",
      "Average test loss: 0.0036140402058760326\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08580371797084808\n",
      "Average test loss: 0.003051796553656459\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08406196520063612\n",
      "Average test loss: 0.0030335973161790108\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08380234346124861\n",
      "Average test loss: 0.003459669772949484\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0840888391799397\n",
      "Average test loss: 0.003097952984687355\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.787231666988797\n",
      "Average test loss: 0.007927107694248358\n",
      "Epoch 2/300\n",
      "Average training loss: 1.9319870045979817\n",
      "Average test loss: 0.0062834644441803296\n",
      "Epoch 3/300\n",
      "Average training loss: 1.1939090186225043\n",
      "Average test loss: 0.005638798496789403\n",
      "Epoch 4/300\n",
      "Average training loss: 0.8451126266585456\n",
      "Average test loss: 0.004664208391888274\n",
      "Epoch 5/300\n",
      "Average training loss: 0.6416164019902547\n",
      "Average test loss: 0.004302238176266352\n",
      "Epoch 6/300\n",
      "Average training loss: 0.5111284188694424\n",
      "Average test loss: 0.0041633561075561576\n",
      "Epoch 7/300\n",
      "Average training loss: 0.41914848375320435\n",
      "Average test loss: 0.005199764048680663\n",
      "Epoch 8/300\n",
      "Average training loss: 0.35255687279171416\n",
      "Average test loss: 0.0038547074500885274\n",
      "Epoch 9/300\n",
      "Average training loss: 0.3027071446445253\n",
      "Average test loss: 0.0038967346830500495\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2653228898975584\n",
      "Average test loss: 0.0037833802132970756\n",
      "Epoch 11/300\n",
      "Average training loss: 0.2364479689862993\n",
      "Average test loss: 0.0034732345576501556\n",
      "Epoch 12/300\n",
      "Average training loss: 0.214704459561242\n",
      "Average test loss: 0.0035851850079165566\n",
      "Epoch 13/300\n",
      "Average training loss: 0.19674921578831142\n",
      "Average test loss: 0.003627856717341476\n",
      "Epoch 14/300\n",
      "Average training loss: 0.18379139092233446\n",
      "Average test loss: 0.0043254294830063975\n",
      "Epoch 15/300\n",
      "Average training loss: 0.17069738886091443\n",
      "Average test loss: 0.00835859795825349\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1612297506067488\n",
      "Average test loss: 0.0031144109103414747\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1540370445781284\n",
      "Average test loss: 0.003381157138488359\n",
      "Epoch 18/300\n",
      "Average training loss: 0.14709975544611614\n",
      "Average test loss: 0.0030033350557916692\n",
      "Epoch 19/300\n",
      "Average training loss: 0.14037939046488868\n",
      "Average test loss: 0.003082027483524548\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13581275301509432\n",
      "Average test loss: 0.0026928918407195146\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1300004222922855\n",
      "Average test loss: 0.002667587721513377\n",
      "Epoch 22/300\n",
      "Average training loss: 0.12531848992241754\n",
      "Average test loss: 0.0029629915588431886\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12152845203876496\n",
      "Average test loss: 0.0026676172285030287\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1175599846177631\n",
      "Average test loss: 0.0028170373301125234\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11330712173382441\n",
      "Average test loss: 0.0028244919427153135\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1097338557905621\n",
      "Average test loss: 0.0025061970448328388\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1068606414463785\n",
      "Average test loss: 0.00236922687664628\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10348604441351361\n",
      "Average test loss: 0.0024247256252500748\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10105249878433016\n",
      "Average test loss: 0.002807368570110864\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09791764401727253\n",
      "Average test loss: 0.00249812517190973\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09483818827072779\n",
      "Average test loss: 0.00226745591354039\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09221256019009484\n",
      "Average test loss: 0.002514446958485577\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09050028383069568\n",
      "Average test loss: 0.002288896917468972\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08868821932872137\n",
      "Average test loss: 0.00226923719379637\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08709763826926549\n",
      "Average test loss: 0.003260140481715401\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08547680512401792\n",
      "Average test loss: 0.0024258700952761703\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08357634922530915\n",
      "Average test loss: 0.0025698387411733467\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08509227446715037\n",
      "Average test loss: 0.0022445878243694704\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08763076324595345\n",
      "Average test loss: 0.0022520036773963107\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08479236396153768\n",
      "Average test loss: 0.002354248597804043\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09008566698100831\n",
      "Average test loss: 0.002236520727785925\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0827067192196846\n",
      "Average test loss: 0.002140408848722776\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08039519759350353\n",
      "Average test loss: 0.002153518978920248\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07961251967483096\n",
      "Average test loss: 0.0022138063771029314\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08023047782315149\n",
      "Average test loss: 0.0020986350905150175\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07812137610382504\n",
      "Average test loss: 0.002442303431737754\n",
      "Epoch 47/300\n",
      "Average training loss: 0.07746476473410924\n",
      "Average test loss: 0.002110232417575187\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07764209863874648\n",
      "Average test loss: 0.0021416024330796466\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07714408073822657\n",
      "Average test loss: 0.002438039535449611\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07664698521296183\n",
      "Average test loss: 0.002060538005704681\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0757857925163375\n",
      "Average test loss: 0.0021556529079874358\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07520239409804344\n",
      "Average test loss: 0.0020802127112531\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07529297603501214\n",
      "Average test loss: 0.0021205569324601026\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07455848903126187\n",
      "Average test loss: 0.002088889013044536\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07411353582806057\n",
      "Average test loss: 0.002043575936824911\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07692897585696644\n",
      "Average test loss: 0.00208709098967827\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07377697633041276\n",
      "Average test loss: 0.0020312659742517604\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07288020227352777\n",
      "Average test loss: 0.0020960035471038686\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07238924928506216\n",
      "Average test loss: 0.002050784994227191\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07215122675895691\n",
      "Average test loss: 0.0020267745641370616\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07298488640785217\n",
      "Average test loss: 0.0021164771330853304\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07201385433144039\n",
      "Average test loss: 0.0020091167180281548\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07111947431498104\n",
      "Average test loss: 0.0019673656494253212\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07142761784129673\n",
      "Average test loss: 0.0020608096975419255\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07085376193788316\n",
      "Average test loss: 0.001978621208005481\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07047907470332251\n",
      "Average test loss: 0.0019752874156046245\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07070640357997682\n",
      "Average test loss: 0.0019826236561768583\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0700643619497617\n",
      "Average test loss: 0.0020065335741059646\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07015900630421108\n",
      "Average test loss: 0.0020459666239718596\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07109283077716827\n",
      "Average test loss: 0.0024116383199062614\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06927460055549939\n",
      "Average test loss: 0.0020812137329743966\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0693747067881955\n",
      "Average test loss: 0.001981166931593584\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06910705495377381\n",
      "Average test loss: 0.002016340041326152\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06978441958294974\n",
      "Average test loss: 0.0020693777499513496\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06855540863341755\n",
      "Average test loss: 0.0019472518468068706\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06828040401803123\n",
      "Average test loss: 0.0019896895417736635\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06890413834320175\n",
      "Average test loss: 0.001976536294031474\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06850930517580774\n",
      "Average test loss: 0.0021433275732108286\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06765676471259859\n",
      "Average test loss: 0.0020020712684426043\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06808130629195107\n",
      "Average test loss: 0.002145384242447714\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06721911638975143\n",
      "Average test loss: 0.4524972945406205\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06774508472283682\n",
      "Average test loss: 0.002024483175741302\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0672483226855596\n",
      "Average test loss: 0.0019485708619985315\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06750302730003993\n",
      "Average test loss: 0.0019598281974386837\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07166556325885985\n",
      "Average test loss: 0.001964873575501972\n",
      "Epoch 86/300\n",
      "Average training loss: 0.0680594573020935\n",
      "Average test loss: 0.0020890638335711426\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06650655285517375\n",
      "Average test loss: 0.0019972305577248333\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06729225355386734\n",
      "Average test loss: 0.0019364596063064204\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0663055212828848\n",
      "Average test loss: 0.0020287211748460927\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06852395579881139\n",
      "Average test loss: 0.0019292973424825403\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0658071520196067\n",
      "Average test loss: 0.0020452546229999926\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06610582613945007\n",
      "Average test loss: 0.002281572069144911\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06665356317824787\n",
      "Average test loss: 0.002007852617858185\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06602425567309061\n",
      "Average test loss: 0.001969560980693334\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06575670364167956\n",
      "Average test loss: 0.0019250105956776275\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06552032074994511\n",
      "Average test loss: 0.0019749763581073947\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06624569590886434\n",
      "Average test loss: 0.001939672503206465\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06552867585751745\n",
      "Average test loss: 0.0021269366265171106\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06536219769385126\n",
      "Average test loss: 0.002685788349972831\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06593961396482256\n",
      "Average test loss: 0.0019167529993380109\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06500452553894785\n",
      "Average test loss: 0.0019217349263942903\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06574428739150365\n",
      "Average test loss: 0.001932201576522655\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06470857748058108\n",
      "Average test loss: 0.0019925867541589672\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06461417462759547\n",
      "Average test loss: 0.0019423802437053786\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0643915672534042\n",
      "Average test loss: 0.0019181448730329672\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06457531575693025\n",
      "Average test loss: 0.004102521954104305\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06457873048716122\n",
      "Average test loss: 0.0024655371171732744\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06485190730624729\n",
      "Average test loss: 0.0019281265371375613\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06667983004781936\n",
      "Average test loss: 0.0020370090713517533\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06483834331565433\n",
      "Average test loss: 0.0019485239020755722\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06406906280252668\n",
      "Average test loss: 0.001947074586318599\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06406658826271693\n",
      "Average test loss: 0.0020986613740937576\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06457970633440548\n",
      "Average test loss: 0.0019325663501189815\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06337712904148632\n",
      "Average test loss: 0.0021659378228295182\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06379245536857181\n",
      "Average test loss: 0.0020202463428593345\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06334021861023373\n",
      "Average test loss: 0.0019504856231311958\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06392526849110922\n",
      "Average test loss: 0.004366286058806711\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06343620975481139\n",
      "Average test loss: 0.0019787415315707523\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06381178511844741\n",
      "Average test loss: 0.002221889703327583\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06322577286097739\n",
      "Average test loss: 0.0019523402308631276\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06642198019888666\n",
      "Average test loss: 0.002043780317116115\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06289046672648854\n",
      "Average test loss: 0.002146792959007952\n",
      "Epoch 123/300\n",
      "Average training loss: 0.06291766662067837\n",
      "Average test loss: 0.001990500460896227\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06296537061201202\n",
      "Average test loss: 0.0021366766987161506\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06287347229652934\n",
      "Average test loss: 0.00196283181808475\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06254211107889811\n",
      "Average test loss: 0.002178877372501625\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06316222408745024\n",
      "Average test loss: 0.0019897670078401763\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06275379908084869\n",
      "Average test loss: 0.0019400935785637962\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06264403695861498\n",
      "Average test loss: 0.0020942972482492525\n",
      "Epoch 130/300\n",
      "Average training loss: 0.062459950771596696\n",
      "Average test loss: 0.0019638583759466807\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06266190040111541\n",
      "Average test loss: 0.002795636293788751\n",
      "Epoch 132/300\n",
      "Average training loss: 0.06330654634369744\n",
      "Average test loss: 0.0019809372569951746\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06230010688636038\n",
      "Average test loss: 0.0020816277489066125\n",
      "Epoch 134/300\n",
      "Average training loss: 0.061901372912857265\n",
      "Average test loss: 0.001953802988967962\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06217890723215209\n",
      "Average test loss: 0.0021841281075030564\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06233266921506988\n",
      "Average test loss: 0.0019465589723032383\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06199391044179599\n",
      "Average test loss: 0.001960428384029203\n",
      "Epoch 138/300\n",
      "Average training loss: 0.061910390853881836\n",
      "Average test loss: 0.0020021735715369383\n",
      "Epoch 139/300\n",
      "Average training loss: 0.062497266696559056\n",
      "Average test loss: 0.0021360970419935055\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06143278926610947\n",
      "Average test loss: 0.0019362052815655867\n",
      "Epoch 141/300\n",
      "Average training loss: 0.06189353185229831\n",
      "Average test loss: 0.0019688386159638564\n",
      "Epoch 142/300\n",
      "Average training loss: 0.06150929398669137\n",
      "Average test loss: 0.0020374606667707363\n",
      "Epoch 143/300\n",
      "Average training loss: 0.061463644898600046\n",
      "Average test loss: 0.001977949446481135\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06304733314116796\n",
      "Average test loss: 0.0020160653716367154\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06137381483448876\n",
      "Average test loss: 0.002989402427855465\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07790795078542498\n",
      "Average test loss: 0.002026112203796705\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06402314324511423\n",
      "Average test loss: 0.001953341687305106\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0614711347023646\n",
      "Average test loss: 0.0019642051404549017\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06108132233553463\n",
      "Average test loss: 0.0019301187499529786\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0607565153837204\n",
      "Average test loss: 0.0019777415131943094\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06100735593504376\n",
      "Average test loss: 0.002103222375942601\n",
      "Epoch 152/300\n",
      "Average training loss: 0.061235072116057075\n",
      "Average test loss: 0.003670867133885622\n",
      "Epoch 153/300\n",
      "Average training loss: 0.060726432111528186\n",
      "Average test loss: 0.0019608556951085728\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06078656896948814\n",
      "Average test loss: 0.0020336747751053836\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06397296129994923\n",
      "Average test loss: 0.0019389869436207746\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06056158650252554\n",
      "Average test loss: 1.9588400993744532\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06061157775256369\n",
      "Average test loss: 0.0022943951731754673\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06114066626959377\n",
      "Average test loss: 0.0020160277056404286\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06053409552574158\n",
      "Average test loss: 0.0019525859579961333\n",
      "Epoch 160/300\n",
      "Average training loss: 0.061448675649033654\n",
      "Average test loss: 0.0021550512241406573\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06046146794822481\n",
      "Average test loss: 0.0021211175568815735\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06071439519524574\n",
      "Average test loss: 0.0020882615855791504\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06033336360587014\n",
      "Average test loss: 0.001948004812312623\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06033209713962343\n",
      "Average test loss: 0.0027231001311706173\n",
      "Epoch 165/300\n",
      "Average training loss: 0.060598955012030074\n",
      "Average test loss: 0.001962002264542712\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06025284308857388\n",
      "Average test loss: 0.001981070724833343\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06096934445036782\n",
      "Average test loss: 0.0019702620123409564\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06039720915754636\n",
      "Average test loss: 0.0027368548752533064\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0599382595287429\n",
      "Average test loss: 0.00251674369805389\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06049182416001956\n",
      "Average test loss: 0.0019696371539288924\n",
      "Epoch 171/300\n",
      "Average training loss: 0.060680334130922955\n",
      "Average test loss: 1.7048400619659159\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06051410156488419\n",
      "Average test loss: 0.001972530357022252\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05949503921469053\n",
      "Average test loss: 0.0019866737466719417\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05969939432872666\n",
      "Average test loss: 0.002073145979911917\n",
      "Epoch 175/300\n",
      "Average training loss: 0.059784170548121136\n",
      "Average test loss: 0.002217912466161781\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0599961430033048\n",
      "Average test loss: 0.016125248885816998\n",
      "Epoch 177/300\n",
      "Average training loss: 0.059782779216766356\n",
      "Average test loss: 0.0019895786607844962\n",
      "Epoch 178/300\n",
      "Average training loss: 0.059937104668882156\n",
      "Average test loss: 0.0019907452813867064\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05981919244925181\n",
      "Average test loss: 0.0020426797972371183\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05992688492933909\n",
      "Average test loss: 0.0021185800976430374\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05953643351793289\n",
      "Average test loss: 0.0021645566730035676\n",
      "Epoch 182/300\n",
      "Average training loss: 0.059432214852836396\n",
      "Average test loss: 0.002015053879883554\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0593774915933609\n",
      "Average test loss: 0.0023575312226182883\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05922059379352464\n",
      "Average test loss: 0.0020099736704594557\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05950815875331561\n",
      "Average test loss: 0.0020283958657334247\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05950169723563724\n",
      "Average test loss: 0.0020239807086893254\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06025968714555104\n",
      "Average test loss: 0.002000863496835033\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0592133694920275\n",
      "Average test loss: 0.0024289068933576346\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05971891686320305\n",
      "Average test loss: 0.08948878923720784\n",
      "Epoch 190/300\n",
      "Average training loss: 0.058932875133223005\n",
      "Average test loss: 0.002080574698332283\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05939952104621463\n",
      "Average test loss: 0.003308225037323104\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05925997567176819\n",
      "Average test loss: 0.019811124386472836\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05927325534158283\n",
      "Average test loss: 0.011820296898070308\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05924144947859976\n",
      "Average test loss: 0.0020318838492449785\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05971666101283497\n",
      "Average test loss: 0.002041984805216392\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05877673292160034\n",
      "Average test loss: 0.001997886517706017\n",
      "Epoch 197/300\n",
      "Average training loss: 0.058707126604186165\n",
      "Average test loss: 0.002221006083819601\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05870091390609741\n",
      "Average test loss: 0.002018200676474306\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05906236065427462\n",
      "Average test loss: 0.00216879499082764\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05879838477902942\n",
      "Average test loss: 0.0020329498352689877\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05860467976000574\n",
      "Average test loss: 0.0022339763487171796\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0590352056423823\n",
      "Average test loss: 0.0021363010165385073\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05919135285417239\n",
      "Average test loss: 0.0019820272352339494\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05820714751548237\n",
      "Average test loss: 0.0021775779938325284\n",
      "Epoch 205/300\n",
      "Average training loss: 0.05885032922360632\n",
      "Average test loss: 0.0021857412537145946\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05889391164978345\n",
      "Average test loss: 0.0023639470494041842\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05840086041225327\n",
      "Average test loss: 0.0020194723070081737\n",
      "Epoch 208/300\n",
      "Average training loss: 0.058330216705799104\n",
      "Average test loss: 0.0020039069306933216\n",
      "Epoch 209/300\n",
      "Average training loss: 0.058294745723406476\n",
      "Average test loss: 0.0022323491747180623\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05876239430573252\n",
      "Average test loss: 0.0021307138539850713\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05986684903171327\n",
      "Average test loss: 0.0019880905161715214\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05815836015012529\n",
      "Average test loss: 0.001988387191668153\n",
      "Epoch 213/300\n",
      "Average training loss: 0.058651491863860025\n",
      "Average test loss: 0.00206316809852918\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0582604102326764\n",
      "Average test loss: 0.002297352532959647\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05846698160303963\n",
      "Average test loss: 0.008771967546807395\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05863337358501222\n",
      "Average test loss: 0.0020909748931104938\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05793441306220161\n",
      "Average test loss: 0.002009803847409785\n",
      "Epoch 218/300\n",
      "Average training loss: 0.057977496471669936\n",
      "Average test loss: 0.002831582201127377\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05827630037400457\n",
      "Average test loss: 0.00214449362612019\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05790758106774754\n",
      "Average test loss: 0.0020383421545848253\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05778536056478818\n",
      "Average test loss: 0.002682603781421979\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05840290165940921\n",
      "Average test loss: 0.002133260195971363\n",
      "Epoch 223/300\n",
      "Average training loss: 0.057710239940219454\n",
      "Average test loss: 0.00202732645554675\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05812345455090205\n",
      "Average test loss: 0.002003101984866791\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05819868665933609\n",
      "Average test loss: 0.0020324547458440067\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05775551888677809\n",
      "Average test loss: 0.0020565597719202438\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05824609966410531\n",
      "Average test loss: 0.0021390208195274075\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05789855476882723\n",
      "Average test loss: 0.002462304811924696\n",
      "Epoch 229/300\n",
      "Average training loss: 0.059440315743287404\n",
      "Average test loss: 0.002020375009212229\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05778170970744557\n",
      "Average test loss: 0.0020484007987090283\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05785890955064032\n",
      "Average test loss: 0.0023632926595293815\n",
      "Epoch 232/300\n",
      "Average training loss: 0.057510533471902214\n",
      "Average test loss: 0.0020681510962959793\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05721448311209679\n",
      "Average test loss: 0.0020283949921528497\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05807230952713224\n",
      "Average test loss: 279715649029.12\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05851316870252291\n",
      "Average test loss: 0.002160569563508034\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05701832543479072\n",
      "Average test loss: 0.002019229700581895\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05748302870988846\n",
      "Average test loss: 0.0028191826958209277\n",
      "Epoch 238/300\n",
      "Average training loss: 0.057919274843401376\n",
      "Average test loss: 0.0020855693178147903\n",
      "Epoch 239/300\n",
      "Average training loss: 0.057399056156476336\n",
      "Average test loss: 0.0023190460950136185\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05733906275033951\n",
      "Average test loss: 0.0028620511562459997\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05895767696036233\n",
      "Average test loss: 0.0021020907846589884\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05723056222663985\n",
      "Average test loss: 814.7459866841634\n",
      "Epoch 243/300\n",
      "Average training loss: 0.058166493064827386\n",
      "Average test loss: 0.00202965342419015\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05709794013036622\n",
      "Average test loss: 0.00209758632702546\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05686407971713278\n",
      "Average test loss: 0.002095605770746867\n",
      "Epoch 246/300\n",
      "Average training loss: 0.059937695364157356\n",
      "Average test loss: 0.0021143041060616573\n",
      "Epoch 247/300\n",
      "Average training loss: 0.057228982716798785\n",
      "Average test loss: 0.00203791235656374\n",
      "Epoch 248/300\n",
      "Average training loss: 0.056768228388494914\n",
      "Average test loss: 0.0020503034635136527\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05708295523458057\n",
      "Average test loss: 0.0020828731659178934\n",
      "Epoch 250/300\n",
      "Average training loss: 0.057095827493402695\n",
      "Average test loss: 0.0022258765155242547\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06189729113048977\n",
      "Average test loss: 0.002005831783378704\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05701390602522426\n",
      "Average test loss: 0.0022352563275231255\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05783538597159916\n",
      "Average test loss: 0.0020531213235937887\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05665185206466251\n",
      "Average test loss: 0.0022516676479329666\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05702222948935297\n",
      "Average test loss: 0.00203092703782022\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05678918351729711\n",
      "Average test loss: 0.7827924830781089\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05717287503348457\n",
      "Average test loss: 0.0020433893131299153\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05704403199089898\n",
      "Average test loss: 0.002079071497544646\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05767038067513042\n",
      "Average test loss: 0.0020553601079930863\n",
      "Epoch 260/300\n",
      "Average training loss: 0.056834993362426756\n",
      "Average test loss: 0.0020235460655142865\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05703978869650099\n",
      "Average test loss: 0.002031461075362232\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05713957897159788\n",
      "Average test loss: 0.002405585053480334\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05690878776046965\n",
      "Average test loss: 0.03609036836855942\n",
      "Epoch 264/300\n",
      "Average training loss: 0.057134046024746366\n",
      "Average test loss: 0.002085407758131623\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05698620120353169\n",
      "Average test loss: 0.002983031876178251\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05714537364906735\n",
      "Average test loss: 0.002054639770338933\n",
      "Epoch 267/300\n",
      "Average training loss: 0.057051115251249736\n",
      "Average test loss: 0.0021775266215619113\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05687072563502524\n",
      "Average test loss: 0.0020467829073054925\n",
      "Epoch 269/300\n",
      "Average training loss: 0.056620868467622334\n",
      "Average test loss: 0.002076001053262088\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05639640540546841\n",
      "Average test loss: 0.0023897490123700764\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05691096023718516\n",
      "Average test loss: 0.0020425739669137533\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05672284551130401\n",
      "Average test loss: 0.0020244214261571566\n",
      "Epoch 273/300\n",
      "Average training loss: 0.057424526863627964\n",
      "Average test loss: 356.6606884901259\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05675964142547713\n",
      "Average test loss: 0.0020678140407221187\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05648045270972782\n",
      "Average test loss: 0.0020542088526611527\n",
      "Epoch 276/300\n",
      "Average training loss: 0.056491457025210065\n",
      "Average test loss: 0.00204225897250904\n",
      "Epoch 277/300\n",
      "Average training loss: 0.056496953020493186\n",
      "Average test loss: 0.002136804253483812\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05652046470509635\n",
      "Average test loss: 0.0020328464067230624\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05662793632513947\n",
      "Average test loss: 0.0021135060958978203\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05669372335738606\n",
      "Average test loss: 0.0025576187999298175\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05676512881120046\n",
      "Average test loss: 0.0021699406008960473\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05630554757515589\n",
      "Average test loss: 0.0020558165377005935\n",
      "Epoch 283/300\n",
      "Average training loss: 0.056929721342192756\n",
      "Average test loss: 0.0020525688897404405\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05617896136310366\n",
      "Average test loss: 0.0020679728231496283\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05657106442583932\n",
      "Average test loss: 0.002057871000116898\n",
      "Epoch 286/300\n",
      "Average training loss: 0.056272805386119416\n",
      "Average test loss: 0.0025201435834169388\n",
      "Epoch 287/300\n",
      "Average training loss: 0.056722840173376934\n",
      "Average test loss: 0.0020996990415992007\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05593801265292697\n",
      "Average test loss: 0.0020758001069641776\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05668564006355074\n",
      "Average test loss: 0.0021514130812138317\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05611457261443138\n",
      "Average test loss: 0.002054930577468541\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05667727593580882\n",
      "Average test loss: 0.0020611294577312137\n",
      "Epoch 292/300\n",
      "Average training loss: 0.056509512305259704\n",
      "Average test loss: 0.0024523635972291233\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05599041450354788\n",
      "Average test loss: 0.0022524343246801033\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05604968495832549\n",
      "Average test loss: 0.0025203309459611773\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05631910411516825\n",
      "Average test loss: 0.002081003510393202\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05655608085460133\n",
      "Average test loss: 0.0020568860826186007\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05586627504229546\n",
      "Average test loss: 0.00271098385606375\n",
      "Epoch 298/300\n",
      "Average training loss: 0.055993186212248275\n",
      "Average test loss: 0.002055926963686943\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05611987317932977\n",
      "Average test loss: 0.06735568643278546\n",
      "Epoch 300/300\n",
      "Average training loss: 0.058510123954878916\n",
      "Average test loss: 0.051437671780586246\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1310.8037646959092\n",
      "Average test loss: 0.09679577530754938\n",
      "Epoch 2/300\n",
      "Average training loss: 15.547376926845974\n",
      "Average test loss: 0.023223228651616308\n",
      "Epoch 3/300\n",
      "Average training loss: 11.268596597459581\n",
      "Average test loss: 0.022564915924436515\n",
      "Epoch 4/300\n",
      "Average training loss: 8.656423989189996\n",
      "Average test loss: 0.010548226198388471\n",
      "Epoch 5/300\n",
      "Average training loss: 6.8991377182006834\n",
      "Average test loss: 0.00898295165432824\n",
      "Epoch 6/300\n",
      "Average training loss: 5.64338999387953\n",
      "Average test loss: 0.011320167544401355\n",
      "Epoch 7/300\n",
      "Average training loss: 4.731180904388427\n",
      "Average test loss: 0.07005453927566607\n",
      "Epoch 8/300\n",
      "Average training loss: 3.9830917010837132\n",
      "Average test loss: 0.0058956778736578095\n",
      "Epoch 9/300\n",
      "Average training loss: 3.4342867584228514\n",
      "Average test loss: 0.005502476901643806\n",
      "Epoch 10/300\n",
      "Average training loss: 2.9032145493825277\n",
      "Average test loss: 0.10782446203587784\n",
      "Epoch 11/300\n",
      "Average training loss: 2.4911162486606173\n",
      "Average test loss: 0.06377571228974395\n",
      "Epoch 12/300\n",
      "Average training loss: 2.157458381017049\n",
      "Average test loss: 0.004530435607044234\n",
      "Epoch 13/300\n",
      "Average training loss: 1.8738713193469578\n",
      "Average test loss: 0.004262135146806637\n",
      "Epoch 14/300\n",
      "Average training loss: 1.6226265720791286\n",
      "Average test loss: 0.004096616022702721\n",
      "Epoch 15/300\n",
      "Average training loss: 1.405831672668457\n",
      "Average test loss: 0.003928916707634926\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2181436598036024\n",
      "Average test loss: 0.0037446707660953203\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0550698357688055\n",
      "Average test loss: 0.0037520194657974775\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9139567607773675\n",
      "Average test loss: 0.0034049049484440024\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7899398095872667\n",
      "Average test loss: 0.00352631510876947\n",
      "Epoch 20/300\n",
      "Average training loss: 0.682463828086853\n",
      "Average test loss: 0.0033858051430433987\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5890421597162883\n",
      "Average test loss: 0.0032774985567149187\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5081078305774265\n",
      "Average test loss: 0.003051355785379807\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4385377694765727\n",
      "Average test loss: 0.002980081126300825\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3811009084913466\n",
      "Average test loss: 0.0029396350160241125\n",
      "Epoch 25/300\n",
      "Average training loss: 0.3329850297768911\n",
      "Average test loss: 0.003098391890318857\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2921975430117713\n",
      "Average test loss: 0.0026563402759946053\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2592919840282864\n",
      "Average test loss: 0.0025837545440428786\n",
      "Epoch 28/300\n",
      "Average training loss: 0.23108573151959313\n",
      "Average test loss: 0.0030770006645470856\n",
      "Epoch 29/300\n",
      "Average training loss: 0.20803700575563644\n",
      "Average test loss: 0.0025004743691533805\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1895718885262807\n",
      "Average test loss: 0.002379772877113687\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1740745640065935\n",
      "Average test loss: 0.002412749746400449\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15989036508401236\n",
      "Average test loss: 0.002415934681892395\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1488851076496972\n",
      "Average test loss: 0.00239871435219215\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1400353175666597\n",
      "Average test loss: 0.0021887928831080595\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13152002996206283\n",
      "Average test loss: 0.0022719569616019724\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12596334714359708\n",
      "Average test loss: 0.0021120070181787015\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11950147777133517\n",
      "Average test loss: 0.0020047956496063207\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1155199699997902\n",
      "Average test loss: 0.002096419493978222\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10975142339203092\n",
      "Average test loss: 0.0019279622309323814\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10638711896869871\n",
      "Average test loss: 0.0021330477527032295\n",
      "Epoch 41/300\n",
      "Average training loss: 0.188584310789903\n",
      "Average test loss: 0.0023162115435633396\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10757276836368773\n",
      "Average test loss: 0.002215813942046629\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09859101726942592\n",
      "Average test loss: 0.0019242719182123742\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09398530595170126\n",
      "Average test loss: 0.002042871448935734\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0901188300119506\n",
      "Average test loss: 0.0017741961342593033\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08710265213913387\n",
      "Average test loss: 0.0017748006967206796\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08429527181718084\n",
      "Average test loss: 0.0020435398204459083\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08173021434413062\n",
      "Average test loss: 0.0017487370791948503\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08046982769171397\n",
      "Average test loss: 0.0018827600152128273\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07886847124497096\n",
      "Average test loss: 0.0017452020835545328\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07515174926651849\n",
      "Average test loss: 0.0017377091815902127\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07315246615807215\n",
      "Average test loss: 0.0018427142220445805\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07178580301337772\n",
      "Average test loss: 0.0017001781153182188\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07045429206887881\n",
      "Average test loss: 0.0015680051058944728\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0687354005144702\n",
      "Average test loss: 0.0015569245926001006\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06938702819082472\n",
      "Average test loss: 0.0016652134441667132\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06494867783122593\n",
      "Average test loss: 0.0014975399140579005\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0645802365872595\n",
      "Average test loss: 0.001633029205000235\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06344859652386771\n",
      "Average test loss: 0.0015916844890970323\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06257293508781327\n",
      "Average test loss: 0.0015615899557144277\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06090163675612874\n",
      "Average test loss: 0.0015650023507575194\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06028356519341469\n",
      "Average test loss: 0.0015588122691131301\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0641905890736315\n",
      "Average test loss: 0.0015271680146041844\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05928220659163263\n",
      "Average test loss: 0.0014457064375488294\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05953707078099251\n",
      "Average test loss: 0.0014892742166088687\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05824792824188868\n",
      "Average test loss: 0.0035535849450776973\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05859767504533132\n",
      "Average test loss: 0.001514819907852345\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05950866556498739\n",
      "Average test loss: 0.001482946256434338\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05697695803642273\n",
      "Average test loss: 0.0015996058258331484\n",
      "Epoch 73/300\n",
      "Average training loss: 0.056792458968030084\n",
      "Average test loss: 0.0014642107639875677\n",
      "Epoch 74/300\n",
      "Average training loss: 0.055952071633603835\n",
      "Average test loss: 0.0014104649041675858\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05556626187430488\n",
      "Average test loss: 0.0014247858948591683\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05691151906715499\n",
      "Average test loss: 0.0022311439245111414\n",
      "Epoch 78/300\n",
      "Average training loss: 0.054894319143560195\n",
      "Average test loss: 0.0014945540697210364\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05501679968833923\n",
      "Average test loss: 0.0014458653164199657\n",
      "Epoch 80/300\n",
      "Average training loss: 0.054348257968823116\n",
      "Average test loss: 0.0015184655742098887\n",
      "Epoch 81/300\n",
      "Average training loss: 0.055493456224600474\n",
      "Average test loss: 0.001453612890922361\n",
      "Epoch 82/300\n",
      "Average training loss: 0.053721351775858135\n",
      "Average test loss: 0.001461114349981977\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05430111523138152\n",
      "Average test loss: 0.0013789480414448513\n",
      "Epoch 84/300\n",
      "Average training loss: 0.053217964142560956\n",
      "Average test loss: 0.0014009663988318708\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05308048650953505\n",
      "Average test loss: 0.0014229552570937409\n",
      "Epoch 86/300\n",
      "Average training loss: 0.052851031565003925\n",
      "Average test loss: 0.0013660519929188822\n",
      "Epoch 87/300\n",
      "Average test loss: 0.001643159852984051\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05230449648367034\n",
      "Average test loss: 0.0014333710862427123\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05208897003200319\n",
      "Average test loss: 0.0013721774550568726\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05207407387925519\n",
      "Average test loss: 0.0014006902844541603\n",
      "Epoch 92/300\n",
      "Average training loss: 0.051487962308857174\n",
      "Average test loss: 0.0013792829459740056\n",
      "Epoch 93/300\n",
      "Average training loss: 0.052688032825787864\n",
      "Average test loss: 0.001508272865580188\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05098348293039534\n",
      "Average test loss: 0.0013758021253678534\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05107690447568893\n",
      "Average test loss: 0.0013477102453923887\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05093125686711735\n",
      "Average test loss: 0.001419564078665442\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05082390058702893\n",
      "Average test loss: 0.0029865264991919198\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05061594956119855\n",
      "Average test loss: 0.0014991484744888213\n",
      "Epoch 99/300\n",
      "Average training loss: 0.050675440669059754\n",
      "Average test loss: 0.0013297867278258006\n",
      "Epoch 100/300\n",
      "Average training loss: 0.050390130294693844\n",
      "Average test loss: 0.0013444397308760218\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05066088141004244\n",
      "Average test loss: 0.0015741784719543325\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04974752339720726\n",
      "Average test loss: 0.0014884388523383272\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04976676501830419\n",
      "Average test loss: 0.0019895263962033723\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04994652897781796\n",
      "Average test loss: 0.0013434550481744938\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04946571913030413\n",
      "Average test loss: 0.0014133862839597795\n",
      "Epoch 106/300\n",
      "Average training loss: 0.049219749814934204\n",
      "Average test loss: 0.0014196797890795602\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0497043672915962\n",
      "Average test loss: 0.002110868568221728\n",
      "Epoch 110/300\n",
      "Average training loss: 0.048694208823972275\n",
      "Average test loss: 0.0013313466565062603\n",
      "Epoch 111/300\n",
      "Average training loss: 0.048980883876482646\n",
      "Average test loss: 0.02007532820229729\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0490404800656769\n",
      "Average test loss: 0.0013289641229849722\n",
      "Epoch 113/300\n",
      "Average training loss: 0.049219098051389056\n",
      "Average test loss: 0.001324800525015841\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04834625967343648\n",
      "Average test loss: 0.0014220212349254223\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0483233716653453\n",
      "Average test loss: 0.019230173587799073\n",
      "Epoch 116/300\n",
      "Average training loss: 0.048499204190240966\n",
      "Average test loss: 0.0013679256059436335\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04836047192083465\n",
      "Average test loss: 0.0013327711927187112\n",
      "Epoch 118/300\n",
      "Average training loss: 0.048952486114369496\n",
      "Average test loss: 0.0013983293629458381\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04799849559532272\n",
      "Average test loss: 0.0015307747793073456\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04927166761954625\n",
      "Average test loss: 0.0033943754993379116\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04805388527115186\n",
      "Average test loss: 0.0013204698629884256\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04765588394138548\n",
      "Average test loss: 0.0013418369830275575\n",
      "Epoch 123/300\n",
      "Average training loss: 0.048546886950731276\n",
      "Average test loss: 0.001338766908273101\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04746810713410377\n",
      "Average test loss: 0.0013193789213481876\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04747174638178613\n",
      "Average test loss: 0.0015179101828899648\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04802794284953012\n",
      "Average test loss: 0.001389907133558558\n",
      "Epoch 128/300\n",
      "Average training loss: 0.04779265546467569\n",
      "Average test loss: 0.0014121168021423121\n",
      "Epoch 129/300\n",
      "Average training loss: 0.047135365969604916\n",
      "Average test loss: 0.0024044213582658106\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04725769724448522\n",
      "Average test loss: 0.3176967366122537\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04800761429137654\n",
      "Average test loss: 0.001545085486335059\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0470274315211508\n",
      "Average test loss: 0.001358397948038247\n",
      "Epoch 133/300\n",
      "Average training loss: 0.046751061177916\n",
      "Average test loss: 0.001387333226390183\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04695073919826084\n",
      "Average test loss: 0.00215438426234242\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0468380353682571\n",
      "Average test loss: 0.0014943590671237972\n",
      "Epoch 136/300\n",
      "Average training loss: 0.047644888208972086\n",
      "Average test loss: 0.0013249087505456474\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04647601146168179\n",
      "Average test loss: 0.0015120152972845568\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0468467240565353\n",
      "Average test loss: 0.001988957544788718\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04683983189198706\n",
      "Average test loss: 0.01699371995611323\n",
      "Epoch 140/300\n",
      "Average training loss: 0.046337958580917785\n",
      "Average test loss: 0.0013675163274423944\n",
      "Epoch 141/300\n",
      "Average training loss: 0.046241860568523405\n",
      "Average test loss: 0.0013522357273743385\n",
      "Epoch 144/300\n",
      "Average training loss: 0.046266893674929935\n",
      "Average test loss: 0.0014216419949920642\n",
      "Epoch 145/300\n",
      "Average training loss: 0.045955996331241394\n",
      "Average test loss: 0.0013514500655647781\n",
      "Epoch 146/300\n",
      "Average training loss: 0.046532014813688065\n",
      "Average test loss: 0.0014349102829065588\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04598507208294338\n",
      "Average test loss: 0.001405301205193003\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04668378429611524\n",
      "Average test loss: 0.006356661573673288\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04590177356203397\n",
      "Average test loss: 0.0013624030908362733\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04594562520914607\n",
      "Average test loss: 0.0014334769775159657\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04672798391845491\n",
      "Average test loss: 0.0023246946404170657\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0468970257739226\n",
      "Average test loss: 0.017249410262538328\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04544966884785228\n",
      "Average test loss: 0.0013471752250981\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04572306432657772\n",
      "Average test loss: 0.003137414293984572\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04546141751938396\n",
      "Average test loss: 0.0013315497796154686\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0455561281144619\n",
      "Average test loss: 0.0014024876027057568\n",
      "Epoch 157/300\n",
      "Average training loss: 0.045261827412578795\n",
      "Average test loss: 0.0013581400882038805\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04757530364394188\n",
      "Average test loss: 0.0013486671924798025\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0452840226524406\n",
      "Average test loss: 0.0015777772786095738\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04535173722770479\n",
      "Average test loss: 0.0013521336430890693\n",
      "Epoch 163/300\n",
      "Average training loss: 0.046042633924219346\n",
      "Average test loss: 0.001369613794195983\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04510710610946019\n",
      "Average test loss: 0.001364855290700992\n",
      "Epoch 165/300\n",
      "Average training loss: 0.045720550676186876\n",
      "Average test loss: 0.0013459080850912466\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04530526949299706\n",
      "Average test loss: 0.0013569652091504798\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04477108388808038\n",
      "Average test loss: 0.0014145824648439883\n",
      "Epoch 168/300\n",
      "Average training loss: 0.044892332553863525\n",
      "Average test loss: 0.001475303258953823\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04480481774277157\n",
      "Average test loss: 0.001424038770608604\n",
      "Epoch 170/300\n",
      "Average training loss: 0.044881594704257115\n",
      "Average test loss: 0.009802350333788328\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0450403533577919\n",
      "Average test loss: 0.0015100808855560092\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04476414495375421\n",
      "Average test loss: 0.001452578165464931\n",
      "Epoch 173/300\n",
      "Average training loss: 0.045040291325913535\n",
      "Average test loss: 0.001436371166879932\n",
      "Epoch 174/300\n",
      "Average training loss: 0.045536137064297995\n",
      "Average test loss: 0.2477123623092969\n",
      "Epoch 175/300\n",
      "Average training loss: 0.044619116488430234\n",
      "Average test loss: 0.0013990804859333567\n",
      "Epoch 176/300\n",
      "Average training loss: 0.044778319583998784\n",
      "Average test loss: 0.0013787542259734538\n",
      "Epoch 177/300\n",
      "Average training loss: 0.044325485451353924\n",
      "Average test loss: 0.001415583934997105\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05098942456642787\n",
      "Average test loss: 0.0013852366214204166\n",
      "Epoch 179/300\n",
      "Average training loss: 0.045257122255033914\n",
      "Average test loss: 0.0013564558719388314\n",
      "Epoch 180/300\n",
      "Average training loss: 0.044011579669184155\n",
      "Average test loss: 0.0013760368592209285\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04612014163037141\n",
      "Average test loss: 0.0014050112538971007\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0441871381584141\n",
      "Average test loss: 0.0013639991161827412\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04399830605917507\n",
      "Average test loss: 0.0013710155382545457\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04403762545850542\n",
      "Average test loss: 0.001687336198364695\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04397264823317528\n",
      "Average test loss: 0.0029880789087878335\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04441563889053133\n",
      "Average test loss: 0.0026103997542005447\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04470767648352517\n",
      "Average test loss: 0.001359705625101924\n",
      "Epoch 190/300\n",
      "Average training loss: 0.044275761591063605\n",
      "Average test loss: 0.0014445541129551\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04376551121473313\n",
      "Average test loss: 0.0013609737352364593\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04380007076263428\n",
      "Average test loss: 0.0015726801788227425\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04401479200522105\n",
      "Average test loss: 0.0014187358545346392\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04392990317278438\n",
      "Average test loss: 0.0013787610131419368\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04414979766143693\n",
      "Average test loss: 0.0019620689580010045\n",
      "Epoch 196/300\n",
      "Average training loss: 0.043878192835383946\n",
      "Average test loss: 0.0014809831952055294\n",
      "Epoch 197/300\n",
      "Average training loss: 0.044007275745272634\n",
      "Average test loss: 0.008867650889688068\n",
      "Epoch 198/300\n",
      "Average training loss: 0.044315415079394974\n",
      "Average test loss: 0.001392643911867506\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04386133355233404\n",
      "Average test loss: 0.0013746465022365251\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04367808157702287\n",
      "Average test loss: 0.0013773056286283665\n",
      "Epoch 201/300\n",
      "Average training loss: 0.043995239820745255\n",
      "Average test loss: 0.00163996080091844\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04350559355484115\n",
      "Average test loss: 0.0015229513929742905\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04388979489935769\n",
      "Average test loss: 0.0016517827208671304\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04366749537653393\n",
      "Average test loss: 0.0013781909862429732\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04344289267063141\n",
      "Average test loss: 0.0013830697688067125\n",
      "Epoch 208/300\n",
      "Average training loss: 0.043520042929384446\n",
      "Average test loss: 0.0014401112451321548\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04341393330362108\n",
      "Average test loss: 0.0013971858962128559\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04359026244944996\n",
      "Average test loss: 0.003868241672921512\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04307793261938625\n",
      "Average test loss: 0.0013877936977272232\n",
      "Epoch 212/300\n",
      "Average training loss: 0.043408178225159647\n",
      "Average test loss: 0.001453824628661904\n",
      "Epoch 213/300\n",
      "Average training loss: 0.043553598513205845\n",
      "Average test loss: 0.0014607840637779897\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04333360622988807\n",
      "Average test loss: 0.00308033737519549\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04475642039709621\n",
      "Average test loss: 0.0023006410722931224\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04404685015810861\n",
      "Average test loss: 0.0016122739141186079\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04305506553252538\n",
      "Average test loss: 0.0014403739790949557\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04313741330636872\n",
      "Average test loss: 49473.5622593316\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04431476932101779\n",
      "Average test loss: 0.0014372894098568295\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04281158286995358\n",
      "Average test loss: 0.0014236005809572009\n",
      "Epoch 223/300\n",
      "Average training loss: 0.042838182985782625\n",
      "Average test loss: 0.0016627967153779333\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04334956766002708\n",
      "Average test loss: 0.0015282742087211873\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04270072697599729\n",
      "Average test loss: 0.001630805410237776\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04312292246023814\n",
      "Average test loss: 0.039207727018329835\n",
      "Epoch 227/300\n",
      "Average training loss: 0.042846702814102175\n",
      "Average test loss: 0.0015147684040582843\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04324285227391455\n",
      "Average test loss: 0.0014120552376326587\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04286857109268506\n",
      "Average test loss: 0.001543572538946238\n",
      "Epoch 230/300\n",
      "Average training loss: 0.043237484200133215\n",
      "Average test loss: 0.0014421100019373827\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04278191336989403\n",
      "Average test loss: 0.001503380782281359\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0425940981441074\n",
      "Average test loss: 0.0014278077190845377\n",
      "Epoch 233/300\n",
      "Average training loss: 0.043173495991362464\n",
      "Average test loss: 0.0014286989784902996\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04272104473577605\n",
      "Average test loss: 0.0014829051263837351\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04270302656955189\n",
      "Average test loss: 0.00141147856021093\n",
      "Epoch 236/300\n",
      "Average training loss: 0.042634067171149787\n",
      "Average test loss: 0.028041452354027167\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0426002601451344\n",
      "Average test loss: 0.0014226757113097442\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04253234188424217\n",
      "Average test loss: 0.001437284899668561\n",
      "Epoch 240/300\n",
      "Average training loss: 0.043072035981549156\n",
      "Average test loss: 0.0018323848787695168\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04245055326157146\n",
      "Average test loss: 0.0013882910372275444\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04243705005778207\n",
      "Average test loss: 0.0019612356960359545\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04276711090405782\n",
      "Average test loss: 0.004537276920552055\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04249782082107332\n",
      "Average test loss: 0.0014247259846888482\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04349396674169435\n",
      "Average test loss: 0.002907586598975791\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04348048491610421\n",
      "Average test loss: 0.0017929695929504103\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04249567389653788\n",
      "Average test loss: 0.0014930936533750759\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04230234259366989\n",
      "Average test loss: 0.0014192837810454268\n",
      "Epoch 249/300\n",
      "Average training loss: 0.042552765384316446\n",
      "Average test loss: 0.010614998755976558\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04213103552328216\n",
      "Average test loss: 0.0013907465601546898\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04237204010950194\n",
      "Average test loss: 0.0074854986543456715\n",
      "Epoch 252/300\n",
      "Average training loss: 0.043243099610010785\n",
      "Average test loss: 0.0015518226778755584\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04214753356907103\n",
      "Average test loss: 0.0014168454360009896\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04244754877024227\n",
      "Average test loss: 0.0073153468560841345\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04204305004411273\n",
      "Average test loss: 0.0013975162411936455\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04270510644382901\n",
      "Average test loss: 0.001458036656284498\n",
      "Epoch 259/300\n",
      "Average training loss: 0.042610514207018745\n",
      "Average test loss: 0.0017824477816207542\n",
      "Epoch 260/300\n",
      "Average training loss: 0.041916750126414826\n",
      "Average test loss: 0.001370224089982609\n",
      "Epoch 261/300\n",
      "Average training loss: 0.042150086396270325\n",
      "Average test loss: 0.001423332572604219\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04225357927547561\n",
      "Average test loss: 0.0014040386577447255\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04222974620925055\n",
      "Average test loss: 0.0014172113554345236\n",
      "Epoch 264/300\n",
      "Average training loss: 0.042341584818230735\n",
      "Average test loss: 0.00182291849102411\n",
      "Epoch 265/300\n",
      "Average training loss: 0.041820770889520645\n",
      "Average test loss: 0.0014045738579912318\n",
      "Epoch 266/300\n",
      "Average training loss: 0.042612596369451944\n",
      "Average test loss: 0.0015051954447602232\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04165848316086663\n",
      "Average test loss: 0.001675487660906381\n",
      "Epoch 268/300\n",
      "Average training loss: 0.044449650797579024\n",
      "Average test loss: 0.0013987696958291862\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0415578039586544\n",
      "Average test loss: 0.00914128710826238\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04166108996007178\n",
      "Average test loss: 0.001395713858306408\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04184878951311111\n",
      "Average test loss: 0.0014481375582723155\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04176655170652602\n",
      "Average test loss: 0.0014439334138813945\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04191470518708229\n",
      "Average test loss: 0.046393881030380724\n",
      "Epoch 274/300\n",
      "Average training loss: 0.041700378904740015\n",
      "Average test loss: 0.0014015836879197093\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04225487882561154\n",
      "Average test loss: 0.0013929284174616138\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04333969398339589\n",
      "Average test loss: 0.0014563964459424217\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04176056901613871\n",
      "Average test loss: 0.0014491174734301037\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04170537215802404\n",
      "Average test loss: 0.0014046751098293396\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04180341487460666\n",
      "Average test loss: 0.0019253420680761337\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04195288275016679\n",
      "Average test loss: 0.001407625582276119\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04152600412898593\n",
      "Average test loss: 0.0014160423177397912\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04209325348834197\n",
      "Average test loss: 0.00149385508408563\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04182995905147659\n",
      "Average test loss: 0.0014139629055021537\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04131756482521693\n",
      "Average test loss: 0.001468932686994473\n",
      "Epoch 285/300\n",
      "Average training loss: 0.042175513923168186\n",
      "Average test loss: 0.0015271952783481942\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04181830479370223\n",
      "Average test loss: 0.0016649676037745343\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04171875685453415\n",
      "Average test loss: 0.0014106445877502362\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04157295666800605\n",
      "Average test loss: 0.0023624762027627893\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04146681785252359\n",
      "Average test loss: 0.0022705645646072096\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04141217338376575\n",
      "Average test loss: 0.00146212078910321\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04242734583218893\n",
      "Average test loss: 0.0014396253246814013\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04150451827711529\n",
      "Average test loss: 0.002095049763822721\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04137980819410748\n",
      "Average test loss: 0.001516389286559489\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04238380486435361\n",
      "Average test loss: 0.0014121497675983442\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04128485459420416\n",
      "Average test loss: 0.0016915188108881315\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04167004076970948\n",
      "Average test loss: 0.0014781450840334097\n",
      "Epoch 297/300\n",
      "Average training loss: 0.041388634979724885\n",
      "Average test loss: 0.001441630815466245\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04183414605591032\n",
      "Average test loss: 0.0036284369530363214\n",
      "Epoch 299/300\n",
      "Average training loss: 0.041673059559530684\n",
      "Average test loss: 0.0014273387528955937\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04112309685349464\n",
      "Average test loss: 0.0014696946654261815\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth3-.01/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 20.14\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.03\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.69\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.30\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.53\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.04\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.18\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.41\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.59\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.79\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.91\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.08\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.16\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.11\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.29\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.46\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.75\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.80\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.46\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.51\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.90\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.02\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.21\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.59\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.23\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.23\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.42\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.09\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.89\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.94\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.23\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.25\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.28\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.35\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.91\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.39\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.88\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.95\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.96\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.04\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.28\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.17\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.12\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.35\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.54\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.61\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.67\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.58\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.69\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.83\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.81\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
