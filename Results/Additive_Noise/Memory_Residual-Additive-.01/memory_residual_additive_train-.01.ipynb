{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.01)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.01)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.01)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.01)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.15809917668501536\n",
      "Average test loss: 0.010854511864483357\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06296878491838773\n",
      "Average test loss: 0.009631501639468803\n",
      "Epoch 3/300\n",
      "Average training loss: 0.05628347381287151\n",
      "Average test loss: 0.009873398885130882\n",
      "Epoch 4/300\n",
      "Average training loss: 0.052977216104666394\n",
      "Average test loss: 0.008958039844201671\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05070559852984217\n",
      "Average test loss: 0.008214687291946675\n",
      "Epoch 6/300\n",
      "Average training loss: 0.04848013406329685\n",
      "Average test loss: 0.008474054489698676\n",
      "Epoch 7/300\n",
      "Average training loss: 0.047013102567858164\n",
      "Average test loss: 0.00826598258730438\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0457529137134552\n",
      "Average test loss: 0.008065733924094173\n",
      "Epoch 9/300\n",
      "Average training loss: 0.044905243006017476\n",
      "Average test loss: 0.007824032732182079\n",
      "Epoch 10/300\n",
      "Average training loss: 0.044060433218876524\n",
      "Average test loss: 0.00811190306312508\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04336859118938446\n",
      "Average test loss: 0.007265275011873907\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04259957328438759\n",
      "Average test loss: 0.007154037640740474\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04184816242754459\n",
      "Average test loss: 0.007281116856055128\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0413393665585253\n",
      "Average test loss: 0.0070017149671912195\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04080259394314554\n",
      "Average test loss: 0.007288294924630059\n",
      "Epoch 16/300\n",
      "Average training loss: 0.040403214550680584\n",
      "Average test loss: 0.007214880655623144\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04008987647294998\n",
      "Average test loss: 0.007020056157476372\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03954240933391783\n",
      "Average test loss: 0.006724784143269062\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03913264927268028\n",
      "Average test loss: 0.006747255859275659\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0388106833944718\n",
      "Average test loss: 0.006773331625594033\n",
      "Epoch 21/300\n",
      "Average training loss: 0.038494475957420136\n",
      "Average test loss: 0.006708611200253169\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03820015692048603\n",
      "Average test loss: 0.006514423155950176\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03796422953075833\n",
      "Average test loss: 0.006782709080311987\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03768483275175095\n",
      "Average test loss: 0.006469773739162419\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03738116155730353\n",
      "Average test loss: 0.006665747241841422\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03730160946316189\n",
      "Average test loss: 0.006558558223148187\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03695488897959391\n",
      "Average test loss: 0.006372329910389251\n",
      "Epoch 28/300\n",
      "Average training loss: 0.036692950967285365\n",
      "Average test loss: 0.006485296842124727\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03657262291842037\n",
      "Average test loss: 0.006334402790500058\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03706444714135594\n",
      "Average test loss: 0.006606323586155971\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03624604274166955\n",
      "Average test loss: 0.0063532154922270115\n",
      "Epoch 32/300\n",
      "Average training loss: 0.036031291047732034\n",
      "Average test loss: 0.0062888413675957254\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03589211898710993\n",
      "Average test loss: 0.006213876034650538\n",
      "Epoch 34/300\n",
      "Average training loss: 0.035771434257427855\n",
      "Average test loss: 0.006276590918501218\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03562071926395098\n",
      "Average test loss: 0.006390907795892822\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03546548066205449\n",
      "Average test loss: 0.0065206091263228\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03538748564653926\n",
      "Average test loss: 0.006155564674486717\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03529527589016491\n",
      "Average test loss: 0.00610411246576243\n",
      "Epoch 39/300\n",
      "Average training loss: 0.035035567720731096\n",
      "Average test loss: 0.006196738110648261\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03498206467429797\n",
      "Average test loss: 0.006280608722733127\n",
      "Epoch 41/300\n",
      "Average training loss: 0.034961711628569495\n",
      "Average test loss: 0.006022831211073531\n",
      "Epoch 42/300\n",
      "Average training loss: 0.034776129863328405\n",
      "Average test loss: 0.006154048472228978\n",
      "Epoch 43/300\n",
      "Average training loss: 0.034758644415272606\n",
      "Average test loss: 0.006241220396425989\n",
      "Epoch 44/300\n",
      "Average training loss: 0.034544102436966366\n",
      "Average test loss: 0.006336226008832455\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0344550976951917\n",
      "Average test loss: 0.006054055811630355\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03438501458035575\n",
      "Average test loss: 0.006084145961536301\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0344235815041595\n",
      "Average test loss: 0.006087572265416384\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03428980850842264\n",
      "Average test loss: 0.006842581681079335\n",
      "Epoch 49/300\n",
      "Average training loss: 0.034070771030253834\n",
      "Average test loss: 0.006041257998181714\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03408249138130082\n",
      "Average test loss: 0.008201254414187537\n",
      "Epoch 51/300\n",
      "Average training loss: 0.034003715799914465\n",
      "Average test loss: 0.00708083496366938\n",
      "Epoch 52/300\n",
      "Average training loss: 0.033901938266224334\n",
      "Average test loss: 0.0059744715833415585\n",
      "Epoch 53/300\n",
      "Average training loss: 0.033896281000640655\n",
      "Average test loss: 0.006075229096743796\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03370843416452408\n",
      "Average test loss: 0.005962338192181455\n",
      "Epoch 55/300\n",
      "Average training loss: 0.033702501161230934\n",
      "Average test loss: 0.005954466520084275\n",
      "Epoch 56/300\n",
      "Average training loss: 0.033645523034864004\n",
      "Average test loss: 0.006205893715222677\n",
      "Epoch 57/300\n",
      "Average training loss: 0.033522871808873284\n",
      "Average test loss: 0.006032579433586862\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03350905777679549\n",
      "Average test loss: 0.0061204972020867795\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03340383130808671\n",
      "Average test loss: 0.005925538802726401\n",
      "Epoch 60/300\n",
      "Average training loss: 0.033361738054288756\n",
      "Average test loss: 0.005959258498003085\n",
      "Epoch 61/300\n",
      "Average training loss: 0.033287644315097065\n",
      "Average test loss: 0.006349004253745079\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03328101701537768\n",
      "Average test loss: 0.006771571651101113\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03323377111223009\n",
      "Average test loss: 0.0064154664696090755\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03320066116584672\n",
      "Average test loss: 0.0062016260085834395\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0332046229938666\n",
      "Average test loss: 0.13261874873108334\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10751060148742464\n",
      "Average test loss: 0.00768387694698241\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04567057672142982\n",
      "Average test loss: 0.006687100818587674\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04184478693207105\n",
      "Average test loss: 0.006483302179723978\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03986055539382829\n",
      "Average test loss: 0.006213182328475847\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03829274673594369\n",
      "Average test loss: 0.006080748970723814\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03724317231443193\n",
      "Average test loss: 0.006185087940345208\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0362894260055489\n",
      "Average test loss: 0.006018023271527556\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03558904467026393\n",
      "Average test loss: 0.0061374032522241275\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03494188132882118\n",
      "Average test loss: 0.005947229206769003\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03445656145281262\n",
      "Average test loss: 0.0059474483244121076\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03411032908492618\n",
      "Average test loss: 0.005899078853428364\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03373980285392867\n",
      "Average test loss: 0.006009385238091151\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0335105217927032\n",
      "Average test loss: 0.0061008489167110784\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03329636209872034\n",
      "Average test loss: 0.005887055684294966\n",
      "Epoch 80/300\n",
      "Average training loss: 0.033228097683853576\n",
      "Average test loss: 0.005927861938873927\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03317909277809991\n",
      "Average test loss: 0.006304233940939108\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03307015670339267\n",
      "Average test loss: 0.00641435017519527\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03297751608822081\n",
      "Average test loss: 0.006085178133514192\n",
      "Epoch 84/300\n",
      "Average training loss: 0.032985817593004965\n",
      "Average test loss: 0.00591835840501719\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03299371428291003\n",
      "Average test loss: 0.005864385251369742\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03281576116548644\n",
      "Average test loss: 0.006340255225284232\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03281448406974474\n",
      "Average test loss: 0.005958204341017538\n",
      "Epoch 88/300\n",
      "Average training loss: 0.032716607040829127\n",
      "Average test loss: 0.005922790553834704\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0327338731388251\n",
      "Average test loss: 0.005989596445527341\n",
      "Epoch 90/300\n",
      "Average training loss: 0.032750073317024446\n",
      "Average test loss: 0.005934978670544095\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03264380550053385\n",
      "Average test loss: 0.006315533201727602\n",
      "Epoch 92/300\n",
      "Average training loss: 0.032523825115627715\n",
      "Average test loss: 0.00597647021835049\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03253741262687577\n",
      "Average test loss: 0.005942069049510691\n",
      "Epoch 94/300\n",
      "Average training loss: 0.032511506958140264\n",
      "Average test loss: 0.0059159957302941215\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03245369939837191\n",
      "Average test loss: 0.0060202652634018\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03239893764422999\n",
      "Average test loss: 0.006090422222597732\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03237362606326739\n",
      "Average test loss: 0.005867707807570696\n",
      "Epoch 98/300\n",
      "Average training loss: 0.032290401233567134\n",
      "Average test loss: 0.005932222335288922\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03230190147293938\n",
      "Average test loss: 0.0060271215376754604\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03227752778099643\n",
      "Average test loss: 0.005960325857831372\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03223835762672954\n",
      "Average test loss: 0.00596715134051111\n",
      "Epoch 102/300\n",
      "Average training loss: 0.032169439108835324\n",
      "Average test loss: 0.0058266000354455574\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03211830882396963\n",
      "Average test loss: 0.0059266709312796595\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03209946001238293\n",
      "Average test loss: 0.00579405370561613\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03205578532483842\n",
      "Average test loss: 0.006043536875396967\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03201932156085968\n",
      "Average test loss: 0.0059620725090305015\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03198725238773558\n",
      "Average test loss: 0.0058726518356965645\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03196594021717707\n",
      "Average test loss: 0.006643290059020122\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03197091380092833\n",
      "Average test loss: 0.005987962863925431\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03191600285967191\n",
      "Average test loss: 0.005871494359854195\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03187188941571448\n",
      "Average test loss: 0.005989249983595477\n",
      "Epoch 112/300\n",
      "Average training loss: 0.031806917448838554\n",
      "Average test loss: 0.00602426384223832\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03178628890381919\n",
      "Average test loss: 0.0061690144543018605\n",
      "Epoch 114/300\n",
      "Average training loss: 0.031734575470288594\n",
      "Average test loss: 0.006480198559247785\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03176673397421837\n",
      "Average test loss: 0.00587296248641279\n",
      "Epoch 116/300\n",
      "Average training loss: 0.031709794534577264\n",
      "Average test loss: 0.005885959488236242\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03164597036275599\n",
      "Average test loss: 0.006052419088159998\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0316788969569736\n",
      "Average test loss: 0.005901220399058527\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03157312762240569\n",
      "Average test loss: 0.006020427638457881\n",
      "Epoch 120/300\n",
      "Average training loss: 0.031590000891023214\n",
      "Average test loss: 0.005886468247820934\n",
      "Epoch 121/300\n",
      "Average training loss: 0.031555947009060115\n",
      "Average test loss: 0.006000065439691146\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03156691433323754\n",
      "Average test loss: 0.00593750910833478\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03151182078321775\n",
      "Average test loss: 0.0058551888205111025\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03146918044818772\n",
      "Average test loss: 0.006216597860058149\n",
      "Epoch 125/300\n",
      "Average training loss: 0.031451509906186\n",
      "Average test loss: 0.005910284215377436\n",
      "Epoch 126/300\n",
      "Average training loss: 0.031410192348890836\n",
      "Average test loss: 0.005852481623904573\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03140618862211704\n",
      "Average test loss: 0.00596165057644248\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03139262333015601\n",
      "Average test loss: 0.005952484822935528\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0313228165242407\n",
      "Average test loss: 0.005884334007898967\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03135708440343539\n",
      "Average test loss: 0.005875336200412777\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03135681719250149\n",
      "Average test loss: 0.005869400830732452\n",
      "Epoch 132/300\n",
      "Average training loss: 0.031241316411230298\n",
      "Average test loss: 0.006091614159858889\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03122084841794438\n",
      "Average test loss: 0.006308676966776451\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03125161659220854\n",
      "Average test loss: 0.006223671231211887\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031181619975301954\n",
      "Average test loss: 0.006006885774847534\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031197886281543307\n",
      "Average test loss: 0.006768235258344147\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03116153304444419\n",
      "Average test loss: 0.00594874497420258\n",
      "Epoch 138/300\n",
      "Average training loss: 0.031147502104441324\n",
      "Average test loss: 0.005912970277584261\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03113333264655537\n",
      "Average test loss: 0.005903441416720549\n",
      "Epoch 140/300\n",
      "Average training loss: 0.031069019865658548\n",
      "Average test loss: 0.005934150537268983\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03114570157064332\n",
      "Average test loss: 0.005969050777869092\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03112208752334118\n",
      "Average test loss: 0.006049126596500476\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031031494273079767\n",
      "Average test loss: 0.005962004527449608\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030980345315403408\n",
      "Average test loss: 0.005920504770759079\n",
      "Epoch 145/300\n",
      "Average training loss: 0.031035006226764784\n",
      "Average test loss: 0.0059344131309125156\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030910228966010943\n",
      "Average test loss: 0.005922271548666888\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030987259089946748\n",
      "Average test loss: 0.005841323873649041\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03094470003247261\n",
      "Average test loss: 0.005873808518466022\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030929952477415402\n",
      "Average test loss: 0.00596364340144727\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03088633578022321\n",
      "Average test loss: 0.005889057617634535\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03087220205201043\n",
      "Average test loss: 0.006732623405340645\n",
      "Epoch 152/300\n",
      "Average training loss: 0.030843431568808026\n",
      "Average test loss: 0.006191364630228943\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030838200940026178\n",
      "Average test loss: 0.005964878352151977\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030842785646518073\n",
      "Average test loss: 0.006004194104009204\n",
      "Epoch 155/300\n",
      "Average training loss: 0.030758883721298643\n",
      "Average test loss: 0.005838201051370965\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030740657806396485\n",
      "Average test loss: 0.005943302666354511\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03079654765791363\n",
      "Average test loss: 0.005826976886226071\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030729529400666554\n",
      "Average test loss: 0.0058824865072965625\n",
      "Epoch 159/300\n",
      "Average training loss: 0.030680119458172058\n",
      "Average test loss: 0.0059366093762218955\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03066459412376086\n",
      "Average test loss: 0.006163959486616982\n",
      "Epoch 161/300\n",
      "Average training loss: 0.030697789922356604\n",
      "Average test loss: 0.00601483951177862\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030584021480547056\n",
      "Average test loss: 0.005929697173337142\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03069933872918288\n",
      "Average test loss: 0.006377344564431243\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03065978701909383\n",
      "Average test loss: 0.006092328120850855\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03064209017985397\n",
      "Average test loss: 0.006007929040326013\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0305217258317603\n",
      "Average test loss: 0.005930606160312891\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030601088701022995\n",
      "Average test loss: 0.00615909329470661\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03051953193379773\n",
      "Average test loss: 0.0059512564779983625\n",
      "Epoch 169/300\n",
      "Average training loss: 0.030499360650777817\n",
      "Average test loss: 0.0060748996105459\n",
      "Epoch 170/300\n",
      "Average training loss: 0.030520840866698158\n",
      "Average test loss: 0.005947831162561973\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030504229242602984\n",
      "Average test loss: 0.00630216190757023\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030544683360391192\n",
      "Average test loss: 0.005942708008198274\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030471566117472117\n",
      "Average test loss: 0.006033416979428795\n",
      "Epoch 174/300\n",
      "Average training loss: 0.030487005149324734\n",
      "Average test loss: 0.0060088041014969345\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030454613076315987\n",
      "Average test loss: 0.0060331681979199255\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030421830056442153\n",
      "Average test loss: 0.008502084310683939\n",
      "Epoch 177/300\n",
      "Average training loss: 0.030391486808657647\n",
      "Average test loss: 0.0061254230116804444\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03039605044821898\n",
      "Average test loss: 0.0068124112540649045\n",
      "Epoch 179/300\n",
      "Average training loss: 0.030386014092299674\n",
      "Average test loss: 0.0059499055892229084\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030401234307222896\n",
      "Average test loss: 0.006085646451347404\n",
      "Epoch 181/300\n",
      "Average training loss: 0.030367797169420455\n",
      "Average test loss: 0.0060335444108479555\n",
      "Epoch 182/300\n",
      "Average training loss: 0.030310513810978996\n",
      "Average test loss: 0.005991161772567365\n",
      "Epoch 183/300\n",
      "Average training loss: 0.030273969944980408\n",
      "Average test loss: 0.005901819078044759\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03035701882176929\n",
      "Average test loss: 0.007199140726278226\n",
      "Epoch 185/300\n",
      "Average training loss: 0.030324205686648686\n",
      "Average test loss: 0.0061546039320528505\n",
      "Epoch 186/300\n",
      "Average training loss: 0.030281105173958673\n",
      "Average test loss: 0.005977386912951867\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0302531172103352\n",
      "Average test loss: 0.0060452485502594045\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03025574914448791\n",
      "Average test loss: 0.006215859274069468\n",
      "Epoch 189/300\n",
      "Average training loss: 0.030248438792096245\n",
      "Average test loss: 0.00592051096384724\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03021541205379698\n",
      "Average test loss: 0.005922639792164167\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03021296257774035\n",
      "Average test loss: 0.005874635485311349\n",
      "Epoch 192/300\n",
      "Average training loss: 0.030215897457467184\n",
      "Average test loss: 0.00599509092213379\n",
      "Epoch 193/300\n",
      "Average training loss: 0.030153590106301836\n",
      "Average test loss: 0.005962738654679722\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03018364318377442\n",
      "Average test loss: 0.006041655207259787\n",
      "Epoch 195/300\n",
      "Average training loss: 0.030144799649715424\n",
      "Average test loss: 0.006069717130106357\n",
      "Epoch 196/300\n",
      "Average training loss: 0.030113891176051562\n",
      "Average test loss: 0.0060935140852299\n",
      "Epoch 197/300\n",
      "Average training loss: 0.030168051070637174\n",
      "Average test loss: 0.005971664676649703\n",
      "Epoch 198/300\n",
      "Average training loss: 0.030144296096430885\n",
      "Average test loss: 0.006047756383816401\n",
      "Epoch 199/300\n",
      "Average training loss: 0.030120032454530397\n",
      "Average test loss: 0.006152465660125017\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03011970269680023\n",
      "Average test loss: 0.006063708381106456\n",
      "Epoch 201/300\n",
      "Average training loss: 0.030113700482580396\n",
      "Average test loss: 0.006047448904563983\n",
      "Epoch 202/300\n",
      "Average training loss: 0.030037583467033176\n",
      "Average test loss: 0.006109092075791624\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03003499749965138\n",
      "Average test loss: 0.005945871664832036\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03004493819177151\n",
      "Average test loss: 0.006128239080723789\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03004426188270251\n",
      "Average test loss: 0.005982450733582179\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02998564781083001\n",
      "Average test loss: 0.0059561085899670916\n",
      "Epoch 207/300\n",
      "Average training loss: 0.029994265822900667\n",
      "Average test loss: 0.0061248249664074845\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02997001369131936\n",
      "Average test loss: 0.005984754780514373\n",
      "Epoch 209/300\n",
      "Average training loss: 0.029982408122883903\n",
      "Average test loss: 0.005984423287212849\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029978202818168535\n",
      "Average test loss: 0.006075538302875228\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029941578259070713\n",
      "Average test loss: 0.006050793968141079\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029981153577566148\n",
      "Average test loss: 0.005961914168463813\n",
      "Epoch 213/300\n",
      "Average training loss: 0.029895463433530594\n",
      "Average test loss: 0.0061528081475860546\n",
      "Epoch 214/300\n",
      "Average training loss: 0.029925609272387292\n",
      "Average test loss: 0.005982146623233954\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02991419576936298\n",
      "Average test loss: 0.006139333626462354\n",
      "Epoch 216/300\n",
      "Average training loss: 0.029913886406355433\n",
      "Average test loss: 0.006181264303210709\n",
      "Epoch 217/300\n",
      "Average training loss: 0.029834399825996824\n",
      "Average test loss: 0.00608150028437376\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02991856287419796\n",
      "Average test loss: 0.006127067104809814\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02987038270963563\n",
      "Average test loss: 0.005983038594325384\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02989937000638909\n",
      "Average test loss: 0.006194533438732227\n",
      "Epoch 221/300\n",
      "Average training loss: 0.029858417795764074\n",
      "Average test loss: 0.006076697606179449\n",
      "Epoch 222/300\n",
      "Average training loss: 0.029815591957834033\n",
      "Average test loss: 0.006149878578260541\n",
      "Epoch 223/300\n",
      "Average training loss: 0.029849954679608347\n",
      "Average test loss: 0.006041938352915976\n",
      "Epoch 224/300\n",
      "Average training loss: 0.029814388218853207\n",
      "Average test loss: 0.00612755397003558\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02978237096303039\n",
      "Average test loss: 0.00601299474015832\n",
      "Epoch 226/300\n",
      "Average training loss: 0.029845165517595078\n",
      "Average test loss: 0.006276215505268839\n",
      "Epoch 227/300\n",
      "Average training loss: 0.029801013771030636\n",
      "Average test loss: 0.006032358282142216\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0297294849736823\n",
      "Average test loss: 0.006644415818568733\n",
      "Epoch 229/300\n",
      "Average training loss: 0.029738408550620078\n",
      "Average test loss: 0.006118078795572122\n",
      "Epoch 230/300\n",
      "Average training loss: 0.029732089208232032\n",
      "Average test loss: 0.005951027462258935\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02973508910006947\n",
      "Average test loss: 0.006000339558968941\n",
      "Epoch 232/300\n",
      "Average training loss: 0.029713450729846956\n",
      "Average test loss: 0.006040588829666376\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02976146390206284\n",
      "Average test loss: 0.006068332093457381\n",
      "Epoch 234/300\n",
      "Average training loss: 0.029704356799523036\n",
      "Average test loss: 0.006305207435869508\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02973658536374569\n",
      "Average test loss: 0.005921283197485738\n",
      "Epoch 236/300\n",
      "Average training loss: 0.029678397493229973\n",
      "Average test loss: 0.005969334973643223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02968387243151665\n",
      "Average test loss: 0.006284979242831469\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02977734402153227\n",
      "Average test loss: 0.006097403165366914\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0296833535598384\n",
      "Average test loss: 0.0062193091635902725\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02962268287440141\n",
      "Average test loss: 0.006174538647135098\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0296610902150472\n",
      "Average test loss: 0.006156321419609917\n",
      "Epoch 242/300\n",
      "Average training loss: 0.029608215202887853\n",
      "Average test loss: 0.00617971395370033\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02961297971341345\n",
      "Average test loss: 0.006395594512836801\n",
      "Epoch 244/300\n",
      "Average training loss: 0.029586449263824355\n",
      "Average test loss: 0.006055982890228431\n",
      "Epoch 245/300\n",
      "Average training loss: 0.029574142523937755\n",
      "Average test loss: 0.006079460257043441\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02957130493554804\n",
      "Average test loss: 0.005948234513401985\n",
      "Epoch 247/300\n",
      "Average training loss: 0.029615227174427775\n",
      "Average test loss: 0.006067440135611428\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02954465326997969\n",
      "Average test loss: 0.00618541180383828\n",
      "Epoch 249/300\n",
      "Average training loss: 0.029585066748989954\n",
      "Average test loss: 0.006038618634558387\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02954836129811075\n",
      "Average test loss: 0.006183763653039932\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02954254204365942\n",
      "Average test loss: 0.006193495405217012\n",
      "Epoch 252/300\n",
      "Average training loss: 0.029602273450957405\n",
      "Average test loss: 0.006055511643075281\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02952106226152844\n",
      "Average test loss: 0.005987193569954898\n",
      "Epoch 254/300\n",
      "Average training loss: 0.029492479738261964\n",
      "Average test loss: 0.006018002635074986\n",
      "Epoch 255/300\n",
      "Average training loss: 0.029462890300485822\n",
      "Average test loss: 0.005992146218816439\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02945024993022283\n",
      "Average test loss: 0.006062271763053205\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0295380853795343\n",
      "Average test loss: 0.006146827122403516\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02949653295179208\n",
      "Average test loss: 0.00624185391349925\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02941861738098992\n",
      "Average test loss: 0.006161512874894672\n",
      "Epoch 260/300\n",
      "Average training loss: 0.029463428179423014\n",
      "Average test loss: 0.006124469531493055\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02950090444750256\n",
      "Average test loss: 0.006036842413660553\n",
      "Epoch 262/300\n",
      "Average training loss: 0.029471432656049727\n",
      "Average test loss: 0.006091510728001594\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02939008369545142\n",
      "Average test loss: 0.007222208295017481\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02950047800441583\n",
      "Average test loss: 0.006146144966284434\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02943570086856683\n",
      "Average test loss: 0.006278198884593116\n",
      "Epoch 266/300\n",
      "Average training loss: 0.029437584610448944\n",
      "Average test loss: 0.0061824889352752104\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02939857946170701\n",
      "Average test loss: 0.00600550056869785\n",
      "Epoch 268/300\n",
      "Average training loss: 0.029408289823267195\n",
      "Average test loss: 0.006052165176305506\n",
      "Epoch 269/300\n",
      "Average training loss: 0.029426469379001195\n",
      "Average test loss: 0.0062323889699247145\n",
      "Epoch 270/300\n",
      "Average training loss: 0.029317273600233926\n",
      "Average test loss: 0.006292356730335289\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0294252399901549\n",
      "Average test loss: 0.006951234772801399\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02939051671160592\n",
      "Average test loss: 0.006250165008422401\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02940733332104153\n",
      "Average test loss: 0.006197304448319806\n",
      "Epoch 274/300\n",
      "Average training loss: 0.029319158954752816\n",
      "Average test loss: 0.006012414107306136\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02931093113289939\n",
      "Average test loss: 0.006562004562881258\n",
      "Epoch 276/300\n",
      "Average training loss: 0.029401872234212027\n",
      "Average test loss: 0.006309442821062273\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02930417262348864\n",
      "Average test loss: 0.006621856464280022\n",
      "Epoch 278/300\n",
      "Average training loss: 0.029265179203616248\n",
      "Average test loss: 0.006056414011865854\n",
      "Epoch 279/300\n",
      "Average training loss: 0.029368175912234517\n",
      "Average test loss: 0.006345189173188474\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02927189201944404\n",
      "Average test loss: 0.00608831628681057\n",
      "Epoch 281/300\n",
      "Average training loss: 0.029249809859527482\n",
      "Average test loss: 0.0061653491660124725\n",
      "Epoch 282/300\n",
      "Average training loss: 0.029299093022942542\n",
      "Average test loss: 0.00610847808006737\n",
      "Epoch 283/300\n",
      "Average training loss: 0.029274780237012438\n",
      "Average test loss: 0.006218060623854399\n",
      "Epoch 284/300\n",
      "Average training loss: 0.029273590034908717\n",
      "Average test loss: 0.006112219235963292\n",
      "Epoch 285/300\n",
      "Average training loss: 0.029239283662703303\n",
      "Average test loss: 0.006073133605221907\n",
      "Epoch 286/300\n",
      "Average training loss: 0.029282832715246412\n",
      "Average test loss: 0.005978069802125295\n",
      "Epoch 287/300\n",
      "Average training loss: 0.029295450404286386\n",
      "Average test loss: 0.006153566708167394\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02925610327223937\n",
      "Average test loss: 0.00615505381135477\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02923398874534501\n",
      "Average test loss: 0.006157452631327841\n",
      "Epoch 290/300\n",
      "Average training loss: 0.029223229555620088\n",
      "Average test loss: 0.006142871045817931\n",
      "Epoch 291/300\n",
      "Average training loss: 0.029198832689060105\n",
      "Average test loss: 0.00610598776034183\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02930911292632421\n",
      "Average test loss: 0.006169152111642891\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02925640900598632\n",
      "Average test loss: 0.006130536447382636\n",
      "Epoch 294/300\n",
      "Average training loss: 0.029206558777226343\n",
      "Average test loss: 0.006217210198442141\n",
      "Epoch 295/300\n",
      "Average training loss: 0.029301835336618953\n",
      "Average test loss: 0.006165046918309397\n",
      "Epoch 296/300\n",
      "Average training loss: 0.029394468213121097\n",
      "Average test loss: 0.006540006183915668\n",
      "Epoch 297/300\n",
      "Average training loss: 0.029148056995537547\n",
      "Average test loss: 0.0061365643164349925\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02912360426121288\n",
      "Average test loss: 0.006058663464254803\n",
      "Epoch 299/300\n",
      "Average training loss: 0.029129781365394593\n",
      "Average test loss: 0.006093926551855273\n",
      "Epoch 300/300\n",
      "Average training loss: 0.029201670193009906\n",
      "Average test loss: 0.006203824872771899\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.13493586149480608\n",
      "Average test loss: 0.007510814060767492\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0501484441988998\n",
      "Average test loss: 0.013937560800876882\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04479720210366779\n",
      "Average test loss: 0.006747272366864814\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04130422990851932\n",
      "Average test loss: 0.005778788470145729\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03871916283501519\n",
      "Average test loss: 0.00547935606042544\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03685533880856302\n",
      "Average test loss: 0.007742458727624682\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03506354335612721\n",
      "Average test loss: 0.0052791392103665406\n",
      "Epoch 8/300\n",
      "Average training loss: 0.033603865014182194\n",
      "Average test loss: 0.0052849260626567736\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03279883156220118\n",
      "Average test loss: 0.004854179282155302\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03172666315568818\n",
      "Average test loss: 0.004911650040290422\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03088369972507159\n",
      "Average test loss: 0.004698666518761052\n",
      "Epoch 12/300\n",
      "Average training loss: 0.030252107525865237\n",
      "Average test loss: 0.00461743509521087\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02957000566025575\n",
      "Average test loss: 0.00433657350577414\n",
      "Epoch 14/300\n",
      "Average training loss: 0.029168425227204958\n",
      "Average test loss: 0.004347474906593561\n",
      "Epoch 15/300\n",
      "Average training loss: 0.02862314292622937\n",
      "Average test loss: 0.004323863579995102\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02822544707523452\n",
      "Average test loss: 0.004246475668210122\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02782991749048233\n",
      "Average test loss: 0.004354750307483806\n",
      "Epoch 18/300\n",
      "Average training loss: 0.027467075086302226\n",
      "Average test loss: 0.004035203957930208\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02717396230995655\n",
      "Average test loss: 0.004462192628325688\n",
      "Epoch 20/300\n",
      "Average training loss: 0.026839203695456188\n",
      "Average test loss: 0.004023369300489624\n",
      "Epoch 21/300\n",
      "Average training loss: 0.026704042833712367\n",
      "Average test loss: 0.0039514903782142535\n",
      "Epoch 22/300\n",
      "Average training loss: 0.026424812825189697\n",
      "Average test loss: 0.0038813323784205648\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02613852611846394\n",
      "Average test loss: 0.003833121201954782\n",
      "Epoch 24/300\n",
      "Average training loss: 0.025966365627116628\n",
      "Average test loss: 0.003831658179561297\n",
      "Epoch 25/300\n",
      "Average training loss: 0.025778916910290717\n",
      "Average test loss: 0.003799891442888313\n",
      "Epoch 26/300\n",
      "Average training loss: 0.025568440539969338\n",
      "Average test loss: 0.0037975477541072502\n",
      "Epoch 27/300\n",
      "Average training loss: 0.025408443570137024\n",
      "Average test loss: 0.0037623052187263964\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025333181414339278\n",
      "Average test loss: 0.004408903538559874\n",
      "Epoch 29/300\n",
      "Average training loss: 0.025096200784047444\n",
      "Average test loss: 0.003777162820721666\n",
      "Epoch 30/300\n",
      "Average training loss: 0.025011429472929902\n",
      "Average test loss: 0.00385424244362447\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024939157318737772\n",
      "Average test loss: 0.0036517106618525253\n",
      "Epoch 32/300\n",
      "Average training loss: 0.024767930361959668\n",
      "Average test loss: 0.003686400501264466\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02460700245698293\n",
      "Average test loss: 0.0037539586143361197\n",
      "Epoch 34/300\n",
      "Average training loss: 0.024563540937172043\n",
      "Average test loss: 0.0037734933122992517\n",
      "Epoch 35/300\n",
      "Average training loss: 0.024462606307533052\n",
      "Average test loss: 0.0036339073030071125\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02437716095480654\n",
      "Average test loss: 0.004657085894503527\n",
      "Epoch 37/300\n",
      "Average training loss: 0.024246207841568522\n",
      "Average test loss: 0.0035770931299775838\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024174613912900288\n",
      "Average test loss: 0.003620830012899306\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02408255259361532\n",
      "Average test loss: 0.0035885316419104734\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024016793645090528\n",
      "Average test loss: 0.0036189346543202797\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02400460865100225\n",
      "Average test loss: 0.0035808600936498907\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02392083748181661\n",
      "Average test loss: 0.0035307721953011222\n",
      "Epoch 43/300\n",
      "Average training loss: 0.023824970738755332\n",
      "Average test loss: 0.0036099699361042846\n",
      "Epoch 44/300\n",
      "Average training loss: 0.023793689691358144\n",
      "Average test loss: 0.0037328368737879728\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02373957466913594\n",
      "Average test loss: 0.0036734829474654464\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02362440409925249\n",
      "Average test loss: 0.003491144861198134\n",
      "Epoch 47/300\n",
      "Average training loss: 0.023631685043374696\n",
      "Average test loss: 0.003581045180766119\n",
      "Epoch 48/300\n",
      "Average training loss: 0.023564352355069585\n",
      "Average test loss: 0.003781316383431355\n",
      "Epoch 49/300\n",
      "Average training loss: 0.023535603193773163\n",
      "Average test loss: 0.003478222340138422\n",
      "Epoch 50/300\n",
      "Average training loss: 0.023474610028995407\n",
      "Average test loss: 0.0034780141398724584\n",
      "Epoch 51/300\n",
      "Average training loss: 0.02339699108567503\n",
      "Average test loss: 0.0035321265885399447\n",
      "Epoch 52/300\n",
      "Average training loss: 0.023391104045841427\n",
      "Average test loss: 0.0034869214817881584\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02332981508804692\n",
      "Average test loss: 0.0035031621569974556\n",
      "Epoch 54/300\n",
      "Average training loss: 0.023266383056839306\n",
      "Average test loss: 0.003547629179432988\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0232519002325005\n",
      "Average test loss: 0.003663252531654305\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02321722482310401\n",
      "Average test loss: 0.0038424101927214196\n",
      "Epoch 57/300\n",
      "Average training loss: 0.023225312602188852\n",
      "Average test loss: 0.003622452477614085\n",
      "Epoch 58/300\n",
      "Average training loss: 0.023102180161409906\n",
      "Average test loss: 0.003515684242877695\n",
      "Epoch 59/300\n",
      "Average training loss: 0.023090648791856235\n",
      "Average test loss: 0.0034681121206118003\n",
      "Epoch 60/300\n",
      "Average training loss: 0.023027576986286376\n",
      "Average test loss: 0.0034954716103772323\n",
      "Epoch 61/300\n",
      "Average training loss: 0.022992146824796996\n",
      "Average test loss: 0.0035461462810635566\n",
      "Epoch 62/300\n",
      "Average training loss: 0.023095043243633376\n",
      "Average test loss: 0.003490903389122751\n",
      "Epoch 63/300\n",
      "Average training loss: 0.022948655999369093\n",
      "Average test loss: 0.0034843241315748955\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02294288236896197\n",
      "Average test loss: 0.0034773849652459223\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02286626103354825\n",
      "Average test loss: 0.0034957326853440867\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02285821570456028\n",
      "Average test loss: 0.003460439511678285\n",
      "Epoch 67/300\n",
      "Average training loss: 0.022799937936994766\n",
      "Average test loss: 0.0034455581679940224\n",
      "Epoch 68/300\n",
      "Average training loss: 0.022799877002835273\n",
      "Average test loss: 0.0035181774211426576\n",
      "Epoch 69/300\n",
      "Average training loss: 0.022781421977612706\n",
      "Average test loss: 0.0038284125468797153\n",
      "Epoch 70/300\n",
      "Average training loss: 0.022714526952968703\n",
      "Average test loss: 0.0038246366195380686\n",
      "Epoch 71/300\n",
      "Average training loss: 0.022726385653018953\n",
      "Average test loss: 0.0034656184965537653\n",
      "Epoch 72/300\n",
      "Average training loss: 0.022752333703968262\n",
      "Average test loss: 0.003447168524066607\n",
      "Epoch 73/300\n",
      "Average training loss: 0.022620273091726834\n",
      "Average test loss: 0.003773637074149317\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02260168509847588\n",
      "Average test loss: 0.0034360263815356624\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02259154704378711\n",
      "Average test loss: 0.003435876324151953\n",
      "Epoch 76/300\n",
      "Average training loss: 0.022601417730251948\n",
      "Average test loss: 0.003500855249042312\n",
      "Epoch 77/300\n",
      "Average training loss: 0.022574243812097442\n",
      "Average test loss: 0.0034100008747643897\n",
      "Epoch 78/300\n",
      "Average training loss: 0.022535066791706616\n",
      "Average test loss: 0.003438015544373128\n",
      "Epoch 79/300\n",
      "Average training loss: 0.022574210635489887\n",
      "Average test loss: 0.003436292683912648\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02246778049568335\n",
      "Average test loss: 0.003474380947235558\n",
      "Epoch 81/300\n",
      "Average training loss: 0.022437281794018216\n",
      "Average test loss: 0.0034677054348091286\n",
      "Epoch 82/300\n",
      "Average training loss: 0.022438212340076764\n",
      "Average test loss: 0.003470714182489448\n",
      "Epoch 83/300\n",
      "Average training loss: 0.022465972767935858\n",
      "Average test loss: 0.0033931046614630355\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02236411931117376\n",
      "Average test loss: 0.003438546642454134\n",
      "Epoch 85/300\n",
      "Average training loss: 0.022359381972087755\n",
      "Average test loss: 0.003567481727649768\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02234473938908842\n",
      "Average test loss: 0.0034099257559412057\n",
      "Epoch 87/300\n",
      "Average training loss: 0.022342022066315016\n",
      "Average test loss: 0.0034329219189369016\n",
      "Epoch 88/300\n",
      "Average training loss: 0.022312896658976872\n",
      "Average test loss: 0.0034930279416342577\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02229815478126208\n",
      "Average test loss: 0.0034166901028818554\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02224758281062047\n",
      "Average test loss: 0.003715389065237509\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02225516921116246\n",
      "Average test loss: 0.0033940976690500974\n",
      "Epoch 92/300\n",
      "Average training loss: 0.022255881566140386\n",
      "Average test loss: 0.003389227617532015\n",
      "Epoch 93/300\n",
      "Average training loss: 0.022220612332224846\n",
      "Average test loss: 0.0034095796288715467\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022217421568102306\n",
      "Average test loss: 0.003926148430754741\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022149970167213016\n",
      "Average test loss: 0.0034404031907518703\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022160410523414612\n",
      "Average test loss: 0.0034290644329869084\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02212826820049021\n",
      "Average test loss: 0.0035009693617208135\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02214125917520788\n",
      "Average test loss: 0.003442382451767723\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0221324427144395\n",
      "Average test loss: 0.0034039559558861785\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02206272537012895\n",
      "Average test loss: 0.0034456499585260947\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022086378494898477\n",
      "Average test loss: 0.003507962077545623\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02211021384265688\n",
      "Average test loss: 0.0033955900865710446\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02201929239432017\n",
      "Average test loss: 0.003491929112623135\n",
      "Epoch 104/300\n",
      "Average training loss: 0.022016818604535526\n",
      "Average test loss: 0.0034932776091413367\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02202659676803483\n",
      "Average test loss: 0.0034730799140201673\n",
      "Epoch 106/300\n",
      "Average training loss: 0.022012030909458795\n",
      "Average test loss: 0.003475224274728033\n",
      "Epoch 107/300\n",
      "Average training loss: 0.021977174123128256\n",
      "Average test loss: 0.003618414810548226\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02195769801404741\n",
      "Average test loss: 0.003384429381539424\n",
      "Epoch 109/300\n",
      "Average training loss: 0.021944737053579754\n",
      "Average test loss: 0.00345576263529559\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021890578067964978\n",
      "Average test loss: 0.0034570952310330337\n",
      "Epoch 111/300\n",
      "Average training loss: 0.021930537628630797\n",
      "Average test loss: 0.0034985661407311757\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02187122069299221\n",
      "Average test loss: 0.003497144882877668\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021912293939126862\n",
      "Average test loss: 0.0035212984925342933\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021877484609683355\n",
      "Average test loss: 0.0036778651260667376\n",
      "Epoch 115/300\n",
      "Average training loss: 0.021884778804249234\n",
      "Average test loss: 0.003446943586071332\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0218653694027\n",
      "Average test loss: 0.003426627571384112\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02183321934276157\n",
      "Average test loss: 0.0034628143285711606\n",
      "Epoch 118/300\n",
      "Average training loss: 0.021801370869080225\n",
      "Average test loss: 0.003476224672049284\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02179158138566547\n",
      "Average test loss: 0.003441382040373153\n",
      "Epoch 120/300\n",
      "Average training loss: 0.021798304544554815\n",
      "Average test loss: 0.003415415407054954\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021751401167776848\n",
      "Average test loss: 0.0034250708060959974\n",
      "Epoch 122/300\n",
      "Average training loss: 0.021758885527650516\n",
      "Average test loss: 0.003707238364136881\n",
      "Epoch 123/300\n",
      "Average training loss: 0.021785269868870576\n",
      "Average test loss: 0.00359011870291498\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021728280584017434\n",
      "Average test loss: 0.0033804742474522857\n",
      "Epoch 125/300\n",
      "Average training loss: 0.021729529461926883\n",
      "Average test loss: 0.003490267909028464\n",
      "Epoch 126/300\n",
      "Average training loss: 0.021710475343796942\n",
      "Average test loss: 0.0036385699945191544\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02169793548517757\n",
      "Average test loss: 0.0039031401547706787\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021673633739352226\n",
      "Average test loss: 0.003543374013983541\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02168197456498941\n",
      "Average test loss: 0.0034062059180190165\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021674002935489017\n",
      "Average test loss: 0.0034996229865484766\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021642003140515752\n",
      "Average test loss: 0.0035013012948135534\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021646372583177354\n",
      "Average test loss: 0.003434787544525332\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021645477169089846\n",
      "Average test loss: 0.0035575144046710596\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02161758449011379\n",
      "Average test loss: 0.0034392008394416836\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021607497755024168\n",
      "Average test loss: 0.0036477706304026973\n",
      "Epoch 136/300\n",
      "Average training loss: 0.021588811683985923\n",
      "Average test loss: 0.0035916110121955473\n",
      "Epoch 137/300\n",
      "Average training loss: 0.021569239258766176\n",
      "Average test loss: 0.0034491266068071126\n",
      "Epoch 138/300\n",
      "Average training loss: 0.021564202340112793\n",
      "Average test loss: 0.003432575707220369\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02155729261206256\n",
      "Average test loss: 0.0036600818120770986\n",
      "Epoch 140/300\n",
      "Average training loss: 0.021539571581615343\n",
      "Average test loss: 0.0040124963633716105\n",
      "Epoch 141/300\n",
      "Average training loss: 0.021510802558726734\n",
      "Average test loss: 0.003447280355418722\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02154580892291334\n",
      "Average test loss: 0.8977370076974233\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0217007516870896\n",
      "Average test loss: 0.003418197429428498\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02145062336822351\n",
      "Average test loss: 0.003392372099061807\n",
      "Epoch 145/300\n",
      "Average training loss: 0.021468152036269506\n",
      "Average test loss: 0.003447272003938754\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02147758620811833\n",
      "Average test loss: 0.0034267094776862196\n",
      "Epoch 147/300\n",
      "Average training loss: 0.021481750377350384\n",
      "Average test loss: 0.0034170481274939245\n",
      "Epoch 148/300\n",
      "Average training loss: 0.021445195007655356\n",
      "Average test loss: 0.0035199185725715426\n",
      "Epoch 149/300\n",
      "Average training loss: 0.021429296539889443\n",
      "Average test loss: 0.003501778597012162\n",
      "Epoch 150/300\n",
      "Average training loss: 0.021441558193829324\n",
      "Average test loss: 0.00361084325487415\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02147160133885013\n",
      "Average test loss: 0.0034579805835253663\n",
      "Epoch 152/300\n",
      "Average training loss: 0.021408462638656298\n",
      "Average test loss: 0.0034908352215877836\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02142095029105743\n",
      "Average test loss: 0.0034763297316514783\n",
      "Epoch 154/300\n",
      "Average training loss: 0.021393422219488357\n",
      "Average test loss: 0.0035140107017424373\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02137570270564821\n",
      "Average test loss: 0.0034345752238813376\n",
      "Epoch 156/300\n",
      "Average training loss: 0.021367630581061045\n",
      "Average test loss: 0.0036222663979149526\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0213982552372747\n",
      "Average test loss: 0.0036377612758013936\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02132560285594728\n",
      "Average test loss: 0.0034919626067082088\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02138787088957098\n",
      "Average test loss: 0.003385227507704662\n",
      "Epoch 160/300\n",
      "Average training loss: 0.021370224161280527\n",
      "Average test loss: 0.003533558695473605\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02130926378733582\n",
      "Average test loss: 0.003470065734659632\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0213126712159978\n",
      "Average test loss: 0.003389257317201959\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02130307940310902\n",
      "Average test loss: 0.0034543484865377345\n",
      "Epoch 164/300\n",
      "Average training loss: 0.021306027313073475\n",
      "Average test loss: 0.0034975898009207513\n",
      "Epoch 165/300\n",
      "Average training loss: 0.021294260988632838\n",
      "Average test loss: 0.003651725702815586\n",
      "Epoch 166/300\n",
      "Average training loss: 0.021307338077161047\n",
      "Average test loss: 0.003450106026811732\n",
      "Epoch 167/300\n",
      "Average training loss: 0.021280449610617425\n",
      "Average test loss: 0.0034923607162717315\n",
      "Epoch 168/300\n",
      "Average training loss: 0.021254997632569737\n",
      "Average test loss: 0.003462448488507006\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021269812231262526\n",
      "Average test loss: 0.0034969845774273076\n",
      "Epoch 170/300\n",
      "Average training loss: 0.021271681987577014\n",
      "Average test loss: 0.0034379463748385507\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02123555716044373\n",
      "Average test loss: 0.003408724149585598\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02124079624315103\n",
      "Average test loss: 0.003504948360017604\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02122966931263606\n",
      "Average test loss: 0.0035830890010628436\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0212491238978174\n",
      "Average test loss: 0.003806970750292142\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0212257519364357\n",
      "Average test loss: 0.003549329104522864\n",
      "Epoch 176/300\n",
      "Average training loss: 0.021158317392071087\n",
      "Average test loss: 0.0034947952901323636\n",
      "Epoch 177/300\n",
      "Average training loss: 0.021192552763554785\n",
      "Average test loss: 0.0034976078673369356\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02119219124317169\n",
      "Average test loss: 0.0035611775542298955\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021165215376350615\n",
      "Average test loss: 0.003462038402342134\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02117475937638018\n",
      "Average test loss: 0.003494763976997799\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021197385690278476\n",
      "Average test loss: 0.0035740875001582834\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021119879176219304\n",
      "Average test loss: 0.0036346512352012925\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02114085554414325\n",
      "Average test loss: 0.0035404890506631798\n",
      "Epoch 184/300\n",
      "Average training loss: 0.021146183090077507\n",
      "Average test loss: 0.0034778335849857994\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021114975823296443\n",
      "Average test loss: 0.0037599562677658265\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021158097645474806\n",
      "Average test loss: 0.003539840944732229\n",
      "Epoch 187/300\n",
      "Average training loss: 0.021099124355448615\n",
      "Average test loss: 0.003589003162458539\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02114497817142142\n",
      "Average test loss: 0.004035387389154898\n",
      "Epoch 189/300\n",
      "Average training loss: 0.021094622681538264\n",
      "Average test loss: 0.0034906168611099323\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021083452362153265\n",
      "Average test loss: 0.0034553524549636575\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02109108429816034\n",
      "Average test loss: 0.003541110703514682\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02105810083448887\n",
      "Average test loss: 0.0034722951367083523\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021065969445639186\n",
      "Average test loss: 0.003547617423451609\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021112018704414367\n",
      "Average test loss: 0.003497285360677375\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021050170640150707\n",
      "Average test loss: 0.0036273767376939457\n",
      "Epoch 196/300\n",
      "Average training loss: 0.021054091576072905\n",
      "Average test loss: 0.0037505131748815377\n",
      "Epoch 197/300\n",
      "Average training loss: 0.021052644943197567\n",
      "Average test loss: 0.0037692564908001157\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021063224717974664\n",
      "Average test loss: 0.0037038416862487794\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021049064035216968\n",
      "Average test loss: 0.003582010251780351\n",
      "Epoch 200/300\n",
      "Average training loss: 0.021008296463224625\n",
      "Average test loss: 0.0034323647502395842\n",
      "Epoch 201/300\n",
      "Average training loss: 0.021000945900877317\n",
      "Average test loss: 0.0034660157557162975\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02101430394583278\n",
      "Average test loss: 0.003536534660185377\n",
      "Epoch 203/300\n",
      "Average training loss: 0.020997390492094887\n",
      "Average test loss: 0.003548107807214061\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02100440189242363\n",
      "Average test loss: 0.0036466088539196385\n",
      "Epoch 205/300\n",
      "Average training loss: 0.020965933922264313\n",
      "Average test loss: 0.003539474759250879\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020995436471369533\n",
      "Average test loss: 0.003704650695125262\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02095790690680345\n",
      "Average test loss: 0.003529421874218517\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020965275054176648\n",
      "Average test loss: 0.021330719616678026\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02096522898806466\n",
      "Average test loss: 0.0034831233717915085\n",
      "Epoch 210/300\n",
      "Average training loss: 0.020938009071681234\n",
      "Average test loss: 0.0034913529606742993\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02095836349328359\n",
      "Average test loss: 0.003505474469314019\n",
      "Epoch 212/300\n",
      "Average training loss: 0.020922545451256963\n",
      "Average test loss: 0.0039663873745335474\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02095260279211733\n",
      "Average test loss: 0.0034690925899065203\n",
      "Epoch 214/300\n",
      "Average training loss: 0.020925645195775562\n",
      "Average test loss: 0.00344107976079815\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020898791002730528\n",
      "Average test loss: 0.0036615021307435302\n",
      "Epoch 216/300\n",
      "Average training loss: 0.020959233248399364\n",
      "Average test loss: 0.0035865954255892172\n",
      "Epoch 217/300\n",
      "Average training loss: 0.020919562795095974\n",
      "Average test loss: 0.003842830411468943\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020877043447560733\n",
      "Average test loss: 0.0034858924301548137\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020893350703848734\n",
      "Average test loss: 0.003508116289973259\n",
      "Epoch 220/300\n",
      "Average training loss: 0.020895598754286766\n",
      "Average test loss: 0.00345349020883441\n",
      "Epoch 221/300\n",
      "Average training loss: 0.02087985108130508\n",
      "Average test loss: 0.003453209560778406\n",
      "Epoch 222/300\n",
      "Average training loss: 0.020896701695190537\n",
      "Average test loss: 0.0035554107951207294\n",
      "Epoch 223/300\n",
      "Average training loss: 0.020847891418470276\n",
      "Average test loss: 0.0035898928480843704\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02087208813097742\n",
      "Average test loss: 0.0035563352743370664\n",
      "Epoch 225/300\n",
      "Average training loss: 0.020866247321168582\n",
      "Average test loss: 0.0034576189822206893\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02084340884950426\n",
      "Average test loss: 0.003635061760743459\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02084043951994843\n",
      "Average test loss: 0.003599703086954024\n",
      "Epoch 228/300\n",
      "Average training loss: 0.020837205817302067\n",
      "Average test loss: 0.003561452325226532\n",
      "Epoch 229/300\n",
      "Average training loss: 0.020861561690767607\n",
      "Average test loss: 0.0034584642226497334\n",
      "Epoch 230/300\n",
      "Average training loss: 0.020819098838501505\n",
      "Average test loss: 0.0035399166233837606\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020842709796296224\n",
      "Average test loss: 0.0035699290885693497\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02081856943666935\n",
      "Average test loss: 0.003536376905731029\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020806362587544654\n",
      "Average test loss: 0.0035674721284045113\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02079138994548056\n",
      "Average test loss: 0.003466107149918874\n",
      "Epoch 235/300\n",
      "Average training loss: 0.020793622377845974\n",
      "Average test loss: 0.0036238744515511723\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020796055760648517\n",
      "Average test loss: 0.003659904471081164\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020811990540888576\n",
      "Average test loss: 0.0035911688757025535\n",
      "Epoch 238/300\n",
      "Average training loss: 0.020772102796369128\n",
      "Average test loss: 0.0037017791817585627\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02076626453962591\n",
      "Average test loss: 0.003609043808033069\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020790112480521202\n",
      "Average test loss: 0.0035700843959218928\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02078223969042301\n",
      "Average test loss: 0.003582074568917354\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02079389642841286\n",
      "Average test loss: 0.003595403485827976\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02075141672127777\n",
      "Average test loss: 0.0034535478663941226\n",
      "Epoch 244/300\n",
      "Average training loss: 0.020767460856172773\n",
      "Average test loss: 0.00349619730686148\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020734299858411152\n",
      "Average test loss: 0.003539847325016227\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02077776060005029\n",
      "Average test loss: 0.003486813704793652\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020750108583105935\n",
      "Average test loss: 0.003592352765301863\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02073835140797827\n",
      "Average test loss: 0.0035379939919544592\n",
      "Epoch 249/300\n",
      "Average training loss: 0.020708078116178513\n",
      "Average test loss: 0.0037046210256715616\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020748130415876708\n",
      "Average test loss: 0.0035062595763140254\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0206974365943008\n",
      "Average test loss: 0.0036704823217458193\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02069103687008222\n",
      "Average test loss: 0.0035058907251805066\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020677452304297024\n",
      "Average test loss: 0.0035594738895694416\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02070343236459626\n",
      "Average test loss: 0.003514785976873504\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020713087211052576\n",
      "Average test loss: 0.003587499841219849\n",
      "Epoch 256/300\n",
      "Average training loss: 0.020689641616410678\n",
      "Average test loss: 0.0036210591132856077\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020698646917939188\n",
      "Average test loss: 0.003498716091944112\n",
      "Epoch 258/300\n",
      "Average training loss: 0.020676647651526662\n",
      "Average test loss: 0.0034932545518709555\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020668446766005624\n",
      "Average test loss: 0.0035955930314958097\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02065194747514195\n",
      "Average test loss: 0.0035321865025907755\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02069829023546643\n",
      "Average test loss: 0.0037500453367829323\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020656975279251734\n",
      "Average test loss: 0.003687977468387948\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02065732095307774\n",
      "Average test loss: 0.003618019499298599\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02064371845126152\n",
      "Average test loss: 0.0035416426559289295\n",
      "Epoch 265/300\n",
      "Average training loss: 0.020664787535866102\n",
      "Average test loss: 0.0035688147050225074\n",
      "Epoch 266/300\n",
      "Average training loss: 0.020636702343821527\n",
      "Average test loss: 0.003686477253420485\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02060670509768857\n",
      "Average test loss: 0.0036188625055882666\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02066468394961622\n",
      "Average test loss: 0.0035714517430298857\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0206045553104745\n",
      "Average test loss: 0.003530739255870382\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020602075601617496\n",
      "Average test loss: 0.003586854047866331\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020632958887351883\n",
      "Average test loss: 0.003598579222957293\n",
      "Epoch 272/300\n",
      "Average training loss: 0.020629263237118722\n",
      "Average test loss: 0.0036139864013012913\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02061007869740327\n",
      "Average test loss: 0.004226215894023578\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020591135500205887\n",
      "Average test loss: 0.003521879128076964\n",
      "Epoch 275/300\n",
      "Average training loss: 0.020615722944339117\n",
      "Average test loss: 0.003557803873386648\n",
      "Epoch 276/300\n",
      "Average training loss: 0.020581299935777984\n",
      "Average test loss: 0.0035635001876701913\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020589638907048437\n",
      "Average test loss: 0.00354367068865233\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02058177424967289\n",
      "Average test loss: 0.003630874718022015\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02059083987110191\n",
      "Average test loss: 0.003655166108575132\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02055957569430272\n",
      "Average test loss: 0.003537477205817898\n",
      "Epoch 281/300\n",
      "Average training loss: 0.020577231047881974\n",
      "Average test loss: 0.003610732030744354\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020569098591804506\n",
      "Average test loss: 0.0035890587146083512\n",
      "Epoch 283/300\n",
      "Average training loss: 0.020584511164161896\n",
      "Average test loss: 0.004822524505356947\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02056446899804804\n",
      "Average test loss: 0.0036275962469064526\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02053656074239148\n",
      "Average test loss: 0.003733395654294226\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02056419748067856\n",
      "Average test loss: 0.0034820117679321104\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020518490647276243\n",
      "Average test loss: 0.0036880001020100383\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02055148499376244\n",
      "Average test loss: 0.0035918652947164245\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02054896655678749\n",
      "Average test loss: 0.003615395083402594\n",
      "Epoch 290/300\n",
      "Average training loss: 0.020561009236507947\n",
      "Average test loss: 0.003563419392125474\n",
      "Epoch 291/300\n",
      "Average training loss: 0.020525372066431576\n",
      "Average test loss: 0.003574083363844289\n",
      "Epoch 292/300\n",
      "Average training loss: 0.020575621157884597\n",
      "Average test loss: 0.004120518429411782\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02051917048295339\n",
      "Average test loss: 0.003469784598590599\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02051921507053905\n",
      "Average test loss: 0.0036051472489618593\n",
      "Epoch 295/300\n",
      "Average training loss: 0.020521954228480657\n",
      "Average test loss: 0.003756249137636688\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020497167065739633\n",
      "Average test loss: 0.0035304274559020995\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020502892326977516\n",
      "Average test loss: 0.003624238403927949\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02050408307214578\n",
      "Average test loss: 0.0038207408423639004\n",
      "Epoch 299/300\n",
      "Average training loss: 0.020484144864810837\n",
      "Average test loss: 0.0037368632232149443\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02060747169620461\n",
      "Average test loss: 0.0035161934275594023\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1221862872077359\n",
      "Average test loss: 0.006020911110772027\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04223594723145167\n",
      "Average test loss: 0.004834285725942917\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03715930916203393\n",
      "Average test loss: 0.004444702842169338\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03428226765327983\n",
      "Average test loss: 0.004400255442286531\n",
      "Epoch 5/300\n",
      "Average training loss: 0.031975175105863146\n",
      "Average test loss: 0.0045287128877308635\n",
      "Epoch 6/300\n",
      "Average training loss: 0.030211353537109163\n",
      "Average test loss: 0.003815940384235647\n",
      "Epoch 7/300\n",
      "Average training loss: 0.028521168230308428\n",
      "Average test loss: 0.0037387529723346235\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02726083981494109\n",
      "Average test loss: 0.0037134837170855867\n",
      "Epoch 9/300\n",
      "Average training loss: 0.026387643660108248\n",
      "Average test loss: 0.003952119269925687\n",
      "Epoch 10/300\n",
      "Average training loss: 0.025456318216191397\n",
      "Average test loss: 0.0035313209436006014\n",
      "Epoch 11/300\n",
      "Average training loss: 0.02477237712426318\n",
      "Average test loss: 0.00358459091393484\n",
      "Epoch 12/300\n",
      "Average training loss: 0.024137366006771722\n",
      "Average test loss: 0.003371555128859149\n",
      "Epoch 13/300\n",
      "Average training loss: 0.023611881443195874\n",
      "Average test loss: 0.003166966701961226\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02315866709748904\n",
      "Average test loss: 0.0030314666852355003\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0226637253281143\n",
      "Average test loss: 0.003433048669041859\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02243938170042303\n",
      "Average test loss: 0.002936836360229386\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02195519088870949\n",
      "Average test loss: 0.0029763621143582795\n",
      "Epoch 18/300\n",
      "Average training loss: 0.021659269663194817\n",
      "Average test loss: 0.002812617310322821\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021398483225040964\n",
      "Average test loss: 0.002912851540578736\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02115696658856339\n",
      "Average test loss: 0.0027783800328357354\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020963322051697306\n",
      "Average test loss: 0.0026849370662950806\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020760713204741477\n",
      "Average test loss: 0.002728471296115054\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020585561250646908\n",
      "Average test loss: 0.002612780924265583\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020411028693119684\n",
      "Average test loss: 0.00259253842342231\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020227403985957303\n",
      "Average test loss: 0.002557201407953269\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020108607956104807\n",
      "Average test loss: 0.0025452180879397524\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02003795464668009\n",
      "Average test loss: 0.002567524484462208\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01989672142929501\n",
      "Average test loss: 0.002509111151099205\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019775941456357637\n",
      "Average test loss: 0.002946076914668083\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019665365478230848\n",
      "Average test loss: 0.002566709669513835\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019558886986639764\n",
      "Average test loss: 0.0024767186854862504\n",
      "Epoch 32/300\n",
      "Average training loss: 0.019455854604641595\n",
      "Average test loss: 0.002541576799729632\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019376195554931957\n",
      "Average test loss: 0.0024489036224161586\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019296689291795094\n",
      "Average test loss: 0.0025035385133491623\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01918432350539499\n",
      "Average test loss: 0.002465008669015434\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01912765939368142\n",
      "Average test loss: 0.002488084401107497\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01906921144657665\n",
      "Average test loss: 0.0024884841224799554\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01900418428911103\n",
      "Average test loss: 0.002391216499110063\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01891711970915397\n",
      "Average test loss: 0.0023902958488712708\n",
      "Epoch 40/300\n",
      "Average training loss: 0.018871139964295756\n",
      "Average test loss: 0.002379137831533121\n",
      "Epoch 41/300\n",
      "Average training loss: 0.018856113806366922\n",
      "Average test loss: 0.0023657398325287633\n",
      "Epoch 42/300\n",
      "Average training loss: 0.018786241408851413\n",
      "Average test loss: 0.003941692310902807\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01873250855753819\n",
      "Average test loss: 0.0023706069118860696\n",
      "Epoch 44/300\n",
      "Average training loss: 0.018699166557855078\n",
      "Average test loss: 0.0024775452305459316\n",
      "Epoch 45/300\n",
      "Average training loss: 0.018643854009608427\n",
      "Average test loss: 0.0025621272873961264\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01856709819700983\n",
      "Average test loss: 0.002377098949833049\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01856405342121919\n",
      "Average test loss: 0.0024205129680534205\n",
      "Epoch 48/300\n",
      "Average training loss: 0.018514001425769595\n",
      "Average test loss: 0.00237771618925035\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01848939112822215\n",
      "Average test loss: 0.0023525362950232295\n",
      "Epoch 50/300\n",
      "Average training loss: 0.018447682168748643\n",
      "Average test loss: 0.002479874684785803\n",
      "Epoch 51/300\n",
      "Average training loss: 0.018394971794552274\n",
      "Average test loss: 0.0023495002455181544\n",
      "Epoch 52/300\n",
      "Average training loss: 0.018372415227194627\n",
      "Average test loss: 0.002370254085916612\n",
      "Epoch 53/300\n",
      "Average training loss: 0.018316719762153096\n",
      "Average test loss: 0.0023274908943308726\n",
      "Epoch 54/300\n",
      "Average training loss: 0.018261938475900227\n",
      "Average test loss: 0.0024249408715921973\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01827752843995889\n",
      "Average test loss: 0.0024425294128143124\n",
      "Epoch 56/300\n",
      "Average training loss: 0.018218252332674132\n",
      "Average test loss: 0.0024052705403624307\n",
      "Epoch 57/300\n",
      "Average training loss: 0.018220359855228\n",
      "Average test loss: 0.002430787201453414\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01815557196405199\n",
      "Average test loss: 0.002336286591262453\n",
      "Epoch 59/300\n",
      "Average training loss: 0.018115308495859305\n",
      "Average test loss: 0.002410422882479098\n",
      "Epoch 60/300\n",
      "Average training loss: 0.018091642493175134\n",
      "Average test loss: 0.0024712266193495855\n",
      "Epoch 61/300\n",
      "Average training loss: 0.018074828833341598\n",
      "Average test loss: 0.00232426231354475\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0180386435257064\n",
      "Average test loss: 0.0023881852390865484\n",
      "Epoch 63/300\n",
      "Average training loss: 0.018031285322374767\n",
      "Average test loss: 0.0023664480415690275\n",
      "Epoch 64/300\n",
      "Average training loss: 0.017998190100822185\n",
      "Average test loss: 0.0023603339118676053\n",
      "Epoch 65/300\n",
      "Average training loss: 0.017955994055502945\n",
      "Average test loss: 0.0023276426572766567\n",
      "Epoch 66/300\n",
      "Average training loss: 0.017930834957294993\n",
      "Average test loss: 0.0023303403386639226\n",
      "Epoch 67/300\n",
      "Average training loss: 0.017947339683771133\n",
      "Average test loss: 0.0023571355200062197\n",
      "Epoch 68/300\n",
      "Average training loss: 0.017881386526756818\n",
      "Average test loss: 0.002315790902202328\n",
      "Epoch 69/300\n",
      "Average training loss: 0.017892889100644325\n",
      "Average test loss: 0.0025054190676245423\n",
      "Epoch 70/300\n",
      "Average training loss: 0.017850527824627028\n",
      "Average test loss: 0.0023189508238186437\n",
      "Epoch 71/300\n",
      "Average training loss: 0.017836397391226555\n",
      "Average test loss: 0.002391882079549962\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01782539435889986\n",
      "Average test loss: 0.002402987595854534\n",
      "Epoch 73/300\n",
      "Average training loss: 0.017771224424242973\n",
      "Average test loss: 0.0024971333137816854\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01779904503458076\n",
      "Average test loss: 0.0023140324300362004\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01774219328330623\n",
      "Average test loss: 0.002350380963128474\n",
      "Epoch 76/300\n",
      "Average training loss: 0.017691032939487032\n",
      "Average test loss: 0.002370438283930222\n",
      "Epoch 77/300\n",
      "Average training loss: 0.017722610983583662\n",
      "Average test loss: 0.002345306191386448\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01767470423546102\n",
      "Average test loss: 0.0023697004270636372\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01767862858209345\n",
      "Average test loss: 0.002335976594231195\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01766097214569648\n",
      "Average test loss: 0.0026532873331258696\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017625597758425607\n",
      "Average test loss: 0.0023333050643818245\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017634115538663335\n",
      "Average test loss: 0.002314883294618792\n",
      "Epoch 83/300\n",
      "Average training loss: 0.017574895852969754\n",
      "Average test loss: 0.002538725920021534\n",
      "Epoch 84/300\n",
      "Average training loss: 0.017582286041643885\n",
      "Average test loss: 0.002317733877234989\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01756809081054396\n",
      "Average test loss: 0.0024048768306771913\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017524246813522444\n",
      "Average test loss: 0.0026630420426113736\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01751514281332493\n",
      "Average test loss: 0.002332434858712885\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017513246561090153\n",
      "Average test loss: 0.0023104113967468342\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01750717058363888\n",
      "Average test loss: 0.0023455785440487996\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017485119223594664\n",
      "Average test loss: 0.0025077559244301583\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01743744834512472\n",
      "Average test loss: 0.002408399157433046\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01743318757911523\n",
      "Average test loss: 0.0023012016692923174\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01742171335220337\n",
      "Average test loss: 0.002342971170683288\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017423044423262278\n",
      "Average test loss: 0.0023063596362868943\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01740371064013905\n",
      "Average test loss: 0.0023652551364567545\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017377092180152733\n",
      "Average test loss: 0.0023372329014043013\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017374122305048836\n",
      "Average test loss: 0.0024958988637146023\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01736316093802452\n",
      "Average test loss: 0.002590890470271309\n",
      "Epoch 99/300\n",
      "Average training loss: 0.017327702949444454\n",
      "Average test loss: 0.002380137058181895\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01733304847445753\n",
      "Average test loss: 0.0022676611681365305\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01733317504160934\n",
      "Average test loss: 0.0023750001831601064\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0173570500065883\n",
      "Average test loss: 0.0023331469199102785\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01726480464802848\n",
      "Average test loss: 0.0023802575406928856\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01726278649436103\n",
      "Average test loss: 0.002302381806489494\n",
      "Epoch 105/300\n",
      "Average training loss: 0.017282135821051068\n",
      "Average test loss: 0.0023227208519561423\n",
      "Epoch 106/300\n",
      "Average training loss: 0.017266520508461527\n",
      "Average test loss: 0.0024068351623912653\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01725599756009049\n",
      "Average test loss: 0.002309549515652988\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01724210181004471\n",
      "Average test loss: 0.002325667412744628\n",
      "Epoch 109/300\n",
      "Average training loss: 0.017197992707292237\n",
      "Average test loss: 0.0024191456526103947\n",
      "Epoch 110/300\n",
      "Average training loss: 0.017252848100331095\n",
      "Average test loss: 0.002306690479318301\n",
      "Epoch 111/300\n",
      "Average training loss: 0.017158912190132672\n",
      "Average test loss: 0.002290882281959057\n",
      "Epoch 112/300\n",
      "Average training loss: 0.017151417169306012\n",
      "Average test loss: 0.0025680664454897245\n",
      "Epoch 113/300\n",
      "Average training loss: 0.017196601231892903\n",
      "Average test loss: 0.002384045090733303\n",
      "Epoch 114/300\n",
      "Average training loss: 0.017158346068527962\n",
      "Average test loss: 0.011237322843323152\n",
      "Epoch 115/300\n",
      "Average training loss: 0.017145638585090637\n",
      "Average test loss: 0.0023445057932080494\n",
      "Epoch 116/300\n",
      "Average training loss: 0.017123792837063473\n",
      "Average test loss: 0.0023176980819553136\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01712315352757772\n",
      "Average test loss: 0.0023529050494026805\n",
      "Epoch 118/300\n",
      "Average training loss: 0.017107379265957408\n",
      "Average test loss: 0.0024194460617792276\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01712929064200984\n",
      "Average test loss: 0.0023470246340665554\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017071910103989972\n",
      "Average test loss: 0.0023413732089102267\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017075807409154046\n",
      "Average test loss: 0.0023945897988354166\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01704175898515516\n",
      "Average test loss: 0.0026968155574674407\n",
      "Epoch 123/300\n",
      "Average training loss: 0.017065072941283385\n",
      "Average test loss: 0.0023059945541640953\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01702760703033871\n",
      "Average test loss: 0.00228915835192634\n",
      "Epoch 125/300\n",
      "Average training loss: 0.017017793774604797\n",
      "Average test loss: 0.0023423529704515306\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017039741835660406\n",
      "Average test loss: 0.0022894617710262537\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016996234357357025\n",
      "Average test loss: 0.002291720833008488\n",
      "Epoch 128/300\n",
      "Average training loss: 0.017028596590790483\n",
      "Average test loss: 0.0024056014938073026\n",
      "Epoch 129/300\n",
      "Average training loss: 0.017019386801454758\n",
      "Average test loss: 0.002347634597784943\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016971142675313685\n",
      "Average test loss: 0.003034502906103929\n",
      "Epoch 131/300\n",
      "Average training loss: 0.016963924000660578\n",
      "Average test loss: 0.0023590220944542024\n",
      "Epoch 132/300\n",
      "Average training loss: 0.017003470222983097\n",
      "Average test loss: 0.002310604494065046\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016948394965794353\n",
      "Average test loss: 0.0025158136991990936\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01695418163140615\n",
      "Average test loss: 0.0023372423909604548\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01693633125722408\n",
      "Average test loss: 0.0024474818267756038\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016929885711934833\n",
      "Average test loss: 0.0024678983570386964\n",
      "Epoch 137/300\n",
      "Average training loss: 0.016919791186849276\n",
      "Average test loss: 0.002284374221538504\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016904665170444384\n",
      "Average test loss: 0.0023639017041358684\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016904743273225095\n",
      "Average test loss: 0.0024350278805941343\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016912751587728658\n",
      "Average test loss: 0.002337789415071408\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016869433518913058\n",
      "Average test loss: 0.002395910011190507\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01687490939266152\n",
      "Average test loss: 0.0023359324543012513\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01685357538693481\n",
      "Average test loss: 0.002318074363180333\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01683859878944026\n",
      "Average test loss: 0.00230167263539301\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016859197253982227\n",
      "Average test loss: 0.002512979092076421\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016846487116482522\n",
      "Average test loss: 0.002383089908709129\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01686033010151651\n",
      "Average test loss: 0.002328957270623909\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016819256315628688\n",
      "Average test loss: 0.002371874844862355\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016823446688552696\n",
      "Average test loss: 0.0023664104050646227\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016779322195384236\n",
      "Average test loss: 0.0023195299661407866\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016789334242542586\n",
      "Average test loss: 0.0023225296563986276\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016803596153855325\n",
      "Average test loss: 0.0023266334573013915\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01678167094124688\n",
      "Average test loss: 0.0026314875607689222\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01678696822292275\n",
      "Average test loss: 0.002375335816707876\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01676924414601591\n",
      "Average test loss: 0.00231504531763494\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016749374230702718\n",
      "Average test loss: 0.002324097013618383\n",
      "Epoch 157/300\n",
      "Average training loss: 0.016741526515947448\n",
      "Average test loss: 0.002305840271628565\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01673033541523748\n",
      "Average test loss: 0.0023530529971337983\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016758361377649838\n",
      "Average test loss: 0.002327553105747534\n",
      "Epoch 160/300\n",
      "Average training loss: 0.016745012231998972\n",
      "Average test loss: 0.002372773136322697\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016742188844415876\n",
      "Average test loss: 0.0022797504077768987\n",
      "Epoch 162/300\n",
      "Average training loss: 0.016713392852081194\n",
      "Average test loss: 0.002315724183080925\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01668020082016786\n",
      "Average test loss: 0.016825265001919533\n",
      "Epoch 164/300\n",
      "Average training loss: 0.016717395512594116\n",
      "Average test loss: 0.0023440121265335214\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01674390391425954\n",
      "Average test loss: 0.0023185429332984817\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01665810006029076\n",
      "Average test loss: 0.002302369430454241\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01665573259856966\n",
      "Average test loss: 0.0023335700678742593\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01667197564078702\n",
      "Average test loss: 0.002345674196465148\n",
      "Epoch 169/300\n",
      "Average training loss: 0.016650320681432882\n",
      "Average test loss: 0.0023713660548544593\n",
      "Epoch 170/300\n",
      "Average training loss: 0.016654478443993464\n",
      "Average test loss: 0.002381448635003633\n",
      "Epoch 171/300\n",
      "Average training loss: 0.016660704836249353\n",
      "Average test loss: 0.00229755812490152\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01663030057483249\n",
      "Average test loss: 0.0023418035234014194\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01664783547487524\n",
      "Average test loss: 0.0022977654313047725\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01665750799824794\n",
      "Average test loss: 0.002375444270256493\n",
      "Epoch 175/300\n",
      "Average training loss: 0.016641010098987156\n",
      "Average test loss: 0.002413913941424754\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01659788846804036\n",
      "Average test loss: 0.002291870083245966\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01659386124379105\n",
      "Average test loss: 0.002357856654872497\n",
      "Epoch 178/300\n",
      "Average training loss: 0.016594128601253033\n",
      "Average test loss: 0.0023740773329304324\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01660366192791197\n",
      "Average test loss: 0.0023695556422074636\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01658377273546325\n",
      "Average test loss: 0.002363519594590697\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016593991306093004\n",
      "Average test loss: 0.0024505541668170027\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016581217755046156\n",
      "Average test loss: 0.0023512262817886142\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016554670623607104\n",
      "Average test loss: 0.0023321796604949567\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016559380610783894\n",
      "Average test loss: 0.002338513820328646\n",
      "Epoch 185/300\n",
      "Average training loss: 0.016540045213368205\n",
      "Average test loss: 0.0025046232723527485\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01654277359942595\n",
      "Average test loss: 0.002360013941509856\n",
      "Epoch 187/300\n",
      "Average training loss: 0.016523054843975437\n",
      "Average test loss: 0.0023294386306984558\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01655559289786551\n",
      "Average test loss: 0.0024072841650082007\n",
      "Epoch 189/300\n",
      "Average training loss: 0.016502972406645614\n",
      "Average test loss: 0.0023733782443321415\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016529053391681776\n",
      "Average test loss: 0.0022943569094770484\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01650380939576361\n",
      "Average test loss: 0.0024328805649032194\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016526514208979076\n",
      "Average test loss: 0.0024768378676639662\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016501948051982455\n",
      "Average test loss: 0.0023265676252130006\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016496501904394893\n",
      "Average test loss: 0.002313516663801339\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016518867214520772\n",
      "Average test loss: 0.002437954672301809\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016492706373333933\n",
      "Average test loss: 0.0023797774479414027\n",
      "Epoch 197/300\n",
      "Average training loss: 0.016496628268725343\n",
      "Average test loss: 0.0023540599991877876\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016462452227870624\n",
      "Average test loss: 0.002413595953749286\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01645829175081518\n",
      "Average test loss: 0.003906945369102888\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016470916398697428\n",
      "Average test loss: 0.0023824433204200535\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016462648463745913\n",
      "Average test loss: 0.0024755622022267845\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01644539345138603\n",
      "Average test loss: 0.002345368870223562\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01645887990627024\n",
      "Average test loss: 0.0023496116058280073\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016460039908687274\n",
      "Average test loss: 0.002391804323221246\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01646448515769508\n",
      "Average test loss: 0.002353805059670574\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016409468887580766\n",
      "Average test loss: 0.002343896476034489\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016435429379343988\n",
      "Average test loss: 0.002623390680592921\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01640724018216133\n",
      "Average test loss: 0.002330955112352967\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016402425390150813\n",
      "Average test loss: 0.00236379038501117\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01641214463611444\n",
      "Average test loss: 0.0023871350617458422\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01638474837359455\n",
      "Average test loss: 0.0024072126806196237\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016407031213243804\n",
      "Average test loss: 0.002309803913657864\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01640897638930215\n",
      "Average test loss: 0.002315528699507316\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01638888071394629\n",
      "Average test loss: 0.0024753554962161513\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01636349847084946\n",
      "Average test loss: 0.002304699334005515\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01637217155098915\n",
      "Average test loss: 0.0024268780706657306\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01639306749569045\n",
      "Average test loss: 0.002399733883121775\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01634894864923424\n",
      "Average test loss: 0.0023410302633419633\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016346001885831355\n",
      "Average test loss: 0.002450338668707344\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016355853577454886\n",
      "Average test loss: 0.002477523872628808\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01634361479514175\n",
      "Average test loss: 0.0024808116325487693\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016361806899309158\n",
      "Average test loss: 0.0024875460144960218\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01632514942354626\n",
      "Average test loss: 0.0023921226585904756\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01633117239508364\n",
      "Average test loss: 0.002457630859894885\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01634970316125287\n",
      "Average test loss: 0.002352519885947307\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01630611547993289\n",
      "Average test loss: 0.0025985032344857852\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01633726295000977\n",
      "Average test loss: 0.002301091764225728\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016311650053494508\n",
      "Average test loss: 0.00238186331341664\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016307190333803494\n",
      "Average test loss: 0.0024609225663459965\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01631011595990923\n",
      "Average test loss: 0.002364951601665881\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016286277507742246\n",
      "Average test loss: 0.0023115524671350916\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016313991074760757\n",
      "Average test loss: 0.002323255174482862\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01627966705047422\n",
      "Average test loss: 0.00239042232485695\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01629581563340293\n",
      "Average test loss: 0.0023663634192198513\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016279294406374295\n",
      "Average test loss: 0.002357416926158799\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01627779629495409\n",
      "Average test loss: 0.002399056437942717\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016260206447707284\n",
      "Average test loss: 0.002385220083615018\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01627034750994709\n",
      "Average test loss: 0.0025789713729172945\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01625020136601395\n",
      "Average test loss: 0.0024788390735371243\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016241260280211767\n",
      "Average test loss: 0.0026177819605088895\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01624845252931118\n",
      "Average test loss: 0.0023085041056490605\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01622856538080507\n",
      "Average test loss: 0.0024717156218571797\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01627704618540075\n",
      "Average test loss: 0.0023545509276704656\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01624353202515178\n",
      "Average test loss: 0.0023650193384124173\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016263307952218586\n",
      "Average test loss: 0.002388481312120954\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016240948839320077\n",
      "Average test loss: 0.002391423811722133\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016216348439455033\n",
      "Average test loss: 0.002479711530937089\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016219009670946333\n",
      "Average test loss: 0.0025105220046308307\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016213419480456247\n",
      "Average test loss: 0.0023541102305882507\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01622826643784841\n",
      "Average test loss: 0.0024545405606428783\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016208878108196788\n",
      "Average test loss: 0.0024748252098345095\n",
      "Epoch 252/300\n",
      "Average training loss: 0.016194575510091253\n",
      "Average test loss: 0.002348367626882262\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016196552048954697\n",
      "Average test loss: 0.0023946978491213586\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01622584434847037\n",
      "Average test loss: 0.002358858841781815\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016200501006510524\n",
      "Average test loss: 0.002314502952620387\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016195579815242025\n",
      "Average test loss: 0.0023691966293586625\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01618870812571711\n",
      "Average test loss: 0.0023470253532545432\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01619484654814005\n",
      "Average test loss: 0.0024524624983055725\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016180670395493507\n",
      "Average test loss: 0.002423316951220234\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016167042199108337\n",
      "Average test loss: 0.0023464434302101532\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01617821800046497\n",
      "Average test loss: 0.0023512018227742778\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016160484907527764\n",
      "Average test loss: 0.002411276399364902\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016152257445785734\n",
      "Average test loss: 0.0023565053107837837\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016156839694413873\n",
      "Average test loss: 0.0023914285191438266\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016173587151699597\n",
      "Average test loss: 0.0023681768708758884\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0161671280099286\n",
      "Average test loss: 0.0025362712995459637\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0161420707081755\n",
      "Average test loss: 0.0024245712606029376\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016136665316919485\n",
      "Average test loss: 0.0024560078450789054\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01613380615082052\n",
      "Average test loss: 0.0023770028952923086\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016138156464530363\n",
      "Average test loss: 0.0023973151386405033\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016160787201590008\n",
      "Average test loss: 0.002398959767487314\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016113375138905314\n",
      "Average test loss: 0.0023941826329876978\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016115435239341523\n",
      "Average test loss: 0.002444551992954479\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016142844985756608\n",
      "Average test loss: 0.0024288375114815105\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016115287244319917\n",
      "Average test loss: 0.002437590419107841\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01609401640213198\n",
      "Average test loss: 0.00242532461664329\n",
      "Epoch 277/300\n",
      "Average training loss: 0.016111255780690246\n",
      "Average test loss: 0.0024220219022697874\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016105957368181812\n",
      "Average test loss: 0.0024415638953861262\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016097706622547572\n",
      "Average test loss: 0.002450830455455515\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016086007810301253\n",
      "Average test loss: 0.002409366321232584\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016090628387199508\n",
      "Average test loss: 0.0023700269855972795\n",
      "Epoch 282/300\n",
      "Average training loss: 0.016102440080708928\n",
      "Average test loss: 0.0026067159194499254\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016077915474772452\n",
      "Average test loss: 0.002451448834811648\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016077018154991997\n",
      "Average test loss: 0.0023831518531466523\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01606950782984495\n",
      "Average test loss: 0.002386227533221245\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016069397120012176\n",
      "Average test loss: 0.0024416730737106666\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01608659666776657\n",
      "Average test loss: 0.0023762922468077807\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016064253876606625\n",
      "Average test loss: 0.002384765774011612\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016092330690887238\n",
      "Average test loss: 0.0024174852133211164\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01606488702032301\n",
      "Average test loss: 0.002414519652310345\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01605313931571113\n",
      "Average test loss: 0.0023542575755467017\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016032653351624807\n",
      "Average test loss: 0.0023878046358004213\n",
      "Epoch 293/300\n",
      "Average training loss: 0.016052852457596196\n",
      "Average test loss: 0.002377538985055354\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016029289036989212\n",
      "Average test loss: 0.0023989648158765503\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01605272846751743\n",
      "Average test loss: 0.002395503633862568\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016038483074969717\n",
      "Average test loss: 0.0024147940271844466\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016036557800239985\n",
      "Average test loss: 0.0027477151395546066\n",
      "Epoch 298/300\n",
      "Average training loss: 0.016035476267337798\n",
      "Average test loss: 0.002410267752284805\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01603234186106258\n",
      "Average test loss: 0.0024909790712926124\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016021242092053096\n",
      "Average test loss: 0.0023589038512566024\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1102391972442468\n",
      "Average test loss: 0.005119413178828028\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03614591882295079\n",
      "Average test loss: 0.004058637604117394\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03141388084822231\n",
      "Average test loss: 0.004218592557021313\n",
      "Epoch 4/300\n",
      "Average training loss: 0.028829375359747146\n",
      "Average test loss: 0.0035809229152897995\n",
      "Epoch 5/300\n",
      "Average training loss: 0.027051750257611275\n",
      "Average test loss: 0.003054281199764874\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02515017675525612\n",
      "Average test loss: 0.0030891455391214954\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02394325108660592\n",
      "Average test loss: 0.002998311423800058\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022686702301104863\n",
      "Average test loss: 0.0028832367757956187\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021764549306697314\n",
      "Average test loss: 0.0027843209279494155\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02099610973563459\n",
      "Average test loss: 0.00253626222598056\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020398974572618803\n",
      "Average test loss: 0.0025712104325907098\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019855159575740498\n",
      "Average test loss: 0.0025505016392303836\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01939943931831254\n",
      "Average test loss: 0.0022514160242345597\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019045899560054144\n",
      "Average test loss: 0.00244672908861604\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018670221656560897\n",
      "Average test loss: 0.0024097364751829043\n",
      "Epoch 16/300\n",
      "Average training loss: 0.018355255661739244\n",
      "Average test loss: 0.002150645004171464\n",
      "Epoch 17/300\n",
      "Average training loss: 0.018144201894601187\n",
      "Average test loss: 0.0021264697364014058\n",
      "Epoch 18/300\n",
      "Average training loss: 0.017808201567994224\n",
      "Average test loss: 0.0022449899098525446\n",
      "Epoch 19/300\n",
      "Average training loss: 0.017627192454205617\n",
      "Average test loss: 0.0021419974146410824\n",
      "Epoch 20/300\n",
      "Average training loss: 0.017457791729105842\n",
      "Average test loss: 0.0019462339056448804\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01724643711745739\n",
      "Average test loss: 0.002062034422945645\n",
      "Epoch 22/300\n",
      "Average training loss: 0.017126752943628365\n",
      "Average test loss: 0.0019995989573912487\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016955455038282607\n",
      "Average test loss: 0.0019477476953632302\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01684907371468014\n",
      "Average test loss: 0.002078481312841177\n",
      "Epoch 25/300\n",
      "Average training loss: 0.016726781899730366\n",
      "Average test loss: 0.0019159683554122846\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01661564322974947\n",
      "Average test loss: 0.001894093117987116\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016460150664051373\n",
      "Average test loss: 0.0018502425470699867\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016430448272162012\n",
      "Average test loss: 0.0019384600938194328\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016282188082734742\n",
      "Average test loss: 0.0018148045005897681\n",
      "Epoch 30/300\n",
      "Average training loss: 0.016214483555820253\n",
      "Average test loss: 0.0018518735725018714\n",
      "Epoch 31/300\n",
      "Average training loss: 0.016143419575360085\n",
      "Average test loss: 0.0017757227946486739\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016045811441209583\n",
      "Average test loss: 0.0018373254203341073\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0160043971132901\n",
      "Average test loss: 0.0017941710161459114\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015986297448476156\n",
      "Average test loss: 0.0018058799084069\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015890891345838706\n",
      "Average test loss: 0.0017732030641701487\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015792222237421407\n",
      "Average test loss: 0.0017766800775296159\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015758566689987978\n",
      "Average test loss: 0.0017480278099990553\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01567805880970425\n",
      "Average test loss: 0.0018007465886572998\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015668681156304147\n",
      "Average test loss: 0.001805864345903198\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015638127469354206\n",
      "Average test loss: 0.0017775561548769474\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015562008132537206\n",
      "Average test loss: 0.0017201548245631985\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015511092383828428\n",
      "Average test loss: 0.0017384579423815013\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015504446485804187\n",
      "Average test loss: 0.0017417917912825943\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0154334620717499\n",
      "Average test loss: 0.0017116196849900815\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015438162138892545\n",
      "Average test loss: 0.0018209748046679628\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01539559447682566\n",
      "Average test loss: 0.0017706176098436117\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015311343635949824\n",
      "Average test loss: 0.001697751479326851\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015288450984491242\n",
      "Average test loss: 0.0017134304204955697\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015253970414400101\n",
      "Average test loss: 0.0017450081974060999\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015248009805878003\n",
      "Average test loss: 0.0017124279975477192\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015209025144577027\n",
      "Average test loss: 0.0017019103157023588\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015146589298215177\n",
      "Average test loss: 0.001713837627114521\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015166112368305524\n",
      "Average test loss: 0.0016893235792716345\n",
      "Epoch 54/300\n",
      "Average training loss: 0.015093041110369894\n",
      "Average test loss: 0.0019235342788613504\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01509898745516936\n",
      "Average test loss: 0.0017767977386506067\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015049902766942978\n",
      "Average test loss: 0.0017082696203142404\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015022433946530024\n",
      "Average test loss: 0.001694527492341068\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014981393060750432\n",
      "Average test loss: 0.0016969904262158606\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014999747946030564\n",
      "Average test loss: 0.0017112184323163496\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014945965267303917\n",
      "Average test loss: 0.001783396735580431\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014924196588496367\n",
      "Average test loss: 0.0017566521160511507\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014906455488668547\n",
      "Average test loss: 0.0017768831451733906\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01488533396853341\n",
      "Average test loss: 0.0017049591761185891\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014869287057055368\n",
      "Average test loss: 0.0018100242341558138\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014819759568406476\n",
      "Average test loss: 0.0016749159358441829\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014840885596970717\n",
      "Average test loss: 0.0016524293549979726\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014797522246837617\n",
      "Average test loss: 0.002298696077739199\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014779584202501509\n",
      "Average test loss: 0.0018589229066338804\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014770131693945992\n",
      "Average test loss: 0.0016870543259299463\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01474595881998539\n",
      "Average test loss: 0.0017183870183717874\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014718755662441253\n",
      "Average test loss: 0.0017032648514335353\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01469997420658668\n",
      "Average test loss: 0.0016992378788482812\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014687858873771296\n",
      "Average test loss: 0.0016874548178166151\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01470091478443808\n",
      "Average test loss: 0.0017642384661982456\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01464525860051314\n",
      "Average test loss: 0.0016986694062749545\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01464825600054529\n",
      "Average test loss: 0.0017716136478508513\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014622256848547194\n",
      "Average test loss: 0.001677523823454976\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014596531988018089\n",
      "Average test loss: 0.0017336951897790034\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014578975235422452\n",
      "Average test loss: 0.0016956757691999277\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014569066774513986\n",
      "Average test loss: 0.0017146698942201006\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014535552159779601\n",
      "Average test loss: 0.0016967167105111812\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014547262060145538\n",
      "Average test loss: 0.00167990523411168\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01450261929879586\n",
      "Average test loss: 0.001680600044834945\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014524684130317635\n",
      "Average test loss: 0.0017241542583538425\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014513493458429972\n",
      "Average test loss: 0.0016693687071609828\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014458497199747298\n",
      "Average test loss: 0.00166789364349097\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014460202246490451\n",
      "Average test loss: 0.0017689167082102762\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01444637374000417\n",
      "Average test loss: 0.0016751009530077377\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014436573073267936\n",
      "Average test loss: 0.0017512424211535188\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014413753479719162\n",
      "Average test loss: 0.006099894819574223\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014440929542813036\n",
      "Average test loss: 0.0016871667175243298\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014376738857063982\n",
      "Average test loss: 0.0017081990341345468\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014374407206144598\n",
      "Average test loss: 0.0016719427777247297\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014355243022243181\n",
      "Average test loss: 0.001692996962616841\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014363306326998605\n",
      "Average test loss: 0.001718752660892076\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014343003378974067\n",
      "Average test loss: 0.0017702022210384408\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014341404133372837\n",
      "Average test loss: 0.0016694478486768073\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014330260516454776\n",
      "Average test loss: 0.001687220011320379\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014302114331059986\n",
      "Average test loss: 0.0017306724596354696\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014274339698255062\n",
      "Average test loss: 0.001693623067604171\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014282009362346595\n",
      "Average test loss: 0.0016551663105686505\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014297042465872234\n",
      "Average test loss: 0.0017019152931041187\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014255530370606317\n",
      "Average test loss: 0.0016584415743127465\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01424545411600007\n",
      "Average test loss: 0.0017486894857138395\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01425698469662004\n",
      "Average test loss: 0.0016513129579317238\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014242054124673208\n",
      "Average test loss: 0.001697746683532993\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014200746002296608\n",
      "Average test loss: 0.0016455418537888262\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014196590761343638\n",
      "Average test loss: 0.001658558941135804\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014194047738280561\n",
      "Average test loss: 0.0016529898686955373\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014168351174228721\n",
      "Average test loss: 0.0022133009082948166\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01419690104160044\n",
      "Average test loss: 0.0017979167978175812\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014172391972608037\n",
      "Average test loss: 0.0017373615611965458\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014161517332825396\n",
      "Average test loss: 0.0016491409099350374\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014151141579780313\n",
      "Average test loss: 0.0016874339770939614\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014125887602567673\n",
      "Average test loss: 0.0016551163920925723\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014133832979119486\n",
      "Average test loss: 0.0017906416261361704\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014110476028588084\n",
      "Average test loss: 0.0017229666691273451\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014118710421853595\n",
      "Average test loss: 0.0016764336863739624\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014088153393732176\n",
      "Average test loss: 0.0016681091609514421\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014081162670420276\n",
      "Average test loss: 0.0016658431563733353\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014088083611594306\n",
      "Average test loss: 0.0017142610393671527\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014068867865535947\n",
      "Average test loss: 0.0018815737242499988\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014068352671133147\n",
      "Average test loss: 0.001685031470325258\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014055894032120705\n",
      "Average test loss: 0.0016407875939168865\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014035712463988197\n",
      "Average test loss: 0.0016591731203306052\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014034758998288049\n",
      "Average test loss: 0.0017966811609350973\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01402332350032197\n",
      "Average test loss: 0.0016766603512482511\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014001588655842676\n",
      "Average test loss: 0.001730210679841952\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014023086575170358\n",
      "Average test loss: 0.0017145979773874084\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014011914947794543\n",
      "Average test loss: 0.0016794028085552984\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01396260778274801\n",
      "Average test loss: 0.001670939747761521\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013966551536487208\n",
      "Average test loss: 0.0017431239892418185\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013965583825276958\n",
      "Average test loss: 0.0016463150201986233\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013972269613709714\n",
      "Average test loss: 0.0017197345483841167\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013965380382206705\n",
      "Average test loss: 0.0018234214055248433\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013927246545751889\n",
      "Average test loss: 0.001701594703261637\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013937406402495172\n",
      "Average test loss: 0.0016961395849163334\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013926525413990021\n",
      "Average test loss: 0.0016781872173564302\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013931665679646862\n",
      "Average test loss: 0.0017935040030214522\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013925835552314917\n",
      "Average test loss: 0.0017405377391518817\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0139219328628646\n",
      "Average test loss: 0.0017011557693282764\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013931770405835575\n",
      "Average test loss: 0.0016696765252078574\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013880240301291149\n",
      "Average test loss: 0.0016723189264949824\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01390001840558317\n",
      "Average test loss: 0.0016618698442147839\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01387602200690243\n",
      "Average test loss: 0.0016640154840424656\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013877585680120522\n",
      "Average test loss: 0.0017174083894739547\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013855085327393478\n",
      "Average test loss: 0.0016857360728705923\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013863464255299832\n",
      "Average test loss: 0.0016630746440754996\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0138456206570069\n",
      "Average test loss: 0.0016678132232692507\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013842354541851414\n",
      "Average test loss: 0.0016980021871212457\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01381785270323356\n",
      "Average test loss: 0.0016838520957777898\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013840995449986723\n",
      "Average test loss: 0.001658035343202452\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013820204589102004\n",
      "Average test loss: 0.0016751073113539152\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013808639249867864\n",
      "Average test loss: 0.0016665379135972923\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013808644756674766\n",
      "Average test loss: 0.0017244092281907797\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013811826386385494\n",
      "Average test loss: 0.0016955125355679129\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013797391104201476\n",
      "Average test loss: 0.00169598365161154\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013776916914516026\n",
      "Average test loss: 0.0018197814522104131\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013784162845876483\n",
      "Average test loss: 0.0017266291458573606\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013752758275303575\n",
      "Average test loss: 0.001707489179033372\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013783765541182625\n",
      "Average test loss: 0.0016607398778821032\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013737848342292839\n",
      "Average test loss: 0.0016570423473086621\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013748473890125752\n",
      "Average test loss: 0.0017324627322248287\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013752210370368428\n",
      "Average test loss: 0.0017346317902103895\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013723135540054904\n",
      "Average test loss: 0.0017332401170084874\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013746915191411972\n",
      "Average test loss: 0.0017775875561767153\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013724816139373514\n",
      "Average test loss: 0.0017419106459452046\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013744819191594919\n",
      "Average test loss: 0.0017125848790423737\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013726603985660606\n",
      "Average test loss: 0.0016991578373644087\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013697896123760276\n",
      "Average test loss: 0.0017230947898286913\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013699067961010667\n",
      "Average test loss: 0.0018566503725532028\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013702450479070346\n",
      "Average test loss: 0.0016870388025417924\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013692090975741545\n",
      "Average test loss: 0.001699004494688577\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013686153013673093\n",
      "Average test loss: 0.001690735303900308\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01367702571882142\n",
      "Average test loss: 0.001703651289972994\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013679807340933216\n",
      "Average test loss: 0.0017150721853185032\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013666710380878713\n",
      "Average test loss: 0.0017008929480281142\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013654892213642597\n",
      "Average test loss: 0.0017741211908352044\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013661134747167429\n",
      "Average test loss: 0.0017117897789511417\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013660915277898311\n",
      "Average test loss: 0.001673219812878718\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01364758716440863\n",
      "Average test loss: 0.0016827330155712035\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013627517962621318\n",
      "Average test loss: 0.0016960567604336473\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013632757659587595\n",
      "Average test loss: 0.0016687645986676217\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013630105297598574\n",
      "Average test loss: 0.0017219875643236769\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013606080925299061\n",
      "Average test loss: 0.0021367597468197348\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013622099455859927\n",
      "Average test loss: 0.001713010505048765\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013618208383520444\n",
      "Average test loss: 0.0016771392837787668\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013601427131228977\n",
      "Average test loss: 0.0022679568220757775\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013597382225924068\n",
      "Average test loss: 0.0016882201973348857\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013582366780274444\n",
      "Average test loss: 0.0017393728852685955\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013583508234885003\n",
      "Average test loss: 0.001719576918002632\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01357655968848202\n",
      "Average test loss: 0.0018303917563623853\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013587897739476627\n",
      "Average test loss: 0.001662207565166884\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013569112656017145\n",
      "Average test loss: 0.0017110682707279922\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013570754047069285\n",
      "Average test loss: 0.0016876142701754968\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013538596330417527\n",
      "Average test loss: 0.0017449225398401419\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013559699723290073\n",
      "Average test loss: 0.0017057231449418599\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0135538589656353\n",
      "Average test loss: 0.0017046339386660191\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013540763217541907\n",
      "Average test loss: 0.049609772506687376\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013609848419825236\n",
      "Average test loss: 0.001751261777503209\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013532229346533616\n",
      "Average test loss: 0.0017066581535877454\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013543075669970777\n",
      "Average test loss: 0.001674841906564931\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013534125554892753\n",
      "Average test loss: 0.0017662259970481198\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013520960764752494\n",
      "Average test loss: 0.0017220271614690622\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01351520482947429\n",
      "Average test loss: 0.0016791793653327558\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013520680853062206\n",
      "Average test loss: 0.0017342842322670751\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013528277680277824\n",
      "Average test loss: 0.0017245261960973342\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013495215193265015\n",
      "Average test loss: 0.0017245743211565746\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013508443160189523\n",
      "Average test loss: 0.0017495092085252205\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013480279364519648\n",
      "Average test loss: 0.0017585823515223133\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013480924368732505\n",
      "Average test loss: 0.0017467509986211856\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013473053203688728\n",
      "Average test loss: 0.001765322695589728\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013518239365683662\n",
      "Average test loss: 0.0016997317030198044\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013478633273806837\n",
      "Average test loss: 0.0016765758248253\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013488921467628744\n",
      "Average test loss: 0.001693015957251191\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013461631650726\n",
      "Average test loss: 0.0017133205241213242\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013465884894960456\n",
      "Average test loss: 0.0016846155498383773\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013444799467093415\n",
      "Average test loss: 0.0016993382270965311\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01345486869663\n",
      "Average test loss: 0.00173647520194451\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01345564231938786\n",
      "Average test loss: 0.0017403574384128053\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013456065422958798\n",
      "Average test loss: 0.001791663171723485\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013444491842554675\n",
      "Average test loss: 0.0017115080365911126\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013419401822818651\n",
      "Average test loss: 0.0017942217126902607\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013452885737021764\n",
      "Average test loss: 0.0018806179150318105\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013461712650954723\n",
      "Average test loss: 0.0017574849552992318\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013416022558179166\n",
      "Average test loss: 0.0018275048887977997\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013419612509508927\n",
      "Average test loss: 0.0017524531050067808\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013427841507726246\n",
      "Average test loss: 0.0016803801125950283\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013421786059935888\n",
      "Average test loss: 0.0016894208582945995\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013416127227246761\n",
      "Average test loss: 0.0017580651180922158\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013394749609132608\n",
      "Average test loss: 0.0017184857023465965\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013394638654258516\n",
      "Average test loss: 0.0017140247504123384\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013395323702858553\n",
      "Average test loss: 0.0017651651409558123\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013399464545150598\n",
      "Average test loss: 0.0016730385428915422\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01338186949574285\n",
      "Average test loss: 0.0017401193100959062\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013389669476283922\n",
      "Average test loss: 0.0017241753578289515\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013377781779401831\n",
      "Average test loss: 0.0017733246883791353\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013366917088627816\n",
      "Average test loss: 0.0017057004249137308\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013359042852289147\n",
      "Average test loss: 0.0017383338367152545\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013381019805040624\n",
      "Average test loss: 0.001698292958860596\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013361694686114789\n",
      "Average test loss: 0.0017521879694735011\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013354828358524376\n",
      "Average test loss: 0.0071014841488666\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013356437486079004\n",
      "Average test loss: 0.0017304422748792502\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013343266783903043\n",
      "Average test loss: 0.0017891310767994986\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013340363817910353\n",
      "Average test loss: 0.0017357683316287066\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013356675330135558\n",
      "Average test loss: 0.0017477989793341193\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013323715285294586\n",
      "Average test loss: 0.001719345525527994\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013322361792955133\n",
      "Average test loss: 0.0017134851668443945\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013341389894485474\n",
      "Average test loss: 0.0016999896525198388\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01334091317322519\n",
      "Average test loss: 0.0018297729301783774\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01332354647666216\n",
      "Average test loss: 0.0017341760098934173\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013325189222064283\n",
      "Average test loss: 0.001715947332791984\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01332529555592272\n",
      "Average test loss: 0.0017581839077174663\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013324770501918264\n",
      "Average test loss: 0.001714536920707259\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013308371224337154\n",
      "Average test loss: 0.0016825435584824945\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013293453177644146\n",
      "Average test loss: 0.0017325373919059833\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013297027291523086\n",
      "Average test loss: 0.0019124677570329773\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013295569353633456\n",
      "Average test loss: 0.0017488625246203607\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013319879917634858\n",
      "Average test loss: 0.0016894918410107493\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01328103122032351\n",
      "Average test loss: 0.0017338835199673971\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013279690390659703\n",
      "Average test loss: 0.0017673130258917809\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013281097487443024\n",
      "Average test loss: 0.0016903634781224861\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013295292136569818\n",
      "Average test loss: 0.0016838061117256681\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013287050989766916\n",
      "Average test loss: 0.0017218658220436837\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013265920658906302\n",
      "Average test loss: 0.0017339157915363709\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013268516645663314\n",
      "Average test loss: 0.0017012819637440973\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013277092677023676\n",
      "Average test loss: 0.0018183221699049075\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01326690638727612\n",
      "Average test loss: 0.0017191430712118745\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013259289134707716\n",
      "Average test loss: 0.0017004356001917686\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013235820796754625\n",
      "Average test loss: 0.0017064497468786107\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013250176327923933\n",
      "Average test loss: 0.0017081533800810576\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013246007985538906\n",
      "Average test loss: 0.0016973971185377903\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013277081537577841\n",
      "Average test loss: 0.0016873285174887212\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013231468489600553\n",
      "Average test loss: 0.0017515541927682029\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013237366909782092\n",
      "Average test loss: 0.0017377497622122367\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013258107089334064\n",
      "Average test loss: 0.0028618607154736915\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013236182513336341\n",
      "Average test loss: 0.0017361755193107658\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013238230086863041\n",
      "Average test loss: 0.0017215350559498701\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013216697559588486\n",
      "Average test loss: 0.0017175015243184235\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013210786501566568\n",
      "Average test loss: 0.0017033481680684619\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01322297425650888\n",
      "Average test loss: 0.0018436566868589984\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013219603185024527\n",
      "Average test loss: 0.0017611858162821996\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013225851689775785\n",
      "Average test loss: 0.001732016346934769\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013211273006267017\n",
      "Average test loss: 0.001710587832869755\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013206642194754547\n",
      "Average test loss: 0.0017754141863228546\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01320103512538804\n",
      "Average test loss: 0.0016916847181402974\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01319728038708369\n",
      "Average test loss: 0.001723870727337069\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013199480075803068\n",
      "Average test loss: 0.0017146309880125853\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013184694784382979\n",
      "Average test loss: 0.001690579670895305\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013192366402182314\n",
      "Average test loss: 0.0017721935994923115\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013188746710618336\n",
      "Average test loss: 0.0016833054012515478\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013191089083751042\n",
      "Average test loss: 0.0018123573892646365\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01319594221479363\n",
      "Average test loss: 0.0018011683540211784\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013172381499575244\n",
      "Average test loss: 0.0017401948147970768\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013190219937927194\n",
      "Average test loss: 0.0017334426384833124\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01318364809287919\n",
      "Average test loss: 0.0017668634220543834\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013158127892348501\n",
      "Average test loss: 0.0018636902868747712\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013178668619857893\n",
      "Average test loss: 0.0017358864135005407\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013165693461894988\n",
      "Average test loss: 0.001823244581826859\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013158009058899349\n",
      "Average test loss: 0.0017302221013233065\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive-.01/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.45\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.05\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.14\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.29\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.98\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.72\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.06\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.80\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.886194650967916\n",
      "Average test loss: 0.012806542128324508\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7016878674825032\n",
      "Average test loss: 0.012622279617521498\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4831912709342109\n",
      "Average test loss: 0.009009522802299923\n",
      "Epoch 4/300\n",
      "Average training loss: 0.379781901995341\n",
      "Average test loss: 0.008333019759505986\n",
      "Epoch 5/300\n",
      "Average training loss: 0.314919617123074\n",
      "Average test loss: 0.008779224986831348\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2729892054663764\n",
      "Average test loss: 0.007795513302915626\n",
      "Epoch 7/300\n",
      "Average training loss: 0.24308177842034234\n",
      "Average test loss: 0.008734839975833892\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2175407735771603\n",
      "Average test loss: 0.007562858883705404\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2015765220456653\n",
      "Average test loss: 0.00809540738289555\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1870921067661709\n",
      "Average test loss: 0.007317677050001092\n",
      "Epoch 11/300\n",
      "Average training loss: 0.17458800703949398\n",
      "Average test loss: 0.0070621580945120915\n",
      "Epoch 12/300\n",
      "Average training loss: 0.16432825725608402\n",
      "Average test loss: 0.007464712662829293\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15668751827875774\n",
      "Average test loss: 0.0072521262504160406\n",
      "Epoch 14/300\n",
      "Average training loss: 0.15224456780486637\n",
      "Average test loss: 0.006639468896306224\n",
      "Epoch 15/300\n",
      "Average training loss: 0.14518357445134056\n",
      "Average test loss: 0.006768381628725264\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13923261365625594\n",
      "Average test loss: 0.006559804467691315\n",
      "Epoch 17/300\n",
      "Average training loss: 0.1357896335389879\n",
      "Average test loss: 0.00689111888450053\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13138315845860377\n",
      "Average test loss: 0.00651730657617251\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12714979171090657\n",
      "Average test loss: 0.006178378007892105\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12401627198855082\n",
      "Average test loss: 0.005981892697513104\n",
      "Epoch 21/300\n",
      "Average training loss: 0.12089455307192272\n",
      "Average test loss: 0.006263478223648336\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11854551916652256\n",
      "Average test loss: 0.006524492488967048\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11592215871810913\n",
      "Average test loss: 0.005856039743456575\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11330916174252828\n",
      "Average test loss: 0.006158174380660057\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11105360624525282\n",
      "Average test loss: 0.005678941340496142\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10942934138245053\n",
      "Average test loss: 0.005687139557467567\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10802319816748301\n",
      "Average test loss: 0.006070305639257033\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10595544222990672\n",
      "Average test loss: 0.00561103214075168\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10468392421801885\n",
      "Average test loss: 0.0055974656881557575\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10309808864196142\n",
      "Average test loss: 0.005877660170404447\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10184453756279416\n",
      "Average test loss: 0.005556513773898284\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10111287177933587\n",
      "Average test loss: 0.006290722869336605\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09946981957885954\n",
      "Average test loss: 0.00546823529774944\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09879407019747628\n",
      "Average test loss: 0.005700356665584776\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09769619325134489\n",
      "Average test loss: 0.00582391290490826\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09791879006226857\n",
      "Average test loss: 0.005400917199543781\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09590097178353203\n",
      "Average test loss: 0.005600815658768018\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09521163093381457\n",
      "Average test loss: 0.005483020796957943\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09479231351613998\n",
      "Average test loss: 0.00540340593457222\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09421777972910139\n",
      "Average test loss: 0.0057164292844633265\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09318599949942695\n",
      "Average test loss: 0.005395151800993416\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09275755195816358\n",
      "Average test loss: 0.005425760643349754\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09213566792673535\n",
      "Average test loss: 0.005494935995174779\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09154772935973274\n",
      "Average test loss: 0.00888885131229957\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09096884749333063\n",
      "Average test loss: 0.00533912008545465\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09019978339804544\n",
      "Average test loss: 0.0053066397048532965\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08979111807876163\n",
      "Average test loss: 0.005307028010073636\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08955470196406047\n",
      "Average test loss: 0.12300519298844867\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08889735156959958\n",
      "Average test loss: 0.005408087880040209\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08852937545379003\n",
      "Average test loss: 0.0057105922622399195\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08832741373777389\n",
      "Average test loss: 0.005486789961655934\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08778507436646356\n",
      "Average test loss: 0.005492947331940134\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08740752753615379\n",
      "Average test loss: 0.00899421423032052\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08742909150653415\n",
      "Average test loss: 0.005399593874812126\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08669306747780905\n",
      "Average test loss: 0.0053145995918247434\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08647052530447642\n",
      "Average test loss: 0.017081459765632946\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0861586617761188\n",
      "Average test loss: 0.005505840370638503\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08564920193619198\n",
      "Average test loss: 0.00548220726268159\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08536169460084703\n",
      "Average test loss: 0.005317985643115308\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08502963360150655\n",
      "Average test loss: 0.00525616902526882\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08486888162957297\n",
      "Average test loss: 0.005496819930358066\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08449756297800276\n",
      "Average test loss: 0.005473587257166704\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08444946392377217\n",
      "Average test loss: 0.005571091075738271\n",
      "Epoch 64/300\n",
      "Average training loss: 464.37298230938114\n",
      "Average test loss: 0.027031583607196808\n",
      "Epoch 65/300\n",
      "Average training loss: 9.50675641716851\n",
      "Average test loss: 0.02062984294692675\n",
      "Epoch 66/300\n",
      "Average training loss: 8.489488727145725\n",
      "Average test loss: 0.016204699734846752\n",
      "Epoch 67/300\n",
      "Average training loss: 7.82168809805976\n",
      "Average test loss: 0.016242046832210488\n",
      "Epoch 68/300\n",
      "Average training loss: 7.381296220991347\n",
      "Average test loss: 0.01339104035579496\n",
      "Epoch 69/300\n",
      "Average training loss: 6.971941116756863\n",
      "Average test loss: 0.011578333810799651\n",
      "Epoch 70/300\n",
      "Average training loss: 6.556985967424181\n",
      "Average test loss: 0.011174678899347782\n",
      "Epoch 71/300\n",
      "Average training loss: 6.159720868428548\n",
      "Average test loss: 0.010387681141495705\n",
      "Epoch 72/300\n",
      "Average training loss: 5.803004496680366\n",
      "Average test loss: 0.009732482131984499\n",
      "Epoch 73/300\n",
      "Average training loss: 5.4755428047180175\n",
      "Average test loss: 0.009219489417142339\n",
      "Epoch 74/300\n",
      "Average training loss: 5.155296129862467\n",
      "Average test loss: 0.008858002602640126\n",
      "Epoch 75/300\n",
      "Average training loss: 4.826404395209418\n",
      "Average test loss: 0.008685938545399242\n",
      "Epoch 76/300\n",
      "Average training loss: 4.493853508419461\n",
      "Average test loss: 0.008302977605826324\n",
      "Epoch 77/300\n",
      "Average training loss: 4.165245364507039\n",
      "Average test loss: 0.00805570779575242\n",
      "Epoch 78/300\n",
      "Average training loss: 3.845967438591851\n",
      "Average test loss: 0.007645013679232863\n",
      "Epoch 79/300\n",
      "Average training loss: 3.5376702579922146\n",
      "Average test loss: 0.007476645700633526\n",
      "Epoch 80/300\n",
      "Average training loss: 3.2418756137424043\n",
      "Average test loss: 0.007241639573540953\n",
      "Epoch 81/300\n",
      "Average training loss: 2.951613135655721\n",
      "Average test loss: 0.0071767492844826645\n",
      "Epoch 82/300\n",
      "Average training loss: 2.6641343835194906\n",
      "Average test loss: 0.00798607050254941\n",
      "Epoch 83/300\n",
      "Average training loss: 2.3710625315772162\n",
      "Average test loss: 0.006896119700951709\n",
      "Epoch 84/300\n",
      "Average training loss: 2.072236511654324\n",
      "Average test loss: 0.006475181731912825\n",
      "Epoch 85/300\n",
      "Average training loss: 1.7746777724160088\n",
      "Average test loss: 0.006395165440109041\n",
      "Epoch 86/300\n",
      "Average training loss: 1.4872124949561225\n",
      "Average test loss: 0.006549902790122562\n",
      "Epoch 87/300\n",
      "Average training loss: 1.2201604200998941\n",
      "Average test loss: 0.006100497203154696\n",
      "Epoch 88/300\n",
      "Average training loss: 0.9830624542236328\n",
      "Average test loss: 0.010561603605747222\n",
      "Epoch 89/300\n",
      "Average training loss: 0.783997867266337\n",
      "Average test loss: 0.0059569134650131065\n",
      "Epoch 90/300\n",
      "Average training loss: 0.6147123430040148\n",
      "Average test loss: 0.0061484174910518856\n",
      "Epoch 91/300\n",
      "Average training loss: 0.46486733934614394\n",
      "Average test loss: 0.005802210722946458\n",
      "Epoch 92/300\n",
      "Average training loss: 0.35460233147939046\n",
      "Average test loss: 0.0058613273443447215\n",
      "Epoch 93/300\n",
      "Average training loss: 0.2811179839902454\n",
      "Average test loss: 0.005647567275497648\n",
      "Epoch 94/300\n",
      "Average training loss: 0.23240889015462662\n",
      "Average test loss: 0.005935366014639536\n",
      "Epoch 95/300\n",
      "Average training loss: 0.20299179224173228\n",
      "Average test loss: 0.005724117008348306\n",
      "Epoch 96/300\n",
      "Average training loss: 0.18217760252952575\n",
      "Average test loss: 0.005541655963493718\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1661918245156606\n",
      "Average test loss: 0.005575844798651006\n",
      "Epoch 98/300\n",
      "Average training loss: 0.152886572043101\n",
      "Average test loss: 0.005791417189770275\n",
      "Epoch 99/300\n",
      "Average training loss: 0.14239253976609972\n",
      "Average test loss: 0.005412741931362285\n",
      "Epoch 100/300\n",
      "Average training loss: 0.13399717764721977\n",
      "Average test loss: 0.005454489198409848\n",
      "Epoch 101/300\n",
      "Average training loss: 0.12715068780051336\n",
      "Average test loss: 0.005356368365801043\n",
      "Epoch 102/300\n",
      "Average training loss: 0.12118698607550726\n",
      "Average test loss: 0.005342348614500629\n",
      "Epoch 103/300\n",
      "Average training loss: 0.11557778131961823\n",
      "Average test loss: 0.020250909073485267\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11131696359978782\n",
      "Average test loss: 0.005374269268992874\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10847405710485246\n",
      "Average test loss: 0.005821769247452418\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10481404405170017\n",
      "Average test loss: 0.00551477680189742\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10248087025350995\n",
      "Average test loss: 0.005253666207194328\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10232163824306593\n",
      "Average test loss: 0.0055399912446737285\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09826424533128739\n",
      "Average test loss: 0.0053937026494079165\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09642193474372228\n",
      "Average test loss: 0.005317217447484533\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09486472523874706\n",
      "Average test loss: 0.0053031397052109245\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09370991302861108\n",
      "Average test loss: 0.005228226319783264\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09217791260613335\n",
      "Average test loss: 0.005218855367352565\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09114916062355041\n",
      "Average test loss: 0.0051781899101204345\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09006088391277525\n",
      "Average test loss: 0.00521426185965538\n",
      "Epoch 116/300\n",
      "Average training loss: 0.08883901929855346\n",
      "Average test loss: 0.005219982935322656\n",
      "Epoch 117/300\n",
      "Average training loss: 0.08802330772082011\n",
      "Average test loss: 0.005183801133185625\n",
      "Epoch 118/300\n",
      "Average training loss: 0.08717470760477913\n",
      "Average test loss: 0.005206096045258972\n",
      "Epoch 119/300\n",
      "Average training loss: 0.08633318796422747\n",
      "Average test loss: 0.006288845350344976\n",
      "Epoch 120/300\n",
      "Average training loss: 0.08576064124372271\n",
      "Average test loss: 0.00954221334722307\n",
      "Epoch 121/300\n",
      "Average training loss: 0.08550461593601438\n",
      "Average test loss: 0.005489228068126573\n",
      "Epoch 122/300\n",
      "Average training loss: 0.08489625795019998\n",
      "Average test loss: 0.005292079283959336\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0846732111705674\n",
      "Average test loss: 0.0054334795280463165\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08381166054142845\n",
      "Average test loss: 0.005627524472773075\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08364680553807152\n",
      "Average test loss: 0.005420272066361374\n",
      "Epoch 126/300\n",
      "Average training loss: 0.08381736230187946\n",
      "Average test loss: 0.0053219486532939805\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08315847568379509\n",
      "Average test loss: 0.00532195781957772\n",
      "Epoch 128/300\n",
      "Average training loss: 0.08280394989914364\n",
      "Average test loss: 0.0053154494493371915\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0825593188603719\n",
      "Average test loss: 0.0057358513726956314\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08232324300209681\n",
      "Average test loss: 0.005960207442442576\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0821062765982416\n",
      "Average test loss: 0.005773914369444052\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08168625705109703\n",
      "Average test loss: 0.027398524983061685\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08182641999257935\n",
      "Average test loss: 0.00605228706366486\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08141536793443892\n",
      "Average test loss: 0.0053166791655951074\n",
      "Epoch 135/300\n",
      "Average training loss: 0.08100064018699858\n",
      "Average test loss: 0.005390609338879585\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08090617091457049\n",
      "Average test loss: 0.005488799987981717\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08062794010506735\n",
      "Average test loss: 0.005257791830847661\n",
      "Epoch 138/300\n",
      "Average training loss: 1286.1227647574478\n",
      "Average test loss: 0.32247977622350055\n",
      "Epoch 139/300\n",
      "Average training loss: 12.350131770663792\n",
      "Average test loss: 0.06835924026701186\n",
      "Epoch 140/300\n",
      "Average training loss: 10.574455035739474\n",
      "Average test loss: 0.040009227709637746\n",
      "Epoch 141/300\n",
      "Average training loss: 9.7105050523546\n",
      "Average test loss: 0.02596503940721353\n",
      "Epoch 142/300\n",
      "Average training loss: 9.030645410325793\n",
      "Average test loss: 0.019842888169818454\n",
      "Epoch 143/300\n",
      "Average training loss: 8.459711047702365\n",
      "Average test loss: 0.01604675799773799\n",
      "Epoch 144/300\n",
      "Average training loss: 7.909593307918972\n",
      "Average test loss: 0.01573290570328633\n",
      "Epoch 145/300\n",
      "Average training loss: 7.384473260667589\n",
      "Average test loss: 0.015805608978701962\n",
      "Epoch 146/300\n",
      "Average training loss: 6.835430866241455\n",
      "Average test loss: 0.014390463552541203\n",
      "Epoch 147/300\n",
      "Average training loss: 6.264260528564453\n",
      "Average test loss: 0.015162452005677753\n",
      "Epoch 148/300\n",
      "Average training loss: 5.634505161709256\n",
      "Average test loss: 0.011654960960149764\n",
      "Epoch 149/300\n",
      "Average training loss: 4.951529655880398\n",
      "Average test loss: 0.01051590051750342\n",
      "Epoch 150/300\n",
      "Average training loss: 4.313971968332926\n",
      "Average test loss: 0.01323218254496654\n",
      "Epoch 151/300\n",
      "Average training loss: 3.7471900706821017\n",
      "Average test loss: 0.009622600224283006\n",
      "Epoch 152/300\n",
      "Average training loss: 46.3891241645813\n",
      "Average test loss: 0.0460063806242413\n",
      "Epoch 153/300\n",
      "Average training loss: 7.492375009748671\n",
      "Average test loss: 0.032606135457754135\n",
      "Epoch 154/300\n",
      "Average training loss: 6.343050822363959\n",
      "Average test loss: 0.018545262652966712\n",
      "Epoch 155/300\n",
      "Average training loss: 5.611030490875244\n",
      "Average test loss: 0.016688218903210428\n",
      "Epoch 156/300\n",
      "Average training loss: 5.059575278811985\n",
      "Average test loss: 0.015543008687595526\n",
      "Epoch 157/300\n",
      "Average training loss: 4.609411847008599\n",
      "Average test loss: 0.012970050843225585\n",
      "Epoch 158/300\n",
      "Average training loss: 4.204735028160943\n",
      "Average test loss: 0.011553624655637477\n",
      "Epoch 159/300\n",
      "Average training loss: 3.831841512468126\n",
      "Average test loss: 0.010680707595414585\n",
      "Epoch 160/300\n",
      "Average training loss: 3.4797579964531793\n",
      "Average test loss: 0.010222931396630075\n",
      "Epoch 161/300\n",
      "Average training loss: 3.1363969326019285\n",
      "Average test loss: 0.009659551405244403\n",
      "Epoch 162/300\n",
      "Average training loss: 2.79573238796658\n",
      "Average test loss: 0.009174202016658254\n",
      "Epoch 163/300\n",
      "Average training loss: 2.456077561484443\n",
      "Average test loss: 0.008522961277928617\n",
      "Epoch 164/300\n",
      "Average training loss: 2.121830102072822\n",
      "Average test loss: 0.008577953677210542\n",
      "Epoch 165/300\n",
      "Average training loss: 1.8137122443517049\n",
      "Average test loss: 0.007967260696821743\n",
      "Epoch 166/300\n",
      "Average training loss: 1.5288511339823405\n",
      "Average test loss: 0.007673481120831436\n",
      "Epoch 167/300\n",
      "Average training loss: 1.2721512129041883\n",
      "Average test loss: 0.0075682261404063965\n",
      "Epoch 168/300\n",
      "Average training loss: 1.047069541507297\n",
      "Average test loss: 0.007143297580381234\n",
      "Epoch 169/300\n",
      "Average training loss: 0.8504717338879904\n",
      "Average test loss: 0.007279246381587452\n",
      "Epoch 170/300\n",
      "Average training loss: 0.6835924277305603\n",
      "Average test loss: 0.00684912818007999\n",
      "Epoch 171/300\n",
      "Average training loss: 0.5496855788230895\n",
      "Average test loss: 0.006448869363715251\n",
      "Epoch 172/300\n",
      "Average training loss: 0.43770677349302506\n",
      "Average test loss: 0.006505033599419726\n",
      "Epoch 173/300\n",
      "Average training loss: 0.3448398749563429\n",
      "Average test loss: 0.006149367114942935\n",
      "Epoch 174/300\n",
      "Average training loss: 0.2743736386564043\n",
      "Average test loss: 0.0060841113366186615\n",
      "Epoch 175/300\n",
      "Average training loss: 0.22542027635044523\n",
      "Average test loss: 0.005825878304739793\n",
      "Epoch 176/300\n",
      "Average training loss: 0.19267872156037225\n",
      "Average test loss: 0.005704345358742608\n",
      "Epoch 177/300\n",
      "Average training loss: 0.168336179084248\n",
      "Average test loss: 0.005640303591059314\n",
      "Epoch 178/300\n",
      "Average training loss: 0.14949482873413297\n",
      "Average test loss: 0.00563136330867807\n",
      "Epoch 179/300\n",
      "Average training loss: 0.1356262605852551\n",
      "Average test loss: 0.005533163773516814\n",
      "Epoch 180/300\n",
      "Average training loss: 0.12734703534179265\n",
      "Average test loss: 0.005515932904349433\n",
      "Epoch 181/300\n",
      "Average training loss: 0.12067349773645401\n",
      "Average test loss: 0.005437313487960233\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11575331440236833\n",
      "Average test loss: 0.0055020247457755935\n",
      "Epoch 183/300\n",
      "Average training loss: 0.11142200836208131\n",
      "Average test loss: 0.0053677717298269275\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10827772237857183\n",
      "Average test loss: 0.005430491493807899\n",
      "Epoch 185/300\n",
      "Average training loss: 0.105524980518553\n",
      "Average test loss: 0.005333454118006759\n",
      "Epoch 186/300\n",
      "Average training loss: 0.10291378237141503\n",
      "Average test loss: 0.005334571166170968\n",
      "Epoch 187/300\n",
      "Average training loss: 0.10022772290971545\n",
      "Average test loss: 0.014560656648543146\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09800820775164498\n",
      "Average test loss: 0.005278246735947\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09576194206211303\n",
      "Average test loss: 0.0052455283680723774\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09363471545113458\n",
      "Average test loss: 0.005235911353801688\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09120341459578937\n",
      "Average test loss: 0.005298716522132357\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08897904613945219\n",
      "Average test loss: 0.0052922997747858365\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08732075950834486\n",
      "Average test loss: 0.005239972362501753\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08548401676946216\n",
      "Average test loss: 0.00541860339935455\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08423308289051055\n",
      "Average test loss: 0.005283529919882616\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08361336845159531\n",
      "Average test loss: 0.005650796357128355\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08269462764925427\n",
      "Average test loss: 0.008495312818222576\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08382426359918382\n",
      "Average test loss: 0.005368527810606692\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08210481697320939\n",
      "Average test loss: 0.006374386429786682\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08150111632876926\n",
      "Average test loss: 0.0052706689520014656\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08108999519877963\n",
      "Average test loss: 0.005830590092059639\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08068413046333525\n",
      "Average test loss: 0.005342447602500518\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08042412954568863\n",
      "Average test loss: 0.0052808736583424935\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08012943220138549\n",
      "Average test loss: 0.005324134779887067\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08025685697131686\n",
      "Average test loss: 0.0052926153250866465\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08015782943036821\n",
      "Average test loss: 0.005248847928932971\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07944655407137341\n",
      "Average test loss: 0.03283440825674269\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07930488477812873\n",
      "Average test loss: 0.0053935044854879375\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07915443870094087\n",
      "Average test loss: 0.005329832605603668\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07906738282574548\n",
      "Average test loss: 0.005398439646595054\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07876853590541416\n",
      "Average test loss: 0.0053419280693762835\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07859711305631531\n",
      "Average test loss: 0.005443977581544055\n",
      "Epoch 213/300\n",
      "Average training loss: 2478.6469561378094\n",
      "Average test loss: 785.5076200546689\n",
      "Epoch 214/300\n",
      "Average training loss: 8.12567190890842\n",
      "Average test loss: 0.21152960830926895\n",
      "Epoch 215/300\n",
      "Average training loss: 5.859354814741347\n",
      "Average test loss: 0.025252632757027944\n",
      "Epoch 216/300\n",
      "Average training loss: 4.8713560837639704\n",
      "Average test loss: 0.017451362942655882\n",
      "Epoch 217/300\n",
      "Average training loss: 4.2296470506456165\n",
      "Average test loss: 0.015573651366763645\n",
      "Epoch 218/300\n",
      "Average training loss: 3.7787617973751493\n",
      "Average test loss: 0.12092000016238955\n",
      "Epoch 219/300\n",
      "Average training loss: 3.4371952747768826\n",
      "Average test loss: 0.018366646092798974\n",
      "Epoch 220/300\n",
      "Average training loss: 3.1450701457129586\n",
      "Average test loss: 0.036879651844501496\n",
      "Epoch 221/300\n",
      "Average training loss: 2.8789868795606823\n",
      "Average test loss: 0.010715444372759925\n",
      "Epoch 222/300\n",
      "Average training loss: 2.620075650109185\n",
      "Average test loss: 0.23422354504797194\n",
      "Epoch 223/300\n",
      "Average training loss: 2.376570627848307\n",
      "Average test loss: 0.012847695163554615\n",
      "Epoch 224/300\n",
      "Average training loss: 2.1192172978719075\n",
      "Average test loss: 0.008053309548232291\n",
      "Epoch 225/300\n",
      "Average training loss: 1.868977444966634\n",
      "Average test loss: 0.00850127932512098\n",
      "Epoch 226/300\n",
      "Average training loss: 1.6622536796993679\n",
      "Average test loss: 0.010715038954383797\n",
      "Epoch 227/300\n",
      "Average training loss: 1.498809471766154\n",
      "Average test loss: 0.007637598568366634\n",
      "Epoch 228/300\n",
      "Average training loss: 1.3479909069273206\n",
      "Average test loss: 0.0072239040546119215\n",
      "Epoch 229/300\n",
      "Average training loss: 1.2097557196087307\n",
      "Average test loss: 0.007707058219446076\n",
      "Epoch 230/300\n",
      "Average training loss: 1.079412080976698\n",
      "Average test loss: 0.006609589944283167\n",
      "Epoch 231/300\n",
      "Average training loss: 0.9533922432793511\n",
      "Average test loss: 0.006587174466914601\n",
      "Epoch 232/300\n",
      "Average training loss: 0.8358110420968797\n",
      "Average test loss: 0.006349034899638758\n",
      "Epoch 233/300\n",
      "Average training loss: 0.7273617959552341\n",
      "Average test loss: 0.006264715847041872\n",
      "Epoch 234/300\n",
      "Average training loss: 0.6301417211956448\n",
      "Average test loss: 0.006074144544700781\n",
      "Epoch 235/300\n",
      "Average training loss: 0.5430126873917049\n",
      "Average test loss: 0.0059939313977956776\n",
      "Epoch 236/300\n",
      "Average training loss: 0.4657117640707228\n",
      "Average test loss: 0.005881951348649131\n",
      "Epoch 237/300\n",
      "Average training loss: 0.39717877440982396\n",
      "Average test loss: 0.0058465172683613165\n",
      "Epoch 238/300\n",
      "Average training loss: 0.33923521513409083\n",
      "Average test loss: 0.005722027049710353\n",
      "Epoch 239/300\n",
      "Average training loss: 0.28950324512852565\n",
      "Average test loss: 0.005739822847975625\n",
      "Epoch 240/300\n",
      "Average training loss: 0.24746802736653223\n",
      "Average test loss: 0.005794586673792865\n",
      "Epoch 241/300\n",
      "Average training loss: 0.21383983340528276\n",
      "Average test loss: 0.0056141252327296465\n",
      "Epoch 242/300\n",
      "Average training loss: 0.18654849791526795\n",
      "Average test loss: 0.005650984294298622\n",
      "Epoch 243/300\n",
      "Average training loss: 0.16583790792359246\n",
      "Average test loss: 0.006490140181862646\n",
      "Epoch 244/300\n",
      "Average training loss: 0.14971305525302886\n",
      "Average test loss: 0.005788507879194286\n",
      "Epoch 245/300\n",
      "Average training loss: 0.12227570813231999\n",
      "Average test loss: 0.005688886142439313\n",
      "Epoch 248/300\n",
      "Average training loss: 0.11707924856742223\n",
      "Average test loss: 0.005329628587183025\n",
      "Epoch 249/300\n",
      "Average training loss: 0.11243759364551968\n",
      "Average test loss: 0.008322653932703867\n",
      "Epoch 250/300\n",
      "Average training loss: 0.10810969583855735\n",
      "Average test loss: 0.005502789364713762\n",
      "Epoch 251/300\n",
      "Average training loss: 0.10471235559384028\n",
      "Average test loss: 0.005282831981778145\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0978443892068333\n",
      "Average test loss: 0.005384480062044329\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09593659683730867\n",
      "Average test loss: 0.005223625960449378\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09547898993227216\n",
      "Average test loss: 0.023218716621398926\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0928251665102111\n",
      "Average test loss: 0.012040231373575\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09136347076627943\n",
      "Average test loss: 0.006500151744319333\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09012073346641329\n",
      "Average test loss: 0.005259177279969056\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0895281351407369\n",
      "Average test loss: 0.0052085899276038015\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08786967924568388\n",
      "Average test loss: 0.005257832477076186\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08672155292166604\n",
      "Average test loss: 0.0053842993254462875\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08563886770937178\n",
      "Average test loss: 0.00521791240428057\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08470665500561396\n",
      "Average test loss: 0.005282891393535667\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08369424288802677\n",
      "Average test loss: 0.005327590398490429\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08305290457937453\n",
      "Average test loss: 0.005347771135883199\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08223795614971055\n",
      "Average test loss: 0.0053131850647429625\n",
      "Epoch 268/300\n",
      "Average training loss: 0.19892498316367468\n",
      "Average test loss: 0.006558755822479725\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10671258910497029\n",
      "Average test loss: 0.005348138381209638\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09849058324098588\n",
      "Average test loss: 0.005572916411277321\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09456541626320945\n",
      "Average test loss: 0.005347524360650116\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09186310672097736\n",
      "Average test loss: 0.005476117534769906\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08958739628394445\n",
      "Average test loss: 0.00524921219319933\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08787226243813832\n",
      "Average test loss: 0.005363254004468521\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08645426998535792\n",
      "Average test loss: 0.005425251548488935\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08621774808565776\n",
      "Average test loss: 0.0053099134537494845\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08464028178320991\n",
      "Average test loss: 0.005344021163466904\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08182440937227672\n",
      "Average test loss: 0.00527202316497763\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0810677126844724\n",
      "Average test loss: 0.0054779796960453195\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08087734142276976\n",
      "Average test loss: 0.005275360277957386\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08035880420605342\n",
      "Average test loss: 0.005330157287832763\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0803191752632459\n",
      "Average test loss: 0.023771348424255847\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07977720662289195\n",
      "Average test loss: 0.005323902242713504\n",
      "Epoch 286/300\n",
      "Average training loss: 0.07940703097316953\n",
      "Average test loss: 0.005348999286691348\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07917133207453622\n",
      "Average test loss: 0.005439805368168487\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07904718427525627\n",
      "Average test loss: 0.005475060696817107\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07918152048852709\n",
      "Average test loss: 0.006204576690163877\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07846776741743088\n",
      "Average test loss: 0.005324988824211889\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0782781031065517\n",
      "Average test loss: 0.006209956489917305\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07806070072783364\n",
      "Average test loss: 0.005432568221870396\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07757831488053003\n",
      "Average test loss: 0.014314339601331287\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07997204703092575\n",
      "Average test loss: 0.005418710200322999\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07808850514888764\n",
      "Average test loss: 0.005426769926316208\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07732714272207684\n",
      "Average test loss: 0.005338612200071415\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07715678305758371\n",
      "Average test loss: 0.03648104074100653\n",
      "Epoch 298/300\n",
      "Average training loss: 255.8831293010513\n",
      "Average test loss: 0.06356872636079788\n",
      "Epoch 299/300\n",
      "Average training loss: 5.704145421346029\n",
      "Average test loss: 0.02156728935241699\n",
      "Epoch 300/300\n",
      "Average training loss: 4.404439479404026\n",
      "Average test loss: 0.01507655120227072\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.6114779279496936\n",
      "Average test loss: 0.007780781830350558\n",
      "Epoch 2/300\n",
      "Average training loss: 0.5619680097103119\n",
      "Average test loss: 0.006516570938544141\n",
      "Epoch 3/300\n",
      "Average training loss: 0.37920839455392624\n",
      "Average test loss: 0.006114675770617193\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2924451005723741\n",
      "Average test loss: 0.00555152550422483\n",
      "Epoch 5/300\n",
      "Average training loss: 0.23872528012593588\n",
      "Average test loss: 0.005359540179785755\n",
      "Epoch 6/300\n",
      "Average training loss: 0.20237903471787771\n",
      "Average test loss: 0.005095658731957276\n",
      "Epoch 7/300\n",
      "Average training loss: 0.17652992594242095\n",
      "Average test loss: 0.004900555844522185\n",
      "Epoch 8/300\n",
      "Average training loss: 0.157626324945026\n",
      "Average test loss: 0.004645371910184622\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14310466321309406\n",
      "Average test loss: 0.004484149039619498\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1321398243904114\n",
      "Average test loss: 0.004689422643846935\n",
      "Epoch 11/300\n",
      "Average training loss: 0.12287693003813425\n",
      "Average test loss: 0.00437699567195442\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11587071876393425\n",
      "Average test loss: 0.006135104832135968\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10978163990047243\n",
      "Average test loss: 0.004045373657925262\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10494034243292279\n",
      "Average test loss: 0.004080696726010905\n",
      "Epoch 15/300\n",
      "Average training loss: 0.10028815303908455\n",
      "Average test loss: 0.00405475857688321\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09699199343389935\n",
      "Average test loss: 0.0052383118118676875\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09339704471826553\n",
      "Average test loss: 0.0036697389425502883\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09097788189517127\n",
      "Average test loss: 0.003636593117689093\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08732687747478485\n",
      "Average test loss: 0.0034983883949203623\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08506004180510839\n",
      "Average test loss: 0.003762195023811526\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08165762839714687\n",
      "Average test loss: 0.003586523172756036\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0791160820292102\n",
      "Average test loss: 0.004595349359843466\n",
      "Epoch 23/300\n",
      "Average training loss: 0.07265837302803993\n",
      "Average test loss: 0.003413608431816101\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07058499639895227\n",
      "Average test loss: 0.0033877797077099484\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06904057446453306\n",
      "Average test loss: 0.0032013079175311657\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06729119352499643\n",
      "Average test loss: 0.0031987055780159103\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06609939902689722\n",
      "Average test loss: 0.0039885393852988876\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06509764655099974\n",
      "Average test loss: 0.003144800524537762\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06365440108378728\n",
      "Average test loss: 0.004758434325042698\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0632999488512675\n",
      "Average test loss: 0.003308303694344229\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0621865799691942\n",
      "Average test loss: 0.003949992308807042\n",
      "Epoch 34/300\n",
      "Average training loss: 0.061507159289386534\n",
      "Average test loss: 0.0038406468174523776\n",
      "Epoch 35/300\n",
      "Average training loss: 0.060608817040920256\n",
      "Average test loss: 0.003284311978560355\n",
      "Epoch 36/300\n",
      "Average training loss: 0.060722883270846476\n",
      "Average test loss: 0.00349042678065598\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05950960274537404\n",
      "Average test loss: 0.0030860166266146634\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05911455350120862\n",
      "Average test loss: 0.0031644550626062683\n",
      "Epoch 39/300\n",
      "Average training loss: 0.15769205569558672\n",
      "Average test loss: 0.00377958959630794\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0792255426214801\n",
      "Average test loss: 0.003336312097600765\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0712607553270128\n",
      "Average test loss: 0.0032176732199473515\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06760315385129717\n",
      "Average test loss: 0.0032248261417779658\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06582023022572199\n",
      "Average test loss: 0.0031809737326370345\n",
      "Epoch 44/300\n",
      "Average training loss: 0.062476830820242565\n",
      "Average test loss: 0.0032045787521120576\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06086583517988523\n",
      "Average test loss: 0.0031271268425302375\n",
      "Epoch 48/300\n",
      "Average training loss: 0.060498105896843804\n",
      "Average test loss: 0.0031172132769392595\n",
      "Epoch 49/300\n",
      "Average training loss: 0.059716467532846665\n",
      "Average test loss: 0.003094671981823113\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05930698466300964\n",
      "Average test loss: 0.0031713128816336393\n",
      "Epoch 51/300\n",
      "Average training loss: 0.058857199804650415\n",
      "Average test loss: 0.003064492982087864\n",
      "Epoch 52/300\n",
      "Average training loss: 0.058518396725257237\n",
      "Average test loss: 0.003085650901413626\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05833636680907673\n",
      "Average test loss: 0.0030531257057769432\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05787978096471892\n",
      "Average test loss: 0.0031883200808531706\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0574377501308918\n",
      "Average test loss: 0.003102197193644113\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05722904502020942\n",
      "Average test loss: 0.003039549953200751\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05685823205444548\n",
      "Average test loss: 0.0030929140802472828\n",
      "Epoch 58/300\n",
      "Average training loss: 0.056678814278708564\n",
      "Average test loss: 0.04522487938404083\n",
      "Epoch 59/300\n",
      "Average training loss: 0.056235810220241544\n",
      "Average test loss: 0.003198116013987197\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05615368698702918\n",
      "Average test loss: 0.004149356767121288\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05566705584526062\n",
      "Average test loss: 0.0035475309449765416\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05563975558347172\n",
      "Average test loss: 0.003047429420053959\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05522779975997077\n",
      "Average test loss: 0.0030009224489331247\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05519299028317134\n",
      "Average test loss: 0.0029980565077728696\n",
      "Epoch 65/300\n",
      "Average training loss: 0.054843279129929014\n",
      "Average test loss: 0.002997035421844986\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05476651250984934\n",
      "Average test loss: 0.003159758105667101\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0542416130900383\n",
      "Average test loss: 0.0030314631753911576\n",
      "Epoch 68/300\n",
      "Average training loss: 0.053982153548134695\n",
      "Average test loss: 0.003176056742668152\n",
      "Epoch 71/300\n",
      "Average training loss: 0.053476936370134356\n",
      "Average test loss: 0.010052973118921121\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05312566750248273\n",
      "Average test loss: 0.00296844296488497\n",
      "Epoch 73/300\n",
      "Average training loss: 0.052964526623487475\n",
      "Average test loss: 0.0030585681398709614\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05273060667514801\n",
      "Average test loss: 0.003004178076982498\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06560784426662657\n",
      "Average test loss: 0.002970063873463207\n",
      "Epoch 76/300\n",
      "Average training loss: 0.2001641246146626\n",
      "Average test loss: 0.004820618958522876\n",
      "Epoch 77/300\n",
      "Average training loss: 0.13874217442671458\n",
      "Average test loss: 0.003445089295713438\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09302517482969495\n",
      "Average test loss: 0.00323735058762961\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08025164545906914\n",
      "Average test loss: 0.0032036550655547115\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07374622224436866\n",
      "Average test loss: 0.0031705519031319353\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06995462594760789\n",
      "Average test loss: 0.0031057691857632663\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06719281123081842\n",
      "Average test loss: 0.003066740305473407\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06499994908438789\n",
      "Average test loss: 0.003273180656549003\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06329136347439554\n",
      "Average test loss: 0.0030220115242732895\n",
      "Epoch 85/300\n",
      "Average training loss: 0.061753615909152564\n",
      "Average test loss: 0.003137939549361666\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06039277645283275\n",
      "Average test loss: 0.003735090859234333\n",
      "Epoch 87/300\n",
      "Average training loss: 0.059171985179185864\n",
      "Average test loss: 0.003085612136249741\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05742301677995258\n",
      "Average test loss: 0.003456948998487658\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05664637304345767\n",
      "Average test loss: 0.0029870219040248128\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05605704982082049\n",
      "Average test loss: 0.0030062904440694383\n",
      "Epoch 92/300\n",
      "Average training loss: 0.056015281862682766\n",
      "Average test loss: 0.0029797554365876646\n",
      "Epoch 93/300\n",
      "Average training loss: 0.0551995971136623\n",
      "Average test loss: 0.0029645497631281613\n",
      "Epoch 94/300\n",
      "Average training loss: 0.054622879945569566\n",
      "Average test loss: 0.0042551085100405745\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05422674036026001\n",
      "Average test loss: 0.0029606955768540504\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05407043393452962\n",
      "Average test loss: 0.003100586657101909\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05363539919919438\n",
      "Average test loss: 0.0032733052170111072\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05359367436501715\n",
      "Average test loss: 0.003049498370331195\n",
      "Epoch 99/300\n",
      "Average training loss: 0.053279424352778326\n",
      "Average test loss: 0.00297716299434089\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07387220063143306\n",
      "Average test loss: 0.0033591133625143106\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0622623025642501\n",
      "Average test loss: 0.0032223951845533317\n",
      "Epoch 102/300\n",
      "Average training loss: 0.057715053114626144\n",
      "Average test loss: 0.003039545980592569\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05579307711786694\n",
      "Average test loss: 0.0030441053749786485\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0547607294552856\n",
      "Average test loss: 0.0030334780768801767\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05389230352640152\n",
      "Average test loss: 0.0030298119806167153\n",
      "Epoch 106/300\n",
      "Average training loss: 0.053724418179856404\n",
      "Average test loss: 0.0030050162995855015\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0531920721994506\n",
      "Average test loss: 0.002970203639318546\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05297588784164853\n",
      "Average test loss: 0.0030581927390562163\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0527009600798289\n",
      "Average test loss: 0.0030855251418219674\n",
      "Epoch 110/300\n",
      "Average training loss: 0.052528926700353626\n",
      "Average test loss: 0.002984381269456612\n",
      "Epoch 111/300\n",
      "Average training loss: 0.052337378886010914\n",
      "Average test loss: 0.003114610508705179\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05271365804142422\n",
      "Average test loss: 0.0030399650821669234\n",
      "Epoch 113/300\n",
      "Average training loss: 0.051997719797823166\n",
      "Average test loss: 0.00300617475890451\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05193063745896021\n",
      "Average test loss: 0.003040731664126118\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05156906217005518\n",
      "Average test loss: 0.003026006109184689\n",
      "Epoch 116/300\n",
      "Average training loss: 0.051399017416768604\n",
      "Average test loss: 0.0030532333076828057\n",
      "Epoch 117/300\n",
      "Average training loss: 0.051413464453485276\n",
      "Average test loss: 0.003102086607987682\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05119664810763465\n",
      "Average test loss: 0.003144429196189675\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05105713152223163\n",
      "Average test loss: 0.003001453340260519\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05090598124927945\n",
      "Average test loss: 0.0031391471278750236\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05083024307092031\n",
      "Average test loss: 0.003164709697994921\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05063037230902248\n",
      "Average test loss: 0.0040735229638715585\n",
      "Epoch 123/300\n",
      "Average training loss: 0.050662974576155345\n",
      "Average test loss: 0.003006444867286417\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05130160833729638\n",
      "Average test loss: 0.003001523817785912\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05019343872865041\n",
      "Average test loss: 0.0030466708302911783\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05010604609383477\n",
      "Average test loss: 0.0030220195677959256\n",
      "Epoch 127/300\n",
      "Average training loss: 0.050016715321275926\n",
      "Average test loss: 0.0029773903658820524\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07306684241029951\n",
      "Average test loss: 0.0030281828130698865\n",
      "Epoch 131/300\n",
      "Average training loss: 0.06077879340118832\n",
      "Average test loss: 0.002963629080189599\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05640861074792014\n",
      "Average test loss: 0.002957504218651189\n",
      "Epoch 133/300\n",
      "Average training loss: 0.054174594273169834\n",
      "Average test loss: 0.0034575040367328456\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05330486074421141\n",
      "Average test loss: 0.0031558259195751614\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05169261883033646\n",
      "Average test loss: 0.0029749574261820977\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05089333331253793\n",
      "Average test loss: 0.004391818309823672\n",
      "Epoch 137/300\n",
      "Average training loss: 0.050500400069687104\n",
      "Average test loss: 0.002986953595653176\n",
      "Epoch 138/300\n",
      "Average training loss: 0.050231435043944256\n",
      "Average test loss: 0.0030110243069421914\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04995278290576405\n",
      "Average test loss: 0.0031450967374775144\n",
      "Epoch 140/300\n",
      "Average training loss: 0.049715278224812615\n",
      "Average test loss: 0.003321330152451992\n",
      "Epoch 141/300\n",
      "Average training loss: 0.049458355655272804\n",
      "Average test loss: 0.003037142303254869\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04951153077350722\n",
      "Average test loss: 0.003138407630018062\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04921934864918391\n",
      "Average test loss: 0.0035968492498828306\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04918419351511531\n",
      "Average test loss: 0.0030891020749178196\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04903443499737316\n",
      "Average test loss: 0.003136116354312334\n",
      "Epoch 146/300\n",
      "Average training loss: 0.049008058551285004\n",
      "Average test loss: 0.003025358460326162\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0488684152563413\n",
      "Average test loss: 0.003030972220831447\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04888003992703226\n",
      "Average test loss: 0.004638050209316943\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04866967148582141\n",
      "Average test loss: 0.0030488999113440513\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04873881929781702\n",
      "Average test loss: 0.0031854577269405126\n",
      "Epoch 151/300\n",
      "Average training loss: 0.048546132587724264\n",
      "Average test loss: 0.0030889850076701907\n",
      "Epoch 152/300\n",
      "Average training loss: 0.048448244707451925\n",
      "Average test loss: 0.003057748586146368\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04878424618641535\n",
      "Average test loss: 0.002981933669290609\n",
      "Epoch 154/300\n",
      "Average training loss: 0.1609149320423603\n",
      "Average test loss: 0.005251374090711276\n",
      "Epoch 155/300\n",
      "Average training loss: 0.2845511194997364\n",
      "Average test loss: 0.005163119155085749\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10519686224725511\n",
      "Average test loss: 0.0031751391515135764\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08269412978490194\n",
      "Average test loss: 0.003183587581747108\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07429814026090834\n",
      "Average test loss: 0.0034475105663554534\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06901622580157386\n",
      "Average test loss: 0.003031468974426389\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06567698674069511\n",
      "Average test loss: 0.0030314828640677863\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06303753634956148\n",
      "Average test loss: 0.003580771063144008\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06114142515924242\n",
      "Average test loss: 0.0034999430576960248\n",
      "Epoch 163/300\n",
      "Average training loss: 0.059592175957229404\n",
      "Average test loss: 0.0031096602140201464\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05817303907540109\n",
      "Average test loss: 0.003172287786172496\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05688711204793718\n",
      "Average test loss: 0.0030179903333385784\n",
      "Epoch 166/300\n",
      "Average training loss: 0.054032905138201186\n",
      "Average test loss: 0.0030647036470472814\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05323474837011761\n",
      "Average test loss: 0.002983668813068006\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05268803741865688\n",
      "Average test loss: 0.003041424959897995\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05196463686558935\n",
      "Average test loss: 0.0033732836238212056\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05127725443243981\n",
      "Average test loss: 0.00430513824398319\n",
      "Epoch 173/300\n",
      "Average training loss: 0.050861035287380216\n",
      "Average test loss: 0.0030205501309699483\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05039542470375697\n",
      "Average test loss: 0.003010456366257535\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0500091297990746\n",
      "Average test loss: 0.0030603071465674374\n",
      "Epoch 176/300\n",
      "Average training loss: 0.049630802783701154\n",
      "Average test loss: 0.0030029297224763367\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04932444746957885\n",
      "Average test loss: 0.0037886281936532922\n",
      "Epoch 178/300\n",
      "Average training loss: 0.049236402405632866\n",
      "Average test loss: 0.003047850117708246\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04883378276228905\n",
      "Average test loss: 0.00299471526634362\n",
      "Epoch 180/300\n",
      "Average training loss: 0.048716707613733076\n",
      "Average test loss: 0.0030504743698984383\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04843273259202639\n",
      "Average test loss: 0.003018437112785048\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04844860420293278\n",
      "Average test loss: 0.0033309152832047806\n",
      "Epoch 183/300\n",
      "Average training loss: 0.048232085661755665\n",
      "Average test loss: 0.0031384944445970986\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04817251512408256\n",
      "Average test loss: 0.0030853484173615775\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0480332885881265\n",
      "Average test loss: 17.99822290579478\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04784170253740417\n",
      "Average test loss: 0.0030886430158797236\n",
      "Epoch 188/300\n",
      "Average training loss: 0.047608039190371834\n",
      "Average test loss: 0.0031911229602992535\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04757253135575189\n",
      "Average test loss: 0.0031229451497395834\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05377526556783252\n",
      "Average test loss: 0.003002459059158961\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06027589413192537\n",
      "Average test loss: 0.0033758044487072363\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0594851778878106\n",
      "Average test loss: 0.0030533978338870738\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05343307373921077\n",
      "Average test loss: 0.0030117354517181713\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0510035368833277\n",
      "Average test loss: 0.003345080332623588\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04957623048292266\n",
      "Average test loss: 0.0031078978921804164\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04877907192375925\n",
      "Average test loss: 0.003070033753497733\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04816485089063644\n",
      "Average test loss: 0.003799151913987266\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04778971295555433\n",
      "Average test loss: 0.0031387456531325976\n",
      "Epoch 199/300\n",
      "Average training loss: 0.047460495760043464\n",
      "Average test loss: 0.003449289767899447\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04734499755832884\n",
      "Average test loss: 0.003081391922922598\n",
      "Epoch 201/300\n",
      "Average training loss: 0.047150638845231796\n",
      "Average test loss: 0.004142710566934612\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04709658933679263\n",
      "Average test loss: 0.003041498504165146\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04705133718914456\n",
      "Average test loss: 0.003078639216100176\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0469659851094087\n",
      "Average test loss: 0.0030489385331877402\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04687089999516805\n",
      "Average test loss: 0.003062451138264603\n",
      "Epoch 206/300\n",
      "Average training loss: 0.046974120540751355\n",
      "Average test loss: 0.003856445931726032\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04674723062912623\n",
      "Average test loss: 0.003030337023652262\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04662185715304481\n",
      "Average test loss: 0.0032753281458798383\n",
      "Epoch 210/300\n",
      "Average training loss: 0.046752108216285705\n",
      "Average test loss: 0.00346187653640906\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04708131255375014\n",
      "Average test loss: 0.0033914310899045732\n",
      "Epoch 212/300\n",
      "Average training loss: 0.046487921486298246\n",
      "Average test loss: 0.0031484511393225856\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04705006914337476\n",
      "Average test loss: 0.0033041190356016157\n",
      "Epoch 214/300\n",
      "Average training loss: 0.046315191176202565\n",
      "Average test loss: 0.0203431961801317\n",
      "Epoch 215/300\n",
      "Average training loss: 0.046184643752045104\n",
      "Average test loss: 0.00307524225819442\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04609761154320505\n",
      "Average test loss: 0.0036421514201081463\n",
      "Epoch 217/300\n",
      "Average training loss: 0.046188116361697514\n",
      "Average test loss: 0.0032139603219305474\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04603734521402253\n",
      "Average test loss: 0.0032252790046234927\n",
      "Epoch 219/300\n",
      "Average training loss: 0.045975303024053574\n",
      "Average test loss: 0.0031839817141493163\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04587122812867164\n",
      "Average test loss: 0.0031549018716646566\n",
      "Epoch 221/300\n",
      "Average training loss: 0.045912602249119014\n",
      "Average test loss: 0.0031000964966499144\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04578271687693066\n",
      "Average test loss: 0.003125318045831389\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04585278786553277\n",
      "Average test loss: 0.0031966681900537676\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0460221123430464\n",
      "Average test loss: 0.0031253264099359514\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04599052140116692\n",
      "Average test loss: 0.0054570195683174665\n",
      "Epoch 226/300\n",
      "Average training loss: 0.045508998450305725\n",
      "Average test loss: 0.003330661327474647\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04551392592324151\n",
      "Average test loss: 0.0031522958767082956\n",
      "Epoch 231/300\n",
      "Average training loss: 0.045645656274424656\n",
      "Average test loss: 0.0041843751693765325\n",
      "Epoch 232/300\n",
      "Average training loss: 0.074476472430759\n",
      "Average test loss: 0.004053433904217349\n",
      "Epoch 233/300\n",
      "Average training loss: 0.051320098413361444\n",
      "Average test loss: 0.00305159250791702\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0480437220202552\n",
      "Average test loss: 0.003160332787368033\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04643923920724127\n",
      "Average test loss: 0.003193003648892045\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04597449768251843\n",
      "Average test loss: 0.0031014246452185845\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0453412065770891\n",
      "Average test loss: 0.0032954248249944715\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04524723465243975\n",
      "Average test loss: 0.0032053610680417882\n",
      "Epoch 239/300\n",
      "Average training loss: 0.045140393508805166\n",
      "Average test loss: 0.003157556302017636\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04505662537283368\n",
      "Average test loss: 0.00315268051231073\n",
      "Epoch 241/300\n",
      "Average training loss: 0.045073679861095214\n",
      "Average test loss: 0.003126688009955817\n",
      "Epoch 242/300\n",
      "Average training loss: 0.045125853058364654\n",
      "Average test loss: 0.0031191421101490657\n",
      "Epoch 243/300\n",
      "Average training loss: 0.045160477524002395\n",
      "Average test loss: 0.0032608359284285043\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04514539157681995\n",
      "Average test loss: 0.00406814989199241\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04511762375301785\n",
      "Average test loss: 0.0031127892043441535\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04535916045970387\n",
      "Average test loss: 0.003289333460645543\n",
      "Epoch 247/300\n",
      "Average training loss: 0.045520768615934584\n",
      "Average test loss: 0.003213980541461044\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04520535776350233\n",
      "Average test loss: 0.00309726142241723\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04486329351531135\n",
      "Average test loss: 0.003755155923879809\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04484650664197074\n",
      "Average test loss: 0.0035153031792077754\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04479137568341361\n",
      "Average test loss: 0.0035729482012490433\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04474535844723384\n",
      "Average test loss: 0.0031205463593618737\n",
      "Epoch 254/300\n",
      "Average training loss: 0.044717007842328814\n",
      "Average test loss: 0.003142100057668156\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04472427673141162\n",
      "Average test loss: 0.0031274248647193115\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04468553614616394\n",
      "Average test loss: 0.0033189870063215494\n",
      "Epoch 257/300\n",
      "Average training loss: 0.044695466592907906\n",
      "Average test loss: 0.003196330210814873\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04457769335971938\n",
      "Average test loss: 0.004462810769263241\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04458312845892377\n",
      "Average test loss: 0.0031823442654891148\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04460390300883187\n",
      "Average test loss: 0.004336949018140634\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04452987126509349\n",
      "Average test loss: 0.0031505621266033914\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04447320556640625\n",
      "Average test loss: 0.003205719091826015\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04426730495029026\n",
      "Average test loss: 0.0031565827288561397\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04439978323214584\n",
      "Average test loss: 0.0033007298848695223\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04439140376448631\n",
      "Average test loss: 0.003723972462531593\n",
      "Epoch 266/300\n",
      "Average training loss: 0.044307043145100276\n",
      "Average test loss: 0.027377327180571027\n",
      "Epoch 267/300\n",
      "Average training loss: 0.044266379525264105\n",
      "Average test loss: 0.003203202077704999\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04425059798028734\n",
      "Average test loss: 0.0035134655079907843\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04421825719210837\n",
      "Average test loss: 0.0032135124783962965\n",
      "Epoch 270/300\n",
      "Average training loss: 0.044203451252645916\n",
      "Average test loss: 0.0032848090144495167\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04474385706583659\n",
      "Average test loss: 0.0031675695300930075\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04398294976353646\n",
      "Average test loss: 0.012362877791954411\n",
      "Epoch 275/300\n",
      "Average training loss: 0.044005450463957256\n",
      "Average test loss: 0.0035966987647116184\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04394321858882904\n",
      "Average test loss: 0.0032294564145720667\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04392743758691682\n",
      "Average test loss: 0.011000877706127034\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04403583909736739\n",
      "Average test loss: 0.0031497318430079354\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04427548676398065\n",
      "Average test loss: 0.0033089116497172248\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04446108141210344\n",
      "Average test loss: 0.0032867397059583\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04407233229279518\n",
      "Average test loss: 0.0032120146908693843\n",
      "Epoch 282/300\n",
      "Average training loss: 0.044217990616957345\n",
      "Average test loss: 0.0032156894442935784\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04372303021616406\n",
      "Average test loss: 0.003364818828180432\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04372611179947853\n",
      "Average test loss: 0.003196717297451364\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04370898948444261\n",
      "Average test loss: 0.0034995775364546314\n",
      "Epoch 286/300\n",
      "Average training loss: 0.043738181425465476\n",
      "Average test loss: 0.003195587763148877\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04374610249201457\n",
      "Average test loss: 0.013688171221150292\n",
      "Epoch 288/300\n",
      "Average training loss: 0.043766295591990155\n",
      "Average test loss: 0.0031997886430472134\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04364796427554554\n",
      "Average test loss: 0.003222154452568955\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04359714264008734\n",
      "Average test loss: 0.003206977711990476\n",
      "Epoch 291/300\n",
      "Average training loss: 0.043629379358556535\n",
      "Average test loss: 0.05289681639605098\n",
      "Epoch 292/300\n",
      "Average training loss: 0.043704133457607694\n",
      "Average test loss: 0.0034217265103426244\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04360313383903768\n",
      "Average test loss: 0.0043995491738120715\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04359025429685911\n",
      "Average test loss: 0.003204910604697135\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04352080403930611\n",
      "Average test loss: 0.0033056492147346336\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04353352915081713\n",
      "Average test loss: 0.003495173363635937\n",
      "Epoch 297/300\n",
      "Average training loss: 0.043852123320102694\n",
      "Average test loss: 0.0031386581493748557\n",
      "Epoch 300/300\n",
      "Average training loss: 0.043630070984363556\n",
      "Average test loss: 0.0032022779766056274\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5024949350886874\n",
      "Average test loss: 0.005555496877680222\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4556586921215057\n",
      "Average test loss: 0.004772864776766962\n",
      "Epoch 3/300\n",
      "Average training loss: 0.29478854064146676\n",
      "Average test loss: 0.004363521499352323\n",
      "Epoch 4/300\n",
      "Average training loss: 0.22211148602432676\n",
      "Average test loss: 0.0038804429007901087\n",
      "Epoch 5/300\n",
      "Average training loss: 0.17920858732859293\n",
      "Average test loss: 0.0039993394493228855\n",
      "Epoch 6/300\n",
      "Average training loss: 0.15174809314144982\n",
      "Average test loss: 0.0035689405558837783\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0997427907122506\n",
      "Average test loss: 0.0031354134438766374\n",
      "Epoch 11/300\n",
      "Average training loss: 0.09318918982479307\n",
      "Average test loss: 0.0035718419667747285\n",
      "Epoch 12/300\n",
      "Average training loss: 0.08777210140890546\n",
      "Average test loss: 0.0038028864711523057\n",
      "Epoch 13/300\n",
      "Average training loss: 0.08358711041344537\n",
      "Average test loss: 0.0037870131416453253\n",
      "Epoch 14/300\n",
      "Average training loss: 0.07916940023501715\n",
      "Average test loss: 0.003742211585243543\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07593920350074768\n",
      "Average test loss: 0.0028712911899718974\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07235423339075513\n",
      "Average test loss: 0.0028634122067855462\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0696821269525422\n",
      "Average test loss: 0.0024897338774883086\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06754023265838623\n",
      "Average test loss: 0.0024530217902113995\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06456087694896592\n",
      "Average test loss: 0.0027396534286025496\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06216767835617065\n",
      "Average test loss: 0.005985543122101161\n",
      "Epoch 21/300\n",
      "Average training loss: 0.060840188960234326\n",
      "Average test loss: 0.0027081393890289796\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05850969530145327\n",
      "Average test loss: 0.0024189136444280543\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0560850750207901\n",
      "Average test loss: 0.002295574968059858\n",
      "Epoch 24/300\n",
      "Average training loss: 0.054678365540173315\n",
      "Average test loss: 0.00244258488714695\n",
      "Epoch 25/300\n",
      "Average training loss: 0.052580147726668254\n",
      "Average test loss: 0.0024853031130300626\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05119881992207633\n",
      "Average test loss: 0.002215160334068868\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05003417521052891\n",
      "Average test loss: 0.002373575878329575\n",
      "Epoch 28/300\n",
      "Average training loss: 0.048586795303556654\n",
      "Average test loss: 0.0021955204043123456\n",
      "Epoch 29/300\n",
      "Average training loss: 0.047777331623766156\n",
      "Average test loss: 0.0021864839169300266\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04771653581327862\n",
      "Average test loss: 0.0021389370061871077\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08152799747718704\n",
      "Average test loss: 0.0024285810308323965\n",
      "Epoch 32/300\n",
      "Average training loss: 0.048116835908757316\n",
      "Average test loss: 0.0023828340106540257\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04708408395780457\n",
      "Average test loss: 0.0021413483085731665\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04644626559813817\n",
      "Average test loss: 0.0021263573120037715\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04621186985241042\n",
      "Average test loss: 0.0022904461750553715\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0454250753223896\n",
      "Average test loss: 0.002803104789927602\n",
      "Epoch 40/300\n",
      "Average training loss: 0.044877857903639476\n",
      "Average test loss: 0.0021701402145748337\n",
      "Epoch 41/300\n",
      "Average training loss: 0.044546440879503886\n",
      "Average test loss: 0.0021079760690530143\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04418874969416194\n",
      "Average test loss: 0.0020727872780213754\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04376076572140058\n",
      "Average test loss: 0.002068157521386941\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04335555995504061\n",
      "Average test loss: 0.0020685711256745788\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04304381881819831\n",
      "Average test loss: 0.00206596648424036\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04266134888264868\n",
      "Average test loss: 0.0020664440024023255\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04243616165386306\n",
      "Average test loss: 0.0020280472091916534\n",
      "Epoch 48/300\n",
      "Average training loss: 0.042019403401348326\n",
      "Average test loss: 0.0020647143328355417\n",
      "Epoch 49/300\n",
      "Average training loss: 0.044586442059940765\n",
      "Average test loss: 0.0021150429344011677\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04192727636297544\n",
      "Average test loss: 0.002041496991283364\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04153175791766908\n",
      "Average test loss: 0.0020114061082195903\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04120208419362704\n",
      "Average test loss: 0.0021299899597134854\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04098758201135529\n",
      "Average test loss: 0.0020200193331887323\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04081018562780486\n",
      "Average test loss: 0.002044655032042\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04034084541267819\n",
      "Average test loss: 0.0022182922665443684\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04023991562260522\n",
      "Average test loss: 0.002092203468394776\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03992261858450042\n",
      "Average test loss: 0.0021364611610770225\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03925772845745087\n",
      "Average test loss: 0.0019567815956349176\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03909066960215569\n",
      "Average test loss: 0.001989130951774617\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03997523759967751\n",
      "Average test loss: 0.0019634142700168822\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04081591226326095\n",
      "Average test loss: 0.0022810394023027687\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03889586942725711\n",
      "Average test loss: 0.002040442649482025\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03861027672224575\n",
      "Average test loss: 0.002068858154738943\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03848508442772759\n",
      "Average test loss: 0.016317528024315835\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03839440988169776\n",
      "Average test loss: 0.002126497603332003\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03822730312413639\n",
      "Average test loss: 0.0021923147280597023\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0381750240938531\n",
      "Average test loss: 0.002497813292261627\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03796728217601776\n",
      "Average test loss: 0.001959926929531826\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03788059635294808\n",
      "Average test loss: 0.0020523147227035628\n",
      "Epoch 72/300\n",
      "Average training loss: 0.037734271441896755\n",
      "Average test loss: 0.002005592634384003\n",
      "Epoch 73/300\n",
      "Average training loss: 0.037626531995005076\n",
      "Average test loss: 0.02255937364117967\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03772328032718764\n",
      "Average test loss: 0.0019584768975360527\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03743758472634686\n",
      "Average test loss: 0.002320482724863622\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03723138404223654\n",
      "Average test loss: 0.00196621973419355\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03716036747561561\n",
      "Average test loss: 0.002786213084227509\n",
      "Epoch 78/300\n",
      "Average training loss: 0.036934414827161366\n",
      "Average test loss: 0.002458671665440003\n",
      "Epoch 79/300\n",
      "Average training loss: 0.036975477470291986\n",
      "Average test loss: 0.0019395927005550929\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03671692742904027\n",
      "Average test loss: 0.001971636143150843\n",
      "Epoch 81/300\n",
      "Average training loss: 0.036753129141198264\n",
      "Average test loss: 0.0028743649418983197\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03659140522446897\n",
      "Average test loss: 0.0019849319393850035\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03649606650736597\n",
      "Average test loss: 0.001981814705663257\n",
      "Epoch 84/300\n",
      "Average training loss: 0.036427343477805456\n",
      "Average test loss: 0.0019875623068461817\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03634210705426004\n",
      "Average test loss: 0.0019634186197072267\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0361413644105196\n",
      "Average test loss: 0.001968381818383932\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0358984995815489\n",
      "Average test loss: 0.0022660725189165938\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03576388721995884\n",
      "Average test loss: 0.0019359329429765543\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03568466314342287\n",
      "Average test loss: 0.001969265152803726\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03572750457459026\n",
      "Average test loss: 0.0019513834844239884\n",
      "Epoch 95/300\n",
      "Average training loss: 0.035511723617712654\n",
      "Average test loss: 0.0019438013542029593\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03561245913969146\n",
      "Average test loss: 0.001981101269626783\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03571943220496178\n",
      "Average test loss: 0.0021013424987387326\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03536077555682924\n",
      "Average test loss: 0.0031075676472650635\n",
      "Epoch 99/300\n",
      "Average training loss: 0.035497865286138325\n",
      "Average test loss: 0.0019421406319985787\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03585741391777992\n",
      "Average test loss: 0.0019258770637421143\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03515449452234639\n",
      "Average test loss: 0.0019884688136064346\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03516279769771629\n",
      "Average test loss: 0.0019947024033301407\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03503482223219342\n",
      "Average test loss: 0.001971384983147598\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03509051627251837\n",
      "Average test loss: 0.0019423413765099313\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0350112873762846\n",
      "Average test loss: 0.0019473239867430594\n",
      "Epoch 106/300\n",
      "Average training loss: 0.034871095064613555\n",
      "Average test loss: 0.0019795518854839934\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03514245127307044\n",
      "Average test loss: 0.0019691274611072407\n",
      "Epoch 108/300\n",
      "Average training loss: 0.035035154355896846\n",
      "Average test loss: 0.0032830615267157555\n",
      "Epoch 109/300\n",
      "Average training loss: 0.034503945853975086\n",
      "Average test loss: 0.00199853816587064\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03446181487374836\n",
      "Average test loss: 0.0020151382606062623\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03451876386668947\n",
      "Average test loss: 0.00195841992356711\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03438429183099005\n",
      "Average test loss: 0.002110314377169642\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03449766170978546\n",
      "Average test loss: 0.0019964551619357533\n",
      "Epoch 117/300\n",
      "Average training loss: 0.034707210659980774\n",
      "Average test loss: 0.0019724482679222195\n",
      "Epoch 118/300\n",
      "Average training loss: 0.034171870016389425\n",
      "Average test loss: 0.001979888996730248\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03426471207042535\n",
      "Average test loss: 0.0021371315231339797\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03406088539130158\n",
      "Average test loss: 0.0034023355197989277\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03419377539886369\n",
      "Average test loss: 0.003017875256223811\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03417987096640799\n",
      "Average test loss: 0.0023074263468798664\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03432957921922207\n",
      "Average test loss: 0.001965273326987194\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03419109859897031\n",
      "Average test loss: 0.002025013085247742\n",
      "Epoch 125/300\n",
      "Average training loss: 0.033972038719389176\n",
      "Average test loss: 0.0020576592196399966\n",
      "Epoch 126/300\n",
      "Average training loss: 0.033852972709470325\n",
      "Average test loss: 0.0019714258182793854\n",
      "Epoch 127/300\n",
      "Average training loss: 0.033982170303662615\n",
      "Average test loss: 0.020644068223734695\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03379583145512475\n",
      "Average test loss: 0.004679113069963124\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03382495385408402\n",
      "Average test loss: 0.0020194898996916083\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0337981512579653\n",
      "Average test loss: 0.0019763422111670178\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03383089729150136\n",
      "Average test loss: 0.0019876834615651104\n",
      "Epoch 132/300\n",
      "Average training loss: 0.033661320818795096\n",
      "Average test loss: 0.0038373533938493992\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03352357848650879\n",
      "Average test loss: 0.0066612169908152686\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03362133135398229\n",
      "Average test loss: 0.0022544855328483714\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03368554187648826\n",
      "Average test loss: 0.0022284409587995874\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0334868810756339\n",
      "Average test loss: 0.002147611853149202\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03349776934915119\n",
      "Average test loss: 0.0021582071781158447\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03346933175788985\n",
      "Average test loss: 0.006149265577188797\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03341134905318419\n",
      "Average test loss: 0.003530580567816893\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03351202746894624\n",
      "Average test loss: 0.00213504244097405\n",
      "Epoch 143/300\n",
      "Average training loss: 0.033490668601459925\n",
      "Average test loss: 0.0021729066111147405\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03331057629485925\n",
      "Average test loss: 0.0030679898429661987\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03320375695493486\n",
      "Average test loss: 0.004521169754159119\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03335263664192623\n",
      "Average test loss: 0.3443136670291424\n",
      "Epoch 147/300\n",
      "Average training loss: 0.033356921507252585\n",
      "Average test loss: 0.0021129375644442106\n",
      "Epoch 148/300\n",
      "Average training loss: 0.033130295832951864\n",
      "Average test loss: 0.0034013302243418165\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03343174080385102\n",
      "Average test loss: 0.0019910013657063247\n",
      "Epoch 150/300\n",
      "Average training loss: 0.033146796766254635\n",
      "Average test loss: 0.005218781317480737\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03303768094380696\n",
      "Average test loss: 0.0020452918846988015\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03302609063519372\n",
      "Average test loss: 0.00241300266298155\n",
      "Epoch 153/300\n",
      "Average training loss: 0.033247181738416356\n",
      "Average test loss: 0.002040060356259346\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03314127952853839\n",
      "Average test loss: 0.00204336420353502\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03294188080893622\n",
      "Average test loss: 0.00203937116296341\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03286353385117319\n",
      "Average test loss: 0.0030155045323901706\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03283626314997673\n",
      "Average test loss: 0.004785393852533565\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03295157865020964\n",
      "Average test loss: 0.0020642627628727093\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03285061727629768\n",
      "Average test loss: 0.0020740629461490447\n",
      "Epoch 162/300\n",
      "Average training loss: 0.032808828067448406\n",
      "Average test loss: 0.0021074559522999657\n",
      "Epoch 163/300\n",
      "Average training loss: 0.032763236249486606\n",
      "Average test loss: 0.0021065813459249004\n",
      "Epoch 166/300\n",
      "Average training loss: 0.032888647720217705\n",
      "Average test loss: 0.0021671433496392436\n",
      "Epoch 167/300\n",
      "Average training loss: 0.032812300831079486\n",
      "Average test loss: 0.0294362695813179\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03259692605833212\n",
      "Average test loss: 0.002151379907814165\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03260502921872669\n",
      "Average test loss: 0.0021916057252221637\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03271450341410107\n",
      "Average test loss: 0.0020056325021303363\n",
      "Epoch 172/300\n",
      "Average training loss: 0.032546016034152775\n",
      "Average test loss: 0.002032925593563252\n",
      "Epoch 173/300\n",
      "Average training loss: 0.032470382023188804\n",
      "Average test loss: 0.0021580207902524204\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03254495528671476\n",
      "Average test loss: 0.003115042586500446\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03270095910628636\n",
      "Average test loss: 0.0020896457177069453\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03233440524670813\n",
      "Average test loss: 0.0021296886649603644\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03275801152653164\n",
      "Average test loss: 0.002022260167842938\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03230066525273853\n",
      "Average test loss: 0.0020968994462034767\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03239179997973972\n",
      "Average test loss: 0.0021627601871473923\n",
      "Epoch 180/300\n",
      "Average training loss: 0.032477911369668114\n",
      "Average test loss: 0.0035224782307114867\n",
      "Epoch 181/300\n",
      "Average training loss: 0.032417443979117604\n",
      "Average test loss: 0.002568183249483506\n",
      "Epoch 182/300\n",
      "Average training loss: 0.032336405406395596\n",
      "Average test loss: 0.0038758770341260564\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03234014217389954\n",
      "Average test loss: 0.0029190662720551095\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03240388225184546\n",
      "Average test loss: 0.002063924265611503\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03240887748863962\n",
      "Average test loss: 0.0021266099591222072\n",
      "Epoch 186/300\n",
      "Average training loss: 0.032452014565467834\n",
      "Average test loss: 0.0020648426048250662\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0322646707991759\n",
      "Average test loss: 0.0021006547989737656\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03229858254724079\n",
      "Average test loss: 0.0021018744048972924\n",
      "Epoch 189/300\n",
      "Average training loss: 0.032276995577745965\n",
      "Average test loss: 0.05979557678434584\n",
      "Epoch 190/300\n",
      "Average training loss: 0.032325047175089515\n",
      "Average test loss: 0.0022227172562852503\n",
      "Epoch 191/300\n",
      "Average training loss: 0.032150542724463674\n",
      "Average test loss: 0.0028547243501784074\n",
      "Epoch 192/300\n",
      "Average training loss: 0.032297949333985644\n",
      "Average test loss: 0.0023086603986513282\n",
      "Epoch 193/300\n",
      "Average training loss: 0.032143859959310954\n",
      "Average test loss: 0.002127478378721409\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03218240741888682\n",
      "Average test loss: 0.0024451492619183327\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03217959097027779\n",
      "Average test loss: 0.0021919752868513266\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03229640123910374\n",
      "Average test loss: 0.0022286865283838576\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03199676952428288\n",
      "Average test loss: 0.0020716111965270504\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03198347781267431\n",
      "Average test loss: 0.0021316958189838463\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03203765476246675\n",
      "Average test loss: 0.0021744489354184934\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03204056277871132\n",
      "Average test loss: 0.003868443282528056\n",
      "Epoch 201/300\n",
      "Average training loss: 0.031950292925039925\n",
      "Average test loss: 0.002229888357429041\n",
      "Epoch 204/300\n",
      "Average training loss: 0.031966000709268784\n",
      "Average test loss: 0.0020769476008911927\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03202586859795782\n",
      "Average test loss: 0.002127711185771558\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03183598914080196\n",
      "Average test loss: 0.002078656503309806\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03185259735584259\n",
      "Average test loss: 0.0020646482016891243\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03186039326588313\n",
      "Average test loss: 0.002091532290809684\n",
      "Epoch 211/300\n",
      "Average training loss: 0.031885002975662546\n",
      "Average test loss: 0.002142351740453806\n",
      "Epoch 212/300\n",
      "Average training loss: 0.031866892218589786\n",
      "Average test loss: 0.002106710759508941\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03193019219736258\n",
      "Average test loss: 0.0026337240013397404\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03178998500936561\n",
      "Average test loss: 0.0021145485297052397\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03171538171503279\n",
      "Average test loss: 0.002125974424286849\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0317844385140472\n",
      "Average test loss: 0.0021588681245015727\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03181956236561139\n",
      "Average test loss: 0.0026900848880824116\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031748275152511066\n",
      "Average test loss: 0.002183268983227511\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03185555483400822\n",
      "Average test loss: 0.0023531853417969413\n",
      "Epoch 220/300\n",
      "Average training loss: 0.031668444568912185\n",
      "Average test loss: 0.0021533431022738416\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03175182142853737\n",
      "Average test loss: 0.002117281172424555\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03167520429028405\n",
      "Average test loss: 0.0022592865967502197\n",
      "Epoch 223/300\n",
      "Average training loss: 0.031747576398981944\n",
      "Average test loss: 0.0021219751147760284\n",
      "Epoch 224/300\n",
      "Average training loss: 0.031597319622834526\n",
      "Average test loss: 0.002144538971078065\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03170507008996275\n",
      "Average test loss: 0.002389116696185536\n",
      "Epoch 226/300\n",
      "Average training loss: 0.031612085878849026\n",
      "Average test loss: 0.002130306290048692\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03196549491418733\n",
      "Average test loss: 0.0021025979794147944\n",
      "Epoch 228/300\n",
      "Average training loss: 0.031535010205374824\n",
      "Average test loss: 0.0021244795653555127\n",
      "Epoch 229/300\n",
      "Average training loss: 0.031590528574254775\n",
      "Average test loss: 0.0023195074572124417\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03156854253676203\n",
      "Average test loss: 0.002118429282473193\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03153443334334426\n",
      "Average test loss: 0.0021746553047042754\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0317008950776524\n",
      "Average test loss: 0.002185059591817359\n",
      "Epoch 236/300\n",
      "Average training loss: 0.031511994356910385\n",
      "Average test loss: 0.00212928479242449\n",
      "Epoch 237/300\n",
      "Average training loss: 0.031451789528131484\n",
      "Average test loss: 0.0021535861330727736\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03149518193304539\n",
      "Average test loss: 0.0021736060362309217\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03147586010230912\n",
      "Average test loss: 0.0021411544893764787\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03149928055538072\n",
      "Average test loss: 0.002168310344633129\n",
      "Epoch 241/300\n",
      "Average training loss: 0.031488920561141436\n",
      "Average test loss: 0.002152370240849753\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03145784983535608\n",
      "Average test loss: 0.0021025470675279695\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03144179773496257\n",
      "Average test loss: 0.002087530159908864\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031399736816684405\n",
      "Average test loss: 0.00309710301603708\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03152729976839489\n",
      "Average test loss: 0.002412874787218041\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03139257228043344\n",
      "Average test loss: 0.0022320557915502122\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03157944951123662\n",
      "Average test loss: 0.00213985950841258\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03135585896008544\n",
      "Average test loss: 0.0023127678833487963\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03140530613064766\n",
      "Average test loss: 0.0026495174275090296\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03131590169005924\n",
      "Average test loss: 0.0021079599501358137\n",
      "Epoch 251/300\n",
      "Average training loss: 0.031329029993878474\n",
      "Average test loss: 0.002071011672831244\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03130845252341694\n",
      "Average test loss: 0.0039741680382026565\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0312707326511542\n",
      "Average test loss: 0.002788969687703583\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03117644164628453\n",
      "Average test loss: 0.002141426341401206\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03131018769409921\n",
      "Average test loss: 0.0023131831569804086\n",
      "Epoch 258/300\n",
      "Average training loss: 0.031247151434421538\n",
      "Average test loss: 0.0021339621843977107\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03123252118958367\n",
      "Average test loss: 0.00216136700908343\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03127145350807243\n",
      "Average test loss: 0.002192526062950492\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03126813195645809\n",
      "Average test loss: 0.002090000060490436\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03123505930768119\n",
      "Average test loss: 0.0032226311972157824\n",
      "Epoch 263/300\n",
      "Average training loss: 0.031185572504997254\n",
      "Average test loss: 0.002138979456697901\n",
      "Epoch 264/300\n",
      "Average training loss: 0.031207498441139857\n",
      "Average test loss: 0.0021393255739369328\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03119171076019605\n",
      "Average test loss: 0.0020885703972437315\n",
      "Epoch 266/300\n",
      "Average training loss: 0.031211984386046727\n",
      "Average test loss: 0.002133344868198037\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03134409802158673\n",
      "Average test loss: 0.0021559331297046607\n",
      "Epoch 268/300\n",
      "Average training loss: 0.031091220794452562\n",
      "Average test loss: 0.002175297301262617\n",
      "Epoch 269/300\n",
      "Average training loss: 0.031236846817864313\n",
      "Average test loss: 0.0021476627370963495\n",
      "Epoch 270/300\n",
      "Average training loss: 0.031135386966996723\n",
      "Average test loss: 0.002247486455986897\n",
      "Epoch 271/300\n",
      "Average training loss: 0.031135314196348192\n",
      "Average test loss: 0.0028195078898635177\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03111642739839024\n",
      "Average test loss: 0.003995035509682364\n",
      "Epoch 273/300\n",
      "Average training loss: 0.031201040264632968\n",
      "Average test loss: 0.002380858171524273\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03107999219497045\n",
      "Average test loss: 0.019948157012462615\n",
      "Epoch 275/300\n",
      "Average training loss: 0.031230898689892556\n",
      "Average test loss: 0.1302121144971914\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03112005470196406\n",
      "Average test loss: 0.0035600504477818805\n",
      "Epoch 279/300\n",
      "Average training loss: 0.031044345418612163\n",
      "Average test loss: 38.929814025243125\n",
      "Epoch 280/300\n",
      "Average training loss: 0.031142710006899305\n",
      "Average test loss: 0.007329702412295673\n",
      "Epoch 281/300\n",
      "Average training loss: 0.031098714701003498\n",
      "Average test loss: 0.002328604180469281\n",
      "Epoch 282/300\n",
      "Average training loss: 0.031026080465979045\n",
      "Average test loss: 0.0022696056726078193\n",
      "Epoch 283/300\n",
      "Average training loss: 0.031015659673346414\n",
      "Average test loss: 0.0021213603886879153\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03096700147125456\n",
      "Average test loss: 0.0020748157176292605\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03099174862437778\n",
      "Average test loss: 0.0021323062053157224\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03104781542552842\n",
      "Average test loss: 0.002664730673034986\n",
      "Epoch 287/300\n",
      "Average training loss: 0.030943547043535444\n",
      "Average test loss: 0.00224324022171398\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030894130607446033\n",
      "Average test loss: 0.0021822287984606292\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03106390274895562\n",
      "Average test loss: 0.0021757927547312447\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030973019841644498\n",
      "Average test loss: 0.002483513867792984\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0309581968171729\n",
      "Average test loss: 0.0021158394128498103\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03094326880739795\n",
      "Average test loss: 0.002170110211811132\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030938440912299685\n",
      "Average test loss: 0.0021931915268715886\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03094117310974333\n",
      "Average test loss: 0.002276556864898238\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03093613365623686\n",
      "Average test loss: 0.0021832609560547603\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030941513856252035\n",
      "Average test loss: 0.002151414213391642\n",
      "Epoch 297/300\n",
      "Average training loss: 0.030902454518609578\n",
      "Average test loss: 0.07313811947405338\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03104687417878045\n",
      "Average test loss: 0.0021505634697775045\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03086636639965905\n",
      "Average test loss: 0.0022372635110384886\n",
      "Epoch 300/300\n",
      "Average training loss: 0.030890227917167876\n",
      "Average test loss: 0.0026883285449196895\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.3768173818058438\n",
      "Average test loss: 0.00526892200153735\n",
      "Epoch 2/300\n",
      "Average training loss: 0.4151350891325209\n",
      "Average test loss: 0.003951243786762158\n",
      "Epoch 3/300\n",
      "Average training loss: 0.2684495153824488\n",
      "Average test loss: 0.003448176666473349\n",
      "Epoch 4/300\n",
      "Average training loss: 0.19898554960886639\n",
      "Average test loss: 0.0034323811454491484\n",
      "Epoch 5/300\n",
      "Average training loss: 0.15968029585149554\n",
      "Average test loss: 0.0030255932381583585\n",
      "Epoch 6/300\n",
      "Average training loss: 0.133687522093455\n",
      "Average test loss: 0.003185606609409054\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11626926195621491\n",
      "Average test loss: 0.004558120602120956\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10353320168124305\n",
      "Average test loss: 0.0028985940739512444\n",
      "Epoch 9/300\n",
      "Average training loss: 0.09372415967120065\n",
      "Average test loss: 0.0025976327177551057\n",
      "Epoch 10/300\n",
      "Average training loss: 0.08655269676446915\n",
      "Average test loss: 0.004119433726701471\n",
      "Epoch 11/300\n",
      "Average training loss: 0.08065413195888202\n",
      "Average test loss: 0.002455440670458807\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07579664443598853\n",
      "Average test loss: 0.002540690486629804\n",
      "Epoch 13/300\n",
      "Average training loss: 0.07137119569381078\n",
      "Average test loss: 0.0025258841515622206\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06746113293369611\n",
      "Average test loss: 0.002467114752779404\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06433592239353392\n",
      "Average test loss: 0.0021343741614578498\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06179134276840422\n",
      "Average test loss: 0.002445505053218868\n",
      "Epoch 17/300\n",
      "Average training loss: 0.058845123317506576\n",
      "Average test loss: 0.0023589191858967144\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05660325016578038\n",
      "Average test loss: 0.002761383994172017\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05440364907847511\n",
      "Average test loss: 0.0020146366192234887\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05216736392511262\n",
      "Average test loss: 0.002400140885884563\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05038924879166815\n",
      "Average test loss: 0.0021170279439538717\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04866305947966046\n",
      "Average test loss: 0.0020175186917185785\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04359335993395912\n",
      "Average test loss: 0.0017090107349471914\n",
      "Epoch 26/300\n",
      "Average training loss: 0.042070195436477664\n",
      "Average test loss: 0.002093757073705395\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04070515585773521\n",
      "Average test loss: 0.0017769425174014436\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03681327920489841\n",
      "Average test loss: 0.0015369592389712732\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03601586407423019\n",
      "Average test loss: 0.0016763094904729062\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03616525550021066\n",
      "Average test loss: 0.0016506261738638082\n",
      "Epoch 34/300\n",
      "Average training loss: 0.034784178791774645\n",
      "Average test loss: 0.0015303980774349637\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03414913076659044\n",
      "Average test loss: 0.0015346943507384924\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0337678888026211\n",
      "Average test loss: 0.0014470652404965625\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03327136190235615\n",
      "Average test loss: 0.0014783429654522074\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03294900364345974\n",
      "Average test loss: 0.0014682864894469579\n",
      "Epoch 39/300\n",
      "Average training loss: 0.032524468107355965\n",
      "Average test loss: 0.0014672151561826467\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03231954448421796\n",
      "Average test loss: 0.001594287817262941\n",
      "Epoch 41/300\n",
      "Average training loss: 0.032466339487168525\n",
      "Average test loss: 0.0015184045036633809\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0316042860498031\n",
      "Average test loss: 0.0014486939735296699\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03148330145246453\n",
      "Average test loss: 0.001416026772569037\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03141301985416147\n",
      "Average test loss: 0.0023911502448221047\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03103047963645723\n",
      "Average test loss: 0.0013899679159124693\n",
      "Epoch 46/300\n",
      "Average training loss: 0.030660761641131507\n",
      "Average test loss: 0.0018041550954803825\n",
      "Epoch 47/300\n",
      "Average training loss: 0.030525598410103057\n",
      "Average test loss: 0.001400504749475254\n",
      "Epoch 48/300\n",
      "Average training loss: 0.030580053442054323\n",
      "Average test loss: 0.0014024636718548006\n",
      "Epoch 49/300\n",
      "Average training loss: 0.030201914995080896\n",
      "Average test loss: 0.0016112728643024132\n",
      "Epoch 50/300\n",
      "Average training loss: 0.030033782266908223\n",
      "Average test loss: 0.07925958110226525\n",
      "Epoch 51/300\n",
      "Average training loss: 0.029685998832186065\n",
      "Average test loss: 0.0014248886962110797\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02965805177556144\n",
      "Average test loss: 0.0013830835281146898\n",
      "Epoch 54/300\n",
      "Average training loss: 0.029343950796458455\n",
      "Average test loss: 0.0016082165102577872\n",
      "Epoch 55/300\n",
      "Average training loss: 0.029438966769311164\n",
      "Average test loss: 0.0013551957988076739\n",
      "Epoch 56/300\n",
      "Average training loss: 0.029218681850367124\n",
      "Average test loss: 0.0013504131278023123\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02900076240632269\n",
      "Average test loss: 0.001404840442765918\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02902821944653988\n",
      "Average test loss: 0.0014219140503555535\n",
      "Epoch 59/300\n",
      "Average training loss: 0.028917164886991184\n",
      "Average test loss: 0.001624478256329894\n",
      "Epoch 60/300\n",
      "Average training loss: 0.028847896670301756\n",
      "Average test loss: 0.0014658359293308523\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02879882996612125\n",
      "Average test loss: 0.00202426909075843\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02915767538216379\n",
      "Average test loss: 0.0015812162220374577\n",
      "Epoch 63/300\n",
      "Average training loss: 0.028452086533109346\n",
      "Average test loss: 0.0014471253813761805\n",
      "Epoch 64/300\n",
      "Average training loss: 0.028426461891995537\n",
      "Average test loss: 0.001882755292372571\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02843463096850448\n",
      "Average test loss: 0.0013365748592962822\n",
      "Epoch 66/300\n",
      "Average training loss: 0.028262495941585964\n",
      "Average test loss: 0.0013317495484112037\n",
      "Epoch 67/300\n",
      "Average training loss: 0.028477405387494298\n",
      "Average test loss: 0.001424063607222504\n",
      "Epoch 68/300\n",
      "Average training loss: 0.028129646743337313\n",
      "Average test loss: 0.001348769553626577\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02815472659965356\n",
      "Average test loss: 0.0018418067486749754\n",
      "Epoch 70/300\n",
      "Average training loss: 0.028025159860650697\n",
      "Average test loss: 0.0027253984208736154\n",
      "Epoch 71/300\n",
      "Average training loss: 0.028026111366020307\n",
      "Average test loss: 0.0013411094760522246\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02793274725642469\n",
      "Average test loss: 0.0013894513800947203\n",
      "Epoch 73/300\n",
      "Average training loss: 0.028044136078821287\n",
      "Average test loss: 0.04208174556990465\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02781739807128906\n",
      "Average test loss: 0.0013604505212149686\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02753322081433402\n",
      "Average test loss: 0.0015488688031004534\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02747146421339777\n",
      "Average test loss: 0.0013541741507748762\n",
      "Epoch 80/300\n",
      "Average training loss: 0.027430078193545343\n",
      "Average test loss: 0.0021641190997842286\n",
      "Epoch 81/300\n",
      "Average training loss: 0.027304226901796128\n",
      "Average test loss: 0.0013515865030801959\n",
      "Epoch 82/300\n",
      "Average training loss: 0.027349642808238664\n",
      "Average test loss: 0.0013359447782859207\n",
      "Epoch 83/300\n",
      "Average training loss: 0.027221098336908552\n",
      "Average test loss: 0.0015242652113859852\n",
      "Epoch 84/300\n",
      "Average training loss: 0.027259552697340646\n",
      "Average test loss: 0.001365585561738246\n",
      "Epoch 85/300\n",
      "Average training loss: 0.027204785502619212\n",
      "Average test loss: 0.0013653471557837393\n",
      "Epoch 86/300\n",
      "Average training loss: 0.027189010136657293\n",
      "Average test loss: 0.002405399271700945\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02719052200184928\n",
      "Average test loss: 0.0016018186267465354\n",
      "Epoch 88/300\n",
      "Average training loss: 0.027021209615800117\n",
      "Average test loss: 0.003791618932866388\n",
      "Epoch 89/300\n",
      "Average training loss: 0.027009349210394752\n",
      "Average test loss: 0.0015022953988777267\n",
      "Epoch 90/300\n",
      "Average training loss: 0.026918923844893774\n",
      "Average test loss: 0.0013572735309393868\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02681279409262869\n",
      "Average test loss: 0.0013729439765835802\n",
      "Epoch 92/300\n",
      "Average training loss: 0.026772673348585765\n",
      "Average test loss: 0.0016182148692508538\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02678836911585596\n",
      "Average test loss: 0.0014032422114784518\n",
      "Epoch 94/300\n",
      "Average training loss: 0.026733973629772662\n",
      "Average test loss: 0.011227819914619129\n",
      "Epoch 95/300\n",
      "Average training loss: 0.026632663157251147\n",
      "Average test loss: 0.0018806661684066057\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02668069198810392\n",
      "Average test loss: 0.0013920853650197388\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02665894733203782\n",
      "Average test loss: 0.0016043494852880638\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026609005232652028\n",
      "Average test loss: 0.001360443729079432\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02637440951499674\n",
      "Average test loss: 0.0013600446054091056\n",
      "Epoch 102/300\n",
      "Average training loss: 0.026371351707312796\n",
      "Average test loss: 0.0014396082563325762\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02644450664189127\n",
      "Average test loss: 0.001665044922899041\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026316190590461096\n",
      "Average test loss: 0.0013966232196738323\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026297198154860073\n",
      "Average test loss: 0.0013795243315398694\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02622469990948836\n",
      "Average test loss: 0.0013752895090728997\n",
      "Epoch 107/300\n",
      "Average training loss: 0.026273820375402768\n",
      "Average test loss: 0.0015631581400003698\n",
      "Epoch 108/300\n",
      "Average training loss: 0.026233164676361612\n",
      "Average test loss: 0.0014805340243296491\n",
      "Epoch 109/300\n",
      "Average training loss: 0.026086285702056355\n",
      "Average test loss: 0.0014659784134063456\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02607197413345178\n",
      "Average test loss: 0.0014130602109556397\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02615215858651532\n",
      "Average test loss: 0.001359139619809058\n",
      "Epoch 112/300\n",
      "Average training loss: 0.026097281384799217\n",
      "Average test loss: 0.0013951738817203376\n",
      "Epoch 113/300\n",
      "Average training loss: 0.026013803063167466\n",
      "Average test loss: 0.002206135361972782\n",
      "Epoch 114/300\n",
      "Average training loss: 0.026053557104534573\n",
      "Average test loss: 0.0013557510597424374\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0259287282427152\n",
      "Average test loss: 0.0014082059322132006\n",
      "Epoch 116/300\n",
      "Average training loss: 0.025916731362541516\n",
      "Average test loss: 0.0047374612101250225\n",
      "Epoch 117/300\n",
      "Average training loss: 0.026172598504357866\n",
      "Average test loss: 0.001541470351628959\n",
      "Epoch 118/300\n",
      "Average training loss: 0.025854620748096042\n",
      "Average test loss: 0.0014470626077511244\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025887202031082576\n",
      "Average test loss: 0.0014181804133372175\n",
      "Epoch 120/300\n",
      "Average training loss: 0.025873624639378652\n",
      "Average test loss: 0.001392191253705985\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025882249198026126\n",
      "Average test loss: 0.0013967562423398098\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02575307605995072\n",
      "Average test loss: 0.0015194650534540415\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02566743719577789\n",
      "Average test loss: 0.0017937865618409383\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02569359844095177\n",
      "Average test loss: 0.0016416443375249705\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025777600235409207\n",
      "Average test loss: 0.0015588390479485193\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025624658630953895\n",
      "Average test loss: 0.0015675154733988974\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02562634076178074\n",
      "Average test loss: 0.0014159499127935204\n",
      "Epoch 130/300\n",
      "Average training loss: 0.025537701406412653\n",
      "Average test loss: 0.0013956757875987224\n",
      "Epoch 131/300\n",
      "Average training loss: 0.025443457717696825\n",
      "Average test loss: 0.0014430213704084358\n",
      "Epoch 135/300\n",
      "Average training loss: 0.025501001535190478\n",
      "Average test loss: 0.0020913382662046285\n",
      "Epoch 136/300\n",
      "Average training loss: 0.025424957601560487\n",
      "Average test loss: 0.00250303350119955\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02546491864654753\n",
      "Average test loss: 0.0014040362137990694\n",
      "Epoch 138/300\n",
      "Average training loss: 0.025448944355050723\n",
      "Average test loss: 0.0014375480374114382\n",
      "Epoch 139/300\n",
      "Average training loss: 0.025293578480680784\n",
      "Average test loss: 0.001403280821421908\n",
      "Epoch 140/300\n",
      "Average training loss: 0.025347601362400586\n",
      "Average test loss: 0.0013818447237006492\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025372398437725173\n",
      "Average test loss: 0.0015987935207991137\n",
      "Epoch 142/300\n",
      "Average training loss: 0.025247898961106935\n",
      "Average test loss: 0.0013832382193456093\n",
      "Epoch 143/300\n",
      "Average training loss: 0.025400772896077898\n",
      "Average test loss: 0.001534970980654988\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02526011396944523\n",
      "Average test loss: 0.0014136120426572031\n",
      "Epoch 145/300\n",
      "Average training loss: 0.025420057601398892\n",
      "Average test loss: 0.0015274039998960992\n",
      "Epoch 146/300\n",
      "Average training loss: 0.025155684201253786\n",
      "Average test loss: 0.0013942959144090613\n",
      "Epoch 147/300\n",
      "Average training loss: 0.025159978326823977\n",
      "Average test loss: 0.001383774575777352\n",
      "Epoch 148/300\n",
      "Average training loss: 0.025351042320330936\n",
      "Average test loss: 0.001632092474028468\n",
      "Epoch 149/300\n",
      "Average training loss: 0.025085635483264923\n",
      "Average test loss: 0.0015877853278070688\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02518150561220116\n",
      "Average test loss: 0.0014598025863265826\n",
      "Epoch 151/300\n",
      "Average training loss: 0.025342619109484883\n",
      "Average test loss: 0.0013930564898376665\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02507792491051886\n",
      "Average test loss: 0.0014717959933396843\n",
      "Epoch 153/300\n",
      "Average training loss: 0.025040803169210753\n",
      "Average test loss: 0.0016913515862284436\n",
      "Epoch 154/300\n",
      "Average training loss: 0.0250093697739972\n",
      "Average test loss: 0.0014432292645797133\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02517651851144102\n",
      "Average test loss: 0.006476982230941454\n",
      "Epoch 156/300\n",
      "Average training loss: 0.025066023169292344\n",
      "Average test loss: 0.0014805130835415588\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02497146300309234\n",
      "Average test loss: 0.001483504145147486\n",
      "Epoch 158/300\n",
      "Average training loss: 0.025031159571475455\n",
      "Average test loss: 0.0014558500771721203\n",
      "Epoch 159/300\n",
      "Average training loss: 0.025008067432377074\n",
      "Average test loss: 0.001415914792360531\n",
      "Epoch 160/300\n",
      "Average training loss: 0.024953082419104045\n",
      "Average test loss: 0.0014422761795835363\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02498826599617799\n",
      "Average test loss: 0.0028158860090706085\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02483535766767131\n",
      "Average test loss: 0.0014621286487931178\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02484500405854649\n",
      "Average test loss: 0.0014283980017320977\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024982381496164534\n",
      "Average test loss: 0.0014432366262707445\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024792091538508733\n",
      "Average test loss: 0.004264619555738237\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024858924865722658\n",
      "Average test loss: 0.0021701742781119215\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02483545507821772\n",
      "Average test loss: 0.0016277690051744382\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02480334957440694\n",
      "Average test loss: 0.001748060994885034\n",
      "Epoch 171/300\n",
      "Average training loss: 0.024835478466418055\n",
      "Average test loss: 0.0015333067434322503\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024797532012065252\n",
      "Average test loss: 0.0016425138039307462\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024755067978468208\n",
      "Average test loss: 0.001435213195780913\n",
      "Epoch 174/300\n",
      "Average training loss: 0.024726161448491945\n",
      "Average test loss: 0.0017306460580892033\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024685203456216387\n",
      "Average test loss: 0.0014594174856320025\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024721598878502844\n",
      "Average test loss: 0.0014357013611329926\n",
      "Epoch 177/300\n",
      "Average training loss: 0.024729557833737797\n",
      "Average test loss: 0.001415455562269522\n",
      "Epoch 178/300\n",
      "Average training loss: 0.024671726183758843\n",
      "Average test loss: 0.0014762910989423593\n",
      "Epoch 179/300\n",
      "Average training loss: 0.024588322156005437\n",
      "Average test loss: 0.0014067480895254348\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02480946526924769\n",
      "Average test loss: 0.0014157016991327206\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024612024087044927\n",
      "Average test loss: 0.001618477738617609\n",
      "Epoch 182/300\n",
      "Average training loss: 0.024977134987711907\n",
      "Average test loss: 0.001531183046185308\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024527211523718305\n",
      "Average test loss: 0.0019494605459686783\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02463354012204541\n",
      "Average test loss: 0.0015403845321594013\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02473140833940771\n",
      "Average test loss: 0.0038183871408303577\n",
      "Epoch 186/300\n",
      "Average training loss: 0.024605354823999934\n",
      "Average test loss: 0.001467955705916716\n",
      "Epoch 187/300\n",
      "Average training loss: 0.024534775621361204\n",
      "Average test loss: 0.001645941505001651\n",
      "Epoch 188/300\n",
      "Average training loss: 0.024559172766076193\n",
      "Average test loss: 0.001422487417442931\n",
      "Epoch 189/300\n",
      "Average training loss: 0.024560301570428742\n",
      "Average test loss: 0.0017403350590417782\n",
      "Epoch 190/300\n",
      "Average training loss: 0.024493994855218464\n",
      "Average test loss: 0.0014504330014396044\n",
      "Epoch 191/300\n",
      "Average training loss: 0.024479060464435154\n",
      "Average test loss: 0.0018730724537745118\n",
      "Epoch 192/300\n",
      "Average training loss: 0.024573849256667824\n",
      "Average test loss: 0.17051038149992626\n",
      "Epoch 193/300\n",
      "Average training loss: 0.024572402238845825\n",
      "Average test loss: 0.0014418954727136426\n",
      "Epoch 194/300\n",
      "Average training loss: 0.024499799660510486\n",
      "Average test loss: 0.0015062677243517504\n",
      "Epoch 195/300\n",
      "Average training loss: 0.024488425359129904\n",
      "Average test loss: 0.0018529773144465354\n",
      "Epoch 196/300\n",
      "Average training loss: 0.024423340337144005\n",
      "Average test loss: 0.0015577855027384228\n",
      "Epoch 197/300\n",
      "Average training loss: 0.024522329035732482\n",
      "Average test loss: 0.0014362168879144721\n",
      "Epoch 198/300\n",
      "Average training loss: 0.024523962744408184\n",
      "Average test loss: 0.0015782929389841027\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02433673980500963\n",
      "Average test loss: 0.0016824774776274959\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02438600255217817\n",
      "Average test loss: 0.0015668039440901744\n",
      "Epoch 201/300\n",
      "Average training loss: 0.024393291387293074\n",
      "Average test loss: 0.0014469782687309715\n",
      "Epoch 202/300\n",
      "Average training loss: 0.024373901360564762\n",
      "Average test loss: 0.0014729105911941993\n",
      "Epoch 203/300\n",
      "Average training loss: 0.024400206269489393\n",
      "Average test loss: 0.0015836298345691627\n",
      "Epoch 204/300\n",
      "Average training loss: 0.024351520069771343\n",
      "Average test loss: 0.0018551747329119178\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02427759902841515\n",
      "Average test loss: 0.0014612010744296843\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02445417534477181\n",
      "Average test loss: 0.0014613176585278576\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02441024777127637\n",
      "Average test loss: 0.0015034628481500679\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02421893634564347\n",
      "Average test loss: 0.00295544222328398\n",
      "Epoch 209/300\n",
      "Average training loss: 0.024439819544553758\n",
      "Average test loss: 0.0022279737095038097\n",
      "Epoch 210/300\n",
      "Average training loss: 0.024259389630622335\n",
      "Average test loss: 0.0014832639939462145\n",
      "Epoch 211/300\n",
      "Average training loss: 0.024251878591047393\n",
      "Average test loss: 0.0015929176033371024\n",
      "Epoch 212/300\n",
      "Average training loss: 0.024325321283605365\n",
      "Average test loss: 0.0044426738081706895\n",
      "Epoch 213/300\n",
      "Average training loss: 0.024197774784432517\n",
      "Average test loss: 0.006137218284110228\n",
      "Epoch 214/300\n",
      "Average training loss: 0.024404628483785523\n",
      "Average test loss: 0.001502507493003375\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02419345333178838\n",
      "Average test loss: 0.001508483253315919\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02420298855834537\n",
      "Average test loss: 0.001595899944866283\n",
      "Epoch 217/300\n",
      "Average training loss: 0.024313214745786457\n",
      "Average test loss: 0.00361548527268072\n",
      "Epoch 218/300\n",
      "Average training loss: 0.024144878213604292\n",
      "Average test loss: 0.0014833548605545528\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02426028362247679\n",
      "Average test loss: 0.0014798663864947028\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02412599267396662\n",
      "Average test loss: 0.0014333312345875635\n",
      "Epoch 221/300\n",
      "Average training loss: 0.024358151422606573\n",
      "Average test loss: 0.0017344395463458366\n",
      "Epoch 222/300\n",
      "Average training loss: 0.024296272474858497\n",
      "Average test loss: 0.0014441308109089733\n",
      "Epoch 223/300\n",
      "Average training loss: 0.024106391959720187\n",
      "Average test loss: 0.0017012012055557634\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02418672817862696\n",
      "Average test loss: 0.0024292054733054506\n",
      "Epoch 225/300\n",
      "Average training loss: 0.024107508353061146\n",
      "Average test loss: 1.3271876424683464\n",
      "Epoch 226/300\n",
      "Average training loss: 0.024154708837469417\n",
      "Average test loss: 0.0015134341427021556\n",
      "Epoch 227/300\n",
      "Average training loss: 0.024186926565236515\n",
      "Average test loss: 0.0015616569152722755\n",
      "Epoch 228/300\n",
      "Average training loss: 0.024060948509309028\n",
      "Average test loss: 0.0015859172728119625\n",
      "Epoch 229/300\n",
      "Average training loss: 0.024079089610113038\n",
      "Average test loss: 0.001463344106864598\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02406117418905099\n",
      "Average test loss: 0.00416964615508914\n",
      "Epoch 231/300\n",
      "Average training loss: 0.025095584536592167\n",
      "Average test loss: 0.0018772255507194333\n",
      "Epoch 232/300\n",
      "Average training loss: 0.024737034685081906\n",
      "Average test loss: 0.0015141537779321274\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02398065087116427\n",
      "Average test loss: 0.0015092237883040475\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023892382353544236\n",
      "Average test loss: 0.0015144784833408065\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02395608342025015\n",
      "Average test loss: 0.0023732100440603163\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02435213663180669\n",
      "Average test loss: 0.0017949502021074296\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02396369074119462\n",
      "Average test loss: 0.0014948403810461362\n",
      "Epoch 238/300\n",
      "Average training loss: 0.023981967288586828\n",
      "Average test loss: 0.0014687154450350338\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023932562000221678\n",
      "Average test loss: 0.0015530471882472436\n",
      "Epoch 240/300\n",
      "Average training loss: 0.024028418143590292\n",
      "Average test loss: 0.0018311493413315878\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02393685304787424\n",
      "Average test loss: 0.00183490044489089\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02396078638318512\n",
      "Average test loss: 0.0015711473212060001\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02400784681406286\n",
      "Average test loss: 0.0015940852982716427\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02396972439189752\n",
      "Average test loss: 0.0014522769886793363\n",
      "Epoch 245/300\n",
      "Average training loss: 0.024009963570369615\n",
      "Average test loss: 0.0015048018337951766\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0239555668251382\n",
      "Average test loss: 0.0014937217080344756\n",
      "Epoch 247/300\n",
      "Average training loss: 0.024280157973368962\n",
      "Average test loss: 0.0023863649047497246\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023931627253691357\n",
      "Average test loss: 0.0020440404071576067\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023865225755506092\n",
      "Average test loss: 0.0014349870160222052\n",
      "Epoch 250/300\n",
      "Average training loss: 0.023898967552516196\n",
      "Average test loss: 0.0014647965378438432\n",
      "Epoch 251/300\n",
      "Average training loss: 0.023951103193892372\n",
      "Average test loss: 0.00156048317750295\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023890768892235226\n",
      "Average test loss: 0.0018531564750398199\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023873533051874903\n",
      "Average test loss: 0.001437171055831843\n",
      "Epoch 254/300\n",
      "Average training loss: 0.023867035577694574\n",
      "Average test loss: 0.00480191070958972\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023932313251826497\n",
      "Average test loss: 0.011326644291480382\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023873279339737364\n",
      "Average test loss: 0.001484244384906358\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02379290679593881\n",
      "Average test loss: 0.0016088994807667203\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02384153120385276\n",
      "Average test loss: 0.0014578275779365665\n",
      "Epoch 259/300\n",
      "Average training loss: 0.023829576777087316\n",
      "Average test loss: 0.0015308210998773574\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023834713091452915\n",
      "Average test loss: 0.0015736730046984222\n",
      "Epoch 261/300\n",
      "Average training loss: 0.023895230415794586\n",
      "Average test loss: 0.0017549860113196902\n",
      "Epoch 262/300\n",
      "Average training loss: 0.023844235585795508\n",
      "Average test loss: 0.001553232072904292\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023833127718832756\n",
      "Average test loss: 0.0014755921223097378\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023762952266467942\n",
      "Average test loss: 0.027667393630577457\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023777246009144517\n",
      "Average test loss: 0.0015752029218193558\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023838069159123634\n",
      "Average test loss: 0.0015327571309689018\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023739023104310034\n",
      "Average test loss: 0.001483470819114397\n",
      "Epoch 268/300\n",
      "Average training loss: 0.023771475791931153\n",
      "Average test loss: 0.0034480970543291833\n",
      "Epoch 269/300\n",
      "Average training loss: 0.024469317618343565\n",
      "Average test loss: 0.0014806439305345216\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02367743982374668\n",
      "Average test loss: 0.0017786815656969945\n",
      "Epoch 271/300\n",
      "Average training loss: 0.023719975594017242\n",
      "Average test loss: 0.0015869824053305719\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02373149865369002\n",
      "Average test loss: 0.0016233493781959017\n",
      "Epoch 273/300\n",
      "Average training loss: 0.023798213011688656\n",
      "Average test loss: 0.0014995456455896298\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023708746822343932\n",
      "Average test loss: 0.001521576057188213\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023780507114198473\n",
      "Average test loss: 0.0014921940850714842\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02402058111793465\n",
      "Average test loss: 0.001496293613066276\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023618075084355143\n",
      "Average test loss: 0.0014734726165317826\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02368467257089085\n",
      "Average test loss: 0.0014995572494549885\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023729194328188895\n",
      "Average test loss: 0.005226484786719084\n",
      "Epoch 280/300\n",
      "Average training loss: 0.023626834420694246\n",
      "Average test loss: 0.0014749930613777705\n",
      "Epoch 281/300\n",
      "Average training loss: 0.024889227802554766\n",
      "Average test loss: 0.0014564969026380115\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023599607611695925\n",
      "Average test loss: 0.0014764027725387779\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02360894121395217\n",
      "Average test loss: 0.0015658898913922408\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02360699854956733\n",
      "Average test loss: 0.0014809932694252995\n",
      "Epoch 285/300\n",
      "Average training loss: 0.023606460210349824\n",
      "Average test loss: 0.0014880456486716866\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02364039867785242\n",
      "Average test loss: 0.0015515465198291673\n",
      "Epoch 287/300\n",
      "Average training loss: 0.023633186345299085\n",
      "Average test loss: 0.0014871270851128631\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02379932016796536\n",
      "Average test loss: 0.0015365561809804705\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0236429254098071\n",
      "Average test loss: 0.00149001321165512\n",
      "Epoch 290/300\n",
      "Average training loss: 0.023581514411502413\n",
      "Average test loss: 0.0035594657787846196\n",
      "Epoch 291/300\n",
      "Average training loss: 0.023627229852808847\n",
      "Average test loss: 0.0017646249374374747\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0236501335269875\n",
      "Average test loss: 0.0030720293993751206\n",
      "Epoch 293/300\n",
      "Average training loss: 0.023585129231214523\n",
      "Average test loss: 0.0015397837043015492\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02363717241750823\n",
      "Average test loss: 0.001483576743139161\n",
      "Epoch 295/300\n",
      "Average training loss: 0.023561269498533672\n",
      "Average test loss: 0.0015181139367115167\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02366729275054402\n",
      "Average test loss: 0.005871248038278686\n",
      "Epoch 297/300\n",
      "Average training loss: 0.023560619605912103\n",
      "Average test loss: 0.0015916802084797786\n",
      "Epoch 298/300\n",
      "Average training loss: 0.023718326525555715\n",
      "Average test loss: 0.001478540701377723\n",
      "Epoch 299/300\n",
      "Average training loss: 0.023631486487057474\n",
      "Average test loss: 0.0014796081866241164\n",
      "Epoch 300/300\n",
      "Average training loss: 0.023521012547943327\n",
      "Average test loss: 0.0016600040400193798\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive-.01/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.07\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.85\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.30\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.75\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.96\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.10\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.33\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.62\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.26\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.97\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.80\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.00\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.65\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.98\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.32\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.50\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.55\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.86\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.34\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.71\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.98\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.25\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.42\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.17\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.50\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.63\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.81\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.84\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 15.602181479560004\n",
      "Average test loss: 0.013320453308522702\n",
      "Epoch 2/300\n",
      "Average training loss: 8.44936319012112\n",
      "Average test loss: 0.011094846439030435\n",
      "Epoch 3/300\n",
      "Average training loss: 6.740153477138943\n",
      "Average test loss: 0.009566486915780438\n",
      "Epoch 4/300\n",
      "Average training loss: 6.114902653164334\n",
      "Average test loss: 0.009496295360227426\n",
      "Epoch 5/300\n",
      "Average training loss: 5.184983698527018\n",
      "Average test loss: 0.008376583737631638\n",
      "Epoch 6/300\n",
      "Average training loss: 4.529090869055854\n",
      "Average test loss: 0.008083518678115473\n",
      "Epoch 7/300\n",
      "Average training loss: 3.971942061530219\n",
      "Average test loss: 0.008717528175976541\n",
      "Epoch 8/300\n",
      "Average training loss: 3.573058843400743\n",
      "Average test loss: 0.007517120502268274\n",
      "Epoch 9/300\n",
      "Average training loss: 3.111204784605238\n",
      "Average test loss: 0.00782040534541011\n",
      "Epoch 10/300\n",
      "Average training loss: 2.6477419077555338\n",
      "Average test loss: 0.007854613188654184\n",
      "Epoch 11/300\n",
      "Average training loss: 2.3349574921925864\n",
      "Average test loss: 0.00740663097343511\n",
      "Epoch 12/300\n",
      "Average training loss: 2.0860824138853284\n",
      "Average test loss: 0.006894510217838817\n",
      "Epoch 13/300\n",
      "Average training loss: 1.8292795761956109\n",
      "Average test loss: 0.0074029194712638855\n",
      "Epoch 14/300\n",
      "Average training loss: 1.5458636504279242\n",
      "Average test loss: 0.007750621702108118\n",
      "Epoch 15/300\n",
      "Average training loss: 1.3104396343231202\n",
      "Average test loss: 0.0068117227264576485\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1380976696014404\n",
      "Average test loss: 0.007167800239390797\n",
      "Epoch 17/300\n",
      "Average training loss: 0.9800230497254265\n",
      "Average test loss: 0.006590303642882241\n",
      "Epoch 18/300\n",
      "Average training loss: 0.7375558332337273\n",
      "Average test loss: 0.009798004567623139\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6454853377342225\n",
      "Average test loss: 0.0077325977318816715\n",
      "Epoch 21/300\n",
      "Average training loss: 0.575134613249037\n",
      "Average test loss: 0.007273183117310206\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5175362743271722\n",
      "Average test loss: 0.005882074704186784\n",
      "Epoch 23/300\n",
      "Average training loss: 0.47041262653138904\n",
      "Average test loss: 0.01188783593227466\n",
      "Epoch 24/300\n",
      "Average training loss: 0.37106214865048726\n",
      "Average test loss: 0.010422099211149746\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3450576255056593\n",
      "Average test loss: 0.007240631202028857\n",
      "Epoch 28/300\n",
      "Average training loss: 0.32380452638202245\n",
      "Average test loss: 0.00578593120806747\n",
      "Epoch 29/300\n",
      "Average training loss: 0.3055721139245563\n",
      "Average test loss: 0.005484882553832398\n",
      "Epoch 30/300\n",
      "Average training loss: 0.2915769760873583\n",
      "Average test loss: 0.005690886923422416\n",
      "Epoch 31/300\n",
      "Average training loss: 0.27948684122827316\n",
      "Average test loss: 0.0060965761285689144\n",
      "Epoch 32/300\n",
      "Average training loss: 0.26981192011303373\n",
      "Average test loss: 0.0055692046201891365\n",
      "Epoch 33/300\n",
      "Average training loss: 0.25907562189631994\n",
      "Average test loss: 0.0055101101944843925\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2506831939087974\n",
      "Average test loss: 0.007196049175328678\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2415954433017307\n",
      "Average test loss: 0.005365297401116954\n",
      "Epoch 36/300\n",
      "Average training loss: 0.23560286147064632\n",
      "Average test loss: 0.005304985096057256\n",
      "Epoch 37/300\n",
      "Average training loss: 0.21988372896777258\n",
      "Average test loss: 0.005570139770706494\n",
      "Epoch 40/300\n",
      "Average training loss: 0.21562052687009176\n",
      "Average test loss: 0.005468101747747925\n",
      "Epoch 41/300\n",
      "Average training loss: 0.2111068119208018\n",
      "Average test loss: 0.007231273110128111\n",
      "Epoch 42/300\n",
      "Average training loss: 0.2077607446908951\n",
      "Average test loss: 0.005272800783937176\n",
      "Epoch 43/300\n",
      "Average training loss: 0.20239926404423184\n",
      "Average test loss: 0.006789702595108085\n",
      "Epoch 44/300\n",
      "Average training loss: 0.19920373747083875\n",
      "Average test loss: 13210.62507638889\n",
      "Epoch 45/300\n",
      "Average training loss: 0.19683842228518592\n",
      "Average test loss: 0.007065100830462244\n",
      "Epoch 46/300\n",
      "Average training loss: 0.19364504109488592\n",
      "Average test loss: 0.005198020019878943\n",
      "Epoch 47/300\n",
      "Average training loss: 0.19103145156966314\n",
      "Average test loss: 0.005333287400090032\n",
      "Epoch 48/300\n",
      "Average training loss: 0.18979144758648342\n",
      "Average test loss: 0.005225500411871407\n",
      "Epoch 49/300\n",
      "Average training loss: 0.18710432660579682\n",
      "Average test loss: 0.5058186820215649\n",
      "Epoch 50/300\n",
      "Average training loss: 0.18491673168871137\n",
      "Average test loss: 0.005426751210457749\n",
      "Epoch 51/300\n",
      "Average training loss: 0.18329724301894507\n",
      "Average test loss: 0.005136430305325322\n",
      "Epoch 52/300\n",
      "Average training loss: 0.181024269051022\n",
      "Average test loss: 0.005579218021697468\n",
      "Epoch 53/300\n",
      "Average training loss: 0.18047501544157665\n",
      "Average test loss: 0.005296351555320952\n",
      "Epoch 54/300\n",
      "Average training loss: 0.17846663574377697\n",
      "Average test loss: 0.005151899000836744\n",
      "Epoch 55/300\n",
      "Average training loss: 0.17733394765853883\n",
      "Average test loss: 0.005317607508765327\n",
      "Epoch 56/300\n",
      "Average training loss: 0.17543138488796023\n",
      "Average test loss: 0.005439103026770883\n",
      "Epoch 57/300\n",
      "Average training loss: 0.17460019754038916\n",
      "Average test loss: 0.005047286909487512\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1729433704217275\n",
      "Average test loss: 0.005158529404550791\n",
      "Epoch 59/300\n",
      "Average training loss: 0.17246081427733104\n",
      "Average test loss: 0.005060741192350785\n",
      "Epoch 60/300\n",
      "Average training loss: 0.17283258177174463\n",
      "Average test loss: 0.006318253650433488\n",
      "Epoch 61/300\n",
      "Average training loss: 0.16998904747433133\n",
      "Average test loss: 0.005152851146128443\n",
      "Epoch 62/300\n",
      "Average training loss: 0.16831072586774826\n",
      "Average test loss: 0.0050639714404112765\n",
      "Epoch 63/300\n",
      "Average training loss: 0.16766570377349854\n",
      "Average test loss: 0.023076915726065635\n",
      "Epoch 64/300\n",
      "Average training loss: 0.16722750858465832\n",
      "Average test loss: 0.005094535800731844\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1659664782418145\n",
      "Average test loss: 0.005248146228906181\n",
      "Epoch 66/300\n",
      "Average training loss: 0.16551115798950194\n",
      "Average test loss: 0.0051519076950434185\n",
      "Epoch 67/300\n",
      "Average training loss: 0.16360486861069998\n",
      "Average test loss: 0.00545423252777093\n",
      "Epoch 68/300\n",
      "Average training loss: 0.16401792497767342\n",
      "Average test loss: 0.005302845841480626\n",
      "Epoch 69/300\n",
      "Average training loss: 0.16393233895301818\n",
      "Average test loss: 0.015039090712865194\n",
      "Epoch 70/300\n",
      "Average training loss: 0.16076497099134657\n",
      "Average test loss: 0.005118315752595663\n",
      "Epoch 71/300\n",
      "Average training loss: 0.16116609059439765\n",
      "Average test loss: 0.005157209071434206\n",
      "Epoch 72/300\n",
      "Average training loss: 0.16028548542658488\n",
      "Average test loss: 0.00536543506797817\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1611720563173294\n",
      "Average test loss: 0.0053878698733945685\n",
      "Epoch 74/300\n",
      "Average training loss: 0.15913473047150506\n",
      "Average test loss: 0.0050920034479349854\n",
      "Epoch 75/300\n",
      "Average training loss: 0.15857165410783555\n",
      "Average test loss: 0.005019717217733462\n",
      "Epoch 76/300\n",
      "Average training loss: 0.15801913758118946\n",
      "Average test loss: 0.005054629721575313\n",
      "Epoch 77/300\n",
      "Average training loss: 0.15778012451198367\n",
      "Average test loss: 0.005182885738710562\n",
      "Epoch 78/300\n",
      "Average training loss: 0.15680584296915268\n",
      "Average test loss: 0.005152459633019235\n",
      "Epoch 79/300\n",
      "Average training loss: 0.15562855451636845\n",
      "Average test loss: 0.11573376641008588\n",
      "Epoch 80/300\n",
      "Average training loss: 0.15593627087275186\n",
      "Average test loss: 0.005146205271283785\n",
      "Epoch 81/300\n",
      "Average training loss: 0.15515488702721067\n",
      "Average test loss: 0.00527501264317996\n",
      "Epoch 82/300\n",
      "Average training loss: 0.15525021249718135\n",
      "Average test loss: 0.011963256560679938\n",
      "Epoch 83/300\n",
      "Average test loss: 0.005604630750086572\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1531138128042221\n",
      "Average test loss: 0.0052409598860475755\n",
      "Epoch 85/300\n",
      "Average training loss: 0.17408620394600763\n",
      "Average test loss: 0.005369434787167443\n",
      "Epoch 86/300\n",
      "Average training loss: 0.15789383753140768\n",
      "Average test loss: 0.005103062092843983\n",
      "Epoch 87/300\n",
      "Average training loss: 0.15358891876538594\n",
      "Average test loss: 0.005127756859279341\n",
      "Epoch 88/300\n",
      "Average training loss: 0.1525117239157359\n",
      "Average test loss: 0.0057971437387168406\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1518645560476515\n",
      "Average test loss: 0.00509964251352681\n",
      "Epoch 90/300\n",
      "Average training loss: 0.15153183444341023\n",
      "Average test loss: 0.005162097039322059\n",
      "Epoch 91/300\n",
      "Average training loss: 0.1512999098830753\n",
      "Average test loss: 0.005287495363503695\n",
      "Epoch 92/300\n",
      "Average training loss: 0.15021292487780252\n",
      "Average test loss: 0.005354710856245623\n",
      "Epoch 93/300\n",
      "Average training loss: 0.15015249400668673\n",
      "Average test loss: 0.005172270067036152\n",
      "Epoch 94/300\n",
      "Average training loss: 0.1498673121266895\n",
      "Average test loss: 0.005184780864251984\n",
      "Epoch 95/300\n",
      "Average training loss: 0.14949280921618144\n",
      "Average test loss: 0.00563504641254743\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1487782127128707\n",
      "Average test loss: 0.0057731405368281734\n",
      "Epoch 97/300\n",
      "Average training loss: 12430489.37485002\n",
      "Average test loss: 1182.6144699615902\n",
      "Epoch 98/300\n",
      "Average training loss: 20.309498089260526\n",
      "Average test loss: 201.07652661355337\n",
      "Epoch 99/300\n",
      "Average training loss: 18.072213048299155\n",
      "Average test loss: 25398.306580427383\n",
      "Epoch 100/300\n",
      "Average training loss: 16.756426242404515\n",
      "Average test loss: 8250.882728241973\n",
      "Epoch 101/300\n",
      "Average training loss: 15.77846482594808\n",
      "Average test loss: 9.29031996650166\n",
      "Epoch 102/300\n",
      "Average training loss: 14.886661172654893\n",
      "Average test loss: 18.097706679940224\n",
      "Epoch 103/300\n",
      "Average training loss: 14.134212885538737\n",
      "Average test loss: 766.414430909925\n",
      "Epoch 104/300\n",
      "Average training loss: 13.54458854760064\n",
      "Average test loss: 1063.9852156268755\n",
      "Epoch 105/300\n",
      "Average training loss: 12.918638935512966\n",
      "Average test loss: 0.03838905687133471\n",
      "Epoch 106/300\n",
      "Average training loss: 12.516063262939452\n",
      "Average test loss: 0.024065069026417203\n",
      "Epoch 107/300\n",
      "Average training loss: 12.064252629597982\n",
      "Average test loss: 0.156161852783627\n",
      "Epoch 108/300\n",
      "Average training loss: 11.666251078287761\n",
      "Average test loss: 0.9282838669915994\n",
      "Epoch 109/300\n",
      "Average training loss: 11.325599537319608\n",
      "Average test loss: 47.34003152359856\n",
      "Epoch 110/300\n",
      "Average training loss: 10.954196498446994\n",
      "Average test loss: 52.154946597231756\n",
      "Epoch 111/300\n",
      "Average training loss: 10.663921037462023\n",
      "Average test loss: 19.666560484223897\n",
      "Epoch 112/300\n",
      "Average training loss: 10.342012890285917\n",
      "Average test loss: 0.05917612865649992\n",
      "Epoch 113/300\n",
      "Average training loss: 10.01124262237549\n",
      "Average test loss: 1397.541567545573\n",
      "Epoch 114/300\n",
      "Average training loss: 9.688843204922147\n",
      "Average test loss: 0.7045759117205938\n",
      "Epoch 115/300\n",
      "Average training loss: 9.397801434834799\n",
      "Average test loss: 0.010701497936414348\n",
      "Epoch 116/300\n",
      "Average training loss: 9.090428968641493\n",
      "Average test loss: 0.009209673085974323\n",
      "Epoch 117/300\n",
      "Average training loss: 8.838753966437446\n",
      "Average test loss: 0.0180607946332958\n",
      "Epoch 118/300\n",
      "Average training loss: 8.576334170871311\n",
      "Average test loss: 0.06643505368671483\n",
      "Epoch 119/300\n",
      "Average training loss: 8.351618070814345\n",
      "Average test loss: 0.0823904691628284\n",
      "Epoch 120/300\n",
      "Average training loss: 8.094148712158203\n",
      "Average test loss: 0.020226812892489964\n",
      "Epoch 121/300\n",
      "Average training loss: 7.85684619140625\n",
      "Average test loss: 0.7726660886274443\n",
      "Epoch 122/300\n",
      "Average training loss: 7.619931304931641\n",
      "Average test loss: 0.3393224024309052\n",
      "Epoch 123/300\n",
      "Average training loss: 7.371796704610189\n",
      "Average test loss: 0.010784828931093216\n",
      "Epoch 124/300\n",
      "Average training loss: 7.123196261511908\n",
      "Average test loss: 0.007935129707886113\n",
      "Epoch 125/300\n",
      "Average training loss: 6.861042386372884\n",
      "Average test loss: 0.007335685351656543\n",
      "Epoch 126/300\n",
      "Average training loss: 6.585802201588948\n",
      "Average test loss: 0.007590020934326781\n",
      "Epoch 127/300\n",
      "Average training loss: 6.321096820831299\n",
      "Average test loss: 0.012904739285922713\n",
      "Epoch 128/300\n",
      "Average training loss: 6.023334767659505\n",
      "Average test loss: 0.006617368157539103\n",
      "Epoch 129/300\n",
      "Average training loss: 5.732738864474826\n",
      "Average test loss: 0.07090504818161329\n",
      "Epoch 130/300\n",
      "Average training loss: 5.461617524464925\n",
      "Average test loss: 0.006373274461676677\n",
      "Epoch 131/300\n",
      "Average training loss: 5.169990928649902\n",
      "Average test loss: 0.00630809276840753\n",
      "Epoch 132/300\n",
      "Average training loss: 4.908242888980442\n",
      "Average test loss: 0.216207248830133\n",
      "Epoch 133/300\n",
      "Average training loss: 4.6453913370768225\n",
      "Average test loss: 0.00603915403659145\n",
      "Epoch 134/300\n",
      "Average training loss: 4.401898801167806\n",
      "Average test loss: 0.006060108861989445\n",
      "Epoch 135/300\n",
      "Average training loss: 4.1690496516757545\n",
      "Average test loss: 0.006184463911586338\n",
      "Epoch 136/300\n",
      "Average training loss: 3.9355576593610975\n",
      "Average test loss: 0.005840040205667417\n",
      "Epoch 137/300\n",
      "Average training loss: 3.748059796863132\n",
      "Average test loss: 0.005764556104938189\n",
      "Epoch 138/300\n",
      "Average training loss: 3.5622892462412517\n",
      "Average test loss: 0.006102168338166343\n",
      "Epoch 139/300\n",
      "Average training loss: 3.3799959496392145\n",
      "Average test loss: 0.005610096123483446\n",
      "Epoch 140/300\n",
      "Average training loss: 3.1788094916873506\n",
      "Average test loss: 0.005523451324552298\n",
      "Epoch 141/300\n",
      "Average training loss: 2.975566178003947\n",
      "Average test loss: 0.008585482259177499\n",
      "Epoch 142/300\n",
      "Average training loss: 2.7779010514153373\n",
      "Average test loss: 0.005489757100327147\n",
      "Epoch 143/300\n",
      "Average training loss: 2.585431925031874\n",
      "Average test loss: 0.006010448472781314\n",
      "Epoch 144/300\n",
      "Average training loss: 2.39686504512363\n",
      "Average test loss: 0.01449408525766598\n",
      "Epoch 145/300\n",
      "Average training loss: 2.217829742219713\n",
      "Average test loss: 0.005332675152768691\n",
      "Epoch 146/300\n",
      "Average training loss: 2.0381489963531494\n",
      "Average test loss: 0.007018756357745992\n",
      "Epoch 147/300\n",
      "Average training loss: 1.8805407598283557\n",
      "Average test loss: 0.0053190691469030245\n",
      "Epoch 148/300\n",
      "Average training loss: 1.717578651216295\n",
      "Average test loss: 0.005641930592142874\n",
      "Epoch 149/300\n",
      "Average training loss: 1.5754323842790392\n",
      "Average test loss: 0.005304627630859614\n",
      "Epoch 150/300\n",
      "Average training loss: 1.4240046423806085\n",
      "Average test loss: 0.00589857525130113\n",
      "Epoch 151/300\n",
      "Average training loss: 1.2709220475090874\n",
      "Average test loss: 0.005248446322977543\n",
      "Epoch 152/300\n",
      "Average training loss: 1.1331697397232057\n",
      "Average test loss: 0.014861570720043448\n",
      "Epoch 153/300\n",
      "Average training loss: 0.9837950265672472\n",
      "Average test loss: 0.005150968405107657\n",
      "Epoch 154/300\n",
      "Average training loss: 0.8520817596647474\n",
      "Average test loss: 0.005203882379250394\n",
      "Epoch 155/300\n",
      "Average training loss: 0.7199261713557773\n",
      "Average test loss: 0.005531121983296341\n",
      "Epoch 156/300\n",
      "Average training loss: 0.5995905984772576\n",
      "Average test loss: 0.005157123847968049\n",
      "Epoch 157/300\n",
      "Average training loss: 0.5020946063730451\n",
      "Average test loss: 0.005159680853287379\n",
      "Epoch 158/300\n",
      "Average training loss: 0.42928543864356145\n",
      "Average test loss: 0.005106588828480906\n",
      "Epoch 159/300\n",
      "Average training loss: 0.37170422789785595\n",
      "Average test loss: 0.005135659399545855\n",
      "Epoch 160/300\n",
      "Average training loss: 0.3268595143953959\n",
      "Average test loss: 0.012298092313110829\n",
      "Epoch 161/300\n",
      "Average training loss: 0.29684283675087825\n",
      "Average test loss: 0.005503602313084735\n",
      "Epoch 162/300\n",
      "Average training loss: 0.27961369208494824\n",
      "Average test loss: 0.005122230131593015\n",
      "Epoch 163/300\n",
      "Average training loss: 0.25440163309044306\n",
      "Average test loss: 0.005114146459019846\n",
      "Epoch 164/300\n",
      "Average training loss: 0.23850334819157917\n",
      "Average test loss: 0.005236251740405957\n",
      "Epoch 165/300\n",
      "Average training loss: 0.22433414863215553\n",
      "Average test loss: 0.005269126255479124\n",
      "Epoch 166/300\n",
      "Average training loss: 0.21401700064871046\n",
      "Average test loss: 0.005257500364134709\n",
      "Epoch 167/300\n",
      "Average training loss: 0.20433368462986418\n",
      "Average test loss: 0.005119181617887484\n",
      "Epoch 168/300\n",
      "Average training loss: 0.19557015948825412\n",
      "Average test loss: 0.005449546916617288\n",
      "Epoch 169/300\n",
      "Average training loss: 0.190102157553037\n",
      "Average test loss: 0.005285764838879307\n",
      "Epoch 170/300\n",
      "Average training loss: 0.18327510751618278\n",
      "Average test loss: 0.005157427422288391\n",
      "Epoch 171/300\n",
      "Average training loss: 0.17879413060347238\n",
      "Average test loss: 0.005293598317851623\n",
      "Epoch 172/300\n",
      "Average training loss: 0.1760625727971395\n",
      "Average test loss: 0.00508631079726749\n",
      "Epoch 173/300\n",
      "Average training loss: 0.1725391161441803\n",
      "Average test loss: 0.0051158801520036325\n",
      "Epoch 174/300\n",
      "Average training loss: 0.16979155596097312\n",
      "Average test loss: 0.005139479322152005\n",
      "Epoch 175/300\n",
      "Average training loss: 0.16994112022717794\n",
      "Average test loss: 0.005679605562239886\n",
      "Epoch 176/300\n",
      "Average training loss: 0.16465709246529472\n",
      "Average test loss: 0.005744158911208312\n",
      "Epoch 177/300\n",
      "Average training loss: 0.1583778773546219\n",
      "Average test loss: 0.0050907226755387254\n",
      "Epoch 180/300\n",
      "Average training loss: 0.1561697330209944\n",
      "Average test loss: 0.005226960666891601\n",
      "Epoch 181/300\n",
      "Average training loss: 0.15528197881910535\n",
      "Average test loss: 0.005120194386276934\n",
      "Epoch 182/300\n",
      "Average training loss: 0.1528655265437232\n",
      "Average test loss: 0.005227470670723253\n",
      "Epoch 183/300\n",
      "Average training loss: 0.1520934995810191\n",
      "Average test loss: 0.005273563870125347\n",
      "Epoch 184/300\n",
      "Average training loss: 0.15120221073097653\n",
      "Average test loss: 0.005224653361365199\n",
      "Epoch 185/300\n",
      "Average training loss: 0.15038998681969112\n",
      "Average test loss: 0.005222943004220724\n",
      "Epoch 186/300\n",
      "Average training loss: 0.14904292123847537\n",
      "Average test loss: 0.005169647477981117\n",
      "Epoch 187/300\n",
      "Average training loss: 0.1487296733458837\n",
      "Average test loss: 0.005156116855227285\n",
      "Epoch 188/300\n",
      "Average training loss: 0.1481112898323271\n",
      "Average test loss: 0.0053086610831734205\n",
      "Epoch 189/300\n",
      "Average training loss: 0.1470019049247106\n",
      "Average test loss: 0.005147323944088486\n",
      "Epoch 191/300\n",
      "Average training loss: 0.14676772483852174\n",
      "Average test loss: 0.005130452437947194\n",
      "Epoch 192/300\n",
      "Average training loss: 66.2529055243598\n",
      "Average test loss: 200.17961751524606\n",
      "Epoch 194/300\n",
      "Average training loss: 57.81061336263021\n",
      "Average test loss: 3448144.3305261843\n",
      "Epoch 195/300\n",
      "Average training loss: 52.268286475287546\n",
      "Average test loss: 17169.38230003198\n",
      "Epoch 196/300\n",
      "Average training loss: 48.20858208211263\n",
      "Average test loss: 15040.698340792338\n",
      "Epoch 197/300\n",
      "Average training loss: 44.675061614990234\n",
      "Average test loss: 1411.3848182419672\n",
      "Epoch 198/300\n",
      "Average training loss: 41.79157182481554\n",
      "Average test loss: 59.83798850111167\n",
      "Epoch 199/300\n",
      "Average training loss: 38.96506275770399\n",
      "Average test loss: 1207.5907520476446\n",
      "Epoch 200/300\n",
      "Average training loss: 36.61941761610243\n",
      "Average test loss: 2.347501346740458\n",
      "Epoch 201/300\n",
      "Average training loss: 34.28725605943468\n",
      "Average test loss: 6879.539705667152\n",
      "Epoch 202/300\n",
      "Average training loss: 32.37132897949219\n",
      "Average test loss: 14479.497205820984\n",
      "Epoch 203/300\n",
      "Average training loss: 30.741996844821507\n",
      "Average test loss: 7094.61705039374\n",
      "Epoch 204/300\n",
      "Average training loss: 29.074590033637154\n",
      "Average test loss: 0.047846190109848974\n",
      "Epoch 205/300\n",
      "Average training loss: 27.448264456854925\n",
      "Average test loss: 13.983906389829185\n",
      "Epoch 206/300\n",
      "Average training loss: 25.834394132826063\n",
      "Average test loss: 0.43844224784109326\n",
      "Epoch 207/300\n",
      "Average training loss: 24.317971135457356\n",
      "Average test loss: 19963.72357395343\n",
      "Epoch 208/300\n",
      "Average training loss: 23.027627807617186\n",
      "Average test loss: 241.7013535070088\n",
      "Epoch 209/300\n",
      "Average training loss: 20.83357351175944\n",
      "Average test loss: 9.631018450112807\n",
      "Epoch 211/300\n",
      "Average training loss: 19.86573432413737\n",
      "Average test loss: 32174.131896809897\n",
      "Epoch 212/300\n",
      "Average training loss: 18.867345837063258\n",
      "Average test loss: 1076.5993363004493\n",
      "Epoch 213/300\n",
      "Average training loss: 17.928629474216038\n",
      "Average test loss: 1.167588884036574\n",
      "Epoch 214/300\n",
      "Average training loss: 16.946802741156684\n",
      "Average test loss: 2677.44035877296\n",
      "Epoch 215/300\n",
      "Average training loss: 16.05296944088406\n",
      "Average test loss: 51.421040298610926\n",
      "Epoch 216/300\n",
      "Average training loss: 15.192987080044217\n",
      "Average test loss: 0.12835303742231594\n",
      "Epoch 217/300\n",
      "Average training loss: 14.3765951300727\n",
      "Average test loss: 893.3292864583333\n",
      "Epoch 218/300\n",
      "Average training loss: 13.58916099802653\n",
      "Average test loss: 1.5293694813979997\n",
      "Epoch 219/300\n",
      "Average training loss: 12.141133917066787\n",
      "Average test loss: 0.020779767776942914\n",
      "Epoch 221/300\n",
      "Average training loss: 11.487108888414172\n",
      "Average test loss: 0.007783406134280893\n",
      "Epoch 222/300\n",
      "Average training loss: 10.839388388739692\n",
      "Average test loss: 0.007521668634066979\n",
      "Epoch 223/300\n",
      "Average training loss: 10.17993797047933\n",
      "Average test loss: 0.0086011796494325\n",
      "Epoch 224/300\n",
      "Average training loss: 9.538011224534776\n",
      "Average test loss: 0.008856694052616755\n",
      "Epoch 225/300\n",
      "Average training loss: 8.917780287000868\n",
      "Average test loss: 0.007669344496395853\n",
      "Epoch 226/300\n",
      "Average training loss: 8.335642159356011\n",
      "Average test loss: 0.007247876348594824\n",
      "Epoch 227/300\n",
      "Average training loss: 7.771359293619792\n",
      "Average test loss: 0.006690330586499638\n",
      "Epoch 228/300\n",
      "Average training loss: 7.201176051669651\n",
      "Average test loss: 0.006354885245362918\n",
      "Epoch 229/300\n",
      "Average training loss: 6.608742928822835\n",
      "Average test loss: 0.006832653514626953\n",
      "Epoch 230/300\n",
      "Average training loss: 6.022110409206814\n",
      "Average test loss: 0.00861140409277545\n",
      "Epoch 231/300\n",
      "Average training loss: 5.490049196031358\n",
      "Average test loss: 0.007585106496181753\n",
      "Epoch 232/300\n",
      "Average training loss: 4.7031066004435225\n",
      "Average test loss: 0.0058057985740403335\n",
      "Epoch 234/300\n",
      "Average training loss: 4.389001087188721\n",
      "Average test loss: 0.006393200196325779\n",
      "Epoch 235/300\n",
      "Average training loss: 4.0842806169721815\n",
      "Average test loss: 0.00589666283585959\n",
      "Epoch 236/300\n",
      "Average training loss: 3.7996365275912813\n",
      "Average test loss: 0.005626060581248668\n",
      "Epoch 237/300\n",
      "Average training loss: 3.5346270639631485\n",
      "Average test loss: 0.005621419211228689\n",
      "Epoch 238/300\n",
      "Average training loss: 3.2876992842356363\n",
      "Average test loss: 0.005536414623260498\n",
      "Epoch 239/300\n",
      "Average training loss: 3.043670988294813\n",
      "Average test loss: 0.005867840929577748\n",
      "Epoch 240/300\n",
      "Average training loss: 2.8062959774865046\n",
      "Average test loss: 0.005405784956283039\n",
      "Epoch 241/300\n",
      "Average training loss: 2.560392678790622\n",
      "Average test loss: 0.0057395761604938245\n",
      "Epoch 242/300\n",
      "Average training loss: 2.320978505452474\n",
      "Average test loss: 0.005302305629062983\n",
      "Epoch 243/300\n",
      "Average training loss: 2.0494731342527603\n",
      "Average test loss: 0.005324777965330416\n",
      "Epoch 244/300\n",
      "Average training loss: 1.7296060025956896\n",
      "Average test loss: 0.005387922195096811\n",
      "Epoch 245/300\n",
      "Average training loss: 1.57467260922326\n",
      "Average test loss: 0.00604258984948198\n",
      "Epoch 246/300\n",
      "Average training loss: 1.3968068191740248\n",
      "Average test loss: 0.005471342641860247\n",
      "Epoch 247/300\n",
      "Average training loss: 1.2496877379947238\n",
      "Average test loss: 0.005249629563755459\n",
      "Epoch 248/300\n",
      "Average training loss: 1.1079880074395074\n",
      "Average test loss: 0.005794354953699642\n",
      "Epoch 249/300\n",
      "Average training loss: 0.9718871715333727\n",
      "Average test loss: 0.00527962863072753\n",
      "Epoch 250/300\n",
      "Average training loss: 0.8348014284239875\n",
      "Average test loss: 0.005307375477006038\n",
      "Epoch 251/300\n",
      "Average training loss: 0.7025682983398438\n",
      "Average test loss: 0.005204121086125573\n",
      "Epoch 252/300\n",
      "Average training loss: 0.5936986137496101\n",
      "Average test loss: 0.0051815483967463175\n",
      "Epoch 253/300\n",
      "Average training loss: 0.5096561167769962\n",
      "Average test loss: 0.006526718662016921\n",
      "Epoch 254/300\n",
      "Average training loss: 0.4394899956385295\n",
      "Average test loss: 0.005175948926558097\n",
      "Epoch 255/300\n",
      "Average training loss: 0.3861219198438856\n",
      "Average test loss: 0.005854444494263994\n",
      "Epoch 256/300\n",
      "Average training loss: 0.34900370099809436\n",
      "Average test loss: 0.007364784599178368\n",
      "Epoch 257/300\n",
      "Average training loss: 0.30922149409188165\n",
      "Average test loss: 0.005186274279943771\n",
      "Epoch 258/300\n",
      "Average training loss: 0.2759651639726427\n",
      "Average test loss: 0.005107332911342383\n",
      "Epoch 259/300\n",
      "Average training loss: 0.25058662411901683\n",
      "Average test loss: 0.005222082788745562\n",
      "Epoch 260/300\n",
      "Average training loss: 0.23233537658055622\n",
      "Average test loss: 0.005413073713166846\n",
      "Epoch 261/300\n",
      "Average training loss: 0.21839341905381945\n",
      "Average test loss: 0.005138511261178387\n",
      "Epoch 262/300\n",
      "Average training loss: 0.20818657490942213\n",
      "Average test loss: 0.010360609798795647\n",
      "Epoch 263/300\n",
      "Average training loss: 0.20493552287419636\n",
      "Average test loss: 0.005082366047634019\n",
      "Epoch 264/300\n",
      "Average training loss: 0.1934983720779419\n",
      "Average test loss: 0.005216596772273382\n",
      "Epoch 265/300\n",
      "Average training loss: 0.19079869445164999\n",
      "Average test loss: 0.005103080241216554\n",
      "Epoch 266/300\n",
      "Average training loss: 0.18285320240921443\n",
      "Average test loss: 0.005059188711974356\n",
      "Epoch 267/300\n",
      "Average training loss: 0.17867969552675883\n",
      "Average test loss: 0.005944241783685154\n",
      "Epoch 268/300\n",
      "Average training loss: 0.17607222898801167\n",
      "Average test loss: 0.0053717769905924796\n",
      "Epoch 269/300\n",
      "Average training loss: 0.17341540144549475\n",
      "Average test loss: 0.005517459091213014\n",
      "Epoch 270/300\n",
      "Average training loss: 0.17052942462099924\n",
      "Average test loss: 0.005198567874108752\n",
      "Epoch 271/300\n",
      "Average training loss: 0.16816251157389747\n",
      "Average test loss: 0.005102621158791913\n",
      "Epoch 272/300\n",
      "Average training loss: 0.1662603222131729\n",
      "Average test loss: 0.00504204465697209\n",
      "Epoch 273/300\n",
      "Average training loss: 0.1639035092724694\n",
      "Average test loss: 0.005090685067077477\n",
      "Epoch 274/300\n",
      "Average training loss: 0.16171753040949505\n",
      "Average test loss: 0.005322226459367408\n",
      "Epoch 275/300\n",
      "Average training loss: 0.16004529046350055\n",
      "Average test loss: 0.005096349039011531\n",
      "Epoch 276/300\n",
      "Average training loss: 0.15890553530057272\n",
      "Average test loss: 0.005416239491146471\n",
      "Epoch 277/300\n",
      "Average training loss: 0.15653460427125296\n",
      "Average test loss: 0.005603078441487418\n",
      "Epoch 278/300\n",
      "Average training loss: 0.15541589777999454\n",
      "Average test loss: 0.005125262678083446\n",
      "Epoch 279/300\n",
      "Average training loss: 0.15353796231746675\n",
      "Average test loss: 0.0051019599114855135\n",
      "Epoch 280/300\n",
      "Average training loss: 6992404.154394385\n",
      "Average test loss: 6020.283636284722\n",
      "Epoch 281/300\n",
      "Average training loss: 24.098890906439888\n",
      "Average test loss: 108.47871932903925\n",
      "Epoch 282/300\n",
      "Average training loss: 23.010935957166883\n",
      "Average test loss: 2.3374709444708293\n",
      "Epoch 283/300\n",
      "Average training loss: 22.319284171210395\n",
      "Average test loss: 0.13013776234123442\n",
      "Epoch 284/300\n",
      "Average training loss: 21.783411848280164\n",
      "Average test loss: 12369.591662147204\n",
      "Epoch 285/300\n",
      "Average training loss: 21.28276623365614\n",
      "Average test loss: 18.384144463926553\n",
      "Epoch 286/300\n",
      "Average training loss: 20.84494734700521\n",
      "Average test loss: 0.4565693476465013\n",
      "Epoch 287/300\n",
      "Average training loss: 20.40278177218967\n",
      "Average test loss: 0.016767952205406295\n",
      "Epoch 288/300\n",
      "Average training loss: 19.984847766452365\n",
      "Average test loss: 1.8602798790120416\n",
      "Epoch 289/300\n",
      "Average training loss: 19.518605529785155\n",
      "Average test loss: 0.053075547206732965\n",
      "Epoch 290/300\n",
      "Average training loss: 19.012706617567275\n",
      "Average test loss: 0.01551565818157461\n",
      "Epoch 291/300\n",
      "Average training loss: 18.595095542060005\n",
      "Average test loss: 2.158224807103475\n",
      "Epoch 292/300\n",
      "Average training loss: 18.113993160671658\n",
      "Average test loss: 3201.816279025608\n",
      "Epoch 293/300\n",
      "Average training loss: 17.600572207980687\n",
      "Average test loss: 0.6445643499559827\n",
      "Epoch 294/300\n",
      "Average training loss: 17.160053705851237\n",
      "Average test loss: 0.010534602007104292\n",
      "Epoch 295/300\n",
      "Average training loss: 16.67016129896376\n",
      "Average test loss: 0.21404292829169166\n",
      "Epoch 296/300\n",
      "Average training loss: 16.169861906263563\n",
      "Average test loss: 2.172321646577782\n",
      "Epoch 297/300\n",
      "Average training loss: 15.637883664449056\n",
      "Average test loss: 0.008559947776297728\n",
      "Epoch 298/300\n",
      "Average training loss: 15.09160558742947\n",
      "Average test loss: 26.60663645050923\n",
      "Epoch 299/300\n",
      "Average training loss: 14.487716272142197\n",
      "Average test loss: 44.04619546518723\n",
      "Epoch 300/300\n",
      "Average training loss: 13.8348128874037\n",
      "Average test loss: 4.183255730291622\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 13.855093146853976\n",
      "Average test loss: 2.7801806345250872\n",
      "Epoch 2/300\n",
      "Average training loss: 7.084068328009711\n",
      "Average test loss: 0.007102340306258864\n",
      "Epoch 3/300\n",
      "Average training loss: 5.240965443505181\n",
      "Average test loss: 0.005922186037318574\n",
      "Epoch 4/300\n",
      "Average training loss: 4.322831360710992\n",
      "Average test loss: 0.04585075163220366\n",
      "Epoch 5/300\n",
      "Average training loss: 3.7196121440463594\n",
      "Average test loss: 0.005367291546116273\n",
      "Epoch 6/300\n",
      "Average training loss: 3.198450467215644\n",
      "Average test loss: 0.005284569644679626\n",
      "Epoch 7/300\n",
      "Average training loss: 2.7817532234191895\n",
      "Average test loss: 0.0051182496423522635\n",
      "Epoch 8/300\n",
      "Average training loss: 2.4795385773976646\n",
      "Average test loss: 0.005012217034896215\n",
      "Epoch 9/300\n",
      "Average training loss: 2.167622978422377\n",
      "Average test loss: 0.004586796294483874\n",
      "Epoch 10/300\n",
      "Average training loss: 1.7911971977021959\n",
      "Average test loss: 0.004498577499969138\n",
      "Epoch 11/300\n",
      "Average training loss: 1.5707578660117256\n",
      "Average test loss: 0.004573450643983152\n",
      "Epoch 12/300\n",
      "Average training loss: 1.369706475045946\n",
      "Average test loss: 0.00429960026509232\n",
      "Epoch 13/300\n",
      "Average training loss: 1.1914673719406128\n",
      "Average test loss: 0.004093814011249277\n",
      "Epoch 14/300\n",
      "Average training loss: 1.0192778567208185\n",
      "Average test loss: 0.0040378870585312445\n",
      "Epoch 15/300\n",
      "Average training loss: 0.8638687561353048\n",
      "Average test loss: 0.003908435103793939\n",
      "Epoch 16/300\n",
      "Average training loss: 0.7394398013220893\n",
      "Average test loss: 0.005979338616960579\n",
      "Epoch 17/300\n",
      "Average training loss: 0.6262001587020026\n",
      "Average test loss: 0.0039130334709253575\n",
      "Epoch 18/300\n",
      "Average training loss: 0.5362226948738098\n",
      "Average test loss: 0.0036198236513882874\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4647958014541202\n",
      "Average test loss: 0.004895225683848063\n",
      "Epoch 20/300\n",
      "Average training loss: 0.40904923203256394\n",
      "Average test loss: 0.0036116394110851818\n",
      "Epoch 21/300\n",
      "Average training loss: 0.36612832901212905\n",
      "Average test loss: 0.0036462642260723646\n",
      "Epoch 22/300\n",
      "Average training loss: 0.329674897591273\n",
      "Average test loss: 0.003412693825860818\n",
      "Epoch 23/300\n",
      "Average training loss: 0.29823008354504904\n",
      "Average test loss: 0.0041044705655011865\n",
      "Epoch 24/300\n",
      "Average training loss: 0.27450396725866527\n",
      "Average test loss: 0.0033338492125686674\n",
      "Epoch 25/300\n",
      "Average training loss: 0.2527304561138153\n",
      "Average test loss: 0.0032963922443903157\n",
      "Epoch 26/300\n",
      "Average training loss: 0.233955582830641\n",
      "Average test loss: 0.003427144737707244\n",
      "Epoch 27/300\n",
      "Average training loss: 0.21899952779875861\n",
      "Average test loss: 0.0047817984128163925\n",
      "Epoch 28/300\n",
      "Average training loss: 0.20672313622633617\n",
      "Average test loss: 0.003269139530758063\n",
      "Epoch 29/300\n",
      "Average training loss: 0.195600355664889\n",
      "Average test loss: 0.003211817876332336\n",
      "Epoch 30/300\n",
      "Average training loss: 0.18609563591745165\n",
      "Average test loss: 0.005048865679237577\n",
      "Epoch 31/300\n",
      "Average training loss: 0.17938321144051023\n",
      "Average test loss: 0.003457402156665921\n",
      "Epoch 32/300\n",
      "Average training loss: 0.17095866325828765\n",
      "Average test loss: 0.0032596371229737997\n",
      "Epoch 33/300\n",
      "Average training loss: 0.16488230102592044\n",
      "Average test loss: 0.003410222835631834\n",
      "Epoch 34/300\n",
      "Average training loss: 0.15964423939916822\n",
      "Average test loss: 0.0031295362446043225\n",
      "Epoch 35/300\n",
      "Average training loss: 0.15436593491501277\n",
      "Average test loss: 0.003131493074612485\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15007399670283\n",
      "Average test loss: 0.0030814118267347415\n",
      "Epoch 37/300\n",
      "Average training loss: 0.14569186475541857\n",
      "Average test loss: 0.0030841260364072192\n",
      "Epoch 38/300\n",
      "Average training loss: 0.14281335781680213\n",
      "Average test loss: 0.003262795831594202\n",
      "Epoch 39/300\n",
      "Average training loss: 0.13795978177918328\n",
      "Average test loss: 0.003254336772279607\n",
      "Epoch 40/300\n",
      "Average training loss: 0.13474339749415715\n",
      "Average test loss: 0.0030583563393188845\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1310093534456359\n",
      "Average test loss: 0.0030236185625609423\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12909072834915586\n",
      "Average test loss: 0.0031004196521308685\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12555799992879232\n",
      "Average test loss: 0.01252272767573595\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12225283057159848\n",
      "Average test loss: 0.0036121148843732145\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11986498900254568\n",
      "Average test loss: 0.0030464332066476346\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11921340562899907\n",
      "Average test loss: 0.0030075353350904254\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11512846219539642\n",
      "Average test loss: 0.0031670071141173444\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11369678409894307\n",
      "Average test loss: 0.0030794550387395754\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11556310648388332\n",
      "Average test loss: 0.0029739347497622173\n",
      "Epoch 50/300\n",
      "Average training loss: 2.0171471708615623\n",
      "Average test loss: 0.04867335089109838\n",
      "Epoch 51/300\n",
      "Average training loss: 0.30700935734642876\n",
      "Average test loss: 0.01855559919608964\n",
      "Epoch 52/300\n",
      "Average training loss: 0.2340177600118849\n",
      "Average test loss: 0.003544326057036718\n",
      "Epoch 53/300\n",
      "Average training loss: 0.20889671176009708\n",
      "Average test loss: 1.0646797176744376\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1845851713816325\n",
      "Average test loss: 7.140790865826938\n",
      "Epoch 55/300\n",
      "Average training loss: 0.16654525626367994\n",
      "Average test loss: 0.003203927975975805\n",
      "Epoch 56/300\n",
      "Average training loss: 0.15258344546953836\n",
      "Average test loss: 0.003277485112556153\n",
      "Epoch 57/300\n",
      "Average training loss: 0.14603360202577378\n",
      "Average test loss: 0.0031635211925539705\n",
      "Epoch 58/300\n",
      "Average training loss: 0.141246787932184\n",
      "Average test loss: 0.0032336057753612596\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1367296384771665\n",
      "Average test loss: 0.0030940709207206963\n",
      "Epoch 60/300\n",
      "Average training loss: 0.13329914283752442\n",
      "Average test loss: 0.0030640554970337284\n",
      "Epoch 61/300\n",
      "Average training loss: 0.13006953696409862\n",
      "Average test loss: 0.0030481255404237245\n",
      "Epoch 62/300\n",
      "Average training loss: 0.1271071706281768\n",
      "Average test loss: 0.003251723040516178\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12499039327436023\n",
      "Average test loss: 0.0030622632201347086\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12240522556172477\n",
      "Average test loss: 0.003020570910225312\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12020421157942877\n",
      "Average test loss: 0.003024467730273803\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11889931644996007\n",
      "Average test loss: 0.019945658806297513\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11691740563180711\n",
      "Average test loss: 0.002964649555169874\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11581278632084528\n",
      "Average test loss: 0.00508555112241043\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11390280974573559\n",
      "Average test loss: 0.003000867265587052\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11284400860468546\n",
      "Average test loss: 0.003151538840908971\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11382925820350646\n",
      "Average test loss: 0.0030030098336024415\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11079410576820374\n",
      "Average test loss: 0.002937466802282466\n",
      "Epoch 73/300\n",
      "Average training loss: 0.10977842438883252\n",
      "Average test loss: 0.00295072661464413\n",
      "Epoch 74/300\n",
      "Average training loss: 0.10933935017055936\n",
      "Average test loss: 0.0029887393378756114\n",
      "Epoch 75/300\n",
      "Average training loss: 0.10857866641547945\n",
      "Average test loss: 0.0032888367500984008\n",
      "Epoch 76/300\n",
      "Average training loss: 0.10845003304216597\n",
      "Average test loss: 0.0030608280127247174\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1062196899122662\n",
      "Average test loss: 0.0031594276268863014\n",
      "Epoch 78/300\n",
      "Average training loss: 0.10721452728907267\n",
      "Average test loss: 0.002940356519487169\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1048783814907074\n",
      "Average test loss: 0.03107895113983088\n",
      "Epoch 80/300\n",
      "Average training loss: 0.10406340425875452\n",
      "Average test loss: 0.0031205994452660284\n",
      "Epoch 81/300\n",
      "Average training loss: 0.10348730646239386\n",
      "Average test loss: 0.002898250640887353\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10317593076494005\n",
      "Average test loss: 0.002919429508969188\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10223288909594218\n",
      "Average test loss: 0.0030389530240661567\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10165848826699787\n",
      "Average test loss: 0.003295199325101243\n",
      "Epoch 85/300\n",
      "Average training loss: 0.10074371793535021\n",
      "Average test loss: 0.003262894418504503\n",
      "Epoch 86/300\n",
      "Average training loss: 0.1000228517320421\n",
      "Average test loss: 0.003504223824168245\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09971871701214048\n",
      "Average test loss: 0.0029526707204058766\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09941577827268176\n",
      "Average test loss: 0.002893456347079741\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09839775021208658\n",
      "Average test loss: 0.0031042455008460416\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10003722516695658\n",
      "Average test loss: 0.0030428165720982683\n",
      "Epoch 91/300\n",
      "Average training loss: 83.94347064128186\n",
      "Average test loss: 0.7926788359963232\n",
      "Epoch 92/300\n",
      "Average training loss: 4.947100335439046\n",
      "Average test loss: 0.00546144853077001\n",
      "Epoch 93/300\n",
      "Average training loss: 3.3916403579711916\n",
      "Average test loss: 0.004165177254420188\n",
      "Epoch 94/300\n",
      "Average training loss: 2.6145215335422094\n",
      "Average test loss: 0.0037123176356156666\n",
      "Epoch 95/300\n",
      "Average training loss: 2.03870432472229\n",
      "Average test loss: 0.1069984981968171\n",
      "Epoch 96/300\n",
      "Average training loss: 1.5740265631145902\n",
      "Average test loss: 0.00559126998608311\n",
      "Epoch 97/300\n",
      "Average training loss: 1.2596176895565456\n",
      "Average test loss: 0.044950396940939955\n",
      "Epoch 98/300\n",
      "Average training loss: 1.0101411618656582\n",
      "Average test loss: 0.12386045744187302\n",
      "Epoch 99/300\n",
      "Average training loss: 0.801918591870202\n",
      "Average test loss: 0.0034076653795523776\n",
      "Epoch 100/300\n",
      "Average training loss: 0.6494958115683661\n",
      "Average test loss: 0.0032475566814343135\n",
      "Epoch 101/300\n",
      "Average training loss: 0.5276278018951416\n",
      "Average test loss: 9.645040980994702\n",
      "Epoch 102/300\n",
      "Average training loss: 0.43471323291460673\n",
      "Average test loss: 0.008456938786639107\n",
      "Epoch 103/300\n",
      "Average training loss: 0.3652647372351752\n",
      "Average test loss: 0.003483794552377529\n",
      "Epoch 104/300\n",
      "Average training loss: 0.3150053713321686\n",
      "Average test loss: 0.013762897743533054\n",
      "Epoch 105/300\n",
      "Average training loss: 0.26995244312286376\n",
      "Average test loss: 0.003122332936566737\n",
      "Epoch 106/300\n",
      "Average training loss: 0.23633549274338617\n",
      "Average test loss: 1.6660180522633923\n",
      "Epoch 107/300\n",
      "Average training loss: 0.21151368912061055\n",
      "Average test loss: 0.007001101526949141\n",
      "Epoch 108/300\n",
      "Average training loss: 0.19149209886789323\n",
      "Average test loss: 0.0036842719571044047\n",
      "Epoch 109/300\n",
      "Average training loss: 0.17759092399809095\n",
      "Average test loss: 0.0030747959015683993\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1661887173652649\n",
      "Average test loss: 0.003259038257723053\n",
      "Epoch 111/300\n",
      "Average training loss: 0.1566349775923623\n",
      "Average test loss: 0.004191740926355124\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1490549131764306\n",
      "Average test loss: 0.0030473190856476623\n",
      "Epoch 113/300\n",
      "Average training loss: 0.14181550403436025\n",
      "Average test loss: 0.025883897595935398\n",
      "Epoch 114/300\n",
      "Average training loss: 0.13721794823805492\n",
      "Average test loss: 0.004661280882855256\n",
      "Epoch 115/300\n",
      "Average training loss: 0.13292980978224012\n",
      "Average test loss: 0.0031940555636667542\n",
      "Epoch 116/300\n",
      "Average training loss: 0.12840432540575664\n",
      "Average test loss: 0.0034309093720383114\n",
      "Epoch 117/300\n",
      "Average training loss: 0.12495215815967983\n",
      "Average test loss: 0.006846377173438669\n",
      "Epoch 118/300\n",
      "Average training loss: 0.12142393672466278\n",
      "Average test loss: 0.003405615982909997\n",
      "Epoch 119/300\n",
      "Average training loss: 0.11850491807195876\n",
      "Average test loss: 0.005456558499899175\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11599211400747299\n",
      "Average test loss: 0.0028823919636714788\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11180131418175168\n",
      "Average test loss: 0.0029402156099677087\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11004232270187801\n",
      "Average test loss: 0.0031087419218901132\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10817503707938723\n",
      "Average test loss: 0.0029540606658491823\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10650571527745989\n",
      "Average test loss: 0.003086137641014324\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10547301795085272\n",
      "Average test loss: 0.0029244912535780006\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10412851550181706\n",
      "Average test loss: 0.002972032829705212\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10331674049297969\n",
      "Average test loss: 0.0028851293698988027\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10241596298747592\n",
      "Average test loss: 0.00518462067635523\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10245447658499082\n",
      "Average test loss: 0.0029044499542150233\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10079243273205227\n",
      "Average test loss: 0.0033162103599558273\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10082306517495049\n",
      "Average test loss: 0.0031254431632243927\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09925240689516067\n",
      "Average test loss: 0.002972066116415792\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09885608405537076\n",
      "Average test loss: 0.0029237063438114193\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09908670605553521\n",
      "Average test loss: 0.003898521949019697\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0981452873316076\n",
      "Average test loss: 0.0031204702651335134\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09745641221602758\n",
      "Average test loss: 0.0030639796670940186\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09672168920437495\n",
      "Average test loss: 0.003315312780853775\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09678290991650687\n",
      "Average test loss: 0.0029575073023637138\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09687063624461492\n",
      "Average test loss: 0.010833344891667367\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09545777332451609\n",
      "Average test loss: 0.003117141079985433\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09556642480691274\n",
      "Average test loss: 0.002895252577960491\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09648474931716919\n",
      "Average test loss: 0.0029979239156883624\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09486622420284484\n",
      "Average test loss: 0.0029179835925913517\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0944259761373202\n",
      "Average test loss: 0.002921035678229398\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0934111377067036\n",
      "Average test loss: 0.0029295149575918914\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09274577252732383\n",
      "Average test loss: 0.04074466169708305\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09316362621386846\n",
      "Average test loss: 0.0029097884231143525\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09262919482919905\n",
      "Average test loss: 0.0031561769244985447\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09183903176254696\n",
      "Average test loss: 13.900524516714944\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09227866209877862\n",
      "Average test loss: 0.0029400160223659543\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09137955325841904\n",
      "Average test loss: 0.003818977920545472\n",
      "Epoch 152/300\n",
      "Average training loss: 0.09128202415174907\n",
      "Average test loss: 0.003074038492515683\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09074437112278408\n",
      "Average test loss: 0.003824548860390981\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09109773226247893\n",
      "Average test loss: 0.0030514231462859445\n",
      "Epoch 155/300\n",
      "Average training loss: 0.1772352515326606\n",
      "Average test loss: 0.003116837806171841\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10923797008064058\n",
      "Average test loss: 0.0030869897043125497\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10153612316317029\n",
      "Average test loss: 0.025512305396298566\n",
      "Epoch 158/300\n",
      "Average training loss: 0.6318384410871399\n",
      "Average test loss: 0.00348955395941933\n",
      "Epoch 159/300\n",
      "Average training loss: 0.25019447644551596\n",
      "Average test loss: 0.0031099069424801403\n",
      "Epoch 160/300\n",
      "Average training loss: 0.1838054939640893\n",
      "Average test loss: 0.003086021722604831\n",
      "Epoch 161/300\n",
      "Average training loss: 0.15854502706395254\n",
      "Average test loss: 0.0038308070061935317\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1406175235907237\n",
      "Average test loss: 0.0029510697554796933\n",
      "Epoch 163/300\n",
      "Average training loss: 0.12596426141262054\n",
      "Average test loss: 0.0029045919142663477\n",
      "Epoch 164/300\n",
      "Average training loss: 0.11698855738507377\n",
      "Average test loss: 0.004503998296128379\n",
      "Epoch 165/300\n",
      "Average training loss: 0.11337493560711542\n",
      "Average test loss: 0.002915585837223464\n",
      "Epoch 166/300\n",
      "Average training loss: 0.1086676052874989\n",
      "Average test loss: 0.0033234142849428785\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10577587509155273\n",
      "Average test loss: 0.0028817150385843384\n",
      "Epoch 168/300\n",
      "Average training loss: 0.10372463159428702\n",
      "Average test loss: 0.0028852059171638553\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10152803251478407\n",
      "Average test loss: 0.003152034778561857\n",
      "Epoch 170/300\n",
      "Average training loss: 0.10000821140077379\n",
      "Average test loss: 0.003193954877762331\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10147656734784444\n",
      "Average test loss: 0.003110616320744157\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0972293788658248\n",
      "Average test loss: 0.0051261371572812395\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09607050411568747\n",
      "Average test loss: 0.002934943679720163\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09488135470946629\n",
      "Average test loss: 0.0029785894334523216\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0936829880144861\n",
      "Average test loss: 0.02658891029159228\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09638904068205091\n",
      "Average test loss: 0.0033599253998448453\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09683797123697069\n",
      "Average test loss: 0.0028904420108430916\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09179947792159186\n",
      "Average test loss: 0.0030126407843910985\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09078482781185045\n",
      "Average test loss: 0.0029215245911230646\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09056852571169535\n",
      "Average test loss: 0.0029601312970949545\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09118230058087243\n",
      "Average test loss: 0.0030650603303478824\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09068834867080053\n",
      "Average test loss: 0.003161276466730568\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08944114887714386\n",
      "Average test loss: 0.0037290973166624703\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0886529285841518\n",
      "Average test loss: 0.0034918252068261305\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08900673913293415\n",
      "Average test loss: 0.0029724074126117757\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08868224550618066\n",
      "Average test loss: 0.004194579422059986\n",
      "Epoch 187/300\n",
      "Average training loss: 0.08846548176805179\n",
      "Average test loss: 0.004706791975431972\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08755349744028515\n",
      "Average test loss: 0.0031011805567476482\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08718664725621542\n",
      "Average test loss: 0.0034345662204755675\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08780148796240489\n",
      "Average test loss: 0.0036507609147164556\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08875556878248851\n",
      "Average test loss: 0.00295502673358553\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0863759006526735\n",
      "Average test loss: 0.0032187768386469947\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0878057896229956\n",
      "Average test loss: 0.0036896990020242\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08632436305946774\n",
      "Average test loss: 0.005023921580364307\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08547009708484014\n",
      "Average test loss: 0.003010370486519403\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08555839173661338\n",
      "Average test loss: 0.0030412920355382895\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0873442467517323\n",
      "Average test loss: 0.0029997693217462965\n",
      "Epoch 198/300\n",
      "Average training loss: 0.08477285002006425\n",
      "Average test loss: 0.0029871218451816175\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0848323108487659\n",
      "Average test loss: 0.0031056081145587893\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09961973323424657\n",
      "Average test loss: 0.08165040757921006\n",
      "Epoch 201/300\n",
      "Average training loss: 0.13598887626330058\n",
      "Average test loss: 0.007350086015131738\n",
      "Epoch 202/300\n",
      "Average training loss: 0.10257447315255801\n",
      "Average test loss: 139.2218476293352\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0950067271788915\n",
      "Average test loss: 0.002907965228996343\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09095709774229262\n",
      "Average test loss: 0.004106398935119311\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08912628078460694\n",
      "Average test loss: 0.0029894188408636385\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08683851259284549\n",
      "Average test loss: 0.003346948553290632\n",
      "Epoch 207/300\n",
      "Average training loss: 0.08590932515594694\n",
      "Average test loss: 0.0029552164344737925\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08493630726469888\n",
      "Average test loss: 0.0474183082514339\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08440389146407445\n",
      "Average test loss: 0.00298621368366811\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08423984303739336\n",
      "Average test loss: 0.0030705885402858256\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08388132956955167\n",
      "Average test loss: 0.0034097371202790076\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08530870550870895\n",
      "Average test loss: 0.0029863761303325492\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08394592967298296\n",
      "Average test loss: 0.0659519996030463\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08302323011226125\n",
      "Average test loss: 0.0031701927457211745\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08370740443468094\n",
      "Average test loss: 0.0032254126204384697\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08376935504542457\n",
      "Average test loss: 0.0029539905302226545\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08314612893263498\n",
      "Average test loss: 0.0034700721436076694\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08297327096263568\n",
      "Average test loss: 0.0031310193604893156\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08288562617699305\n",
      "Average test loss: 0.0030550201597313088\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08305998549527592\n",
      "Average test loss: 0.0031060195358263124\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08239848923683167\n",
      "Average test loss: 0.0038687404774957234\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08186957436137729\n",
      "Average test loss: 0.003100822126285897\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0816522569126553\n",
      "Average test loss: 0.0031080461512837144\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08207999387052324\n",
      "Average test loss: 622468.7970833334\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08124778329663807\n",
      "Average test loss: 0.00352739965212014\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08124123870664172\n",
      "Average test loss: 0.0030932744095722832\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08176241738266415\n",
      "Average test loss: 0.006646756129960219\n",
      "Epoch 228/300\n",
      "Average training loss: 0.081096707019541\n",
      "Average test loss: 0.0031030087551722923\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08071366941928863\n",
      "Average test loss: 0.003471029497269127\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08151616466707653\n",
      "Average test loss: 0.0033639247131844363\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08014691383639971\n",
      "Average test loss: 0.0030654762904677125\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0808944650888443\n",
      "Average test loss: 0.0032137660797064505\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08004851820733812\n",
      "Average test loss: 0.003093608184614115\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08407129946019914\n",
      "Average test loss: 0.0031500232964754103\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08478485208087497\n",
      "Average test loss: 0.0030114420060482292\n",
      "Epoch 236/300\n",
      "Average training loss: 0.07941179791424009\n",
      "Average test loss: 0.003171904516302877\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08021578913264804\n",
      "Average test loss: 0.15144601525531876\n",
      "Epoch 238/300\n",
      "Average training loss: 0.079696001874076\n",
      "Average test loss: 0.003007952066966229\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0795214140382078\n",
      "Average test loss: 0.003041637914876143\n",
      "Epoch 240/300\n",
      "Average training loss: 0.07898452214399973\n",
      "Average test loss: 0.003034459849198659\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07934061317311393\n",
      "Average test loss: 0.003127059908790721\n",
      "Epoch 242/300\n",
      "Average training loss: 0.07969498643610212\n",
      "Average test loss: 0.0035282274012764293\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07907943467299143\n",
      "Average test loss: 0.0031973642303297915\n",
      "Epoch 244/300\n",
      "Average training loss: 0.07945107585854\n",
      "Average test loss: 0.003022909802902076\n",
      "Epoch 245/300\n",
      "Average training loss: 0.07876916240983539\n",
      "Average test loss: 0.004943512097001076\n",
      "Epoch 246/300\n",
      "Average training loss: 0.07887065589427948\n",
      "Average test loss: 0.003157205751579669\n",
      "Epoch 247/300\n",
      "Average training loss: 0.07835274465878804\n",
      "Average test loss: 0.0032260681701203188\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0783104377720091\n",
      "Average test loss: 0.003096790955091516\n",
      "Epoch 249/300\n",
      "Average training loss: 0.14872000137633748\n",
      "Average test loss: 0.003014994617551565\n",
      "Epoch 250/300\n",
      "Average training loss: 0.10166061541438103\n",
      "Average test loss: 0.00299293694148461\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0908792954352167\n",
      "Average test loss: 0.0030778837400592036\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08455991790692012\n",
      "Average test loss: 0.003129369302963217\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0808798823621538\n",
      "Average test loss: 0.0030848542832665974\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07916866116391288\n",
      "Average test loss: 0.0030863535335908334\n",
      "Epoch 255/300\n",
      "Average training loss: 0.07843084019422532\n",
      "Average test loss: 0.003059690697946482\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0778613877163993\n",
      "Average test loss: 0.0030969348680227994\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0781756497224172\n",
      "Average test loss: 0.0035691795843756863\n",
      "Epoch 258/300\n",
      "Average training loss: 0.07760603755050235\n",
      "Average test loss: 0.003646553988671965\n",
      "Epoch 259/300\n",
      "Average training loss: 0.07785443045695623\n",
      "Average test loss: 0.003303782105135421\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08217652589082718\n",
      "Average test loss: 0.00370421329802937\n",
      "Epoch 261/300\n",
      "Average training loss: 0.07726855130328072\n",
      "Average test loss: 0.011290361578265826\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07771225396792093\n",
      "Average test loss: 0.003123667019522852\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07764218727747599\n",
      "Average test loss: 0.0030839415959392985\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07851018693712022\n",
      "Average test loss: 0.003192710349129306\n",
      "Epoch 265/300\n",
      "Average training loss: 0.07834750413232379\n",
      "Average test loss: 0.003322818624890513\n",
      "Epoch 266/300\n",
      "Average training loss: 0.07752490937047535\n",
      "Average test loss: 0.0041991772049417096\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08053851503133774\n",
      "Average test loss: 0.00302259810662104\n",
      "Epoch 268/300\n",
      "Average training loss: 0.07906257432699204\n",
      "Average test loss: 0.0036625662160416446\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07658649328019884\n",
      "Average test loss: 0.0031395836083425417\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0767495535744561\n",
      "Average test loss: 0.0033555536448127694\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0770996887716982\n",
      "Average test loss: 0.003099103332393699\n",
      "Epoch 272/300\n",
      "Average training loss: 0.07663253875573477\n",
      "Average test loss: 0.003144680435458819\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08444571122195986\n",
      "Average test loss: 0.009017920825216505\n",
      "Epoch 274/300\n",
      "Average training loss: 0.07832546981838015\n",
      "Average test loss: 0.003003349924977455\n",
      "Epoch 275/300\n",
      "Average training loss: 0.07773372288544973\n",
      "Average test loss: 0.0030922835452689063\n",
      "Epoch 276/300\n",
      "Average training loss: 0.07667723787493176\n",
      "Average test loss: 0.003163988199498918\n",
      "Epoch 277/300\n",
      "Average training loss: 0.07653389646278487\n",
      "Average test loss: 0.0033207906244529617\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07663834122816722\n",
      "Average test loss: 0.0031226557062731847\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07628663751151826\n",
      "Average test loss: 0.0030681467306696705\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0767637635005845\n",
      "Average test loss: 0.003079136043166121\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0766855292585161\n",
      "Average test loss: 0.003086440025518338\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0759496860967742\n",
      "Average test loss: 0.0033058293755683634\n",
      "Epoch 283/300\n",
      "Average training loss: 1.9641168826354876\n",
      "Average test loss: 17.30405487781432\n",
      "Epoch 284/300\n",
      "Average training loss: 0.8983575895627339\n",
      "Average test loss: 0.003254491221987539\n",
      "Epoch 285/300\n",
      "Average training loss: 0.47305033842722577\n",
      "Average test loss: 0.003740965354980694\n",
      "Epoch 286/300\n",
      "Average training loss: 0.3269735338687897\n",
      "Average test loss: 0.0030636467207223175\n",
      "Epoch 287/300\n",
      "Average training loss: 0.2433183720641666\n",
      "Average test loss: 0.018946394357416366\n",
      "Epoch 288/300\n",
      "Average training loss: 0.19250968607266744\n",
      "Average test loss: 0.0034866550819327435\n",
      "Epoch 289/300\n",
      "Average training loss: 0.16661782725652058\n",
      "Average test loss: 0.003162730877184206\n",
      "Epoch 290/300\n",
      "Average training loss: 0.15134221875667572\n",
      "Average test loss: 0.0032333863596949314\n",
      "Epoch 291/300\n",
      "Average training loss: 0.14005480456352234\n",
      "Average test loss: 0.002961706125901805\n",
      "Epoch 292/300\n",
      "Average training loss: 0.13025043321318097\n",
      "Average test loss: 0.0029355796422395443\n",
      "Epoch 293/300\n",
      "Average training loss: 0.12146040087938309\n",
      "Average test loss: 0.003301242969826692\n",
      "Epoch 294/300\n",
      "Average training loss: 0.1146249458193779\n",
      "Average test loss: 0.0029691561812327967\n",
      "Epoch 295/300\n",
      "Average training loss: 0.10972489788797167\n",
      "Average test loss: 0.0029589686006721524\n",
      "Epoch 296/300\n",
      "Average training loss: 0.10609255186054442\n",
      "Average test loss: 0.0029744761880073282\n",
      "Epoch 297/300\n",
      "Average training loss: 0.1050399608678288\n",
      "Average test loss: 0.0030800184071477915\n",
      "Epoch 298/300\n",
      "Average training loss: 0.1000956031481425\n",
      "Average test loss: 0.0030074180002427763\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09733319281869464\n",
      "Average test loss: 0.002970565912210279\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0950427326493793\n",
      "Average test loss: 0.003826445291439692\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.278778057946099\n",
      "Average test loss: 0.006615342511898941\n",
      "Epoch 2/300\n",
      "Average training loss: 6.842376218583849\n",
      "Average test loss: 0.010288116870654954\n",
      "Epoch 3/300\n",
      "Average training loss: 4.518280974494086\n",
      "Average test loss: 0.004606320240845283\n",
      "Epoch 4/300\n",
      "Average training loss: 3.6029494840833878\n",
      "Average test loss: 0.004502424843816293\n",
      "Epoch 5/300\n",
      "Average training loss: 3.3171304382748072\n",
      "Average test loss: 0.19618798253105746\n",
      "Epoch 6/300\n",
      "Average training loss: 2.7657448925442165\n",
      "Average test loss: 0.003926859692980846\n",
      "Epoch 7/300\n",
      "Average training loss: 2.419218577914768\n",
      "Average test loss: 0.007940892021896111\n",
      "Epoch 8/300\n",
      "Average training loss: 2.1368622523413765\n",
      "Average test loss: 0.0037212504740390512\n",
      "Epoch 9/300\n",
      "Average training loss: 1.83325084400177\n",
      "Average test loss: 0.004546851173043251\n",
      "Epoch 10/300\n",
      "Average training loss: 1.5888636097378201\n",
      "Average test loss: 0.003297075999693738\n",
      "Epoch 11/300\n",
      "Average training loss: 1.3529164430830214\n",
      "Average test loss: 0.003320648296839661\n",
      "Epoch 12/300\n",
      "Average training loss: 1.1608676381640963\n",
      "Average test loss: 0.003396242376209961\n",
      "Epoch 13/300\n",
      "Average training loss: 1.0019239994684854\n",
      "Average test loss: 0.0047304302495386865\n",
      "Epoch 14/300\n",
      "Average training loss: 0.8617763546307882\n",
      "Average test loss: 0.003100189664711555\n",
      "Epoch 15/300\n",
      "Average training loss: 0.739877765390608\n",
      "Average test loss: 0.002890082537300057\n",
      "Epoch 16/300\n",
      "Average training loss: 0.6391237879329258\n",
      "Average test loss: 0.0028215147215459083\n",
      "Epoch 17/300\n",
      "Average training loss: 0.5527148337364197\n",
      "Average test loss: 0.0029421848104231887\n",
      "Epoch 18/300\n",
      "Average test loss: 0.0031727294160260096\n",
      "Epoch 19/300\n",
      "Average training loss: 0.4167310132185618\n",
      "Average test loss: 0.002559088765747017\n",
      "Epoch 20/300\n",
      "Average training loss: 0.32231987902853226\n",
      "Average test loss: 0.0034834436265130837\n",
      "Epoch 22/300\n",
      "Average training loss: 0.28628201683362325\n",
      "Average test loss: 0.0026107450088279113\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2553151343266169\n",
      "Average test loss: 0.0023744450750657255\n",
      "Epoch 24/300\n",
      "Average training loss: 0.22989334173997245\n",
      "Average test loss: 0.0031456401262225375\n",
      "Epoch 25/300\n",
      "Average training loss: 0.19216221121946972\n",
      "Average test loss: 0.0022844112126363648\n",
      "Epoch 27/300\n",
      "Average training loss: 0.17868671736452316\n",
      "Average test loss: 0.014089996220130059\n",
      "Epoch 28/300\n",
      "Average training loss: 0.16568449772728813\n",
      "Average test loss: 0.002228264790856176\n",
      "Epoch 29/300\n",
      "Average training loss: 0.15521432299084134\n",
      "Average test loss: 0.0021935041480594213\n",
      "Epoch 30/300\n",
      "Average training loss: 0.146249995748202\n",
      "Average test loss: 0.003946809652778838\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13927562327517404\n",
      "Average test loss: 0.0023214021639691457\n",
      "Epoch 32/300\n",
      "Average training loss: 0.13336187166637845\n",
      "Average test loss: 0.0021480033319029542\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12679751275645362\n",
      "Average test loss: 0.0022132228676022755\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12107729540930853\n",
      "Average test loss: 0.0020761573082870906\n",
      "Epoch 35/300\n",
      "Average training loss: 0.11630233958032396\n",
      "Average test loss: 0.002119981651711795\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11168272625075447\n",
      "Average test loss: 0.0024379234299477604\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10818024194902844\n",
      "Average test loss: 0.002026084359217849\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10456716692447662\n",
      "Average test loss: 0.002066644386905763\n",
      "Epoch 39/300\n",
      "Average test loss: 0.0020069595012399888\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09827079093456269\n",
      "Average test loss: 0.002027353850710723\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09571451765961117\n",
      "Average test loss: 0.0020198177265831167\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09195894951290555\n",
      "Average test loss: 0.0021170201593389113\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09192490603526433\n",
      "Average test loss: 0.0020353017722566924\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08891329611010021\n",
      "Average test loss: 0.002112703610625532\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08574066142572297\n",
      "Average test loss: 0.0020006113447662856\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08735464913315243\n",
      "Average test loss: 0.0019608313950399557\n",
      "Epoch 47/300\n",
      "Average training loss: 0.15432528962029352\n",
      "Average test loss: 0.00238524315195779\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10007317701975504\n",
      "Average test loss: 0.0021407197275095514\n",
      "Epoch 49/300\n",
      "Average training loss: 0.091693035085996\n",
      "Average test loss: 0.002067636869640814\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08747034439113405\n",
      "Average test loss: 0.0020577983560247553\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08481303290526072\n",
      "Average test loss: 0.002200260636086265\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08260780498054293\n",
      "Average test loss: 0.001972260636380977\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0812079667184088\n",
      "Average test loss: 0.001972716599288914\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07985326506031884\n",
      "Average test loss: 0.00446217648146881\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0787861517601543\n",
      "Average test loss: 0.001988037000617219\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07786837306287553\n",
      "Average test loss: 0.0021055358739362822\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07714072461260689\n",
      "Average test loss: 0.0019540259937445323\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07749220458004209\n",
      "Average test loss: 0.001915285611939099\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0754943729043007\n",
      "Average test loss: 0.0019332890812721517\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07530778089496824\n",
      "Average test loss: 0.002142693191663259\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07404897770616743\n",
      "Average test loss: 0.0020164479008979268\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07401921722624037\n",
      "Average test loss: 0.002781517611609565\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07276601107915243\n",
      "Average test loss: 0.0019371782432620725\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07216809344622824\n",
      "Average test loss: 0.0020867929990506834\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07156094730231496\n",
      "Average test loss: 0.001933005795503656\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07078949200444751\n",
      "Average test loss: 0.0019054385726857517\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07027637848589155\n",
      "Average test loss: 0.0019612804781645537\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07010755166742537\n",
      "Average test loss: 0.0019961842632749013\n",
      "Epoch 69/300\n",
      "Average training loss: 0.4236564224296146\n",
      "Average test loss: 0.002284163686343365\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12367928844028049\n",
      "Average test loss: 0.00996269335763322\n",
      "Epoch 71/300\n",
      "Average training loss: 0.10064969484673605\n",
      "Average test loss: 0.0020446333096673093\n",
      "Epoch 72/300\n",
      "Average training loss: 0.0920756035513348\n",
      "Average test loss: 0.002023161854905387\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08671205994155672\n",
      "Average test loss: 0.0019507059435256654\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0830771407286326\n",
      "Average test loss: 0.001980299287993047\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08054595472415288\n",
      "Average test loss: 0.002012927999513017\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07841927094591988\n",
      "Average test loss: 0.0019306057220738796\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07678742581605912\n",
      "Average test loss: 0.0025660341998769176\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07544278579288059\n",
      "Average test loss: 0.0019152781527696384\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07435220565398534\n",
      "Average test loss: 0.0019050687768807015\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07314882689052157\n",
      "Average test loss: 0.0020070595449457568\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0716583318842782\n",
      "Average test loss: 0.031138104456166427\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07106985927290387\n",
      "Average test loss: 0.00198023048746917\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07163109831015269\n",
      "Average test loss: 0.0019031261852425006\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06999379626247618\n",
      "Average test loss: 0.0019129240964021947\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06936131465435028\n",
      "Average test loss: 0.0019388904576707218\n",
      "Epoch 87/300\n",
      "Average training loss: 0.06913001804881626\n",
      "Average test loss: 0.003078587224913968\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06900736079613368\n",
      "Average test loss: 0.0019038198654436403\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06846938676635424\n",
      "Average test loss: 0.0019276352862103118\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06797259977459907\n",
      "Average test loss: 0.011723601154983044\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06805021943979793\n",
      "Average test loss: 0.0021003195743800867\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06756053406662411\n",
      "Average test loss: 0.001876114580883748\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06699333257476489\n",
      "Average test loss: 0.00582920145160622\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06675219013955858\n",
      "Average test loss: 0.0022581240164322985\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06682303294870588\n",
      "Average test loss: 0.0019001984143008788\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06576924282974667\n",
      "Average test loss: 0.0019565302356042797\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06561779998408424\n",
      "Average test loss: 0.002084146489492721\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06503563067648147\n",
      "Average test loss: 0.0018780066077080037\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06467035005158848\n",
      "Average test loss: 0.0020349371849248806\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06439952910608715\n",
      "Average test loss: 0.0019549380149692297\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06478853339950244\n",
      "Average test loss: 0.0018871939509279199\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06370344260003831\n",
      "Average test loss: 0.00221582108301421\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06368145074778132\n",
      "Average test loss: 0.002049488913992213\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06311883291602134\n",
      "Average test loss: 0.0033401146808432207\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06289225488238864\n",
      "Average test loss: 0.0042100877066453294\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0625930321349038\n",
      "Average test loss: 0.0019383976904468404\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06237954521841473\n",
      "Average test loss: 0.0018860121824675136\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06177826139330864\n",
      "Average test loss: 0.001986256152184473\n",
      "Epoch 112/300\n",
      "Average training loss: 0.06171247470047739\n",
      "Average test loss: 0.002224734263908532\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06149891999363899\n",
      "Average test loss: 0.00195352927967906\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06098286488983366\n",
      "Average test loss: 0.001979994916667541\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06093115026421017\n",
      "Average test loss: 0.0019019359682376186\n",
      "Epoch 117/300\n",
      "Average training loss: 0.060627416716681586\n",
      "Average test loss: 0.0018975407423244583\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06038404388560189\n",
      "Average test loss: 0.001971816880007585\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05992269067631827\n",
      "Average test loss: 0.002515074695770939\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06009223957525359\n",
      "Average test loss: 0.001931579661452108\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05987463838524289\n",
      "Average test loss: 0.00262281707715657\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0596499614516894\n",
      "Average test loss: 0.001976473943123387\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05917286501990424\n",
      "Average test loss: 0.002028394962557488\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05902337214019564\n",
      "Average test loss: 0.001976915376674798\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0593195890850491\n",
      "Average test loss: 0.0023504190197628405\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05847045133842362\n",
      "Average test loss: 0.0019416296484155787\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05859276024169392\n",
      "Average test loss: 0.0019234075515220563\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05889635683099429\n",
      "Average test loss: 0.0019489917355693049\n",
      "Epoch 130/300\n",
      "Average training loss: 0.059181250198019876\n",
      "Average test loss: 0.001926727535823981\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05817122580938869\n",
      "Average test loss: 0.001963660608149237\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05814899199538761\n",
      "Average test loss: 0.0019536575955442256\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0580146408478419\n",
      "Average training loss: 0.057663281798362734\n",
      "Average test loss: 0.0023185696190016138\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05712951494918929\n",
      "Average test loss: 0.0034985236397220027\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05770548599296146\n",
      "Average test loss: 0.002045064589733051\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05726915325721105\n",
      "Average test loss: 0.0032862610537558795\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05788740426633093\n",
      "Average test loss: 0.0019600620003831057\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05665640598204401\n",
      "Average test loss: 0.002120289459410641\n",
      "Epoch 141/300\n",
      "Average training loss: 0.056728183448314665\n",
      "Average test loss: 0.0019609100702736113\n",
      "Epoch 142/300\n",
      "Average training loss: 0.056432402494880886\n",
      "Average test loss: 0.009808248112392094\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05725094203485383\n",
      "Average test loss: 0.0019589585970259376\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05714042357934846\n",
      "Average test loss: 0.002083902152462138\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05617610112163755\n",
      "Average test loss: 0.0022869757182068296\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05635527327656746\n",
      "Average test loss: 0.003907332898014122\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05584905751215087\n",
      "Average test loss: 0.0020706237155116267\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05582703389392959\n",
      "Average test loss: 0.002266657521534297\n",
      "Epoch 149/300\n",
      "Average training loss: 0.056036652316649754\n",
      "Average test loss: 0.0022606258125354847\n",
      "Epoch 152/300\n",
      "Average training loss: 0.055336997598409654\n",
      "Average test loss: 0.0019679050891556675\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05553225528200467\n",
      "Average test loss: 0.002101394717271129\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05540932522217433\n",
      "Average test loss: 0.002007883969694376\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05550138430131806\n",
      "Average test loss: 0.002067947458061907\n",
      "Epoch 156/300\n",
      "Average training loss: 0.055090337574481965\n",
      "Average test loss: 0.002114073556330469\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05804903683066368\n",
      "Average test loss: 0.0030569329528758924\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05547642099195056\n",
      "Average test loss: 0.0020507626874993246\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05490787652134895\n",
      "Average test loss: 0.001999175671384566\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05494171523385578\n",
      "Average test loss: 0.0020599145715435347\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05449960491557916\n",
      "Average test loss: 0.002044333709921274\n",
      "Epoch 162/300\n",
      "Average training loss: 0.054466201537185246\n",
      "Average test loss: 0.002002399295878907\n",
      "Epoch 163/300\n",
      "Average training loss: 0.055303732057412465\n",
      "Average test loss: 0.00214999377520548\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05472187229328685\n",
      "Average test loss: 0.002102929043997493\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05426398168669807\n",
      "Average test loss: 0.002583566569723189\n",
      "Epoch 166/300\n",
      "Average training loss: 0.054296564592255486\n",
      "Average test loss: 0.0020225447205205757\n",
      "Epoch 168/300\n",
      "Average training loss: 0.054766277184089024\n",
      "Average test loss: 0.0244072558482488\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05397296779354414\n",
      "Average test loss: 0.0021068819873034956\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05376320808463626\n",
      "Average test loss: 0.0021926194874880216\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05529273770915137\n",
      "Average test loss: 0.0022632537805992695\n",
      "Epoch 172/300\n",
      "Average training loss: 0.053605806324217055\n",
      "Average test loss: 0.0023703192826877864\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05366869157883856\n",
      "Average test loss: 0.0022015955737895434\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05769359741608302\n",
      "Average test loss: 0.002573556237336662\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05380348589022954\n",
      "Average test loss: 0.002117700399003095\n",
      "Epoch 176/300\n",
      "Average training loss: 0.053316885107093384\n",
      "Average test loss: 0.0021065255738794805\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05349153813388612\n",
      "Average test loss: 0.001960319070559409\n",
      "Epoch 178/300\n",
      "Average training loss: 0.053715477304326166\n",
      "Average test loss: 0.0021318646857721936\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0539701572921541\n",
      "Average test loss: 0.0023886695510397355\n",
      "Epoch 180/300\n",
      "Average training loss: 0.053361275219255024\n",
      "Average test loss: 0.005257875761845046\n",
      "Epoch 182/300\n",
      "Average training loss: 0.053063678976562285\n",
      "Average test loss: 0.02722607056186017\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05363473532597224\n",
      "Average test loss: 0.002554344008159306\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05284651255276468\n",
      "Average test loss: 0.0021537641264084313\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05326450424724155\n",
      "Average test loss: 0.0020056615700531338\n",
      "Epoch 186/300\n",
      "Average training loss: 0.056212880303462345\n",
      "Average test loss: 0.003979782212111685\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05615351561042998\n",
      "Average test loss: 0.2993332977162467\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05343404788772265\n",
      "Average test loss: 0.002287376091608571\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05258719142609172\n",
      "Average test loss: 0.008131396751436922\n",
      "Epoch 190/300\n",
      "Average training loss: 0.052488282008303536\n",
      "Average test loss: 0.002014361152321928\n",
      "Epoch 191/300\n",
      "Average training loss: 0.061134579118755125\n",
      "Average test loss: 0.002107134307631188\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05284088883135054\n",
      "Average test loss: 0.0021559755576567516\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05213336456815402\n",
      "Average test loss: 0.002180528191332188\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05240001119507684\n",
      "Average test loss: 0.0026616298653599287\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05244035820497407\n",
      "Average test loss: 0.0025985949070503316\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05239561538563834\n",
      "Average test loss: 0.0021575979884299965\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05330095261335373\n",
      "Average test loss: 0.003617693420706524\n",
      "Epoch 199/300\n",
      "Average training loss: 0.052251748568481866\n",
      "Average test loss: 0.0024026667417751417\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05222227734658453\n",
      "Average test loss: 0.0020789816085663108\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05308692564235793\n",
      "Average test loss: 0.002020470576567782\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0519960094889005\n",
      "Average test loss: 0.002153382983783053\n",
      "Epoch 203/300\n",
      "Average training loss: 0.052452832311391834\n",
      "Average test loss: 0.004166466368155347\n",
      "Epoch 204/300\n",
      "Average training loss: 0.052052002671692105\n",
      "Average test loss: 0.0020901185952954824\n",
      "Epoch 205/300\n",
      "Average training loss: 0.052106759121020634\n",
      "Average test loss: 0.0021841088653438622\n",
      "Epoch 206/300\n",
      "Average training loss: 0.052186671220593984\n",
      "Average test loss: 0.0025907978028472927\n",
      "Epoch 207/300\n",
      "Average training loss: 0.053927415056361096\n",
      "Average test loss: 0.002481033586172594\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05170364986525641\n",
      "Average test loss: 0.0030291458941582175\n",
      "Epoch 209/300\n",
      "Average training loss: 0.051806639398137726\n",
      "Average test loss: 0.002087493746024039\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05205688705709245\n",
      "Average test loss: 0.00277563467528671\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05181455519795418\n",
      "Average test loss: 0.0020927448742505575\n",
      "Epoch 228/300\n",
      "Average training loss: 0.050765502399868434\n",
      "Average test loss: 0.002158234899863601\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05119406092829174\n",
      "Average test loss: 0.0020576983829960226\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05149916248851352\n",
      "Average test loss: 0.002139239211980667\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05119388472371631\n",
      "Average test loss: 0.002100296945000688\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05072470813658502\n",
      "Average test loss: 0.0022207513942072788\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05187814582056469\n",
      "Average test loss: 0.0022723780152284438\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05084649907880359\n",
      "Average test loss: 0.0020981004927307367\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05076174607872963\n",
      "Average test loss: 0.0020560715662108525\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05111185547047191\n",
      "Average test loss: 0.0022195706608601744\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05155355085929235\n",
      "Average test loss: 0.0021033402420580385\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05079539069202211\n",
      "Average test loss: 0.002139233142344488\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05086150240566995\n",
      "Average test loss: 0.002161860398327311\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05042059359285567\n",
      "Average test loss: 0.0021111148194306426\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05092958149645064\n",
      "Average test loss: 0.002310580268709196\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0508447792298264\n",
      "Average test loss: 0.0023846396311289733\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0506836508611838\n",
      "Average test loss: 0.0021168481295721396\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05060975689358181\n",
      "Average test loss: 0.0022601295372264253\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0505867120789157\n",
      "Average test loss: 0.002419360282520453\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05069601780838436\n",
      "Average test loss: 0.0020345721897772617\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05033597538206312\n",
      "Average test loss: 0.0027209197766044075\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05099732793702019\n",
      "Average test loss: 0.002110921277768082\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05126368142498864\n",
      "Average test loss: 0.002112355522811413\n",
      "Epoch 251/300\n",
      "Average training loss: 0.050097772823439704\n",
      "Average test loss: 0.002092588812423249\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05024693682127529\n",
      "Average test loss: 0.002094618012714717\n",
      "Epoch 253/300\n",
      "Average training loss: 0.050244896852307847\n",
      "Average test loss: 0.0021272118530339663\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05056948589947489\n",
      "Average test loss: 0.002249221658748057\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0517647996213701\n",
      "Average test loss: 0.0021076372805982828\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04998230497042338\n",
      "Average test loss: 0.0022136502349749208\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05015110513899061\n",
      "Average test loss: 0.0021094713024795053\n",
      "Epoch 259/300\n",
      "Average training loss: 0.050396080791950223\n",
      "Average test loss: 0.0023033873602333996\n",
      "Epoch 262/300\n",
      "Average training loss: 0.050142379416359795\n",
      "Average test loss: 0.0020713488049805165\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04978808003664017\n",
      "Average test loss: 0.002121649454244309\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05166923978593614\n",
      "Average test loss: 0.0020838222367068134\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04995596164796087\n",
      "Average test loss: 0.002147355968546536\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04971656783090697\n",
      "Average test loss: 0.002087354796214236\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04984912481572893\n",
      "Average test loss: 0.0021805701286842427\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04987803453538153\n",
      "Average test loss: 0.0020971297381652725\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05028041641248597\n",
      "Average test loss: 0.002612103699809975\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0501603115995725\n",
      "Average test loss: 0.002188138481022583\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04967626279592514\n",
      "Average test loss: 0.0025985067989677192\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05068738364179929\n",
      "Average test loss: 0.0020746921083579463\n",
      "Epoch 275/300\n",
      "Average training loss: 0.049647776977883445\n",
      "Average test loss: 0.002160002555905117\n",
      "Epoch 276/300\n",
      "Average training loss: 0.049571561853090924\n",
      "Average test loss: 0.002138026891793642\n",
      "Epoch 277/300\n",
      "Average training loss: 0.049784800042708716\n",
      "Average test loss: 0.002530793426765336\n",
      "Epoch 278/300\n",
      "Average training loss: 0.050186253087388145\n",
      "Average test loss: 0.00215968979936507\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04959178557991981\n",
      "Average test loss: 0.002329380622340573\n",
      "Epoch 280/300\n",
      "Average training loss: 0.050176354944705966\n",
      "Average test loss: 0.00207668871499805\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04943237868613667\n",
      "Average test loss: 0.002120131015156706\n",
      "Epoch 282/300\n",
      "Average training loss: 0.050073023405339985\n",
      "Average test loss: 0.0032585539852993356\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04994251706533962\n",
      "Average test loss: 0.0021632904650436507\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04919499418139458\n",
      "Average test loss: 0.0020852257304100528\n",
      "Epoch 285/300\n",
      "Average training loss: 0.049335107243723336\n",
      "Average test loss: 0.002137065787903137\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04967887999945217\n",
      "Average test loss: 0.0040581977081795536\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04945398877064387\n",
      "Average test loss: 0.002288105643871758\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0502147156463729\n",
      "Average test loss: 0.002644717607854141\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04909057586391767\n",
      "Average test loss: 0.002193620515987277\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04927155247992939\n",
      "Average test loss: 0.0021182442406813304\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05238856781688001\n",
      "Average test loss: 0.0024414389719151787\n",
      "Epoch 292/300\n",
      "Average training loss: 0.049106028583314684\n",
      "Average test loss: 0.004242300352288617\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0489984331826369\n",
      "Average test loss: 2.8238411904970806\n",
      "Epoch 294/300\n",
      "Average training loss: 0.0516264942155944\n",
      "Average test loss: 0.002460255349158413\n",
      "Epoch 295/300\n",
      "Average training loss: 0.049275507198439705\n",
      "Average test loss: 0.0020949551527284912\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04899058954583274\n",
      "Average test loss: 0.005452649745365812\n",
      "Epoch 297/300\n",
      "Average training loss: 0.049085028575526345\n",
      "Average test loss: 0.00212433355094658\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04933234180344476\n",
      "Average test loss: 0.002236996359502276\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04922850628362761\n",
      "Average test loss: 0.0021454209737065766\n",
      "Epoch 300/300\n",
      "Average training loss: 0.049122673253218334\n",
      "Average test loss: 0.0021418691751443676\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 684.8526222169664\n",
      "Average test loss: 1290.5559037894707\n",
      "Epoch 2/300\n",
      "Average training loss: 13.042477879842123\n",
      "Average test loss: 38.90031319087082\n",
      "Epoch 3/300\n",
      "Average training loss: 9.643579411824545\n",
      "Average test loss: 0.15050162618524499\n",
      "Epoch 4/300\n",
      "Average training loss: 8.372105776468914\n",
      "Average test loss: 0.00836426516291168\n",
      "Epoch 5/300\n",
      "Average training loss: 7.410598745134141\n",
      "Average test loss: 5.642920494653699\n",
      "Epoch 6/300\n",
      "Average training loss: 6.118624307420519\n",
      "Average test loss: 7.127093967862013\n",
      "Epoch 7/300\n",
      "Average training loss: 5.398773279401991\n",
      "Average test loss: 0.006242623419811328\n",
      "Epoch 8/300\n",
      "Average training loss: 4.798251626756456\n",
      "Average test loss: 0.006192121013585064\n",
      "Epoch 9/300\n",
      "Average training loss: 4.232111029307047\n",
      "Average test loss: 0.01927146776351664\n",
      "Epoch 10/300\n",
      "Average training loss: 3.8306764549679224\n",
      "Average test loss: 0.004282031948781676\n",
      "Epoch 11/300\n",
      "Average training loss: 3.2400602192348904\n",
      "Average test loss: 0.11393863335582945\n",
      "Epoch 12/300\n",
      "Average training loss: 2.8994861727820505\n",
      "Average test loss: 0.004114557076245546\n",
      "Epoch 13/300\n",
      "Average training loss: 2.5434511831071642\n",
      "Average test loss: 0.009356015523895622\n",
      "Epoch 14/300\n",
      "Average training loss: 2.2266837095684475\n",
      "Average test loss: 0.003586571735640367\n",
      "Epoch 15/300\n",
      "Average training loss: 1.9591351659562852\n",
      "Average test loss: 0.003334554309853249\n",
      "Epoch 16/300\n",
      "Average training loss: 1.7209135666953193\n",
      "Average test loss: 0.0036058372230165533\n",
      "Epoch 17/300\n",
      "Average training loss: 1.513681233935886\n",
      "Average test loss: 0.003146261876448989\n",
      "Epoch 18/300\n",
      "Average training loss: 1.334748903910319\n",
      "Average test loss: 0.0029804930622792905\n",
      "Epoch 19/300\n",
      "Average training loss: 1.1717395054499309\n",
      "Average test loss: 0.0031459108622123796\n",
      "Epoch 20/300\n",
      "Average training loss: 1.0292898524602254\n",
      "Average test loss: 0.002913841648441222\n",
      "Epoch 21/300\n",
      "Average training loss: 0.9042966572443644\n",
      "Average test loss: 0.0027425667966405553\n",
      "Epoch 22/300\n",
      "Average training loss: 0.794907272444831\n",
      "Average test loss: 0.0026514671916762988\n",
      "Epoch 23/300\n",
      "Average training loss: 0.698679522673289\n",
      "Average test loss: 0.0027157766489932936\n",
      "Epoch 24/300\n",
      "Average training loss: 0.614230120393965\n",
      "Average test loss: 0.0025717647034260963\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5399580237070719\n",
      "Average test loss: 0.0024195293341245914\n",
      "Epoch 26/300\n",
      "Average training loss: 0.47399238920211795\n",
      "Average test loss: 0.0024630390936508776\n",
      "Epoch 27/300\n",
      "Average training loss: 0.4165516693856981\n",
      "Average test loss: 0.0022119599061293732\n",
      "Epoch 28/300\n",
      "Average training loss: 0.3660551900333828\n",
      "Average test loss: 0.002692207528692153\n",
      "Epoch 29/300\n",
      "Average training loss: 0.3226366052362654\n",
      "Average test loss: 0.0021497684713039132\n",
      "Epoch 30/300\n",
      "Average training loss: 0.28420979444185895\n",
      "Average test loss: 0.0020943584338658384\n",
      "Epoch 31/300\n",
      "Average training loss: 0.2504760287205378\n",
      "Average test loss: 0.0021526765728162394\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2228825428618325\n",
      "Average test loss: 0.0022765473891049625\n",
      "Epoch 33/300\n",
      "Average training loss: 0.20006850760512881\n",
      "Average test loss: 0.0019089217869978812\n",
      "Epoch 34/300\n",
      "Average training loss: 0.1806680455605189\n",
      "Average test loss: 0.0019889481828237574\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1634755653672748\n",
      "Average test loss: 0.0019036516499602132\n",
      "Epoch 36/300\n",
      "Average training loss: 0.15078502337137858\n",
      "Average test loss: 0.001844708325755265\n",
      "Epoch 37/300\n",
      "Average training loss: 0.13938532960414887\n",
      "Average test loss: 0.002041001948217551\n",
      "Epoch 38/300\n",
      "Average training loss: 0.1331061106522878\n",
      "Average test loss: 0.0024070165670580334\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12110395087136162\n",
      "Average test loss: 0.0017874523611325357\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11481604562865363\n",
      "Average test loss: 0.0023974376186314556\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11319739560948477\n",
      "Average test loss: 0.0073520697591205435\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11979796510272556\n",
      "Average test loss: 0.0020234852135181426\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12342481823762258\n",
      "Average test loss: 0.006850102379918098\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10998899660838975\n",
      "Average test loss: 0.0018254277015932732\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09724404013819164\n",
      "Average test loss: 0.0018658610747920143\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09227537350853285\n",
      "Average test loss: 0.0017850845188109412\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08891368052694533\n",
      "Average test loss: 0.0019402214143839148\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08647831041283077\n",
      "Average test loss: 0.0018304150839232737\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08201992007096609\n",
      "Average test loss: 0.0017482070709682173\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08089370419581732\n",
      "Average test loss: 0.001883126176893711\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07717301098505656\n",
      "Average test loss: 0.0015726189239778452\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07652961148818334\n",
      "Average test loss: 0.0017730372877170642\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0773709126147959\n",
      "Average test loss: 0.0024720615677328575\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0718926385541757\n",
      "Average test loss: 0.0015639006274027957\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07000916488965353\n",
      "Average test loss: 0.0019420226980000734\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06734509281317393\n",
      "Average test loss: 0.0014708437699203689\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06623192951745457\n",
      "Average test loss: 0.20622892045478025\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06386080799831284\n",
      "Average test loss: 0.0014576143751748735\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06316028003229035\n",
      "Average test loss: 0.0015034494174954792\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06192397999101215\n",
      "Average test loss: 0.001465745497494936\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06067544297046132\n",
      "Average test loss: 0.0014404053008183837\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05957833871245384\n",
      "Average test loss: 0.0017500158830856284\n",
      "Epoch 63/300\n",
      "Average training loss: 0.058285759776830676\n",
      "Average test loss: 0.0015456676168574227\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05766449895501137\n",
      "Average test loss: 0.0013643939980409212\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0569158489604791\n",
      "Average test loss: 0.0014391108429473308\n",
      "Epoch 66/300\n",
      "Average training loss: 0.058886830200751625\n",
      "Average test loss: 0.0014838100341666076\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0561738136543168\n",
      "Average test loss: 0.0013612053820656405\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05508368920617633\n",
      "Average test loss: 0.004082761521968577\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05458242858449618\n",
      "Average test loss: 0.0013805548037505813\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05430428655280007\n",
      "Average test loss: 0.0013664496605181032\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05397415766119957\n",
      "Average test loss: 0.0014340122278986706\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05305804794033368\n",
      "Average test loss: 0.0013526753539012538\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05285280070371098\n",
      "Average test loss: 0.0013397966166958212\n",
      "Epoch 74/300\n",
      "Average training loss: 0.052673316028383044\n",
      "Average test loss: 0.001346128208666212\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05182758609784974\n",
      "Average test loss: 0.001343393012467358\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05124010672171911\n",
      "Average test loss: 0.0014606720629251665\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05163537468181716\n",
      "Average test loss: 0.0013496959950360986\n",
      "Epoch 78/300\n",
      "Average training loss: 0.050868068873882295\n",
      "Average test loss: 0.0015199421557287375\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05071697340077824\n",
      "Average test loss: 0.001332556761801243\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05004607923163308\n",
      "Average test loss: 0.0014222693538500203\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04998867994878027\n",
      "Average test loss: 0.0013501689113262626\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04940579747160276\n",
      "Average test loss: 0.0020753192438019647\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04879734616478284\n",
      "Average test loss: 0.00132862581892146\n",
      "Epoch 84/300\n",
      "Average training loss: 0.048914617429176965\n",
      "Average test loss: 0.001456770511292335\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04810492663913303\n",
      "Average test loss: 0.00132461943767137\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04796050464775827\n",
      "Average test loss: 0.0013290561488829553\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0479189848071999\n",
      "Average test loss: 0.002429521908983588\n",
      "Epoch 88/300\n",
      "Average training loss: 0.048593962083260216\n",
      "Average test loss: 0.0025104390105439556\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04732173278265529\n",
      "Average test loss: 0.0016471956678562694\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04690759953194194\n",
      "Average test loss: 0.0013105909195211198\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04705458416541417\n",
      "Average test loss: 0.0019479705935551061\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0464564161433114\n",
      "Average test loss: 0.0017413021590974595\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04642341219385465\n",
      "Average test loss: 0.0013159194675584633\n",
      "Epoch 94/300\n",
      "Average training loss: 0.046202888435787626\n",
      "Average test loss: 10106514.156\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04685686580671204\n",
      "Average test loss: 0.0013445423448251352\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04576970773604181\n",
      "Average test loss: 0.0021916459448014696\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04569241526722908\n",
      "Average test loss: 0.0013131303328813779\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04544158411026001\n",
      "Average test loss: 0.0014981015650555492\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04542185030712022\n",
      "Average test loss: 0.0013533675827913814\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04529543339875009\n",
      "Average test loss: 0.001319270798108644\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04482793842752775\n",
      "Average test loss: 0.003480306493325366\n",
      "Epoch 102/300\n",
      "Average training loss: 0.044807740175061755\n",
      "Average test loss: 0.0024939742723686827\n",
      "Epoch 103/300\n",
      "Average training loss: 0.044822684387365974\n",
      "Average test loss: 0.0014115526592358948\n",
      "Epoch 104/300\n",
      "Average training loss: 0.044332779076364306\n",
      "Average test loss: 0.0014001182335325413\n",
      "Epoch 105/300\n",
      "Average training loss: 0.044341692599985334\n",
      "Average test loss: 0.0030277569774124356\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0450869264933798\n",
      "Average test loss: 0.0015600596244136492\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04376052193509208\n",
      "Average test loss: 0.001366663769663622\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04519684553146362\n",
      "Average test loss: 0.0013752204891708162\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04386683852805032\n",
      "Average test loss: 0.0013542299367901352\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04380251634452078\n",
      "Average test loss: 0.0014675830712334978\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04343124665485488\n",
      "Average test loss: 0.0013757720144672526\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05096154835820198\n",
      "Average test loss: 0.0014565125847649243\n",
      "Epoch 113/300\n",
      "Average training loss: 0.043476817647616066\n",
      "Average test loss: 0.0013671798854031497\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04274911604324977\n",
      "Average test loss: 0.001344014915637672\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04261633010043038\n",
      "Average test loss: 0.024126109801232816\n",
      "Epoch 116/300\n",
      "Average training loss: 0.042594882918728726\n",
      "Average test loss: 0.0013947149271973304\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0429573038385974\n",
      "Average test loss: 0.014416341924212045\n",
      "Epoch 118/300\n",
      "Average training loss: 0.043249656240145366\n",
      "Average test loss: 0.003499872913998034\n",
      "Epoch 119/300\n",
      "Average training loss: 0.043063956671290926\n",
      "Average test loss: 0.01377106629105078\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04290982670254177\n",
      "Average test loss: 0.0031934549101731843\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04260074606206682\n",
      "Average test loss: 0.0014936800674638815\n",
      "Epoch 122/300\n",
      "Average training loss: 0.042336892472373114\n",
      "Average test loss: 0.0014587216650963658\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04207609505785836\n",
      "Average test loss: 0.0016951618616779646\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04249150785472658\n",
      "Average test loss: 0.01277200286835432\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04228381344676018\n",
      "Average test loss: 0.0013609549177086187\n",
      "Epoch 126/300\n",
      "Average training loss: 0.042031345983346305\n",
      "Average test loss: 0.001358461124719017\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04168671922882398\n",
      "Average test loss: 0.012424398352495498\n",
      "Epoch 128/300\n",
      "Average training loss: 0.042567771901686986\n",
      "Average test loss: 0.0013753597371073234\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04154705509212282\n",
      "Average test loss: 0.0018272628516165746\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04137629675865173\n",
      "Average test loss: 0.0014250911047889126\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04249556615286403\n",
      "Average test loss: 0.0014329223768371675\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0411715835498439\n",
      "Average test loss: 0.00141413665883657\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04112660730547375\n",
      "Average test loss: 0.14661509148145302\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04155792564484808\n",
      "Average test loss: 0.001524564530596965\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0433656686147054\n",
      "Average test loss: 0.001344020881690085\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04098671743604872\n",
      "Average test loss: 0.005106902929333349\n",
      "Epoch 137/300\n",
      "Average training loss: 0.040972662034961915\n",
      "Average test loss: 0.01516440272041493\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0411761088139481\n",
      "Average test loss: 0.0014576460699447327\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04198010390996933\n",
      "Average test loss: 0.0013735222230768867\n",
      "Epoch 140/300\n",
      "Average training loss: 0.040595500179462965\n",
      "Average test loss: 0.0013862879828860362\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04079479073484739\n",
      "Average test loss: 0.0013813994541350338\n",
      "Epoch 142/300\n",
      "Average training loss: 0.040624881726172235\n",
      "Average test loss: 0.0043293228801339866\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04043424046701855\n",
      "Average test loss: 0.003934081732398934\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04098725661966536\n",
      "Average test loss: 0.0017273330946659877\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04017868151598507\n",
      "Average test loss: 0.03175598860118124\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04171554715765847\n",
      "Average test loss: 0.002394207839957542\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0407141042318609\n",
      "Average test loss: 0.0013865873372803132\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04009428377615081\n",
      "Average test loss: 0.0035008198000076743\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04004243963956833\n",
      "Average test loss: 0.0013767193738992015\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04064264131916894\n",
      "Average test loss: 0.004137885472840733\n",
      "Epoch 152/300\n",
      "Average training loss: 0.039996180531051424\n",
      "Average test loss: 0.0014615042028534744\n",
      "Epoch 153/300\n",
      "Average training loss: 0.044768246471881865\n",
      "Average test loss: 0.00145769277225352\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04019572320911619\n",
      "Average test loss: 0.0014287354819373123\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0409612412750721\n",
      "Average test loss: 0.002404601938298179\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03963847306701872\n",
      "Average test loss: 0.010801687004044653\n",
      "Epoch 157/300\n",
      "Average training loss: 0.039648899116449886\n",
      "Average test loss: 0.001424629824856917\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04033153636753559\n",
      "Average test loss: 0.0015527709000226525\n",
      "Epoch 159/300\n",
      "Average training loss: 0.039833483586708705\n",
      "Average test loss: 0.002060463514489432\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03964931551615397\n",
      "Average test loss: 37.08583505588108\n",
      "Epoch 161/300\n",
      "Average training loss: 0.039626214674777456\n",
      "Average test loss: 0.0031495154843562178\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03958557721972465\n",
      "Average test loss: 0.008012330061859555\n",
      "Epoch 163/300\n",
      "Average training loss: 0.040219397922356924\n",
      "Average test loss: 0.0014168639887745182\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03935087991423077\n",
      "Average test loss: 0.0014409112588812908\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03973197541468673\n",
      "Average test loss: 0.0014980971209911838\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03950027840005027\n",
      "Average test loss: 0.0014583314429554674\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04256277248925633\n",
      "Average test loss: 0.007190729034443696\n",
      "Epoch 168/300\n",
      "Average training loss: 0.040856658746798835\n",
      "Average test loss: 0.0014191365017452173\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03896368403236071\n",
      "Average test loss: 0.0015857602950806419\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03914734828472138\n",
      "Average test loss: 0.0018608387197471326\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03913301673531532\n",
      "Average test loss: 0.5712371188476681\n",
      "Epoch 172/300\n",
      "Average training loss: 0.039243035386006035\n",
      "Average test loss: 0.0014097734523109264\n",
      "Epoch 173/300\n",
      "Average training loss: 0.038970882687303754\n",
      "Average test loss: 0.0013898382837780649\n",
      "Epoch 174/300\n",
      "Average training loss: 0.038915559576617346\n",
      "Average test loss: 0.0016809948804891771\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04738854949010743\n",
      "Average test loss: 0.0017498395376735264\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0400484829876158\n",
      "Average test loss: 0.0014376884626431598\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03941761703954803\n",
      "Average test loss: 0.002115775976330042\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03861572593119409\n",
      "Average test loss: 0.0014888546338511837\n",
      "Epoch 179/300\n",
      "Average training loss: 0.038667235483725866\n",
      "Average test loss: 0.003689759384840727\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04002305598391427\n",
      "Average test loss: 0.0018144097229879762\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03904246304101414\n",
      "Average test loss: 0.0014113639895286824\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03886531648039818\n",
      "Average test loss: 0.0015467999880719517\n",
      "Epoch 183/300\n",
      "Average training loss: 0.038519489162498054\n",
      "Average test loss: 0.0018027594081229633\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03864782303240564\n",
      "Average test loss: 0.0014645809905810489\n",
      "Epoch 185/300\n",
      "Average training loss: 0.039305609742800396\n",
      "Average test loss: 0.0014263123805738158\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03871972412698799\n",
      "Average test loss: 0.001468381910170946\n",
      "Epoch 187/300\n",
      "Average training loss: 0.039188071843650606\n",
      "Average test loss: 0.0015083701448618538\n",
      "Epoch 188/300\n",
      "Average training loss: 0.038606843602326184\n",
      "Average test loss: 0.0021406708646358715\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03872402362359895\n",
      "Average test loss: 0.0016824258213034935\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03837368548909823\n",
      "Average test loss: 0.0018705968187294072\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06261080235905117\n",
      "Average test loss: 0.001343648315096895\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04766310842500793\n",
      "Average test loss: 0.0015881283788217438\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04263943256934484\n",
      "Average test loss: 0.0013925797479848066\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04017333784368303\n",
      "Average test loss: 0.0016670557541979684\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04104415249493387\n",
      "Average test loss: 0.00970742725332578\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03998774039745331\n",
      "Average test loss: 0.0016299568547142877\n",
      "Epoch 197/300\n",
      "Average training loss: 0.038967808849281735\n",
      "Average test loss: 0.0014098018032188218\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03817148378160265\n",
      "Average test loss: 0.0014607900580805209\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0382778157989184\n",
      "Average test loss: 51.51820557242963\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03827540873156653\n",
      "Average test loss: 0.0037415420611699424\n",
      "Epoch 201/300\n",
      "Average training loss: 0.038458943833907445\n",
      "Average test loss: 0.001449443856978582\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03907041677832603\n",
      "Average test loss: 0.001988953974718849\n",
      "Epoch 203/300\n",
      "Average training loss: 0.038082278167208036\n",
      "Average test loss: 0.0014567428636364638\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0380659680697653\n",
      "Average test loss: 0.6312042387657696\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03958162463042471\n",
      "Average test loss: 0.0014312927796919313\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04099195921421051\n",
      "Average test loss: 0.0017061024243012072\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03812323612802559\n",
      "Average test loss: 0.0014383690057115422\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03798143523600366\n",
      "Average test loss: 0.0014682128888865312\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03796056957377328\n",
      "Average test loss: 0.0014351768271169728\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03821647970875104\n",
      "Average test loss: 0.0017689495678577158\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03779496132334073\n",
      "Average test loss: 0.0014773643240332604\n",
      "Epoch 212/300\n",
      "Average training loss: 0.039600211411714555\n",
      "Average test loss: 0.0015144323885647787\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03881342585881551\n",
      "Average test loss: 0.2697060685687595\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03799062561988831\n",
      "Average test loss: 0.0014165316574896376\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03779388159182336\n",
      "Average test loss: 0.001544948728962077\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03771823850274086\n",
      "Average test loss: 0.001445603664809217\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03806210303306579\n",
      "Average test loss: 0.0014295865884050728\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0380991362730662\n",
      "Average test loss: 0.001935476202207307\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03843729438053237\n",
      "Average test loss: 0.0015402688816603686\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03785802085366514\n",
      "Average test loss: 0.0015419639525935053\n",
      "Epoch 221/300\n",
      "Average training loss: 0.037871932556231815\n",
      "Average test loss: 0.0014187132084431747\n",
      "Epoch 222/300\n",
      "Average training loss: 0.037913284771972235\n",
      "Average test loss: 0.0016509578420470159\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03745731583237648\n",
      "Average test loss: 0.001588595548644662\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03863982273803817\n",
      "Average test loss: 0.0064087434829109245\n",
      "Epoch 225/300\n",
      "Average training loss: 0.037703856199979784\n",
      "Average test loss: 0.0015412389576021169\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0378852759665913\n",
      "Average test loss: 0.0014848617003816698\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03779845463898447\n",
      "Average test loss: 0.0015253503796541028\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03765987518429756\n",
      "Average test loss: 0.0014977722915096416\n",
      "Epoch 229/300\n",
      "Average training loss: 0.038266821791728335\n",
      "Average test loss: 0.001396770663248996\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0376347300161918\n",
      "Average test loss: 0.0014370801963636444\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03746811384956042\n",
      "Average test loss: 0.0017004307988617154\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03769185855322414\n",
      "Average test loss: 0.001498636017036107\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04069664590557416\n",
      "Average test loss: 0.0014319550935178995\n",
      "Epoch 234/300\n",
      "Average training loss: 0.037169993221759796\n",
      "Average test loss: 0.001776101372204721\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03750951734185219\n",
      "Average test loss: 0.004182276466033525\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03724709004163742\n",
      "Average test loss: 0.0016032354418809216\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0371666594809956\n",
      "Average test loss: 0.0016983298377858268\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03838157874676916\n",
      "Average test loss: 0.0014671046967721648\n",
      "Epoch 239/300\n",
      "Average training loss: 0.037283403696285355\n",
      "Average test loss: 5482.333045879561\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03706954831547207\n",
      "Average test loss: 0.001446934705393182\n",
      "Epoch 241/300\n",
      "Average training loss: 0.037201366444428764\n",
      "Average test loss: 0.0015026161743121014\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03794805960688326\n",
      "Average test loss: 0.0014613441923219297\n",
      "Epoch 243/300\n",
      "Average training loss: 0.037184234622451996\n",
      "Average test loss: 0.0022700153447480667\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03705729804270797\n",
      "Average test loss: 0.0014650127820463644\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03694494916333092\n",
      "Average test loss: 0.0015587788704368804\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03779137258397208\n",
      "Average test loss: 0.001981943367256059\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03718419182962841\n",
      "Average test loss: 0.0015582635757616825\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03693480458193355\n",
      "Average test loss: 0.0017874327347510391\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03705820961793264\n",
      "Average test loss: 0.0016290705010501875\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03698152571585443\n",
      "Average test loss: 0.0014592717214384013\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03957532006502151\n",
      "Average test loss: 0.0015368738847060337\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06394671269257864\n",
      "Average test loss: 0.0013457348838241563\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04060412491692437\n",
      "Average test loss: 0.0014028105789588557\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03760868834455808\n",
      "Average test loss: 0.0014580101624338164\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03673934371934997\n",
      "Average test loss: 0.013603716225259833\n",
      "Epoch 256/300\n",
      "Average training loss: 0.036542259203063115\n",
      "Average test loss: 0.001577961943215794\n",
      "Epoch 257/300\n",
      "Average training loss: 0.039066692183415096\n",
      "Average test loss: 0.001439682839334839\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03648063141273128\n",
      "Average test loss: 0.0014494218334762587\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03710975776281622\n",
      "Average test loss: 0.002097472231214245\n",
      "Epoch 260/300\n",
      "Average training loss: 0.036568072335587606\n",
      "Average test loss: 0.001494105090490646\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03693622686134444\n",
      "Average test loss: 0.0014332226518955495\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0374388712214099\n",
      "Average test loss: 0.0014972122463708124\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03678242427441809\n",
      "Average test loss: 0.0016754996263318592\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0374383162856102\n",
      "Average test loss: 0.0018039764719497827\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03700037639339765\n",
      "Average test loss: 0.0015128080522020658\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03659380197028319\n",
      "Average test loss: 0.001602382969007724\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03764066035548846\n",
      "Average test loss: 0.001504565626486308\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03718364764584435\n",
      "Average test loss: 13879.222782118055\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03661177788178126\n",
      "Average test loss: 0.0015438495369421111\n",
      "Epoch 270/300\n",
      "Average training loss: 0.037190980772177376\n",
      "Average test loss: 0.0020641023493889305\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03731355680690871\n",
      "Average test loss: 0.0014632149005515708\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03640609582927492\n",
      "Average test loss: 0.0015084417004966075\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03659436810678906\n",
      "Average test loss: 0.0015431139605740706\n",
      "Epoch 274/300\n",
      "Average training loss: 0.036681791176398595\n",
      "Average test loss: 0.0015065743767966827\n",
      "Epoch 275/300\n",
      "Average training loss: 0.037610192520750894\n",
      "Average test loss: 0.0022118128368424045\n",
      "Epoch 276/300\n",
      "Average training loss: 0.036398779799540835\n",
      "Average test loss: 0.0014305257047009137\n",
      "Epoch 277/300\n",
      "Average training loss: 0.037261768523189756\n",
      "Average test loss: 0.0014913750139272047\n",
      "Epoch 278/300\n",
      "Average training loss: 0.036401659157541066\n",
      "Average test loss: 0.0014532777114460865\n",
      "Epoch 279/300\n",
      "Average training loss: 0.036411077903376686\n",
      "Average test loss: 0.0014576015963426066\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06240038251545694\n",
      "Average test loss: 0.001383727551644875\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04356701501376099\n",
      "Average test loss: 0.0013965039325671063\n",
      "Epoch 282/300\n",
      "Average training loss: 0.039560355032483736\n",
      "Average test loss: 27092.89314561632\n",
      "Epoch 283/300\n",
      "Average training loss: 0.037888887163665556\n",
      "Average test loss: 0.0014427015783472193\n",
      "Epoch 284/300\n",
      "Average training loss: 0.037254473540518016\n",
      "Average test loss: 0.0014472888654304876\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0368312675886684\n",
      "Average test loss: 0.0015356992373449934\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03655523194207085\n",
      "Average test loss: 0.0015151315558080871\n",
      "Epoch 287/300\n",
      "Average training loss: 0.036415573047267065\n",
      "Average test loss: 0.0014610193578733339\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03676385692258676\n",
      "Average test loss: 0.002163141505792737\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03655385931001769\n",
      "Average test loss: 0.0014725428451266554\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03659589343600803\n",
      "Average test loss: 0.0015064038623952202\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03684830122854975\n",
      "Average test loss: 0.017697894420060846\n",
      "Epoch 292/300\n",
      "Average training loss: 0.036333849297629464\n",
      "Average test loss: 0.0014405226798521148\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03738752029505041\n",
      "Average test loss: 0.001508487926175197\n",
      "Epoch 294/300\n",
      "Average training loss: 0.037811968432532414\n",
      "Average test loss: 0.0015456782241041462\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0360581208633052\n",
      "Average test loss: 0.0014924443809108603\n",
      "Epoch 296/300\n",
      "Average training loss: 0.036111520363224875\n",
      "Average test loss: 0.001445242426616864\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0370747701757484\n",
      "Average test loss: 0.0019074592845928337\n",
      "Epoch 298/300\n",
      "Average training loss: 0.036099250730541016\n",
      "Average test loss: 0.03684459777341949\n",
      "Epoch 299/300\n",
      "Average training loss: 0.036184422542651494\n",
      "Average test loss: 0.002487358448613021\n",
      "Epoch 300/300\n",
      "Average training loss: 0.037632341461049185\n",
      "Average test loss: 0.0014668533882747094\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive-.01/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.52\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.27\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.99\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.68\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.14\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.41\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.63\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.81\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.90\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.11\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.13\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.22\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.32\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.43\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.52\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.62\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.68\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.78\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.73\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.81\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.09\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 21.54\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.15\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.52\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.63\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.97\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.64\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.14\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.46\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.99\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.07\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.35\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.59\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.62\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.71\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 31.02\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.21\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.59\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.13\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.62\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.31\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.57\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.87\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.58\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.90\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.00\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.77\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.03\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.23\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.32\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.45\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.08\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.17\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.46\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.67\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.68\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.81\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
