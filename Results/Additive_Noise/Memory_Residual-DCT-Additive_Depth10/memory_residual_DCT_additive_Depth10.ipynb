{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.05)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.05)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.05)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.05)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11847397464844915\n",
      "Average test loss: 0.005280228657027086\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03083330227682988\n",
      "Average test loss: 0.004900587577372789\n",
      "Epoch 3/300\n",
      "Average training loss: 0.026602168695794213\n",
      "Average test loss: 0.004820007323804829\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02503612716661559\n",
      "Average test loss: 0.004585843668422765\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0242237798207336\n",
      "Average test loss: 0.004546706678966681\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02374621006515291\n",
      "Average test loss: 0.0044803112517628405\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0234045856313573\n",
      "Average test loss: 0.004481906333317359\n",
      "Epoch 8/300\n",
      "Average training loss: 0.023112048589520984\n",
      "Average test loss: 0.004432597273753749\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022899824172258377\n",
      "Average test loss: 0.004449394560315543\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022706423012746706\n",
      "Average test loss: 0.00439013988773028\n",
      "Epoch 11/300\n",
      "Average training loss: 0.022527405336499214\n",
      "Average test loss: 0.004368281485719813\n",
      "Epoch 12/300\n",
      "Average training loss: 0.022369409716791577\n",
      "Average test loss: 0.004346803267796834\n",
      "Epoch 13/300\n",
      "Average training loss: 0.022242480588455994\n",
      "Average test loss: 0.004337547205802467\n",
      "Epoch 14/300\n",
      "Average training loss: 0.022118114466468494\n",
      "Average test loss: 0.004320885904133319\n",
      "Epoch 15/300\n",
      "Average training loss: 0.022008980009290906\n",
      "Average test loss: 0.00429890321339998\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021890807294183308\n",
      "Average test loss: 0.004328557465225458\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02177959794137213\n",
      "Average test loss: 0.00431893582124677\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02168598237964842\n",
      "Average test loss: 0.004264978242831098\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021597720545199182\n",
      "Average test loss: 0.004265097803125779\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021494911342859267\n",
      "Average test loss: 0.004246655485696263\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02142323041293356\n",
      "Average test loss: 0.004252855771945583\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021341405833760898\n",
      "Average test loss: 0.004241642182485925\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02128357648021645\n",
      "Average test loss: 0.004236662233041392\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02120669557319747\n",
      "Average test loss: 0.004205479651689529\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021143205561571652\n",
      "Average test loss: 0.004210622177976701\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02108578525814745\n",
      "Average test loss: 0.004203146776598361\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02103727740049362\n",
      "Average test loss: 0.004201928942774733\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020980099452866448\n",
      "Average test loss: 0.004214080518111587\n",
      "Epoch 29/300\n",
      "Average training loss: 0.020923797460065948\n",
      "Average test loss: 0.004230993353244331\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02088012152744664\n",
      "Average test loss: 0.004211336472796069\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02082665909992324\n",
      "Average test loss: 0.004175262023177412\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020767150322596233\n",
      "Average test loss: 0.004175446663879686\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020737449750304223\n",
      "Average test loss: 0.004178168827460872\n",
      "Epoch 34/300\n",
      "Average training loss: 0.02067942470477687\n",
      "Average test loss: 0.004200783090045054\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020653774918781388\n",
      "Average test loss: 0.004179525228424205\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020594430984722245\n",
      "Average test loss: 0.0041861722384476\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02055602240562439\n",
      "Average test loss: 0.004203713756675521\n",
      "Epoch 38/300\n",
      "Average training loss: 0.020506373688578605\n",
      "Average test loss: 0.0041954446277684635\n",
      "Epoch 39/300\n",
      "Average training loss: 0.020468884527683258\n",
      "Average test loss: 0.004170336994032065\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02043269463380178\n",
      "Average test loss: 0.00416511507332325\n",
      "Epoch 41/300\n",
      "Average training loss: 0.020386345919635562\n",
      "Average test loss: 0.0042312800120562315\n",
      "Epoch 42/300\n",
      "Average training loss: 0.020355573534965515\n",
      "Average test loss: 0.004188060305184788\n",
      "Epoch 43/300\n",
      "Average training loss: 0.020305335713757408\n",
      "Average test loss: 0.00416865934100416\n",
      "Epoch 44/300\n",
      "Average training loss: 0.020265163219637342\n",
      "Average test loss: 0.004213251249243816\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02021013112531768\n",
      "Average test loss: 0.00417183738698562\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020185111863745584\n",
      "Average test loss: 0.004199798055820995\n",
      "Epoch 47/300\n",
      "Average training loss: 0.020136000560389625\n",
      "Average test loss: 0.004207145607305897\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02011250867280695\n",
      "Average test loss: 0.004282811069447133\n",
      "Epoch 49/300\n",
      "Average training loss: 0.020062439678443802\n",
      "Average test loss: 0.004481912221966518\n",
      "Epoch 50/300\n",
      "Average training loss: 0.020020209247039424\n",
      "Average test loss: 0.004203636852196521\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019964349849356546\n",
      "Average test loss: 0.004205379579009281\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019932186086972555\n",
      "Average test loss: 0.004182632476712266\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019890896641545825\n",
      "Average test loss: 0.004237285003479984\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01984960220588578\n",
      "Average test loss: 0.004210829734802246\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019808504667547015\n",
      "Average test loss: 0.00419817019212577\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019773915694819556\n",
      "Average test loss: 0.0042351584155112506\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01972844531801012\n",
      "Average test loss: 0.00418746843892667\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019679818491141\n",
      "Average test loss: 0.004215054343144099\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01964070063498285\n",
      "Average test loss: 0.0042532294616103175\n",
      "Epoch 60/300\n",
      "Average training loss: 0.01961203631013632\n",
      "Average test loss: 0.004246315347237719\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01956039001709885\n",
      "Average test loss: 0.004275925943420993\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01954129022028711\n",
      "Average test loss: 0.00425066748364932\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019519201993942262\n",
      "Average test loss: 0.004259737820261055\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01944664020670785\n",
      "Average test loss: 0.004250009107093017\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01941103152765168\n",
      "Average test loss: 0.004217464317878087\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019378847031129732\n",
      "Average test loss: 0.0042688754349946975\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019329081109000577\n",
      "Average test loss: 0.004353031191147036\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019336384566293822\n",
      "Average test loss: 0.004250038717769914\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0192764811043938\n",
      "Average test loss: 0.004387391014231576\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019221984146369827\n",
      "Average test loss: 0.004256682946864101\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019205771661467023\n",
      "Average test loss: 0.004271056918220387\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019170588405595886\n",
      "Average test loss: 0.0042683363395432635\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019117949816915725\n",
      "Average test loss: 0.004315667709542646\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019098582891954315\n",
      "Average test loss: 0.004317872917486562\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019063737606008848\n",
      "Average test loss: 0.004267125126388338\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019041148694025147\n",
      "Average test loss: 0.004275321519209279\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019023085113200878\n",
      "Average test loss: 0.004404988296743896\n",
      "Epoch 78/300\n",
      "Average training loss: 0.018959969833493233\n",
      "Average test loss: 0.004329247852994336\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01894110610915555\n",
      "Average test loss: 0.004288718769533767\n",
      "Epoch 80/300\n",
      "Average training loss: 0.018897221835123167\n",
      "Average test loss: 0.004281153721734881\n",
      "Epoch 81/300\n",
      "Average training loss: 0.018872345677680438\n",
      "Average test loss: 0.00429752493939466\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0188539050453239\n",
      "Average test loss: 0.004280363309300608\n",
      "Epoch 83/300\n",
      "Average training loss: 0.018812438901927735\n",
      "Average test loss: 0.004275169768681129\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01879783547586865\n",
      "Average test loss: 0.004402752300517426\n",
      "Epoch 85/300\n",
      "Average training loss: 0.018757697681585948\n",
      "Average test loss: 0.004354341985657811\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018737332145373025\n",
      "Average test loss: 0.004285632068705227\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018695899532900916\n",
      "Average test loss: 0.004342267266164223\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01866907862159941\n",
      "Average test loss: 0.004352361737853951\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01864572411113315\n",
      "Average test loss: 0.004295429512030549\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018605367557042174\n",
      "Average test loss: 0.004437240176523726\n",
      "Epoch 91/300\n",
      "Average training loss: 0.018600522393981616\n",
      "Average test loss: 0.004382495026000672\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018563538713587655\n",
      "Average test loss: 0.004380332611087296\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018541114919715457\n",
      "Average test loss: 0.004336815348929829\n",
      "Epoch 94/300\n",
      "Average training loss: 0.018520042932695813\n",
      "Average test loss: 0.004382566066458821\n",
      "Epoch 95/300\n",
      "Average training loss: 0.018498408690094947\n",
      "Average test loss: 0.004325647818131579\n",
      "Epoch 96/300\n",
      "Average training loss: 0.018452859674890836\n",
      "Average test loss: 0.004339836995220846\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01843070482628213\n",
      "Average test loss: 0.00435503926500678\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018405299739705193\n",
      "Average test loss: 0.0044193285351826084\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018398197337157196\n",
      "Average test loss: 0.0043689740875528914\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01836503156605694\n",
      "Average test loss: 0.00443304748667611\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018349845493833224\n",
      "Average test loss: 0.004338158047033681\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01833273851292001\n",
      "Average test loss: 0.004389483125052518\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01829824340509044\n",
      "Average test loss: 0.004353974445412557\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018280347999599246\n",
      "Average test loss: 0.004465434419198168\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01823955314192507\n",
      "Average test loss: 0.004440854363143444\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018226913895871905\n",
      "Average test loss: 0.004463076234691673\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01822438236574332\n",
      "Average test loss: 0.0044935538350707955\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018191770704256163\n",
      "Average test loss: 0.004391187768843439\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01817581253581577\n",
      "Average test loss: 0.0043828513936864\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018147693251570066\n",
      "Average test loss: 0.004445530929499202\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018117881875899102\n",
      "Average test loss: 0.004534729410790735\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018098543537987604\n",
      "Average test loss: 0.004349518847134378\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01810070784555541\n",
      "Average test loss: 0.0044928960276560655\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01805635175274478\n",
      "Average test loss: 0.004331224734998412\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018063567790720197\n",
      "Average test loss: 0.0044900656686060955\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018029517927103573\n",
      "Average test loss: 0.004483181149595314\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018005430392093128\n",
      "Average test loss: 0.004452349277627137\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01799099766711394\n",
      "Average test loss: 0.004386212022768126\n",
      "Epoch 119/300\n",
      "Average training loss: 0.017959009105960527\n",
      "Average test loss: 0.004457032825797796\n",
      "Epoch 120/300\n",
      "Average training loss: 0.017971217091712687\n",
      "Average test loss: 0.004479216729601224\n",
      "Epoch 121/300\n",
      "Average training loss: 0.017865793138742446\n",
      "Average test loss: 0.004508784297646748\n",
      "Epoch 126/300\n",
      "Average training loss: 0.017844990466203955\n",
      "Average test loss: 0.004548701955212487\n",
      "Epoch 127/300\n",
      "Average training loss: 0.017838762551546098\n",
      "Average test loss: 0.004473065399047401\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01781519616312451\n",
      "Average test loss: 0.004511340090384086\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0177934871489803\n",
      "Average test loss: 0.004503189149830077\n",
      "Epoch 130/300\n",
      "Average training loss: 0.017767350572678777\n",
      "Average test loss: 0.004390255462378263\n",
      "Epoch 131/300\n",
      "Average training loss: 0.017775881876548133\n",
      "Average test loss: 0.004450673115336232\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01776236135429806\n",
      "Average test loss: 0.004407130914429824\n",
      "Epoch 133/300\n",
      "Average training loss: 0.017724947418603632\n",
      "Average test loss: 0.004502590918292602\n",
      "Epoch 134/300\n",
      "Average training loss: 0.017713262210289637\n",
      "Average test loss: 0.0044719686417116055\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01768489979704221\n",
      "Average test loss: 0.004527588232937786\n",
      "Epoch 137/300\n",
      "Average training loss: 0.017680274435215526\n",
      "Average test loss: 0.004393066854112678\n",
      "Epoch 138/300\n",
      "Average training loss: 0.017654193287094434\n",
      "Average test loss: 0.004481164106892215\n",
      "Epoch 139/300\n",
      "Average training loss: 0.017663082995348505\n",
      "Average test loss: 0.0044760122288846306\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01756925233370728\n",
      "Average test loss: 0.004520344723429945\n",
      "Epoch 145/300\n",
      "Average training loss: 0.017551445364952088\n",
      "Average test loss: 0.004409953737010559\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01753908607446485\n",
      "Average test loss: 0.004554904819776615\n",
      "Epoch 147/300\n",
      "Average training loss: 0.017557119854622417\n",
      "Average test loss: 0.00444117342804869\n",
      "Epoch 148/300\n",
      "Average training loss: 0.017509267982509402\n",
      "Average test loss: 0.0045114389972554314\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01751030625237359\n",
      "Average test loss: 0.004575355319927136\n",
      "Epoch 150/300\n",
      "Average training loss: 0.017502708268662293\n",
      "Average test loss: 0.004548858944947521\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017470045939087867\n",
      "Average test loss: 0.004557519202844964\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017465377014544276\n",
      "Average test loss: 0.00457620575858487\n",
      "Epoch 153/300\n",
      "Average training loss: 0.017465330433514382\n",
      "Average test loss: 0.0045134296206136544\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017435775047375097\n",
      "Average test loss: 0.004413720218671693\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01742583010180129\n",
      "Average test loss: 0.004477273544917504\n",
      "Epoch 156/300\n",
      "Average training loss: 0.017419717200100424\n",
      "Average test loss: 0.0044881393917732765\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017390161242749957\n",
      "Average test loss: 0.004673224199977186\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017380965906712745\n",
      "Average test loss: 0.00466868759608931\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017397565063503054\n",
      "Average test loss: 0.0045196122146315045\n",
      "Epoch 160/300\n",
      "Average training loss: 0.017366575222876338\n",
      "Average test loss: 0.004393084665553437\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017356861220465765\n",
      "Average test loss: 0.004551155407602588\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017358942470616763\n",
      "Average test loss: 0.0044861935310893586\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01734241498510043\n",
      "Average test loss: 0.004556015410770973\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017332036796543333\n",
      "Average test loss: 0.004441883195605543\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017301430753535695\n",
      "Average test loss: 0.004492554497801595\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017295337177813053\n",
      "Average test loss: 0.004558190101964606\n",
      "Epoch 167/300\n",
      "Average training loss: 0.017288883419500457\n",
      "Average test loss: 0.00452209127197663\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017276950580378373\n",
      "Average test loss: 0.004732211557527383\n",
      "Epoch 169/300\n",
      "Average training loss: 0.017259472221136093\n",
      "Average test loss: 0.0045005233921110634\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01726243341796928\n",
      "Average test loss: 0.0044796898768593866\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017245096892118453\n",
      "Average test loss: 0.0046448943341771765\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017226930209332043\n",
      "Average test loss: 0.004556427030927605\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017218300322691598\n",
      "Average test loss: 0.004465937828852071\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01721300199793445\n",
      "Average test loss: 0.004451875821997722\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017201416181193457\n",
      "Average test loss: 0.004569525063865714\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01719001387970315\n",
      "Average test loss: 0.0045315343961119655\n",
      "Epoch 177/300\n",
      "Average training loss: 0.017184480651385253\n",
      "Average test loss: 0.004610330120970806\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017160142256153955\n",
      "Average test loss: 0.004444875638104148\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017105466263161766\n",
      "Average test loss: 0.00450373107360469\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01710322201003631\n",
      "Average test loss: 0.004558461799803707\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017092551678419114\n",
      "Average test loss: 0.004493835651212268\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01710704097400109\n",
      "Average test loss: 0.004484158697434597\n",
      "Epoch 187/300\n",
      "Average training loss: 0.017083086815145282\n",
      "Average test loss: 0.004632436595443222\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01708762720723947\n",
      "Average test loss: 0.00482165109531747\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017072768179906737\n",
      "Average test loss: 0.004718739219423797\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01706182880451282\n",
      "Average test loss: 0.00454130465288957\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017063361864950922\n",
      "Average test loss: 0.004516463198181656\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017023995985587438\n",
      "Average test loss: 0.004517441301710076\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017017744368977017\n",
      "Average test loss: 0.004490486635516087\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01701908044765393\n",
      "Average test loss: 0.004611413153509299\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01703056121038066\n",
      "Average test loss: 0.004509519557779034\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01699954149292575\n",
      "Average test loss: 0.004648824465357595\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017001820797721546\n",
      "Average test loss: 0.004508888399228453\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016979671118987932\n",
      "Average test loss: 0.004708784961762528\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0169764917443196\n",
      "Average test loss: 0.004552464925373594\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01696373446120156\n",
      "Average test loss: 0.004585445342792405\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016948984178404014\n",
      "Average test loss: 0.004679507124755118\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01696647354380952\n",
      "Average test loss: 0.004413280072311561\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016946354255080225\n",
      "Average test loss: 0.004632822898940908\n",
      "Epoch 204/300\n",
      "Average training loss: 0.016926973392566044\n",
      "Average test loss: 0.004544027040402095\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016921984874539907\n",
      "Average test loss: 0.004616668298633562\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01691607127421432\n",
      "Average test loss: 0.004642032959601946\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016907036286261348\n",
      "Average test loss: 0.0045092967986646625\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01689030957553122\n",
      "Average test loss: 0.004648797601461411\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016906904192434416\n",
      "Average test loss: 0.004581675093414055\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016889986366033553\n",
      "Average test loss: 0.004615276755972041\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016887461655669744\n",
      "Average test loss: 0.004575741171836853\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01685886144141356\n",
      "Average test loss: 0.004488073544783725\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016855496271616883\n",
      "Average test loss: 0.004502043669008546\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01683607091009617\n",
      "Average test loss: 0.0046674065200818906\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016834025729033683\n",
      "Average test loss: 0.004497398245044881\n",
      "Epoch 216/300\n",
      "Average training loss: 0.016847893946700627\n",
      "Average test loss: 0.004510912287152476\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016831863421532842\n",
      "Average test loss: 0.004676266603585747\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01682429841491911\n",
      "Average test loss: 0.004577077785713805\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016798519922627344\n",
      "Average test loss: 0.004646098859608173\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016803069738050302\n",
      "Average test loss: 0.004587217487187849\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016801626176469856\n",
      "Average test loss: 0.0046366523959570465\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016790878335634866\n",
      "Average test loss: 0.0044910543879701035\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01678036576343907\n",
      "Average test loss: 0.004712543881601757\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016780843373801972\n",
      "Average test loss: 0.004712569140725665\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016785461061530643\n",
      "Average test loss: 0.0045954096458024445\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01674432438777553\n",
      "Average test loss: 0.004527003742754459\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016746004407604535\n",
      "Average test loss: 0.004762863400081793\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016747467541032367\n",
      "Average test loss: 0.004500112582412031\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016734274599287245\n",
      "Average test loss: 0.004461844661169582\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01672523377670182\n",
      "Average test loss: 0.004629673497958316\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016745197971661886\n",
      "Average test loss: 0.004690588352994786\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016729100117252933\n",
      "Average test loss: 0.004665680413444837\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01670908431046539\n",
      "Average test loss: 0.004754824268942078\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016720159706142212\n",
      "Average test loss: 0.00463113585466312\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016709983239571253\n",
      "Average test loss: 0.004490086341690686\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01668893134759532\n",
      "Average test loss: 0.004553936898294422\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016698368128803043\n",
      "Average test loss: 0.00458023610814578\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016677750779522788\n",
      "Average test loss: 0.0045463306022187075\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016677710003323024\n",
      "Average test loss: 0.004647978588317832\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016691437660819954\n",
      "Average test loss: 0.00460135675014721\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016662563354604774\n",
      "Average test loss: 0.004730153616517782\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016640959151917035\n",
      "Average test loss: 0.004605105605390337\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016655968654486868\n",
      "Average test loss: 0.004588169004354212\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016642182596855692\n",
      "Average test loss: 0.004568291661640008\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016623348010083038\n",
      "Average test loss: 0.004516894270976384\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016638615523775418\n",
      "Average test loss: 0.00457144549013012\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01662286172972785\n",
      "Average test loss: 0.004647460089789496\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016611829994453323\n",
      "Average test loss: 0.0045151682195978034\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016620623879962496\n",
      "Average test loss: 0.00456195143941376\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016610514633357526\n",
      "Average test loss: 0.004634973579603765\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016587260825766458\n",
      "Average test loss: 0.004622592924783627\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01659306334124671\n",
      "Average test loss: 0.004604627447409762\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016588732083638508\n",
      "Average test loss: 0.00453699643123481\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016578114224804773\n",
      "Average test loss: 0.004645206958883339\n",
      "Epoch 255/300\n",
      "Average training loss: 0.016578287391199004\n",
      "Average test loss: 0.004671941861510277\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016558763738307686\n",
      "Average test loss: 0.0046238303801251785\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01655300686425633\n",
      "Average test loss: 0.004647412516590622\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01656144626190265\n",
      "Average test loss: 0.004634787434505092\n",
      "Epoch 259/300\n",
      "Average training loss: 0.016549822100334698\n",
      "Average test loss: 0.004599340125711428\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016561942463947668\n",
      "Average test loss: 0.004660908370796177\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016545878431863253\n",
      "Average test loss: 0.004784242090872593\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01654089682135317\n",
      "Average test loss: 0.004731239096572002\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016526915656195747\n",
      "Average test loss: 0.0046654510274529454\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01654175949179464\n",
      "Average test loss: 0.004756587634897894\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016525310118993122\n",
      "Average test loss: 0.004789315555658606\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016511483929223483\n",
      "Average test loss: 0.004667199810345968\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01650446150534683\n",
      "Average test loss: 0.00448656362377935\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016491238557630114\n",
      "Average test loss: 0.004660137237774001\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016485965073936515\n",
      "Average test loss: 0.004603162956527538\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016511245309478707\n",
      "Average test loss: 0.004563989528765281\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01647792083852821\n",
      "Average test loss: 0.00458756110072136\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016471883276270496\n",
      "Average test loss: 0.004714355310218202\n",
      "Epoch 273/300\n",
      "Average training loss: 0.016469072358475792\n",
      "Average test loss: 0.004582686799474888\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016474376763734554\n",
      "Average test loss: 0.004614845607843664\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016455983272029295\n",
      "Average test loss: 0.004592569332983759\n",
      "Epoch 276/300\n",
      "Average training loss: 0.016449048516650994\n",
      "Average test loss: 0.004678452448298534\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01644615200161934\n",
      "Average test loss: 0.0045343984233008495\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016455060367782912\n",
      "Average test loss: 0.004615964534795946\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016439017982946502\n",
      "Average test loss: 0.004618527308520344\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01643951677944925\n",
      "Average test loss: 0.00463251425491439\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01643124410013358\n",
      "Average test loss: 0.004734229436765114\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01642376704679595\n",
      "Average test loss: 0.004622409189119935\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016427381621466744\n",
      "Average test loss: 0.004675125053359402\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016414707708689903\n",
      "Average test loss: 0.004681289455542962\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01641217068831126\n",
      "Average test loss: 0.004615057088848617\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016404287007947764\n",
      "Average test loss: 0.004618304670684867\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01638831366929743\n",
      "Average test loss: 0.00462785538389451\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01638846857017941\n",
      "Average test loss: 0.004693292095636328\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0164013816052013\n",
      "Average test loss: 0.004668716481576363\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016382239206797548\n",
      "Average test loss: 0.0048055902160704135\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016378742693199052\n",
      "Average test loss: 0.004631121151149273\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01637897567451\n",
      "Average test loss: 0.0048034293912351134\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01638128892166747\n",
      "Average test loss: 0.004670239903446701\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016355384778645305\n",
      "Average test loss: 0.004602699531449212\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01635604637612899\n",
      "Average test loss: 0.004621483591074745\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01636317219916317\n",
      "Average test loss: 0.004634366577284204\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016351162917084164\n",
      "Average test loss: 0.0045772387157711715\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01635952822698487\n",
      "Average test loss: 0.004685597584893306\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016350405553976696\n",
      "Average test loss: 0.004611722403930293\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016340048477053642\n",
      "Average test loss: 0.004532142975885007\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.11300818292631043\n",
      "Average test loss: 0.005030769023630354\n",
      "Epoch 2/300\n",
      "Average training loss: 0.027582171804375118\n",
      "Average test loss: 0.004415037175847424\n",
      "Epoch 3/300\n",
      "Average training loss: 0.024109427905744975\n",
      "Average test loss: 0.004406968413541715\n",
      "Epoch 4/300\n",
      "Average training loss: 0.022792687997221946\n",
      "Average test loss: 0.00411324578192499\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02207775863011678\n",
      "Average test loss: 0.004114675579385625\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021593015096253818\n",
      "Average test loss: 0.003950617732273207\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02122150956094265\n",
      "Average test loss: 0.00397524259156651\n",
      "Epoch 8/300\n",
      "Average training loss: 0.020917378722793527\n",
      "Average test loss: 0.0038606456112530497\n",
      "Epoch 9/300\n",
      "Average training loss: 0.02064086356262366\n",
      "Average test loss: 0.0038309419862926006\n",
      "Epoch 10/300\n",
      "Average training loss: 0.020395639679498142\n",
      "Average test loss: 0.0038213268704712392\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0201657680546244\n",
      "Average test loss: 0.003742572350634469\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019950279835197662\n",
      "Average test loss: 0.003702433524032434\n",
      "Epoch 13/300\n",
      "Average training loss: 0.019730562620692782\n",
      "Average test loss: 0.003674573393745555\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019542956396109527\n",
      "Average test loss: 0.0036840735723574955\n",
      "Epoch 15/300\n",
      "Average training loss: 0.019347032308578493\n",
      "Average test loss: 0.003654132477939129\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01916269032822715\n",
      "Average test loss: 0.0036140862268706163\n",
      "Epoch 17/300\n",
      "Average training loss: 0.018996740976969403\n",
      "Average test loss: 0.003652212062229713\n",
      "Epoch 18/300\n",
      "Average training loss: 0.018845113567180105\n",
      "Average test loss: 0.003673260326600737\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01868166252805127\n",
      "Average test loss: 0.003590386539697647\n",
      "Epoch 20/300\n",
      "Average training loss: 0.018535732875267663\n",
      "Average test loss: 0.0035378262932515806\n",
      "Epoch 21/300\n",
      "Average training loss: 0.01839544149570995\n",
      "Average test loss: 0.003564022855212291\n",
      "Epoch 22/300\n",
      "Average training loss: 0.018272597562935616\n",
      "Average test loss: 0.003511795912558834\n",
      "Epoch 23/300\n",
      "Average training loss: 0.018155536526607143\n",
      "Average test loss: 0.003501877125352621\n",
      "Epoch 24/300\n",
      "Average training loss: 0.018042986024585036\n",
      "Average test loss: 0.00352680222897066\n",
      "Epoch 25/300\n",
      "Average training loss: 0.017939445017112626\n",
      "Average test loss: 0.0034792607865399786\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017837333823243776\n",
      "Average test loss: 0.0034809150236348313\n",
      "Epoch 27/300\n",
      "Average training loss: 0.017733357701036664\n",
      "Average test loss: 0.0034883950671388045\n",
      "Epoch 28/300\n",
      "Average training loss: 0.017658234303196272\n",
      "Average test loss: 0.0034626241330471303\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01757287375960085\n",
      "Average test loss: 0.00347607977502048\n",
      "Epoch 30/300\n",
      "Average training loss: 0.017470935047500663\n",
      "Average test loss: 0.00343569008136789\n",
      "Epoch 31/300\n",
      "Average training loss: 0.017400099989440705\n",
      "Average test loss: 0.0034297472462058065\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017326326958007284\n",
      "Average test loss: 0.0034743341501388284\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01726479030152162\n",
      "Average test loss: 0.0034941651676264076\n",
      "Epoch 34/300\n",
      "Average training loss: 0.017176189245449173\n",
      "Average test loss: 0.003418812346127298\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017126403015520836\n",
      "Average test loss: 0.003415434171963069\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017023664996027945\n",
      "Average test loss: 0.003408382013440132\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0169618387868007\n",
      "Average test loss: 0.003505788823796643\n",
      "Epoch 38/300\n",
      "Average training loss: 0.016897312694125706\n",
      "Average test loss: 0.003475049799722102\n",
      "Epoch 39/300\n",
      "Average training loss: 0.016827853615085285\n",
      "Average test loss: 0.003407151790542735\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01675236018664307\n",
      "Average test loss: 0.003443607716096772\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01669943117764261\n",
      "Average test loss: 0.003575754633794228\n",
      "Epoch 42/300\n",
      "Average training loss: 0.016631444820099407\n",
      "Average test loss: 0.00342145230455531\n",
      "Epoch 43/300\n",
      "Average training loss: 0.016581343181431292\n",
      "Average test loss: 0.0034261364425636\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01649115120205614\n",
      "Average test loss: 0.0034376189636273514\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0164403292702304\n",
      "Average test loss: 0.0034507245746337703\n",
      "Epoch 46/300\n",
      "Average training loss: 0.016375720007552042\n",
      "Average test loss: 0.0035325269620451664\n",
      "Epoch 47/300\n",
      "Average training loss: 0.016315707307722832\n",
      "Average test loss: 0.003435885906840364\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01625581502252155\n",
      "Average test loss: 0.003530444694682956\n",
      "Epoch 49/300\n",
      "Average training loss: 0.016192966477738487\n",
      "Average test loss: 0.0035499244512369236\n",
      "Epoch 50/300\n",
      "Average training loss: 0.016131124667823314\n",
      "Average test loss: 0.0036196043182992275\n",
      "Epoch 51/300\n",
      "Average training loss: 0.016080776100357373\n",
      "Average test loss: 0.0034346306208107207\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01601080674595303\n",
      "Average test loss: 0.003477959357202053\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015962663998206457\n",
      "Average test loss: 0.0035360097371869618\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01590886323319541\n",
      "Average test loss: 0.0034163878411054612\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01586862538423803\n",
      "Average test loss: 0.0034178661250819763\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0158103542957041\n",
      "Average test loss: 0.003462398149487045\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015741568786402545\n",
      "Average test loss: 0.0034313405965351396\n",
      "Epoch 58/300\n",
      "Average training loss: 0.015680950361821388\n",
      "Average test loss: 0.003615596382154359\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015676487074957952\n",
      "Average test loss: 0.003478681973699066\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015610475265317492\n",
      "Average test loss: 0.0035217726826667784\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015543871715664863\n",
      "Average test loss: 0.0034715055140356224\n",
      "Epoch 62/300\n",
      "Average training loss: 0.015512303336626954\n",
      "Average test loss: 0.0036321976302812497\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015448578188816706\n",
      "Average test loss: 0.003487053020339873\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015424312974843713\n",
      "Average test loss: 0.0034947035047743057\n",
      "Epoch 65/300\n",
      "Average training loss: 0.015377463936805726\n",
      "Average test loss: 0.0034889402000440493\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015342500450710455\n",
      "Average test loss: 0.0035068002637061806\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015291392125189304\n",
      "Average test loss: 0.003524051077456938\n",
      "Epoch 68/300\n",
      "Average training loss: 0.015267106292148431\n",
      "Average test loss: 0.0035687772010763487\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015210035446617339\n",
      "Average test loss: 0.0034579285466008716\n",
      "Epoch 70/300\n",
      "Average training loss: 0.015186402580804295\n",
      "Average test loss: 0.0035805369456195168\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01514276065925757\n",
      "Average test loss: 0.0035169627740979197\n",
      "Epoch 72/300\n",
      "Average training loss: 0.015116560836633047\n",
      "Average test loss: 0.0036158741551140946\n",
      "Epoch 73/300\n",
      "Average training loss: 0.015055790506303311\n",
      "Average test loss: 0.003755688221918212\n",
      "Epoch 74/300\n",
      "Average training loss: 0.015031556511918703\n",
      "Average test loss: 0.003523489636886451\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01501543821560012\n",
      "Average test loss: 0.0035664404605825744\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014983980049689611\n",
      "Average test loss: 0.0035413990649912092\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0149444681364629\n",
      "Average test loss: 0.0035796868234044977\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014884815072019894\n",
      "Average test loss: 0.003550007734861639\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014882595534125964\n",
      "Average test loss: 0.003662856304811107\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01485167278101047\n",
      "Average test loss: 0.0035637085768911573\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014813464732633697\n",
      "Average test loss: 0.0036641323988636336\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014758712746202945\n",
      "Average test loss: 0.0036386136925882764\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014768678776092\n",
      "Average test loss: 0.0034890521965507005\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014721414839227995\n",
      "Average test loss: 0.0036226969106743735\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014699148260056973\n",
      "Average test loss: 0.003737247981544998\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01466808112545146\n",
      "Average test loss: 0.0035369383378161326\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014664152056806618\n",
      "Average test loss: 0.003569813488672177\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014619119693007734\n",
      "Average test loss: 0.0035298704985115264\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014603277078933186\n",
      "Average test loss: 0.003785836078019606\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014558496584494908\n",
      "Average test loss: 0.0037845531097716757\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01456531295594242\n",
      "Average test loss: 0.0036151221543550493\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014534028840561708\n",
      "Average test loss: 0.003558331961433093\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014556509560181036\n",
      "Average test loss: 0.0035189800072047447\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014455607797536586\n",
      "Average test loss: 0.003933972084687816\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014446329965359635\n",
      "Average test loss: 0.0036427267293135325\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014440730641285579\n",
      "Average test loss: 0.0036987415411406095\n",
      "Epoch 97/300\n",
      "Average test loss: 0.0036307818185951973\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014385559128390419\n",
      "Average test loss: 0.003567620125495725\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014361848262449106\n",
      "Average test loss: 0.0035788718894537953\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014338292418254746\n",
      "Average test loss: 0.0036860717750257918\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014322259777949917\n",
      "Average test loss: 0.0036342063090867467\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014294959786865446\n",
      "Average test loss: 0.0035729321816729176\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01429586886945698\n",
      "Average test loss: 0.0036326437861555153\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01426151931616995\n",
      "Average test loss: 0.0036810501577953497\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014255656450986863\n",
      "Average test loss: 0.003564251359138224\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014234866650568114\n",
      "Average test loss: 0.003599191574793723\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014242090409000714\n",
      "Average test loss: 0.003605086907123526\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014188852686021063\n",
      "Average test loss: 0.003578822755979167\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014163007133536869\n",
      "Average test loss: 0.0036927955848061376\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014154311993055873\n",
      "Average test loss: 0.003715372159663174\n",
      "Epoch 111/300\n",
      "Average training loss: 0.014151149912012948\n",
      "Average test loss: 0.0036592354629602698\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014110333397984504\n",
      "Average test loss: 0.0036986090735428865\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014125412553548813\n",
      "Average test loss: 0.0036783349607139824\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014103368673887519\n",
      "Average test loss: 0.003703085681423545\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014114398668209712\n",
      "Average test loss: 0.003723017202897204\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014059666234585974\n",
      "Average test loss: 0.003724555667696728\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014066052187648084\n",
      "Average test loss: 0.003782387083603276\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01403019499944316\n",
      "Average test loss: 0.0037723056595358583\n",
      "Epoch 119/300\n",
      "Average training loss: 0.014036596250202921\n",
      "Average test loss: 0.003704490952193737\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013977435401744313\n",
      "Average test loss: 0.0036135295070707798\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013980841348568598\n",
      "Average test loss: 0.0036125456819103823\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013973270186947452\n",
      "Average test loss: 0.0036275585552470553\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013947994341452916\n",
      "Average test loss: 0.003647948079639011\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013950928092002868\n",
      "Average test loss: 0.003657941055173675\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01392603120704492\n",
      "Average test loss: 0.003701573868178659\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013905534571243657\n",
      "Average test loss: 0.003639664438863595\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013925299591488308\n",
      "Average test loss: 0.0036260929728547733\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01387980645067162\n",
      "Average test loss: 0.0035662872205591864\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01387895028375917\n",
      "Average test loss: 0.0036810395994948016\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013847509384155273\n",
      "Average test loss: 0.003801673624664545\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013836837252808942\n",
      "Average test loss: 0.0037812176330222023\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013837095088428921\n",
      "Average test loss: 0.003767447789717052\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013837024766537878\n",
      "Average test loss: 0.0036570591032505035\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013790249081949393\n",
      "Average test loss: 0.00373401234133376\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013801187277668053\n",
      "Average test loss: 0.003755795539667209\n",
      "Epoch 136/300\n",
      "Average training loss: 0.01381077621380488\n",
      "Average test loss: 0.0036082856905543144\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013784771769411034\n",
      "Average test loss: 0.0035826698798272346\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013797044343418545\n",
      "Average test loss: 0.0036537181490825282\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013757677600615555\n",
      "Average test loss: 0.00375884437871476\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013725374081068569\n",
      "Average test loss: 0.0039005250479612085\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01371138279678093\n",
      "Average test loss: 0.0036677310466766357\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013723269747363197\n",
      "Average test loss: 0.0036427376669728095\n",
      "Epoch 143/300\n",
      "Average training loss: 0.0137083550973071\n",
      "Average test loss: 0.0036788181674977144\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013698489962352647\n",
      "Average test loss: 0.0036988565718962087\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01367970959842205\n",
      "Average test loss: 0.0037336701322346925\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013673588101234702\n",
      "Average test loss: 0.003741258489174975\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013670760755737622\n",
      "Average test loss: 0.0036841721238775388\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01365312500215239\n",
      "Average test loss: 0.0037653653216030863\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013665800150897767\n",
      "Average test loss: 0.0037845826335251333\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0136365297080742\n",
      "Average test loss: 0.003643877103096909\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013614669879277548\n",
      "Average test loss: 0.003825603512633178\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013628758838607206\n",
      "Average test loss: 0.003774592416154014\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013596017974946233\n",
      "Average test loss: 0.003702379084709618\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013581723738875654\n",
      "Average test loss: 0.003695290945884254\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013585742705398135\n",
      "Average test loss: 0.0036652760874066087\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01358743077268203\n",
      "Average test loss: 0.003805899231384198\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013582906360427539\n",
      "Average test loss: 0.0038090128087335164\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013547971241176128\n",
      "Average test loss: 0.0037362477216455674\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013539120859569974\n",
      "Average test loss: 0.0037504150724659365\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013516346683104833\n",
      "Average test loss: 0.0038095975536853073\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013543699931767251\n",
      "Average test loss: 0.003676454450521204\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013503210422065523\n",
      "Average test loss: 0.0036869453847822218\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013510307509038184\n",
      "Average test loss: 0.003710595867286126\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01350613001237313\n",
      "Average test loss: 0.003664555936637852\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013482278924849298\n",
      "Average test loss: 0.003675590203040176\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013480663393106725\n",
      "Average test loss: 0.0036976267813394466\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013456792949802347\n",
      "Average test loss: 0.003901837554242876\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01346686107913653\n",
      "Average test loss: 0.0038916770279821423\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013449025239381525\n",
      "Average test loss: 0.003758447304368019\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013443319502804014\n",
      "Average test loss: 0.003704976940734519\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013435841561191611\n",
      "Average test loss: 0.003769153576758173\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013412036322057248\n",
      "Average test loss: 0.003881659631513887\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013430347286992602\n",
      "Average test loss: 0.003926817952137855\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013425978988409042\n",
      "Average test loss: 0.003665914203143782\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013395569676326382\n",
      "Average test loss: 0.003751285570570164\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013396103270351886\n",
      "Average test loss: 0.0037700696078439553\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013381779139240583\n",
      "Average test loss: 0.0037840771780659754\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013373324010935094\n",
      "Average test loss: 0.0037240163190290334\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013385170742041534\n",
      "Average test loss: 0.003979573275480005\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013359942980110645\n",
      "Average test loss: 0.0037634807340800763\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013348127840293779\n",
      "Average test loss: 0.0038408251255750655\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01337221507065826\n",
      "Average test loss: 0.003949631656209628\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013351770831478966\n",
      "Average test loss: 0.0037305903919041157\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013323090720507833\n",
      "Average test loss: 0.003895686333792077\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01336397371109989\n",
      "Average test loss: 0.0037044736792643863\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013344507670236958\n",
      "Average test loss: 0.0036505793039169577\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013338425999714269\n",
      "Average test loss: 0.003748969795803229\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013302590360244115\n",
      "Average test loss: 0.0038623148422274323\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0132815588903096\n",
      "Average test loss: 0.0036684579838895137\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013294305864307616\n",
      "Average test loss: 0.004033413712349203\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0133213101948301\n",
      "Average test loss: 0.003801407194178965\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013277977570063538\n",
      "Average test loss: 0.0038894546640415987\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013252436500042677\n",
      "Average test loss: 0.0038937073060207896\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013270320906407304\n",
      "Average test loss: 0.0038162463940680028\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013259395067062642\n",
      "Average test loss: 0.0038566553480923176\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013242469606300196\n",
      "Average test loss: 0.003723016502542628\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013237974522014459\n",
      "Average test loss: 0.0036343997592727345\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013243236848877536\n",
      "Average test loss: 0.0037433494004524416\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013226839311420917\n",
      "Average test loss: 0.0037085962140311797\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013214106371005375\n",
      "Average test loss: 0.0038670867609067096\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013251299984753133\n",
      "Average test loss: 0.00374880533085929\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013225134328007699\n",
      "Average test loss: 0.0038881417599817116\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013189221031135984\n",
      "Average test loss: 0.0037294564832829765\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013200462287498846\n",
      "Average test loss: 0.003815170486768087\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01321594030658404\n",
      "Average test loss: 0.0037404007700582347\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013180626994205845\n",
      "Average test loss: 0.0039044535238709713\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013198990470005407\n",
      "Average test loss: 0.003820496872895294\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013181713826954366\n",
      "Average test loss: 0.0037973570204857324\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013177019269929991\n",
      "Average test loss: 0.0039422318624953425\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013171541203227308\n",
      "Average test loss: 0.003698519406633245\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013141296985248725\n",
      "Average test loss: 0.003794816428174575\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013157348388599025\n",
      "Average test loss: 0.0038156776869048676\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01314347764932447\n",
      "Average test loss: 0.003901189335104492\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01314035295529498\n",
      "Average test loss: 0.003888937131398254\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013132376376125547\n",
      "Average test loss: 0.0037177605194350085\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013116031035780906\n",
      "Average test loss: 0.003728459398365683\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013112291625804372\n",
      "Average test loss: 0.0038209639595200616\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013104916036128999\n",
      "Average test loss: 0.0040403397145370645\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013114347291489442\n",
      "Average test loss: 0.0038090612892475394\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013097893646193876\n",
      "Average test loss: 0.0037330360727177727\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01313332672831085\n",
      "Average test loss: 0.0037275293175545003\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013082741549445524\n",
      "Average test loss: 0.0038216894264850353\n",
      "Epoch 223/300\n",
      "Average training loss: 0.013087609988119867\n",
      "Average test loss: 0.003734404387987322\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013067730017006398\n",
      "Average test loss: 0.004059757878383001\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013072796906861995\n",
      "Average test loss: 0.003803541385154757\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013071378789842129\n",
      "Average test loss: 0.0039686046298593285\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013083946455683972\n",
      "Average test loss: 0.00376084334589541\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013057907940612899\n",
      "Average test loss: 0.003699419643316004\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01305698719703489\n",
      "Average test loss: 0.003680787294689152\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013055537845525477\n",
      "Average test loss: 0.0039009664934128523\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013062415430943171\n",
      "Average test loss: 0.00392600506130192\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013044616137113836\n",
      "Average test loss: 0.0037924782992858025\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013049816694524553\n",
      "Average test loss: 0.003984290409419272\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01301524179180463\n",
      "Average test loss: 0.003875955304958754\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013029869310557843\n",
      "Average test loss: 0.0038450248783661258\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013020428688161903\n",
      "Average test loss: 0.0038803392147852313\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013009703733026982\n",
      "Average test loss: 0.003799872614443302\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012992683154013421\n",
      "Average test loss: 0.0038261151059220235\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012994666625228192\n",
      "Average test loss: 0.003879455675267511\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01301826653132836\n",
      "Average test loss: 0.0037279511371420488\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013003318943911128\n",
      "Average test loss: 0.003995599078842335\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012991260165141688\n",
      "Average test loss: 0.003739882965882619\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013001545877920257\n",
      "Average test loss: 0.0037769081112411285\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01298091130041414\n",
      "Average test loss: 0.003866384228070577\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013005719744496875\n",
      "Average test loss: 0.0036987374325593314\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012957698765728208\n",
      "Average test loss: 0.00378300288774901\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012962063911888335\n",
      "Average test loss: 0.0038636032380163668\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01296106641077333\n",
      "Average test loss: 0.0037792362220999266\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012951165788703495\n",
      "Average test loss: 0.0038797580765353307\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012954108309414653\n",
      "Average test loss: 0.0040042997664875455\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012936732038855552\n",
      "Average test loss: 0.003922098030232721\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012962621732718414\n",
      "Average test loss: 0.003870532854149739\n",
      "Epoch 253/300\n",
      "Average training loss: 0.012924368180334569\n",
      "Average test loss: 0.004066504308746921\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01292619874742296\n",
      "Average test loss: 0.0038697682809498576\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0129535534216298\n",
      "Average test loss: 0.003817072650624646\n",
      "Epoch 256/300\n",
      "Average training loss: 0.012928492253853214\n",
      "Average test loss: 0.003911227186313934\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0129405163154006\n",
      "Average test loss: 0.0037675950251933603\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012926507367855973\n",
      "Average test loss: 0.003737213526748949\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01291618831952413\n",
      "Average test loss: 0.0038378678725825414\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01289964673254225\n",
      "Average test loss: 0.0038757690925978953\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0128807870787051\n",
      "Average test loss: 0.003817118597527345\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012906339056789874\n",
      "Average test loss: 0.0038482164210743375\n",
      "Epoch 263/300\n",
      "Average training loss: 0.012894899087647597\n",
      "Average test loss: 0.0038855811222973796\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012922397168974082\n",
      "Average test loss: 0.003907044597384003\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01290158446547058\n",
      "Average test loss: 0.003987005929773053\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01290192182858785\n",
      "Average test loss: 0.004079734160254399\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012872086464530893\n",
      "Average test loss: 0.00384388899223672\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01286138037012683\n",
      "Average test loss: 0.0037967202882799836\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01287702894045247\n",
      "Average test loss: 0.0038096180591318343\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012871483590867785\n",
      "Average test loss: 0.003998987735559543\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012847421480549707\n",
      "Average test loss: 0.0037921991567644807\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012848532334797913\n",
      "Average test loss: 0.0039173693977710275\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012848681651883655\n",
      "Average test loss: 0.0037262327124675115\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012843880818949806\n",
      "Average test loss: 0.003921828649938106\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012826553617086676\n",
      "Average test loss: 0.0038616881453328662\n",
      "Epoch 276/300\n",
      "Average training loss: 0.012842013713386323\n",
      "Average test loss: 0.003784653999325302\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012849053136176533\n",
      "Average test loss: 0.0038090881676309638\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01285555508484443\n",
      "Average test loss: 0.003785378998352422\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012839787952601909\n",
      "Average test loss: 0.003887744648795989\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01282031306789981\n",
      "Average test loss: 0.003765143013662762\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012797212532824941\n",
      "Average test loss: 0.0037538369780199393\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012803194493055343\n",
      "Average test loss: 0.0038149660513218907\n",
      "Epoch 288/300\n",
      "Average training loss: 0.012797768884234959\n",
      "Average test loss: 0.003859190482232306\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012793153782685598\n",
      "Average test loss: 0.0038385274587199093\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012782172737022241\n",
      "Average test loss: 0.0038425194368594223\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012785376317799092\n",
      "Average test loss: 0.0037179542529500193\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012775593255956967\n",
      "Average test loss: 0.0038343407867683305\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012782563435534636\n",
      "Average test loss: 0.003953916464414862\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012772353208727306\n",
      "Average test loss: 0.0039023418372703925\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012763438125451406\n",
      "Average test loss: 0.003752616648044851\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012780404826833142\n",
      "Average test loss: 0.00397399929828114\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012772204366823038\n",
      "Average test loss: 0.0038233464227782356\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012770478719638453\n",
      "Average test loss: 0.0038312096753054196\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012789709020819929\n",
      "Average test loss: 0.003947183469517363\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01277014545765188\n",
      "Average test loss: 0.004026228417952855\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1195553073419465\n",
      "Average test loss: 0.004810839208256867\n",
      "Epoch 2/300\n",
      "Average training loss: 0.027265799972746106\n",
      "Average test loss: 0.003955884036090639\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022657034614019925\n",
      "Average test loss: 0.0037379615002622207\n",
      "Epoch 4/300\n",
      "Average training loss: 0.021013957391182583\n",
      "Average test loss: 0.003578606869611475\n",
      "Epoch 5/300\n",
      "Average training loss: 0.020136822521686554\n",
      "Average test loss: 0.0034709592738913163\n",
      "Epoch 6/300\n",
      "Average training loss: 0.019545917340450816\n",
      "Average test loss: 0.0034392595015880134\n",
      "Epoch 7/300\n",
      "Average training loss: 0.019076877765357495\n",
      "Average test loss: 0.003312244780154692\n",
      "Epoch 8/300\n",
      "Average training loss: 0.0186990790632036\n",
      "Average test loss: 0.0032217594008478855\n",
      "Epoch 9/300\n",
      "Average training loss: 0.018364649857083956\n",
      "Average test loss: 0.003166962090258797\n",
      "Epoch 10/300\n",
      "Average training loss: 0.018071032646629546\n",
      "Average test loss: 0.003204700833186507\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017781398358444372\n",
      "Average test loss: 0.0030959553223931127\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01728075984451506\n",
      "Average test loss: 0.0030562300802105\n",
      "Epoch 14/300\n",
      "Average training loss: 0.017041272653473747\n",
      "Average test loss: 0.0030214427701301047\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01600846552848816\n",
      "Average test loss: 0.0028977108606033854\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01583067226658265\n",
      "Average test loss: 0.0029056278076022863\n",
      "Epoch 21/300\n",
      "Average training loss: 0.015666826751497057\n",
      "Average test loss: 0.002917837673591243\n",
      "Epoch 22/300\n",
      "Average training loss: 0.015005796152684424\n",
      "Average test loss: 0.0028175525963306425\n",
      "Epoch 27/300\n",
      "Average training loss: 0.014899608944025304\n",
      "Average test loss: 0.002803650502218968\n",
      "Epoch 28/300\n",
      "Average training loss: 0.014786027293238375\n",
      "Average test loss: 0.002786529909715884\n",
      "Epoch 29/300\n",
      "Average training loss: 0.014707786896990406\n",
      "Average test loss: 0.0027810236577772434\n",
      "Epoch 30/300\n",
      "Average training loss: 0.014627474781539706\n",
      "Average test loss: 0.002774366376300653\n",
      "Epoch 31/300\n",
      "Average training loss: 0.014529268407159382\n",
      "Average test loss: 0.0027657528004298606\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01415270900229613\n",
      "Average test loss: 0.002765557936289244\n",
      "Epoch 37/300\n",
      "Average training loss: 0.014111007999214862\n",
      "Average test loss: 0.0027591925971210004\n",
      "Epoch 38/300\n",
      "Average training loss: 0.014036688628296058\n",
      "Average test loss: 0.002771983700700932\n",
      "Epoch 39/300\n",
      "Average training loss: 0.013968695446021027\n",
      "Average test loss: 0.0027412029788311987\n",
      "Epoch 40/300\n",
      "Average training loss: 0.013900682180292076\n",
      "Average test loss: 0.0027765306530313357\n",
      "Epoch 41/300\n",
      "Average training loss: 0.013841868265635437\n",
      "Average test loss: 0.002782483699524568\n",
      "Epoch 42/300\n",
      "Average training loss: 0.013789472839898533\n",
      "Average test loss: 0.0028725750346978505\n",
      "Epoch 43/300\n",
      "Average training loss: 0.013744497265252802\n",
      "Average test loss: 0.002754965192327897\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01367832115623686\n",
      "Average test loss: 0.00274766765617662\n",
      "Epoch 45/300\n",
      "Average training loss: 0.013623130910098553\n",
      "Average test loss: 0.0027951303691499765\n",
      "Epoch 46/300\n",
      "Average training loss: 0.013556358772847388\n",
      "Average test loss: 0.0027506003610582815\n",
      "Epoch 47/300\n",
      "Average training loss: 0.013506587005323834\n",
      "Average test loss: 0.0027513873699224656\n",
      "Epoch 48/300\n",
      "Average training loss: 0.013457586648563544\n",
      "Average test loss: 0.0027360415150307947\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01341821957545148\n",
      "Average test loss: 0.002742456798131267\n",
      "Epoch 50/300\n",
      "Average training loss: 0.013354193489584658\n",
      "Average test loss: 0.0028688123625599674\n",
      "Epoch 51/300\n",
      "Average training loss: 0.013308557498786185\n",
      "Average test loss: 0.002786740562568108\n",
      "Epoch 52/300\n",
      "Average training loss: 0.013259529607991377\n",
      "Average test loss: 0.002743007298765911\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01319168467571338\n",
      "Average test loss: 0.002798787479185396\n",
      "Epoch 54/300\n",
      "Average training loss: 0.013154203612771299\n",
      "Average test loss: 0.0027737751485159\n",
      "Epoch 55/300\n",
      "Average training loss: 0.013121173425681061\n",
      "Average test loss: 0.0028087229693515435\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013065325207180447\n",
      "Average test loss: 0.0027858724196751913\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013022722341120244\n",
      "Average test loss: 0.002837335971287555\n",
      "Epoch 58/300\n",
      "Average training loss: 0.012984503106938468\n",
      "Average test loss: 0.0028079375363886357\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01293335584965017\n",
      "Average test loss: 0.0027464879849511716\n",
      "Epoch 60/300\n",
      "Average training loss: 0.012908131673932075\n",
      "Average test loss: 0.002784009723406699\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01286671895451016\n",
      "Average test loss: 0.0028485583184907834\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01283544645541244\n",
      "Average test loss: 0.002762663956524597\n",
      "Epoch 63/300\n",
      "Average training loss: 0.012766731614040005\n",
      "Average test loss: 0.0027724291255904567\n",
      "Epoch 64/300\n",
      "Average training loss: 0.012765795883205202\n",
      "Average test loss: 0.002776282713438074\n",
      "Epoch 65/300\n",
      "Average training loss: 0.012737020904819171\n",
      "Average test loss: 0.002803078583545155\n",
      "Epoch 66/300\n",
      "Average training loss: 0.012665746936367617\n",
      "Average test loss: 0.00292224668400983\n",
      "Epoch 67/300\n",
      "Average training loss: 0.012635521652797858\n",
      "Average test loss: 0.0027970705285875336\n",
      "Epoch 68/300\n",
      "Average training loss: 0.012599270240300232\n",
      "Average test loss: 0.0029133487877746425\n",
      "Epoch 69/300\n",
      "Average training loss: 0.012555409531626436\n",
      "Average test loss: 0.0027643055713011157\n",
      "Epoch 70/300\n",
      "Average training loss: 0.012532080338233047\n",
      "Average test loss: 0.002809544912435942\n",
      "Epoch 71/300\n",
      "Average training loss: 0.012509524416592387\n",
      "Average training loss: 0.012384544293085734\n",
      "Average test loss: 0.0029377181737994156\n",
      "Epoch 76/300\n",
      "Average training loss: 0.012344538764821158\n",
      "Average test loss: 0.002867568290171524\n",
      "Epoch 77/300\n",
      "Average training loss: 0.012329856786462996\n",
      "Average test loss: 0.0027878208909597663\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01230124470508761\n",
      "Average test loss: 0.002799192741513252\n",
      "Epoch 79/300\n",
      "Average training loss: 0.012270497434669072\n",
      "Average test loss: 0.002841169552049703\n",
      "Epoch 80/300\n",
      "Average training loss: 0.012248168904748228\n",
      "Average test loss: 0.0028943394944071768\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01221846954855654\n",
      "Average test loss: 0.002934272271477514\n",
      "Epoch 82/300\n",
      "Average training loss: 0.012182156074378225\n",
      "Average test loss: 0.00294423416401777\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0121749854311347\n",
      "Average test loss: 0.002855119400140312\n",
      "Epoch 84/300\n",
      "Average training loss: 0.012148405773772134\n",
      "Average test loss: 0.0028479746834685405\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01212247008499172\n",
      "Average test loss: 0.0028555710228780904\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012089389338261551\n",
      "Average test loss: 0.0028477588122089704\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01208932095186578\n",
      "Average test loss: 0.002842889221592082\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01206766321675645\n",
      "Average test loss: 0.0028655845332476826\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012040016199979518\n",
      "Average test loss: 0.002799943389991919\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012020774638487233\n",
      "Average test loss: 0.002920596967347794\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011984285391039318\n",
      "Average test loss: 0.002856849797276987\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012014062537087335\n",
      "Average test loss: 0.0029273526018692387\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011942825501991643\n",
      "Average test loss: 0.0029297017951806386\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011938510035475094\n",
      "Average test loss: 0.002933640389185813\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011897086871994866\n",
      "Average test loss: 0.0029344548446436722\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011915871743526724\n",
      "Average test loss: 0.00283903507846925\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011886041152808401\n",
      "Average test loss: 0.002937376618799236\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011875742486781544\n",
      "Average test loss: 0.0029704379623548853\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011844134117994044\n",
      "Average test loss: 0.0029231244441535738\n",
      "Epoch 100/300\n",
      "Average training loss: 0.011774026522205936\n",
      "Average test loss: 0.0029064146764576433\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011760024031831158\n",
      "Average test loss: 0.0029015155412877598\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011741522059672408\n",
      "Average test loss: 0.0028827331248256895\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01176010093672408\n",
      "Average test loss: 0.0029129756237897607\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01172255780051152\n",
      "Average test loss: 0.002955115720940133\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011705758805076282\n",
      "Average test loss: 0.002921258253355821\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011688125416636467\n",
      "Average test loss: 0.00293720538324366\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01167493164870474\n",
      "Average test loss: 0.002897960456916028\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011654264831708536\n",
      "Average test loss: 0.0029716117117140027\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01163708123813073\n",
      "Average test loss: 0.002938681138679385\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011608863156702783\n",
      "Average test loss: 0.002990444369200203\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011630720007750723\n",
      "Average test loss: 0.0028961506519052716\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011608812193075815\n",
      "Average test loss: 0.0029923397993875875\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011590472648541133\n",
      "Average test loss: 0.0029456138275563717\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011603949619664086\n",
      "Average test loss: 0.0029126443026794326\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011548066195514467\n",
      "Average test loss: 0.002997182979144984\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011552291954557101\n",
      "Average test loss: 0.0029855868468681972\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011548014228542645\n",
      "Average test loss: 0.0029277377555974654\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011525572708083524\n",
      "Average test loss: 0.0029731273899475733\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011513051667147212\n",
      "Average test loss: 0.00289204900401334\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011505608713461292\n",
      "Average test loss: 0.002916607627438174\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011501957808103827\n",
      "Average test loss: 0.003077909140744143\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011465497273537848\n",
      "Average test loss: 0.0028856577109545468\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011469596573048168\n",
      "Average test loss: 0.0030156219595422347\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011453473433024353\n",
      "Average test loss: 0.0028993795580334134\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011464974492788315\n",
      "Average test loss: 0.002919639065447781\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011417476835350196\n",
      "Average test loss: 0.002944716654304001\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011444152288138866\n",
      "Average test loss: 0.0029291691224401194\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011409783845974339\n",
      "Average test loss: 0.003004396672050158\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011392483991881211\n",
      "Average test loss: 0.002991836915206578\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011389412595993943\n",
      "Average test loss: 0.0029519321450756654\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011412841233114401\n",
      "Average test loss: 0.0029598424333251187\n",
      "Epoch 135/300\n",
      "Average training loss: 0.011365828217731582\n",
      "Average test loss: 0.0029561404389225774\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011352997603515784\n",
      "Average test loss: 0.0029309460860159663\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011309969807664553\n",
      "Average test loss: 0.003013221731616391\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011316208038893012\n",
      "Average test loss: 0.003069870757146014\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011321040013598072\n",
      "Average test loss: 0.0029989185045576757\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011308679792616103\n",
      "Average test loss: 0.0029082536353833145\n",
      "Epoch 144/300\n",
      "Average training loss: 0.011284398222963015\n",
      "Average test loss: 0.002974995648074481\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011277466273970074\n",
      "Average test loss: 0.003114102059768306\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011280323262015978\n",
      "Average test loss: 0.002967662498354912\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0112719606032802\n",
      "Average test loss: 0.0030296572101198966\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011240626635650794\n",
      "Average test loss: 0.0030477657947275373\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011257749914295144\n",
      "Average test loss: 0.003061021404340863\n",
      "Epoch 150/300\n",
      "Average training loss: 0.011250947029226356\n",
      "Average test loss: 0.003023356521709098\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01122461234447029\n",
      "Average test loss: 0.0031278961261527404\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01122470600489113\n",
      "Average test loss: 0.0029489595780356065\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011217751410272387\n",
      "Average test loss: 0.0030423433722721206\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011206703239430984\n",
      "Average test loss: 0.0031363831243167322\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01120760074754556\n",
      "Average test loss: 0.002976173303400477\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011196715987390943\n",
      "Average test loss: 0.0030142504626678094\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011180244486365053\n",
      "Average test loss: 0.0029749978048106036\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011182196092274454\n",
      "Average test loss: 0.0030074649593896338\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011154308698243565\n",
      "Average test loss: 0.002927705188592275\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01117367488808102\n",
      "Average test loss: 0.003118930789538556\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011146490277929438\n",
      "Average test loss: 0.0030975002739578486\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011144682752589384\n",
      "Average test loss: 0.0029429195316301453\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011141266843511\n",
      "Average test loss: 0.0031137052023162445\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011142177446848816\n",
      "Average test loss: 0.0030775126690665883\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011131367147796684\n",
      "Average test loss: 0.0030476652797725465\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011121319517493247\n",
      "Average test loss: 0.0030256127725458806\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011120354590316614\n",
      "Average test loss: 0.003050860482164555\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011101712324966987\n",
      "Average test loss: 0.003223934639348752\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011114303205576208\n",
      "Average test loss: 0.0030231101382523777\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011085068973402182\n",
      "Average test loss: 0.003005743463420206\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011093392990529537\n",
      "Average test loss: 0.0030852430344869694\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01107728922781017\n",
      "Average test loss: 0.003019376166785757\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011060297187003824\n",
      "Average test loss: 0.0029985427514960367\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011068243011832237\n",
      "Average test loss: 0.003055573005022274\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011045360531244013\n",
      "Average test loss: 0.003054578409840663\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011062519196006987\n",
      "Average test loss: 0.002980254567331738\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011052877239055103\n",
      "Average test loss: 0.0030492224248333108\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01104255492116014\n",
      "Average test loss: 0.003099192169598407\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011040998765163951\n",
      "Average test loss: 0.0030775515054249102\n",
      "Epoch 180/300\n",
      "Average training loss: 0.011013479033278095\n",
      "Average test loss: 0.0030516791513396633\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01102702737516827\n",
      "Average test loss: 0.003031855485919449\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011029552924136321\n",
      "Average test loss: 0.003056684508919716\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011022779198984305\n",
      "Average test loss: 0.0031424848104102746\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010999184628327687\n",
      "Average test loss: 0.0031325025618490245\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010996116639839279\n",
      "Average test loss: 0.003001031758574148\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01099852190580633\n",
      "Average test loss: 0.0031117776001079213\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010988952582081159\n",
      "Average test loss: 0.0032066961067418256\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010980742226872179\n",
      "Average test loss: 0.0030904405419197346\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010980071374939547\n",
      "Average test loss: 0.003196461852846874\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010974501961635219\n",
      "Average test loss: 0.0030115073888252178\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010957146605683697\n",
      "Average test loss: 0.003007886849136816\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010986236996120877\n",
      "Average test loss: 0.0030944713888068996\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010942908739878071\n",
      "Average test loss: 0.0031078179598682458\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010946803162495295\n",
      "Average test loss: 0.0030299264126353795\n",
      "Epoch 195/300\n",
      "Average training loss: 0.010947687892450226\n",
      "Average test loss: 0.0030701211825427083\n",
      "Epoch 196/300\n",
      "Average training loss: 0.010939677655696868\n",
      "Average test loss: 0.0030194044032444555\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010932109966874123\n",
      "Average test loss: 0.003094226272569762\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010930032673809263\n",
      "Average test loss: 0.002999905429987444\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010914451083375349\n",
      "Average test loss: 0.0030446806868745222\n",
      "Epoch 200/300\n",
      "Average training loss: 0.010920710356699096\n",
      "Average test loss: 0.0031600612041850886\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01091610849979851\n",
      "Average test loss: 0.003005841920980149\n",
      "Epoch 202/300\n",
      "Average training loss: 0.010915095234910648\n",
      "Average test loss: 0.002992026352220111\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010893720113568836\n",
      "Average test loss: 0.003140528975468543\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010914893671870231\n",
      "Average test loss: 0.003120698379145728\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010893537476658822\n",
      "Average test loss: 0.003145789813871185\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010899138181573814\n",
      "Average test loss: 0.003004328899499443\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010898593096269501\n",
      "Average test loss: 0.003101062632062369\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010861487785975139\n",
      "Average test loss: 0.003031970624708467\n",
      "Epoch 209/300\n",
      "Average training loss: 0.010887118366029527\n",
      "Average test loss: 0.0030907152723520996\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010857055518362257\n",
      "Average test loss: 0.003034079710642497\n",
      "Epoch 211/300\n",
      "Average training loss: 0.010872431738509072\n",
      "Average test loss: 0.0030254154748593767\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01086011556784312\n",
      "Average test loss: 0.003067282427309288\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010861907388601039\n",
      "Average test loss: 0.003035413829609752\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010865778773609135\n",
      "Average test loss: 0.003167338522565034\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01085605210562547\n",
      "Average test loss: 0.003106099015929633\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010835515293810103\n",
      "Average test loss: 0.003051712494964401\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010816929313871595\n",
      "Average test loss: 0.003115572065425416\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01083965794245402\n",
      "Average test loss: 0.0031519225492245622\n",
      "Epoch 219/300\n",
      "Average training loss: 0.010831524406870206\n",
      "Average test loss: 0.003174711604292194\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010837411865592003\n",
      "Average test loss: 0.0031048577914221417\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010831588248411815\n",
      "Average test loss: 0.003074285331492623\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010817483756691218\n",
      "Average test loss: 0.003196429252417551\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010811281219124794\n",
      "Average test loss: 0.0030600471455189916\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010824185807671811\n",
      "Average test loss: 0.0031002266961667273\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01079845611875256\n",
      "Average test loss: 0.003103651596026288\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010800254539483124\n",
      "Average test loss: 0.0030173628547539313\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01080835675365395\n",
      "Average test loss: 0.003143641530225674\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010778523160351647\n",
      "Average test loss: 0.003083123321334521\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010787085483471553\n",
      "Average test loss: 0.0030921831789116065\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010800455547869205\n",
      "Average test loss: 0.0031787731792363855\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010792844815386666\n",
      "Average test loss: 0.003081525387863318\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010769440039164491\n",
      "Average test loss: 0.0030211202618148593\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010774269214106931\n",
      "Average test loss: 0.0030866218737016123\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01076454204486476\n",
      "Average test loss: 0.003025180565400256\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010765966312752829\n",
      "Average test loss: 0.0030537321027368307\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010757718332939677\n",
      "Average test loss: 0.0030105162126322586\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010753271299931737\n",
      "Average test loss: 0.003168602452096012\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010751391579707463\n",
      "Average test loss: 0.0030608256117751202\n",
      "Epoch 239/300\n",
      "Average training loss: 0.010758028283715249\n",
      "Average test loss: 0.0030635893589092624\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010734781415926086\n",
      "Average test loss: 0.0031187615611900886\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010742592914236916\n",
      "Average test loss: 0.003130914525853263\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010739280948208437\n",
      "Average test loss: 0.0031238260343670844\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010722110922137896\n",
      "Average test loss: 0.0031090604757062264\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010721999131143092\n",
      "Average test loss: 0.0031409590747207405\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010725987316833602\n",
      "Average test loss: 0.0031645976826548575\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01071931692916486\n",
      "Average test loss: 0.0031397517294519477\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010712764552897876\n",
      "Average test loss: 0.003053048034095102\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010725589725706312\n",
      "Average test loss: 0.003141800975220071\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01071942458798488\n",
      "Average test loss: 0.003170357796467013\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010714399015737905\n",
      "Average test loss: 0.0031612658724188805\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010707907650205824\n",
      "Average test loss: 0.003070445723003811\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01069472240904967\n",
      "Average test loss: 0.003105130124009318\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01071511348668072\n",
      "Average test loss: 0.0030242034472111197\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010688175182375644\n",
      "Average test loss: 0.003121174876888593\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010693704513212046\n",
      "Average test loss: 0.003160541594442394\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010690052589608564\n",
      "Average test loss: 0.003088818638895949\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010690276506046453\n",
      "Average test loss: 0.0030845440031132764\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01069503517366118\n",
      "Average test loss: 0.003180659886656536\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010680937428855234\n",
      "Average test loss: 0.0032201834261003466\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010667305061386691\n",
      "Average test loss: 0.003141709768316812\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01067887745011184\n",
      "Average test loss: 0.003106430161330435\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01066894238938888\n",
      "Average test loss: 0.0030556546923600964\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010661275435652998\n",
      "Average test loss: 0.003093785934150219\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010654515558646785\n",
      "Average test loss: 0.003098161321754257\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010647836912009451\n",
      "Average test loss: 0.003105788325683938\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010654881878031625\n",
      "Average test loss: 0.0031033544583866993\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01064024541940954\n",
      "Average test loss: 0.0031713639576401976\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010662124782800675\n",
      "Average test loss: 0.003122252765422066\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010634375552336374\n",
      "Average test loss: 0.0031076390734977193\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01066298914452394\n",
      "Average test loss: 0.003071811140825351\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010635768032736248\n",
      "Average test loss: 0.003127459079027176\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01063500460402833\n",
      "Average test loss: 0.003050070073041651\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01065018024461137\n",
      "Average test loss: 0.0031882661843879354\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010640036865240997\n",
      "Average test loss: 0.0032735223662522105\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01063647397607565\n",
      "Average test loss: 0.0031344920274698073\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010613453716039658\n",
      "Average test loss: 0.003177909983528985\n",
      "Epoch 277/300\n",
      "Average training loss: 0.010604678676360182\n",
      "Average test loss: 0.0030593129317793583\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01061035328441196\n",
      "Average test loss: 0.0030932427758557928\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010618784073326323\n",
      "Average test loss: 0.0032346581381021275\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010607720699575213\n",
      "Average test loss: 0.0031340711120929985\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010616607116328345\n",
      "Average test loss: 0.0031580070128871336\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010614810614122285\n",
      "Average test loss: 0.003107838418127762\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010588488298985694\n",
      "Average test loss: 0.003072383076987333\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010582539494666789\n",
      "Average test loss: 0.0031235438465244242\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010599799512161149\n",
      "Average test loss: 0.0031068714008563093\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010599829996625582\n",
      "Average test loss: 0.0031471861909247107\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010598403917418587\n",
      "Average test loss: 0.003075150000759297\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010579354017145104\n",
      "Average test loss: 0.0030811704794565836\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010590231988992956\n",
      "Average test loss: 0.0030756867598328327\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010580847279065185\n",
      "Average test loss: 0.0031097447093990115\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01057453332013554\n",
      "Average test loss: 0.0031250013694581057\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010573495212528441\n",
      "Average test loss: 0.0031705118361860514\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010571852193110519\n",
      "Average test loss: 0.00315925466393431\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010566708755162028\n",
      "Average test loss: 0.0030938798340244427\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010569430492818355\n",
      "Average test loss: 0.0030815258166856235\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010563885489271747\n",
      "Average test loss: 0.0032370332655393417\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010568535861041811\n",
      "Average test loss: 0.0032002081676489776\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010565772049956852\n",
      "Average test loss: 0.0031016627012027636\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010546535384323862\n",
      "Average test loss: 0.003071151988580823\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010542368069291114\n",
      "Average test loss: 0.0031921423704673844\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1118023302571641\n",
      "Average test loss: 0.003946040266503891\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02295347907476955\n",
      "Average test loss: 0.003554404763297902\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019494718117846383\n",
      "Average test loss: 0.0031528401606612734\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018252333010236422\n",
      "Average test loss: 0.003015754154158963\n",
      "Epoch 5/300\n",
      "Average training loss: 0.017528636044926115\n",
      "Average test loss: 0.0029668829815669194\n",
      "Epoch 6/300\n",
      "Average training loss: 0.016996089691917102\n",
      "Average test loss: 0.002856750580585665\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01657997918460104\n",
      "Average test loss: 0.002739170805551112\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01621589096552796\n",
      "Average test loss: 0.0027253468541635407\n",
      "Epoch 9/300\n",
      "Average training loss: 0.015870429328746265\n",
      "Average test loss: 0.0026309871763611835\n",
      "Epoch 10/300\n",
      "Average training loss: 0.015568534733520614\n",
      "Average test loss: 0.0025944538354459737\n",
      "Epoch 11/300\n",
      "Average training loss: 0.015268917828798294\n",
      "Average test loss: 0.0025546940478185813\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014991855654451582\n",
      "Average test loss: 0.0025059317033737896\n",
      "Epoch 13/300\n",
      "Average training loss: 0.014719050702121523\n",
      "Average test loss: 0.002531075248080823\n",
      "Epoch 14/300\n",
      "Average training loss: 0.014456813297337955\n",
      "Average test loss: 0.002440848330242766\n",
      "Epoch 15/300\n",
      "Average training loss: 0.014204325103925334\n",
      "Average test loss: 0.002419000377671586\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013949858635663986\n",
      "Average test loss: 0.0023900544161183966\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013726273997790283\n",
      "Average test loss: 0.002615662758756015\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013512967792650064\n",
      "Average test loss: 0.0023583568690551653\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013316085495054721\n",
      "Average test loss: 0.002322435623034835\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013123567086954911\n",
      "Average test loss: 0.00231542569047047\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012956392171896166\n",
      "Average test loss: 0.002407054264926248\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012804233727355799\n",
      "Average test loss: 0.0022844195171362824\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012663267298705047\n",
      "Average test loss: 0.0023581874321939216\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012513269216650063\n",
      "Average test loss: 0.0022549268359111415\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012402120647331078\n",
      "Average test loss: 0.0022849135673087503\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012287504454453787\n",
      "Average test loss: 0.002241254625428054\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012180249692665205\n",
      "Average test loss: 0.0022222113135374256\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01208499963581562\n",
      "Average test loss: 0.0022347056064754723\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012001716382801533\n",
      "Average test loss: 0.0022295616331199806\n",
      "Epoch 30/300\n",
      "Average training loss: 0.011924659899539418\n",
      "Average test loss: 0.0022273097665359578\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011847268709705936\n",
      "Average test loss: 0.0022075968751062948\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01177675304065148\n",
      "Average test loss: 0.0021922946963459254\n",
      "Epoch 33/300\n",
      "Average training loss: 0.011723155311412282\n",
      "Average test loss: 0.0021910295113921167\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011660762435860105\n",
      "Average test loss: 0.002207223690425356\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011603385579254891\n",
      "Average test loss: 0.002180281011905107\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011532286898129516\n",
      "Average test loss: 0.0021736186898003024\n",
      "Epoch 37/300\n",
      "Average training loss: 0.011485275988777478\n",
      "Average test loss: 0.0021914330128994255\n",
      "Epoch 38/300\n",
      "Average training loss: 0.011430071706573168\n",
      "Average test loss: 0.0021796839584906896\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01137880791972081\n",
      "Average test loss: 0.0021902687249498237\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011342758815321657\n",
      "Average test loss: 0.00218874022219744\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011314518578764464\n",
      "Average test loss: 0.002224926573948728\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011241970817248027\n",
      "Average test loss: 0.002171153546621402\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0112116549677319\n",
      "Average test loss: 0.0022018956103258664\n",
      "Epoch 44/300\n",
      "Average training loss: 0.011178537599742413\n",
      "Average test loss: 0.002167684072835578\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01111090561416414\n",
      "Average test loss: 0.00223318180296984\n",
      "Epoch 46/300\n",
      "Average training loss: 0.0110839916964372\n",
      "Average test loss: 0.0021798297555910215\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011048803658949004\n",
      "Average test loss: 0.0021645948102490768\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0110086632081204\n",
      "Average test loss: 0.002176433612075117\n",
      "Epoch 49/300\n",
      "Average training loss: 0.010953157094617685\n",
      "Average test loss: 0.0021678348969047267\n",
      "Epoch 50/300\n",
      "Average training loss: 0.010932034962707097\n",
      "Average test loss: 0.002185477889349891\n",
      "Epoch 51/300\n",
      "Average training loss: 0.010910912430120838\n",
      "Average test loss: 0.0022417152074890004\n",
      "Epoch 52/300\n",
      "Average training loss: 0.010861792868210209\n",
      "Average test loss: 0.0021856591316560905\n",
      "Epoch 53/300\n",
      "Average training loss: 0.010820549459920989\n",
      "Average test loss: 0.0021750451436059345\n",
      "Epoch 54/300\n",
      "Average training loss: 0.010773547566599316\n",
      "Average test loss: 0.00217246853456729\n",
      "Epoch 55/300\n",
      "Average training loss: 0.010735037881467078\n",
      "Average test loss: 0.0021661316408879226\n",
      "Epoch 56/300\n",
      "Average training loss: 0.010704833697941568\n",
      "Average test loss: 0.0021926428785340652\n",
      "Epoch 57/300\n",
      "Average training loss: 0.010685728107061651\n",
      "Average test loss: 0.0021917639810384976\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01066479030996561\n",
      "Average test loss: 0.0021983533876223696\n",
      "Epoch 59/300\n",
      "Average training loss: 0.010605459625522295\n",
      "Average test loss: 0.0022251095125037763\n",
      "Epoch 60/300\n",
      "Average training loss: 0.010583490509125921\n",
      "Average test loss: 0.002161413366181983\n",
      "Epoch 61/300\n",
      "Average training loss: 0.010546004687746366\n",
      "Average test loss: 0.002459966804211338\n",
      "Epoch 62/300\n",
      "Average training loss: 0.010532028552558687\n",
      "Average test loss: 0.002181628643224637\n",
      "Epoch 63/300\n",
      "Average training loss: 0.010484893249968688\n",
      "Average test loss: 0.0022019450867341622\n",
      "Epoch 64/300\n",
      "Average training loss: 0.010452114591995874\n",
      "Average test loss: 0.002220993141540223\n",
      "Epoch 65/300\n",
      "Average training loss: 0.010427466583748659\n",
      "Average test loss: 0.002192675089256631\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01040562850733598\n",
      "Average test loss: 0.002228001642351349\n",
      "Epoch 67/300\n",
      "Average training loss: 0.010364807816843192\n",
      "Average test loss: 0.0021892161005073124\n",
      "Epoch 68/300\n",
      "Average training loss: 0.010369751608206166\n",
      "Average test loss: 0.0022302050420807467\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01032149542040295\n",
      "Average test loss: 0.0022151615507900713\n",
      "Epoch 70/300\n",
      "Average training loss: 0.010286728532777891\n",
      "Average test loss: 0.002243607233174973\n",
      "Epoch 71/300\n",
      "Average training loss: 0.010254655758539835\n",
      "Average test loss: 0.0021922082556411625\n",
      "Epoch 72/300\n",
      "Average training loss: 0.010263205079568757\n",
      "Average test loss: 0.002201108975749877\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010231893781986502\n",
      "Average test loss: 0.0022168534933071997\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010189518303506903\n",
      "Average test loss: 0.0022189853321760893\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010167120441794396\n",
      "Average test loss: 0.0022633407010386386\n",
      "Epoch 76/300\n",
      "Average training loss: 0.010169256996777322\n",
      "Average test loss: 0.0022598763331770897\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010132243290543557\n",
      "Average test loss: 0.002234961273562577\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010093026489019394\n",
      "Average test loss: 0.0023143390060092013\n",
      "Epoch 79/300\n",
      "Average training loss: 0.010078780088987615\n",
      "Average test loss: 0.002253230488134755\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010042936969962385\n",
      "Average test loss: 0.0022894518771726224\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01005305584354533\n",
      "Average test loss: 0.0023207995450745026\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010016802944242955\n",
      "Average test loss: 0.0022367732934653758\n",
      "Epoch 83/300\n",
      "Average training loss: 0.010006905828913053\n",
      "Average test loss: 0.0022869794461876152\n",
      "Epoch 84/300\n",
      "Average training loss: 0.009993083480331634\n",
      "Average test loss: 0.0023017682167184024\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0099932274379664\n",
      "Average test loss: 0.0022506210725340577\n",
      "Epoch 86/300\n",
      "Average training loss: 0.00992885514597098\n",
      "Average test loss: 0.0022743144755562146\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00992838764604595\n",
      "Average test loss: 0.0023225373348428145\n",
      "Epoch 88/300\n",
      "Average training loss: 0.009903756951292355\n",
      "Average test loss: 0.002298377949020101\n",
      "Epoch 89/300\n",
      "Average training loss: 0.009900921794275442\n",
      "Average test loss: 0.0022631964311003684\n",
      "Epoch 90/300\n",
      "Average training loss: 0.009870077362491025\n",
      "Average test loss: 0.002299247588651876\n",
      "Epoch 91/300\n",
      "Average training loss: 0.009852610793378618\n",
      "Average test loss: 0.0022203850022827587\n",
      "Epoch 92/300\n",
      "Average training loss: 0.009838938964737786\n",
      "Average test loss: 0.002253654255842169\n",
      "Epoch 93/300\n",
      "Average training loss: 0.00982773988114463\n",
      "Average test loss: 0.002246659386282166\n",
      "Epoch 94/300\n",
      "Average training loss: 0.009803949196719461\n",
      "Average test loss: 0.0023277388798693815\n",
      "Epoch 95/300\n",
      "Average training loss: 0.009803384251064725\n",
      "Average test loss: 0.0022837150103102127\n",
      "Epoch 96/300\n",
      "Average training loss: 0.009781099982559681\n",
      "Average test loss: 0.002236039278614852\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0097557917162776\n",
      "Average test loss: 0.002459696779648463\n",
      "Epoch 98/300\n",
      "Average training loss: 0.009749398734834458\n",
      "Average test loss: 0.002422144201066759\n",
      "Epoch 99/300\n",
      "Average training loss: 0.009728614625831446\n",
      "Average test loss: 0.0022665279153734445\n",
      "Epoch 100/300\n",
      "Average training loss: 0.00970019135872523\n",
      "Average test loss: 0.002284726870453192\n",
      "Epoch 101/300\n",
      "Average training loss: 0.009706586276491484\n",
      "Average test loss: 0.00226894536614418\n",
      "Epoch 102/300\n",
      "Average training loss: 0.009698760932518376\n",
      "Average test loss: 0.0023166520446538923\n",
      "Epoch 103/300\n",
      "Average training loss: 0.00966195421665907\n",
      "Average test loss: 0.0022619460085406898\n",
      "Epoch 104/300\n",
      "Average training loss: 0.009670808699395921\n",
      "Average test loss: 0.0022638938661871683\n",
      "Epoch 105/300\n",
      "Average training loss: 0.0096396326298515\n",
      "Average test loss: 0.0023648019569615522\n",
      "Epoch 106/300\n",
      "Average training loss: 0.009626968622207641\n",
      "Average test loss: 0.0022900735555837553\n",
      "Epoch 107/300\n",
      "Average training loss: 0.009618802830576896\n",
      "Average test loss: 0.0024232953308771055\n",
      "Epoch 108/300\n",
      "Average training loss: 0.009603048072506984\n",
      "Average test loss: 0.002434937932735516\n",
      "Epoch 109/300\n",
      "Average training loss: 0.00959712745083703\n",
      "Average test loss: 0.0022971313577145338\n",
      "Epoch 110/300\n",
      "Average training loss: 0.009582733993728956\n",
      "Average test loss: 0.002380491325010856\n",
      "Epoch 111/300\n",
      "Average training loss: 0.009576122058762444\n",
      "Average test loss: 0.0023300457037985327\n",
      "Epoch 112/300\n",
      "Average training loss: 0.009558914214372634\n",
      "Average test loss: 0.002335506236180663\n",
      "Epoch 113/300\n",
      "Average training loss: 0.009546473394665453\n",
      "Average test loss: 0.002331790851532585\n",
      "Epoch 114/300\n",
      "Average training loss: 0.00953316182974312\n",
      "Average test loss: 0.0023773472437428104\n",
      "Epoch 115/300\n",
      "Average training loss: 0.009527104814847311\n",
      "Average test loss: 0.0022824122013731137\n",
      "Epoch 116/300\n",
      "Average training loss: 0.00952594778397017\n",
      "Average test loss: 0.0023210301000831856\n",
      "Epoch 117/300\n",
      "Average training loss: 0.009505051578084628\n",
      "Average test loss: 0.002326866826456454\n",
      "Epoch 118/300\n",
      "Average training loss: 0.009484172895550728\n",
      "Average test loss: 0.0023045156040332384\n",
      "Epoch 119/300\n",
      "Average training loss: 0.009473104434708755\n",
      "Average test loss: 0.0023725421536299916\n",
      "Epoch 120/300\n",
      "Average training loss: 0.009475034127632776\n",
      "Average test loss: 0.0022954765454762513\n",
      "Epoch 121/300\n",
      "Average training loss: 0.009461945508917173\n",
      "Average test loss: 0.0023511608818338977\n",
      "Epoch 122/300\n",
      "Average training loss: 0.009446920218567053\n",
      "Average test loss: 0.002324688132541875\n",
      "Epoch 123/300\n",
      "Average training loss: 0.009444700457155704\n",
      "Average test loss: 0.0023137670845414203\n",
      "Epoch 124/300\n",
      "Average training loss: 0.009435540091660288\n",
      "Average test loss: 0.002355003835219476\n",
      "Epoch 125/300\n",
      "Average training loss: 0.009423945686883397\n",
      "Average test loss: 0.002336420593576299\n",
      "Epoch 126/300\n",
      "Average training loss: 0.009411065910425452\n",
      "Average test loss: 0.0023668430831490293\n",
      "Epoch 127/300\n",
      "Average training loss: 0.009402189303603438\n",
      "Average test loss: 0.0023746816023356384\n",
      "Epoch 128/300\n",
      "Average training loss: 0.009382902588281367\n",
      "Average test loss: 0.0022989245696614187\n",
      "Epoch 129/300\n",
      "Average training loss: 0.009395971576372782\n",
      "Average test loss: 0.002371754273565279\n",
      "Epoch 130/300\n",
      "Average training loss: 0.009392978246013324\n",
      "Average test loss: 0.002339581812835402\n",
      "Epoch 131/300\n",
      "Average training loss: 0.009378682726787196\n",
      "Average test loss: 0.0023651880768852103\n",
      "Epoch 132/300\n",
      "Average training loss: 0.009367666700647937\n",
      "Average test loss: 0.0023100771616316506\n",
      "Epoch 133/300\n",
      "Average training loss: 0.009352943809496032\n",
      "Average test loss: 0.002342287353032993\n",
      "Epoch 134/300\n",
      "Average training loss: 0.009342267830338744\n",
      "Average test loss: 0.002383253790645136\n",
      "Epoch 135/300\n",
      "Average training loss: 0.009359774881766902\n",
      "Average test loss: 0.0023396347704240017\n",
      "Epoch 136/300\n",
      "Average training loss: 0.009323999216159186\n",
      "Average test loss: 0.002357065337606602\n",
      "Epoch 137/300\n",
      "Average training loss: 0.009333834843503105\n",
      "Average test loss: 0.0024275503411061235\n",
      "Epoch 138/300\n",
      "Average training loss: 0.009317096961041292\n",
      "Average test loss: 0.002468437963268823\n",
      "Epoch 139/300\n",
      "Average training loss: 0.009302165471431282\n",
      "Average test loss: 0.002404258528103431\n",
      "Epoch 140/300\n",
      "Average training loss: 0.009297608030339083\n",
      "Average test loss: 0.002365627992898226\n",
      "Epoch 141/300\n",
      "Average training loss: 0.009301176771521568\n",
      "Average test loss: 0.0023841163511905406\n",
      "Epoch 142/300\n",
      "Average training loss: 0.00928294884827402\n",
      "Average test loss: 0.002387844467949536\n",
      "Epoch 143/300\n",
      "Average training loss: 0.009272976560725106\n",
      "Average test loss: 0.0023894749085108438\n",
      "Epoch 144/300\n",
      "Average training loss: 0.009273405279136367\n",
      "Average test loss: 0.002427533355438047\n",
      "Epoch 145/300\n",
      "Average training loss: 0.009263684197432466\n",
      "Average test loss: 0.00235022103238023\n",
      "Epoch 146/300\n",
      "Average training loss: 0.009259481486346987\n",
      "Average test loss: 0.0023806364298280744\n",
      "Epoch 147/300\n",
      "Average training loss: 0.00924810377177265\n",
      "Average test loss: 0.002300367459654808\n",
      "Epoch 148/300\n",
      "Average training loss: 0.00925514344125986\n",
      "Average test loss: 0.002346446704533365\n",
      "Epoch 149/300\n",
      "Average training loss: 0.00923502144879765\n",
      "Average test loss: 0.00234239352049513\n",
      "Epoch 150/300\n",
      "Average training loss: 0.009219428687459893\n",
      "Average test loss: 0.0024249835322714512\n",
      "Epoch 151/300\n",
      "Average training loss: 0.009218727590309249\n",
      "Average test loss: 0.0024433192311682636\n",
      "Epoch 152/300\n",
      "Average training loss: 0.009212065236435996\n",
      "Average test loss: 0.0023861035625967713\n",
      "Epoch 153/300\n",
      "Average training loss: 0.009197049841284752\n",
      "Average test loss: 0.0024503707455264196\n",
      "Epoch 154/300\n",
      "Average training loss: 0.009206476877133052\n",
      "Average test loss: 0.002451319579035044\n",
      "Epoch 155/300\n",
      "Average training loss: 0.009192349172300763\n",
      "Average test loss: 0.002359106064049734\n",
      "Epoch 156/300\n",
      "Average training loss: 0.00918809773441818\n",
      "Average test loss: 0.0023873825517172617\n",
      "Epoch 157/300\n",
      "Average training loss: 0.009187559494541751\n",
      "Average test loss: 0.0023678634379886918\n",
      "Epoch 158/300\n",
      "Average training loss: 0.00917018344004949\n",
      "Average test loss: 0.002448899888536996\n",
      "Epoch 159/300\n",
      "Average training loss: 0.009168326830284464\n",
      "Average test loss: 0.002383622625221809\n",
      "Epoch 160/300\n",
      "Average training loss: 0.009166109585927592\n",
      "Average test loss: 0.0024288137047034173\n",
      "Epoch 161/300\n",
      "Average training loss: 0.009165068661587106\n",
      "Average test loss: 0.002429563873137037\n",
      "Epoch 162/300\n",
      "Average training loss: 0.009161430026921961\n",
      "Average test loss: 0.0023594381291833187\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0091574409181873\n",
      "Average test loss: 0.002392233453070124\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00913152026053932\n",
      "Average test loss: 0.002366060110222962\n",
      "Epoch 165/300\n",
      "Average training loss: 0.009134241663333443\n",
      "Average test loss: 0.002443901402875781\n",
      "Epoch 166/300\n",
      "Average training loss: 0.009127196273869938\n",
      "Average test loss: 0.0023798439343356423\n",
      "Epoch 167/300\n",
      "Average training loss: 0.009122953190571732\n",
      "Average test loss: 0.0023797469815860194\n",
      "Epoch 168/300\n",
      "Average training loss: 0.009121668092492555\n",
      "Average test loss: 0.0025210863227645556\n",
      "Epoch 169/300\n",
      "Average training loss: 0.009129183089567556\n",
      "Average test loss: 0.0024410343223975764\n",
      "Epoch 170/300\n",
      "Average training loss: 0.009098354967104065\n",
      "Average test loss: 0.0023732743730975523\n",
      "Epoch 171/300\n",
      "Average training loss: 0.009094047523621055\n",
      "Average test loss: 0.0024149989357425106\n",
      "Epoch 172/300\n",
      "Average training loss: 0.009090495796253284\n",
      "Average test loss: 0.002422115457554658\n",
      "Epoch 173/300\n",
      "Average training loss: 0.00909458821763595\n",
      "Average test loss: 0.0024895927239623333\n",
      "Epoch 174/300\n",
      "Average training loss: 0.009089588771263759\n",
      "Average test loss: 0.002407751872928606\n",
      "Epoch 175/300\n",
      "Average training loss: 0.009080094571742747\n",
      "Average test loss: 0.0024787109910200038\n",
      "Epoch 176/300\n",
      "Average training loss: 0.00907363417910205\n",
      "Average test loss: 0.00246258918216659\n",
      "Epoch 177/300\n",
      "Average training loss: 0.009072551113035943\n",
      "Average test loss: 0.00238823453285214\n",
      "Epoch 178/300\n",
      "Average training loss: 0.009060490453408824\n",
      "Average test loss: 0.0024097666941169235\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0090548808682296\n",
      "Average test loss: 0.0024029301042772003\n",
      "Epoch 180/300\n",
      "Average training loss: 0.009056272108521727\n",
      "Average test loss: 0.0024226219721345437\n",
      "Epoch 181/300\n",
      "Average training loss: 0.009063458030422529\n",
      "Average test loss: 0.0023768309127125474\n",
      "Epoch 182/300\n",
      "Average training loss: 0.009045323159959581\n",
      "Average test loss: 0.0023758242399328283\n",
      "Epoch 183/300\n",
      "Average training loss: 0.009042774291502105\n",
      "Average test loss: 0.002398797140042815\n",
      "Epoch 184/300\n",
      "Average training loss: 0.009042936838335461\n",
      "Average test loss: 0.002428285069142779\n",
      "Epoch 185/300\n",
      "Average training loss: 0.009040913722167412\n",
      "Average test loss: 0.0024180854053960908\n",
      "Epoch 186/300\n",
      "Average training loss: 0.009020266373124388\n",
      "Average test loss: 0.00246744682536357\n",
      "Epoch 187/300\n",
      "Average training loss: 0.009025925926864147\n",
      "Average test loss: 0.0024080397768153085\n",
      "Epoch 188/300\n",
      "Average training loss: 0.009011041519542535\n",
      "Average test loss: 0.002367246528259582\n",
      "Epoch 189/300\n",
      "Average training loss: 0.009027109758307537\n",
      "Average test loss: 0.00234462322274016\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00901050079945061\n",
      "Average test loss: 0.0024674242194741964\n",
      "Epoch 191/300\n",
      "Average training loss: 0.00900080059717099\n",
      "Average test loss: 0.0024692030037856763\n",
      "Epoch 192/300\n",
      "Average training loss: 0.009010133377379841\n",
      "Average test loss: 0.002413344163033697\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008990556163092455\n",
      "Average test loss: 0.0023944592455195056\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008991435332430733\n",
      "Average test loss: 0.00250091319055193\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008994434548748865\n",
      "Average test loss: 0.0023920761566195224\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008989950654821263\n",
      "Average test loss: 0.002408080634350578\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008994202771120601\n",
      "Average test loss: 0.0023975227226813636\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008977298650476667\n",
      "Average test loss: 0.002416588350509604\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008972709513372845\n",
      "Average test loss: 0.002472432880765862\n",
      "Epoch 200/300\n",
      "Average training loss: 0.00896806478417582\n",
      "Average test loss: 0.0024053209291564092\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008967155075735516\n",
      "Average test loss: 0.0024666046563328967\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00896407607735859\n",
      "Average test loss: 0.002443791292814745\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00896657651414474\n",
      "Average test loss: 0.0024082846784343323\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008972772965828577\n",
      "Average test loss: 0.002370208870205614\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008956539621783628\n",
      "Average test loss: 0.0023845948562439944\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008950285604016648\n",
      "Average test loss: 0.0023606607009553247\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0089408271378941\n",
      "Average test loss: 0.002467585186370545\n",
      "Epoch 208/300\n",
      "Average training loss: 0.008934673180182776\n",
      "Average test loss: 0.0024143936562662323\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008925166003819969\n",
      "Average test loss: 0.0024334115584691367\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008941596573425664\n",
      "Average test loss: 0.002430775166799625\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008939658083849483\n",
      "Average test loss: 0.0024803321377063793\n",
      "Epoch 212/300\n",
      "Average training loss: 0.00892134805685944\n",
      "Average test loss: 0.0024325629545168745\n",
      "Epoch 213/300\n",
      "Average training loss: 0.00891812025838428\n",
      "Average test loss: 0.0024210603543453747\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008926178991380665\n",
      "Average test loss: 0.0024597738830165732\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008917692503167523\n",
      "Average test loss: 0.0023797829523682593\n",
      "Epoch 216/300\n",
      "Average training loss: 0.00891516162869003\n",
      "Average test loss: 0.0024426349765724604\n",
      "Epoch 217/300\n",
      "Average training loss: 0.00891358728127347\n",
      "Average test loss: 0.0024488023527794414\n",
      "Epoch 218/300\n",
      "Average training loss: 0.008898542972074614\n",
      "Average test loss: 0.0024658864490273926\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008909557970033751\n",
      "Average test loss: 0.0024425267200503084\n",
      "Epoch 220/300\n",
      "Average training loss: 0.00889281210468875\n",
      "Average test loss: 0.0024225789521717363\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008885223750438955\n",
      "Average test loss: 0.0024973565766380893\n",
      "Epoch 222/300\n",
      "Average training loss: 0.008897762950095865\n",
      "Average test loss: 0.002421267509046528\n",
      "Epoch 223/300\n",
      "Average training loss: 0.008887438996798462\n",
      "Average test loss: 0.002475528112716145\n",
      "Epoch 224/300\n",
      "Average training loss: 0.008890493459171719\n",
      "Average test loss: 0.0024110933103495174\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008893129402564632\n",
      "Average test loss: 0.0024712886743040547\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008871934173835649\n",
      "Average test loss: 0.0024606858591238656\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008867024621201886\n",
      "Average test loss: 0.002454950285454591\n",
      "Epoch 228/300\n",
      "Average training loss: 0.00886467096540663\n",
      "Average test loss: 0.002435990156398879\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008865151418579948\n",
      "Average test loss: 0.002518825416556663\n",
      "Epoch 230/300\n",
      "Average training loss: 0.00886664699514707\n",
      "Average test loss: 0.002436831271275878\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008860503272049958\n",
      "Average test loss: 0.0024284969396475287\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008862237148814732\n",
      "Average test loss: 0.002404773463184635\n",
      "Epoch 233/300\n",
      "Average training loss: 0.008856497696704334\n",
      "Average test loss: 0.002456620872227682\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008852026670343346\n",
      "Average test loss: 0.0024964949283748867\n",
      "Epoch 235/300\n",
      "Average training loss: 0.00884415039130383\n",
      "Average test loss: 0.002488259381511145\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008843472517198987\n",
      "Average test loss: 0.002508295822681652\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008845532615979512\n",
      "Average test loss: 0.0024116700862844786\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00884704110191928\n",
      "Average test loss: 0.002449767054989934\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008837099605136447\n",
      "Average test loss: 0.002539054156281054\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008823514892823167\n",
      "Average test loss: 0.0024425628261847626\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008824723535527785\n",
      "Average test loss: 0.0024839487796028455\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008825674263139566\n",
      "Average test loss: 0.0024851855852951604\n",
      "Epoch 243/300\n",
      "Average training loss: 0.008830000010215574\n",
      "Average test loss: 0.0024426771980813806\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008822329725656244\n",
      "Average test loss: 0.0024309064800747565\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008824058505396049\n",
      "Average test loss: 0.002463392382901576\n",
      "Epoch 246/300\n",
      "Average training loss: 0.008813596648474534\n",
      "Average test loss: 0.0024075785951895847\n",
      "Epoch 247/300\n",
      "Average training loss: 0.008815054727097352\n",
      "Average test loss: 0.002421412083734241\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008813936040633255\n",
      "Average test loss: 0.002383181488555339\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008804249650074376\n",
      "Average test loss: 0.002464465498096413\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008805144500401286\n",
      "Average test loss: 0.0024080590814765956\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008817597664478753\n",
      "Average test loss: 0.002511775547845496\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008792676760090722\n",
      "Average test loss: 0.0024086735378950836\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008797301045722431\n",
      "Average test loss: 0.002442453560833302\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00879208796388573\n",
      "Average test loss: 0.002490108321524329\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008787220791810088\n",
      "Average test loss: 0.0024519624635577202\n",
      "Epoch 256/300\n",
      "Average training loss: 0.008785654441350037\n",
      "Average test loss: 0.002433432585042384\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008798700425773858\n",
      "Average test loss: 0.002470643027582102\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008790327930616008\n",
      "Average test loss: 0.002505067620012495\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008780497683419122\n",
      "Average test loss: 0.0025088742355712586\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008770047819448842\n",
      "Average test loss: 0.00238473883147041\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008770932926899858\n",
      "Average test loss: 0.0025097826818625134\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008769158656398456\n",
      "Average test loss: 0.002495120646845963\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008764473845561345\n",
      "Average test loss: 0.00247944748101549\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008778439344631302\n",
      "Average test loss: 0.0024405789888567397\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008770842934648197\n",
      "Average test loss: 0.0025048159609238306\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008773820529381434\n",
      "Average test loss: 0.0023794578193790383\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008761495533088843\n",
      "Average test loss: 0.002484800176281068\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008750804911884997\n",
      "Average test loss: 0.0024472922162256305\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008761758105622398\n",
      "Average test loss: 0.002461357612369789\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008763963628974226\n",
      "Average test loss: 0.0024757740897023016\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008759397806392776\n",
      "Average test loss: 0.0025133890319201683\n",
      "Epoch 272/300\n",
      "Average training loss: 0.008752679688235124\n",
      "Average test loss: 0.0025200325520709156\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008744515544838375\n",
      "Average test loss: 0.0024022595232559576\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008740664478805331\n",
      "Average test loss: 0.00245354541670531\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008731475855741236\n",
      "Average test loss: 0.002459554258320067\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008741630656851662\n",
      "Average test loss: 0.0024182725600484347\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008736480571329594\n",
      "Average test loss: 0.002458674975567394\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008732458910594383\n",
      "Average test loss: 0.0025567727026840053\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008724354135493438\n",
      "Average test loss: 0.0024504090957343577\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008723139970666833\n",
      "Average test loss: 0.0024607205390930176\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008729413003557259\n",
      "Average test loss: 0.0024752908390429287\n",
      "Epoch 282/300\n",
      "Average training loss: 0.00872024529096153\n",
      "Average test loss: 0.0024595479431251685\n",
      "Epoch 283/300\n",
      "Average training loss: 0.00872660978924897\n",
      "Average test loss: 0.002507026585853762\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008722798010541334\n",
      "Average test loss: 0.0023949014940816494\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008736982257829772\n",
      "Average test loss: 0.002448660218467315\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008708006181650692\n",
      "Average test loss: 0.0024484556208675106\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008714230084584818\n",
      "Average test loss: 0.002465080382509364\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008710259085728063\n",
      "Average test loss: 0.002464743653519286\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008715531032118532\n",
      "Average test loss: 0.0023840868839373193\n",
      "Epoch 290/300\n",
      "Average training loss: 0.00871286676906877\n",
      "Average test loss: 0.0024355001812800766\n",
      "Epoch 291/300\n",
      "Average training loss: 0.008695439118478034\n",
      "Average test loss: 0.002423511847025818\n",
      "Epoch 292/300\n",
      "Average training loss: 0.008706931870016787\n",
      "Average test loss: 0.002486869444656703\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00870118580179082\n",
      "Average test loss: 0.002481533662726482\n",
      "Epoch 294/300\n",
      "Average training loss: 0.008697015932864613\n",
      "Average test loss: 0.0024651402357137864\n",
      "Epoch 295/300\n",
      "Average training loss: 0.008688903294503688\n",
      "Average test loss: 0.0025093134610603253\n",
      "Epoch 296/300\n",
      "Average training loss: 0.008695321037537522\n",
      "Average test loss: 0.0023807361045231424\n",
      "Epoch 297/300\n",
      "Average training loss: 0.008694076125406557\n",
      "Average test loss: 0.0025143503189707797\n",
      "Epoch 298/300\n",
      "Average training loss: 0.008674876981311374\n",
      "Average test loss: 0.0025012270210103857\n",
      "Epoch 299/300\n",
      "Average training loss: 0.008691265924109353\n",
      "Average test loss: 0.002451781394994921\n",
      "Epoch 300/300\n",
      "Average training loss: 0.008676966532237\n",
      "Average test loss: 0.002465063365900682\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth10/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.79\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.95\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.04\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.18\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.57\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.48920220957862\n",
      "Average test loss: 0.007076031241152022\n",
      "Epoch 2/300\n",
      "Average training loss: 0.9671934022903442\n",
      "Average test loss: 0.005336230638954375\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5478480949136946\n",
      "Average test loss: 0.0048936198788384596\n",
      "Epoch 4/300\n",
      "Average training loss: 0.37016886202494304\n",
      "Average test loss: 0.01762278480993377\n",
      "Epoch 5/300\n",
      "Average training loss: 0.28464762491650053\n",
      "Average test loss: 0.017616070902181997\n",
      "Epoch 6/300\n",
      "Average training loss: 0.22980844399664138\n",
      "Average test loss: 0.0046111225254005855\n",
      "Epoch 7/300\n",
      "Average training loss: 0.19083112805419497\n",
      "Average test loss: 0.004546569008794096\n",
      "Epoch 8/300\n",
      "Average training loss: 0.16299562699927225\n",
      "Average test loss: 0.004495847891395291\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14178976191414727\n",
      "Average test loss: 0.004467321132206254\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12574197469154993\n",
      "Average test loss: 0.0044283225573599335\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11341614284118016\n",
      "Average test loss: 0.004453017003420327\n",
      "Epoch 12/300\n",
      "Average training loss: 0.10343809177478154\n",
      "Average test loss: 0.004414582757900159\n",
      "Epoch 13/300\n",
      "Average training loss: 0.09570577984386021\n",
      "Average test loss: 0.004370931537201007\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08968337117301094\n",
      "Average test loss: 0.004398564313434892\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08456687023904588\n",
      "Average test loss: 0.004329735204577446\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08072249205907187\n",
      "Average test loss: 0.004515267453673813\n",
      "Epoch 17/300\n",
      "Average training loss: 0.07776660166846382\n",
      "Average test loss: 0.060710158361328974\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07543403901656469\n",
      "Average test loss: 0.007210531496339374\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0733673929505878\n",
      "Average test loss: 0.004277888565427727\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07267870183123483\n",
      "Average test loss: 0.004267075467026896\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07081643191973368\n",
      "Average test loss: 0.0042500601108703346\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0694189728266663\n",
      "Average test loss: 0.004312001489930683\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0684408483505249\n",
      "Average test loss: 0.004320210133575731\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06775711888406012\n",
      "Average test loss: 0.0042323208240171275\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06726368474298053\n",
      "Average test loss: 0.004219718803548151\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06664593529038959\n",
      "Average test loss: 0.004216010462906625\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0661172189017137\n",
      "Average test loss: 0.004312914093128509\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0656222914689117\n",
      "Average test loss: 0.004211885667095581\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06523648584551281\n",
      "Average test loss: 0.004209373501439889\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0649620803727044\n",
      "Average test loss: 0.0042005504746403955\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06456396062837706\n",
      "Average test loss: 0.004198169786069128\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06436664946873982\n",
      "Average test loss: 0.004185219939384196\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06406634053587913\n",
      "Average test loss: 0.004195810601943069\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06379585456185871\n",
      "Average test loss: 0.0041849883832037445\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06359777220752504\n",
      "Average test loss: 0.004222218106604285\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06338639563984341\n",
      "Average test loss: 0.004179442377967967\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06317789157562785\n",
      "Average test loss: 0.004173215886784924\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06303326097461913\n",
      "Average test loss: 0.004208211113595301\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06278523850109842\n",
      "Average test loss: 0.004189173996034596\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06264172427521812\n",
      "Average test loss: 0.004212984132270018\n",
      "Epoch 41/300\n",
      "Average training loss: 0.062418381485674114\n",
      "Average test loss: 0.004156354901691278\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06226903905140029\n",
      "Average test loss: 0.004171881429437134\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06218012446827359\n",
      "Average test loss: 0.004175744793481297\n",
      "Epoch 44/300\n",
      "Average training loss: 0.061991135365433166\n",
      "Average test loss: 0.004234287480513255\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06183618160751131\n",
      "Average test loss: 0.004179995440360572\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06169458982679579\n",
      "Average test loss: 0.004162549009753598\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06153852363096343\n",
      "Average test loss: 0.004232135627418756\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06132944207059012\n",
      "Average test loss: 0.004251255033951667\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0612180950111813\n",
      "Average test loss: 0.004591363116270966\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06104863038659096\n",
      "Average test loss: 0.0041745550148189065\n",
      "Epoch 51/300\n",
      "Average training loss: 0.060893384830819235\n",
      "Average test loss: 0.004154465063164632\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06075865407122506\n",
      "Average test loss: 0.004169237074752649\n",
      "Epoch 53/300\n",
      "Average training loss: 0.060682163970337974\n",
      "Average test loss: 0.004142313303218947\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06041952556702826\n",
      "Average test loss: 0.004179800813396772\n",
      "Epoch 55/300\n",
      "Average training loss: 0.060283516960011586\n",
      "Average test loss: 0.004197593016342985\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0602222425142924\n",
      "Average test loss: 0.004231507998166813\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06001953497197893\n",
      "Average test loss: 0.004173901619182693\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05978083940347036\n",
      "Average test loss: 0.004155371285561058\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05968054228027662\n",
      "Average test loss: 0.004248661714916428\n",
      "Epoch 60/300\n",
      "Average training loss: 0.059551249821980796\n",
      "Average test loss: 0.004160194941278961\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0593849595354663\n",
      "Average test loss: 0.00419333426343898\n",
      "Epoch 62/300\n",
      "Average training loss: 0.059206400546762676\n",
      "Average test loss: 0.004148602375967635\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05903277183903588\n",
      "Average test loss: 0.004201504316801826\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05894191493259536\n",
      "Average test loss: 0.004192921089629332\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05873749025000466\n",
      "Average test loss: 0.004164449742891722\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05854813952578439\n",
      "Average test loss: 0.004294365681293938\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05845997542805142\n",
      "Average test loss: 0.004212575615487165\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05827076150642501\n",
      "Average test loss: 0.004228664419717259\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05812175883187188\n",
      "Average test loss: 0.0041668092660191985\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05797523557477527\n",
      "Average test loss: 0.004257349756856759\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05778727537559138\n",
      "Average test loss: 0.004279793413148986\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05761306579907735\n",
      "Average test loss: 0.004244800570938323\n",
      "Epoch 73/300\n",
      "Average training loss: 0.057528617991341485\n",
      "Average test loss: 0.004329653920398818\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05728988865349028\n",
      "Average test loss: 0.004432403473804395\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05720517623755667\n",
      "Average test loss: 0.004222287704547246\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05705106881260872\n",
      "Average test loss: 0.004254391695062319\n",
      "Epoch 77/300\n",
      "Average training loss: 0.056955805874533125\n",
      "Average test loss: 0.0042281315254254475\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05672607907321718\n",
      "Average test loss: 0.004289906346135669\n",
      "Epoch 79/300\n",
      "Average training loss: 0.056578874303234945\n",
      "Average test loss: 0.004271590444362826\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05644382018182013\n",
      "Average test loss: 0.004277798754473527\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05633445332116551\n",
      "Average test loss: 0.004342758725086848\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05615510344174173\n",
      "Average test loss: 0.004272263000615769\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05604484645856751\n",
      "Average test loss: 0.004207930995772283\n",
      "Epoch 84/300\n",
      "Average training loss: 0.055914917767047885\n",
      "Average test loss: 0.00440192862558696\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0557754938768016\n",
      "Average test loss: 0.0042474332149657935\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05567499217059877\n",
      "Average test loss: 0.004282268955061833\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05550923249787754\n",
      "Average test loss: 0.004270295398102866\n",
      "Epoch 88/300\n",
      "Average training loss: 0.055391549496187104\n",
      "Average test loss: 0.0043627733521991304\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05525120640794436\n",
      "Average test loss: 0.0043215202643639514\n",
      "Epoch 90/300\n",
      "Average training loss: 0.055149759077363546\n",
      "Average test loss: 0.004285531871020794\n",
      "Epoch 91/300\n",
      "Average training loss: 0.055023210710949365\n",
      "Average test loss: 0.00442443093078004\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05490493209163348\n",
      "Average test loss: 0.00431195739739471\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05470055796702703\n",
      "Average test loss: 0.004443457018997934\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05461817150480217\n",
      "Average test loss: 0.004352509472105238\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05446596901615461\n",
      "Average test loss: 0.004344919161664115\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05446355097492536\n",
      "Average test loss: 0.004291937320596642\n",
      "Epoch 97/300\n",
      "Average training loss: 0.054388405174016954\n",
      "Average test loss: 0.004367930269489686\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05412311864892642\n",
      "Average test loss: 0.004316855190321803\n",
      "Epoch 99/300\n",
      "Average training loss: 0.054083502779404324\n",
      "Average test loss: 0.0044348706756201055\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05397260341379378\n",
      "Average test loss: 0.004259427237013976\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05379747282134162\n",
      "Average test loss: 0.004277459228618277\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0537808714972602\n",
      "Average test loss: 0.004371160823851824\n",
      "Epoch 103/300\n",
      "Average training loss: 0.053548112418916494\n",
      "Average test loss: 0.00449683968267507\n",
      "Epoch 104/300\n",
      "Average training loss: 0.053617783278226853\n",
      "Average test loss: 0.004398690865892503\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05350081574585703\n",
      "Average test loss: 0.0043133524527980225\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05331547559963332\n",
      "Average test loss: 0.0043066370656920806\n",
      "Epoch 107/300\n",
      "Average training loss: 0.053164805110957884\n",
      "Average test loss: 0.004331136539785398\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05312230140301916\n",
      "Average test loss: 0.004422175329799454\n",
      "Epoch 109/300\n",
      "Average training loss: 0.053036560780472225\n",
      "Average test loss: 0.004416475456414951\n",
      "Epoch 110/300\n",
      "Average training loss: 0.052865844133827423\n",
      "Average test loss: 0.004440494688434733\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05279885897702641\n",
      "Average test loss: 0.004381735824048519\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05268326021896468\n",
      "Average test loss: 0.004456282080461582\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05261694388257133\n",
      "Average test loss: 0.004318744619894359\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05252504554722044\n",
      "Average test loss: 0.004359143419398202\n",
      "Epoch 115/300\n",
      "Average training loss: 0.052448932296699946\n",
      "Average test loss: 0.004385406292974949\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0523102120856444\n",
      "Average test loss: 0.004451916502581702\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05224090172184838\n",
      "Average test loss: 0.0044161986203657255\n",
      "Epoch 118/300\n",
      "Average training loss: 0.052187893980079225\n",
      "Average test loss: 0.0045136298048827385\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05212697941395972\n",
      "Average test loss: 0.004431823641061783\n",
      "Epoch 120/300\n",
      "Average training loss: 0.052058417522244986\n",
      "Average test loss: 0.004467662853913174\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05190065974328253\n",
      "Average test loss: 0.004469780361486806\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05180246693558163\n",
      "Average test loss: 0.004320337598936425\n",
      "Epoch 123/300\n",
      "Average training loss: 0.051780378265513315\n",
      "Average test loss: 0.004601754146731562\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05162937759028541\n",
      "Average test loss: 0.004346445958233542\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05164458594719569\n",
      "Average test loss: 0.004480399411585596\n",
      "Epoch 126/300\n",
      "Average training loss: 0.051519892364740374\n",
      "Average test loss: 0.00459287048296796\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05134285279115041\n",
      "Average test loss: 0.004426560803833935\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05142556770973735\n",
      "Average test loss: 0.004467518112518721\n",
      "Epoch 129/300\n",
      "Average training loss: 0.051266984916395614\n",
      "Average test loss: 0.0044528892143732976\n",
      "Epoch 130/300\n",
      "Average training loss: 0.051179402771923274\n",
      "Average test loss: 0.004473296269774437\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05113942297299703\n",
      "Average test loss: 0.004480186697095633\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05106117454502317\n",
      "Average test loss: 0.004543890408343739\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05096756176153819\n",
      "Average test loss: 0.004461999352193541\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05089489431182544\n",
      "Average test loss: 0.004584670369823774\n",
      "Epoch 135/300\n",
      "Average training loss: 0.050790946119361455\n",
      "Average test loss: 0.004475582800805569\n",
      "Epoch 136/300\n",
      "Average training loss: 0.050772995442152025\n",
      "Average test loss: 0.004437578458752897\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05063604418436686\n",
      "Average test loss: 0.004413188444657459\n",
      "Epoch 138/300\n",
      "Average training loss: 0.050617753396431604\n",
      "Average test loss: 0.004527941456271543\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05052950341833962\n",
      "Average test loss: 0.00450519616363777\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05043888125816981\n",
      "Average test loss: 0.004450604935487112\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05033268493082788\n",
      "Average test loss: 0.004481906954612997\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05031784699691667\n",
      "Average test loss: 0.004570551285727156\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05026212078664038\n",
      "Average test loss: 0.00446609715744853\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05015915820995966\n",
      "Average test loss: 0.004501231312337849\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05015272590186861\n",
      "Average test loss: 0.00448822993454006\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05004323253697819\n",
      "Average test loss: 0.004505071319639683\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04992870897054672\n",
      "Average test loss: 0.0045941346664395595\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04998562920093536\n",
      "Average test loss: 0.004500170970542563\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04982553217808405\n",
      "Average test loss: 0.004498242360022333\n",
      "Epoch 150/300\n",
      "Average training loss: 0.049828140904506046\n",
      "Average test loss: 0.0045515171289443965\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04976115786698129\n",
      "Average test loss: 0.004431818437659078\n",
      "Epoch 152/300\n",
      "Average training loss: 0.049654995405011705\n",
      "Average test loss: 0.004471534877187676\n",
      "Epoch 153/300\n",
      "Average training loss: 0.049624417225519815\n",
      "Average test loss: 0.004420609403815535\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04956984265976482\n",
      "Average test loss: 0.004648671519632141\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04945218223664496\n",
      "Average test loss: 0.004564484453035726\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04942070292433103\n",
      "Average test loss: 0.004481042159514294\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04939750925037596\n",
      "Average test loss: 0.004472909450944927\n",
      "Epoch 158/300\n",
      "Average training loss: 0.049345260381698605\n",
      "Average test loss: 0.004398730715943707\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04925383701920509\n",
      "Average test loss: 0.004423202582324545\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04921107544170485\n",
      "Average test loss: 0.004585715227656894\n",
      "Epoch 161/300\n",
      "Average training loss: 0.04916089019179344\n",
      "Average test loss: 0.0045379323412974674\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0491068042020003\n",
      "Average test loss: 0.004469884837667147\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04906067596541511\n",
      "Average test loss: 0.004581615412814749\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0490511993335353\n",
      "Average test loss: 0.004637057900014851\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0489301885565122\n",
      "Average test loss: 0.004518152070956098\n",
      "Epoch 166/300\n",
      "Average training loss: 0.048889284018013214\n",
      "Average test loss: 0.004552304796460602\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04882047246893247\n",
      "Average test loss: 0.004554724224739604\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04879364643494288\n",
      "Average test loss: 0.004623690508719948\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04872428791721662\n",
      "Average test loss: 0.004552907193079591\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04868340888288286\n",
      "Average test loss: 0.00456447116451131\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04859749504923821\n",
      "Average test loss: 0.004615167131854428\n",
      "Epoch 172/300\n",
      "Average training loss: 0.048579976111650464\n",
      "Average test loss: 0.004481020425756772\n",
      "Epoch 173/300\n",
      "Average training loss: 0.048524360766013463\n",
      "Average test loss: 0.004532430383066336\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04845391466882494\n",
      "Average test loss: 0.004565675349285205\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04842883479595184\n",
      "Average test loss: 0.0045805056716005\n",
      "Epoch 176/300\n",
      "Average training loss: 0.048346562776300644\n",
      "Average test loss: 0.004606440536263916\n",
      "Epoch 177/300\n",
      "Average training loss: 0.048360544734530976\n",
      "Average test loss: 0.004639697110487355\n",
      "Epoch 178/300\n",
      "Average training loss: 0.048250891834497454\n",
      "Average test loss: 0.00455547875041763\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04817019565900167\n",
      "Average test loss: 0.00449449642168151\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04820823035471969\n",
      "Average test loss: 0.004481156297441986\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04817162506116761\n",
      "Average test loss: 0.0046393730528652664\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04815894946455956\n",
      "Average test loss: 0.004739248855246438\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04801531253589524\n",
      "Average test loss: 0.0046612103035052616\n",
      "Epoch 184/300\n",
      "Average training loss: 0.047996198101176155\n",
      "Average test loss: 0.004557392213907507\n",
      "Epoch 185/300\n",
      "Average training loss: 0.047954696360561586\n",
      "Average test loss: 0.004529654330677456\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04788757856355773\n",
      "Average test loss: 0.004626065860605902\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04787007871601317\n",
      "Average test loss: 0.004524028574220009\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04780039243234528\n",
      "Average test loss: 0.00454895431175828\n",
      "Epoch 189/300\n",
      "Average training loss: 0.047756850040621225\n",
      "Average test loss: 0.004500241751886076\n",
      "Epoch 190/300\n",
      "Average training loss: 0.047778262697988086\n",
      "Average test loss: 0.0045031173800428705\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04769003857175509\n",
      "Average test loss: 0.004481826449433963\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04760476209719976\n",
      "Average test loss: 0.0044295345805585385\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04758310788538721\n",
      "Average test loss: 0.004602791387587786\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04755737441778183\n",
      "Average test loss: 0.004845716934858097\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04752545166346762\n",
      "Average test loss: 0.0044928881443209116\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04748382327953975\n",
      "Average test loss: 0.004468967659191953\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04744534580575095\n",
      "Average test loss: 0.004730873834548725\n",
      "Epoch 198/300\n",
      "Average training loss: 0.047327901479270726\n",
      "Average test loss: 0.0044821577974491645\n",
      "Epoch 199/300\n",
      "Average training loss: 0.047399102124902934\n",
      "Average test loss: 0.004576620748059617\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04733128390047285\n",
      "Average test loss: 0.004541093681007624\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04727504117952453\n",
      "Average test loss: 0.0046241069473326206\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04728921763930056\n",
      "Average test loss: 0.004547876371691625\n",
      "Epoch 203/300\n",
      "Average training loss: 0.047151025576723946\n",
      "Average test loss: 0.004543380031155215\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04714223922292392\n",
      "Average test loss: 0.00458181804128819\n",
      "Epoch 205/300\n",
      "Average training loss: 0.047079224053356385\n",
      "Average test loss: 0.004621839789880647\n",
      "Epoch 207/300\n",
      "Average training loss: 0.046972752696937986\n",
      "Average test loss: 0.0046326666155623066\n",
      "Epoch 209/300\n",
      "Average training loss: 0.046910090383556154\n",
      "Average test loss: 0.004516276891860697\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04691516200039122\n",
      "Average test loss: 0.004544702290246884\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04688031749592887\n",
      "Average test loss: 0.004580801243997282\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04678139673670133\n",
      "Average test loss: 0.004611183649135961\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04676196188396878\n",
      "Average test loss: 0.0045977903393407665\n",
      "Epoch 215/300\n",
      "Average training loss: 0.046786062088277604\n",
      "Average test loss: 0.0044602566725677915\n",
      "Epoch 216/300\n",
      "Average training loss: 0.046663578005300625\n",
      "Average test loss: 0.0045584118925035\n",
      "Epoch 217/300\n",
      "Average training loss: 0.046586007131470576\n",
      "Average test loss: 0.004560735721968942\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0466323742667834\n",
      "Average test loss: 0.004565294302999973\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04659627337919341\n",
      "Average test loss: 0.004696549275269111\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04670513385534286\n",
      "Average test loss: 0.004748027543640799\n",
      "Epoch 221/300\n",
      "Average training loss: 0.046511731382873324\n",
      "Average test loss: 0.004760646384209395\n",
      "Epoch 222/300\n",
      "Average training loss: 0.046421402484178545\n",
      "Average test loss: 0.004565990204198493\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04642084916101562\n",
      "Average test loss: 0.004553535343044334\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04641470381617546\n",
      "Average test loss: 0.004571004146503078\n",
      "Epoch 225/300\n",
      "Average training loss: 0.046338055498070185\n",
      "Average test loss: 0.004595125414431095\n",
      "Epoch 227/300\n",
      "Average training loss: 0.046286620845397315\n",
      "Average test loss: 0.004525368896209531\n",
      "Epoch 228/300\n",
      "Average training loss: 0.046205635223123764\n",
      "Average test loss: 0.004517071829073959\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04621653244230482\n",
      "Average test loss: 0.004531241692602634\n",
      "Epoch 230/300\n",
      "Average training loss: 0.046144446495506496\n",
      "Average test loss: 0.004610614993506007\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04616424140996403\n",
      "Average test loss: 0.004560440907668736\n",
      "Epoch 232/300\n",
      "Average training loss: 0.046139237211810215\n",
      "Average test loss: 0.004577731081801984\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04617205145292812\n",
      "Average test loss: 0.004945504664132992\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04612526445587476\n",
      "Average test loss: 0.004769078540719218\n",
      "Epoch 235/300\n",
      "Average training loss: 0.045980206635263234\n",
      "Average test loss: 0.004731076284829113\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0459654475748539\n",
      "Average test loss: 0.0045388500255843\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04592651637064086\n",
      "Average test loss: 0.00453543598887821\n",
      "Epoch 238/300\n",
      "Average training loss: 0.045951997800005806\n",
      "Average test loss: 0.0045882400617831285\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04589421912034353\n",
      "Average test loss: 0.004683496497364508\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04583812665608194\n",
      "Average test loss: 0.004770712916221884\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0458768554561668\n",
      "Average test loss: 0.004671380058758789\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04575822660326958\n",
      "Average test loss: 0.0045931581712017455\n",
      "Epoch 243/300\n",
      "Average training loss: 0.045742225365506275\n",
      "Average test loss: 0.0049007462670819625\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04580788088507123\n",
      "Average test loss: 0.004645720333274868\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04572110367483563\n",
      "Average test loss: 0.004724531630674998\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04568761862648858\n",
      "Average test loss: 0.004536951781560977\n",
      "Epoch 247/300\n",
      "Average training loss: 0.045605164090792336\n",
      "Average test loss: 0.004624019475653767\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04559729954600334\n",
      "Average test loss: 0.004624366322118375\n",
      "Epoch 249/300\n",
      "Average training loss: 0.045593801711996396\n",
      "Average test loss: 0.004624511674046516\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04562876011927922\n",
      "Average test loss: 0.004735181983353363\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04556715730494923\n",
      "Average test loss: 0.004463106930669811\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04549677774641249\n",
      "Average test loss: 0.004591756983349721\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04546882876753807\n",
      "Average test loss: 0.004686379576722781\n",
      "Epoch 254/300\n",
      "Average training loss: 0.045469775391949546\n",
      "Average test loss: 0.00454868918698695\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0453458242184586\n",
      "Average test loss: 0.004662166577660375\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04541316709915797\n",
      "Average test loss: 0.004673016340989206\n",
      "Epoch 257/300\n",
      "Average training loss: 0.045429098354445564\n",
      "Average test loss: 0.004641389085600773\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0453701392908891\n",
      "Average test loss: 0.004546957364512815\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0453230568435457\n",
      "Average test loss: 0.004555803136693107\n",
      "Epoch 260/300\n",
      "Average training loss: 0.045250176201264064\n",
      "Average test loss: 0.004673057885219653\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04526724654767248\n",
      "Average test loss: 0.004706060748133394\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04525822281506327\n",
      "Average test loss: 0.004757260637357831\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04520006842083401\n",
      "Average test loss: 0.004750200160675578\n",
      "Epoch 264/300\n",
      "Average test loss: 0.004605322611000803\n",
      "Epoch 266/300\n",
      "Average training loss: 0.045137299007839624\n",
      "Average test loss: 0.004572451360523701\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04507244291901588\n",
      "Average test loss: 0.0046000016654531165\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04507505844367875\n",
      "Average test loss: 0.004906844668918186\n",
      "Epoch 269/300\n",
      "Average training loss: 0.045110975921154026\n",
      "Average test loss: 0.004819959938526154\n",
      "Epoch 270/300\n",
      "Average training loss: 0.045021102090676626\n",
      "Average test loss: 0.004532411833604177\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04498637109001478\n",
      "Average test loss: 0.004656609715272983\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04488795219858487\n",
      "Average test loss: 0.004660445175651047\n",
      "Epoch 273/300\n",
      "Average training loss: 0.044981279098325305\n",
      "Average test loss: 0.004689098697569635\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04494818337427245\n",
      "Average test loss: 0.004687577010442813\n",
      "Epoch 275/300\n",
      "Average training loss: 0.044877598938014775\n",
      "Average test loss: 0.004592120033585363\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04494139505757226\n",
      "Average test loss: 0.00457583433844977\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04481813732783\n",
      "Average test loss: 0.0045755887909068\n",
      "Epoch 278/300\n",
      "Average training loss: 0.044786761068635515\n",
      "Average test loss: 0.004749917819682095\n",
      "Epoch 279/300\n",
      "Average training loss: 0.044760277893808154\n",
      "Average test loss: 0.004615120540062586\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04476684681574503\n",
      "Average test loss: 0.004587255971092317\n",
      "Epoch 282/300\n",
      "Average training loss: 0.044723183191484876\n",
      "Average test loss: 0.004535411326628593\n",
      "Epoch 283/300\n",
      "Average training loss: 0.044662202245659297\n",
      "Average test loss: 0.004581768385238117\n",
      "Epoch 284/300\n",
      "Average training loss: 0.044647848198811214\n",
      "Average test loss: 0.004483942663297057\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04470315502418412\n",
      "Average test loss: 0.004560046415776014\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04461891628636254\n",
      "Average test loss: 0.004625971656706598\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04458854767349031\n",
      "Average test loss: 0.004622694889290465\n",
      "Epoch 288/300\n",
      "Average training loss: 0.044607280496093964\n",
      "Average test loss: 0.004618855178563131\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0445734480784999\n",
      "Average test loss: 0.004582113067516022\n",
      "Epoch 290/300\n",
      "Average training loss: 0.044515760726398894\n",
      "Average test loss: 0.004685937453889184\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04452348576982816\n",
      "Average test loss: 0.004576969039109018\n",
      "Epoch 292/300\n",
      "Average training loss: 0.044413420226838855\n",
      "Average test loss: 0.004649060918639103\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04447316487630208\n",
      "Average test loss: 0.0046226131984343135\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04440759544240104\n",
      "Average test loss: 0.004686342991060681\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04443447021643321\n",
      "Average test loss: 0.005899758304986688\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04433583573500315\n",
      "Average test loss: 0.004464332361602121\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04432068363494343\n",
      "Average test loss: 0.004731056447244353\n",
      "Epoch 300/300\n",
      "Average training loss: 0.044345329854223466\n",
      "Average test loss: 0.004667018866787354\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7550101792547438\n",
      "Average test loss: 0.004715952402187719\n",
      "Epoch 4/300\n",
      "Average training loss: 0.5252489122019873\n",
      "Average test loss: 0.004385220851749182\n",
      "Epoch 5/300\n",
      "Average training loss: 0.4049189789030287\n",
      "Average test loss: 0.0042331119767493675\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3197127789656321\n",
      "Average test loss: 0.007687588561740187\n",
      "Epoch 7/300\n",
      "Average training loss: 0.25994885550604924\n",
      "Average test loss: 0.004108217085194257\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2168751839266883\n",
      "Average test loss: 0.004027452282607555\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1842428310977088\n",
      "Average test loss: 0.004094605547272497\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15887418666150835\n",
      "Average test loss: 0.003957947908590238\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1383360685110092\n",
      "Average test loss: 0.003866959285404947\n",
      "Epoch 12/300\n",
      "Average training loss: 0.12222036265002356\n",
      "Average test loss: 0.0038144837595108483\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10946760947174496\n",
      "Average test loss: 0.003757133285618491\n",
      "Epoch 14/300\n",
      "Average training loss: 0.09892578742239211\n",
      "Average test loss: 0.005123342647941576\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09068853249814775\n",
      "Average test loss: 0.0038567863379915554\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07461486930317349\n",
      "Average test loss: 0.003607972408541375\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07125138552983602\n",
      "Average test loss: 0.0035842035532825524\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06842497632569737\n",
      "Average test loss: 0.003575162493520313\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06621390537420908\n",
      "Average test loss: 0.0035364955458790062\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06448080561227269\n",
      "Average test loss: 0.0035143398812247646\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06289994640482796\n",
      "Average test loss: 0.003623044652450416\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06166264296902551\n",
      "Average test loss: 0.0035057716853916644\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06061386336882909\n",
      "Average test loss: 0.0034675355847511025\n",
      "Epoch 26/300\n",
      "Average training loss: 0.059675193217065596\n",
      "Average test loss: 0.0034864111844864158\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05902906304597855\n",
      "Average test loss: 0.003450743737320105\n",
      "Epoch 28/300\n",
      "Average training loss: 0.05827561094694667\n",
      "Average test loss: 0.0034563957295484014\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05773503522078196\n",
      "Average test loss: 0.0034647024346308574\n",
      "Epoch 30/300\n",
      "Average training loss: 0.057118555227915445\n",
      "Average test loss: 0.0034581960207886167\n",
      "Epoch 31/300\n",
      "Average training loss: 0.05560490792989731\n",
      "Average test loss: 0.003406386754165093\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0550981916586558\n",
      "Average test loss: 0.003384803206556373\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05465602637661828\n",
      "Average test loss: 0.0033900554387105834\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05431881915860706\n",
      "Average test loss: 0.0034189522278805576\n",
      "Epoch 37/300\n",
      "Average training loss: 0.053918681114912034\n",
      "Average test loss: 0.0033759710681107308\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05356608350740539\n",
      "Average test loss: 0.0033629256768359077\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05317630832062827\n",
      "Average test loss: 0.00341915197401411\n",
      "Epoch 40/300\n",
      "Average training loss: 0.052837335255410935\n",
      "Average test loss: 0.0034014452629619173\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05246838971972465\n",
      "Average test loss: 0.0034829319158775937\n",
      "Epoch 42/300\n",
      "Average training loss: 0.052220412611961364\n",
      "Average test loss: 0.003357226150524285\n",
      "Epoch 43/300\n",
      "Average training loss: 0.051957964771323736\n",
      "Average test loss: 0.0033998619311799605\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05161504763033655\n",
      "Average test loss: 0.0033818843853142527\n",
      "Epoch 45/300\n",
      "Average training loss: 0.051258581664827135\n",
      "Average test loss: 0.00354546302350031\n",
      "Epoch 46/300\n",
      "Average training loss: 0.051044193834066394\n",
      "Average test loss: 0.003461597548176845\n",
      "Epoch 47/300\n",
      "Average test loss: 0.003367468553698725\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05018616067700916\n",
      "Average test loss: 0.0034222311505840886\n",
      "Epoch 50/300\n",
      "Average training loss: 0.049847795175181495\n",
      "Average test loss: 0.0035356336691313318\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04962714056505097\n",
      "Average test loss: 0.003427024619653821\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04924423321750429\n",
      "Average test loss: 0.0034628204575015437\n",
      "Epoch 53/300\n",
      "Average training loss: 0.049021754930416744\n",
      "Average test loss: 0.0034820675611909892\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04875786068042119\n",
      "Average test loss: 0.0035232469000750118\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04851141987575425\n",
      "Average test loss: 0.003437536917419897\n",
      "Epoch 56/300\n",
      "Average training loss: 0.048303857240411974\n",
      "Average test loss: 0.003362976954629024\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04799128251274427\n",
      "Average test loss: 0.003386364097189572\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04780685651633475\n",
      "Average test loss: 0.003346382073437174\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04760788333415985\n",
      "Average test loss: 0.003359388935483164\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04729296206103431\n",
      "Average test loss: 0.0033674927254517873\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04677251689301597\n",
      "Average test loss: 0.0034760009424967897\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04658427596423361\n",
      "Average test loss: 0.0034078260995447636\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04637800215350257\n",
      "Average test loss: 0.0034762926722566286\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04615994061695205\n",
      "Average test loss: 0.0034310685319619044\n",
      "Epoch 66/300\n",
      "Average training loss: 0.045888377457857135\n",
      "Average test loss: 0.0035388925162454445\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04568559634023243\n",
      "Average test loss: 0.0034098555725067853\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04545605416761504\n",
      "Average test loss: 0.0035318051191667718\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04540004462997119\n",
      "Average test loss: 0.003433637348521087\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0451809998816914\n",
      "Average test loss: 0.0033985693384375837\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04497715373502837\n",
      "Average test loss: 0.0035076210550549955\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04470442622900009\n",
      "Average test loss: 0.0034775633646382227\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04441432268420855\n",
      "Average test loss: 0.0034759607762098314\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04424228468868468\n",
      "Average test loss: 0.0034329280451767977\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04412691188520856\n",
      "Average test loss: 0.003457180786256989\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0438888616628117\n",
      "Average test loss: 0.003714663366890616\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04382914929423067\n",
      "Average test loss: 0.0035108410304205287\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04366664664944013\n",
      "Average test loss: 0.003524701797713836\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04350438379247983\n",
      "Average test loss: 0.003555126790992088\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04337438201904297\n",
      "Average test loss: 0.0035164481478018894\n",
      "Epoch 82/300\n",
      "Average training loss: 0.043256845904721154\n",
      "Average test loss: 0.003455889772209856\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04310787757900026\n",
      "Average test loss: 0.0034836892779502603\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04293716712130441\n",
      "Average test loss: 0.003556943327188492\n",
      "Epoch 85/300\n",
      "Average training loss: 0.042830724285708534\n",
      "Average test loss: 0.003509414510490994\n",
      "Epoch 86/300\n",
      "Average training loss: 0.042675769209861755\n",
      "Average test loss: 0.00353273631259799\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04260930856068929\n",
      "Average test loss: 0.0037193134191135565\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04251284141672982\n",
      "Average test loss: 0.0034558020449346966\n",
      "Epoch 89/300\n",
      "Average training loss: 0.042341524329450396\n",
      "Average test loss: 0.00371732271504071\n",
      "Epoch 90/300\n",
      "Average training loss: 0.042251809467871985\n",
      "Average test loss: 0.003489116294309497\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04200668340921402\n",
      "Average test loss: 0.003861642541984717\n",
      "Epoch 93/300\n",
      "Average training loss: 0.041859120349089306\n",
      "Average test loss: 0.0036042074383132987\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04174779952234692\n",
      "Average test loss: 0.0035987740827517377\n",
      "Epoch 95/300\n",
      "Average training loss: 0.041737613234255046\n",
      "Average test loss: 0.0036383267336835465\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04157439613342285\n",
      "Average test loss: 0.003599948688927624\n",
      "Epoch 97/300\n",
      "Average training loss: 0.041439562131961184\n",
      "Average test loss: 0.0035685073799557156\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04132593944006496\n",
      "Average test loss: 0.0034805318088167243\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04123381664355596\n",
      "Average test loss: 0.003600946384171645\n",
      "Epoch 100/300\n",
      "Average training loss: 0.041153046220541\n",
      "Average test loss: 0.0034963766754501397\n",
      "Epoch 101/300\n",
      "Average training loss: 0.041114123867617716\n",
      "Average test loss: 0.003595977367212375\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04110316434171465\n",
      "Average test loss: 0.0035586886981295216\n",
      "Epoch 103/300\n",
      "Average training loss: 0.04092117636733585\n",
      "Average test loss: 0.003719505467555589\n",
      "Epoch 104/300\n",
      "Average training loss: 0.040662544127967624\n",
      "Average test loss: 0.0036279787495732308\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0405798557764954\n",
      "Average test loss: 0.0035050185405545763\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04059503568543328\n",
      "Average test loss: 0.0036032630000263455\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04052075667017036\n",
      "Average test loss: 0.0035379409959746732\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04044758259753386\n",
      "Average test loss: 0.003615902009937498\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04022964559329881\n",
      "Average test loss: 0.003563872889512115\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04026676920718617\n",
      "Average test loss: 0.0035872168648574086\n",
      "Epoch 112/300\n",
      "Average training loss: 0.040090794588128725\n",
      "Average test loss: 0.0037369341769566138\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04003077502714263\n",
      "Average test loss: 0.0036073570305274593\n",
      "Epoch 114/300\n",
      "Average training loss: 0.039998288985755706\n",
      "Average test loss: 0.003536107656442457\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0399075852798091\n",
      "Average test loss: 0.0036871893517673015\n",
      "Epoch 116/300\n",
      "Average training loss: 0.039776916921138765\n",
      "Average test loss: 0.0035760422924326525\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03986014166143206\n",
      "Average test loss: 0.0036119505148380993\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03959928360250261\n",
      "Average test loss: 0.0035788195725116466\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03950488407082028\n",
      "Average test loss: 0.0036557028657860226\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03944888522889879\n",
      "Average test loss: 0.0036302101901835864\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03934436939491166\n",
      "Average test loss: 0.0037103113503091866\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03936373919910855\n",
      "Average test loss: 0.003586442037175099\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0393339076075289\n",
      "Average test loss: 0.0035508312698867587\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03924504240685039\n",
      "Average test loss: 0.0037142514545056553\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03913331694404284\n",
      "Average test loss: 0.003592359640945991\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03910730465584331\n",
      "Average test loss: 0.003579303477787309\n",
      "Epoch 128/300\n",
      "Average training loss: 0.039029881871408885\n",
      "Average test loss: 0.0035849993916021454\n",
      "Epoch 129/300\n",
      "Average training loss: 0.039022760616408456\n",
      "Average test loss: 0.003735724808855189\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0389439726571242\n",
      "Average test loss: 0.00359931772078077\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03888775903979937\n",
      "Average test loss: 0.003639913806070884\n",
      "Epoch 132/300\n",
      "Average training loss: 0.038769854810502796\n",
      "Average test loss: 0.0036135489895111985\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03867443513207965\n",
      "Average test loss: 0.0035673828663097488\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03862056134475602\n",
      "Average test loss: 0.0036132367137405607\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03849664472209083\n",
      "Average test loss: 0.0036786962364696793\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03856347617506981\n",
      "Average test loss: 0.0036100680641829967\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03850224081012938\n",
      "Average test loss: 0.003679732513303558\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03831925048430761\n",
      "Average test loss: 0.0036418853954722484\n",
      "Epoch 141/300\n",
      "Average training loss: 0.038234482819835346\n",
      "Average test loss: 0.0037843167562451627\n",
      "Epoch 142/300\n",
      "Average training loss: 0.038183267520533665\n",
      "Average test loss: 0.003709928626194596\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03820279929041862\n",
      "Average test loss: 0.0036634930330846044\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03814803009066317\n",
      "Average test loss: 0.0036352181161443393\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03805790426002609\n",
      "Average test loss: 0.003561210943592919\n",
      "Epoch 146/300\n",
      "Average training loss: 0.038023505671156775\n",
      "Average test loss: 0.0035491132311936883\n",
      "Epoch 147/300\n",
      "Average training loss: 0.037936121506823434\n",
      "Average test loss: 0.003702591620799568\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0379318864080641\n",
      "Average test loss: 0.0036314804674022726\n",
      "Epoch 149/300\n",
      "Average training loss: 0.037844827589061526\n",
      "Average test loss: 0.0036723733221491178\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03774839146931966\n",
      "Average test loss: 0.0035880959458235236\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03773048530684577\n",
      "Average test loss: 0.003620798410847783\n",
      "Epoch 154/300\n",
      "Average training loss: 0.037679848877920044\n",
      "Average test loss: 0.003623774355277419\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03766412278016408\n",
      "Average test loss: 0.00372073446917865\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03755117074151834\n",
      "Average test loss: 0.003783095508399937\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03751836997270584\n",
      "Average test loss: 0.0036514658983796836\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03748587786985768\n",
      "Average test loss: 0.0036450968028770552\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0374494137234158\n",
      "Average test loss: 0.0036690607594533097\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03739779555797577\n",
      "Average test loss: 0.0036865097346405187\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03734165378411611\n",
      "Average test loss: 0.0036358977576924694\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03730935076706939\n",
      "Average test loss: 0.003670464551800655\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03734952132900556\n",
      "Average test loss: 0.0037410892186065517\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03724204530980852\n",
      "Average test loss: 0.0036950257979333403\n",
      "Epoch 165/300\n",
      "Average training loss: 0.037242625557714036\n",
      "Average test loss: 0.003648276354703638\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03704864169657231\n",
      "Average test loss: 0.003665765046245522\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03709874728322029\n",
      "Average test loss: 0.0037562781845529875\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03707634335094028\n",
      "Average test loss: 0.0038092674687504767\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03703524137867822\n",
      "Average test loss: 0.0037856707957883676\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03688724881410599\n",
      "Average test loss: 0.0036119747947280607\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03694116959969203\n",
      "Average test loss: 0.003616366661671135\n",
      "Epoch 172/300\n",
      "Average training loss: 0.036929042008188036\n",
      "Average test loss: 0.003639195006340742\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03686866260237164\n",
      "Average test loss: 0.0036436197416236005\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03683849626779556\n",
      "Average test loss: 0.003578996987806426\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0368155039648215\n",
      "Average test loss: 0.0036806108293433986\n",
      "Epoch 176/300\n",
      "Average training loss: 0.037071023257242305\n",
      "Average test loss: 0.003760781819207801\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03676149269276195\n",
      "Average test loss: 0.003699065103299088\n",
      "Epoch 178/300\n",
      "Average training loss: 0.036669951061407725\n",
      "Average test loss: 0.0037151650180005367\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03665538791153166\n",
      "Average test loss: 0.003682661898020241\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03660514205031925\n",
      "Average test loss: 0.003763733506202698\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03660635903146532\n",
      "Average test loss: 0.0038387520267731613\n",
      "Epoch 182/300\n",
      "Average training loss: 0.036567543880807026\n",
      "Average test loss: 0.003690373715427187\n",
      "Epoch 183/300\n",
      "Average training loss: 0.036586309098535115\n",
      "Average test loss: 0.003894056109090646\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03647457406587071\n",
      "Average test loss: 0.0036516813702053494\n",
      "Epoch 185/300\n",
      "Average training loss: 0.036425757245885\n",
      "Average test loss: 0.0037021390638417666\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03644225440422694\n",
      "Average test loss: 0.0037866617066578735\n",
      "Epoch 187/300\n",
      "Average training loss: 0.036422143591774835\n",
      "Average test loss: 0.0037635257616639136\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03638377547595236\n",
      "Average test loss: 0.0037802560192843277\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03633243663443459\n",
      "Average test loss: 0.0036977918263938693\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03627771246433258\n",
      "Average test loss: 0.0036956645941568747\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03624421720372306\n",
      "Average test loss: 0.0037330747668941817\n",
      "Epoch 192/300\n",
      "Average training loss: 0.036308753564953805\n",
      "Average test loss: 0.0038236428221894636\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03619232943322923\n",
      "Average test loss: 0.0037105331648555066\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03619843554165628\n",
      "Average test loss: 0.0036764426802595456\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03617701915568776\n",
      "Average test loss: 0.0037999712398482694\n",
      "Epoch 196/300\n",
      "Average training loss: 0.036067843152417074\n",
      "Average test loss: 0.003760354249013795\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03607477554513348\n",
      "Average test loss: 0.0036388669142292606\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03606055642830001\n",
      "Average test loss: 0.003748195833630032\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03601219355397754\n",
      "Average test loss: 0.003619228542678886\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03604040501018365\n",
      "Average test loss: 0.003736293111824327\n",
      "Epoch 201/300\n",
      "Average training loss: 0.035929571817318595\n",
      "Average test loss: 0.0037424112115469243\n",
      "Epoch 202/300\n",
      "Average training loss: 0.035928912825054594\n",
      "Average test loss: 0.0036781274715645446\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03588107726640172\n",
      "Average test loss: 0.003753131434114443\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0358915833234787\n",
      "Average test loss: 0.0037239456665184765\n",
      "Epoch 205/300\n",
      "Average training loss: 0.035871638483471344\n",
      "Average test loss: 0.0036202129138012728\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03585920882721742\n",
      "Average test loss: 0.003759187222768863\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03576092588239246\n",
      "Average test loss: 0.0037073314187841284\n",
      "Epoch 208/300\n",
      "Average training loss: 0.035777791245116125\n",
      "Average test loss: 0.0036638729862040944\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03577965012523863\n",
      "Average test loss: 0.003746777740617593\n",
      "Epoch 210/300\n",
      "Average training loss: 0.035682661255200705\n",
      "Average test loss: 0.0037881906322307056\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03565049302246835\n",
      "Average test loss: 0.0037135870456695558\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03562332132458687\n",
      "Average test loss: 0.00380207893707686\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03565854694114791\n",
      "Average test loss: 0.00405863734624452\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0355593110571305\n",
      "Average test loss: 0.0037368934814714723\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03561390147937669\n",
      "Average test loss: 0.003826701057040029\n",
      "Epoch 216/300\n",
      "Average training loss: 0.035564545803599884\n",
      "Average test loss: 0.003707574050252636\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03551602249675327\n",
      "Average test loss: 0.003587812155692114\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03541041727198495\n",
      "Average test loss: 0.003813985004193253\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03545723407136069\n",
      "Average test loss: 0.0037575670023345287\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03544477978017595\n",
      "Average test loss: 0.0037218633527970977\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03543014539612664\n",
      "Average test loss: 0.0037795519447988936\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03538010687298245\n",
      "Average test loss: 0.003686937843966815\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03537501811318927\n",
      "Average test loss: 0.0037204365982777543\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03530573969748285\n",
      "Average test loss: 0.003777161805373099\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03531932935449812\n",
      "Average test loss: 0.003714649535508619\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03532750771774186\n",
      "Average test loss: 0.0036931919265124533\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03533407978879081\n",
      "Average test loss: 0.003715157106725706\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03524856334593561\n",
      "Average test loss: 0.0037729989542729324\n",
      "Epoch 229/300\n",
      "Average training loss: 0.035192449516720244\n",
      "Average test loss: 0.003783850993340214\n",
      "Epoch 230/300\n",
      "Average training loss: 0.035238674110836456\n",
      "Average test loss: 0.0036421510010129876\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0351765949163172\n",
      "Average test loss: 0.0037656258383972777\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03515935134059853\n",
      "Average test loss: 0.003779254616341657\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03513180085188813\n",
      "Average test loss: 0.0037388973627239465\n",
      "Epoch 234/300\n",
      "Average training loss: 0.035089523416426446\n",
      "Average test loss: 0.0037732099044240183\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03507857002152337\n",
      "Average test loss: 0.0038571837126380867\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03502762692835596\n",
      "Average test loss: 0.0037711185018221537\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03498398057288594\n",
      "Average test loss: 0.0037572717943953142\n",
      "Epoch 238/300\n",
      "Average training loss: 0.035018707974089514\n",
      "Average test loss: 0.0038186476909452013\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035020679470565586\n",
      "Average test loss: 0.003875968725937936\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03493595984246996\n",
      "Average test loss: 0.003843051523384121\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03497000311149491\n",
      "Average test loss: 0.003742796813655231\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0349394028517935\n",
      "Average test loss: 0.003974687651420633\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03489776415295071\n",
      "Average test loss: 0.0037013118606474666\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03487477953566445\n",
      "Average test loss: 0.00367812686579095\n",
      "Epoch 245/300\n",
      "Average training loss: 0.034883968841698434\n",
      "Average test loss: 0.003706502463668585\n",
      "Epoch 246/300\n",
      "Average training loss: 0.034815550211403104\n",
      "Average test loss: 0.0039298243857920166\n",
      "Epoch 247/300\n",
      "Average training loss: 0.034808820558918846\n",
      "Average test loss: 0.0037452364801946613\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03480380736788114\n",
      "Average test loss: 0.0037265699565824533\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03472228597932392\n",
      "Average test loss: 0.0036734123238258893\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03469485502772861\n",
      "Average test loss: 0.003795140734977192\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03470137608713574\n",
      "Average test loss: 0.0037444833740592\n",
      "Epoch 252/300\n",
      "Average training loss: 0.034656414806842804\n",
      "Average test loss: 0.003700799474078748\n",
      "Epoch 253/300\n",
      "Average training loss: 0.034727196084128484\n",
      "Average test loss: 0.0038057784607840907\n",
      "Epoch 254/300\n",
      "Average training loss: 0.034834673490789204\n",
      "Average test loss: 0.0037154216455916565\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03470426121354103\n",
      "Average test loss: 0.004070134879607294\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0346046087079578\n",
      "Average test loss: 0.003774593618181017\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03458707392215729\n",
      "Average test loss: 0.003845027539051241\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03463607528474596\n",
      "Average test loss: 0.0037694045673641896\n",
      "Epoch 259/300\n",
      "Average training loss: 0.034524750371774035\n",
      "Average test loss: 0.00384187834089001\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03454011412792735\n",
      "Average test loss: 0.003749361917376518\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03456676968269878\n",
      "Average test loss: 0.0038788870169470706\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03453348751862844\n",
      "Average test loss: 0.003837919744766421\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03454630211326811\n",
      "Average test loss: 0.0038310949094593526\n",
      "Epoch 264/300\n",
      "Average training loss: 0.034454193646709125\n",
      "Average test loss: 0.003757017840941747\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03441690382692549\n",
      "Average test loss: 0.0037847023910532393\n",
      "Epoch 266/300\n",
      "Average training loss: 0.034478458841641746\n",
      "Average test loss: 0.003759734878523482\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034402665356794995\n",
      "Average test loss: 0.00403232774655852\n",
      "Epoch 268/300\n",
      "Average training loss: 0.034406523552205824\n",
      "Average test loss: 0.0037185538386305173\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03438841278354327\n",
      "Average test loss: 0.0038671541404392985\n",
      "Epoch 270/300\n",
      "Average training loss: 0.034396299534373814\n",
      "Average test loss: 0.0039167362619191405\n",
      "Epoch 271/300\n",
      "Average training loss: 0.034319648053910995\n",
      "Average test loss: 0.003865904562175274\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03428721127576298\n",
      "Average test loss: 0.003818895178743535\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03432779094907973\n",
      "Average test loss: 0.0038241402978698414\n",
      "Epoch 274/300\n",
      "Average training loss: 0.034328474478589166\n",
      "Average test loss: 0.00391341706406739\n",
      "Epoch 275/300\n",
      "Average training loss: 0.034223578217956754\n",
      "Average test loss: 0.0037718112398352887\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03421208225356208\n",
      "Average test loss: 0.0037352724435428778\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03427357296480073\n",
      "Average test loss: 0.0038154787868261335\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03422695706619157\n",
      "Average test loss: 0.0037907731479240787\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03419474022918277\n",
      "Average test loss: 0.0037729633214573064\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03419301124413808\n",
      "Average test loss: 0.003849652111116383\n",
      "Epoch 281/300\n",
      "Average training loss: 0.034136288482281896\n",
      "Average test loss: 0.0037987391087743972\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03412126793132888\n",
      "Average test loss: 0.0037919375076889993\n",
      "Epoch 283/300\n",
      "Average training loss: 0.034115441670020424\n",
      "Average test loss: 0.003760304323087136\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03413124785489506\n",
      "Average test loss: 0.0037900641115589276\n",
      "Epoch 285/300\n",
      "Average training loss: 0.034114545848634506\n",
      "Average test loss: 0.003764139702750577\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03401825458308061\n",
      "Average test loss: 0.003921662061578697\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03408870670530531\n",
      "Average test loss: 0.0037974065670536624\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03402096116542816\n",
      "Average test loss: 0.0037940278854221104\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03397253637181388\n",
      "Average test loss: 0.0037828402059773606\n",
      "Epoch 290/300\n",
      "Average training loss: 0.033974819916817875\n",
      "Average test loss: 0.0038081873377992046\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03408427723745505\n",
      "Average test loss: 0.0037417827252712514\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0339793832997481\n",
      "Average test loss: 0.0037856687034169834\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03394248570005099\n",
      "Average test loss: 0.0038521501094930704\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03393412642346488\n",
      "Average test loss: 0.0037552289358443684\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0339547838403119\n",
      "Average test loss: 0.003740774009169804\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03387779016792774\n",
      "Average test loss: 0.0037564891887207825\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03387091877725389\n",
      "Average test loss: 0.004019548997903863\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03388129683004485\n",
      "Average test loss: 0.0037429815398322213\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0338706601427661\n",
      "Average test loss: 0.003881140052858326\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03384026013149156\n",
      "Average test loss: 0.0037155234817829396\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.348566780196296\n",
      "Average test loss: 0.005240940801385376\n",
      "Epoch 2/300\n",
      "Average training loss: 1.0348671538564893\n",
      "Average test loss: 0.0043878250117931105\n",
      "Epoch 3/300\n",
      "Average training loss: 0.6532742026646932\n",
      "Average test loss: 0.004255206151554982\n",
      "Epoch 4/300\n",
      "Average training loss: 0.4609734414418538\n",
      "Average test loss: 0.0038038371375037562\n",
      "Epoch 5/300\n",
      "Average training loss: 0.3478498845100403\n",
      "Average test loss: 0.0037059886120259762\n",
      "Epoch 6/300\n",
      "Average training loss: 0.27384888019826675\n",
      "Average test loss: 0.003588826305336422\n",
      "Epoch 7/300\n",
      "Average training loss: 0.22330011546611786\n",
      "Average test loss: 0.0034843360322217146\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1857358162138197\n",
      "Average test loss: 0.05239858047995302\n",
      "Epoch 9/300\n",
      "Average training loss: 0.15718564009666444\n",
      "Average test loss: 0.003960335616022348\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1342913707031144\n",
      "Average test loss: 0.003279145864562856\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11701625573635101\n",
      "Average test loss: 0.003961183841650685\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0920193168785837\n",
      "Average test loss: 0.003098105250340369\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0834879423711035\n",
      "Average test loss: 0.0030645802290075355\n",
      "Epoch 15/300\n",
      "Average training loss: 0.07661114322476917\n",
      "Average test loss: 0.0030350491768783992\n",
      "Epoch 16/300\n",
      "Average training loss: 0.07113602302471797\n",
      "Average test loss: 0.002996283506146736\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06690245967441134\n",
      "Average test loss: 0.003012893239864045\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06346640294790268\n",
      "Average test loss: 0.002939829127035207\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06069152660171191\n",
      "Average test loss: 0.0029368173359996743\n",
      "Epoch 20/300\n",
      "Average training loss: 0.058484260963069065\n",
      "Average test loss: 0.0028802322966770995\n",
      "Epoch 21/300\n",
      "Average training loss: 0.056639272534184985\n",
      "Average test loss: 0.002962551553630167\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05514605982767211\n",
      "Average test loss: 0.0028646609193334976\n",
      "Epoch 23/300\n",
      "Average training loss: 0.053840466515885456\n",
      "Average test loss: 0.002839995442993111\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05271081953909662\n",
      "Average test loss: 0.0028104767250931926\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05171934493382772\n",
      "Average test loss: 0.0028148446674976083\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05089139864842097\n",
      "Average test loss: 0.0028104867021449737\n",
      "Epoch 27/300\n",
      "Average training loss: 0.050086152046918866\n",
      "Average test loss: 0.0028051130469474526\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04939727998773257\n",
      "Average test loss: 0.0027779159725954137\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04877022514740626\n",
      "Average test loss: 0.0027668128379931055\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04813380230466525\n",
      "Average test loss: 0.0029682028846194346\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04758868204885059\n",
      "Average test loss: 0.0027452078649981156\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0471718361808194\n",
      "Average test loss: 0.0027464398081517884\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04659475083649158\n",
      "Average test loss: 0.002751064310885138\n",
      "Epoch 34/300\n",
      "Average training loss: 0.046070086065265865\n",
      "Average test loss: 0.0027621413252005974\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04568959818614854\n",
      "Average test loss: 0.0027538745382593736\n",
      "Epoch 36/300\n",
      "Average training loss: 0.045199260440137654\n",
      "Average test loss: 0.002736707399909695\n",
      "Epoch 37/300\n",
      "Average training loss: 0.044747414191563924\n",
      "Average test loss: 0.002717683391955992\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04441374099585745\n",
      "Average test loss: 0.0026975551942984262\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04404328336318334\n",
      "Average test loss: 0.0027148579054822523\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04363533515234788\n",
      "Average test loss: 0.002703164843324986\n",
      "Epoch 41/300\n",
      "Average training loss: 0.043230212370554605\n",
      "Average test loss: 0.002719232147973445\n",
      "Epoch 42/300\n",
      "Average training loss: 0.042933403548267156\n",
      "Average test loss: 0.002711144255060289\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04263133462270101\n",
      "Average test loss: 0.002740765684801671\n",
      "Epoch 44/300\n",
      "Average training loss: 0.042297142856650885\n",
      "Average test loss: 0.002732468008167214\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04194458210137155\n",
      "Average test loss: 0.0028336885531122487\n",
      "Epoch 46/300\n",
      "Average training loss: 0.04175342775384585\n",
      "Average test loss: 0.0026934487604432637\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04147761095563571\n",
      "Average test loss: 0.0026848887136826914\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04107145076990128\n",
      "Average test loss: 0.002746002749643392\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04079593500494957\n",
      "Average test loss: 0.002716011581114597\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04061847715245353\n",
      "Average test loss: 0.0027173167614059314\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04034081341822942\n",
      "Average test loss: 0.002733027004947265\n",
      "Epoch 52/300\n",
      "Average training loss: 0.040055950798922116\n",
      "Average test loss: 0.0027756936663968696\n",
      "Epoch 53/300\n",
      "Average training loss: 0.039678108884228604\n",
      "Average test loss: 0.0027357509715689554\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03941696964038743\n",
      "Average test loss: 0.0027044307496398687\n",
      "Epoch 55/300\n",
      "Average training loss: 0.039219549225436315\n",
      "Average test loss: 0.002720593475219276\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03895182635055648\n",
      "Average test loss: 0.002702051768493321\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03869930347469118\n",
      "Average test loss: 0.008014952876915534\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03848467681308587\n",
      "Average test loss: 0.0027056121720621983\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03824224958651596\n",
      "Average test loss: 0.0027496984294719164\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03795970449844996\n",
      "Average test loss: 0.0027249983097943995\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03775992868675126\n",
      "Average test loss: 0.002719793143992623\n",
      "Epoch 62/300\n",
      "Average training loss: 0.037581530638866954\n",
      "Average test loss: 0.0027974331630393865\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03734537124633789\n",
      "Average test loss: 0.002715270580516921\n",
      "Epoch 64/300\n",
      "Average training loss: 0.037146647353967034\n",
      "Average test loss: 0.002862192390486598\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03697890404860179\n",
      "Average test loss: 0.00296964593583511\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03675212337573369\n",
      "Average test loss: 0.002971283807936642\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036544225520557826\n",
      "Average test loss: 0.0029949712138622997\n",
      "Epoch 68/300\n",
      "Average training loss: 0.036328473827905126\n",
      "Average test loss: 0.002826331825306018\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03622047363718351\n",
      "Average test loss: 0.010513289891390337\n",
      "Epoch 70/300\n",
      "Average training loss: 0.036032583160532844\n",
      "Average test loss: 0.0027327213496383695\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03580585303074784\n",
      "Average test loss: 0.00284066310711205\n",
      "Epoch 72/300\n",
      "Average training loss: 0.035781859152846866\n",
      "Average test loss: 0.0028157246419125135\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03558076103197204\n",
      "Average test loss: 0.002868849511568745\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03545541564292378\n",
      "Average test loss: 0.0027518797100832067\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03516202801465988\n",
      "Average test loss: 0.0027633453965600994\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0351018695169025\n",
      "Average test loss: 0.0027574940574251945\n",
      "Epoch 77/300\n",
      "Average training loss: 0.034956853715909855\n",
      "Average test loss: 0.0028061160484535827\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03484505866302384\n",
      "Average test loss: 0.0028813288456035987\n",
      "Epoch 79/300\n",
      "Average training loss: 0.034697416537337836\n",
      "Average test loss: 0.0027725348317374785\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0344956650171015\n",
      "Average test loss: 0.0028480940837826993\n",
      "Epoch 81/300\n",
      "Average training loss: 0.034391822506984075\n",
      "Average test loss: 0.0027964093213280043\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03429392411973741\n",
      "Average test loss: 0.002778259223534001\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03419569327765041\n",
      "Average test loss: 0.0029114283946239288\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03407928169767062\n",
      "Average test loss: 0.0028877945608562893\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03394088453385565\n",
      "Average test loss: 0.003052234476432204\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03387155214945475\n",
      "Average test loss: 0.0027872076854109766\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03373726473417547\n",
      "Average test loss: 0.0029098731542213097\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03363431850075722\n",
      "Average test loss: 0.0027557993723700443\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03355308813850085\n",
      "Average test loss: 0.0027721590052048367\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03346683298216926\n",
      "Average test loss: 0.002861518901669317\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03337489534417788\n",
      "Average test loss: 0.003006995978247788\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03323598866164684\n",
      "Average test loss: 0.0029059257799138624\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03315964260035091\n",
      "Average test loss: 0.0029449462017251387\n",
      "Epoch 94/300\n",
      "Average training loss: 0.033076442271471024\n",
      "Average test loss: 0.0033629735925545293\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03293358578284582\n",
      "Average test loss: 0.00284902099147439\n",
      "Epoch 96/300\n",
      "Average training loss: 0.032909189548757344\n",
      "Average test loss: 0.0028167070208324325\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03283080674045616\n",
      "Average test loss: 0.003079705263177554\n",
      "Epoch 98/300\n",
      "Average training loss: 0.032758497428562905\n",
      "Average test loss: 0.003033680039147536\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03263452030221621\n",
      "Average test loss: 0.0028652418220622674\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03261544427937931\n",
      "Average test loss: 0.0029178846074889102\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03245161964827114\n",
      "Average test loss: 0.0029423848104973634\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03243401577075322\n",
      "Average test loss: 0.002901625563287073\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03239181558622254\n",
      "Average test loss: 0.0028707091622054578\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03230392439828979\n",
      "Average test loss: 0.002940195847716596\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03220991426209609\n",
      "Average test loss: 0.002952438527925147\n",
      "Epoch 106/300\n",
      "Average training loss: 0.032097681855161986\n",
      "Average test loss: 0.0028990665566590097\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03211252028743426\n",
      "Average test loss: 0.00298204162158072\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03204026521907912\n",
      "Average test loss: 0.0028878134294516512\n",
      "Epoch 109/300\n",
      "Average training loss: 0.032013123237424425\n",
      "Average test loss: 0.002966949404320783\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03182333822217252\n",
      "Average test loss: 0.0029611479931821428\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03177722438838747\n",
      "Average test loss: 0.0028380787167698145\n",
      "Epoch 112/300\n",
      "Average training loss: 0.031728610965940686\n",
      "Average test loss: 0.0030531645913918814\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03168494432171186\n",
      "Average test loss: 0.0029458230179217125\n",
      "Epoch 114/300\n",
      "Average training loss: 0.031624825179576876\n",
      "Average test loss: 0.0029411981637693114\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03157296267151832\n",
      "Average test loss: 0.0028090741330136855\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03156587763958507\n",
      "Average test loss: 0.002932908032503393\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03153762706451946\n",
      "Average test loss: 0.003085110915928251\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03138665992187129\n",
      "Average test loss: 0.00308601400649382\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03130642964442571\n",
      "Average test loss: 0.0031497283490995566\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03127150173319711\n",
      "Average test loss: 0.0029148532862050664\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03127333649330669\n",
      "Average test loss: 0.0029315586160454485\n",
      "Epoch 122/300\n",
      "Average training loss: 0.031234269873963463\n",
      "Average test loss: 0.002872883981714646\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03110004323720932\n",
      "Average test loss: 0.002902231339779165\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03113995697432094\n",
      "Average test loss: 0.0029582687185870276\n",
      "Epoch 125/300\n",
      "Average training loss: 0.030976293325424193\n",
      "Average test loss: 0.002959518226277497\n",
      "Epoch 126/300\n",
      "Average training loss: 0.030996589021550284\n",
      "Average test loss: 0.002941356539105376\n",
      "Epoch 127/300\n",
      "Average training loss: 0.030914434934655826\n",
      "Average test loss: 0.0029369202417631945\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03087883824441168\n",
      "Average test loss: 0.0029424411377145182\n",
      "Epoch 129/300\n",
      "Average training loss: 0.030889481835895113\n",
      "Average test loss: 0.0028841487297581302\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03077621679008007\n",
      "Average test loss: 0.0029334073012901676\n",
      "Epoch 131/300\n",
      "Average training loss: 0.030747365893589125\n",
      "Average test loss: 0.002985923427053624\n",
      "Epoch 132/300\n",
      "Average training loss: 0.030687007347742715\n",
      "Average test loss: 0.003267232629160086\n",
      "Epoch 133/300\n",
      "Average training loss: 0.030704200625419616\n",
      "Average test loss: 0.002884304743881027\n",
      "Epoch 134/300\n",
      "Average training loss: 0.030607645705342294\n",
      "Average test loss: 0.0029471525905860794\n",
      "Epoch 135/300\n",
      "Average training loss: 0.030590274885296823\n",
      "Average test loss: 0.0029352678873886665\n",
      "Epoch 136/300\n",
      "Average training loss: 0.030518747076392173\n",
      "Average test loss: 0.00301444333150155\n",
      "Epoch 137/300\n",
      "Average training loss: 0.030511586917771234\n",
      "Average test loss: 0.002871787930528323\n",
      "Epoch 138/300\n",
      "Average training loss: 0.030500946634345584\n",
      "Average test loss: 0.002872968857280082\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03044930999808841\n",
      "Average test loss: 0.0029499977812584904\n",
      "Epoch 140/300\n",
      "Average training loss: 0.030346303431524172\n",
      "Average test loss: 0.002995736574754119\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03037059759100278\n",
      "Average test loss: 0.002904690563471781\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03025802610649003\n",
      "Average test loss: 0.0029511408425039716\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030240906147493255\n",
      "Average test loss: 0.0035015837624669077\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030220632230242093\n",
      "Average test loss: 0.0029387699808511468\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030213245484564038\n",
      "Average test loss: 0.0029363824151870275\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030104321382111974\n",
      "Average test loss: 0.0032791118625965384\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030125950270228914\n",
      "Average test loss: 0.0029826423244343862\n",
      "Epoch 148/300\n",
      "Average training loss: 0.030057024758723048\n",
      "Average test loss: 0.0029027422784517207\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03003653052283658\n",
      "Average test loss: 0.0029414371972282726\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03004057094289197\n",
      "Average test loss: 0.002908838136328591\n",
      "Epoch 151/300\n",
      "Average training loss: 0.029927642479538916\n",
      "Average test loss: 0.003064792277291417\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02993879900044865\n",
      "Average test loss: 0.0029669172796938153\n",
      "Epoch 153/300\n",
      "Average training loss: 0.029882723790076043\n",
      "Average test loss: 0.0030923740660978687\n",
      "Epoch 154/300\n",
      "Average training loss: 0.029865910046630435\n",
      "Average test loss: 0.0030595141388475894\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02987015240887801\n",
      "Average test loss: 0.0029973419811576605\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0297585149274932\n",
      "Average test loss: 0.002940620752879315\n",
      "Epoch 157/300\n",
      "Average training loss: 0.029761140545209248\n",
      "Average test loss: 0.003314183611629738\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0297197790576352\n",
      "Average test loss: 0.0030471029271268183\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02970216340157721\n",
      "Average test loss: 0.0028985230369079442\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02966203981637955\n",
      "Average test loss: 0.002981239795891775\n",
      "Epoch 161/300\n",
      "Average training loss: 0.029609867087668842\n",
      "Average test loss: 0.0029951111090679963\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029677911973661847\n",
      "Average test loss: 0.0029479571810613076\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029552925624781186\n",
      "Average test loss: 0.002910148632609182\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029554760557081963\n",
      "Average test loss: 0.0030444787459241018\n",
      "Epoch 165/300\n",
      "Average training loss: 0.029520035281777383\n",
      "Average test loss: 0.002967450990031163\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029463168255156942\n",
      "Average test loss: 0.0030558542195293636\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02935807418823242\n",
      "Average test loss: 0.0030146269881063037\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029392985592285793\n",
      "Average test loss: 0.0029878305754520826\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02941604796714253\n",
      "Average test loss: 0.002970693413582113\n",
      "Epoch 170/300\n",
      "Average training loss: 0.029382805675268173\n",
      "Average test loss: 0.003010160649402274\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02936118244131406\n",
      "Average test loss: 0.0029775206396977106\n",
      "Epoch 172/300\n",
      "Average training loss: 0.029290727901789878\n",
      "Average test loss: 0.0029515115124069983\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029327689362896814\n",
      "Average test loss: 0.0030215938486572768\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029189758459726968\n",
      "Average test loss: 0.002942466734184159\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02926666182445155\n",
      "Average test loss: 0.002977770492641462\n",
      "Epoch 176/300\n",
      "Average training loss: 0.029188037715024418\n",
      "Average test loss: 0.0030417185075994994\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029162564019362132\n",
      "Average test loss: 0.0029793909872985547\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02913425961467955\n",
      "Average test loss: 0.003139088115758366\n",
      "Epoch 179/300\n",
      "Average training loss: 0.029162726879119874\n",
      "Average test loss: 0.002957361650135782\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02910958241754108\n",
      "Average test loss: 0.003102372251657976\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02904715279241403\n",
      "Average test loss: 0.0030146499569010405\n",
      "Epoch 182/300\n",
      "Average training loss: 0.029038627674182256\n",
      "Average test loss: 0.003034498179745343\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02901740124821663\n",
      "Average test loss: 0.0030229426353342004\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0290160437491205\n",
      "Average test loss: 0.002956368120594157\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02897105236020353\n",
      "Average test loss: 0.002964848777072297\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028948394437630972\n",
      "Average test loss: 0.002944270262701644\n",
      "Epoch 187/300\n",
      "Average training loss: 0.028919225878185698\n",
      "Average test loss: 0.0029610247113224532\n",
      "Epoch 188/300\n",
      "Average training loss: 0.028894918022884263\n",
      "Average test loss: 0.0030045731320149367\n",
      "Epoch 189/300\n",
      "Average training loss: 0.028852314210600322\n",
      "Average test loss: 0.0029442302263859247\n",
      "Epoch 190/300\n",
      "Average training loss: 0.028848070349958208\n",
      "Average test loss: 0.0030547449470808108\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02878357952667607\n",
      "Average test loss: 0.003000135933359464\n",
      "Epoch 192/300\n",
      "Average training loss: 0.028872244290179677\n",
      "Average test loss: 0.003038421483710408\n",
      "Epoch 193/300\n",
      "Average training loss: 0.028813627786106532\n",
      "Average test loss: 0.003027565214369032\n",
      "Epoch 194/300\n",
      "Average training loss: 0.028768408864736555\n",
      "Average test loss: 0.0030395621228963138\n",
      "Epoch 195/300\n",
      "Average training loss: 0.028729311651653714\n",
      "Average test loss: 0.0029507447599122923\n",
      "Epoch 196/300\n",
      "Average training loss: 0.028672605423463716\n",
      "Average test loss: 0.003004360157168574\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028685215337408913\n",
      "Average test loss: 0.003079857515378131\n",
      "Epoch 198/300\n",
      "Average training loss: 0.028741494446992873\n",
      "Average test loss: 0.0030773763166119655\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02861585944890976\n",
      "Average test loss: 0.0029365912301258907\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02861710059311655\n",
      "Average test loss: 0.0030130166452791954\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02858773119416502\n",
      "Average test loss: 0.0032457487455879648\n",
      "Epoch 202/300\n",
      "Average training loss: 0.028587053592006365\n",
      "Average test loss: 0.0030085783975405825\n",
      "Epoch 203/300\n",
      "Average training loss: 0.028573863436778386\n",
      "Average test loss: 0.0031404727613553404\n",
      "Epoch 204/300\n",
      "Average training loss: 0.028584163745244345\n",
      "Average test loss: 0.003021276380452845\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02850131448192729\n",
      "Average test loss: 0.0030142061037735806\n",
      "Epoch 206/300\n",
      "Average training loss: 0.028516860927144685\n",
      "Average test loss: 0.0033587531379113593\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028482939756578868\n",
      "Average test loss: 0.0030540098185754486\n",
      "Epoch 208/300\n",
      "Average training loss: 0.028427707576089437\n",
      "Average test loss: 0.003013954195918308\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02845129312409295\n",
      "Average test loss: 0.0030305739539778897\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02843653784857856\n",
      "Average test loss: 0.002989691703890761\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02849694870909055\n",
      "Average test loss: 0.0031139645568198627\n",
      "Epoch 212/300\n",
      "Average training loss: 0.028391166157192654\n",
      "Average test loss: 0.0031041806607196728\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02837221967842844\n",
      "Average test loss: 0.0030563439246681003\n",
      "Epoch 214/300\n",
      "Average training loss: 0.028303442397051386\n",
      "Average test loss: 0.0030002189324133924\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02829413672950533\n",
      "Average test loss: 0.003105699721102913\n",
      "Epoch 216/300\n",
      "Average training loss: 0.028255951788690354\n",
      "Average test loss: 0.0030296716438606383\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028263688511318632\n",
      "Average test loss: 0.0030321459066536692\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02827893630332417\n",
      "Average test loss: 0.00452588949414591\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028242949227492015\n",
      "Average test loss: 0.0031031962728334797\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028202522862288686\n",
      "Average test loss: 0.0031043522974683177\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028212396767404346\n",
      "Average test loss: 0.0030248602418642906\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02820924910902977\n",
      "Average test loss: 0.0030869203781088194\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02827694174316194\n",
      "Average test loss: 0.003131349155886306\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028145733755495814\n",
      "Average test loss: 0.0030734930243343115\n",
      "Epoch 225/300\n",
      "Average training loss: 0.028113937257064712\n",
      "Average test loss: 0.0030081771968139544\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028082720551225872\n",
      "Average test loss: 0.0030001944059299098\n",
      "Epoch 227/300\n",
      "Average training loss: 0.028138236416710746\n",
      "Average test loss: 0.0030820994186732506\n",
      "Epoch 228/300\n",
      "Average training loss: 0.028106598744789758\n",
      "Average test loss: 0.0030021300009555286\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028050379453433884\n",
      "Average test loss: 0.0030805364102125168\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02807288639081849\n",
      "Average test loss: 0.0031394350156188013\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02802995545665423\n",
      "Average test loss: 0.0030864806262155373\n",
      "Epoch 232/300\n",
      "Average training loss: 0.028009744657410515\n",
      "Average test loss: 0.003015248616743419\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02799971285627948\n",
      "Average test loss: 0.0030892664500408703\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02800968769358264\n",
      "Average test loss: 0.0030412118832270304\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02798699659605821\n",
      "Average test loss: 0.0033955612294375897\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02791381113562319\n",
      "Average test loss: 0.004336158296300305\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02793358540534973\n",
      "Average test loss: 0.003041324980970886\n",
      "Epoch 238/300\n",
      "Average training loss: 0.027942443443669213\n",
      "Average test loss: 0.0031557327144675785\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02791444255742762\n",
      "Average test loss: 0.0029353455741786293\n",
      "Epoch 240/300\n",
      "Average training loss: 0.027853236221604877\n",
      "Average test loss: 0.002983241227351957\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02786720840467347\n",
      "Average test loss: 0.0030344519468231334\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02787437505192227\n",
      "Average test loss: 0.003031188059391247\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02780348387360573\n",
      "Average test loss: 0.006603445959794852\n",
      "Epoch 244/300\n",
      "Average training loss: 0.027799164147840606\n",
      "Average test loss: 0.003104773380069269\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02779787660141786\n",
      "Average test loss: 0.0030112157542672422\n",
      "Epoch 246/300\n",
      "Average training loss: 0.027768986814551883\n",
      "Average test loss: 0.0031021697767492796\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02776681094368299\n",
      "Average test loss: 0.0030815897434949874\n",
      "Epoch 248/300\n",
      "Average training loss: 0.027754397503203816\n",
      "Average test loss: 0.004848584520940979\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0277412091659175\n",
      "Average test loss: 0.003006785068454014\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02770797601673338\n",
      "Average test loss: 0.003028823712219795\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027757601171731948\n",
      "Average test loss: 0.0030603129580203028\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02765536570880148\n",
      "Average test loss: 0.003036334164440632\n",
      "Epoch 253/300\n",
      "Average training loss: 0.027669815389646423\n",
      "Average test loss: 0.0031062146961275076\n",
      "Epoch 254/300\n",
      "Average training loss: 0.027648440445462864\n",
      "Average test loss: 0.0030353219074507553\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027631244886252616\n",
      "Average test loss: 0.0031375054866075516\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027614634768830405\n",
      "Average test loss: 0.0042818712037470605\n",
      "Epoch 257/300\n",
      "Average training loss: 0.027617364707920287\n",
      "Average test loss: 0.0029826991469081906\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02762687406771713\n",
      "Average test loss: 0.003088820276161035\n",
      "Epoch 259/300\n",
      "Average training loss: 0.027544466621345944\n",
      "Average test loss: 0.0030873385493954022\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02761263252629174\n",
      "Average test loss: 0.003010855575816499\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02758466838962502\n",
      "Average test loss: 0.003003956503752205\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027584464626179802\n",
      "Average test loss: 0.0031132717517515025\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027542921864324145\n",
      "Average test loss: 0.0030203570356178615\n",
      "Epoch 264/300\n",
      "Average training loss: 0.027516205057501793\n",
      "Average test loss: 0.003561026502400637\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027509878956609304\n",
      "Average test loss: 0.003077941062135829\n",
      "Epoch 266/300\n",
      "Average training loss: 0.027505232067571744\n",
      "Average test loss: 0.0030872608391154144\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02748252714342541\n",
      "Average test loss: 0.003035604395179285\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02746975177857611\n",
      "Average test loss: 0.0030811669741653736\n",
      "Epoch 269/300\n",
      "Average training loss: 0.027432166013452743\n",
      "Average test loss: 0.005641271985653374\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027462658064232932\n",
      "Average test loss: 0.0031759188175201417\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027435394427842565\n",
      "Average test loss: 0.003116050034347508\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027424136017759643\n",
      "Average test loss: 0.0029893672907104095\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02740996805826823\n",
      "Average test loss: 0.003492711199240552\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0273619584656424\n",
      "Average test loss: 0.003068761540783776\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027399503850274617\n",
      "Average test loss: 0.0030416058094965086\n",
      "Epoch 276/300\n",
      "Average training loss: 0.027336472890443272\n",
      "Average test loss: 0.0030687268245965242\n",
      "Epoch 277/300\n",
      "Average training loss: 0.027369308567709392\n",
      "Average test loss: 0.0029935210889412297\n",
      "Epoch 278/300\n",
      "Average training loss: 0.027337806678480573\n",
      "Average test loss: 0.0033696423222621284\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027374516430828305\n",
      "Average test loss: 0.003201515596359968\n",
      "Epoch 280/300\n",
      "Average training loss: 0.027297017337547407\n",
      "Average test loss: 0.0030206790852049985\n",
      "Epoch 281/300\n",
      "Average training loss: 0.02730951078898377\n",
      "Average test loss: 0.003665402637587653\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027261080435580676\n",
      "Average test loss: 0.0030191022875822255\n",
      "Epoch 283/300\n",
      "Average training loss: 0.027258438047435548\n",
      "Average test loss: 0.0037823646788795787\n",
      "Epoch 284/300\n",
      "Average training loss: 0.027301182821393013\n",
      "Average test loss: 0.0047515089120715856\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027269292996989356\n",
      "Average test loss: 0.0031110892595930233\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02720610512461927\n",
      "Average test loss: 0.0030541542917490007\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02723573226398892\n",
      "Average test loss: 0.003054391903595792\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02720992953909768\n",
      "Average test loss: 0.003006494846712384\n",
      "Epoch 289/300\n",
      "Average training loss: 0.027205743996633423\n",
      "Average test loss: 0.0030921716172662046\n",
      "Epoch 290/300\n",
      "Average training loss: 0.027174346276455454\n",
      "Average test loss: 0.003027715449945794\n",
      "Epoch 291/300\n",
      "Average training loss: 0.027152941063046457\n",
      "Average test loss: 0.0030480057487471237\n",
      "Epoch 292/300\n",
      "Average training loss: 0.027188530581692856\n",
      "Average test loss: 0.0033613587994542385\n",
      "Epoch 293/300\n",
      "Average training loss: 0.027170589801337984\n",
      "Average test loss: 0.003116122731938958\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02712168967889415\n",
      "Average test loss: 0.0035865190873543423\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02713218865957525\n",
      "Average test loss: 0.0030678659638182985\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02708557633890046\n",
      "Average test loss: 0.0031585299397508303\n",
      "Epoch 297/300\n",
      "Average training loss: 0.027099457149704298\n",
      "Average test loss: 0.0030727962491412957\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027081638135843807\n",
      "Average test loss: 0.0031119541523771154\n",
      "Epoch 299/300\n",
      "Average training loss: 0.027068553952707186\n",
      "Average test loss: 0.00306721479859617\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02707225261297491\n",
      "Average test loss: 0.003215562757725517\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 3.2790587775972155\n",
      "Average test loss: 0.004929594343735112\n",
      "Epoch 2/300\n",
      "Average training loss: 1.3112215371661715\n",
      "Average test loss: 0.003992123399343755\n",
      "Epoch 3/300\n",
      "Average training loss: 0.8919059660699632\n",
      "Average test loss: 0.003795379681719674\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6864674515724182\n",
      "Average test loss: 0.00336159982242518\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5575280372831557\n",
      "Average test loss: 0.0032396455829342206\n",
      "Epoch 6/300\n",
      "Average training loss: 0.46376619805230035\n",
      "Average test loss: 0.0032528781081653303\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3897604975435469\n",
      "Average test loss: 0.003066786146619254\n",
      "Epoch 8/300\n",
      "Average training loss: 0.33066890456941395\n",
      "Average test loss: 0.0029027902498427365\n",
      "Epoch 9/300\n",
      "Average training loss: 0.28355730435583326\n",
      "Average test loss: 0.002804055074436797\n",
      "Epoch 10/300\n",
      "Average training loss: 0.24335988948080275\n",
      "Average test loss: 0.0030695050896869767\n",
      "Epoch 11/300\n",
      "Average training loss: 0.21144306823942396\n",
      "Average test loss: 0.002713625742226011\n",
      "Epoch 12/300\n",
      "Average training loss: 0.18317083883285523\n",
      "Average test loss: 0.0027925154966198735\n",
      "Epoch 13/300\n",
      "Average training loss: 0.15879538521501754\n",
      "Average test loss: 0.0026059466182357734\n",
      "Epoch 14/300\n",
      "Average training loss: 0.13710217494434782\n",
      "Average test loss: 0.0025380617518805794\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1186735853685273\n",
      "Average test loss: 0.002480826293428739\n",
      "Epoch 16/300\n",
      "Average training loss: 0.103593184683058\n",
      "Average test loss: 0.0026063007230146063\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09104819571309619\n",
      "Average test loss: 0.002414584608334634\n",
      "Epoch 18/300\n",
      "Average training loss: 0.08098307715521919\n",
      "Average test loss: 0.002395755980163813\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07295611698097652\n",
      "Average test loss: 0.0023772340272035864\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06660563229852252\n",
      "Average test loss: 0.0023370063559462625\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06154528177115652\n",
      "Average test loss: 0.0023557840629170337\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0574355474114418\n",
      "Average test loss: 0.0023287874282234243\n",
      "Epoch 23/300\n",
      "Average training loss: 0.054128293335437776\n",
      "Average test loss: 0.0022998279090970753\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05150172048807144\n",
      "Average test loss: 0.0022862481102347374\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04938862858547105\n",
      "Average test loss: 0.0022918880521837206\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04767469975683424\n",
      "Average test loss: 0.002257597404635615\n",
      "Epoch 27/300\n",
      "Average training loss: 0.046154427409172055\n",
      "Average test loss: 0.0022195828205181494\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04493263159195582\n",
      "Average test loss: 0.002218049390448464\n",
      "Epoch 29/300\n",
      "Average training loss: 0.043875141617324614\n",
      "Average test loss: 0.0022105841061307325\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04288658680849605\n",
      "Average test loss: 0.0022046745408119428\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04205492026276059\n",
      "Average test loss: 0.00226363070981784\n",
      "Epoch 32/300\n",
      "Average training loss: 0.041365247256226007\n",
      "Average test loss: 0.0022400642338519295\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04075663234790166\n",
      "Average test loss: 0.0022088895100686287\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04012301837404569\n",
      "Average test loss: 0.002185903081877364\n",
      "Epoch 35/300\n",
      "Average training loss: 0.039558098619182906\n",
      "Average test loss: 0.002258017521765497\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03903892982006073\n",
      "Average test loss: 0.0021540965012585124\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03857767914070023\n",
      "Average test loss: 0.002166068583312962\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0380407106479009\n",
      "Average test loss: 0.0021532419909619624\n",
      "Epoch 39/300\n",
      "Average training loss: 0.037573991825183235\n",
      "Average test loss: 0.0021472702819026177\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03707418883674674\n",
      "Average test loss: 0.0022450968595221638\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03672061538365152\n",
      "Average test loss: 0.002131235380243096\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03625801809628804\n",
      "Average test loss: 0.00214227632050299\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03581208988693026\n",
      "Average test loss: 0.00214940705936816\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03549208142360052\n",
      "Average test loss: 0.0021707340364033978\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0351024093810055\n",
      "Average test loss: 0.002222276658233669\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03474897786974907\n",
      "Average test loss: 0.0021909088769720662\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03441129851837953\n",
      "Average test loss: 0.0022364753764122724\n",
      "Epoch 48/300\n",
      "Average training loss: 0.034146750960085126\n",
      "Average test loss: 0.0021307538936121595\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03381850888331731\n",
      "Average test loss: 0.002194077393867903\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03306309921211666\n",
      "Average test loss: 0.002159373394937979\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03276109772258335\n",
      "Average test loss: 0.002120569907542732\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03254244624574979\n",
      "Average test loss: 0.0021381617273307507\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03227111288574007\n",
      "Average test loss: 0.0021494036683191854\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03205489436288675\n",
      "Average test loss: 0.002146506911350621\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03177727961540222\n",
      "Average test loss: 0.0021441477071493862\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03158966874910726\n",
      "Average test loss: 0.0021752858835582933\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03140788315733274\n",
      "Average test loss: 0.002169810943615933\n",
      "Epoch 60/300\n",
      "Average training loss: 0.031177640275822746\n",
      "Average test loss: 0.0021928791031241416\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030980358107222453\n",
      "Average test loss: 0.0021598369822733933\n",
      "Epoch 62/300\n",
      "Average training loss: 0.030767223104834557\n",
      "Average test loss: 0.0022299123930020464\n",
      "Epoch 63/300\n",
      "Average training loss: 0.030597224306729103\n",
      "Average test loss: 0.0021732078733750515\n",
      "Epoch 64/300\n",
      "Average training loss: 0.030291897396246593\n",
      "Average test loss: 0.002340652787644002\n",
      "Epoch 66/300\n",
      "Average training loss: 0.030111547920438977\n",
      "Average test loss: 0.0022928258383439646\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029948544578419792\n",
      "Average test loss: 0.002198081235918734\n",
      "Epoch 68/300\n",
      "Average training loss: 0.029741708132955763\n",
      "Average test loss: 0.0021521018942197165\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02952087355984582\n",
      "Average test loss: 0.002164648548182514\n",
      "Epoch 70/300\n",
      "Average training loss: 0.029492021838823955\n",
      "Average test loss: 0.002714868156446351\n",
      "Epoch 71/300\n",
      "Average training loss: 0.029229270723130966\n",
      "Average test loss: 0.002320692365161247\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02907077687150902\n",
      "Average test loss: 0.0021848235343479446\n",
      "Epoch 73/300\n",
      "Average training loss: 0.028953469157218935\n",
      "Average test loss: 0.0022153657445063194\n",
      "Epoch 74/300\n",
      "Average training loss: 0.028804807782173158\n",
      "Average test loss: 0.0021951790981822542\n",
      "Epoch 75/300\n",
      "Average training loss: 0.028726833146479395\n",
      "Average test loss: 0.002197794001756443\n",
      "Epoch 76/300\n",
      "Average training loss: 0.028477305899063745\n",
      "Average test loss: 0.0021938908543023797\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02826323778099484\n",
      "Average test loss: 0.002447656747367647\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02818375315103266\n",
      "Average test loss: 0.002204764352697465\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028010030845801035\n",
      "Average test loss: 0.0021611333045487603\n",
      "Epoch 81/300\n",
      "Average training loss: 0.027928185989459356\n",
      "Average test loss: 0.002403542004732622\n",
      "Epoch 82/300\n",
      "Average training loss: 0.027898989690674676\n",
      "Average test loss: 0.0021800532020214533\n",
      "Epoch 83/300\n",
      "Average training loss: 0.027735107585787774\n",
      "Average test loss: 0.0022456321753561497\n",
      "Epoch 84/300\n",
      "Average training loss: 0.027692063074972895\n",
      "Average test loss: 0.002213595397563444\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02753607243961758\n",
      "Average test loss: 0.002187959168727199\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02737531296081013\n",
      "Average test loss: 0.00226870622454832\n",
      "Epoch 88/300\n",
      "Average training loss: 0.027416068777441977\n",
      "Average test loss: 0.002479949347881807\n",
      "Epoch 89/300\n",
      "Average training loss: 0.027207656224568683\n",
      "Average test loss: 0.002210367617300815\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02708098239203294\n",
      "Average test loss: 0.002303233185989989\n",
      "Epoch 91/300\n",
      "Average training loss: 0.026994172727068267\n",
      "Average test loss: 0.0022707589318354923\n",
      "Epoch 92/300\n",
      "Average training loss: 0.026894592728879715\n",
      "Average test loss: 0.0022604671791195867\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02681990853945414\n",
      "Average test loss: 0.002317835746953885\n",
      "Epoch 94/300\n",
      "Average training loss: 0.026716403153207568\n",
      "Average test loss: 0.0022482811119407414\n",
      "Epoch 95/300\n",
      "Average training loss: 0.026670363091760213\n",
      "Average test loss: 0.002276545888640814\n",
      "Epoch 96/300\n",
      "Average training loss: 0.026648066244191592\n",
      "Average test loss: 0.002318764896442493\n",
      "Epoch 97/300\n",
      "Average training loss: 0.026503020976980526\n",
      "Average test loss: 0.0022480765831553273\n",
      "Epoch 98/300\n",
      "Average training loss: 0.026410937635435\n",
      "Average test loss: 0.0022743957322090866\n",
      "Epoch 99/300\n",
      "Average training loss: 0.026388539406988355\n",
      "Average test loss: 0.002278437371055285\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02620666863769293\n",
      "Average test loss: 0.0023256260817870497\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026124112567967837\n",
      "Average test loss: 0.00230463820302652\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026051490914490487\n",
      "Average test loss: 0.0022785166634453667\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02601524724562963\n",
      "Average test loss: 0.0022858209429929654\n",
      "Epoch 106/300\n",
      "Average training loss: 0.025944735770424207\n",
      "Average test loss: 0.002348548197497924\n",
      "Epoch 107/300\n",
      "Average training loss: 0.025877940005726286\n",
      "Average test loss: 0.0023162935179554755\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02583342750536071\n",
      "Average test loss: 0.0032116968811800083\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02576918951670329\n",
      "Average test loss: 0.0023148302856004902\n",
      "Epoch 110/300\n",
      "Average training loss: 0.025762371642722024\n",
      "Average test loss: 0.002278676197657155\n",
      "Epoch 111/300\n",
      "Average training loss: 0.025663485984007518\n",
      "Average test loss: 0.0023103389872445\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02559971211353938\n",
      "Average test loss: 0.0022751040309667587\n",
      "Epoch 113/300\n",
      "Average training loss: 0.025527164260546367\n",
      "Average test loss: 0.0023950874362554816\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02548270188768705\n",
      "Average test loss: 0.0022961228932771418\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02543314865893788\n",
      "Average test loss: 0.002311696341054307\n",
      "Epoch 117/300\n",
      "Average training loss: 0.025354601613349386\n",
      "Average test loss: 0.0026244938439793056\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02535189082721869\n",
      "Average test loss: 0.0022665512383812005\n",
      "Epoch 119/300\n",
      "Average training loss: 0.025324259287781186\n",
      "Average test loss: 0.0023240672792825432\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0252184296713935\n",
      "Average test loss: 0.0023188359319335885\n",
      "Epoch 121/300\n",
      "Average training loss: 0.025213569328188898\n",
      "Average test loss: 0.0024845178412894408\n",
      "Epoch 122/300\n",
      "Average training loss: 0.025139365901549658\n",
      "Average test loss: 0.0022898780786328845\n",
      "Epoch 123/300\n",
      "Average training loss: 0.025129418593313958\n",
      "Average test loss: 0.0023296542821658983\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02505750103791555\n",
      "Average test loss: 0.002331191533141666\n",
      "Epoch 125/300\n",
      "Average training loss: 0.025053581050700612\n",
      "Average test loss: 0.0024528311762130922\n",
      "Epoch 126/300\n",
      "Average training loss: 0.024985440312160385\n",
      "Average test loss: 0.00229306210598184\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024892380884952017\n",
      "Average test loss: 0.0023624500700583063\n",
      "Epoch 129/300\n",
      "Average training loss: 0.024871430022848976\n",
      "Average test loss: 0.0024362174111107987\n",
      "Epoch 130/300\n",
      "Average training loss: 0.024843397044473223\n",
      "Average test loss: 0.0023350844526042543\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024804464366700914\n",
      "Average test loss: 0.0023725914899259807\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024757542699575426\n",
      "Average test loss: 0.002320721057140165\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02475186200108793\n",
      "Average test loss: 0.002428143701205651\n",
      "Epoch 134/300\n",
      "Average training loss: 0.024673422780301837\n",
      "Average test loss: 0.0023420082651492624\n",
      "Epoch 135/300\n",
      "Average training loss: 0.024639963957998487\n",
      "Average test loss: 0.0023125421870499848\n",
      "Epoch 136/300\n",
      "Average training loss: 0.024610424924227926\n",
      "Average test loss: 0.0023632161626592277\n",
      "Epoch 137/300\n",
      "Average training loss: 0.024570766612887384\n",
      "Average test loss: 0.002320282502927714\n",
      "Epoch 138/300\n",
      "Average training loss: 0.024556919967134792\n",
      "Average test loss: 0.0023050974354975754\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024497139586342705\n",
      "Average test loss: 0.002345909789308078\n",
      "Epoch 141/300\n",
      "Average training loss: 0.024450293176703983\n",
      "Average test loss: 0.002363946598540578\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024464866052071252\n",
      "Average test loss: 0.002368699721681575\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02438844172656536\n",
      "Average test loss: 0.0028271389197972088\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02442282384302881\n",
      "Average test loss: 0.0023386419552067915\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0243060641321871\n",
      "Average test loss: 0.007328540475418171\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02429960941274961\n",
      "Average test loss: 0.0023180948509317306\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024281251663963\n",
      "Average test loss: 0.0024509020506714783\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024253595487938987\n",
      "Average test loss: 0.002351940565639072\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02426406154367659\n",
      "Average test loss: 0.002353475415872203\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024173001656929653\n",
      "Average test loss: 0.0023630119694603813\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024208583957619136\n",
      "Average test loss: 0.0024749360800617273\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024149144139554764\n",
      "Average test loss: 0.002533607022008962\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02411494878762298\n",
      "Average test loss: 0.002352971261781123\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02404340327117178\n",
      "Average test loss: 0.003949089966921343\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024044656349553002\n",
      "Average test loss: 0.002628288180463844\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02404293341851897\n",
      "Average test loss: 0.002365077416929934\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024020630912648305\n",
      "Average test loss: 0.002354782763040728\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02399219029313988\n",
      "Average test loss: 0.00236245294370585\n",
      "Epoch 159/300\n",
      "Average training loss: 0.023960238710045816\n",
      "Average test loss: 0.0023664013375010754\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02391105128493574\n",
      "Average test loss: 0.0023886761201752558\n",
      "Epoch 161/300\n",
      "Average training loss: 0.023898571921719444\n",
      "Average test loss: 0.002371355179283354\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02386197024418248\n",
      "Average test loss: 0.0023572512577391333\n",
      "Epoch 164/300\n",
      "Average training loss: 0.023864459534486133\n",
      "Average test loss: 0.0023155681933793755\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0238658330357737\n",
      "Average test loss: 0.0026585449933384853\n",
      "Epoch 166/300\n",
      "Average training loss: 0.023835073199537066\n",
      "Average test loss: 0.0023791941749966806\n",
      "Epoch 167/300\n",
      "Average training loss: 0.023768010111318693\n",
      "Average test loss: 0.002763236525468528\n",
      "Epoch 168/300\n",
      "Average training loss: 0.023753418770101335\n",
      "Average test loss: 0.002395952796149585\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02371965856105089\n",
      "Average test loss: 0.002425316656525764\n",
      "Epoch 170/300\n",
      "Average training loss: 0.023675066085325346\n",
      "Average test loss: 0.003639109573844406\n",
      "Epoch 171/300\n",
      "Average training loss: 0.023692697592907482\n",
      "Average test loss: 0.0023856585543188783\n",
      "Epoch 172/300\n",
      "Average training loss: 0.023774136786659558\n",
      "Average test loss: 0.050064248500598804\n",
      "Epoch 173/300\n",
      "Average training loss: 0.023631138212150998\n",
      "Average test loss: 0.002364697474365433\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02358577656580342\n",
      "Average test loss: 0.00238521084551596\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02351074731681082\n",
      "Average test loss: 0.00235927339291407\n",
      "Epoch 178/300\n",
      "Average training loss: 0.023541563486059507\n",
      "Average test loss: 0.0023463713243189787\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02354505825870567\n",
      "Average test loss: 0.002468283718658818\n",
      "Epoch 180/300\n",
      "Average training loss: 0.023489507352312405\n",
      "Average test loss: 0.002366303006807963\n",
      "Epoch 181/300\n",
      "Average training loss: 0.023509052260054484\n",
      "Average test loss: 0.0024394019866983098\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02342815275159147\n",
      "Average test loss: 0.0024272325312097867\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02341575700706906\n",
      "Average test loss: 0.0023906682131605017\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02341132660044564\n",
      "Average test loss: 0.0025113181033068234\n",
      "Epoch 185/300\n",
      "Average training loss: 0.023427650964922376\n",
      "Average test loss: 0.002381127044558525\n",
      "Epoch 186/300\n",
      "Average training loss: 0.023346709256370862\n",
      "Average test loss: 0.002336830752591292\n",
      "Epoch 188/300\n",
      "Average training loss: 0.023370549854305055\n",
      "Average test loss: 0.0023533398610436256\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02330042033890883\n",
      "Average test loss: 0.0024379785671416257\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02333768827882078\n",
      "Average test loss: 0.0024378973870641655\n",
      "Epoch 191/300\n",
      "Average training loss: 0.023267168937457932\n",
      "Average test loss: 0.0023686796913130414\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02324425480928686\n",
      "Average test loss: 0.0024024053929994505\n",
      "Epoch 193/300\n",
      "Average training loss: 0.023230765119194984\n",
      "Average test loss: 0.002384918255938424\n",
      "Epoch 194/300\n",
      "Average training loss: 0.023205377575423982\n",
      "Average test loss: 0.002417386398754186\n",
      "Epoch 195/300\n",
      "Average training loss: 0.023196490006314383\n",
      "Average test loss: 0.0024331302055054242\n",
      "Epoch 196/300\n",
      "Average training loss: 0.023208322611120012\n",
      "Average test loss: 0.002422853312972519\n",
      "Epoch 197/300\n",
      "Average training loss: 0.023198826225267515\n",
      "Average test loss: 0.002392979459423158\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02315437791744868\n",
      "Average test loss: 0.0024131600751231114\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0231163196745846\n",
      "Average test loss: 0.0024688831989963848\n",
      "Epoch 202/300\n",
      "Average training loss: 0.023140214911765523\n",
      "Average test loss: 0.002366489793277449\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02305933636261357\n",
      "Average test loss: 0.0023889998939509195\n",
      "Epoch 204/300\n",
      "Average training loss: 0.023040662590000364\n",
      "Average test loss: 0.002862454784102738\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02307208088040352\n",
      "Average test loss: 0.0023931436333805324\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02305534152686596\n",
      "Average test loss: 0.0024106713723805215\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02301020381351312\n",
      "Average test loss: 0.0024041652981605793\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023005232708321676\n",
      "Average test loss: 0.0023829255077160065\n",
      "Epoch 209/300\n",
      "Average training loss: 0.022960329249501227\n",
      "Average test loss: 0.0024093364917983614\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02295962886346711\n",
      "Average test loss: 0.0023500439394265413\n",
      "Epoch 211/300\n",
      "Average training loss: 0.022924627024266456\n",
      "Average test loss: 0.00236611571866605\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02295884587864081\n",
      "Average test loss: 0.0024098213274652757\n",
      "Epoch 213/300\n",
      "Average training loss: 0.022952951626645193\n",
      "Average test loss: 0.0024282191617207394\n",
      "Epoch 214/300\n",
      "Average training loss: 0.022872557974523967\n",
      "Average test loss: 0.002388765693331758\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02285325073202451\n",
      "Average test loss: 0.0025007060557189916\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02285116898185677\n",
      "Average test loss: 0.0024784942962643173\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02282647734714879\n",
      "Average test loss: 0.0023951207334175707\n",
      "Epoch 220/300\n",
      "Average training loss: 0.022849061378174357\n",
      "Average test loss: 0.0024069550718284313\n",
      "Epoch 221/300\n",
      "Average training loss: 0.022817653717266188\n",
      "Average test loss: 0.0024116208836850194\n",
      "Epoch 222/300\n",
      "Average training loss: 0.022804004442360665\n",
      "Average test loss: 0.002776498205649356\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02279083502292633\n",
      "Average test loss: 0.0024403050382518104\n",
      "Epoch 224/300\n",
      "Average training loss: 0.022771078149477642\n",
      "Average test loss: 0.0024227832831028437\n",
      "Epoch 225/300\n",
      "Average training loss: 0.022762477825085323\n",
      "Average test loss: 0.002401283705814017\n",
      "Epoch 226/300\n",
      "Average training loss: 0.022754937142133714\n",
      "Average test loss: 0.0024099655737065606\n",
      "Epoch 227/300\n",
      "Average training loss: 0.022756779238581656\n",
      "Average test loss: 0.0025902379146880575\n",
      "Epoch 228/300\n",
      "Average training loss: 0.022692060690787105\n",
      "Average test loss: 0.002413305940520432\n",
      "Epoch 229/300\n",
      "Average training loss: 0.022715082850721147\n",
      "Average test loss: 0.0023865291147182385\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02268254515197542\n",
      "Average test loss: 0.0024526284626788563\n",
      "Epoch 231/300\n",
      "Average training loss: 0.022675951856705876\n",
      "Average test loss: 0.0032163471395356786\n",
      "Epoch 232/300\n",
      "Average training loss: 0.022666274391942555\n",
      "Average test loss: 0.003111985426603092\n",
      "Epoch 233/300\n",
      "Average training loss: 0.022644558562172783\n",
      "Average test loss: 0.002418356507188744\n",
      "Epoch 234/300\n",
      "Average training loss: 0.02260154237018691\n",
      "Average test loss: 0.002441625741206937\n",
      "Epoch 235/300\n",
      "Average training loss: 0.022655871628059283\n",
      "Average test loss: 0.0025302068223762844\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02256765185793241\n",
      "Average test loss: 0.002485210144892335\n",
      "Epoch 237/300\n",
      "Average training loss: 0.022574241899781758\n",
      "Average test loss: 0.0024100920719405017\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02261881103946103\n",
      "Average test loss: 0.0036585095591015284\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02256567788289653\n",
      "Average test loss: 0.0030950848129060534\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02255598779188262\n",
      "Average test loss: 0.002446060498762462\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0225460496644179\n",
      "Average test loss: 0.0025713796576278076\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02255364447997676\n",
      "Average test loss: 0.002504232648242679\n",
      "Epoch 243/300\n",
      "Average training loss: 0.022524959393673473\n",
      "Average test loss: 0.0026536683411233957\n",
      "Epoch 244/300\n",
      "Average training loss: 0.022543032445841365\n",
      "Average test loss: 0.0025405752528458833\n",
      "Epoch 245/300\n",
      "Average training loss: 0.022494406471649804\n",
      "Average test loss: 0.002430273168409864\n",
      "Epoch 246/300\n",
      "Average training loss: 0.022483241377605333\n",
      "Average test loss: 0.003111327793035242\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02250009724497795\n",
      "Average test loss: 0.0024890609406348733\n",
      "Epoch 248/300\n",
      "Average training loss: 0.022460626731316248\n",
      "Average test loss: 0.002414750932198432\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02248659966389338\n",
      "Average test loss: 0.0024727076830135452\n",
      "Epoch 250/300\n",
      "Average training loss: 0.022485408183601167\n",
      "Average test loss: 0.002580066004768014\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02244945764541626\n",
      "Average test loss: 0.00241691506974813\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0224040478070577\n",
      "Average test loss: 0.0024280046017633545\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0223978527304199\n",
      "Average test loss: 0.003443375854856438\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02237794933716456\n",
      "Average test loss: 0.0024145593961907756\n",
      "Epoch 255/300\n",
      "Average training loss: 0.022381267199913662\n",
      "Average test loss: 0.0025408892542537714\n",
      "Epoch 256/300\n",
      "Average training loss: 0.022409358092480236\n",
      "Average test loss: 0.0027866247406022416\n",
      "Epoch 257/300\n",
      "Average training loss: 0.022369623510373964\n",
      "Average test loss: 0.0024366965896139544\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02234777598414156\n",
      "Average test loss: 0.3352993750207954\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02235654465523031\n",
      "Average test loss: 0.002371248996299174\n",
      "Epoch 260/300\n",
      "Average training loss: 0.022367604840132924\n",
      "Average test loss: 0.0033663931615236734\n",
      "Epoch 261/300\n",
      "Average training loss: 0.022306894497738945\n",
      "Average test loss: 0.0024153613239112828\n",
      "Epoch 262/300\n",
      "Average training loss: 0.022310245462589792\n",
      "Average test loss: 0.003372929447847936\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02229816970229149\n",
      "Average test loss: 0.002457881100682749\n",
      "Epoch 264/300\n",
      "Average training loss: 0.022307571220729086\n",
      "Average test loss: 0.0023819240828355155\n",
      "Epoch 265/300\n",
      "Average training loss: 0.022269894876413874\n",
      "Average test loss: 0.0024716552508374057\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02229679263300366\n",
      "Average test loss: 0.002910642350713412\n",
      "Epoch 267/300\n",
      "Average training loss: 0.022253635914789304\n",
      "Average test loss: 0.002443909846039282\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02221147982776165\n",
      "Average test loss: 0.0023944050123294197\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02223117390109433\n",
      "Average test loss: 0.0024751785936661893\n",
      "Epoch 270/300\n",
      "Average training loss: 0.022262398900257215\n",
      "Average test loss: 0.0024060286422156623\n",
      "Epoch 271/300\n",
      "Average training loss: 0.022234750900003646\n",
      "Average test loss: 0.0024277470522663658\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02219854290700621\n",
      "Average test loss: 0.0024904610113137297\n",
      "Epoch 273/300\n",
      "Average training loss: 0.022190546747710968\n",
      "Average test loss: 0.0024655102586580648\n",
      "Epoch 274/300\n",
      "Average training loss: 0.022167268208331533\n",
      "Average test loss: 0.0024489686489105223\n",
      "Epoch 275/300\n",
      "Average training loss: 0.022145317176977795\n",
      "Average test loss: 0.0024647103146546415\n",
      "Epoch 276/300\n",
      "Average training loss: 0.022142565495438044\n",
      "Average test loss: 0.0024876355562979974\n",
      "Epoch 277/300\n",
      "Average training loss: 0.022155180290341376\n",
      "Average test loss: 0.0024569363496783707\n",
      "Epoch 278/300\n",
      "Average training loss: 0.022139522458116213\n",
      "Average test loss: 0.0024813047817183864\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02214479115439786\n",
      "Average test loss: 0.002540972512008415\n",
      "Epoch 280/300\n",
      "Average training loss: 0.022179390107591946\n",
      "Average test loss: 0.002388027976680961\n",
      "Epoch 281/300\n",
      "Average training loss: 0.022123668395810658\n",
      "Average test loss: 0.002437812821732627\n",
      "Epoch 282/300\n",
      "Average training loss: 0.022150432480706108\n",
      "Average test loss: 0.00249303951073024\n",
      "Epoch 283/300\n",
      "Average training loss: 0.022070144353641404\n",
      "Average test loss: 0.00252382366400626\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02207146680686209\n",
      "Average test loss: 0.0025152571702169047\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02209668751226531\n",
      "Average test loss: 0.0023699313894742064\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02207829664647579\n",
      "Average test loss: 0.00244500265787873\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02209087546335326\n",
      "Average test loss: 0.002876708816116055\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02205078370620807\n",
      "Average test loss: 0.002512047968804836\n",
      "Epoch 289/300\n",
      "Average training loss: 0.022072420007652707\n",
      "Average test loss: 0.002484875914123323\n",
      "Epoch 290/300\n",
      "Average training loss: 0.022059870329168108\n",
      "Average test loss: 0.0025180644610275824\n",
      "Epoch 291/300\n",
      "Average training loss: 0.021999769431021478\n",
      "Average test loss: 0.0024531084805106125\n",
      "Epoch 292/300\n",
      "Average training loss: 0.022016936709483466\n",
      "Average test loss: 0.00322775198188093\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022032091836134592\n",
      "Average test loss: 0.0025836304335130587\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021980773611201182\n",
      "Average test loss: 0.0025200353943639333\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02200316921538777\n",
      "Average test loss: 0.002416417342507177\n",
      "Epoch 296/300\n",
      "Average training loss: 0.021958462960190245\n",
      "Average test loss: 0.0025680020658506286\n",
      "Epoch 297/300\n",
      "Average training loss: 0.021998319716917143\n",
      "Average test loss: 0.0024179810633262\n",
      "Epoch 298/300\n",
      "Average training loss: 0.021999061486787267\n",
      "Average test loss: 0.0024580557896859115\n",
      "Epoch 299/300\n",
      "Average training loss: 0.021937286870347128\n",
      "Average test loss: 0.0024722405131906273\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02197633136808872\n",
      "Average test loss: 0.002462641634874874\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth10/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.74\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.91\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.09\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.15\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.18\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.33\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.73\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.45\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.32\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.66\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 49.206911241319446\n",
      "Average test loss: 0.008706399066994587\n",
      "Epoch 2/300\n",
      "Average training loss: 23.86168340725369\n",
      "Average test loss: 0.005681043653438488\n",
      "Epoch 3/300\n",
      "Average training loss: 17.399774268256294\n",
      "Average test loss: 0.0058140808782643745\n",
      "Epoch 4/300\n",
      "Average training loss: 15.842140061272515\n",
      "Average test loss: 0.004997721320639054\n",
      "Epoch 5/300\n",
      "Average training loss: 11.690159873962402\n",
      "Average test loss: 0.02701617210606734\n",
      "Epoch 6/300\n",
      "Average training loss: 9.89602776421441\n",
      "Average test loss: 0.18914309958616893\n",
      "Epoch 7/300\n",
      "Average training loss: 9.074097617255317\n",
      "Average test loss: 0.004718318830761644\n",
      "Epoch 8/300\n",
      "Average training loss: 8.002977130889892\n",
      "Average test loss: 0.004600509018119838\n",
      "Epoch 9/300\n",
      "Average training loss: 6.845903408474392\n",
      "Average test loss: 0.04049951831996441\n",
      "Epoch 10/300\n",
      "Average training loss: 6.593447208828397\n",
      "Average test loss: 0.00474134197127488\n",
      "Epoch 11/300\n",
      "Average training loss: 5.527207022772895\n",
      "Average test loss: 0.004732817613416248\n",
      "Epoch 12/300\n",
      "Average training loss: 4.5406651713053385\n",
      "Average test loss: 0.004441064656608634\n",
      "Epoch 13/300\n",
      "Average training loss: 3.469889208475749\n",
      "Average test loss: 0.8248844889762501\n",
      "Epoch 15/300\n",
      "Average training loss: 3.1701608153449166\n",
      "Average test loss: 0.004393358755856752\n",
      "Epoch 16/300\n",
      "Average training loss: 2.9328511644999185\n",
      "Average test loss: 0.005397871918562386\n",
      "Epoch 17/300\n",
      "Average training loss: 2.289652384652032\n",
      "Average test loss: 0.08795537094399333\n",
      "Epoch 19/300\n",
      "Average training loss: 1.5936881544325088\n",
      "Average test loss: 0.004310722721947564\n",
      "Epoch 21/300\n",
      "Average training loss: 1.378884742418925\n",
      "Average test loss: 0.004638891206433376\n",
      "Epoch 22/300\n",
      "Average training loss: 1.2516096265580918\n",
      "Average test loss: 0.01820229236698813\n",
      "Epoch 23/300\n",
      "Average training loss: 1.093731688817342\n",
      "Average test loss: 0.038582112037473255\n",
      "Epoch 24/300\n",
      "Average training loss: 0.9957221440209283\n",
      "Average test loss: 0.01073625932344132\n",
      "Epoch 25/300\n",
      "Average training loss: 0.8590986534754436\n",
      "Average test loss: 0.004313704548610581\n",
      "Epoch 26/300\n",
      "Average training loss: 0.7432713644769456\n",
      "Average test loss: 0.00638058869375123\n",
      "Epoch 27/300\n",
      "Average test loss: 0.004220460451104574\n",
      "Epoch 28/300\n",
      "Average training loss: 0.5685354352527194\n",
      "Average test loss: 0.004227534580975771\n",
      "Epoch 29/300\n",
      "Average training loss: 0.500994836807251\n",
      "Average test loss: 0.004397401243862178\n",
      "Epoch 30/300\n",
      "Average training loss: 0.439906479411655\n",
      "Average test loss: 0.004220377990562055\n",
      "Epoch 31/300\n",
      "Average training loss: 0.387709628979365\n",
      "Average test loss: 0.004725039742266139\n",
      "Epoch 32/300\n",
      "Average training loss: 0.3440748249424828\n",
      "Average test loss: 0.0045853897978862124\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3077653821574317\n",
      "Average test loss: 0.004182061109691858\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2771857281393475\n",
      "Average test loss: 0.0041818371531036165\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2507579714457194\n",
      "Average test loss: 0.004185571025643084\n",
      "Epoch 36/300\n",
      "Average training loss: 0.22858240516980488\n",
      "Average test loss: 0.004232670943770144\n",
      "Epoch 37/300\n",
      "Average training loss: 0.21020714945263333\n",
      "Average test loss: 0.004180436993224753\n",
      "Epoch 38/300\n",
      "Average training loss: 0.19442895460128784\n",
      "Average test loss: 0.00417263751435611\n",
      "Epoch 39/300\n",
      "Average training loss: 0.18158385234408908\n",
      "Average test loss: 0.0041928141208158595\n",
      "Epoch 40/300\n",
      "Average training loss: 0.17105115229553647\n",
      "Average test loss: 0.004150881891863214\n",
      "Epoch 41/300\n",
      "Average training loss: 0.16234157552984027\n",
      "Average test loss: 0.0041604187616871464\n",
      "Epoch 42/300\n",
      "Average training loss: 0.15551426535844803\n",
      "Average test loss: 0.004192590051848027\n",
      "Epoch 43/300\n",
      "Average training loss: 0.14987910815080008\n",
      "Average test loss: 0.004142220659595397\n",
      "Epoch 44/300\n",
      "Average training loss: 0.14541878300242952\n",
      "Average test loss: 0.0041670212015095685\n",
      "Epoch 45/300\n",
      "Average training loss: 0.1416548463900884\n",
      "Average test loss: 0.004354928503847785\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1359436284303665\n",
      "Average test loss: 0.004184317292438613\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1338576676050822\n",
      "Average test loss: 0.0041622066779269115\n",
      "Epoch 49/300\n",
      "Average training loss: 0.13220501593086453\n",
      "Average test loss: 0.0041770612417409815\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1308674328327179\n",
      "Average test loss: 0.004755535474874907\n",
      "Epoch 51/300\n",
      "Average training loss: 0.12967007839017444\n",
      "Average test loss: 0.0041437958075354496\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12867748779058458\n",
      "Average test loss: 0.004169963054979841\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12792908330758412\n",
      "Average test loss: 0.004262498333222336\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12714925842814975\n",
      "Average test loss: 0.004188310701400042\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12660096099641588\n",
      "Average test loss: 0.004172417955266104\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12598113651408088\n",
      "Average test loss: 0.00415587755271958\n",
      "Epoch 57/300\n",
      "Average training loss: 0.1254343603981866\n",
      "Average test loss: 0.0041894592928389705\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12497139010164472\n",
      "Average test loss: 0.0041997881825599406\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1245597801009814\n",
      "Average test loss: 0.004298329253163603\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12419412230120765\n",
      "Average test loss: 0.004224999646345775\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12381492756472694\n",
      "Average test loss: 0.004201467948241366\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12340837697850333\n",
      "Average test loss: 0.004242083964455459\n",
      "Epoch 63/300\n",
      "Average training loss: 0.12298930651611752\n",
      "Average test loss: 0.0042154107354581355\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12277067112922668\n",
      "Average test loss: 0.004147715779228343\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12179775978459252\n",
      "Average test loss: 0.004210993149628242\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12148961528142294\n",
      "Average test loss: 0.004281049970123503\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12121490797731611\n",
      "Average test loss: 0.004241436081421044\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12076159793800778\n",
      "Average test loss: 0.004250404139463273\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12032010428773032\n",
      "Average test loss: 0.00417534266722699\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12010483828518126\n",
      "Average test loss: 0.00416688054613769\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11943588583336937\n",
      "Average test loss: 0.004261911257273621\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11901548623376422\n",
      "Average test loss: 0.004195388216111395\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11870151350895564\n",
      "Average test loss: 0.004278596034894387\n",
      "Epoch 76/300\n",
      "Average training loss: 0.11849833267264896\n",
      "Average test loss: 0.004307241064392858\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11794152082999547\n",
      "Average test loss: 0.00424559951449434\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11787559474839104\n",
      "Average test loss: 0.004294557517394423\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1175739673309856\n",
      "Average test loss: 0.004205989897044168\n",
      "Epoch 80/300\n",
      "Average training loss: 0.1169300880100992\n",
      "Average test loss: 0.004341426044495569\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11675120995442073\n",
      "Average test loss: 0.004230136975646019\n",
      "Epoch 82/300\n",
      "Average training loss: 0.1163061528272099\n",
      "Average test loss: 0.004176831482392218\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11621498322486877\n",
      "Average test loss: 0.0043051524630023374\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1159070505036248\n",
      "Average test loss: 0.004375992581041323\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11539078073369133\n",
      "Average test loss: 0.004254321703894271\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11466446009609434\n",
      "Average test loss: 0.004243877919597758\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11482772993379169\n",
      "Average test loss: 0.004470889278997978\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1142133023738861\n",
      "Average test loss: 0.004333875491387315\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11359883130921258\n",
      "Average test loss: 0.004269710412869851\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11318153298563427\n",
      "Average test loss: 0.004311440793590414\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11319622274239859\n",
      "Average test loss: 0.004248883141204715\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1127175083094173\n",
      "Average test loss: 0.0044137838960935675\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11241705407036676\n",
      "Average test loss: 0.004261445058716668\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11225059350331625\n",
      "Average training loss: 0.11176053473022249\n",
      "Average test loss: 0.00423172627741264\n",
      "Epoch 97/300\n",
      "Average training loss: 0.1114967990650071\n",
      "Average test loss: 0.004364889136205117\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11127046300967534\n",
      "Average test loss: 0.004284693858275811\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11076075486342112\n",
      "Average test loss: 0.004321404287384616\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11073621975051032\n",
      "Average test loss: 0.0043755525632037055\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11004664021730423\n",
      "Average test loss: 0.004373281828438242\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10986316674947738\n",
      "Average test loss: 0.004313283956092265\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10968338431914647\n",
      "Average test loss: 0.004320636134180758\n",
      "Epoch 104/300\n",
      "Average training loss: 0.1093387846218215\n",
      "Average test loss: 0.0043677626065909865\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10901920719279183\n",
      "Average test loss: 0.004362513177924686\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10888353546460469\n",
      "Average test loss: 0.004411795823110474\n",
      "Epoch 107/300\n",
      "Average training loss: 0.1086386759546068\n",
      "Average test loss: 0.004444128931810458\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10811542361312443\n",
      "Average test loss: 0.004346073819531335\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10782355184687509\n",
      "Average test loss: 0.00436046466169258\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10717346652348836\n",
      "Average test loss: 0.004425001609656546\n",
      "Epoch 112/300\n",
      "Average training loss: 0.106928089045816\n",
      "Average test loss: 0.004465589524143272\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10671824612882402\n",
      "Average test loss: 0.004443855313791169\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10644697897964053\n",
      "Average test loss: 0.004448780603292916\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10609654397434659\n",
      "Average test loss: 0.00442139864485297\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10591743203666475\n",
      "Average test loss: 0.004479205974274212\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10580042774147458\n",
      "Average test loss: 0.004518096743358506\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10493020397424697\n",
      "Average test loss: 0.004364695756799645\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10470988840527004\n",
      "Average test loss: 0.004495460445268286\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10464685916238362\n",
      "Average test loss: 0.0044288194802486235\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10432843950721953\n",
      "Average test loss: 0.0043597019306487505\n",
      "Epoch 123/300\n",
      "Average training loss: 0.1040062148835924\n",
      "Average test loss: 0.004465477798134088\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10365643177429835\n",
      "Average test loss: 0.004410626157497367\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10324398783180448\n",
      "Average test loss: 0.004399439673870802\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10296155373917686\n",
      "Average test loss: 0.004496827572998074\n",
      "Epoch 128/300\n",
      "Average training loss: 0.1028877024087641\n",
      "Average test loss: 0.004609187396036254\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10249736758735445\n",
      "Average test loss: 0.004544796716214882\n",
      "Epoch 130/300\n",
      "Average training loss: 0.102273747490512\n",
      "Average test loss: 0.004496458979530467\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10210896770821677\n",
      "Average test loss: 0.00439024622231308\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10186421874496672\n",
      "Average test loss: 0.004485872770349185\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10147378920184241\n",
      "Average test loss: 0.004399299722164869\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10142676253451241\n",
      "Average test loss: 0.00436578086970581\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10113764279418522\n",
      "Average test loss: 0.004621976225740379\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10102022823360231\n",
      "Average test loss: 0.004664491827910145\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10059926089313295\n",
      "Average test loss: 0.00457042652906643\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10015950496991476\n",
      "Average test loss: 0.004641812409791682\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10013964372873306\n",
      "Average test loss: 0.004458408805231254\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09977137141095267\n",
      "Average training loss: 0.0995892957912551\n",
      "Average test loss: 0.004472909076346291\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09924950171179242\n",
      "Average test loss: 0.004555803544405434\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09915071033769184\n",
      "Average test loss: 0.004459147462621331\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09891685381862853\n",
      "Average test loss: 0.004378048279012243\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09864970750941171\n",
      "Average test loss: 0.004514154957814349\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09858400115701887\n",
      "Average test loss: 0.004427117725006409\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09822440295086966\n",
      "Average test loss: 0.004461629232184755\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0977666140794754\n",
      "Average test loss: 0.0044785785105907255\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09761880790525013\n",
      "Average test loss: 0.004391569859865639\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0977276047401958\n",
      "Average test loss: 0.004540732211536831\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09728431668546465\n",
      "Average test loss: 0.00446786387761434\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09714926091167662\n",
      "Average test loss: 0.004535259384868874\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09680302182502216\n",
      "Average test loss: 0.0044753719216419595\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09670785652266609\n",
      "Average test loss: 0.004382985206113921\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09656537591086493\n",
      "Average test loss: 0.004504244686000877\n",
      "Epoch 159/300\n",
      "Average training loss: 0.09610036439365811\n",
      "Average test loss: 0.004462075306102633\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0959236009783215\n",
      "Average test loss: 0.004444938998255465\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09576567222012414\n",
      "Average test loss: 0.005232480007327265\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09540573973788155\n",
      "Average test loss: 0.004497724787642559\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09509106210536426\n",
      "Average test loss: 0.004410057923032178\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09490571876366934\n",
      "Average test loss: 0.004442132852143711\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09475002051062054\n",
      "Average test loss: 0.004616060068623887\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0946125128335423\n",
      "Average test loss: 0.004559591872410642\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09458299958043628\n",
      "Average test loss: 0.004870864094131523\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09432533852921592\n",
      "Average test loss: 0.004442336970319351\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09389719746510188\n",
      "Average test loss: 0.004707747598903047\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09386499417490429\n",
      "Average test loss: 0.004526006700264083\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09357304955191083\n",
      "Average test loss: 0.004454193027897013\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09349536620577177\n",
      "Average test loss: 0.004616033013082213\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09341792537106408\n",
      "Average test loss: 0.004622724519421657\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09300634455018573\n",
      "Average test loss: 0.004784017230901453\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09297084661987093\n",
      "Average test loss: 0.004402060724380943\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09259881531529957\n",
      "Average test loss: 0.004630568949091766\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09265685478846232\n",
      "Average test loss: 0.0045597291924059395\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09227085145976809\n",
      "Average test loss: 0.0045728812242547675\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0921071767144733\n",
      "Average test loss: 0.004559876827316151\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09197343598471748\n",
      "Average test loss: 0.0045270100250426265\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09178080024984148\n",
      "Average test loss: 0.004552155648461647\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09159846452209684\n",
      "Average test loss: 0.004573152733345826\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09148898173040813\n",
      "Average training loss: 0.09159458563062879\n",
      "Average test loss: 0.004533453448779053\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09124274292919371\n",
      "Average test loss: 0.004450234234333038\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09107974572976431\n",
      "Average test loss: 0.004535533848736021\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09100593976842032\n",
      "Average test loss: 0.0045723446837315955\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09068422262536155\n",
      "Average test loss: 0.004369779380866223\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09078937287463082\n",
      "Average test loss: 0.005244409288797114\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09047315873040093\n",
      "Average test loss: 0.004543835035835703\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09032188959916433\n",
      "Average test loss: 0.004581257121016582\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09010816063483557\n",
      "Average test loss: 0.004619117178850704\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08992535863320032\n",
      "Average test loss: 0.0045404503022631005\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08973665336767832\n",
      "Average test loss: 0.004697630768434869\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08984266149335438\n",
      "Average test loss: 0.004713506780150864\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08972249724467596\n",
      "Average test loss: 0.004564273912252652\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08954473595486746\n",
      "Average test loss: 0.004519721367292934\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08913617918226455\n",
      "Average test loss: 0.004471776349883941\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0891912964185079\n",
      "Average test loss: 0.004658988031662173\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0893691010342704\n",
      "Average test loss: 0.00468784518332945\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08883910816907882\n",
      "Average test loss: 0.004611914534535673\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08862932550907135\n",
      "Average test loss: 0.004446281176060438\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08870638120836682\n",
      "Average training loss: 0.08848799893591139\n",
      "Average test loss: 0.004556361065142685\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08832202595472335\n",
      "Average test loss: 0.004617440265499883\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08828681219286388\n",
      "Average test loss: 0.005123354957335525\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08813112921184964\n",
      "Average test loss: 0.004725915810714166\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08803746349281735\n",
      "Average test loss: 0.004568840955280595\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08810612473222945\n",
      "Average test loss: 0.004571455371876558\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08791425801648034\n",
      "Average test loss: 0.021172479622893862\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08766019685400857\n",
      "Average test loss: 0.004725074757304456\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08756929398245282\n",
      "Average test loss: 0.004526173820098242\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0874222276310126\n",
      "Average test loss: 0.004619371045794752\n",
      "Epoch 221/300\n",
      "Average training loss: 0.08731122909651862\n",
      "Average test loss: 0.004545839510858059\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08726133927371767\n",
      "Average test loss: 0.004630950575901402\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08704616956578361\n",
      "Average test loss: 0.004536407097760174\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0872494397494528\n",
      "Average test loss: 0.004726783595358332\n",
      "Epoch 225/300\n",
      "Average training loss: 0.08718174828423395\n",
      "Average test loss: 0.00456681081900994\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08672296490271886\n",
      "Average test loss: 0.004526994411316183\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08670093299945196\n",
      "Average test loss: 0.004508903904507557\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08674402536286248\n",
      "Average test loss: 0.0046623181622061464\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08642528625329335\n",
      "Average test loss: 0.004574453393204345\n",
      "Epoch 230/300\n",
      "Average training loss: 0.08625527595149146\n",
      "Average test loss: 0.004581453974462218\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08632350093788571\n",
      "Average test loss: 0.004896415503488647\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08614806726906035\n",
      "Average test loss: 0.004592370272924503\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0862002609901958\n",
      "Average test loss: 0.004714539390471247\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08608927179045148\n",
      "Average test loss: 0.004504255167519053\n",
      "Epoch 235/300\n",
      "Average training loss: 0.085850680814849\n",
      "Average test loss: 0.004609738710439867\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08577055834399329\n",
      "Average test loss: 0.004653621204611328\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08572803937064277\n",
      "Average test loss: 0.004661578935881456\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0856899602678087\n",
      "Average test loss: 0.004669627730217245\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08551613554027346\n",
      "Average test loss: 0.004545970430390702\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08543457383248541\n",
      "Average test loss: 0.004404253329046899\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08524688721365399\n",
      "Average test loss: 0.004574336438129345\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0852148859169748\n",
      "Average test loss: 0.004547869119379255\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08517053096161949\n",
      "Average test loss: 0.004524350812865628\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08507621171077093\n",
      "Average test loss: 0.00459888208264278\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08481192137797673\n",
      "Average test loss: 0.0045753781321562\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08481500427590476\n",
      "Average test loss: 0.004575059457371632\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08479439685079787\n",
      "Average test loss: 0.004697441305344304\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08467606416013505\n",
      "Average test loss: 0.004581402822087208\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08459210346804724\n",
      "Average test loss: 0.004600861096547709\n",
      "Epoch 250/300\n",
      "Average training loss: 0.0845385566022661\n",
      "Average test loss: 0.004681182756192155\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08433933416340086\n",
      "Average test loss: 0.004982347865485483\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0844521190656556\n",
      "Average test loss: 0.00457062811549339\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0843977786037657\n",
      "Average test loss: 0.004585844756621453\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08411694286929236\n",
      "Average test loss: 0.004523902803245518\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08396631011697982\n",
      "Average test loss: 0.004623490980929798\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08407756867011389\n",
      "Average test loss: 0.004567351508471701\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08389663295613395\n",
      "Average test loss: 0.004446508991221587\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08382625333468119\n",
      "Average test loss: 0.004513914099170102\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08366846251487732\n",
      "Average test loss: 0.004641616539408764\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08371599337127474\n",
      "Average test loss: 0.00460702043481999\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08332427744070689\n",
      "Average test loss: 0.004539213076233864\n",
      "Epoch 262/300\n",
      "Average training loss: 0.08354943252603213\n",
      "Average test loss: 0.0046760908191402755\n",
      "Epoch 263/300\n",
      "Average training loss: 0.08339760440587997\n",
      "Average test loss: 0.004644780177623034\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08341228435436884\n",
      "Average test loss: 0.0045399581506434415\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08321482350428899\n",
      "Average test loss: 0.00451045298948884\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08373491409752104\n",
      "Average test loss: 0.004610274023686846\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0829885438018375\n",
      "Average test loss: 0.004570105734591683\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08280487946006987\n",
      "Average test loss: 0.0044278203515956795\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08302810719940397\n",
      "Average test loss: 0.00461434331536293\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0829058406551679\n",
      "Average test loss: 0.00472248412296176\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08266036159462399\n",
      "Average test loss: 0.005513679362005658\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08262900239891476\n",
      "Average test loss: 0.004541945768313275\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0824631634619501\n",
      "Average test loss: 0.004533273991611269\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08255579306019677\n",
      "Average test loss: 0.004795519086842736\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08264683532714844\n",
      "Average test loss: 0.004566485139644808\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0823670218653149\n",
      "Average test loss: 0.004711056588838498\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08220566201872295\n",
      "Average test loss: 0.004527198678917355\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08215755879878998\n",
      "Average test loss: 0.00482298404061132\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08210522407293319\n",
      "Average test loss: 0.004485610950324271\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0821037111878395\n",
      "Average test loss: 0.0046546538617047994\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08206548218594657\n",
      "Average test loss: 0.00450417219930225\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08222151778803931\n",
      "Average test loss: 0.004664790113766988\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08182398272222943\n",
      "Average test loss: 0.007047835070225927\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08159222681654824\n",
      "Average test loss: 0.004562061858673891\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08166227929790815\n",
      "Average test loss: 0.0046965156590773\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08156281788481606\n",
      "Average test loss: 0.004477068178769615\n",
      "Epoch 287/300\n",
      "Average training loss: 0.081652705570062\n",
      "Average test loss: 0.004469141958488358\n",
      "Epoch 288/300\n",
      "Average training loss: 0.08137660196754667\n",
      "Average test loss: 0.004717862587422132\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08140217656559415\n",
      "Average test loss: 0.004723998778396183\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0814443019065592\n",
      "Average test loss: 0.004544275442759196\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08121067716015709\n",
      "Average test loss: 0.0045811981438762615\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08113851708836026\n",
      "Average test loss: 0.004683555816196733\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08117668347226249\n",
      "Average test loss: 0.004543203813955188\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08099975417057673\n",
      "Average test loss: 0.004637234105832047\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08093336413304011\n",
      "Average test loss: 0.004489105510628886\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08092962472968632\n",
      "Average test loss: 0.004665518749505281\n",
      "Epoch 297/300\n",
      "Average training loss: 0.08091084702147378\n",
      "Average test loss: 0.004630028043977088\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08068988335132599\n",
      "Average test loss: 0.0045571533905135264\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08061088801092571\n",
      "Average test loss: 0.004522569813662105\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08064843338396814\n",
      "Average test loss: 0.004588485519091288\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 32.8800012190077\n",
      "Average test loss: 20.571557048350574\n",
      "Epoch 2/300\n",
      "Average training loss: 16.61090783437093\n",
      "Average test loss: 0.005355732059313192\n",
      "Epoch 3/300\n",
      "Average training loss: 11.437557801140679\n",
      "Average test loss: 0.005283909589466122\n",
      "Epoch 4/300\n",
      "Average training loss: 10.114496537102593\n",
      "Average test loss: 0.004570267712904347\n",
      "Epoch 5/300\n",
      "Average training loss: 8.548619177076551\n",
      "Average test loss: 0.004476848460319969\n",
      "Epoch 6/300\n",
      "Average training loss: 5.91473414738973\n",
      "Average test loss: 0.006296028844184346\n",
      "Epoch 7/300\n",
      "Average training loss: 5.311468849182129\n",
      "Average test loss: 7.7059291192293164\n",
      "Epoch 8/300\n",
      "Average training loss: 4.0274291528066\n",
      "Average test loss: 0.0042738512468834715\n",
      "Epoch 9/300\n",
      "Average training loss: 3.645594702826606\n",
      "Average test loss: 12.381398910002162\n",
      "Epoch 10/300\n",
      "Average training loss: 3.18761596637302\n",
      "Average test loss: 0.042435521826975874\n",
      "Epoch 11/300\n",
      "Average training loss: 2.907162921057807\n",
      "Average test loss: 0.0039519740574889715\n",
      "Epoch 12/300\n",
      "Average training loss: 2.49915575281779\n",
      "Average test loss: 0.006328748568478558\n",
      "Epoch 13/300\n",
      "Average training loss: 2.096497121281094\n",
      "Average test loss: 0.047196483467602066\n",
      "Epoch 14/300\n",
      "Average training loss: 1.7196065173678927\n",
      "Average test loss: 0.004046465130936768\n",
      "Epoch 15/300\n",
      "Average training loss: 1.4279912826750014\n",
      "Average test loss: 0.0037766497749835252\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1966745255788167\n",
      "Average test loss: 0.5312479315925803\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0194979115592109\n",
      "Average test loss: 639.1959236880922\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9095754685401917\n",
      "Average test loss: 0.3596829384416342\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7703730849160089\n",
      "Average test loss: 45.27860580455015\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6580948514938354\n",
      "Average test loss: 0.5360987829135524\n",
      "Epoch 21/300\n",
      "Average training loss: 0.5471642212337918\n",
      "Average test loss: 0.003615783233195543\n",
      "Epoch 22/300\n",
      "Average training loss: 0.46105909898546005\n",
      "Average test loss: 0.0036459147272010642\n",
      "Epoch 23/300\n",
      "Average training loss: 0.39637214120229086\n",
      "Average test loss: 0.003533027361871468\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3468409735361735\n",
      "Average test loss: 0.0035031825641377106\n",
      "Epoch 25/300\n",
      "Average training loss: 0.30545243485768636\n",
      "Average test loss: 0.011663653578195307\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2713910146554311\n",
      "Average test loss: 0.0034972644394470585\n",
      "Epoch 27/300\n",
      "Average training loss: 0.24259501446617973\n",
      "Average test loss: 0.0034608065355569125\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2192695214483473\n",
      "Average test loss: 0.003442244459150566\n",
      "Epoch 29/300\n",
      "Average training loss: 0.19903705973095365\n",
      "Average test loss: 0.003750618314370513\n",
      "Epoch 30/300\n",
      "Average training loss: 0.18278939182228512\n",
      "Average test loss: 0.003441184423863888\n",
      "Epoch 31/300\n",
      "Average training loss: 0.16917127245002322\n",
      "Average test loss: 0.0034537484921101065\n",
      "Epoch 32/300\n",
      "Average training loss: 0.15827262907558018\n",
      "Average test loss: 0.003401450007326073\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14888369558917153\n",
      "Average test loss: 0.003411583128074805\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14149436002307467\n",
      "Average test loss: 0.0034119267109781505\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13520416968398624\n",
      "Average test loss: 0.0034051374323252176\n",
      "Epoch 36/300\n",
      "Average training loss: 0.1303464894890785\n",
      "Average test loss: 0.0033998190872371195\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12602367873324288\n",
      "Average test loss: 0.0034106035263588032\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12245045273171531\n",
      "Average test loss: 0.0034092072445071407\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11958620635668436\n",
      "Average test loss: 0.0033849976304918526\n",
      "Epoch 40/300\n",
      "Average training loss: 0.11734780572520362\n",
      "Average test loss: 0.0036489634753929245\n",
      "Epoch 41/300\n",
      "Average training loss: 0.11520083275106219\n",
      "Average test loss: 0.0033681222554296257\n",
      "Epoch 42/300\n",
      "Average training loss: 0.11354667633771896\n",
      "Average test loss: 0.0034229781998114453\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1119858808517456\n",
      "Average test loss: 0.0033626704903112516\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11073486304283142\n",
      "Average test loss: 0.003448868691507313\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10960366445117527\n",
      "Average test loss: 0.0034800154879275295\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1085892676446173\n",
      "Average test loss: 0.0033669198544489013\n",
      "Epoch 47/300\n",
      "Average training loss: 0.10755541967021094\n",
      "Average test loss: 0.0034071993455290794\n",
      "Epoch 48/300\n",
      "Average training loss: 0.10684676308102078\n",
      "Average test loss: 0.003393516568467021\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10603740033838484\n",
      "Average test loss: 0.00334658615435991\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1052077087495062\n",
      "Average test loss: 0.0033786915989799634\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10448030067152447\n",
      "Average test loss: 0.003466813848664363\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10394585290220049\n",
      "Average test loss: 0.003816693050579892\n",
      "Epoch 53/300\n",
      "Average training loss: 0.1033603850139512\n",
      "Average test loss: 0.0035113471936848426\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10249599693218867\n",
      "Average test loss: 0.003440616758954194\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10201119503710004\n",
      "Average test loss: 0.0033455984896669784\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10138221052620146\n",
      "Average test loss: 0.0033642420365164677\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10063049370712704\n",
      "Average test loss: 0.0038307672548625205\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10004060310787624\n",
      "Average test loss: 0.0033500769719895388\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09934996467828751\n",
      "Average test loss: 0.003363374277121491\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09858550839953953\n",
      "Average test loss: 0.003759666281855769\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0981844002008438\n",
      "Average test loss: 0.0036234515230688783\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09760892748170429\n",
      "Average test loss: 0.0033408584867914517\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09689794208606084\n",
      "Average test loss: 0.0035198788767059645\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09658842937813865\n",
      "Average test loss: 0.003474476315495041\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09603893562157949\n",
      "Average test loss: 0.0034163175214909844\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09514460147751702\n",
      "Average test loss: 0.0034779296142773497\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0947802204158571\n",
      "Average test loss: 0.003401339560954107\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09414444584316678\n",
      "Average test loss: 0.0035677797274871006\n",
      "Epoch 69/300\n",
      "Average training loss: 0.093692917406559\n",
      "Average test loss: 0.003538959597547849\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0930825056301223\n",
      "Average test loss: 0.003501689679299792\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0925332094166014\n",
      "Average test loss: 0.0033613750640716818\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09198321626583734\n",
      "Average test loss: 0.0033959691847364106\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09133529408772786\n",
      "Average test loss: 0.0034873974919319155\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09121314787202411\n",
      "Average test loss: 0.0034489509997268518\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09065534414185418\n",
      "Average test loss: 0.0035411663711484936\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09018828050626648\n",
      "Average test loss: 0.003503438387480047\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08963932138019137\n",
      "Average test loss: 0.0034874478009425934\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08926097860601213\n",
      "Average test loss: 0.0036107907046874364\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08874325125084984\n",
      "Average test loss: 0.0034806229422489803\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08842012178235584\n",
      "Average test loss: 0.0034283993418018025\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0879982737104098\n",
      "Average test loss: 0.0034754977201422056\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08750325242016051\n",
      "Average test loss: 0.0034630934765769376\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08727757381068335\n",
      "Average test loss: 0.0034819669839408663\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0867276764843199\n",
      "Average test loss: 0.00348417447341813\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08700356212589476\n",
      "Average test loss: 0.0035343790153662363\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08613019192881054\n",
      "Average test loss: 0.0034801656171265577\n",
      "Epoch 87/300\n",
      "Average training loss: 0.08542983312739266\n",
      "Average test loss: 0.0035023369561466905\n",
      "Epoch 88/300\n",
      "Average training loss: 0.08523803410596317\n",
      "Average test loss: 0.003619420933847626\n",
      "Epoch 89/300\n",
      "Average training loss: 0.08494398688607746\n",
      "Average test loss: 0.003441323369430999\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0846223213010364\n",
      "Average test loss: 0.0035771088459425502\n",
      "Epoch 91/300\n",
      "Average training loss: 0.08419315662648942\n",
      "Average test loss: 0.0034900911746339664\n",
      "Epoch 92/300\n",
      "Average training loss: 0.08402486251460181\n",
      "Average test loss: 0.0034360695054961574\n",
      "Epoch 93/300\n",
      "Average training loss: 0.08356420053376092\n",
      "Average test loss: 0.003453304463169641\n",
      "Epoch 94/300\n",
      "Average training loss: 0.08336827566888597\n",
      "Average test loss: 0.003466661444554726\n",
      "Epoch 95/300\n",
      "Average training loss: 0.08328115312920677\n",
      "Average test loss: 0.0035592067728026046\n",
      "Epoch 96/300\n",
      "Average training loss: 0.08260255271196365\n",
      "Average test loss: 0.0035902370390378767\n",
      "Epoch 97/300\n",
      "Average training loss: 0.08254234011636839\n",
      "Average test loss: 0.003583081882033083\n",
      "Epoch 98/300\n",
      "Average training loss: 0.08195672665701972\n",
      "Average test loss: 0.003442569997575548\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0818353142340978\n",
      "Average test loss: 0.00356610866346293\n",
      "Epoch 100/300\n",
      "Average training loss: 0.08192366920577156\n",
      "Average test loss: 0.0035451185868846046\n",
      "Epoch 101/300\n",
      "Average training loss: 0.08121497896644804\n",
      "Average test loss: 0.0035955875259306696\n",
      "Epoch 102/300\n",
      "Average training loss: 0.08114080260859595\n",
      "Average test loss: 0.0035922694545653133\n",
      "Epoch 103/300\n",
      "Average training loss: 0.08068953087594774\n",
      "Average test loss: 0.003623044490400288\n",
      "Epoch 104/300\n",
      "Average training loss: 0.08048190367221832\n",
      "Average test loss: 0.0035425576890508333\n",
      "Epoch 105/300\n",
      "Average training loss: 0.08019652802745501\n",
      "Average test loss: 0.0036568575975381664\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07987998704777824\n",
      "Average test loss: 0.0036840016394853593\n",
      "Epoch 107/300\n",
      "Average training loss: 0.08027100778950585\n",
      "Average test loss: 0.003590765146124694\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07940981313255098\n",
      "Average test loss: 0.003542239005987843\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07937632590532302\n",
      "Average test loss: 0.003558099885367685\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07888831966453129\n",
      "Average test loss: 0.0035311134163704184\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07883580033315553\n",
      "Average test loss: 0.0037230220199045206\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07851269602775574\n",
      "Average test loss: 0.0036781409109632175\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07822909672723877\n",
      "Average test loss: 0.0036299013870043887\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07801391421424018\n",
      "Average test loss: 0.0035005254394685228\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0777390641702546\n",
      "Average training loss: 0.07772350110610327\n",
      "Average test loss: 0.0036037130217171378\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07744942074020704\n",
      "Average test loss: 0.003926573141581483\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07712852828370201\n",
      "Average test loss: 0.0035675030557645693\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07702531438403659\n",
      "Average test loss: 0.0037916379620631536\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07693740781810549\n",
      "Average test loss: 0.0035745251352588335\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07710523858997557\n",
      "Average test loss: 0.0036146605488740734\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07684586892525355\n",
      "Average test loss: 0.003869536424262656\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07594851062032912\n",
      "Average test loss: 0.003631001553601689\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07582405492332246\n",
      "Average test loss: 0.0037238661468856865\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07559828736053573\n",
      "Average test loss: 0.00363354248739779\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07532575074831645\n",
      "Average test loss: 0.004079851047860251\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07522648607028855\n",
      "Average test loss: 0.0036030018592460287\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07504074447684818\n",
      "Average test loss: 0.0036809889127810795\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07508335900968975\n",
      "Average test loss: 0.003764076848410898\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07473278110557133\n",
      "Average test loss: 0.003805366275211175\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07460446935229831\n",
      "Average test loss: 0.0036252198080635734\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07464331899086635\n",
      "Average test loss: 0.003608065892631809\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07427070519659254\n",
      "Average test loss: 0.0035855562244024542\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07406664574477408\n",
      "Average test loss: 0.004241589501914051\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07410135638051563\n",
      "Average test loss: 0.0036748485318902468\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07382258099979824\n",
      "Average test loss: 0.0036833894145157603\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07354223452011745\n",
      "Average test loss: 0.0036448404631680914\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07347219673792521\n",
      "Average test loss: 0.003709148860226075\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07312180874082777\n",
      "Average test loss: 0.003587929937367638\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07320100696881612\n",
      "Average test loss: 0.0037228564508259296\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07294820604390569\n",
      "Average test loss: 0.003749155713659194\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07296505111455917\n",
      "Average test loss: 0.003701432370270292\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07273229572839207\n",
      "Average test loss: 0.003957661570360263\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07257852809296714\n",
      "Average test loss: 0.0037029089621371694\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0725393484963311\n",
      "Average test loss: 0.003750033446898063\n",
      "Epoch 147/300\n",
      "Average training loss: 0.07214893140064345\n",
      "Average test loss: 0.0036060613557282423\n",
      "Epoch 148/300\n",
      "Average training loss: 0.07254619389772415\n",
      "Average test loss: 0.0036605309372146925\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07222666522860527\n",
      "Average test loss: 0.003932046594719092\n",
      "Epoch 150/300\n",
      "Average training loss: 0.07196961272425122\n",
      "Average test loss: 0.0037126158473806247\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07170766503281063\n",
      "Average test loss: 0.0036296157439549763\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07144201348225275\n",
      "Average test loss: 0.003744956198665831\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07167728832032945\n",
      "Average test loss: 0.003997109091737204\n",
      "Epoch 154/300\n",
      "Average training loss: 0.07135384229818979\n",
      "Average test loss: 0.003579923137401541\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07109904337591595\n",
      "Average test loss: 0.0037510431673791673\n",
      "Epoch 156/300\n",
      "Average training loss: 0.07109879055288103\n",
      "Average test loss: 0.00360511710246404\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07095049597157373\n",
      "Average test loss: 0.003668879876534144\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07082734385463926\n",
      "Average test loss: 0.003750742956995964\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07094729603661432\n",
      "Average test loss: 0.003634004180216127\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07063283035490248\n",
      "Average test loss: 0.0036186014072348673\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07035388051801257\n",
      "Average test loss: 0.003834482398298052\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07052472074826559\n",
      "Average test loss: 0.003652363910029332\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07022109262148539\n",
      "Average test loss: 0.0035811013024714257\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07007937731345494\n",
      "Average test loss: 0.003666894426362382\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06999115973711013\n",
      "Average test loss: 0.0036473557386133408\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06982176775402493\n",
      "Average test loss: 0.003783639416512516\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06984448460406727\n",
      "Average test loss: 0.003751444457305802\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06969445608059566\n",
      "Average test loss: 0.0037911109291017056\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0694929567111863\n",
      "Average test loss: 0.0038676517928640046\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06953737890058094\n",
      "Average test loss: 0.0038708050321373676\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06950152403447363\n",
      "Average test loss: 0.004602198100752301\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06929364252752727\n",
      "Average test loss: 0.0036465945463213657\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0691323785285155\n",
      "Average test loss: 0.003765884591266513\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06898199093672965\n",
      "Average test loss: 0.0037056104056537153\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06895196399092675\n",
      "Average test loss: 0.0035601327332357565\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06877688141332733\n",
      "Average test loss: 0.003839089483850532\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06867231221993764\n",
      "Average test loss: 0.003692042925912473\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06857510726319418\n",
      "Average test loss: 0.0037399535009430517\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06853160680333774\n",
      "Average test loss: 0.0036709755058917733\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06849865272310045\n",
      "Average test loss: 0.0036479681982762285\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06829810002777312\n",
      "Average test loss: 0.00370200403738353\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06823566055297851\n",
      "Average test loss: 0.003748972153911988\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06804716443353229\n",
      "Average test loss: 0.003608449007074038\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06797322665320503\n",
      "Average test loss: 0.003726364754140377\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06797663630710708\n",
      "Average test loss: 0.0036960172777374587\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06784245404932233\n",
      "Average test loss: 0.00652359597509106\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0676922682788637\n",
      "Average test loss: 0.003770017553327812\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06777950984239578\n",
      "Average test loss: 0.003697809100151062\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06781443803840213\n",
      "Average test loss: 0.0040493541769683365\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06737367573049334\n",
      "Average test loss: 0.00404706965552436\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06742888175116644\n",
      "Average test loss: 0.0037007705486483042\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06728832574354278\n",
      "Average test loss: 0.0038890920985076164\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0672785040669971\n",
      "Average test loss: 0.003626987081848913\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06716598969697953\n",
      "Average test loss: 0.007127782143652439\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06713632975353136\n",
      "Average test loss: 0.003643832171956698\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06703683939907286\n",
      "Average test loss: 0.00370406143905388\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06707703749669923\n",
      "Average test loss: 0.0037308759519623386\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06674963605072763\n",
      "Average test loss: 0.003723806478497055\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06671318577395545\n",
      "Average test loss: 0.003661566981424888\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06673782404926087\n",
      "Average test loss: 0.0037655886529634397\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06642150511013137\n",
      "Average test loss: 0.003692287417749564\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06647479416264428\n",
      "Average test loss: 0.0038535009825395213\n",
      "Epoch 203/300\n",
      "Average training loss: 0.066528035249975\n",
      "Average test loss: 0.003845825341426664\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06644450905256802\n",
      "Average test loss: 0.003784823628349437\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06632069449623426\n",
      "Average test loss: 0.0037857404792060456\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06620758432812161\n",
      "Average test loss: 0.0037053049972487822\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06615764055980576\n",
      "Average test loss: 0.004011189807206392\n",
      "Epoch 208/300\n",
      "Average training loss: 0.0658897739648819\n",
      "Average test loss: 0.0036678023955060376\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06584101023276646\n",
      "Average test loss: 0.003735977576424678\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06583685006035699\n",
      "Average test loss: 0.0037751579969707463\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06580048124988874\n",
      "Average test loss: 0.003765370132194625\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06567281403806474\n",
      "Average test loss: 0.0036964067129625217\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06559042093488905\n",
      "Average test loss: 0.003938327128067612\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0655721108979649\n",
      "Average test loss: 0.003737675782500042\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06558029494020674\n",
      "Average test loss: 0.003813058275108536\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06537642343176736\n",
      "Average test loss: 0.003727342572270168\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06535215334428682\n",
      "Average test loss: 0.0037564997428821195\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06517599167426427\n",
      "Average test loss: 0.003803336096720563\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06515301025244925\n",
      "Average test loss: 0.003840069699411591\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06514865079190996\n",
      "Average test loss: 0.003740762555764781\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06520476540591982\n",
      "Average test loss: 0.005151483813093769\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06509173223376274\n",
      "Average test loss: 0.0038094567395746707\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06505736397372351\n",
      "Average test loss: 0.003677627881574962\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06520981087949541\n",
      "Average test loss: 0.0037759612343377536\n",
      "Epoch 225/300\n",
      "Average training loss: 0.065133372300201\n",
      "Average test loss: 0.0037378422857986555\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06464356731043922\n",
      "Average test loss: 0.003638456647180849\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06451680792702569\n",
      "Average test loss: 0.0037019579151852263\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06443730350997713\n",
      "Average test loss: 0.0038097777946127787\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06441722125477262\n",
      "Average test loss: 0.003710656414222386\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06440023357669512\n",
      "Average test loss: 0.003920816211650769\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06431734208928214\n",
      "Average test loss: 0.0037243127872546512\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06446386567089293\n",
      "Average test loss: 0.004187370508909226\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0644536995821529\n",
      "Average test loss: 0.0037687381164481244\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0642778122888671\n",
      "Average test loss: 0.003623090647988849\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06417338080538644\n",
      "Average test loss: 0.003882591142008702\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06398437116874589\n",
      "Average test loss: 0.0037835441786381934\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06410379109779994\n",
      "Average test loss: 0.004027200040717919\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06400174607502089\n",
      "Average test loss: 0.003994637636467815\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06379604388607873\n",
      "Average test loss: 0.0036445501432236698\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06382804426219728\n",
      "Average test loss: 0.005007680134226878\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0640211734076341\n",
      "Average test loss: 0.003847253984461228\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06376557311415672\n",
      "Average test loss: 0.003748645951764451\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0635588408741686\n",
      "Average test loss: 0.0038112230168448552\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06353424645794763\n",
      "Average test loss: 0.0037766991121073565\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06341069539719157\n",
      "Average test loss: 0.0037491851943648523\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06341922818952136\n",
      "Average test loss: 0.003827501301964124\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06342354432079528\n",
      "Average test loss: 0.003862051463375489\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06328710054688984\n",
      "Average test loss: 0.0037403630810893245\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06341915949185689\n",
      "Average test loss: 0.0038265613723132345\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06347532820039326\n",
      "Average test loss: 0.0038648774358961318\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06319327618181705\n",
      "Average test loss: 0.0038339360314938757\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06308290186193254\n",
      "Average test loss: 0.003775061461246676\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06302101998196707\n",
      "Average test loss: 0.0037919465862214564\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0629991053574615\n",
      "Average test loss: 0.0037724325420955816\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06299715318282445\n",
      "Average test loss: 0.0037649169862270355\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06286353129810757\n",
      "Average test loss: 0.0037249067462980746\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06294760031501452\n",
      "Average test loss: 0.0038464609252081977\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06274214141898685\n",
      "Average test loss: 0.003965911561209294\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06273418342736033\n",
      "Average test loss: 0.003804137606587675\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06263774581750234\n",
      "Average test loss: 0.003839166433033016\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06261939422951804\n",
      "Average test loss: 0.003788226339759098\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06253667334715525\n",
      "Average test loss: 0.003925757277343008\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06255074935489231\n",
      "Average test loss: 0.0038629593586342204\n",
      "Epoch 265/300\n",
      "Average training loss: 0.062372389084762996\n",
      "Average test loss: 0.0038042058443857564\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06227269705467754\n",
      "Average test loss: 0.0038010347179240653\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0625482627219624\n",
      "Average test loss: 0.004006803422338433\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06247200209564633\n",
      "Average test loss: 0.0037322078972227045\n",
      "Epoch 269/300\n",
      "Average training loss: 0.062154817991786536\n",
      "Average test loss: 0.003696814549465974\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06214251687460475\n",
      "Average test loss: 0.0037491084970533847\n",
      "Epoch 273/300\n",
      "Average training loss: 0.062014324824015296\n",
      "Average test loss: 0.003789160267967317\n",
      "Epoch 274/300\n",
      "Average training loss: 0.061878026912609733\n",
      "Average test loss: 0.003909844062394566\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06181496206919352\n",
      "Average test loss: 0.003943320019791524\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06189792340993881\n",
      "Average test loss: 0.003691723647630877\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06188666279448403\n",
      "Average test loss: 0.01388525946272744\n",
      "Epoch 279/300\n",
      "Average training loss: 0.061749677459398904\n",
      "Average test loss: 0.003776271956662337\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06155591861738099\n",
      "Average test loss: 0.0037099554079274336\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06164921826786465\n",
      "Average test loss: 0.0038906049804968965\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06149638047152095\n",
      "Average test loss: 0.0039085204675793645\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06151391231351429\n",
      "Average test loss: 0.003920127934465806\n",
      "Epoch 285/300\n",
      "Average training loss: 0.061393505023585426\n",
      "Average test loss: 0.003960656637532843\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06136871257424355\n",
      "Average test loss: 0.003820455890148878\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06133050674531195\n",
      "Average test loss: 0.0038720616242951815\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06139743423130777\n",
      "Average test loss: 0.0038617856837809086\n",
      "Epoch 289/300\n",
      "Average training loss: 0.061347166690561505\n",
      "Average test loss: 0.003812557539385226\n",
      "Epoch 290/300\n",
      "Average training loss: 0.061129678315586515\n",
      "Average test loss: 0.0037947895862162112\n",
      "Epoch 292/300\n",
      "Average training loss: 0.061196043339040544\n",
      "Average test loss: 0.00380253505313562\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06113188549876213\n",
      "Average test loss: 0.00371197559332682\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06164415589968363\n",
      "Average test loss: 0.003731548532429669\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06093279778957367\n",
      "Average test loss: 0.0037204383317795066\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06095950886276033\n",
      "Average test loss: 0.0037111682585544057\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06079936452044381\n",
      "Average test loss: 0.0038527813628315925\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06074841437074873\n",
      "Average test loss: 0.0038780607442475026\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06092383324768808\n",
      "Average test loss: 0.0037447714952545032\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 27.144621978759766\n",
      "Average test loss: 86.87069669478139\n",
      "Epoch 2/300\n",
      "Average training loss: 14.988457995096843\n",
      "Average test loss: 0.007780864156368706\n",
      "Epoch 3/300\n",
      "Average training loss: 9.797499711354574\n",
      "Average test loss: 11.204279077793162\n",
      "Epoch 4/300\n",
      "Average training loss: 7.701699939303928\n",
      "Average test loss: 0.018521841143154437\n",
      "Epoch 5/300\n",
      "Average training loss: 7.292628956688775\n",
      "Average test loss: 0.021089838390549023\n",
      "Epoch 6/300\n",
      "Average training loss: 5.525449403127035\n",
      "Average test loss: 0.02651161081240409\n",
      "Epoch 7/300\n",
      "Average training loss: 4.808541662428114\n",
      "Average test loss: 0.18312215842658447\n",
      "Epoch 8/300\n",
      "Average training loss: 3.8302913606431748\n",
      "Average test loss: 0.0037668020038141145\n",
      "Epoch 9/300\n",
      "Average training loss: 3.455920624203152\n",
      "Average test loss: 50.55777294574843\n",
      "Epoch 10/300\n",
      "Average training loss: 2.6953141456180147\n",
      "Average test loss: 0.5356656837585486\n",
      "Epoch 11/300\n",
      "Average training loss: 2.311155015733507\n",
      "Average test loss: 0.021016093437870344\n",
      "Epoch 12/300\n",
      "Average training loss: 2.135604777654012\n",
      "Average test loss: 0.6338943749335078\n",
      "Epoch 13/300\n",
      "Average training loss: 1.7760541032155355\n",
      "Average test loss: 0.0033813228884504902\n",
      "Epoch 14/300\n",
      "Average training loss: 1.4829564230177137\n",
      "Average test loss: 123.73163608637866\n",
      "Epoch 15/300\n",
      "Average training loss: 1.3391177029079862\n",
      "Average test loss: 0.006064484166602294\n",
      "Epoch 16/300\n",
      "Average training loss: 1.1435681188371447\n",
      "Average test loss: 1.5263764635487977\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0138271272977193\n",
      "Average test loss: 0.01211130078902675\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9176904017660353\n",
      "Average test loss: 0.004486051725637582\n",
      "Epoch 19/300\n",
      "Average training loss: 0.8067197224828933\n",
      "Average test loss: 0.0030898346085515287\n",
      "Epoch 20/300\n",
      "Average training loss: 0.7137018191019694\n",
      "Average test loss: 5.3363210077732806\n",
      "Epoch 21/300\n",
      "Average training loss: 0.6277582584487067\n",
      "Average test loss: 0.003093415215197537\n",
      "Epoch 22/300\n",
      "Average training loss: 0.5540771053102281\n",
      "Average test loss: 0.003104824040085077\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4276593516667684\n",
      "Average test loss: 0.0028962958123948837\n",
      "Epoch 25/300\n",
      "Average training loss: 0.37734604371918573\n",
      "Average test loss: 0.00291825397995611\n",
      "Epoch 26/300\n",
      "Average training loss: 0.33330566522810195\n",
      "Average test loss: 0.0028316880780168705\n",
      "Epoch 27/300\n",
      "Average training loss: 0.29539430214299095\n",
      "Average test loss: 0.0029038395014488036\n",
      "Epoch 28/300\n",
      "Average training loss: 0.26225214076042175\n",
      "Average test loss: 0.002897866824020942\n",
      "Epoch 29/300\n",
      "Average training loss: 0.23310219168663024\n",
      "Average test loss: 0.0034746639211144714\n",
      "Epoch 30/300\n",
      "Average training loss: 0.20846377846929762\n",
      "Average test loss: 0.0028218737749589815\n",
      "Epoch 31/300\n",
      "Average training loss: 0.186909664856063\n",
      "Average test loss: 0.0027976296250191\n",
      "Epoch 32/300\n",
      "Average test loss: 0.002765452120453119\n",
      "Epoch 33/300\n",
      "Average training loss: 0.15392226329114703\n",
      "Average test loss: 0.002810713610301415\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14114715190728505\n",
      "Average test loss: 0.0038061033375561236\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13111141492260828\n",
      "Average test loss: 0.002728716355541514\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12308340221643448\n",
      "Average test loss: 0.002865147372604244\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11644401899311278\n",
      "Average test loss: 0.002757497769676977\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11116874983575609\n",
      "Average test loss: 0.002718387281936076\n",
      "Epoch 39/300\n",
      "Average training loss: 0.10688929777012932\n",
      "Average test loss: 0.0026943063615924782\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1033643794854482\n",
      "Average test loss: 0.0027449310283280083\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1006225945684645\n",
      "Average test loss: 0.002694092470738623\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09818103928698434\n",
      "Average test loss: 0.002768367436197069\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09612922534677718\n",
      "Average test loss: 0.0026911282882922227\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09302648821141984\n",
      "Average test loss: 0.0027765945456922052\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09157630146212048\n",
      "Average test loss: 0.0027072026169755394\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09100597639878591\n",
      "Average test loss: 0.0027137186957730186\n",
      "Epoch 48/300\n",
      "Average training loss: 0.089372286412451\n",
      "Average test loss: 0.0026789862248632642\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08839842615524927\n",
      "Average test loss: 0.002664978268245856\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08745900130934185\n",
      "Average test loss: 0.002677792500394086\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08657659296194713\n",
      "Average test loss: 0.002672950614657667\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08586949732568529\n",
      "Average test loss: 0.002754729703068733\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08520437816116545\n",
      "Average test loss: 0.002665961937357982\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08402682760688994\n",
      "Average test loss: 0.002745668788337045\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08360549204879338\n",
      "Average test loss: 0.002735543925522102\n",
      "Epoch 56/300\n",
      "Average training loss: 0.0826724847290251\n",
      "Average test loss: 0.002719765398444401\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08197413681613075\n",
      "Average test loss: 0.0027018987903785372\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08132124896181954\n",
      "Average test loss: 0.0026936817256112892\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08059727404514949\n",
      "Average test loss: 0.0026779435640200975\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07979670871628655\n",
      "Average test loss: 0.0027752889167103503\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07851780884133445\n",
      "Average test loss: 0.0026901177521795033\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07830149688985613\n",
      "Average test loss: 0.002781040415374769\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07717324321137534\n",
      "Average test loss: 0.0027270782250497076\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07663397526741028\n",
      "Average test loss: 0.002787065863609314\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07624920454952452\n",
      "Average test loss: 0.002734167076336841\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07568950194451544\n",
      "Average test loss: 0.002708455565489001\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07478719966279136\n",
      "Average test loss: 0.0027609222036682896\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07402209254105886\n",
      "Average test loss: 0.0029318907910750973\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07364603859848447\n",
      "Average test loss: 0.0028249540709786945\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07309715774324205\n",
      "Average test loss: 0.0027903497494343255\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07243742240799798\n",
      "Average test loss: 0.002704245339665148\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07199037398232354\n",
      "Average test loss: 0.0027536742796914447\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07172824980815251\n",
      "Average test loss: 0.0027425004763321744\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07093201054467095\n",
      "Average test loss: 0.0027906897188060813\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07039545185036129\n",
      "Average test loss: 0.002805974336134063\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07014078127013312\n",
      "Average test loss: 0.002767423298623827\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06961274976200528\n",
      "Average test loss: 0.0027123923450708388\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0693839883406957\n",
      "Average test loss: 0.0027734462970660794\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06873986274003982\n",
      "Average test loss: 0.0028169976661188735\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06845922094583512\n",
      "Average test loss: 0.0036113503918879563\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06816773113939498\n",
      "Average test loss: 0.0033257316570315097\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06756159007549285\n",
      "Average test loss: 0.0028112472881459526\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0672781086034245\n",
      "Average test loss: 0.0027878574962831204\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0669889730811119\n",
      "Average test loss: 0.002869881516529454\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06651379672355122\n",
      "Average test loss: 0.002824273243960407\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06619751335846054\n",
      "Average test loss: 0.002768156015417642\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06610647164450752\n",
      "Average test loss: 0.003796486728514234\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06558875088890394\n",
      "Average test loss: 0.0029110007755872276\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06523674822515911\n",
      "Average test loss: 0.0028418378703710108\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0648407107922766\n",
      "Average test loss: 0.0028220613346331648\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06460692758692635\n",
      "Average test loss: 0.002846317006688979\n",
      "Epoch 97/300\n",
      "Average training loss: 0.06446614442269008\n",
      "Average test loss: 0.002888904301242696\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06425306522846222\n",
      "Average test loss: 0.0028437292281952168\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0638202037513256\n",
      "Average test loss: 0.002851713897039493\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06370948639843199\n",
      "Average test loss: 0.00281399922279848\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06361834724081887\n",
      "Average test loss: 0.002920365047744579\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06320172567168872\n",
      "Average test loss: 0.0027945289831194614\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06302550362216101\n",
      "Average test loss: 0.002877308207253615\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06290986119045151\n",
      "Average test loss: 0.002860346677299175\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06266913645135032\n",
      "Average test loss: 0.002828656753939059\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06229829332563612\n",
      "Average test loss: 0.0029379387686236037\n",
      "Epoch 107/300\n",
      "Average training loss: 0.06190853086445067\n",
      "Average test loss: 0.0028214654345065353\n",
      "Epoch 109/300\n",
      "Average training loss: 0.061880569458007816\n",
      "Average test loss: 0.0028072267478952803\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06172109373410543\n",
      "Average test loss: 0.002853308876355489\n",
      "Epoch 111/300\n",
      "Average training loss: 0.061526539670096504\n",
      "Average test loss: 0.009816840074542496\n",
      "Epoch 112/300\n",
      "Average training loss: 0.061408242477311026\n",
      "Average test loss: 0.0027996003302849003\n",
      "Epoch 113/300\n",
      "Average training loss: 0.06098321275247468\n",
      "Average test loss: 0.0028853778013338644\n",
      "Epoch 114/300\n",
      "Average training loss: 0.060804497165812384\n",
      "Average test loss: 0.0028562895694954527\n",
      "Epoch 115/300\n",
      "Average training loss: 0.06075736835267809\n",
      "Average test loss: 0.0028677222821861504\n",
      "Epoch 116/300\n",
      "Average training loss: 0.060560021844175124\n",
      "Average test loss: 0.002839668634865019\n",
      "Epoch 117/300\n",
      "Average training loss: 0.060396422661013076\n",
      "Average test loss: 0.0029099979628291394\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06024400137530433\n",
      "Average test loss: 0.00294474155931837\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06018506233228577\n",
      "Average test loss: 0.002910242291995221\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05996651385890113\n",
      "Average test loss: 0.0029516563804613217\n",
      "Epoch 121/300\n",
      "Average training loss: 0.059854136837853324\n",
      "Average test loss: 0.0028895793449547555\n",
      "Epoch 122/300\n",
      "Average training loss: 0.059453536099857755\n",
      "Average test loss: 0.0028397040703437393\n",
      "Epoch 124/300\n",
      "Average training loss: 0.059305431796444784\n",
      "Average test loss: 0.0029152285837464862\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05915307811896006\n",
      "Average test loss: 0.0027863580389983125\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05908554042047925\n",
      "Average test loss: 0.0028786568832066322\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05883357483148575\n",
      "Average test loss: 0.003008176187467244\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05871272709634569\n",
      "Average test loss: 0.0028804124330894815\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05833041630850898\n",
      "Average test loss: 0.0029211582572509845\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05832210719254282\n",
      "Average test loss: 0.0029819865324017076\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05826931560701794\n",
      "Average test loss: 0.0030998883578512403\n",
      "Epoch 133/300\n",
      "Average training loss: 0.058051077365875246\n",
      "Average test loss: 0.0037199781102438766\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05787295365333557\n",
      "Average test loss: 0.0029311306540750794\n",
      "Epoch 135/300\n",
      "Average training loss: 0.057777211480670505\n",
      "Average training loss: 0.057601314160558914\n",
      "Average test loss: 0.002935960617951221\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05769538936018944\n",
      "Average test loss: 0.0029666568144328066\n",
      "Epoch 138/300\n",
      "Average training loss: 0.057603163762225046\n",
      "Average test loss: 0.002946399002853367\n",
      "Epoch 139/300\n",
      "Average training loss: 0.057226856771442625\n",
      "Average test loss: 0.0028574727254195346\n",
      "Epoch 141/300\n",
      "Average training loss: 0.057055455760823354\n",
      "Average test loss: 0.00300944357448154\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05697261748048994\n",
      "Average test loss: 0.00292261453717947\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05690838299194972\n",
      "Average test loss: 0.0029448269963678387\n",
      "Epoch 145/300\n",
      "Average training loss: 0.056564968360794915\n",
      "Average test loss: 0.0029863125952995485\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05636469767159886\n",
      "Average test loss: 0.0029248454201345644\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05640481158428722\n",
      "Average test loss: 0.003275696440392898\n",
      "Epoch 149/300\n",
      "Average training loss: 0.056170258783631855\n",
      "Average test loss: 0.013876564467946688\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0561185321741634\n",
      "Average test loss: 0.0030315695945173503\n",
      "Epoch 151/300\n",
      "Average training loss: 0.056007318175501294\n",
      "Average test loss: 0.0029214056314279635\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05600451098548041\n",
      "Average test loss: 0.003994488111386697\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05599042139450709\n",
      "Average test loss: 0.003052848323351807\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05577247043285105\n",
      "Average test loss: 0.002978348311347266\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05553472540775935\n",
      "Average test loss: 0.002908977409410808\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0556200081606706\n",
      "Average test loss: 0.002953419441978137\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05549186404214965\n",
      "Average test loss: 0.0031758620016690757\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05540841583742036\n",
      "Average test loss: 0.003015539896157053\n",
      "Epoch 159/300\n",
      "Average training loss: 0.055291152712371616\n",
      "Average test loss: 0.0028777222335338594\n",
      "Epoch 160/300\n",
      "Average training loss: 0.055222607105970385\n",
      "Average test loss: 0.002998141416451997\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05505661557780372\n",
      "Average test loss: 0.0029976255454950865\n",
      "Epoch 162/300\n",
      "Average training loss: 0.054942671696345015\n",
      "Average test loss: 0.003026331519914998\n",
      "Epoch 163/300\n",
      "Average training loss: 0.054975433574782476\n",
      "Average test loss: 0.0029206859483900996\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05485904422071245\n",
      "Average test loss: 0.0029684256642229026\n",
      "Epoch 165/300\n",
      "Average training loss: 0.054769958866967096\n",
      "Average test loss: 0.0029952652401601275\n",
      "Epoch 166/300\n",
      "Average training loss: 0.0547104814781083\n",
      "Average test loss: 0.0029698235533303684\n",
      "Epoch 167/300\n",
      "Average training loss: 0.054639680627319547\n",
      "Average test loss: 0.0029986642681890064\n",
      "Epoch 168/300\n",
      "Average training loss: 0.054533142854770025\n",
      "Average test loss: 0.003054148956719372\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05443262562486861\n",
      "Average test loss: 0.003074187685839004\n",
      "Epoch 170/300\n",
      "Average training loss: 0.054361688706609936\n",
      "Average test loss: 0.0030568423722353247\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05433636557062467\n",
      "Average test loss: 0.002936858983296487\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05419220682647493\n",
      "Average test loss: 0.0029830872842835057\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05419448259803984\n",
      "Average test loss: 0.0030436509328169957\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05410983854532242\n",
      "Average test loss: 0.002917377398038904\n",
      "Epoch 175/300\n",
      "Average training loss: 0.05400676756103834\n",
      "Average test loss: 0.0029641885028945077\n",
      "Epoch 176/300\n",
      "Average training loss: 0.053849814514319104\n",
      "Average test loss: 0.00333530610634221\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05384082690212462\n",
      "Average test loss: 0.003021846320066187\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05380448625816239\n",
      "Average test loss: 0.0030598404086712334\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0536945427093241\n",
      "Average test loss: 0.003015353124899169\n",
      "Epoch 180/300\n",
      "Average training loss: 0.053574312292867236\n",
      "Average test loss: 0.002941418812299768\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05354692330956459\n",
      "Average test loss: 0.0029229438176585566\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05346516473094622\n",
      "Average test loss: 0.002944149306664864\n",
      "Epoch 183/300\n",
      "Average training loss: 0.053420605172713596\n",
      "Average test loss: 0.0029956386681232188\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05336361069149441\n",
      "Average test loss: 0.0030852428670558665\n",
      "Epoch 185/300\n",
      "Average training loss: 0.053255822751257155\n",
      "Average test loss: 0.0029665051591065195\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0532514447040028\n",
      "Average test loss: 0.00577272966876626\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05309772934516271\n",
      "Average test loss: 0.0031449022597322862\n",
      "Epoch 188/300\n",
      "Average training loss: 0.053168515940507255\n",
      "Average test loss: 0.0029000287623041207\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05293763248456849\n",
      "Average test loss: 0.003164304355159402\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05289892532428106\n",
      "Average test loss: 0.0029619189471834237\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05296110676063431\n",
      "Average test loss: 0.0033590628573050103\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05283278167247772\n",
      "Average test loss: 0.0030536272794836096\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05283921757340431\n",
      "Average test loss: 0.003015975120580859\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05262455632289251\n",
      "Average test loss: 0.0030233690519299774\n",
      "Epoch 195/300\n",
      "Average training loss: 0.052632348226176365\n",
      "Average test loss: 0.0029277040669694544\n",
      "Epoch 196/300\n",
      "Average training loss: 0.052665597332848445\n",
      "Average test loss: 0.00313702176614768\n",
      "Epoch 197/300\n",
      "Average training loss: 0.052454830762412814\n",
      "Average test loss: 0.003700802514122592\n",
      "Epoch 198/300\n",
      "Average training loss: 0.052458116951915955\n",
      "Average test loss: 0.0030306522241897054\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05236434390809801\n",
      "Average test loss: 0.003048200708710485\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05231098004513317\n",
      "Average test loss: 0.003021183939029773\n",
      "Epoch 201/300\n",
      "Average training loss: 0.052348376386695435\n",
      "Average test loss: 0.002989633796736598\n",
      "Epoch 202/300\n",
      "Average training loss: 0.052131710095538034\n",
      "Average test loss: 0.0031413418509893946\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05217429435253143\n",
      "Average test loss: 0.0038545870627794\n",
      "Epoch 204/300\n",
      "Average training loss: 0.052116971092091666\n",
      "Average test loss: 0.0030824562737511263\n",
      "Epoch 205/300\n",
      "Average training loss: 0.052005686511596046\n",
      "Average test loss: 0.0029845442573229472\n",
      "Epoch 206/300\n",
      "Average training loss: 0.052002916004922656\n",
      "Average test loss: 0.003002046692288584\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05194264169865184\n",
      "Average test loss: 0.014764655765146018\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05172789788246155\n",
      "Average test loss: 0.004127863872175415\n",
      "Epoch 209/300\n",
      "Average training loss: 0.051737888157367705\n",
      "Average test loss: 0.003058586827168862\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05175172646840413\n",
      "Average test loss: 0.00309148813618554\n",
      "Epoch 211/300\n",
      "Average training loss: 0.051669150084257126\n",
      "Average test loss: 0.0032407496319048937\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05162531014945772\n",
      "Average test loss: 0.003045842751239737\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05154692843225267\n",
      "Average test loss: 0.0029774846132430764\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05152966958615515\n",
      "Average test loss: 0.0029359204192749328\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05146948358416557\n",
      "Average test loss: 0.0031764684720999663\n",
      "Epoch 216/300\n",
      "Average training loss: 0.051432739151848685\n",
      "Average test loss: 0.0030866319739984143\n",
      "Epoch 217/300\n",
      "Average training loss: 0.051531843576166365\n",
      "Average test loss: 0.0029741685423586104\n",
      "Epoch 218/300\n",
      "Average training loss: 0.051372615271144444\n",
      "Average test loss: 0.003302379566555222\n",
      "Epoch 219/300\n",
      "Average training loss: 0.051202400363153884\n",
      "Average test loss: 0.00298114810925391\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05117799767520693\n",
      "Average test loss: 0.0030306040462520388\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05113098617725902\n",
      "Average test loss: 0.003043253232414524\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05112185897429784\n",
      "Average test loss: 0.027202832156792284\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05104855130116145\n",
      "Average test loss: 0.00308800033500625\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05100723155008422\n",
      "Average test loss: 0.003402242780145672\n",
      "Epoch 225/300\n",
      "Average training loss: 0.050905291597048444\n",
      "Average test loss: 0.0030270501606994207\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05091589678658379\n",
      "Average test loss: 0.003147500907795297\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05090248930454254\n",
      "Average test loss: 0.008820567519921396\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05074182199438413\n",
      "Average test loss: 0.0030971746432284513\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05083108931779862\n",
      "Average test loss: 0.0031393738283465307\n",
      "Epoch 230/300\n",
      "Average training loss: 0.050680844386418664\n",
      "Average test loss: 0.0031517773057437607\n",
      "Epoch 231/300\n",
      "Average training loss: 0.050563159856531356\n",
      "Average test loss: 0.00303867773587505\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05080948525004917\n",
      "Average test loss: 0.003106922225198812\n",
      "Epoch 233/300\n",
      "Average training loss: 0.050549114710754815\n",
      "Average test loss: 0.003104636859148741\n",
      "Epoch 234/300\n",
      "Average training loss: 0.050552868750360275\n",
      "Average test loss: 0.0035571431283735566\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05047585477762752\n",
      "Average test loss: 0.004662478441993395\n",
      "Epoch 236/300\n",
      "Average training loss: 0.050476596537563534\n",
      "Average test loss: 0.0031230198639548485\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05042297067907121\n",
      "Average test loss: 0.0030215729255643154\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05036274560954836\n",
      "Average test loss: 0.002989745040734609\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05029883701602618\n",
      "Average test loss: 0.0029721983251058395\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0503681594464514\n",
      "Average test loss: 0.0029791321895188756\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05015437472528882\n",
      "Average test loss: 0.0030301534425881175\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05017553111579683\n",
      "Average test loss: 0.008856005926099088\n",
      "Epoch 243/300\n",
      "Average training loss: 0.050167127781444124\n",
      "Average test loss: 0.004054375284040968\n",
      "Epoch 244/300\n",
      "Average training loss: 0.050098553011814755\n",
      "Average test loss: 0.003033120958755414\n",
      "Epoch 245/300\n",
      "Average training loss: 0.050063472121953966\n",
      "Average test loss: 0.003230132842850354\n",
      "Epoch 246/300\n",
      "Average training loss: 0.049974811775816814\n",
      "Average test loss: 0.0029856507211095755\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04993301162454817\n",
      "Average test loss: 0.003050394658413198\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05001262522737185\n",
      "Average test loss: 0.0034491637792024347\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04997680372993151\n",
      "Average test loss: 0.0031742679137322636\n",
      "Epoch 250/300\n",
      "Average training loss: 0.049833248135116366\n",
      "Average test loss: 0.003167240560054779\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04988714044292768\n",
      "Average test loss: 0.0030015605609450074\n",
      "Epoch 252/300\n",
      "Average training loss: 0.049820137709379195\n",
      "Average test loss: 0.0031716197019235957\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04971946559680833\n",
      "Average test loss: 0.00316426867308716\n",
      "Epoch 254/300\n",
      "Average training loss: 0.049659939504332014\n",
      "Average test loss: 0.0031598352427697845\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04964003309607506\n",
      "Average test loss: 0.003130426683773597\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04962348592281342\n",
      "Average test loss: 0.003811890362037553\n",
      "Epoch 257/300\n",
      "Average training loss: 0.049538230899307464\n",
      "Average test loss: 0.0030843054708093406\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04960629480746057\n",
      "Average test loss: 0.003119726974517107\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04950740916199154\n",
      "Average test loss: 0.0031394576786292925\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04952449432677693\n",
      "Average test loss: 0.003109091856930819\n",
      "Epoch 261/300\n",
      "Average training loss: 0.049436296383539836\n",
      "Average test loss: 0.003055875686307748\n",
      "Epoch 262/300\n",
      "Average training loss: 0.049363406303856106\n",
      "Average test loss: 0.003072019503969285\n",
      "Epoch 263/300\n",
      "Average training loss: 0.049295958499113716\n",
      "Average test loss: 0.0029548808510104817\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04929197049472067\n",
      "Average test loss: 0.003907443165986075\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04934588454498185\n",
      "Average test loss: 0.0031626138581583897\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04929753310812844\n",
      "Average test loss: 0.003109108330267999\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04928169523676237\n",
      "Average test loss: 0.0030339796921859184\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04920481617914306\n",
      "Average test loss: 0.0030152124903268284\n",
      "Epoch 270/300\n",
      "Average training loss: 0.049050494730472564\n",
      "Average test loss: 0.003019034354430106\n",
      "Epoch 271/300\n",
      "Average training loss: 0.049056844237777925\n",
      "Average test loss: 0.01309986434193949\n",
      "Epoch 272/300\n",
      "Average training loss: 0.048934378070963756\n",
      "Average test loss: 0.002999106454352538\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04889818286895752\n",
      "Average test loss: 0.0031011675062278905\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04898482060432434\n",
      "Average test loss: 0.00858189562211434\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04895165379842122\n",
      "Average training loss: 0.04894519778092702\n",
      "Average test loss: 0.003022187258841263\n",
      "Epoch 277/300\n",
      "Average training loss: 0.048966914465030036\n",
      "Average test loss: 0.0029854894563969636\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04881852561235428\n",
      "Average test loss: 0.003006793290997545\n",
      "Epoch 279/300\n",
      "Average training loss: 0.048789788044161266\n",
      "Average test loss: 0.003181602944102552\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04869584146142006\n",
      "Average test loss: 0.0036016032008661164\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04879934265547328\n",
      "Average test loss: 0.0030815688897338177\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04867983046174049\n",
      "Average training loss: 0.0486483003033532\n",
      "Average test loss: 0.003150645994891723\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04864465037319395\n",
      "Average test loss: 0.003189070929462711\n",
      "Epoch 285/300\n",
      "Average training loss: 0.048524504486057496\n",
      "Average test loss: 0.003791670293650693\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04865044027235773\n",
      "Average test loss: 0.0029792261336826615\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04854323316613833\n",
      "Average test loss: 0.003103366884506411\n",
      "Epoch 288/300\n",
      "Average training loss: 0.048467620922459494\n",
      "Average test loss: 0.0030356392823159693\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04839566986759504\n",
      "Average test loss: 0.0031428131183816326\n",
      "Epoch 290/300\n",
      "Average test loss: 0.0031310164967758786\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04843244661225213\n",
      "Average test loss: 0.0029986571903444\n",
      "Epoch 292/300\n",
      "Average training loss: 0.048403994851642186\n",
      "Average test loss: 0.0031525682388908335\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04831875627239545\n",
      "Average test loss: 0.003027158073667023\n",
      "Epoch 295/300\n",
      "Average training loss: 0.048308897458844716\n",
      "Average test loss: 0.003058354795599977\n",
      "Epoch 296/300\n",
      "Average training loss: 0.048248746666643354\n",
      "Average test loss: 0.003117627047623197\n",
      "Epoch 297/300\n",
      "Average training loss: 0.048230635907914904\n",
      "Average test loss: 0.003571099252336555\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04811447678671943\n",
      "Average test loss: 0.003670001842081547\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04809495487146907\n",
      "Average test loss: 0.003022465066570375\n",
      "Epoch 300/300\n",
      "Average test loss: 0.003109961065153281\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 27.844230868869356\n",
      "Average test loss: 268340.95274561463\n",
      "Epoch 2/300\n",
      "Average training loss: 16.248448716905383\n",
      "Average test loss: 259.47358221248123\n",
      "Epoch 3/300\n",
      "Average training loss: 10.767680622524685\n",
      "Average test loss: 2.175764134797785\n",
      "Epoch 4/300\n",
      "Average training loss: 9.290773046705459\n",
      "Average test loss: 0.8676719285407\n",
      "Epoch 5/300\n",
      "Average training loss: 8.31189517381456\n",
      "Average test loss: 0.010129267059266566\n",
      "Epoch 6/300\n",
      "Average training loss: 7.33190226787991\n",
      "Average test loss: 16.143235778540372\n",
      "Epoch 7/300\n",
      "Average training loss: 6.77364566760593\n",
      "Average test loss: 81.73238608664937\n",
      "Epoch 8/300\n",
      "Average training loss: 5.353149773067898\n",
      "Average test loss: 192.44026067887577\n",
      "Epoch 9/300\n",
      "Average training loss: 4.179082005818685\n",
      "Average test loss: 0.0033852990292426613\n",
      "Epoch 10/300\n",
      "Average training loss: 3.8104003950754803\n",
      "Average test loss: 0.0028941043306969934\n",
      "Epoch 11/300\n",
      "Average training loss: 3.3954409124586316\n",
      "Average test loss: 0.02028323087406655\n",
      "Epoch 12/300\n",
      "Average training loss: 3.050923203362359\n",
      "Average test loss: 0.3442601271766341\n",
      "Epoch 13/300\n",
      "Average training loss: 2.550804417504205\n",
      "Average test loss: 7.983514206175175\n",
      "Epoch 14/300\n",
      "Average training loss: 1.9627194670571222\n",
      "Average test loss: 4667.269867815064\n",
      "Epoch 16/300\n",
      "Average training loss: 1.6450868044959175\n",
      "Average test loss: 0.1693236051723361\n",
      "Epoch 17/300\n",
      "Average training loss: 1.4468929541905722\n",
      "Average test loss: 11.692117544888623\n",
      "Epoch 18/300\n",
      "Average training loss: 1.246034357600742\n",
      "Average test loss: 0.021958501731149025\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0992290311389499\n",
      "Average test loss: 0.004639704385565387\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9448348985777961\n",
      "Average test loss: 69.63011429329548\n",
      "Epoch 21/300\n",
      "Average training loss: 0.7886284836133322\n",
      "Average test loss: 0.024129826872092155\n",
      "Epoch 22/300\n",
      "Average training loss: 0.6527618246078492\n",
      "Average test loss: 0.0068916941388613645\n",
      "Epoch 23/300\n",
      "Average training loss: 0.5512419177691141\n",
      "Average test loss: 0.019350959939468237\n",
      "Epoch 24/300\n",
      "Average training loss: 0.4700627922481961\n",
      "Average test loss: 0.005008269659760926\n",
      "Epoch 25/300\n",
      "Average training loss: 0.40566655593448214\n",
      "Average test loss: 0.003700090828869078\n",
      "Epoch 26/300\n",
      "Average training loss: 0.3513716955184937\n",
      "Average test loss: 0.0022904812658412588\n",
      "Epoch 27/300\n",
      "Average training loss: 0.3061563204924266\n",
      "Average test loss: 0.0022728780422152746\n",
      "Epoch 28/300\n",
      "Average training loss: 0.2681687140199873\n",
      "Average test loss: 0.021633327282965183\n",
      "Epoch 29/300\n",
      "Average training loss: 0.23515977860821619\n",
      "Average test loss: 0.002609855319890711\n",
      "Epoch 30/300\n",
      "Average training loss: 0.20777827542357974\n",
      "Average test loss: 0.004980597294246157\n",
      "Epoch 31/300\n",
      "Average training loss: 0.18422186256779566\n",
      "Average test loss: 0.006625081915822294\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1650669904285007\n",
      "Average test loss: 0.0022176000155094597\n",
      "Epoch 33/300\n",
      "Average training loss: 0.14873457066218057\n",
      "Average test loss: 0.004598754205223586\n",
      "Epoch 34/300\n",
      "Average training loss: 0.13550503753291235\n",
      "Average test loss: 0.0024201661226236158\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12505314853456284\n",
      "Average test loss: 0.002170043083218237\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11611715812153287\n",
      "Average test loss: 0.0021715762397895256\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10936565098497603\n",
      "Average test loss: 0.002210663458953301\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10304596462514666\n",
      "Average test loss: 0.002168769595730636\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09818541218174828\n",
      "Average test loss: 0.0021478636471761597\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09393938055303362\n",
      "Average test loss: 0.0021636363652845223\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09067506535847981\n",
      "Average test loss: 0.002148558285915189\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08765205898549822\n",
      "Average test loss: 0.002157141215892302\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0856091689268748\n",
      "Average test loss: 0.00286101758831905\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08323141276836395\n",
      "Average test loss: 0.002130489636523028\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08129950077003903\n",
      "Average test loss: 0.0021431655569208994\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07964808577299118\n",
      "Average test loss: 0.002122172427467174\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0781927027967241\n",
      "Average test loss: 0.0033605796487794983\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0767466283639272\n",
      "Average test loss: 0.0021506610717624427\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07569484780894385\n",
      "Average test loss: 0.002201249048527744\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07450328730212318\n",
      "Average test loss: 0.002128288201159901\n",
      "Epoch 51/300\n",
      "Average training loss: 0.07334976594315634\n",
      "Average test loss: 0.0021371770285897786\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07233238330483437\n",
      "Average test loss: 0.002281501738871965\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07156343015697267\n",
      "Average test loss: 0.0021391056708784566\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07054834600951937\n",
      "Average test loss: 0.002126264309614069\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06962562789519627\n",
      "Average test loss: 0.002149517246418529\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06891834242476358\n",
      "Average test loss: 0.002212620653419031\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06823229188720385\n",
      "Average test loss: 0.002154377272249096\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06735536328951518\n",
      "Average test loss: 0.0021201635632250044\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06675842310322655\n",
      "Average test loss: 0.0022419012751844192\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06614993998739455\n",
      "Average test loss: 0.002181020349678066\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06515527914630043\n",
      "Average test loss: 0.0021285965845195785\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06466886522372564\n",
      "Average test loss: 0.0021210078147964346\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06389236309130987\n",
      "Average test loss: 0.0021679557270059984\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06344189458754328\n",
      "Average test loss: 0.00214316603106757\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06308344447612763\n",
      "Average test loss: 0.0022261392063357764\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06228781950804922\n",
      "Average test loss: 0.0029769595708284114\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06165087192919519\n",
      "Average test loss: 0.002161438846650223\n",
      "Epoch 68/300\n",
      "Average training loss: 0.061210992134279676\n",
      "Average test loss: 0.002414270166721609\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06079507289330165\n",
      "Average test loss: 0.002204472829484277\n",
      "Epoch 70/300\n",
      "Average training loss: 0.060267601827780404\n",
      "Average test loss: 0.0021490818932652475\n",
      "Epoch 71/300\n",
      "Average training loss: 0.059640675647391214\n",
      "Average test loss: 0.0021980031815667946\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05950613656308916\n",
      "Average test loss: 0.0022547185251282323\n",
      "Epoch 73/300\n",
      "Average training loss: 0.059100782596402696\n",
      "Average test loss: 0.002381089466934403\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05839198990662893\n",
      "Average test loss: 0.0021975640235468745\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05788527928127183\n",
      "Average test loss: 0.002217484030044741\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05750584860311614\n",
      "Average test loss: 0.0022413487376438247\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05717004973358578\n",
      "Average test loss: 0.0021562571815318533\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05663542020320892\n",
      "Average test loss: 0.0022393360394570563\n",
      "Epoch 79/300\n",
      "Average training loss: 0.056324255615472794\n",
      "Average test loss: 0.0023194230796976223\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05603717364867528\n",
      "Average test loss: 0.0033946023119820487\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0555883481502533\n",
      "Average test loss: 0.0022313689187996918\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05538168709973494\n",
      "Average test loss: 0.002232394010035528\n",
      "Epoch 83/300\n",
      "Average training loss: 0.055062575280666354\n",
      "Average test loss: 0.0021864394816673463\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05462582736876276\n",
      "Average test loss: 0.0022601629574265747\n",
      "Epoch 85/300\n",
      "Average training loss: 0.054448532630999885\n",
      "Average test loss: 0.011049108372794257\n",
      "Epoch 86/300\n",
      "Average training loss: 0.054283071729871964\n",
      "Average test loss: 0.002188783343260487\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05373452273342345\n",
      "Average test loss: 0.002218923125002119\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05338358692990409\n",
      "Average test loss: 0.0021929955050970116\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05324843339787589\n",
      "Average test loss: 0.002208325778858529\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05290683106250233\n",
      "Average test loss: 0.005540619850158692\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05260484475228522\n",
      "Average test loss: 0.002227345006747378\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0523840454518795\n",
      "Average test loss: 0.0022413982613426115\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05248299514253934\n",
      "Average test loss: 0.0032425810700903337\n",
      "Epoch 94/300\n",
      "Average training loss: 0.052095323198371464\n",
      "Average test loss: 0.0022829421325069334\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05161111722389857\n",
      "Average test loss: 0.0023133099319206343\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05142612887422244\n",
      "Average test loss: 0.0023474617534213595\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05126248499751091\n",
      "Average test loss: 0.0022805083398189808\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0511309483912256\n",
      "Average test loss: 0.002311535989244779\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05092280157075988\n",
      "Average test loss: 0.0022246561001779304\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05074294243587388\n",
      "Average test loss: 0.0022451751296304993\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05036520710587501\n",
      "Average test loss: 0.00225357749633905\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05035909832186169\n",
      "Average test loss: 0.00233525276535915\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05017486698097653\n",
      "Average test loss: 0.00222436716593802\n",
      "Epoch 104/300\n",
      "Average training loss: 0.049929960131645204\n",
      "Average test loss: 0.0022916964882363877\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04969882059428427\n",
      "Average test loss: 0.002303150465504991\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04951487381921874\n",
      "Average test loss: 0.0022410306370713646\n",
      "Epoch 107/300\n",
      "Average training loss: 0.04940735290779008\n",
      "Average test loss: 0.00232805922979282\n",
      "Epoch 108/300\n",
      "Average training loss: 0.049213325954145855\n",
      "Average test loss: 0.002268788089768754\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04897788781589932\n",
      "Average test loss: 0.00229312365108894\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04875885969069269\n",
      "Average test loss: 0.0022616052043934664\n",
      "Epoch 112/300\n",
      "Average training loss: 0.048625337931844925\n",
      "Average test loss: 0.0023262546692664424\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04853326172298855\n",
      "Average test loss: 0.002260690596368578\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0482646093931463\n",
      "Average test loss: 0.002391216425432099\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04817921744121446\n",
      "Average test loss: 0.0022630613800138236\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04804429370827145\n",
      "Average test loss: 0.0023282567477888533\n",
      "Epoch 117/300\n",
      "Average training loss: 0.047775822483830985\n",
      "Average test loss: 0.0023126909769036704\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04762641262345844\n",
      "Average test loss: 0.0024499396844249633\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04751446371277173\n",
      "Average test loss: 0.002317428179292215\n",
      "Epoch 121/300\n",
      "Average training loss: 0.047419960810078515\n",
      "Average test loss: 0.0023278493754979638\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04726009639766481\n",
      "Average test loss: 0.0023158510600527126\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04724341033564673\n",
      "Average test loss: 0.0023691461961716414\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04687167062693172\n",
      "Average test loss: 0.002324096529227164\n",
      "Epoch 126/300\n",
      "Average training loss: 0.047017131434546576\n",
      "Average test loss: 0.0023822824499673315\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04682184346848064\n",
      "Average test loss: 0.0023382823831505244\n",
      "Epoch 128/300\n",
      "Average training loss: 0.046561672452423306\n",
      "Average test loss: 0.0023395084065074723\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04657008629375034\n",
      "Average test loss: 0.002279914676108294\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04643020079533259\n",
      "Average test loss: 0.002367276181777318\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04620252745350202\n",
      "Average test loss: 0.006775144265136785\n",
      "Epoch 133/300\n",
      "Average training loss: 0.046141970929172306\n",
      "Average test loss: 0.002331708454940882\n",
      "Epoch 134/300\n",
      "Average training loss: 0.046011641008986366\n",
      "Average test loss: 0.0023728993468814428\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0459631489680873\n",
      "Average test loss: 0.0023084676700333754\n",
      "Epoch 136/300\n",
      "Average training loss: 0.04585249846511417\n",
      "Average test loss: 0.0023383656117237275\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04568734495176209\n",
      "Average test loss: 0.002481324225457178\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04558169266912672\n",
      "Average test loss: 0.0023729373500165013\n",
      "Epoch 140/300\n",
      "Average training loss: 0.045517897990014815\n",
      "Average test loss: 0.0026175240486239395\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04536085374818908\n",
      "Average test loss: 0.002361723642382357\n",
      "Epoch 142/300\n",
      "Average training loss: 0.045313109384642704\n",
      "Average test loss: 0.0023528167154226037\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04529641925626331\n",
      "Average test loss: 0.00231006742351585\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04499564721518093\n",
      "Average test loss: 0.005011102630860275\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04507958865165711\n",
      "Average test loss: 0.0025193448627574577\n",
      "Epoch 147/300\n",
      "Average training loss: 0.044859746313757366\n",
      "Average test loss: 0.002380620585961474\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04476078206631873\n",
      "Average test loss: 0.0023199484578023356\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04479332633150948\n",
      "Average test loss: 0.002373510419287615\n",
      "Epoch 150/300\n",
      "Average training loss: 0.044786674085590575\n",
      "Average test loss: 0.002898259745600323\n",
      "Epoch 151/300\n",
      "Average training loss: 0.044476755486594306\n",
      "Average test loss: 0.0024436174190292756\n",
      "Epoch 153/300\n",
      "Average training loss: 0.044368926988707647\n",
      "Average test loss: 0.0024183488755176465\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04420895539720853\n",
      "Average test loss: 0.002616747951135039\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04435988500052028\n",
      "Average test loss: 0.0023521152179067336\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04421314431230227\n",
      "Average test loss: 0.0023242753348830672\n",
      "Epoch 158/300\n",
      "Average training loss: 0.044181774139404294\n",
      "Average test loss: 0.008158699311729934\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04395561675561799\n",
      "Average test loss: 0.00245141574078136\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04389863285753462\n",
      "Average test loss: 0.002390546666458249\n",
      "Epoch 161/300\n",
      "Average training loss: 0.043942092690202925\n",
      "Average test loss: 0.0024544280310057932\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04382092947761218\n",
      "Average test loss: 0.002290110667753551\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04379217566880915\n",
      "Average test loss: 0.003700235206633806\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04365389820933342\n",
      "Average test loss: 0.01127987036212451\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04364503931336933\n",
      "Average test loss: 0.002339463960379362\n",
      "Epoch 166/300\n",
      "Average training loss: 0.043561242775784596\n",
      "Average test loss: 0.0024160667046283684\n",
      "Epoch 167/300\n",
      "Average training loss: 0.043581222285827004\n",
      "Average test loss: 0.0023425365827149814\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0434445733792252\n",
      "Average test loss: 0.0023342002951022654\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04345208164718416\n",
      "Average test loss: 0.002363237729089128\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04331481276286973\n",
      "Average test loss: 0.0023623676291770405\n",
      "Epoch 171/300\n",
      "Average training loss: 0.043241610195901656\n",
      "Average test loss: 0.0023590598127080334\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04331528091099527\n",
      "Average test loss: 0.0023229544241395264\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04329263812303543\n",
      "Average test loss: 0.002447816460082928\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04301259337862333\n",
      "Average test loss: 0.002566691213597854\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04298707950115204\n",
      "Average test loss: 0.0025169004543373983\n",
      "Epoch 176/300\n",
      "Average training loss: 0.042989490856726965\n",
      "Average test loss: 0.0023780800650517147\n",
      "Epoch 177/300\n",
      "Average training loss: 0.042970408444603286\n",
      "Average test loss: 0.0023909429584940273\n",
      "Epoch 178/300\n",
      "Average training loss: 0.042957941244045894\n",
      "Average test loss: 0.0024028457357651656\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04282155692908499\n",
      "Average test loss: 0.00239389710355964\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04284374671843317\n",
      "Average test loss: 0.0023658981122490434\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04263587104611927\n",
      "Average test loss: 0.002444787844808565\n",
      "Epoch 182/300\n",
      "Average training loss: 0.042713196674982704\n",
      "Average test loss: 0.0025179920902268756\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04258942354387707\n",
      "Average test loss: 0.0023953214349846045\n",
      "Epoch 184/300\n",
      "Average training loss: 0.042568717923429276\n",
      "Average test loss: 0.002528312870611747\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04261206371916665\n",
      "Average test loss: 0.003698307088265816\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04244855600595474\n",
      "Average test loss: 0.0024368145501034127\n",
      "Epoch 187/300\n",
      "Average training loss: 0.042418241189585794\n",
      "Average test loss: 0.0024313180692907836\n",
      "Epoch 188/300\n",
      "Average training loss: 0.042351733461022376\n",
      "Average test loss: 0.01591126398824983\n",
      "Epoch 189/300\n",
      "Average training loss: 0.042286277088854046\n",
      "Average test loss: 0.0024746883124527005\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04228930133912298\n",
      "Average test loss: 0.0024179334955083\n",
      "Epoch 191/300\n",
      "Average training loss: 0.042230326861143114\n",
      "Average test loss: 0.0024459574395376774\n",
      "Epoch 192/300\n",
      "Average training loss: 0.042083584775527316\n",
      "Average test loss: 0.0024234082069661883\n",
      "Epoch 193/300\n",
      "Average training loss: 0.042178461654318704\n",
      "Average test loss: 0.006531792028910584\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04201132564412223\n",
      "Average test loss: 0.002416594112705853\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04204532940189044\n",
      "Average test loss: 0.008091193292703894\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04204046426879035\n",
      "Average test loss: 0.0024355651303711863\n",
      "Epoch 198/300\n",
      "Average training loss: 0.041850614027844533\n",
      "Average test loss: 0.007841963400029473\n",
      "Epoch 199/300\n",
      "Average training loss: 0.041907419638501274\n",
      "Average test loss: 0.01004457605092062\n",
      "Epoch 200/300\n",
      "Average training loss: 0.041817495985163586\n",
      "Average test loss: 0.0023978278910120328\n",
      "Epoch 201/300\n",
      "Average training loss: 0.041714241643746694\n",
      "Average test loss: 0.0024316566923840177\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0418090181350708\n",
      "Average test loss: 0.002697607212079068\n",
      "Epoch 203/300\n",
      "Average training loss: 0.041683921486139294\n",
      "Average test loss: 0.002520857306611207\n",
      "Epoch 204/300\n",
      "Average training loss: 0.041680492060052025\n",
      "Average test loss: 0.0024120864553583993\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04162222671839926\n",
      "Average test loss: 0.002456445011206799\n",
      "Epoch 206/300\n",
      "Average training loss: 0.041459395670228535\n",
      "Average test loss: 0.0023468797732558513\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04146423137187958\n",
      "Average test loss: 0.0024634091773380836\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04148616725537512\n",
      "Average test loss: 0.0024137491731800968\n",
      "Epoch 211/300\n",
      "Average training loss: 0.041441781904962324\n",
      "Average test loss: 0.0024223758855627642\n",
      "Epoch 212/300\n",
      "Average training loss: 0.041326492614216274\n",
      "Average test loss: 0.0023518948033452035\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04120762238403161\n",
      "Average test loss: 0.0024355509854439234\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04123128830724292\n",
      "Average test loss: 0.0023908211698548663\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04116270789835188\n",
      "Average test loss: 0.0024130450506798097\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04108380193180508\n",
      "Average test loss: 0.002654476500219769\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04116654193401337\n",
      "Average test loss: 0.002431963058602479\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04106507330801752\n",
      "Average test loss: 0.0024204494764821396\n",
      "Epoch 220/300\n",
      "Average training loss: 0.041060417748159835\n",
      "Average test loss: 0.0024744361813904513\n",
      "Epoch 221/300\n",
      "Average training loss: 0.041021958347823885\n",
      "Average test loss: 0.002707704209826059\n",
      "Epoch 222/300\n",
      "Average training loss: 0.041041825241512724\n",
      "Average test loss: 0.0024529169804106154\n",
      "Epoch 223/300\n",
      "Average test loss: 0.002359438606744839\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0409056153330538\n",
      "Average test loss: 0.0026789588125215634\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04086645061108801\n",
      "Average test loss: 0.007915264582468403\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04072990355557866\n",
      "Average test loss: 0.0035987057565814917\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04075361653168996\n",
      "Average test loss: 0.004005895839797126\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04077044669455952\n",
      "Average test loss: 0.002469848557685812\n",
      "Epoch 229/300\n",
      "Average training loss: 0.040693692760335076\n",
      "Average test loss: 0.002656389789448844\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04061695534984271\n",
      "Average test loss: 0.0025801593081818688\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04063978123995993\n",
      "Average test loss: 0.005017757889711195\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04056934833195475\n",
      "Average test loss: 0.002602973334077332\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04062220441632801\n",
      "Average test loss: 0.0028160960018220873\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04052941529287232\n",
      "Average test loss: 0.0024791103376903467\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04069591931170887\n",
      "Average test loss: 0.0027344615939590665\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040427675687604483\n",
      "Average test loss: 0.0026273036799910996\n",
      "Epoch 238/300\n",
      "Average training loss: 0.040394552141427995\n",
      "Average test loss: 0.0024722924602942334\n",
      "Epoch 239/300\n",
      "Average training loss: 0.040343466099765565\n",
      "Average test loss: 0.0074770935653812355\n",
      "Epoch 240/300\n",
      "Average training loss: 0.040372321605682375\n",
      "Average test loss: 0.0024354579953683748\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04029618547691239\n",
      "Average test loss: 0.002409619840482871\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04026452253924476\n",
      "Average test loss: 0.0024150140492452516\n",
      "Epoch 243/300\n",
      "Average training loss: 0.040244730098379984\n",
      "Average test loss: 0.002546158927389317\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04024290617472596\n",
      "Average test loss: 0.0033981794665257138\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04015941999024815\n",
      "Average test loss: 0.0024070454583399827\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04012444291843308\n",
      "Average test loss: 0.003040361933824089\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04020793260799514\n",
      "Average test loss: 0.002414505559951067\n",
      "Epoch 248/300\n",
      "Average training loss: 0.040086316029230754\n",
      "Average test loss: 0.004445159145113495\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04004457521438599\n",
      "Average training loss: 0.04003852495219972\n",
      "Average test loss: 0.0024059579709751738\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03997489909993278\n",
      "Average test loss: 0.0075528619976507295\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04006629333562321\n",
      "Average test loss: 0.0024275224771764543\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04005237830678622\n",
      "Average test loss: 0.0025148646806677183\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03992587404449781\n",
      "Average test loss: 0.026689049904545147\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03993671991096603\n",
      "Average test loss: 0.0026341674141585826\n",
      "Epoch 256/300\n",
      "Average training loss: 0.039839063591427276\n",
      "Average test loss: 0.0024385291644268567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03985750492744976\n",
      "Average test loss: 0.002502000405970547\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03980759835905499\n",
      "Average test loss: 0.002463106978063782\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03967760497331619\n",
      "Average test loss: 0.0025440677675522038\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03973900804254744\n",
      "Average test loss: 0.0024399909728931054\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03978635534644127\n",
      "Average test loss: 0.0025644394586690597\n",
      "Epoch 263/300\n",
      "Average training loss: 0.039684567825661765\n",
      "Average test loss: 0.0032027457025316027\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03959330629640155\n",
      "Average test loss: 0.025909270672334565\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0396808179517587\n",
      "Average test loss: 0.0031839056664870846\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03967390048835013\n",
      "Average test loss: 0.0024821383942746455\n",
      "Epoch 267/300\n",
      "Average training loss: 0.039558522688017954\n",
      "Average test loss: 0.0024265124470823343\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03946165888177024\n",
      "Average test loss: 0.002470018580555916\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0395583520465427\n",
      "Average test loss: 0.0024296678190843927\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03953916860620181\n",
      "Average test loss: 0.0030259816754195424\n",
      "Epoch 271/300\n",
      "Average training loss: 0.039418419192234676\n",
      "Average test loss: 0.002500604040506813\n",
      "Epoch 272/300\n",
      "Average training loss: 0.039438838558064564\n",
      "Average test loss: 0.0025689446861959166\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03937788764966859\n",
      "Average test loss: 0.0025965769487536618\n",
      "Epoch 274/300\n",
      "Average training loss: 0.039396712925699025\n",
      "Average test loss: 0.0024153356581098503\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039297937273979185\n",
      "Average test loss: 0.0025848501960022583\n",
      "Epoch 277/300\n",
      "Average training loss: 0.039312058607737226\n",
      "Average test loss: 0.0033117055578364266\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03918690125147502\n",
      "Average test loss: 0.0042242127433419225\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03939126804802153\n",
      "Average test loss: 0.00258380286478334\n",
      "Epoch 280/300\n",
      "Average training loss: 0.039159555948442885\n",
      "Average test loss: 0.0036432911592225235\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03925793648428387\n",
      "Average test loss: 0.0025755548791752923\n",
      "Epoch 283/300\n",
      "Average training loss: 0.039182168596320684\n",
      "Average test loss: 0.0035186476012070973\n",
      "Epoch 284/300\n",
      "Average training loss: 0.039195023354556824\n",
      "Average test loss: 0.006937554407864809\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03902050250437525\n",
      "Average test loss: 0.0025737434031648768\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03904471875230471\n",
      "Average test loss: 0.0025108540281653406\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03912369065814548\n",
      "Average test loss: 0.002629358337778184\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03900158916910489\n",
      "Average test loss: 0.0024422408369266326\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03917594580186738\n",
      "Average test loss: 0.0036615867304305235\n",
      "Epoch 290/300\n",
      "Average training loss: 0.039004170735677086\n",
      "Average test loss: 0.006782012662125959\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03896598168545299\n",
      "Average test loss: 0.0026883126033676995\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03888948303626643\n",
      "Average test loss: 0.002494485785977708\n",
      "Epoch 293/300\n",
      "Average training loss: 0.038898173885213\n",
      "Average test loss: 0.0024312547029306493\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03888410492406951\n",
      "Average test loss: 316.3116029398309\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03888744263516532\n",
      "Average test loss: 0.002467032311587698\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03887326186233097\n",
      "Average test loss: 0.00258189848334425\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03880492879947026\n",
      "Average test loss: 1578.8697688132393\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03878725357188119\n",
      "Average test loss: 0.0024533308671994343\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth10/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.03\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 22.73\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 23.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.33\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.03\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.45\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.64\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 26.18\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 26.32\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.46\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.59\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.74\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.74\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.78\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.86\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.89\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.01\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.19\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.32\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.71\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 25.05\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.71\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.82\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.05\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.36\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.65\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.57\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.60\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.91\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.00\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.90\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.44\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.81\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.98\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.94\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.24\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.71\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
