{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.025)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.025)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.025)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.025)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.0598173297262854\n",
      "Average test loss: 0.004951344041153789\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02434078371193674\n",
      "Average test loss: 0.004561784656925334\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022815807989901966\n",
      "Average test loss: 0.004475104377501541\n",
      "Epoch 4/300\n",
      "Average training loss: 0.022320239568750063\n",
      "Average test loss: 0.004406129573161404\n",
      "Epoch 5/300\n",
      "Average training loss: 0.022041354871458477\n",
      "Average test loss: 0.0043627168465819625\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021834087562229897\n",
      "Average test loss: 0.004355513517227438\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021664537731144164\n",
      "Average test loss: 0.004301994908187124\n",
      "Epoch 8/300\n",
      "Average training loss: 0.021515470567676755\n",
      "Average test loss: 0.004287960841630896\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021402604507075415\n",
      "Average test loss: 0.0043011290451718705\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021301098457641072\n",
      "Average test loss: 0.0042373897536761235\n",
      "Epoch 11/300\n",
      "Average training loss: 0.021190615326166153\n",
      "Average test loss: 0.0042276251605815355\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02111633627778954\n",
      "Average test loss: 0.004217178559344676\n",
      "Epoch 13/300\n",
      "Average training loss: 0.021028451533781158\n",
      "Average test loss: 0.004196291111202704\n",
      "Epoch 14/300\n",
      "Average training loss: 0.0209431677940819\n",
      "Average test loss: 0.0041948853778756325\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020881742186016506\n",
      "Average test loss: 0.004175889283832577\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020799720972776412\n",
      "Average test loss: 0.004171037012504207\n",
      "Epoch 17/300\n",
      "Average training loss: 0.020752414549390474\n",
      "Average test loss: 0.004159266131619613\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020692592879964247\n",
      "Average test loss: 0.004159538651506106\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02063366971578863\n",
      "Average test loss: 0.004141719985960258\n",
      "Epoch 20/300\n",
      "Average training loss: 0.020591518248120944\n",
      "Average test loss: 0.004133554810244176\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020537756270832486\n",
      "Average test loss: 0.004125164477361573\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020493222980863517\n",
      "Average test loss: 0.00410205915901396\n",
      "Epoch 23/300\n",
      "Average training loss: 0.020448680736952357\n",
      "Average test loss: 0.00409086320300897\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02040276814169354\n",
      "Average test loss: 0.004098671298267113\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020373994732896486\n",
      "Average test loss: 0.004092916299692459\n",
      "Epoch 26/300\n",
      "Average training loss: 0.020332004747456976\n",
      "Average test loss: 0.004088398484306203\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020292415987286302\n",
      "Average test loss: 0.004084820914185709\n",
      "Epoch 28/300\n",
      "Average training loss: 0.020254339961542023\n",
      "Average test loss: 0.0040592892112003435\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02022099200056659\n",
      "Average test loss: 0.00407458130746252\n",
      "Epoch 30/300\n",
      "Average training loss: 0.020198738639553387\n",
      "Average test loss: 0.004162004912065135\n",
      "Epoch 31/300\n",
      "Average training loss: 0.020173834992779627\n",
      "Average test loss: 0.004060403583157394\n",
      "Epoch 32/300\n",
      "Average training loss: 0.020138526401586004\n",
      "Average test loss: 0.004052549938981732\n",
      "Epoch 33/300\n",
      "Average training loss: 0.020118828599651653\n",
      "Average test loss: 0.0040457042306661604\n",
      "Epoch 34/300\n",
      "Average training loss: 0.020092470270064142\n",
      "Average test loss: 0.004059500435160266\n",
      "Epoch 35/300\n",
      "Average training loss: 0.020066598764724202\n",
      "Average test loss: 0.0040490217995312475\n",
      "Epoch 36/300\n",
      "Average training loss: 0.020046635592977206\n",
      "Average test loss: 0.004054347398380439\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02001145485209094\n",
      "Average test loss: 0.004042251396096415\n",
      "Epoch 38/300\n",
      "Average training loss: 0.019995700420604813\n",
      "Average test loss: 0.004033885523469912\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019975760677622426\n",
      "Average test loss: 0.004049852155976826\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01995039508740107\n",
      "Average test loss: 0.0040662628774427705\n",
      "Epoch 41/300\n",
      "Average training loss: 0.019935742472608885\n",
      "Average test loss: 0.004032191942963335\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019912712862094242\n",
      "Average test loss: 0.004023496927486526\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01989585437708431\n",
      "Average test loss: 0.004027267749524779\n",
      "Epoch 44/300\n",
      "Average training loss: 0.019874751311209467\n",
      "Average test loss: 0.004030202822552787\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01986311807235082\n",
      "Average test loss: 0.004015739093224208\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019849506381485197\n",
      "Average test loss: 0.004034117666383584\n",
      "Epoch 47/300\n",
      "Average training loss: 0.019829776960114637\n",
      "Average test loss: 0.00402455255885919\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0198017930239439\n",
      "Average test loss: 0.00404755450371239\n",
      "Epoch 49/300\n",
      "Average training loss: 0.019785488328999942\n",
      "Average test loss: 0.004026745828903383\n",
      "Epoch 50/300\n",
      "Average training loss: 0.019770213827490807\n",
      "Average test loss: 0.004039333902713325\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019749159471856223\n",
      "Average test loss: 0.004033871746104625\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01974399983551767\n",
      "Average test loss: 0.004049169871542189\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019722364698019294\n",
      "Average test loss: 0.004036373941227794\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019698817628953192\n",
      "Average test loss: 0.004030101608071063\n",
      "Epoch 55/300\n",
      "Average training loss: 0.019683561015460226\n",
      "Average test loss: 0.004025702441318168\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01967463260061211\n",
      "Average test loss: 0.004014210937751664\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0195776077657938\n",
      "Average test loss: 0.004037082177069452\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019560384017725786\n",
      "Average test loss: 0.0040424440828048525\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01954721783929401\n",
      "Average test loss: 0.00401311563493477\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01953660808172491\n",
      "Average test loss: 0.004094122504194578\n",
      "Epoch 66/300\n",
      "Average training loss: 0.019526744917035103\n",
      "Average test loss: 0.004049870767527156\n",
      "Epoch 67/300\n",
      "Average training loss: 0.019499824388159646\n",
      "Average test loss: 0.00403126980488499\n",
      "Epoch 68/300\n",
      "Average training loss: 0.019481960149274932\n",
      "Average test loss: 0.004011446225146453\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019476852150426972\n",
      "Average test loss: 0.004022849674647053\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01945343134386672\n",
      "Average test loss: 0.004049066382977698\n",
      "Epoch 71/300\n",
      "Average training loss: 0.019440319259961446\n",
      "Average test loss: 0.004034781922482782\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01943532561759154\n",
      "Average test loss: 0.004031403874771463\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019422217145562172\n",
      "Average test loss: 0.004037804859379927\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019401186969545153\n",
      "Average test loss: 0.004027320591422419\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019379493869013255\n",
      "Average test loss: 0.004030220943192641\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019363609795769055\n",
      "Average test loss: 0.004010488560216295\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019362092950277857\n",
      "Average test loss: 0.0040200039874762295\n",
      "Epoch 78/300\n",
      "Average training loss: 0.01934309694667657\n",
      "Average test loss: 0.00401668305343224\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019327683518330255\n",
      "Average test loss: 0.004057331472428308\n",
      "Epoch 80/300\n",
      "Average training loss: 0.01930406799581316\n",
      "Average test loss: 0.0040457386846343675\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019291827835970454\n",
      "Average test loss: 0.004052622457138366\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019276514639457065\n",
      "Average test loss: 0.004225371671219667\n",
      "Epoch 83/300\n",
      "Average training loss: 0.019271485047207937\n",
      "Average test loss: 0.004015557635989454\n",
      "Epoch 84/300\n",
      "Average training loss: 0.019248811198605432\n",
      "Average test loss: 0.004033887522502078\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019230894928177197\n",
      "Average test loss: 0.004031051936662859\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01921395692229271\n",
      "Average test loss: 0.004054245763561792\n",
      "Epoch 87/300\n",
      "Average training loss: 0.019203154522511694\n",
      "Average test loss: 0.004034253052125374\n",
      "Epoch 88/300\n",
      "Average training loss: 0.019190118696954514\n",
      "Average test loss: 0.004049532993386189\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019178350201911398\n",
      "Average test loss: 0.00405956579082542\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01915977150367366\n",
      "Average test loss: 0.004068336583673954\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01914511628035042\n",
      "Average test loss: 0.004086402847121159\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019136915707753763\n",
      "Average test loss: 0.004048476679457558\n",
      "Epoch 93/300\n",
      "Average training loss: 0.019107662742336592\n",
      "Average test loss: 0.004101888411988814\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019104915943410662\n",
      "Average test loss: 0.004059831779036257\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019080983695884547\n",
      "Average test loss: 0.004077859729735388\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019075095983015166\n",
      "Average test loss: 0.0040850837785336705\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019059552407926984\n",
      "Average test loss: 0.004047682111461957\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019037337912453545\n",
      "Average test loss: 0.004055910415119595\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01902308837738302\n",
      "Average test loss: 0.004105261613718337\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019017293184995652\n",
      "Average test loss: 0.004085661491172181\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018990409896605543\n",
      "Average test loss: 0.0040618876264327105\n",
      "Epoch 102/300\n",
      "Average training loss: 0.018987359851598738\n",
      "Average test loss: 0.004065292182481951\n",
      "Epoch 103/300\n",
      "Average training loss: 0.018891954243183136\n",
      "Average test loss: 0.004112545061442587\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01888680868016349\n",
      "Average test loss: 0.0041913894443876215\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018882764314611753\n",
      "Average test loss: 0.004096207160088751\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018866970154974197\n",
      "Average test loss: 0.004110346743836999\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018849043728576768\n",
      "Average test loss: 0.00409506397942702\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018830007635884816\n",
      "Average test loss: 0.004137215497593085\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018829611347781286\n",
      "Average test loss: 0.004146517615558372\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01880090512169732\n",
      "Average test loss: 0.0040735350747903186\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0188031809768743\n",
      "Average test loss: 0.004126173451128933\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018790121976700096\n",
      "Average test loss: 0.004124812103600966\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018785175423655244\n",
      "Average test loss: 0.004171421274335848\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018751636140876347\n",
      "Average test loss: 0.004148576997220516\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01874360157383813\n",
      "Average test loss: 0.0041385919844938645\n",
      "Epoch 121/300\n",
      "Average training loss: 0.018726465579536226\n",
      "Average test loss: 0.004114568918529484\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018708110100693173\n",
      "Average test loss: 0.004087644911888573\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018703115151988137\n",
      "Average test loss: 0.004081347150107225\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01869352351791329\n",
      "Average test loss: 0.004112276824812094\n",
      "Epoch 125/300\n",
      "Average training loss: 0.018690712738368247\n",
      "Average test loss: 0.004249838341441419\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018670975853999457\n",
      "Average test loss: 0.004101856482111746\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01866646681063705\n",
      "Average test loss: 0.004133146747532818\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018632454578247335\n",
      "Average test loss: 0.0041490106653008195\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018634359574980205\n",
      "Average test loss: 0.004124251264664862\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018639662245909374\n",
      "Average test loss: 0.004094479233854347\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018617967380417717\n",
      "Average test loss: 0.004160524713910288\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018594162078367338\n",
      "Average test loss: 0.0041439295793986985\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018581983842783503\n",
      "Average test loss: 0.004110237940318054\n",
      "Epoch 134/300\n",
      "Average training loss: 0.018578083408375582\n",
      "Average test loss: 0.00411012300517824\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01857455504933993\n",
      "Average test loss: 0.004116881207045581\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018546383109357623\n",
      "Average test loss: 0.004148588473391202\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018548821930256153\n",
      "Average test loss: 0.004272421667455799\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018526934446560013\n",
      "Average test loss: 0.004107399348376526\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01852123853398694\n",
      "Average test loss: 0.004193733743495412\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018514729923672146\n",
      "Average test loss: 0.00427511812104947\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01850292624036471\n",
      "Average test loss: 0.00421333526695768\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018483746983938745\n",
      "Average test loss: 0.004151666269534164\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018475916332668728\n",
      "Average test loss: 0.0041613673377368186\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01846647260255284\n",
      "Average test loss: 0.004227848448687129\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018446140843133133\n",
      "Average test loss: 0.004222803115430805\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018445962096254032\n",
      "Average test loss: 0.004131332198985749\n",
      "Epoch 147/300\n",
      "Average training loss: 0.018427432275480694\n",
      "Average test loss: 0.0041135827441596325\n",
      "Epoch 148/300\n",
      "Average training loss: 0.018430358415676487\n",
      "Average test loss: 0.004232499726116657\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01840986146363947\n",
      "Average test loss: 0.004133520289220744\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018401379969384936\n",
      "Average test loss: 0.004146377375556363\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018394631132483484\n",
      "Average test loss: 0.004303918415887488\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018380986854434015\n",
      "Average test loss: 0.004195503947842452\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01838984440598223\n",
      "Average test loss: 0.004201124531113439\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018360521033406258\n",
      "Average test loss: 0.004164682616169254\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01834612411922879\n",
      "Average test loss: 0.004170094521095355\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01833864612960153\n",
      "Average test loss: 0.004144435005055533\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01832770662340853\n",
      "Average test loss: 0.004167509303117792\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018329643816583686\n",
      "Average test loss: 0.0042341547620793184\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018314176408780945\n",
      "Average test loss: 0.0042188269647045265\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018311568785044884\n",
      "Average test loss: 0.004143843496632245\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01829895200332006\n",
      "Average test loss: 0.0042055840657817\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01828895996345414\n",
      "Average test loss: 0.004119132532013787\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01827676701876852\n",
      "Average test loss: 0.004139367317366931\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01827223808897866\n",
      "Average test loss: 0.0042434329972085025\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018255313294629257\n",
      "Average test loss: 0.004137653395947483\n",
      "Epoch 166/300\n",
      "Average training loss: 0.018242820384601753\n",
      "Average test loss: 0.004245376490263475\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01823650688264105\n",
      "Average test loss: 0.004263008205013142\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01824743278655741\n",
      "Average test loss: 0.0042945594886938734\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01822671477827761\n",
      "Average test loss: 0.004197054973286059\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01821775767621067\n",
      "Average test loss: 0.00418038830637104\n",
      "Epoch 171/300\n",
      "Average training loss: 0.018215064490834872\n",
      "Average test loss: 0.004165753470733762\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01821154648065567\n",
      "Average test loss: 0.004159710264247325\n",
      "Epoch 173/300\n",
      "Average training loss: 0.018186741181545788\n",
      "Average test loss: 0.004269585867101948\n",
      "Epoch 174/300\n",
      "Average training loss: 0.018188698423405487\n",
      "Average test loss: 0.004354839313568341\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01818121709426244\n",
      "Average test loss: 0.004191528365843826\n",
      "Epoch 176/300\n",
      "Average training loss: 0.018162468997968566\n",
      "Average test loss: 0.004382694187263648\n",
      "Epoch 177/300\n",
      "Average training loss: 0.018148553903732035\n",
      "Average test loss: 0.004244134381827381\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01815212083193991\n",
      "Average test loss: 0.004237457997372581\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01813848680092229\n",
      "Average test loss: 0.004147486054235035\n",
      "Epoch 180/300\n",
      "Average training loss: 0.018130259999798403\n",
      "Average test loss: 0.0042258994813180635\n",
      "Epoch 181/300\n",
      "Average training loss: 0.018142335871027575\n",
      "Average test loss: 0.00430645630405181\n",
      "Epoch 182/300\n",
      "Average training loss: 0.018112774747941228\n",
      "Average test loss: 0.004221783384059866\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018107488148742253\n",
      "Average test loss: 0.004204851552844047\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01811036899520291\n",
      "Average test loss: 0.0042712552497784296\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01810287876592742\n",
      "Average test loss: 0.004158103742740221\n",
      "Epoch 186/300\n",
      "Average training loss: 0.018110742945637966\n",
      "Average test loss: 0.004313587807118892\n",
      "Epoch 187/300\n",
      "Average training loss: 0.018074022543099193\n",
      "Average test loss: 0.004254965739117728\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01806846267150508\n",
      "Average test loss: 0.00417226926340825\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018074880194332864\n",
      "Average test loss: 0.004257562385665046\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018061359875731996\n",
      "Average test loss: 0.00424428723897371\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01805026026401255\n",
      "Average test loss: 0.004215341443816821\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01804417574736807\n",
      "Average test loss: 0.004296132053765986\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018034542299807072\n",
      "Average test loss: 0.004216868214723136\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01803202932741907\n",
      "Average test loss: 0.004236074990696378\n",
      "Epoch 195/300\n",
      "Average training loss: 0.018020651052395503\n",
      "Average test loss: 0.0042437347633143266\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018013014056616358\n",
      "Average test loss: 0.00420037966614796\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018014433258109624\n",
      "Average test loss: 0.004188650259955062\n",
      "Epoch 198/300\n",
      "Average training loss: 0.018000280934903357\n",
      "Average test loss: 0.00426713045893444\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0180030219083031\n",
      "Average test loss: 0.004258044634221329\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01800179681016339\n",
      "Average test loss: 0.004256097123440769\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01798309145039982\n",
      "Average test loss: 0.004204956413971053\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017966855774323144\n",
      "Average test loss: 0.004305758674318592\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017980680824981797\n",
      "Average test loss: 0.004282216920206944\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01796283926235305\n",
      "Average test loss: 0.00424708109928502\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01795741957757208\n",
      "Average test loss: 0.00430173432164722\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01795448037154145\n",
      "Average test loss: 0.004248735643302401\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017942241638898848\n",
      "Average test loss: 0.004217911167691151\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017937865992387135\n",
      "Average test loss: 0.004289233832516604\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01793259269495805\n",
      "Average test loss: 0.004210270539340046\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01793687171406216\n",
      "Average test loss: 0.004274943398518694\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017918515475259886\n",
      "Average test loss: 0.004370249825633234\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01790724885629283\n",
      "Average test loss: 0.004373981853326161\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017897511882914437\n",
      "Average test loss: 0.004306956590463718\n",
      "Epoch 214/300\n",
      "Average training loss: 0.017901953323019876\n",
      "Average test loss: 0.004214582732568185\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01788498043600056\n",
      "Average test loss: 0.004278148761226072\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017892200998961924\n",
      "Average test loss: 0.004466944061012732\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017882078854574098\n",
      "Average test loss: 0.00427445553160376\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01787813654045264\n",
      "Average test loss: 0.004222239941772487\n",
      "Epoch 219/300\n",
      "Average training loss: 0.017880688750081593\n",
      "Average test loss: 0.00421693060422937\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01786056337505579\n",
      "Average test loss: 0.004456542841469248\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017848668856753243\n",
      "Average test loss: 0.004190111239958141\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017853384223249225\n",
      "Average test loss: 0.00438287121988833\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017847883316377797\n",
      "Average test loss: 0.004271272637156977\n",
      "Epoch 224/300\n",
      "Average training loss: 0.017843759887748296\n",
      "Average test loss: 0.004317219206028514\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01784024776269992\n",
      "Average test loss: 0.004245088478343355\n",
      "Epoch 226/300\n",
      "Average training loss: 0.017825990037785636\n",
      "Average test loss: 0.004267056139392985\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0178221188849873\n",
      "Average test loss: 0.004296997933752007\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01782110640903314\n",
      "Average test loss: 0.004404708842022552\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017809829668038422\n",
      "Average test loss: 0.004219496404545175\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01780702482495043\n",
      "Average test loss: 0.004351661073664824\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017806911679605644\n",
      "Average test loss: 0.004299188869694869\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01780635433809625\n",
      "Average test loss: 0.0042427779055303995\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017797730583283638\n",
      "Average test loss: 0.0043674131073057655\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017787649942768945\n",
      "Average test loss: 0.004297635399632984\n",
      "Epoch 235/300\n",
      "Average training loss: 0.017790331958068742\n",
      "Average test loss: 0.004229716783182488\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01777828186750412\n",
      "Average test loss: 0.004201618930945793\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017780388419826824\n",
      "Average test loss: 0.004264347460948758\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017780495304200384\n",
      "Average test loss: 0.004344211553533872\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017768638716803658\n",
      "Average test loss: 0.004217501102636258\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017759881849090258\n",
      "Average test loss: 0.004316895481612947\n",
      "Epoch 241/300\n",
      "Average training loss: 0.017741581799255477\n",
      "Average test loss: 0.004217298800332679\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017740997590952448\n",
      "Average test loss: 0.004181861627019114\n",
      "Epoch 243/300\n",
      "Average training loss: 0.017744573982225523\n",
      "Average test loss: 0.004355472917565041\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01774103649291727\n",
      "Average test loss: 0.004213923534378409\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017726533927023412\n",
      "Average test loss: 0.004330636906954977\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01772604301985767\n",
      "Average test loss: 0.004325591318309307\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017728958848449917\n",
      "Average test loss: 0.0042805857575602\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01770855542189545\n",
      "Average test loss: 0.004269377620683776\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01771454356279638\n",
      "Average test loss: 0.0043029670239322715\n",
      "Epoch 250/300\n",
      "Average training loss: 0.017717122761739625\n",
      "Average test loss: 0.004355793063839277\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01770852605543203\n",
      "Average test loss: 0.004363930734288361\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01769295647326443\n",
      "Average test loss: 0.004409835242562824\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017689958165089288\n",
      "Average test loss: 0.004297262553539541\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01769399344921112\n",
      "Average test loss: 0.004247922209401926\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017693314510087172\n",
      "Average test loss: 0.004306263890738289\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01766432954867681\n",
      "Average test loss: 0.0043597643507851495\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01767476371427377\n",
      "Average test loss: 0.004333358774582545\n",
      "Epoch 258/300\n",
      "Average training loss: 0.017676138903531764\n",
      "Average test loss: 0.00428402318019006\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01766768921332227\n",
      "Average test loss: 0.004508146164524886\n",
      "Epoch 260/300\n",
      "Average training loss: 0.017661271476911173\n",
      "Average test loss: 0.004329902826497952\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017659676414397026\n",
      "Average test loss: 0.004310870238062408\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017665630380312602\n",
      "Average test loss: 0.004349943402326769\n",
      "Epoch 263/300\n",
      "Average training loss: 0.017635685452156595\n",
      "Average test loss: 0.004339836171517769\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017645999843875566\n",
      "Average test loss: 0.004388189513236284\n",
      "Epoch 265/300\n",
      "Average training loss: 0.017642666870521174\n",
      "Average test loss: 0.004430948918271396\n",
      "Epoch 266/300\n",
      "Average training loss: 0.017636168917020162\n",
      "Average test loss: 0.004289450233181317\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017641390011542375\n",
      "Average test loss: 0.0043342443853616715\n",
      "Epoch 268/300\n",
      "Average training loss: 0.01763243358582258\n",
      "Average test loss: 0.004301607840591007\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017622149239811633\n",
      "Average test loss: 0.0043156870073742335\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01762031458566586\n",
      "Average test loss: 0.004344277683438526\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0176208763404025\n",
      "Average test loss: 0.004371403835299942\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01762280641413397\n",
      "Average test loss: 0.004364624855212039\n",
      "Epoch 273/300\n",
      "Average training loss: 0.017600937058528266\n",
      "Average test loss: 0.004320124090545707\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01760656830171744\n",
      "Average test loss: 0.004258472145845493\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017598755543430645\n",
      "Average test loss: 0.004297164648771286\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01758916320403417\n",
      "Average test loss: 0.004264436788856983\n",
      "Epoch 277/300\n",
      "Average training loss: 0.017596690742505923\n",
      "Average test loss: 0.004293270495616727\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01758260072933303\n",
      "Average test loss: 0.00426998368340234\n",
      "Epoch 279/300\n",
      "Average training loss: 0.017582664167715444\n",
      "Average test loss: 0.004456596793813838\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01758615508510007\n",
      "Average test loss: 0.004189710737309522\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0175741133193175\n",
      "Average test loss: 0.004339931400285827\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01756536119348473\n",
      "Average test loss: 0.004310130950063467\n",
      "Epoch 283/300\n",
      "Average training loss: 0.017559311335285505\n",
      "Average test loss: 0.004303060558728046\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01755766855345832\n",
      "Average test loss: 0.004317108359187841\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017551254644989966\n",
      "Average test loss: 0.00419706004154351\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017541044215361276\n",
      "Average test loss: 0.004422754167889556\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01756046067757739\n",
      "Average test loss: 0.0044128916023506055\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017546535972091888\n",
      "Average test loss: 0.004360913300265869\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017540400435527164\n",
      "Average test loss: 0.004286625584380494\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01754339564177725\n",
      "Average test loss: 0.0043370152807070145\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017529353212979104\n",
      "Average test loss: 0.004340631509406699\n",
      "Epoch 292/300\n",
      "Average training loss: 0.017543028261926438\n",
      "Average test loss: 0.0043251996050692266\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017541556131508616\n",
      "Average test loss: 0.004458925063411395\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01753013349738386\n",
      "Average test loss: 0.00426559081259701\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017539290454652575\n",
      "Average test loss: 0.004282271439830462\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017517836314936478\n",
      "Average test loss: 0.004356384969419903\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01751743321120739\n",
      "Average test loss: 0.00429913913582762\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017505893872016006\n",
      "Average test loss: 0.004205734873397483\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01749615145060751\n",
      "Average test loss: 0.004280669341691666\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01751449843082163\n",
      "Average test loss: 0.004335782880170478\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.0541325537165006\n",
      "Average test loss: 0.004284699840678109\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02107177886697981\n",
      "Average test loss: 0.004237756691873073\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02004989989929729\n",
      "Average test loss: 0.003944417336748706\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0195502438445886\n",
      "Average test loss: 0.0038461820065147346\n",
      "Epoch 5/300\n",
      "Average training loss: 0.019182923131518894\n",
      "Average test loss: 0.0037550168209191824\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018880854768885506\n",
      "Average test loss: 0.003698196930189927\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01859728092617459\n",
      "Average test loss: 0.003691406055043141\n",
      "Epoch 8/300\n",
      "Average training loss: 0.018341958831581805\n",
      "Average test loss: 0.0036175797763797974\n",
      "Epoch 9/300\n",
      "Average training loss: 0.01812228722539213\n",
      "Average test loss: 0.0035755870305001734\n",
      "Epoch 10/300\n",
      "Average training loss: 0.017924384236335755\n",
      "Average test loss: 0.0035140209934777684\n",
      "Epoch 11/300\n",
      "Average training loss: 0.017731288054751024\n",
      "Average test loss: 0.0034708354889104766\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017563284070955382\n",
      "Average test loss: 0.003467620186921623\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0174179268396563\n",
      "Average test loss: 0.0034092282636298075\n",
      "Epoch 14/300\n",
      "Average training loss: 0.017283258038262528\n",
      "Average test loss: 0.003388424532281028\n",
      "Epoch 15/300\n",
      "Average training loss: 0.017162190980381437\n",
      "Average test loss: 0.0033553621551642814\n",
      "Epoch 16/300\n",
      "Average training loss: 0.017053051103320387\n",
      "Average test loss: 0.003339680629264977\n",
      "Epoch 17/300\n",
      "Average training loss: 0.016936143493486776\n",
      "Average test loss: 0.003314982545872529\n",
      "Epoch 18/300\n",
      "Average training loss: 0.016859470293753676\n",
      "Average test loss: 0.0033105364125221967\n",
      "Epoch 19/300\n",
      "Average training loss: 0.016752984697206152\n",
      "Average test loss: 0.0034071026868704294\n",
      "Epoch 20/300\n",
      "Average training loss: 0.016677786190476683\n",
      "Average test loss: 0.0032735847408572835\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016594170639912288\n",
      "Average test loss: 0.0032434202908641764\n",
      "Epoch 22/300\n",
      "Average training loss: 0.016537012471920916\n",
      "Average test loss: 0.0032246693482415544\n",
      "Epoch 23/300\n",
      "Average training loss: 0.016447058527833885\n",
      "Average test loss: 0.003248453775006864\n",
      "Epoch 24/300\n",
      "Average training loss: 0.016389102689921855\n",
      "Average test loss: 0.0032281115317924153\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01634215960237715\n",
      "Average test loss: 0.0032470128581755693\n",
      "Epoch 26/300\n",
      "Average training loss: 0.01628073309030798\n",
      "Average test loss: 0.0031899515326238343\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016229320110546216\n",
      "Average test loss: 0.0032162024912734824\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01618549518783887\n",
      "Average test loss: 0.003200795650275217\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016129822115103404\n",
      "Average test loss: 0.003184048911349641\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01609010480178727\n",
      "Average test loss: 0.003171579025271866\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01604952110350132\n",
      "Average test loss: 0.003163803129043016\n",
      "Epoch 32/300\n",
      "Average training loss: 0.016009541403916146\n",
      "Average test loss: 0.0031534793020950424\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015973259063230622\n",
      "Average test loss: 0.003153916706227594\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015939238289164173\n",
      "Average test loss: 0.0032026243042200804\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01589450251724985\n",
      "Average test loss: 0.0031567470303012265\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01586752190358109\n",
      "Average test loss: 0.003195100955458151\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015828352138400077\n",
      "Average test loss: 0.00314344227893485\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015790350562996335\n",
      "Average test loss: 0.0031385160418641238\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015765736117959023\n",
      "Average test loss: 0.0031267051746447882\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015732210752036836\n",
      "Average test loss: 0.0031568368987904653\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015697955678734516\n",
      "Average test loss: 0.0031212622303929596\n",
      "Epoch 42/300\n",
      "Average training loss: 0.015669987900389565\n",
      "Average test loss: 0.0031209115232858392\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01564266425702307\n",
      "Average test loss: 0.003133506401752432\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015617120801574654\n",
      "Average test loss: 0.0031291804319868487\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015595792339907752\n",
      "Average test loss: 0.003130968551668856\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015551674413184325\n",
      "Average test loss: 0.003105989569591151\n",
      "Epoch 47/300\n",
      "Average training loss: 0.015532937539120516\n",
      "Average test loss: 0.003108356681548887\n",
      "Epoch 48/300\n",
      "Average training loss: 0.015502387238045534\n",
      "Average test loss: 0.003151476641289062\n",
      "Epoch 49/300\n",
      "Average training loss: 0.015479165802399317\n",
      "Average test loss: 0.0031344780514223707\n",
      "Epoch 50/300\n",
      "Average training loss: 0.015460155706438753\n",
      "Average test loss: 0.0031286716846128304\n",
      "Epoch 51/300\n",
      "Average training loss: 0.015421006614135372\n",
      "Average test loss: 0.0031197504351536433\n",
      "Epoch 52/300\n",
      "Average training loss: 0.015396698758833939\n",
      "Average test loss: 0.00310078019524614\n",
      "Epoch 53/300\n",
      "Average training loss: 0.015375031918287277\n",
      "Average test loss: 0.0031122951224032375\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01535298203263018\n",
      "Average test loss: 0.003137602059998446\n",
      "Epoch 55/300\n",
      "Average training loss: 0.015334648412962755\n",
      "Average test loss: 0.003125498243711061\n",
      "Epoch 56/300\n",
      "Average training loss: 0.015306424526704682\n",
      "Average test loss: 0.003140939069704877\n",
      "Epoch 57/300\n",
      "Average training loss: 0.015296654947929911\n",
      "Average test loss: 0.003175033720831076\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01524752127710316\n",
      "Average test loss: 0.0031232124951978526\n",
      "Epoch 59/300\n",
      "Average training loss: 0.015231873807807763\n",
      "Average test loss: 0.003124891869102915\n",
      "Epoch 60/300\n",
      "Average training loss: 0.015207852556473679\n",
      "Average test loss: 0.0031302062628997696\n",
      "Epoch 61/300\n",
      "Average training loss: 0.015192304776774513\n",
      "Average test loss: 0.0031022038196937907\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01517096770223644\n",
      "Average test loss: 0.0031257501658466125\n",
      "Epoch 63/300\n",
      "Average training loss: 0.015141210747261841\n",
      "Average test loss: 0.0031571889225807453\n",
      "Epoch 64/300\n",
      "Average training loss: 0.015137464183900091\n",
      "Average test loss: 0.0031944838975452713\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0150942224520776\n",
      "Average test loss: 0.003102122735232115\n",
      "Epoch 66/300\n",
      "Average training loss: 0.015078724697232246\n",
      "Average test loss: 0.003163046654313803\n",
      "Epoch 67/300\n",
      "Average training loss: 0.015048301974932352\n",
      "Average test loss: 0.003165494195289082\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01503186413894097\n",
      "Average test loss: 0.0031279195429136358\n",
      "Epoch 69/300\n",
      "Average training loss: 0.015025374822318555\n",
      "Average test loss: 0.003164529015206628\n",
      "Epoch 70/300\n",
      "Average training loss: 0.014998817947175768\n",
      "Average test loss: 0.0031555911410186027\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014973691115776698\n",
      "Average test loss: 0.003134279376309779\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014956298188202911\n",
      "Average test loss: 0.00317397464108136\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014927421681582927\n",
      "Average test loss: 0.0031290435108045737\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014909946489665243\n",
      "Average test loss: 0.0031644978245927227\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014887380007240507\n",
      "Average test loss: 0.003110277983877394\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01487335214101606\n",
      "Average test loss: 0.0031290387546436655\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01485061011546188\n",
      "Average test loss: 0.003138917454621858\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014841761163539356\n",
      "Average test loss: 0.0031256278976798057\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014834994249873692\n",
      "Average test loss: 0.0031808110140264033\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014794187668297026\n",
      "Average test loss: 0.0031614352555738557\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014777978398733668\n",
      "Average test loss: 0.0031551931721882686\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014758726867536704\n",
      "Average test loss: 0.003255262452695105\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014745365012023184\n",
      "Average test loss: 0.003124774735627903\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014738119013607501\n",
      "Average test loss: 0.003152691199961636\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01470551651633448\n",
      "Average test loss: 0.0031630386457675032\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014695904451111953\n",
      "Average test loss: 0.003151831395717131\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014669456734425492\n",
      "Average test loss: 0.0031882463979224364\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014658739091621504\n",
      "Average test loss: 0.003253940436161227\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014650094947881168\n",
      "Average test loss: 0.0032543605723314817\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014620088477515511\n",
      "Average test loss: 0.003232931125495169\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014618434461454551\n",
      "Average test loss: 0.0031504347416468795\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014592439741724068\n",
      "Average test loss: 0.003165873861561219\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014570392474532127\n",
      "Average test loss: 0.003262854309131702\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014562062489489715\n",
      "Average test loss: 0.0031574388947337865\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014556079046593772\n",
      "Average test loss: 0.003189000588738256\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014526320725679397\n",
      "Average test loss: 0.003175456695776019\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014519230048689578\n",
      "Average test loss: 0.003157396081628071\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014503636457026004\n",
      "Average test loss: 0.0031453204593724676\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014480309079918596\n",
      "Average test loss: 0.0032459920946922567\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014476563110119767\n",
      "Average test loss: 0.003179101470236977\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014448295730683538\n",
      "Average test loss: 0.0031783087238048515\n",
      "Epoch 102/300\n",
      "Average training loss: 0.014440775827401215\n",
      "Average test loss: 0.0032366316772790417\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014440055744515524\n",
      "Average test loss: 0.0031429638444549508\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01440895458476411\n",
      "Average test loss: 0.0031594380256202486\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014405033584270213\n",
      "Average test loss: 0.0032428164562831323\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014396256840063466\n",
      "Average test loss: 0.003168567176701294\n",
      "Epoch 107/300\n",
      "Average training loss: 0.014365891589886612\n",
      "Average test loss: 0.003253937691036198\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014361282583740023\n",
      "Average test loss: 0.0032241377122700214\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014354788422584534\n",
      "Average test loss: 0.0031699785223851602\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01432567164219088\n",
      "Average test loss: 0.003176950642011232\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01431813705050283\n",
      "Average test loss: 0.0032479948535975483\n",
      "Epoch 112/300\n",
      "Average training loss: 0.014289834979507659\n",
      "Average test loss: 0.0031685656967262426\n",
      "Epoch 113/300\n",
      "Average training loss: 0.014291721507079072\n",
      "Average test loss: 0.0032589033651683067\n",
      "Epoch 114/300\n",
      "Average training loss: 0.014291302310923735\n",
      "Average test loss: 0.0031586638769755763\n",
      "Epoch 115/300\n",
      "Average training loss: 0.014279869130088224\n",
      "Average test loss: 0.003234182621869776\n",
      "Epoch 116/300\n",
      "Average training loss: 0.014263319165342384\n",
      "Average test loss: 0.0032009105620284875\n",
      "Epoch 117/300\n",
      "Average training loss: 0.014242161865863535\n",
      "Average test loss: 0.0032748182538068957\n",
      "Epoch 118/300\n",
      "Average training loss: 0.014236631512641906\n",
      "Average test loss: 0.003238742867070768\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0142152319252491\n",
      "Average test loss: 0.0032985015033433836\n",
      "Epoch 120/300\n",
      "Average training loss: 0.014230954005486435\n",
      "Average test loss: 0.003221246538269851\n",
      "Epoch 121/300\n",
      "Average training loss: 0.014200344255401029\n",
      "Average test loss: 0.0033389289628507363\n",
      "Epoch 122/300\n",
      "Average training loss: 0.014183578290873104\n",
      "Average test loss: 0.0032742952927947043\n",
      "Epoch 123/300\n",
      "Average training loss: 0.014190448647985856\n",
      "Average test loss: 0.0033509335778653623\n",
      "Epoch 124/300\n",
      "Average training loss: 0.014158988841705852\n",
      "Average test loss: 0.0032284851459165416\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014158269749747382\n",
      "Average test loss: 0.003207437086850405\n",
      "Epoch 126/300\n",
      "Average training loss: 0.014144817993044853\n",
      "Average test loss: 0.003195943099550075\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014133778978553084\n",
      "Average test loss: 0.003231235923866431\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0141280986385213\n",
      "Average test loss: 0.0032436622304634916\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01412154984060261\n",
      "Average test loss: 0.0032817387299405205\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014102440603905255\n",
      "Average test loss: 0.003235554848694139\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014084501231710116\n",
      "Average test loss: 0.0032512706944512\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01408468712783522\n",
      "Average test loss: 0.0033526423966719044\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014087852507829667\n",
      "Average test loss: 0.0033134430684149266\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014056248172289796\n",
      "Average test loss: 0.003304233092504243\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014043052446511056\n",
      "Average test loss: 0.0032288605628742113\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014044583748612139\n",
      "Average test loss: 0.0032485868744552135\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01404554257541895\n",
      "Average test loss: 0.0032254950317243737\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01403049640605847\n",
      "Average test loss: 0.0033062862120568754\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014013571978443199\n",
      "Average test loss: 0.0033301994610163903\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014012898277905252\n",
      "Average test loss: 0.003567244631962644\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01398716943214337\n",
      "Average test loss: 0.0033232742487970326\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013995461529327763\n",
      "Average test loss: 0.0032900016885250806\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013980786060293515\n",
      "Average test loss: 0.0032435371656384733\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01396918588462803\n",
      "Average test loss: 0.0032041118155337043\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013965792447328568\n",
      "Average test loss: 0.00332325500374039\n",
      "Epoch 146/300\n",
      "Average training loss: 0.013961122151050302\n",
      "Average test loss: 0.0032427251040935516\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013943949548734558\n",
      "Average test loss: 0.0032538414475404556\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013954638626012537\n",
      "Average test loss: 0.0032549649859882064\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013927700398696794\n",
      "Average test loss: 0.0033361465957843597\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013919246332512961\n",
      "Average test loss: 0.00343834494240582\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013915041155285305\n",
      "Average test loss: 0.003251368399709463\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013901010830369261\n",
      "Average test loss: 0.0032562733110454347\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013918407286206882\n",
      "Average test loss: 0.0032644158616248102\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013887432362470363\n",
      "Average test loss: 0.003337658107901613\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013880591635902722\n",
      "Average test loss: 0.0032798567737142245\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013886363684303231\n",
      "Average test loss: 0.003423475130564637\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01387275205966499\n",
      "Average test loss: 0.0032889052625331614\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013860080242156982\n",
      "Average test loss: 0.003278420360138019\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013849265118439992\n",
      "Average test loss: 0.003254231526619858\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013848876064850224\n",
      "Average test loss: 0.0032552161895566516\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013848653150929344\n",
      "Average test loss: 0.003319464349705312\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013809183805353112\n",
      "Average test loss: 0.003229326261828343\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013829625687665409\n",
      "Average test loss: 0.0033481028750538826\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013817520213623842\n",
      "Average test loss: 0.003224778938624594\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01381870800919003\n",
      "Average test loss: 0.0033284905335555473\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01380091958741347\n",
      "Average test loss: 0.003301983817583985\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013806420760022269\n",
      "Average test loss: 0.0033272511534806755\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013796131952769226\n",
      "Average test loss: 0.0033249655417684053\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013776408443848292\n",
      "Average test loss: 0.003357486212005218\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013770006501012379\n",
      "Average test loss: 0.003227725493411223\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01378163935078515\n",
      "Average test loss: 0.003257418844124509\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013774316416846382\n",
      "Average test loss: 0.0033737400194836986\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013763018305930826\n",
      "Average test loss: 0.003374852393857307\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013738934186597666\n",
      "Average test loss: 0.0032835225421521397\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01374387972553571\n",
      "Average test loss: 0.003322486157115135\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013741212512056032\n",
      "Average test loss: 0.0033700193264004255\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013728975743883185\n",
      "Average test loss: 0.0033014916057387987\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013730047295490901\n",
      "Average test loss: 0.003422266249027517\n",
      "Epoch 179/300\n",
      "Average training loss: 0.013723496764070458\n",
      "Average test loss: 0.003362332017885314\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013702583476073213\n",
      "Average test loss: 0.003252024960083266\n",
      "Epoch 181/300\n",
      "Average training loss: 0.013716409724619653\n",
      "Average test loss: 0.0033293105955753063\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013704015768236584\n",
      "Average test loss: 0.003287813602015376\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013706289994219939\n",
      "Average test loss: 0.0033324412355820337\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013688456362320318\n",
      "Average test loss: 0.003334463496795959\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013668571658432483\n",
      "Average test loss: 0.003272595828399062\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013694780939982998\n",
      "Average test loss: 0.0034544808338913655\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01365968867805269\n",
      "Average test loss: 0.0033007419010003406\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013663481715238757\n",
      "Average test loss: 0.0033806242597185904\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013668289601802826\n",
      "Average test loss: 0.003288866247360905\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013630090038809512\n",
      "Average test loss: 0.0033341218570454253\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013636892918911245\n",
      "Average test loss: 0.003297297839075327\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01363924143049452\n",
      "Average test loss: 0.0032594870155056316\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013645362839930588\n",
      "Average test loss: 0.0034465468869441084\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013627890134023296\n",
      "Average test loss: 0.0032909381019158497\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013628391162388854\n",
      "Average test loss: 0.0032977509312331677\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013628599441713758\n",
      "Average test loss: 0.003325821161684063\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013611407582130697\n",
      "Average test loss: 0.003546108342707157\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013616855391197735\n",
      "Average test loss: 0.003296464479631848\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013609079973565207\n",
      "Average test loss: 0.0034437788867702087\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013584454057945146\n",
      "Average test loss: 0.00336999928433862\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013593454100191592\n",
      "Average test loss: 0.0033237663981401256\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013584639075729581\n",
      "Average test loss: 0.0033058467542545662\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013590016609264744\n",
      "Average test loss: 0.00336770082410011\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013574244592752721\n",
      "Average test loss: 0.0033871718380186293\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013586578134033415\n",
      "Average test loss: 0.0032777689807116984\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013591007315450244\n",
      "Average test loss: 0.003294515869476729\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01355828397307131\n",
      "Average test loss: 0.00327941675318612\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01356064762837357\n",
      "Average test loss: 0.003408539216344555\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013545096405678326\n",
      "Average test loss: 0.003326623637850086\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013555055774748326\n",
      "Average test loss: 0.003446251037427121\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01354440858380662\n",
      "Average test loss: 0.0032437004587716526\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01354867813653416\n",
      "Average test loss: 0.0032701295924683414\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013530744692517652\n",
      "Average test loss: 0.003385038373164005\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0135349726345804\n",
      "Average test loss: 0.0033588724181883864\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013527294046348996\n",
      "Average test loss: 0.003487742496861352\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013527268561224142\n",
      "Average test loss: 0.003373532937011785\n",
      "Epoch 217/300\n",
      "Average training loss: 0.013526949038936031\n",
      "Average test loss: 0.0033537604591498773\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013537692930135462\n",
      "Average test loss: 0.0034734150558296175\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013497902484403716\n",
      "Average test loss: 0.0033985593917055263\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013494905932082071\n",
      "Average test loss: 0.003236073936853144\n",
      "Epoch 221/300\n",
      "Average training loss: 0.013496353518631724\n",
      "Average test loss: 0.0032544310618605877\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013491435368027951\n",
      "Average test loss: 0.00337726375212272\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01347832247780429\n",
      "Average test loss: 0.0033831707237081394\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01349212115506331\n",
      "Average test loss: 0.003308290568076902\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013481073550052113\n",
      "Average test loss: 0.0032713606316182348\n",
      "Epoch 226/300\n",
      "Average training loss: 0.013477266282671027\n",
      "Average test loss: 0.003424930062972837\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013468445327546862\n",
      "Average test loss: 0.003333354553207755\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013465925272140238\n",
      "Average test loss: 0.0033534115695705016\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013473343254791366\n",
      "Average test loss: 0.0032865642342302533\n",
      "Epoch 230/300\n",
      "Average training loss: 0.013464245692723328\n",
      "Average test loss: 0.003290125584022866\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01344897648692131\n",
      "Average test loss: 0.003405993066728115\n",
      "Epoch 232/300\n",
      "Average training loss: 0.013456767083042198\n",
      "Average test loss: 0.003329772812831733\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013451693300985628\n",
      "Average test loss: 0.003432625198529826\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01346047614928749\n",
      "Average test loss: 0.0033098390843305323\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013430367207361592\n",
      "Average test loss: 0.0033218831891814867\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013441992131372293\n",
      "Average test loss: 0.003490616776463058\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013433630504541928\n",
      "Average test loss: 0.0034077308064119683\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013420893817312188\n",
      "Average test loss: 0.0033940237009276947\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013427070803112454\n",
      "Average test loss: 0.003295883745368984\n",
      "Epoch 240/300\n",
      "Average training loss: 0.013422415916290549\n",
      "Average test loss: 0.003321549156887664\n",
      "Epoch 241/300\n",
      "Average training loss: 0.013426533309949768\n",
      "Average test loss: 0.0033847605350116887\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01342045450458924\n",
      "Average test loss: 0.0033919963704215155\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013408846300509241\n",
      "Average test loss: 0.003285635826488336\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013411776435871919\n",
      "Average test loss: 0.0033363384979052677\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013416658371686936\n",
      "Average test loss: 0.0033881910887236396\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013407246372765965\n",
      "Average test loss: 0.003263343689756261\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013385785044067436\n",
      "Average test loss: 0.0033329295122789013\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013395734143339926\n",
      "Average test loss: 0.0033977076224982737\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013380075514316558\n",
      "Average test loss: 0.0033896286495857767\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013392015267577436\n",
      "Average test loss: 0.0034050946686830785\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01338139896757073\n",
      "Average test loss: 0.0032189393678887022\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013380410491592354\n",
      "Average test loss: 0.0034624136754622063\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013367051909367244\n",
      "Average test loss: 0.0033648970367179975\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013370882987148232\n",
      "Average test loss: 0.003368456548286809\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013360066677133242\n",
      "Average test loss: 0.003405873926149474\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013366058219638136\n",
      "Average test loss: 0.0034473231035388177\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013353149827155802\n",
      "Average test loss: 0.003369831797149446\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013362975479000144\n",
      "Average test loss: 0.0034414442796260117\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013348687300665511\n",
      "Average test loss: 0.003446009995208846\n",
      "Epoch 260/300\n",
      "Average training loss: 0.013338561994334062\n",
      "Average test loss: 0.003443176543133126\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013347204162842697\n",
      "Average test loss: 0.0035059645651943153\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013337740355067783\n",
      "Average test loss: 0.003411235195481115\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013341331678960058\n",
      "Average test loss: 0.0033083643859459296\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013316439940697616\n",
      "Average test loss: 0.003441843142112096\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013347596699992815\n",
      "Average test loss: 0.0033971106757720313\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013320986879368623\n",
      "Average test loss: 0.0032754016982184516\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013334251292877727\n",
      "Average test loss: 0.0033603147466977437\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013330150153901842\n",
      "Average test loss: 0.0034080601489792266\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013319508844779598\n",
      "Average test loss: 0.0034312121272087096\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013304466414782737\n",
      "Average test loss: 0.003507987615548902\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013316957761016157\n",
      "Average test loss: 0.0033803444132208824\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013320629954338074\n",
      "Average test loss: 0.0034837168202632002\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013302781491643852\n",
      "Average test loss: 0.003422276169889503\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013302435924609501\n",
      "Average test loss: 0.00329241933590836\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013310248618324598\n",
      "Average test loss: 0.003310374475394686\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013305325247347354\n",
      "Average test loss: 0.0035201943423599003\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013278810258540842\n",
      "Average test loss: 0.0033549478749434155\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013283664514621098\n",
      "Average test loss: 0.0033550715191910663\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013287029051946269\n",
      "Average test loss: 0.003351602091247009\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01329016847991281\n",
      "Average test loss: 0.003459475056371755\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013291308022207684\n",
      "Average test loss: 0.0033594891726970674\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01327100472855899\n",
      "Average test loss: 0.003366756576630804\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013284349399308363\n",
      "Average test loss: 0.003430058861565259\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013267031654715537\n",
      "Average test loss: 0.0034015398824380504\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013278772390551037\n",
      "Average test loss: 0.0034115184344765213\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01325970096886158\n",
      "Average test loss: 0.003394750337426861\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013269103782044517\n",
      "Average test loss: 0.0035236549460225634\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013256478211945958\n",
      "Average test loss: 0.0033058238470306\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013239747686518564\n",
      "Average test loss: 0.0034537878615988624\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013254068100617992\n",
      "Average test loss: 0.0033413103144201966\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01325408436026838\n",
      "Average test loss: 0.0033589027064541976\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013247382510867383\n",
      "Average test loss: 0.0035228567980229854\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013234180577099323\n",
      "Average test loss: 0.0033651045707778797\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01323835189640522\n",
      "Average test loss: 0.003428723491521345\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013240147017770343\n",
      "Average test loss: 0.003413488766178489\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013247133801380793\n",
      "Average test loss: 0.0034078702732092806\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013235963818927606\n",
      "Average test loss: 0.0033864370404432216\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013241893876757886\n",
      "Average test loss: 0.003388706831882397\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013232244893080659\n",
      "Average test loss: 0.003338385633089476\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013219169370830059\n",
      "Average test loss: 0.003345554862792293\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.049401043105456566\n",
      "Average test loss: 0.0037382929854922822\n",
      "Epoch 2/300\n",
      "Average training loss: 0.018374671772122382\n",
      "Average test loss: 0.003541865095703138\n",
      "Epoch 3/300\n",
      "Average training loss: 0.017369039192795754\n",
      "Average test loss: 0.003331649526125855\n",
      "Epoch 4/300\n",
      "Average training loss: 0.016775361023015446\n",
      "Average test loss: 0.0032099079901559487\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01629909998509619\n",
      "Average test loss: 0.0031703225566695136\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01588693689637714\n",
      "Average test loss: 0.00303144123363826\n",
      "Epoch 7/300\n",
      "Average training loss: 0.015526989133821594\n",
      "Average test loss: 0.002952940652664337\n",
      "Epoch 8/300\n",
      "Average training loss: 0.015213857787350814\n",
      "Average test loss: 0.002895977729724513\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014927903794580036\n",
      "Average test loss: 0.002912922744328777\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01467557911740409\n",
      "Average test loss: 0.0028451961361699632\n",
      "Epoch 11/300\n",
      "Average training loss: 0.014447441943817668\n",
      "Average test loss: 0.002817294686825739\n",
      "Epoch 12/300\n",
      "Average training loss: 0.014250674752725494\n",
      "Average test loss: 0.0028365196761571698\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01408875099486775\n",
      "Average test loss: 0.0026585549604561595\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013923407015701135\n",
      "Average test loss: 0.002657367093074653\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013782576724886895\n",
      "Average test loss: 0.0027601643407510388\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013663458142843511\n",
      "Average test loss: 0.0026080083958804606\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013557317468027274\n",
      "Average test loss: 0.0025848946997688877\n",
      "Epoch 18/300\n",
      "Average training loss: 0.013461855649120278\n",
      "Average test loss: 0.0025616348463421068\n",
      "Epoch 19/300\n",
      "Average training loss: 0.013359574707018004\n",
      "Average test loss: 0.0025223792921751738\n",
      "Epoch 20/300\n",
      "Average training loss: 0.013267625632385413\n",
      "Average test loss: 0.0025256959471023745\n",
      "Epoch 21/300\n",
      "Average training loss: 0.013206360300381978\n",
      "Average test loss: 0.002496572480640478\n",
      "Epoch 22/300\n",
      "Average training loss: 0.013118434663448069\n",
      "Average test loss: 0.0025167617520524396\n",
      "Epoch 23/300\n",
      "Average training loss: 0.013064787983066506\n",
      "Average test loss: 0.0024842660389840605\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012995942773918311\n",
      "Average test loss: 0.0024981994355718295\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0129561425447464\n",
      "Average test loss: 0.0024679423299514584\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012891659982502461\n",
      "Average test loss: 0.002466803900483582\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012839524683852991\n",
      "Average test loss: 0.0024494049594634107\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012796989321708679\n",
      "Average test loss: 0.002430110238492489\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012749379605882698\n",
      "Average test loss: 0.0024448182001296018\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012705698691308498\n",
      "Average test loss: 0.0024219552541358603\n",
      "Epoch 31/300\n",
      "Average training loss: 0.012658138535088964\n",
      "Average test loss: 0.0024000661969184873\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01263204577482409\n",
      "Average test loss: 0.0024217714881524443\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012595443150235548\n",
      "Average test loss: 0.002404915247940355\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012542826984491613\n",
      "Average test loss: 0.002395734002813697\n",
      "Epoch 35/300\n",
      "Average training loss: 0.012512886012593906\n",
      "Average test loss: 0.002402766661718488\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012486905710150799\n",
      "Average test loss: 0.0023910136761971647\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01244245510134432\n",
      "Average test loss: 0.002408321754592988\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012421968940231535\n",
      "Average test loss: 0.0023858003423859677\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012395476235283746\n",
      "Average test loss: 0.0024026337216297784\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012370739358166854\n",
      "Average test loss: 0.002372585487138066\n",
      "Epoch 41/300\n",
      "Average training loss: 0.012334152768055598\n",
      "Average test loss: 0.0023671331968572406\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012298372010389963\n",
      "Average test loss: 0.0024004816549519697\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012293082118862205\n",
      "Average test loss: 0.0023722973954346444\n",
      "Epoch 44/300\n",
      "Average training loss: 0.012254171828428905\n",
      "Average test loss: 0.002382169305864308\n",
      "Epoch 45/300\n",
      "Average training loss: 0.012228269043482013\n",
      "Average test loss: 0.002442404079871873\n",
      "Epoch 46/300\n",
      "Average training loss: 0.012203659582469198\n",
      "Average test loss: 0.0023950962821642557\n",
      "Epoch 47/300\n",
      "Average training loss: 0.012171570704215103\n",
      "Average test loss: 0.0024023164825306997\n",
      "Epoch 48/300\n",
      "Average training loss: 0.012161830816831854\n",
      "Average test loss: 0.0023583678832898537\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01213657218300634\n",
      "Average test loss: 0.002372764101665881\n",
      "Epoch 50/300\n",
      "Average training loss: 0.012120965965092182\n",
      "Average test loss: 0.0023945568869304327\n",
      "Epoch 51/300\n",
      "Average training loss: 0.012089941072795126\n",
      "Average test loss: 0.002359524902680682\n",
      "Epoch 52/300\n",
      "Average training loss: 0.012070984316368898\n",
      "Average test loss: 0.0023734832391556766\n",
      "Epoch 53/300\n",
      "Average training loss: 0.012055453836917878\n",
      "Average test loss: 0.0023540566315253574\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01202321458607912\n",
      "Average test loss: 0.002372043810267415\n",
      "Epoch 55/300\n",
      "Average training loss: 0.012002503429849943\n",
      "Average test loss: 0.002395407169022494\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011982922548221217\n",
      "Average test loss: 0.0024113461086526515\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011975879080593586\n",
      "Average test loss: 0.0023757118413017856\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011943246930837632\n",
      "Average test loss: 0.0023669336844856542\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011923450886375375\n",
      "Average test loss: 0.002378810391657882\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011905038062896993\n",
      "Average test loss: 0.0024087195662367677\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011887255783710214\n",
      "Average test loss: 0.002360687416460779\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011864191084272332\n",
      "Average test loss: 0.0024115826359225644\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011848310457335578\n",
      "Average test loss: 0.002357690165129801\n",
      "Epoch 64/300\n",
      "Average training loss: 0.011834809181590874\n",
      "Average test loss: 0.0024001966164343886\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011813562980956501\n",
      "Average test loss: 0.002385204443294141\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011800168621871207\n",
      "Average test loss: 0.002384051913809445\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01177580892874135\n",
      "Average test loss: 0.0023780074959827793\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011770970957146751\n",
      "Average test loss: 0.0023656228313015566\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011763006984359689\n",
      "Average test loss: 0.0023678465839475395\n",
      "Epoch 70/300\n",
      "Average training loss: 0.011730633224050204\n",
      "Average test loss: 0.002367549819043941\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01170964344425334\n",
      "Average test loss: 0.002370668587792251\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01170588347564141\n",
      "Average test loss: 0.0024276630779107413\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011672005282508002\n",
      "Average test loss: 0.0024536082794268926\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011666737649175855\n",
      "Average test loss: 0.0023581709367119603\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011654231453107462\n",
      "Average test loss: 0.002368945469044977\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0116319791401426\n",
      "Average test loss: 0.002389655644694964\n",
      "Epoch 77/300\n",
      "Average training loss: 0.011615752396484215\n",
      "Average test loss: 0.0023958030562433934\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011602674779792627\n",
      "Average test loss: 0.002386111367907789\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011587741335233052\n",
      "Average test loss: 0.002416334254046281\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011567620444628928\n",
      "Average test loss: 0.0024272622778597807\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011571579568915897\n",
      "Average test loss: 0.002433652025957902\n",
      "Epoch 82/300\n",
      "Average training loss: 0.011535162685645951\n",
      "Average test loss: 0.0023754414226859807\n",
      "Epoch 83/300\n",
      "Average training loss: 0.011529325875971053\n",
      "Average test loss: 0.002434353131490449\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011518496317168076\n",
      "Average test loss: 0.0024061068722771276\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01151183629532655\n",
      "Average test loss: 0.002445083741305603\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011486959673464298\n",
      "Average test loss: 0.002439733492831389\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011474941688279311\n",
      "Average test loss: 0.0024071654590467613\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011457769403027163\n",
      "Average test loss: 0.002369750239368942\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011446236033406523\n",
      "Average test loss: 0.002437092454483112\n",
      "Epoch 90/300\n",
      "Average training loss: 0.011440322891705566\n",
      "Average test loss: 0.002399814210832119\n",
      "Epoch 91/300\n",
      "Average training loss: 0.011420561247815689\n",
      "Average test loss: 0.00243021573457453\n",
      "Epoch 92/300\n",
      "Average training loss: 0.011407378235624896\n",
      "Average test loss: 0.0023842155219366153\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011402917078799672\n",
      "Average test loss: 0.0024043548078172736\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011385424804356363\n",
      "Average test loss: 0.002395854516679214\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011379287417564127\n",
      "Average test loss: 0.0024285896480497385\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011352907929983403\n",
      "Average test loss: 0.00239453119577633\n",
      "Epoch 97/300\n",
      "Average training loss: 0.011356446899473667\n",
      "Average test loss: 0.0024209632424430716\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01133730541417996\n",
      "Average test loss: 0.002445598331797454\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011330767082671325\n",
      "Average test loss: 0.0023954790168338354\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01132266177982092\n",
      "Average test loss: 0.002399656734739741\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011305111757583088\n",
      "Average test loss: 0.002420238013586236\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011283804857068591\n",
      "Average test loss: 0.002435736181421412\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011293081043495072\n",
      "Average test loss: 0.0024159907220552366\n",
      "Epoch 104/300\n",
      "Average training loss: 0.011277346822950575\n",
      "Average test loss: 0.0023678263877001072\n",
      "Epoch 105/300\n",
      "Average training loss: 0.011264891742004289\n",
      "Average test loss: 0.002460896062354247\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011249660010139148\n",
      "Average test loss: 0.002420865987944934\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011242438241011567\n",
      "Average test loss: 0.002421658654179838\n",
      "Epoch 108/300\n",
      "Average training loss: 0.01122875789552927\n",
      "Average test loss: 0.002489037518699964\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011226821133659946\n",
      "Average test loss: 0.0023944191820919514\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011200060390763813\n",
      "Average test loss: 0.002403312518985735\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011194062126179537\n",
      "Average test loss: 0.0024314874454090993\n",
      "Epoch 112/300\n",
      "Average training loss: 0.011192764464351865\n",
      "Average test loss: 0.0024736781494898927\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011175112553768688\n",
      "Average test loss: 0.002465166892028517\n",
      "Epoch 114/300\n",
      "Average training loss: 0.01116518915941318\n",
      "Average test loss: 0.002546270444161362\n",
      "Epoch 115/300\n",
      "Average training loss: 0.011151289600465034\n",
      "Average test loss: 0.00242249860893935\n",
      "Epoch 116/300\n",
      "Average training loss: 0.011153417515257995\n",
      "Average test loss: 0.0024211463566041654\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011151690393686294\n",
      "Average test loss: 0.0024324876382533045\n",
      "Epoch 118/300\n",
      "Average training loss: 0.011146916373736328\n",
      "Average test loss: 0.0024510194195641413\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011123135231435299\n",
      "Average test loss: 0.002456165162432525\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01111034444760945\n",
      "Average test loss: 0.0024486939077162082\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011104588101307551\n",
      "Average test loss: 0.002472970804406537\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011079623635444376\n",
      "Average test loss: 0.002584915954516166\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011084745183587074\n",
      "Average test loss: 0.0024625513810767894\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011076781052682136\n",
      "Average test loss: 0.002393193426438504\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01106568420264456\n",
      "Average test loss: 0.002391806754283607\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011052755560312006\n",
      "Average test loss: 0.0024127619073001875\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011064265205628343\n",
      "Average test loss: 0.0024322437859243817\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011054138712584972\n",
      "Average test loss: 0.0024959396820308432\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011048213962051603\n",
      "Average test loss: 0.0024811017983075645\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011024384257280164\n",
      "Average test loss: 0.002531248224278291\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011005079189108478\n",
      "Average test loss: 0.002571552224043343\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011009371057980591\n",
      "Average test loss: 0.002456884174504214\n",
      "Epoch 133/300\n",
      "Average training loss: 0.010991112867163286\n",
      "Average test loss: 0.002579288590285513\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011002070469160874\n",
      "Average test loss: 0.002462859920743439\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01098992590771781\n",
      "Average test loss: 0.002508183978911903\n",
      "Epoch 136/300\n",
      "Average training loss: 0.010982333961460325\n",
      "Average test loss: 0.0026735334319786894\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010964053962793615\n",
      "Average test loss: 0.0024337584500511485\n",
      "Epoch 138/300\n",
      "Average training loss: 0.010958564940426085\n",
      "Average test loss: 0.0024599667971746788\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010950358373423418\n",
      "Average test loss: 0.002511711636144254\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01095778477895591\n",
      "Average test loss: 0.002494465571517746\n",
      "Epoch 141/300\n",
      "Average training loss: 0.010940070558753278\n",
      "Average test loss: 0.0024181595165282486\n",
      "Epoch 142/300\n",
      "Average training loss: 0.010929737812115087\n",
      "Average test loss: 0.0024527288967122634\n",
      "Epoch 143/300\n",
      "Average training loss: 0.010914999289645089\n",
      "Average test loss: 0.0024890181134558384\n",
      "Epoch 144/300\n",
      "Average training loss: 0.010923450162013371\n",
      "Average test loss: 0.0024339608347250354\n",
      "Epoch 145/300\n",
      "Average training loss: 0.010937058449619346\n",
      "Average test loss: 0.00245410487242043\n",
      "Epoch 146/300\n",
      "Average training loss: 0.010919064240323172\n",
      "Average test loss: 0.002535224124168356\n",
      "Epoch 147/300\n",
      "Average training loss: 0.010904833041959338\n",
      "Average test loss: 0.0024728762803392277\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01087840295334657\n",
      "Average test loss: 0.0025078351522485414\n",
      "Epoch 149/300\n",
      "Average training loss: 0.010881679803960853\n",
      "Average test loss: 0.0025021006111055613\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01089556447416544\n",
      "Average test loss: 0.0024883800773984857\n",
      "Epoch 151/300\n",
      "Average training loss: 0.0108686056509614\n",
      "Average test loss: 0.002479970737049977\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01087766895112064\n",
      "Average test loss: 0.002440824299843775\n",
      "Epoch 153/300\n",
      "Average training loss: 0.010874498425258531\n",
      "Average test loss: 0.00253981748678618\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01085357884152068\n",
      "Average test loss: 0.002524457684200671\n",
      "Epoch 155/300\n",
      "Average training loss: 0.010846144945257239\n",
      "Average test loss: 0.002464318431913853\n",
      "Epoch 156/300\n",
      "Average training loss: 0.010838600343300236\n",
      "Average test loss: 0.002509279216846658\n",
      "Epoch 157/300\n",
      "Average training loss: 0.010839178525739245\n",
      "Average test loss: 0.0024825241286307574\n",
      "Epoch 158/300\n",
      "Average training loss: 0.010842805113229487\n",
      "Average test loss: 0.0024676656799597872\n",
      "Epoch 159/300\n",
      "Average training loss: 0.010821896789802445\n",
      "Average test loss: 0.002446261695689625\n",
      "Epoch 160/300\n",
      "Average training loss: 0.010813388336863782\n",
      "Average test loss: 0.0025384375877264475\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0108205599900749\n",
      "Average test loss: 0.0025289841861360603\n",
      "Epoch 162/300\n",
      "Average training loss: 0.010805937423474259\n",
      "Average test loss: 0.002452367905734314\n",
      "Epoch 163/300\n",
      "Average training loss: 0.01080343637532658\n",
      "Average test loss: 0.002582628982555535\n",
      "Epoch 164/300\n",
      "Average training loss: 0.010791907240119246\n",
      "Average test loss: 0.0024767292026016447\n",
      "Epoch 165/300\n",
      "Average training loss: 0.010800509231785934\n",
      "Average test loss: 0.0024616277823224664\n",
      "Epoch 166/300\n",
      "Average training loss: 0.010801538773708874\n",
      "Average test loss: 0.002502414466192325\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010774169724848535\n",
      "Average test loss: 0.00247438170771218\n",
      "Epoch 168/300\n",
      "Average training loss: 0.010761289115581248\n",
      "Average test loss: 0.002646433938294649\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010773906364209123\n",
      "Average test loss: 0.002499712649939789\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010765070141189628\n",
      "Average test loss: 0.0026432832500172987\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0107585697364476\n",
      "Average test loss: 0.002471041647510396\n",
      "Epoch 172/300\n",
      "Average training loss: 0.010741659242245885\n",
      "Average test loss: 0.002519865305887328\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010757517942123943\n",
      "Average test loss: 0.002502101767187317\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01075182448575894\n",
      "Average test loss: 0.0025066853291872476\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01073559820983145\n",
      "Average test loss: 0.0025716023443059788\n",
      "Epoch 176/300\n",
      "Average training loss: 0.010736989272137483\n",
      "Average test loss: 0.002482896779767341\n",
      "Epoch 177/300\n",
      "Average training loss: 0.010725326611763902\n",
      "Average test loss: 0.0024829669825525745\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010721567018992371\n",
      "Average test loss: 0.0024669783045020367\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01072137853089306\n",
      "Average test loss: 0.002472573617680205\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010702524422771401\n",
      "Average test loss: 0.002478816659707162\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010723501139216952\n",
      "Average test loss: 0.0025267996042966845\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010700062810546822\n",
      "Average test loss: 0.0025404836076001326\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010689235640068849\n",
      "Average test loss: 0.0024197269009633197\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010689476039674546\n",
      "Average test loss: 0.002515060218464997\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010689165393511455\n",
      "Average test loss: 0.0025204456123626893\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0106832621494929\n",
      "Average test loss: 0.0024719534292817116\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010680892106559542\n",
      "Average test loss: 0.0024889186665208803\n",
      "Epoch 188/300\n",
      "Average training loss: 0.010684284869995382\n",
      "Average test loss: 0.002513996712035603\n",
      "Epoch 189/300\n",
      "Average training loss: 0.010660937309265137\n",
      "Average test loss: 0.0025446453717433743\n",
      "Epoch 190/300\n",
      "Average training loss: 0.010661768272519112\n",
      "Average test loss: 0.0025194391982836855\n",
      "Epoch 191/300\n",
      "Average training loss: 0.010670188015533818\n",
      "Average test loss: 0.00256238799314532\n",
      "Epoch 192/300\n",
      "Average training loss: 0.010675373891161547\n",
      "Average test loss: 0.0025050980592560435\n",
      "Epoch 193/300\n",
      "Average training loss: 0.010640511195692752\n",
      "Average test loss: 0.0024991141876412764\n",
      "Epoch 194/300\n",
      "Average training loss: 0.010637949239048693\n",
      "Average test loss: 0.002499938487592671\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01065334139433172\n",
      "Average test loss: 0.00244733702018857\n",
      "Epoch 196/300\n",
      "Average training loss: 0.010633716603120168\n",
      "Average test loss: 0.0024931452304331795\n",
      "Epoch 197/300\n",
      "Average training loss: 0.010621902585857445\n",
      "Average test loss: 0.0024745937927315633\n",
      "Epoch 198/300\n",
      "Average training loss: 0.010629338974754016\n",
      "Average test loss: 0.0025238595352404646\n",
      "Epoch 199/300\n",
      "Average training loss: 0.010624222204089165\n",
      "Average test loss: 0.002840851066634059\n",
      "Epoch 200/300\n",
      "Average training loss: 0.010620356381767325\n",
      "Average test loss: 0.0025446402940692173\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01061139450305038\n",
      "Average test loss: 0.002555473925752772\n",
      "Epoch 202/300\n",
      "Average training loss: 0.010616643990079562\n",
      "Average test loss: 0.0025447401189141803\n",
      "Epoch 203/300\n",
      "Average training loss: 0.010628137556215127\n",
      "Average test loss: 0.0025047991120566923\n",
      "Epoch 204/300\n",
      "Average training loss: 0.010605682529509067\n",
      "Average test loss: 0.002587531340205007\n",
      "Epoch 205/300\n",
      "Average training loss: 0.010612792015903526\n",
      "Average test loss: 0.0024523717457842495\n",
      "Epoch 206/300\n",
      "Average training loss: 0.010587156722942989\n",
      "Average test loss: 0.0024917208206736378\n",
      "Epoch 207/300\n",
      "Average training loss: 0.010598183661699295\n",
      "Average test loss: 0.0025072244892103803\n",
      "Epoch 208/300\n",
      "Average training loss: 0.010605610166986783\n",
      "Average test loss: 0.002535316249355674\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01057811619920863\n",
      "Average test loss: 0.002501577140763402\n",
      "Epoch 210/300\n",
      "Average training loss: 0.010582509905099869\n",
      "Average test loss: 0.0024900452020681567\n",
      "Epoch 211/300\n",
      "Average training loss: 0.010583798020250267\n",
      "Average test loss: 0.0025469768873105448\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01057313321530819\n",
      "Average test loss: 0.0024781057606968614\n",
      "Epoch 213/300\n",
      "Average training loss: 0.010571146437691317\n",
      "Average test loss: 0.0026095046487947305\n",
      "Epoch 214/300\n",
      "Average training loss: 0.010569139936731921\n",
      "Average test loss: 0.002657075429128276\n",
      "Epoch 215/300\n",
      "Average training loss: 0.010559036694467068\n",
      "Average test loss: 0.0024938040371570323\n",
      "Epoch 216/300\n",
      "Average training loss: 0.010554030501180225\n",
      "Average test loss: 0.0025476030287229353\n",
      "Epoch 217/300\n",
      "Average training loss: 0.010562912240624427\n",
      "Average test loss: 0.0025085997230683763\n",
      "Epoch 218/300\n",
      "Average training loss: 0.010553851366870932\n",
      "Average test loss: 0.0025474136833929352\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01054971052830418\n",
      "Average test loss: 0.002557839181688097\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010562055383291509\n",
      "Average test loss: 0.002487203379058176\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010552775049375164\n",
      "Average test loss: 0.0025807429456876385\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01053898765767614\n",
      "Average test loss: 0.0026223534947882095\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010539125276936425\n",
      "Average test loss: 0.002499309873001443\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010530467863712046\n",
      "Average test loss: 0.0026257381989724105\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010538212933474117\n",
      "Average test loss: 0.0025660104180375734\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010526693460014131\n",
      "Average test loss: 0.0025701866454134383\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010537048792673483\n",
      "Average test loss: 0.0025379331592056487\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010528355419635772\n",
      "Average test loss: 0.002624223364517093\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010519845078802771\n",
      "Average test loss: 0.00266154225046436\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010512374521957503\n",
      "Average test loss: 0.002479636641426219\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010506804952190982\n",
      "Average test loss: 0.0025172244858824543\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010510412502619955\n",
      "Average test loss: 0.0024689260826756556\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01050679523580604\n",
      "Average test loss: 0.002637494390106036\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01049864302906725\n",
      "Average test loss: 0.0025875365665803353\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010496123517553012\n",
      "Average test loss: 0.002542385432869196\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010487345358563794\n",
      "Average test loss: 0.002538294020212359\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010495699149866899\n",
      "Average test loss: 0.0026159511575889255\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010497850961155361\n",
      "Average test loss: 0.002618676856574085\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01048730738957723\n",
      "Average test loss: 0.002577637425520354\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010474400472309854\n",
      "Average test loss: 0.0026085393944134316\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010476169519126416\n",
      "Average test loss: 0.0024973930397795305\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010484646105104023\n",
      "Average test loss: 0.002531821564460794\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010477166338099374\n",
      "Average test loss: 0.002530777747846312\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010460701790120867\n",
      "Average test loss: 0.002638693399520384\n",
      "Epoch 245/300\n",
      "Average training loss: 0.010472925417125224\n",
      "Average test loss: 0.0025535081228655245\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010470295896132787\n",
      "Average test loss: 0.0025732490085065363\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010463474811779129\n",
      "Average test loss: 0.002569274314058324\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010451314445585013\n",
      "Average test loss: 0.002573864365203513\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01045824353562461\n",
      "Average test loss: 0.002501317266892228\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010458648004051711\n",
      "Average test loss: 0.002620259469581975\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010455493301981025\n",
      "Average test loss: 0.0024971550808598596\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010450840042697059\n",
      "Average test loss: 0.0025764060140483908\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010441124542719789\n",
      "Average test loss: 0.0025364954990024367\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010453118099934525\n",
      "Average test loss: 0.0025523102020637858\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010442434108091725\n",
      "Average test loss: 0.0025261983652081752\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010443062209420734\n",
      "Average test loss: 0.002521732609305117\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01042994455082549\n",
      "Average test loss: 0.002500472435106834\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010433610464135806\n",
      "Average test loss: 0.00252856964048826\n",
      "Epoch 259/300\n",
      "Average training loss: 0.010425445512764983\n",
      "Average test loss: 0.002475519302404589\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010434898213379912\n",
      "Average test loss: 0.0025318923141393395\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010417289780245887\n",
      "Average test loss: 0.002560369374437465\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010430683282514413\n",
      "Average test loss: 0.002573901037271652\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010419126728342639\n",
      "Average test loss: 0.002542552349054151\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01041303972568777\n",
      "Average test loss: 0.002550058651922478\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010416913468804625\n",
      "Average test loss: 0.002531079209720095\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010410301469266414\n",
      "Average test loss: 0.0025524277887824507\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01040335249569681\n",
      "Average test loss: 0.0026207675794139504\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010402780088285605\n",
      "Average test loss: 0.002516598462851511\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010421515569090843\n",
      "Average test loss: 0.002620142473321822\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01041676325764921\n",
      "Average test loss: 0.002539950086735189\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010394443889458975\n",
      "Average test loss: 0.002598479474377301\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010397163272731835\n",
      "Average test loss: 0.0025735225807875396\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010391914765867922\n",
      "Average test loss: 0.002673247286842929\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010390240773558617\n",
      "Average test loss: 0.0025938156073292097\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01039600335723824\n",
      "Average test loss: 0.0025303446031692957\n",
      "Epoch 276/300\n",
      "Average training loss: 0.010392459962930945\n",
      "Average test loss: 0.002567038213730686\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01038007785462671\n",
      "Average test loss: 0.002598443928692076\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01039442405766911\n",
      "Average test loss: 0.002659275391863452\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01038136358683308\n",
      "Average test loss: 0.0025925126468969717\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010369787120156819\n",
      "Average test loss: 0.0025078323916014698\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010367024927503533\n",
      "Average test loss: 0.0025950567231824\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01037573205596871\n",
      "Average test loss: 0.0026047924239602353\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010364717021584511\n",
      "Average test loss: 0.0026629434236221843\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010378008777896562\n",
      "Average test loss: 0.0025028370052783025\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010364059486322933\n",
      "Average test loss: 0.0025205841534253625\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01036374845272965\n",
      "Average test loss: 0.0025877567442754903\n",
      "Epoch 287/300\n",
      "Average training loss: 0.010361334103677008\n",
      "Average test loss: 0.002576336831992699\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010355375459624661\n",
      "Average test loss: 0.002557581634157234\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010355616629123687\n",
      "Average test loss: 0.002586448915509714\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01035787365006076\n",
      "Average test loss: 0.0025915648767517673\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01036277451283402\n",
      "Average test loss: 0.0025843342639919786\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010332653951313761\n",
      "Average test loss: 0.0025559932077303528\n",
      "Epoch 293/300\n",
      "Average training loss: 0.010335722511013348\n",
      "Average test loss: 0.002550884390456809\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010350838476585017\n",
      "Average test loss: 0.002691338347685006\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010337067960864968\n",
      "Average test loss: 0.0026273203670150705\n",
      "Epoch 296/300\n",
      "Average training loss: 0.010346326961285538\n",
      "Average test loss: 0.002580626827561193\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01034197675353951\n",
      "Average test loss: 0.0026892761724690594\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010339738255573643\n",
      "Average test loss: 0.0024799295220937993\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010335164313515027\n",
      "Average test loss: 0.0025274261521796384\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010337730930911171\n",
      "Average test loss: 0.0025638488059242567\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.046172283722294705\n",
      "Average test loss: 0.003445481794161929\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01566985854672061\n",
      "Average test loss: 0.002963564118163453\n",
      "Epoch 3/300\n",
      "Average training loss: 0.014550170855389701\n",
      "Average test loss: 0.002894448290682501\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013875845563908418\n",
      "Average test loss: 0.0026460708737787273\n",
      "Epoch 5/300\n",
      "Average training loss: 0.013367752186954022\n",
      "Average test loss: 0.002494090908931361\n",
      "Epoch 6/300\n",
      "Average training loss: 0.01292043099221256\n",
      "Average test loss: 0.0024075618982315064\n",
      "Epoch 7/300\n",
      "Average training loss: 0.012559309830268223\n",
      "Average test loss: 0.0023215819430641004\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01222004286117024\n",
      "Average test loss: 0.0022737737740907405\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011925713558163908\n",
      "Average test loss: 0.002218921057259043\n",
      "Epoch 10/300\n",
      "Average training loss: 0.011661286457545227\n",
      "Average test loss: 0.002138352795193593\n",
      "Epoch 11/300\n",
      "Average training loss: 0.011442412421107292\n",
      "Average test loss: 0.002093369809186293\n",
      "Epoch 12/300\n",
      "Average training loss: 0.011246362861659791\n",
      "Average test loss: 0.0021490064290248683\n",
      "Epoch 13/300\n",
      "Average training loss: 0.011075783354540666\n",
      "Average test loss: 0.002028043292876747\n",
      "Epoch 14/300\n",
      "Average training loss: 0.010932255514793926\n",
      "Average test loss: 0.002026353829850753\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010800568838914235\n",
      "Average test loss: 0.0019884358919743036\n",
      "Epoch 16/300\n",
      "Average training loss: 0.010666541836327977\n",
      "Average test loss: 0.001952195342319707\n",
      "Epoch 17/300\n",
      "Average training loss: 0.010576401062309742\n",
      "Average test loss: 0.0019521879496880703\n",
      "Epoch 18/300\n",
      "Average training loss: 0.010472204570141103\n",
      "Average test loss: 0.0019245259650051594\n",
      "Epoch 19/300\n",
      "Average training loss: 0.010387362947066625\n",
      "Average test loss: 0.0019200193212471075\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0103176442798641\n",
      "Average test loss: 0.0018875071775789063\n",
      "Epoch 21/300\n",
      "Average training loss: 0.010241462670266628\n",
      "Average test loss: 0.001876205724457072\n",
      "Epoch 22/300\n",
      "Average training loss: 0.010179896365851164\n",
      "Average test loss: 0.0018777971677482127\n",
      "Epoch 23/300\n",
      "Average training loss: 0.010118911870651774\n",
      "Average test loss: 0.0018509192341524695\n",
      "Epoch 24/300\n",
      "Average training loss: 0.010068612530827522\n",
      "Average test loss: 0.0018789214692595932\n",
      "Epoch 25/300\n",
      "Average training loss: 0.010014035354057947\n",
      "Average test loss: 0.0018643753169518378\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009966703440580102\n",
      "Average test loss: 0.0018538511631389458\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009921844329271052\n",
      "Average test loss: 0.0018330643988317913\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009884999079422817\n",
      "Average test loss: 0.0018435180673582686\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009838087547984388\n",
      "Average test loss: 0.0018183987036140428\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009813438807096746\n",
      "Average test loss: 0.0018043758829848634\n",
      "Epoch 31/300\n",
      "Average training loss: 0.009769439340465598\n",
      "Average test loss: 0.0017927993177953693\n",
      "Epoch 32/300\n",
      "Average training loss: 0.009739119114975135\n",
      "Average test loss: 0.0018053361603783237\n",
      "Epoch 33/300\n",
      "Average training loss: 0.009709015598727597\n",
      "Average test loss: 0.0018007078004173107\n",
      "Epoch 34/300\n",
      "Average training loss: 0.009684310437076621\n",
      "Average test loss: 0.001807528719616433\n",
      "Epoch 35/300\n",
      "Average training loss: 0.009652160874671406\n",
      "Average test loss: 0.0018307343491663535\n",
      "Epoch 36/300\n",
      "Average training loss: 0.009618662500547038\n",
      "Average test loss: 0.001787722725628151\n",
      "Epoch 37/300\n",
      "Average training loss: 0.009597161076135105\n",
      "Average test loss: 0.0017761973920795652\n",
      "Epoch 38/300\n",
      "Average training loss: 0.009575571201741696\n",
      "Average test loss: 0.0017741816908948952\n",
      "Epoch 39/300\n",
      "Average training loss: 0.009545409294466178\n",
      "Average test loss: 0.001779354104358289\n",
      "Epoch 40/300\n",
      "Average training loss: 0.009524433143436909\n",
      "Average test loss: 0.001778587337790264\n",
      "Epoch 41/300\n",
      "Average training loss: 0.009503911401662561\n",
      "Average test loss: 0.0017740028258413077\n",
      "Epoch 42/300\n",
      "Average training loss: 0.009477581948869758\n",
      "Average test loss: 0.001774843637728029\n",
      "Epoch 43/300\n",
      "Average training loss: 0.009459695782926348\n",
      "Average test loss: 0.0017599643210156096\n",
      "Epoch 44/300\n",
      "Average training loss: 0.009434863665037685\n",
      "Average test loss: 0.0017653086782536574\n",
      "Epoch 45/300\n",
      "Average training loss: 0.00941645354943143\n",
      "Average test loss: 0.0017749095836447344\n",
      "Epoch 46/300\n",
      "Average training loss: 0.009403348600284921\n",
      "Average test loss: 0.0017654744835777416\n",
      "Epoch 47/300\n",
      "Average training loss: 0.009378471025990116\n",
      "Average test loss: 0.001765460623221265\n",
      "Epoch 48/300\n",
      "Average training loss: 0.009357608319984542\n",
      "Average test loss: 0.0017615787557636697\n",
      "Epoch 49/300\n",
      "Average training loss: 0.009341750209530195\n",
      "Average test loss: 0.0017546473247930408\n",
      "Epoch 50/300\n",
      "Average training loss: 0.009333978013032013\n",
      "Average test loss: 0.0017500100383121106\n",
      "Epoch 51/300\n",
      "Average training loss: 0.009319973693125778\n",
      "Average test loss: 0.0017692319571764932\n",
      "Epoch 52/300\n",
      "Average training loss: 0.009285184539854526\n",
      "Average test loss: 0.001753940388146374\n",
      "Epoch 53/300\n",
      "Average training loss: 0.00927295871824026\n",
      "Average test loss: 0.0017427519427405463\n",
      "Epoch 54/300\n",
      "Average training loss: 0.009244959437598784\n",
      "Average test loss: 0.0017692842634601726\n",
      "Epoch 55/300\n",
      "Average training loss: 0.00924490767510401\n",
      "Average test loss: 0.0017816078718751668\n",
      "Epoch 56/300\n",
      "Average training loss: 0.009221984417074256\n",
      "Average test loss: 0.00175683902307517\n",
      "Epoch 57/300\n",
      "Average training loss: 0.009211345353060299\n",
      "Average test loss: 0.0017469136754257812\n",
      "Epoch 58/300\n",
      "Average training loss: 0.009186684302985668\n",
      "Average test loss: 0.001767346637116538\n",
      "Epoch 59/300\n",
      "Average training loss: 0.009184605110436677\n",
      "Average test loss: 0.0017577773488851058\n",
      "Epoch 60/300\n",
      "Average training loss: 0.00917014863445527\n",
      "Average test loss: 0.0017912258375436068\n",
      "Epoch 61/300\n",
      "Average training loss: 0.009144029457536009\n",
      "Average test loss: 0.0017571693623645438\n",
      "Epoch 62/300\n",
      "Average training loss: 0.009136273259917895\n",
      "Average test loss: 0.0018439768937726815\n",
      "Epoch 63/300\n",
      "Average training loss: 0.009115270130336285\n",
      "Average test loss: 0.0017393375729314155\n",
      "Epoch 64/300\n",
      "Average training loss: 0.009101893781373898\n",
      "Average test loss: 0.00176422904100683\n",
      "Epoch 65/300\n",
      "Average training loss: 0.009086587389724123\n",
      "Average test loss: 0.001753732161389457\n",
      "Epoch 66/300\n",
      "Average training loss: 0.009087854501273897\n",
      "Average test loss: 0.0017378425871332486\n",
      "Epoch 67/300\n",
      "Average training loss: 0.009059573306391637\n",
      "Average test loss: 0.0017771653588861227\n",
      "Epoch 68/300\n",
      "Average training loss: 0.009052354416085614\n",
      "Average test loss: 0.0017465258276917867\n",
      "Epoch 69/300\n",
      "Average training loss: 0.00903848772495985\n",
      "Average test loss: 0.00175798824781345\n",
      "Epoch 70/300\n",
      "Average training loss: 0.009037197161879805\n",
      "Average test loss: 0.0017540465558154715\n",
      "Epoch 71/300\n",
      "Average training loss: 0.009013871737238433\n",
      "Average test loss: 0.001754631217983034\n",
      "Epoch 72/300\n",
      "Average training loss: 0.009001812388499578\n",
      "Average test loss: 0.001775221678117911\n",
      "Epoch 73/300\n",
      "Average training loss: 0.00899185912642214\n",
      "Average test loss: 0.001744646767206076\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008984771221048302\n",
      "Average test loss: 0.001793515397856633\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008970225547750791\n",
      "Average test loss: 0.0017818425991055038\n",
      "Epoch 76/300\n",
      "Average training loss: 0.00895346474647522\n",
      "Average test loss: 0.001755737352391912\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008954957393722401\n",
      "Average test loss: 0.0017678411423953043\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008926673565473822\n",
      "Average test loss: 0.0017708332154692875\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008921168749531111\n",
      "Average test loss: 0.0017664870170669422\n",
      "Epoch 80/300\n",
      "Average training loss: 0.00890268845649229\n",
      "Average test loss: 0.0018016378148976299\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008901473350822926\n",
      "Average test loss: 0.0017909315413691931\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008885658898287348\n",
      "Average test loss: 0.0017423558512495623\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008904833795709742\n",
      "Average test loss: 0.0017870420242349307\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008867210076914893\n",
      "Average test loss: 0.0017810829370799991\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008855093832645152\n",
      "Average test loss: 0.0017411305202792088\n",
      "Epoch 86/300\n",
      "Average training loss: 0.008842614214453433\n",
      "Average test loss: 0.001751065905102425\n",
      "Epoch 87/300\n",
      "Average training loss: 0.00883560982822544\n",
      "Average test loss: 0.0017890446010149187\n",
      "Epoch 88/300\n",
      "Average training loss: 0.008824208315047953\n",
      "Average test loss: 0.0017460933232473002\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008820004515349865\n",
      "Average test loss: 0.0018119468931108713\n",
      "Epoch 90/300\n",
      "Average training loss: 0.008809531864606672\n",
      "Average test loss: 0.0017929134495142432\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008799167824288209\n",
      "Average test loss: 0.001764750375929806\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008795006840593285\n",
      "Average test loss: 0.0017744990825239156\n",
      "Epoch 93/300\n",
      "Average training loss: 0.008775509174499247\n",
      "Average test loss: 0.0017515558492806223\n",
      "Epoch 94/300\n",
      "Average training loss: 0.008768226436442799\n",
      "Average test loss: 0.0017618270521569583\n",
      "Epoch 95/300\n",
      "Average training loss: 0.008768665639890564\n",
      "Average test loss: 0.0017977297080473767\n",
      "Epoch 96/300\n",
      "Average training loss: 0.008757911203636063\n",
      "Average test loss: 0.0018079974040802983\n",
      "Epoch 97/300\n",
      "Average training loss: 0.00874417974303166\n",
      "Average test loss: 0.0017969794513450728\n",
      "Epoch 98/300\n",
      "Average training loss: 0.008729615660591258\n",
      "Average test loss: 0.0017921851062112383\n",
      "Epoch 99/300\n",
      "Average training loss: 0.008717945979287227\n",
      "Average test loss: 0.0018205272051402264\n",
      "Epoch 100/300\n",
      "Average training loss: 0.008713681003285779\n",
      "Average test loss: 0.001780924181557364\n",
      "Epoch 101/300\n",
      "Average training loss: 0.008711479665504562\n",
      "Average test loss: 0.0017637621554442578\n",
      "Epoch 102/300\n",
      "Average training loss: 0.00870887004584074\n",
      "Average test loss: 0.001802465259614918\n",
      "Epoch 103/300\n",
      "Average training loss: 0.008693531897746855\n",
      "Average test loss: 0.0017740612568126785\n",
      "Epoch 104/300\n",
      "Average training loss: 0.008682703324904045\n",
      "Average test loss: 0.001784201896438996\n",
      "Epoch 105/300\n",
      "Average training loss: 0.008677314028971725\n",
      "Average test loss: 0.0017579806675720546\n",
      "Epoch 106/300\n",
      "Average training loss: 0.008677119633389845\n",
      "Average test loss: 0.0017960018735999862\n",
      "Epoch 107/300\n",
      "Average training loss: 0.008671572339203622\n",
      "Average test loss: 0.001763014184621473\n",
      "Epoch 108/300\n",
      "Average training loss: 0.008650773872931799\n",
      "Average test loss: 0.0017943858642958932\n",
      "Epoch 109/300\n",
      "Average training loss: 0.008650830844210254\n",
      "Average test loss: 0.0017674040018270413\n",
      "Epoch 110/300\n",
      "Average training loss: 0.008639922435912821\n",
      "Average test loss: 0.0019359157465191352\n",
      "Epoch 111/300\n",
      "Average training loss: 0.008633070264425542\n",
      "Average test loss: 0.0018584691111205353\n",
      "Epoch 112/300\n",
      "Average training loss: 0.008623554488850965\n",
      "Average test loss: 0.001819830404698021\n",
      "Epoch 113/300\n",
      "Average training loss: 0.008620056945416663\n",
      "Average test loss: 0.0017726611733022662\n",
      "Epoch 114/300\n",
      "Average training loss: 0.008603598155909114\n",
      "Average test loss: 0.0017683028425607417\n",
      "Epoch 115/300\n",
      "Average training loss: 0.008608990627030532\n",
      "Average test loss: 0.0018077723491523002\n",
      "Epoch 116/300\n",
      "Average training loss: 0.008601108253830009\n",
      "Average test loss: 0.00179132352458934\n",
      "Epoch 117/300\n",
      "Average training loss: 0.008585386558539337\n",
      "Average test loss: 0.001803963964805007\n",
      "Epoch 118/300\n",
      "Average training loss: 0.008583335233645307\n",
      "Average test loss: 0.0017922526148872243\n",
      "Epoch 119/300\n",
      "Average training loss: 0.008581529413660367\n",
      "Average test loss: 0.0018013926450577047\n",
      "Epoch 120/300\n",
      "Average training loss: 0.008572658910519547\n",
      "Average test loss: 0.0017684237001877692\n",
      "Epoch 121/300\n",
      "Average training loss: 0.00856473504876097\n",
      "Average test loss: 0.0017975173502539595\n",
      "Epoch 122/300\n",
      "Average training loss: 0.008554807773066891\n",
      "Average test loss: 0.001973330918078621\n",
      "Epoch 123/300\n",
      "Average training loss: 0.008552927069779899\n",
      "Average test loss: 0.0018215533789868155\n",
      "Epoch 124/300\n",
      "Average training loss: 0.008545624112089475\n",
      "Average test loss: 0.0017804870916944412\n",
      "Epoch 125/300\n",
      "Average training loss: 0.008543239408069187\n",
      "Average test loss: 0.0017876126212585304\n",
      "Epoch 126/300\n",
      "Average training loss: 0.008534014686114259\n",
      "Average test loss: 0.0017970930439316563\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0085278228579296\n",
      "Average test loss: 0.0017902040369808675\n",
      "Epoch 128/300\n",
      "Average training loss: 0.008513015289273528\n",
      "Average test loss: 0.0017977316592199108\n",
      "Epoch 129/300\n",
      "Average training loss: 0.00851418572705653\n",
      "Average test loss: 0.0017964439620781277\n",
      "Epoch 130/300\n",
      "Average training loss: 0.00850606275515424\n",
      "Average test loss: 0.0017994742944008775\n",
      "Epoch 131/300\n",
      "Average training loss: 0.008499304565704531\n",
      "Average test loss: 0.001790687326548828\n",
      "Epoch 132/300\n",
      "Average training loss: 0.008488596566849286\n",
      "Average test loss: 0.0017939667228816284\n",
      "Epoch 133/300\n",
      "Average training loss: 0.008483408445285426\n",
      "Average test loss: 0.0017872628467157483\n",
      "Epoch 134/300\n",
      "Average training loss: 0.008493227240526013\n",
      "Average test loss: 0.0018270594508697589\n",
      "Epoch 135/300\n",
      "Average training loss: 0.008478690097315444\n",
      "Average test loss: 0.001852158858958218\n",
      "Epoch 136/300\n",
      "Average training loss: 0.008481169758571518\n",
      "Average test loss: 0.0018106860546395183\n",
      "Epoch 137/300\n",
      "Average training loss: 0.008463315906210078\n",
      "Average test loss: 0.001813993575465348\n",
      "Epoch 138/300\n",
      "Average training loss: 0.008459370800604422\n",
      "Average test loss: 0.0017969248994357057\n",
      "Epoch 139/300\n",
      "Average training loss: 0.008451608057651255\n",
      "Average test loss: 0.0018556712778906028\n",
      "Epoch 140/300\n",
      "Average training loss: 0.008446117774065998\n",
      "Average test loss: 0.001814826669688854\n",
      "Epoch 141/300\n",
      "Average training loss: 0.008446489052640067\n",
      "Average test loss: 0.0018265256654057239\n",
      "Epoch 142/300\n",
      "Average training loss: 0.008437502249247497\n",
      "Average test loss: 0.0018063951024992599\n",
      "Epoch 143/300\n",
      "Average training loss: 0.008434450733992788\n",
      "Average test loss: 0.001798163830406136\n",
      "Epoch 144/300\n",
      "Average training loss: 0.008428301866683694\n",
      "Average test loss: 0.0017901374470028612\n",
      "Epoch 145/300\n",
      "Average training loss: 0.008422256779339578\n",
      "Average test loss: 0.0018160750087764528\n",
      "Epoch 146/300\n",
      "Average training loss: 0.008413673775891462\n",
      "Average test loss: 0.001836515757565697\n",
      "Epoch 147/300\n",
      "Average training loss: 0.008412842758827739\n",
      "Average test loss: 0.001796227969850103\n",
      "Epoch 148/300\n",
      "Average training loss: 0.008412013344466686\n",
      "Average test loss: 0.0018230703197833565\n",
      "Epoch 149/300\n",
      "Average training loss: 0.008398430010510816\n",
      "Average test loss: 0.001810562849458721\n",
      "Epoch 150/300\n",
      "Average training loss: 0.008415943558845255\n",
      "Average test loss: 0.0017840346340090036\n",
      "Epoch 151/300\n",
      "Average training loss: 0.008388406466692686\n",
      "Average test loss: 0.0017947384546407395\n",
      "Epoch 152/300\n",
      "Average training loss: 0.008382570144202975\n",
      "Average test loss: 0.0017862035371363162\n",
      "Epoch 153/300\n",
      "Average training loss: 0.008386915081904995\n",
      "Average test loss: 0.0018335916248874533\n",
      "Epoch 154/300\n",
      "Average training loss: 0.008377848231958019\n",
      "Average test loss: 0.00177777092013922\n",
      "Epoch 155/300\n",
      "Average training loss: 0.008376021469632785\n",
      "Average test loss: 0.0018802681498022544\n",
      "Epoch 156/300\n",
      "Average training loss: 0.008371338974270556\n",
      "Average test loss: 0.0018342561405152082\n",
      "Epoch 157/300\n",
      "Average training loss: 0.008365982643846009\n",
      "Average test loss: 0.0018199413631939225\n",
      "Epoch 158/300\n",
      "Average training loss: 0.008359923337896666\n",
      "Average test loss: 0.0018353609548260768\n",
      "Epoch 159/300\n",
      "Average training loss: 0.008354269770284493\n",
      "Average test loss: 0.0018441812155975236\n",
      "Epoch 160/300\n",
      "Average training loss: 0.008350244532028834\n",
      "Average test loss: 0.0018476982199483447\n",
      "Epoch 161/300\n",
      "Average training loss: 0.008355299047297902\n",
      "Average test loss: 0.0017999909797476398\n",
      "Epoch 162/300\n",
      "Average training loss: 0.008345764437483416\n",
      "Average test loss: 0.0018096542498096824\n",
      "Epoch 163/300\n",
      "Average training loss: 0.008337340589198801\n",
      "Average test loss: 0.0018067651038161583\n",
      "Epoch 164/300\n",
      "Average training loss: 0.008335095975134108\n",
      "Average test loss: 0.0018111141588952806\n",
      "Epoch 165/300\n",
      "Average training loss: 0.008327101250903473\n",
      "Average test loss: 0.0018137466683983802\n",
      "Epoch 166/300\n",
      "Average training loss: 0.008327027676834001\n",
      "Average test loss: 0.0018335971606688367\n",
      "Epoch 167/300\n",
      "Average training loss: 0.008316676088505321\n",
      "Average test loss: 0.0017997083283132977\n",
      "Epoch 168/300\n",
      "Average training loss: 0.008319079611036513\n",
      "Average test loss: 0.0018215528296099768\n",
      "Epoch 169/300\n",
      "Average training loss: 0.008319098055362701\n",
      "Average test loss: 0.0018156664970641334\n",
      "Epoch 170/300\n",
      "Average training loss: 0.008316456029398574\n",
      "Average test loss: 0.0018235135219163365\n",
      "Epoch 171/300\n",
      "Average training loss: 0.008300946018348137\n",
      "Average test loss: 0.0017980861581034131\n",
      "Epoch 172/300\n",
      "Average training loss: 0.008301502244340049\n",
      "Average test loss: 0.001859659055661824\n",
      "Epoch 173/300\n",
      "Average training loss: 0.008294900228579838\n",
      "Average test loss: 0.0018330551930185821\n",
      "Epoch 174/300\n",
      "Average training loss: 0.008298559522463216\n",
      "Average test loss: 0.0018371013214604722\n",
      "Epoch 175/300\n",
      "Average training loss: 0.00828274933124582\n",
      "Average test loss: 0.001824692474781639\n",
      "Epoch 176/300\n",
      "Average training loss: 0.008285364223851097\n",
      "Average test loss: 0.0018001207833488783\n",
      "Epoch 177/300\n",
      "Average training loss: 0.008292885033620728\n",
      "Average test loss: 0.0018154433026081986\n",
      "Epoch 178/300\n",
      "Average training loss: 0.008279896586305565\n",
      "Average test loss: 0.0018452589462200802\n",
      "Epoch 179/300\n",
      "Average training loss: 0.008275349360373285\n",
      "Average test loss: 0.0018629863547782104\n",
      "Epoch 180/300\n",
      "Average training loss: 0.00827472714914216\n",
      "Average test loss: 0.0018286998137935168\n",
      "Epoch 181/300\n",
      "Average training loss: 0.008271509480973084\n",
      "Average test loss: 0.0018140842345439724\n",
      "Epoch 182/300\n",
      "Average training loss: 0.008266658059424824\n",
      "Average test loss: 0.0018236972726881505\n",
      "Epoch 183/300\n",
      "Average training loss: 0.008260906275775698\n",
      "Average test loss: 0.0018125550531678729\n",
      "Epoch 184/300\n",
      "Average training loss: 0.008257595698452658\n",
      "Average test loss: 0.0018454649977179037\n",
      "Epoch 185/300\n",
      "Average training loss: 0.008248120039701462\n",
      "Average test loss: 0.001851820658478472\n",
      "Epoch 186/300\n",
      "Average training loss: 0.00825869555854135\n",
      "Average test loss: 0.0018778576790872547\n",
      "Epoch 187/300\n",
      "Average training loss: 0.008246587096816964\n",
      "Average test loss: 0.0018264643164972465\n",
      "Epoch 188/300\n",
      "Average training loss: 0.008242057476192713\n",
      "Average test loss: 0.0018052471006392604\n",
      "Epoch 189/300\n",
      "Average training loss: 0.008237196563018693\n",
      "Average test loss: 0.0018455370372782152\n",
      "Epoch 190/300\n",
      "Average training loss: 0.008233264630867375\n",
      "Average test loss: 0.0018904831428080797\n",
      "Epoch 191/300\n",
      "Average training loss: 0.008235421451843448\n",
      "Average test loss: 0.0018382578668081097\n",
      "Epoch 192/300\n",
      "Average training loss: 0.008221043224963876\n",
      "Average test loss: 0.0018384811383568578\n",
      "Epoch 193/300\n",
      "Average training loss: 0.008226924652026759\n",
      "Average test loss: 0.001819751769097315\n",
      "Epoch 194/300\n",
      "Average training loss: 0.008217863065914975\n",
      "Average test loss: 0.0018788537830114365\n",
      "Epoch 195/300\n",
      "Average training loss: 0.008212069643040498\n",
      "Average test loss: 0.0018434700960707332\n",
      "Epoch 196/300\n",
      "Average training loss: 0.008212926228427224\n",
      "Average test loss: 0.0018449676344171166\n",
      "Epoch 197/300\n",
      "Average training loss: 0.008212608171006044\n",
      "Average test loss: 0.0018320645444716017\n",
      "Epoch 198/300\n",
      "Average training loss: 0.008206534575257036\n",
      "Average test loss: 0.0018709532366030747\n",
      "Epoch 199/300\n",
      "Average training loss: 0.008216304793126053\n",
      "Average test loss: 0.002113527817444669\n",
      "Epoch 200/300\n",
      "Average training loss: 0.008208094089809392\n",
      "Average test loss: 0.001857295210266279\n",
      "Epoch 201/300\n",
      "Average training loss: 0.008203213838239512\n",
      "Average test loss: 0.0018493373639260728\n",
      "Epoch 202/300\n",
      "Average training loss: 0.008192804349793328\n",
      "Average test loss: 0.0018374166782531474\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00819038797956374\n",
      "Average test loss: 0.0018490162859153417\n",
      "Epoch 204/300\n",
      "Average training loss: 0.008189148565133413\n",
      "Average test loss: 0.0018367015440016986\n",
      "Epoch 205/300\n",
      "Average training loss: 0.008186415857738919\n",
      "Average test loss: 0.001853922520350251\n",
      "Epoch 206/300\n",
      "Average training loss: 0.008189111647920477\n",
      "Average test loss: 0.001846994801527924\n",
      "Epoch 207/300\n",
      "Average training loss: 0.008188863590773608\n",
      "Average test loss: 0.0018725732444889016\n",
      "Epoch 208/300\n",
      "Average training loss: 0.00818002120571004\n",
      "Average test loss: 0.0018739678119826647\n",
      "Epoch 209/300\n",
      "Average training loss: 0.008191519884599579\n",
      "Average test loss: 0.001837849216742648\n",
      "Epoch 210/300\n",
      "Average training loss: 0.008179924508349763\n",
      "Average test loss: 0.0018485813486493296\n",
      "Epoch 211/300\n",
      "Average training loss: 0.008176073225422038\n",
      "Average test loss: 0.001915700569541918\n",
      "Epoch 212/300\n",
      "Average training loss: 0.008159633809493647\n",
      "Average test loss: 0.0018487201920813984\n",
      "Epoch 213/300\n",
      "Average training loss: 0.008172005975411997\n",
      "Average test loss: 0.0018645421906063953\n",
      "Epoch 214/300\n",
      "Average training loss: 0.008162377190258767\n",
      "Average test loss: 0.0018227224415168165\n",
      "Epoch 215/300\n",
      "Average training loss: 0.008166687582102088\n",
      "Average test loss: 0.0018377050130317608\n",
      "Epoch 216/300\n",
      "Average training loss: 0.008160498110370503\n",
      "Average test loss: 0.0019017933900985452\n",
      "Epoch 217/300\n",
      "Average training loss: 0.00815237808558676\n",
      "Average test loss: 0.0018545138910412789\n",
      "Epoch 218/300\n",
      "Average training loss: 0.00815382309216592\n",
      "Average test loss: 0.0018286717666520012\n",
      "Epoch 219/300\n",
      "Average training loss: 0.008148959394958284\n",
      "Average test loss: 0.0018808568947845034\n",
      "Epoch 220/300\n",
      "Average training loss: 0.008152610758112537\n",
      "Average test loss: 0.0018849364657782847\n",
      "Epoch 221/300\n",
      "Average training loss: 0.008144888151023122\n",
      "Average test loss: 0.001868513993298014\n",
      "Epoch 222/300\n",
      "Average training loss: 0.00814005198702216\n",
      "Average test loss: 0.001869176507099635\n",
      "Epoch 223/300\n",
      "Average training loss: 0.00814079253292746\n",
      "Average test loss: 0.001841194997024205\n",
      "Epoch 224/300\n",
      "Average training loss: 0.00814145471652349\n",
      "Average test loss: 0.0018773378607713513\n",
      "Epoch 225/300\n",
      "Average training loss: 0.008135680907302432\n",
      "Average test loss: 0.001869222721291913\n",
      "Epoch 226/300\n",
      "Average training loss: 0.008131294225653013\n",
      "Average test loss: 0.0018695621111740669\n",
      "Epoch 227/300\n",
      "Average training loss: 0.008136223721007507\n",
      "Average test loss: 0.0019097678732747834\n",
      "Epoch 228/300\n",
      "Average training loss: 0.008130509071879916\n",
      "Average test loss: 0.001859343893205126\n",
      "Epoch 229/300\n",
      "Average training loss: 0.008121913244326909\n",
      "Average test loss: 0.0018230601808884078\n",
      "Epoch 230/300\n",
      "Average training loss: 0.008119914348340697\n",
      "Average test loss: 0.0018954717236984934\n",
      "Epoch 231/300\n",
      "Average training loss: 0.008123320409821139\n",
      "Average test loss: 0.0018500108460171355\n",
      "Epoch 232/300\n",
      "Average training loss: 0.008112843673262331\n",
      "Average test loss: 0.0018802793329167698\n",
      "Epoch 233/300\n",
      "Average training loss: 0.00811442035353846\n",
      "Average test loss: 0.0018650896409526468\n",
      "Epoch 234/300\n",
      "Average training loss: 0.008120882833997409\n",
      "Average test loss: 0.0018624647651902504\n",
      "Epoch 235/300\n",
      "Average training loss: 0.008110587657325797\n",
      "Average test loss: 0.0018853605846977895\n",
      "Epoch 236/300\n",
      "Average training loss: 0.008102517265412543\n",
      "Average test loss: 0.0018734277040801115\n",
      "Epoch 237/300\n",
      "Average training loss: 0.008106635193857882\n",
      "Average test loss: 0.0018808743856433365\n",
      "Epoch 238/300\n",
      "Average training loss: 0.00810066308412287\n",
      "Average test loss: 0.001852676337895294\n",
      "Epoch 239/300\n",
      "Average training loss: 0.008100562831179964\n",
      "Average test loss: 0.0018597006193465657\n",
      "Epoch 240/300\n",
      "Average training loss: 0.008111114667521582\n",
      "Average test loss: 0.0018452562734277713\n",
      "Epoch 241/300\n",
      "Average training loss: 0.008095481080727445\n",
      "Average test loss: 0.0019180853125225338\n",
      "Epoch 242/300\n",
      "Average training loss: 0.008095843960841497\n",
      "Average test loss: 0.002057826479069061\n",
      "Epoch 243/300\n",
      "Average training loss: 0.00809617277690106\n",
      "Average test loss: 0.001845931687288814\n",
      "Epoch 244/300\n",
      "Average training loss: 0.008084497757669952\n",
      "Average test loss: 0.0018955990376157894\n",
      "Epoch 245/300\n",
      "Average training loss: 0.008087835582594077\n",
      "Average test loss: 0.0018801893560836712\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00808932553521461\n",
      "Average test loss: 0.0019033171135621767\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00808457314554188\n",
      "Average test loss: 0.0018817009414649672\n",
      "Epoch 248/300\n",
      "Average training loss: 0.008081398921708265\n",
      "Average test loss: 0.001875430043683284\n",
      "Epoch 249/300\n",
      "Average training loss: 0.008079750919507609\n",
      "Average test loss: 0.0018398040437863934\n",
      "Epoch 250/300\n",
      "Average training loss: 0.008078134701069859\n",
      "Average test loss: 0.0018998447831513154\n",
      "Epoch 251/300\n",
      "Average training loss: 0.008074244355161984\n",
      "Average test loss: 0.0018633039562652508\n",
      "Epoch 252/300\n",
      "Average training loss: 0.008081793625321652\n",
      "Average test loss: 0.0018636949690472749\n",
      "Epoch 253/300\n",
      "Average training loss: 0.008062381024989816\n",
      "Average test loss: 0.001894867756507463\n",
      "Epoch 254/300\n",
      "Average training loss: 0.008068832089503606\n",
      "Average test loss: 0.0019223082348083457\n",
      "Epoch 255/300\n",
      "Average training loss: 0.008070086613297462\n",
      "Average test loss: 0.0018719847795243064\n",
      "Epoch 256/300\n",
      "Average training loss: 0.00806399183306429\n",
      "Average test loss: 0.0018619918620420826\n",
      "Epoch 257/300\n",
      "Average training loss: 0.008062853433605698\n",
      "Average test loss: 0.0018921453023536337\n",
      "Epoch 258/300\n",
      "Average training loss: 0.008054044254952007\n",
      "Average test loss: 0.0018895355781747235\n",
      "Epoch 259/300\n",
      "Average training loss: 0.008054483368164963\n",
      "Average test loss: 0.0018803270888618298\n",
      "Epoch 260/300\n",
      "Average training loss: 0.008060702440225416\n",
      "Average test loss: 0.0018661125434769525\n",
      "Epoch 261/300\n",
      "Average training loss: 0.008056881985730594\n",
      "Average test loss: 0.0018797062190456524\n",
      "Epoch 262/300\n",
      "Average training loss: 0.008047893185582427\n",
      "Average test loss: 0.0018738919495501453\n",
      "Epoch 263/300\n",
      "Average training loss: 0.008052934675580926\n",
      "Average test loss: 0.0018397784488689568\n",
      "Epoch 264/300\n",
      "Average training loss: 0.008043406248092652\n",
      "Average test loss: 0.0018729049800377752\n",
      "Epoch 265/300\n",
      "Average training loss: 0.008044293392449617\n",
      "Average test loss: 0.0019046072960934705\n",
      "Epoch 266/300\n",
      "Average training loss: 0.008049359358019299\n",
      "Average test loss: 0.0018698254145888818\n",
      "Epoch 267/300\n",
      "Average training loss: 0.008046860059516298\n",
      "Average test loss: 0.0019123387606814503\n",
      "Epoch 268/300\n",
      "Average training loss: 0.008042534224689007\n",
      "Average test loss: 0.0018949570851400494\n",
      "Epoch 269/300\n",
      "Average training loss: 0.008039608520766099\n",
      "Average test loss: 0.0018515158429120977\n",
      "Epoch 270/300\n",
      "Average training loss: 0.008038670795245303\n",
      "Average test loss: 0.001851654812383155\n",
      "Epoch 271/300\n",
      "Average training loss: 0.008033459174964163\n",
      "Average test loss: 0.001868297526302437\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0080384048918883\n",
      "Average test loss: 0.0018548931028279994\n",
      "Epoch 273/300\n",
      "Average training loss: 0.008030952283905612\n",
      "Average test loss: 0.0018421201607626346\n",
      "Epoch 274/300\n",
      "Average training loss: 0.008022255735264884\n",
      "Average test loss: 0.0018886813673501214\n",
      "Epoch 275/300\n",
      "Average training loss: 0.008026526399784618\n",
      "Average test loss: 0.0018564178517295254\n",
      "Epoch 276/300\n",
      "Average training loss: 0.008026014195548164\n",
      "Average test loss: 0.0020580245811078284\n",
      "Epoch 277/300\n",
      "Average training loss: 0.008029592756181956\n",
      "Average test loss: 0.0018803130878756445\n",
      "Epoch 278/300\n",
      "Average training loss: 0.008015763733122084\n",
      "Average test loss: 0.0018620977813067535\n",
      "Epoch 279/300\n",
      "Average training loss: 0.008018634739849302\n",
      "Average test loss: 0.0018711590516484447\n",
      "Epoch 280/300\n",
      "Average training loss: 0.008018323322137196\n",
      "Average test loss: 0.001869666268531647\n",
      "Epoch 281/300\n",
      "Average training loss: 0.008020883332524036\n",
      "Average test loss: 0.0019964869974388017\n",
      "Epoch 282/300\n",
      "Average training loss: 0.008026859224670464\n",
      "Average test loss: 0.0019359992050255338\n",
      "Epoch 283/300\n",
      "Average training loss: 0.008012995840774643\n",
      "Average test loss: 0.0019249935017691718\n",
      "Epoch 284/300\n",
      "Average training loss: 0.008012681771897607\n",
      "Average test loss: 0.0018616563220404917\n",
      "Epoch 285/300\n",
      "Average training loss: 0.008009007655084133\n",
      "Average test loss: 0.0018553340861366854\n",
      "Epoch 286/300\n",
      "Average training loss: 0.008002555621994867\n",
      "Average test loss: 0.0018688702442579799\n",
      "Epoch 287/300\n",
      "Average training loss: 0.008008635341293282\n",
      "Average test loss: 0.0018692090140862597\n",
      "Epoch 288/300\n",
      "Average training loss: 0.008000234041776922\n",
      "Average test loss: 0.0019055062000536255\n",
      "Epoch 289/300\n",
      "Average training loss: 0.008002210608373085\n",
      "Average test loss: 0.0018428128593497807\n",
      "Epoch 290/300\n",
      "Average training loss: 0.008014055403156414\n",
      "Average test loss: 0.0018983229491859675\n",
      "Epoch 291/300\n",
      "Average training loss: 0.00800602433623539\n",
      "Average test loss: 0.001847020750037498\n",
      "Epoch 292/300\n",
      "Average training loss: 0.007988750894036558\n",
      "Average test loss: 0.0019285357953566643\n",
      "Epoch 293/300\n",
      "Average training loss: 0.00799959325707621\n",
      "Average test loss: 0.0019149910699990062\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007988797808686892\n",
      "Average test loss: 0.0018800382251954741\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007990241447256671\n",
      "Average test loss: 0.00187593438083099\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007995664677686162\n",
      "Average test loss: 0.0019065648522228003\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007985534423341354\n",
      "Average test loss: 0.0018952934824758105\n",
      "Epoch 298/300\n",
      "Average training loss: 0.007994256528715292\n",
      "Average test loss: 0.001902789817398621\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007989836539659236\n",
      "Average test loss: 0.0019337458993411727\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007984313199917475\n",
      "Average test loss: 0.0018487069764071041\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive-.025/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.64\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.82\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.56\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.68\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.17\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.93710370710161\n",
      "Average test loss: 0.015729534415735137\n",
      "Epoch 2/300\n",
      "Average training loss: 0.20321656019157833\n",
      "Average test loss: 0.0048742277353174155\n",
      "Epoch 3/300\n",
      "Average training loss: 0.13784783983230592\n",
      "Average test loss: 0.00461491140930189\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11107772494024701\n",
      "Average test loss: 0.004517406143455042\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09631573825412326\n",
      "Average test loss: 0.0044768227810661\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08746203635136286\n",
      "Average test loss: 0.004667532270981206\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08084116499291526\n",
      "Average test loss: 0.004384041123506096\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07602377363708285\n",
      "Average test loss: 0.004354461135549678\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0729475302166409\n",
      "Average test loss: 0.004312771190371778\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07077517893579272\n",
      "Average test loss: 0.004279990030038688\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06894681627220578\n",
      "Average test loss: 0.004287302068952057\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06757293130291833\n",
      "Average test loss: 0.004261832092164291\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06655145508713192\n",
      "Average test loss: 0.004228394179087546\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06578630444076326\n",
      "Average test loss: 0.004206705327249235\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0651162514951494\n",
      "Average test loss: 0.0041975694803728\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06456991628474659\n",
      "Average test loss: 0.00418966939051946\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06411385493808322\n",
      "Average test loss: 0.004197150240341822\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06377283560236295\n",
      "Average test loss: 0.0042921088292366925\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06343444342083401\n",
      "Average test loss: 0.004166318262202872\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06313277151849535\n",
      "Average test loss: 0.004135252588325077\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06286810389823384\n",
      "Average test loss: 0.004119183450937271\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06260311437977685\n",
      "Average test loss: 0.00411098503590458\n",
      "Epoch 23/300\n",
      "Average training loss: 0.062384530070755215\n",
      "Average test loss: 0.004137500598198837\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06217123208443324\n",
      "Average test loss: 0.004123205542978313\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06197405327028698\n",
      "Average test loss: 0.004103957547081842\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06175491581360499\n",
      "Average test loss: 0.004092075534164906\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06160381841659546\n",
      "Average test loss: 0.004077959882302417\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06141061257984903\n",
      "Average test loss: 0.004086053487741285\n",
      "Epoch 29/300\n",
      "Average training loss: 0.061281542152166364\n",
      "Average test loss: 0.004066613643119732\n",
      "Epoch 30/300\n",
      "Average training loss: 0.061076916337013246\n",
      "Average test loss: 0.004091280036502414\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06093766749898592\n",
      "Average test loss: 0.004044602973179685\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06080281525519159\n",
      "Average test loss: 0.004075819750212961\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06067456348737081\n",
      "Average test loss: 0.0040380702883832985\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06053862639930513\n",
      "Average test loss: 0.004025368411921792\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06042909933129947\n",
      "Average test loss: 0.004107978636191951\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06029558463891347\n",
      "Average test loss: 0.004036050864598817\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06016067156857914\n",
      "Average test loss: 0.004019926744823654\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06006789171695709\n",
      "Average test loss: 0.004028627809964948\n",
      "Epoch 39/300\n",
      "Average training loss: 0.059951068431138996\n",
      "Average test loss: 0.0040155356410476895\n",
      "Epoch 40/300\n",
      "Average training loss: 0.059822291957007515\n",
      "Average test loss: 0.004035194406078921\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0597092421385977\n",
      "Average test loss: 0.004011168447012703\n",
      "Epoch 42/300\n",
      "Average training loss: 0.059636299843589466\n",
      "Average test loss: 0.004000294507791598\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05950331788924005\n",
      "Average test loss: 0.00400335715090235\n",
      "Epoch 44/300\n",
      "Average training loss: 0.059445529515544576\n",
      "Average test loss: 0.004023984795229302\n",
      "Epoch 45/300\n",
      "Average training loss: 0.05931588155362341\n",
      "Average test loss: 0.004009707333727015\n",
      "Epoch 46/300\n",
      "Average training loss: 0.059220492670933406\n",
      "Average test loss: 0.003983409641103612\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05912700648440255\n",
      "Average test loss: 0.004008115575131443\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05909295899669329\n",
      "Average test loss: 0.00405249311029911\n",
      "Epoch 49/300\n",
      "Average training loss: 0.058965056081612906\n",
      "Average test loss: 0.003985367715358734\n",
      "Epoch 50/300\n",
      "Average training loss: 0.058900534507301115\n",
      "Average test loss: 0.004015784187863271\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0588102747797966\n",
      "Average test loss: 0.0039935094511343375\n",
      "Epoch 52/300\n",
      "Average training loss: 0.058755711697869833\n",
      "Average test loss: 0.003983576317628225\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05862893693645795\n",
      "Average test loss: 0.004003389502358105\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05859757383664449\n",
      "Average test loss: 0.00398303703756796\n",
      "Epoch 55/300\n",
      "Average training loss: 0.058538648648394476\n",
      "Average test loss: 0.003992015073489811\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05842778186665641\n",
      "Average test loss: 0.003998164746496413\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05830946223934491\n",
      "Average test loss: 0.003978746508558591\n",
      "Epoch 58/300\n",
      "Average training loss: 0.058233533518181904\n",
      "Average test loss: 0.003980477251733343\n",
      "Epoch 59/300\n",
      "Average training loss: 0.058182417266898684\n",
      "Average test loss: 0.003997006791540318\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05808268115917842\n",
      "Average test loss: 0.003984130693599582\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05802200943562719\n",
      "Average test loss: 0.003980798185492555\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0579520871506797\n",
      "Average test loss: 0.003985272594003214\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05787903823455175\n",
      "Average test loss: 0.003996144765367111\n",
      "Epoch 64/300\n",
      "Average training loss: 0.05777970559067196\n",
      "Average test loss: 0.003983724404540327\n",
      "Epoch 65/300\n",
      "Average training loss: 0.057663482278585435\n",
      "Average test loss: 0.003976443613568942\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05765239020519786\n",
      "Average test loss: 0.004018723588436842\n",
      "Epoch 67/300\n",
      "Average training loss: 0.05753236978915002\n",
      "Average test loss: 0.004087164950039652\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05750120621919632\n",
      "Average test loss: 0.003993694265269571\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05739233203728994\n",
      "Average test loss: 0.004009089703154233\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05730476701921887\n",
      "Average test loss: 0.004005710813941227\n",
      "Epoch 71/300\n",
      "Average training loss: 0.05725788082016839\n",
      "Average test loss: 0.004018094114545319\n",
      "Epoch 72/300\n",
      "Average training loss: 0.057144503265619276\n",
      "Average test loss: 0.004005792487205731\n",
      "Epoch 73/300\n",
      "Average training loss: 0.057091674586137134\n",
      "Average test loss: 0.0040075357968194615\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05699899884396129\n",
      "Average test loss: 0.004020618640714221\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0568801187939114\n",
      "Average test loss: 0.003986255214032199\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05684520959854126\n",
      "Average test loss: 0.003987230714824465\n",
      "Epoch 77/300\n",
      "Average training loss: 0.056747196935945085\n",
      "Average test loss: 0.004035893561525478\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05668737341960271\n",
      "Average test loss: 0.004008288281452325\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05657178498639001\n",
      "Average test loss: 0.004013442724115319\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0565001535250081\n",
      "Average test loss: 0.00409836838601364\n",
      "Epoch 81/300\n",
      "Average training loss: 0.05636704054143694\n",
      "Average test loss: 0.004024718951433897\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05636944669153955\n",
      "Average test loss: 0.004068167707572381\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05624341120322545\n",
      "Average test loss: 0.004074014348909259\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05613649721278085\n",
      "Average test loss: 0.00400689885020256\n",
      "Epoch 85/300\n",
      "Average training loss: 0.05611299713452657\n",
      "Average test loss: 0.00399840335196091\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05607001623180177\n",
      "Average test loss: 0.004079297655986415\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05589480162329144\n",
      "Average test loss: 0.004041531709747182\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05577724620699882\n",
      "Average test loss: 0.004052410049984852\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05574489221804672\n",
      "Average test loss: 0.004091973527852032\n",
      "Epoch 90/300\n",
      "Average training loss: 0.055680414961444005\n",
      "Average test loss: 0.004072266928023763\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05560412585073047\n",
      "Average test loss: 0.004034276693645451\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0555115041202969\n",
      "Average test loss: 0.00405400825623009\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05541450607776642\n",
      "Average test loss: 0.004069662704235978\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05531905370950699\n",
      "Average test loss: 0.00411868253019121\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05528708535432816\n",
      "Average test loss: 0.004036493816309505\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05522537488407559\n",
      "Average test loss: 0.004038021229414476\n",
      "Epoch 97/300\n",
      "Average training loss: 0.055160556816392474\n",
      "Average test loss: 0.004101504521651401\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05501316957672437\n",
      "Average test loss: 0.004046467987613545\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0549787273771233\n",
      "Average test loss: 0.0040814328754527705\n",
      "Epoch 100/300\n",
      "Average training loss: 0.054950338704718486\n",
      "Average test loss: 0.0041081031180090375\n",
      "Epoch 101/300\n",
      "Average training loss: 0.054871059086587695\n",
      "Average test loss: 0.004242502340839969\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05478082448244095\n",
      "Average test loss: 0.004142397254705429\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0546299646364318\n",
      "Average test loss: 0.004141110895822446\n",
      "Epoch 104/300\n",
      "Average training loss: 0.054622480432192484\n",
      "Average test loss: 0.004105691214195556\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05452134789360894\n",
      "Average test loss: 0.004152209351046218\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05455944504671627\n",
      "Average test loss: 0.004128809402800269\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05438543506132232\n",
      "Average test loss: 0.004092726890411642\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05435135084225072\n",
      "Average test loss: 0.0041487404563360746\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05428240728378296\n",
      "Average test loss: 0.00413842591146628\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05418514824906985\n",
      "Average test loss: 0.004119435396252407\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05407074230909348\n",
      "Average test loss: 0.004161526030964322\n",
      "Epoch 112/300\n",
      "Average training loss: 0.054048140509261024\n",
      "Average test loss: 0.0041089198299580154\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05398327236705356\n",
      "Average test loss: 0.004297643717171417\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05392187991903888\n",
      "Average test loss: 0.004177019593202405\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05384615398115582\n",
      "Average test loss: 0.004218440850575765\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0537658004462719\n",
      "Average test loss: 0.004207034637530645\n",
      "Epoch 117/300\n",
      "Average training loss: 0.053711416241195464\n",
      "Average test loss: 0.004134869160751502\n",
      "Epoch 118/300\n",
      "Average training loss: 0.053633692112233906\n",
      "Average test loss: 0.004136534114885662\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05359329124622875\n",
      "Average test loss: 0.004121115053486493\n",
      "Epoch 120/300\n",
      "Average training loss: 0.053580217391252516\n",
      "Average test loss: 0.0041570857955763735\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05348915047778024\n",
      "Average test loss: 0.004221882295277384\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05340962819920646\n",
      "Average test loss: 0.004176693364149994\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05336093833049138\n",
      "Average test loss: 0.0042167533052464325\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05327061551809311\n",
      "Average test loss: 0.00421953178031577\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05321224261654748\n",
      "Average test loss: 0.0041460165948503545\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05320468455221918\n",
      "Average test loss: 0.004164871098473668\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05309490980704625\n",
      "Average test loss: 0.0041841946099367405\n",
      "Epoch 128/300\n",
      "Average training loss: 0.053075558973683254\n",
      "Average test loss: 0.004220730808667011\n",
      "Epoch 129/300\n",
      "Average training loss: 0.053032032436794704\n",
      "Average test loss: 0.004144198545151287\n",
      "Epoch 130/300\n",
      "Average training loss: 0.052923728959427936\n",
      "Average test loss: 0.004256765948401557\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05286334220899476\n",
      "Average test loss: 0.004161818484879202\n",
      "Epoch 132/300\n",
      "Average training loss: 0.052880920267767376\n",
      "Average test loss: 0.0043239221599780855\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05278584215044975\n",
      "Average test loss: 0.0041972995549440385\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05272075493137042\n",
      "Average test loss: 0.004114841026150518\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05265642316142718\n",
      "Average test loss: 0.004186880142738421\n",
      "Epoch 136/300\n",
      "Average training loss: 0.052654297427998646\n",
      "Average test loss: 0.0042179624554183745\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05253317067358229\n",
      "Average test loss: 0.004151715016199483\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05253653397825029\n",
      "Average test loss: 0.0042395570959068\n",
      "Epoch 139/300\n",
      "Average training loss: 0.052451912071969774\n",
      "Average test loss: 0.004168846203014254\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0524754412803385\n",
      "Average test loss: 0.004128319919937186\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05240288241373168\n",
      "Average test loss: 0.00416491661220789\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05228052358494865\n",
      "Average test loss: 0.004200989896638526\n",
      "Epoch 143/300\n",
      "Average training loss: 0.052306762039661406\n",
      "Average test loss: 0.004136635831453734\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05228304853704241\n",
      "Average test loss: 0.004185419099819329\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05222675636907419\n",
      "Average test loss: 0.004241535224434402\n",
      "Epoch 146/300\n",
      "Average training loss: 0.052203338179323405\n",
      "Average test loss: 0.004162908422036303\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05203791936238607\n",
      "Average test loss: 0.004170131918456819\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05204542501105203\n",
      "Average test loss: 0.00417140446893043\n",
      "Epoch 149/300\n",
      "Average training loss: 0.052042161564032234\n",
      "Average test loss: 0.004174248620453808\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05195849798785315\n",
      "Average test loss: 0.004254953083478742\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05191831447349654\n",
      "Average test loss: 0.004278130778008037\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05190965968039301\n",
      "Average test loss: 0.0041901959520247245\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05183442360493872\n",
      "Average test loss: 0.00422832512938314\n",
      "Epoch 154/300\n",
      "Average training loss: 0.051784711953666476\n",
      "Average test loss: 0.004191620850728618\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0517392921547095\n",
      "Average test loss: 0.004161429285382231\n",
      "Epoch 156/300\n",
      "Average training loss: 0.051693504008981914\n",
      "Average test loss: 0.004328543526637885\n",
      "Epoch 157/300\n",
      "Average training loss: 0.051632348977857165\n",
      "Average test loss: 0.00427694622034\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05163586585389243\n",
      "Average test loss: 0.004268943052738905\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05157040878136953\n",
      "Average test loss: 0.004224907957845264\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05154173661602868\n",
      "Average test loss: 0.004272094765885009\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05152511353294054\n",
      "Average test loss: 0.004193268133534325\n",
      "Epoch 162/300\n",
      "Average training loss: 0.051496719128555725\n",
      "Average test loss: 0.004308714730872049\n",
      "Epoch 163/300\n",
      "Average training loss: 0.051375262439250946\n",
      "Average test loss: 0.004250231260640754\n",
      "Epoch 164/300\n",
      "Average training loss: 0.051366480019357466\n",
      "Average test loss: 0.004206303755649262\n",
      "Epoch 165/300\n",
      "Average training loss: 0.051402716080347695\n",
      "Average test loss: 0.0043364589433703155\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05126505714986059\n",
      "Average test loss: 0.004281288413123952\n",
      "Epoch 167/300\n",
      "Average training loss: 0.051296023107237285\n",
      "Average test loss: 0.004269485057642062\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05123347474800216\n",
      "Average test loss: 0.004297456903590097\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05123337294658025\n",
      "Average test loss: 0.004174187896566259\n",
      "Epoch 170/300\n",
      "Average training loss: 0.051125106973780525\n",
      "Average test loss: 0.0042559322497497006\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05111865814526876\n",
      "Average test loss: 0.004195524505443043\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05115092411306169\n",
      "Average test loss: 0.004192583275544975\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05103854286670685\n",
      "Average test loss: 0.004223083701605598\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05100911222894986\n",
      "Average test loss: 0.0042841882314533\n",
      "Epoch 175/300\n",
      "Average training loss: 0.050957841419511374\n",
      "Average test loss: 0.004252293584454391\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05097361189789242\n",
      "Average test loss: 0.004237926202722722\n",
      "Epoch 177/300\n",
      "Average training loss: 0.05089169352915552\n",
      "Average test loss: 0.0042686506820221745\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05093966920177142\n",
      "Average test loss: 0.004336342622422509\n",
      "Epoch 179/300\n",
      "Average training loss: 0.050796069827344685\n",
      "Average test loss: 0.004235994276901086\n",
      "Epoch 180/300\n",
      "Average training loss: 0.050842137333419586\n",
      "Average test loss: 0.004215512227267027\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05077803750832876\n",
      "Average test loss: 0.004204917303803894\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05075252888600031\n",
      "Average test loss: 0.00422935388609767\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05068331853217549\n",
      "Average test loss: 0.0042385839637782835\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05069597162471877\n",
      "Average test loss: 0.0042959320280287\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05061768180463049\n",
      "Average test loss: 0.004207482357612914\n",
      "Epoch 186/300\n",
      "Average training loss: 0.050637699984841876\n",
      "Average test loss: 0.0042534452693329915\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05057828471726841\n",
      "Average test loss: 0.0042278803164760275\n",
      "Epoch 188/300\n",
      "Average training loss: 0.050567570875088376\n",
      "Average test loss: 0.004206922259595659\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05051381155186229\n",
      "Average test loss: 0.004251968571295341\n",
      "Epoch 190/300\n",
      "Average training loss: 0.050539607220225866\n",
      "Average test loss: 0.004256757448530859\n",
      "Epoch 191/300\n",
      "Average training loss: 0.050441685987843404\n",
      "Average test loss: 0.00422677706554532\n",
      "Epoch 192/300\n",
      "Average training loss: 0.050415001819531124\n",
      "Average test loss: 0.004336912275188499\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05047791045241886\n",
      "Average test loss: 0.004423575114458799\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05041365739703178\n",
      "Average test loss: 0.004359430114014282\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05032481128308508\n",
      "Average test loss: 0.0042561191485987766\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05030884501669142\n",
      "Average test loss: 0.004312658826510112\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0503096385134591\n",
      "Average test loss: 0.004380680016759369\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05024511464436849\n",
      "Average test loss: 0.00434132237566842\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05019396585557196\n",
      "Average test loss: 0.004222097494535976\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05017261646191279\n",
      "Average test loss: 0.004191542019860612\n",
      "Epoch 201/300\n",
      "Average training loss: 0.050144946263896095\n",
      "Average test loss: 0.004353699505329132\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05014773772822486\n",
      "Average test loss: 0.004352066366622846\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05016665905548467\n",
      "Average test loss: 0.004354965213479267\n",
      "Epoch 204/300\n",
      "Average training loss: 0.050040740115775\n",
      "Average test loss: 0.004343930855807331\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04998278899656402\n",
      "Average test loss: 0.004296208704097404\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05000743349062072\n",
      "Average test loss: 0.00436449280857212\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05004313358002239\n",
      "Average test loss: 0.004270650905660457\n",
      "Epoch 208/300\n",
      "Average training loss: 0.050182888332340456\n",
      "Average test loss: 0.0042515642117295\n",
      "Epoch 209/300\n",
      "Average training loss: 0.050071608904335235\n",
      "Average test loss: 0.004262376064848568\n",
      "Epoch 210/300\n",
      "Average training loss: 0.049935949047406517\n",
      "Average test loss: 0.004274297160820829\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04986400764849451\n",
      "Average test loss: 0.00427300307713449\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04987093037698004\n",
      "Average test loss: 0.004341359411262804\n",
      "Epoch 213/300\n",
      "Average training loss: 0.049780239949623745\n",
      "Average test loss: 0.0044317360975676115\n",
      "Epoch 214/300\n",
      "Average training loss: 0.049805579913987055\n",
      "Average test loss: 0.004242836984909243\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0497368878920873\n",
      "Average test loss: 0.0043713400310112374\n",
      "Epoch 216/300\n",
      "Average training loss: 0.049824081718921664\n",
      "Average test loss: 0.004248851387865013\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04978722480601735\n",
      "Average test loss: 0.004297648901533749\n",
      "Epoch 218/300\n",
      "Average training loss: 0.049674995210435655\n",
      "Average test loss: 0.004271814667516285\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04969770941138268\n",
      "Average test loss: 0.00429649210224549\n",
      "Epoch 220/300\n",
      "Average training loss: 0.049662965615590415\n",
      "Average test loss: 0.004175509341888957\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04974228229125341\n",
      "Average test loss: 0.0042650919925007556\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04961388125684526\n",
      "Average test loss: 0.004316090015487538\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04961206165618367\n",
      "Average test loss: 0.004275129708357983\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04957207417819235\n",
      "Average test loss: 0.004309684471123749\n",
      "Epoch 225/300\n",
      "Average training loss: 0.049536073684692386\n",
      "Average test loss: 0.0042559199771947335\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04956086134579447\n",
      "Average test loss: 0.004392222895804379\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04950111503733529\n",
      "Average test loss: 0.004306929173982806\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04948571760455767\n",
      "Average test loss: 0.004276797312415309\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0494824860824479\n",
      "Average test loss: 0.004421266824834877\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04942821937468317\n",
      "Average test loss: 0.004215234282736977\n",
      "Epoch 231/300\n",
      "Average training loss: 0.049372816969950994\n",
      "Average test loss: 0.004289717633277177\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04937529315219985\n",
      "Average test loss: 0.004296629865136412\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04934050679206848\n",
      "Average test loss: 0.00434634985236658\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04931723270813624\n",
      "Average test loss: 0.004260754961106512\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0492917938431104\n",
      "Average test loss: 0.004265715150783459\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04934821720586883\n",
      "Average test loss: 0.004387308061122894\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04922221423188845\n",
      "Average test loss: 0.004286596202601989\n",
      "Epoch 238/300\n",
      "Average training loss: 0.049248787691195804\n",
      "Average test loss: 0.004279404270980093\n",
      "Epoch 239/300\n",
      "Average training loss: 0.0492811776548624\n",
      "Average test loss: 0.00435473084801601\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04916284479035272\n",
      "Average test loss: 0.004360432005797823\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04918614857064353\n",
      "Average test loss: 0.004375005684379075\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04914505608214272\n",
      "Average test loss: 0.004319755973294377\n",
      "Epoch 243/300\n",
      "Average training loss: 0.049171837084823186\n",
      "Average test loss: 0.0044480444397777315\n",
      "Epoch 244/300\n",
      "Average training loss: 0.049077952626678675\n",
      "Average test loss: 0.00423800054192543\n",
      "Epoch 245/300\n",
      "Average training loss: 0.049154882467455335\n",
      "Average test loss: 0.004336366920835442\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04912182585563925\n",
      "Average test loss: 0.004259803909394476\n",
      "Epoch 247/300\n",
      "Average training loss: 0.049041982743475175\n",
      "Average test loss: 0.0044105701512760585\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0490880193511645\n",
      "Average test loss: 0.004350814783324798\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0490479394627942\n",
      "Average test loss: 0.004255534167091052\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04902486519349946\n",
      "Average test loss: 0.004277055545399587\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04897896600763003\n",
      "Average test loss: 0.004308819270796246\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04897742899921205\n",
      "Average test loss: 0.004489952481041352\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04897103748387761\n",
      "Average test loss: 0.004402297093015578\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04890870077742471\n",
      "Average test loss: 0.004414110271881024\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04891239771909184\n",
      "Average test loss: 0.004395027393682136\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04892787500222524\n",
      "Average test loss: 0.004283123213383887\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04888154089781973\n",
      "Average test loss: 0.004313927116079463\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04879076567954487\n",
      "Average test loss: 0.004280743544300397\n",
      "Epoch 259/300\n",
      "Average training loss: 0.048794879992802936\n",
      "Average test loss: 0.004298630672196547\n",
      "Epoch 260/300\n",
      "Average training loss: 0.048755962050623367\n",
      "Average test loss: 0.004291702965067493\n",
      "Epoch 261/300\n",
      "Average training loss: 0.048778495258755154\n",
      "Average test loss: 0.004328848543059495\n",
      "Epoch 262/300\n",
      "Average training loss: 0.048834848076105115\n",
      "Average test loss: 0.0043505926541984085\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0487733783920606\n",
      "Average test loss: 0.004485081551182601\n",
      "Epoch 264/300\n",
      "Average training loss: 0.048672488381465276\n",
      "Average test loss: 0.004295775236768855\n",
      "Epoch 265/300\n",
      "Average training loss: 0.048718369778659606\n",
      "Average test loss: 0.004386332668364048\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04872030286325349\n",
      "Average test loss: 0.004278120619141393\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04873328167200088\n",
      "Average test loss: 0.004330129531936513\n",
      "Epoch 268/300\n",
      "Average training loss: 0.048666287908951444\n",
      "Average test loss: 0.004503593681794074\n",
      "Epoch 269/300\n",
      "Average training loss: 0.048652971029281615\n",
      "Average test loss: 0.004401601770685779\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04860543732510673\n",
      "Average test loss: 0.004414376591228776\n",
      "Epoch 271/300\n",
      "Average training loss: 0.048631661044226754\n",
      "Average test loss: 0.00425233421391911\n",
      "Epoch 272/300\n",
      "Average training loss: 0.048602562059958776\n",
      "Average test loss: 0.004380689672711823\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04855916212995847\n",
      "Average test loss: 0.004446303019093142\n",
      "Epoch 274/300\n",
      "Average training loss: 0.048540228734413784\n",
      "Average test loss: 0.004377952349682649\n",
      "Epoch 275/300\n",
      "Average training loss: 0.048569232278399994\n",
      "Average test loss: 0.004308921005576849\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04847960032522678\n",
      "Average test loss: 0.004482906235588922\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04847238520781199\n",
      "Average test loss: 0.004318418003618717\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04844360785351859\n",
      "Average test loss: 0.004424020490298669\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04849384155538347\n",
      "Average test loss: 0.004399149287698997\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04845429635710186\n",
      "Average test loss: 0.0043876738871137305\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04836074810889032\n",
      "Average test loss: 0.004396764857280585\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04840564549300406\n",
      "Average test loss: 0.0043130166431268055\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04838609540462494\n",
      "Average test loss: 0.004385597979442941\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04840702136688762\n",
      "Average test loss: 0.004720266869291663\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04837728569242689\n",
      "Average test loss: 0.0042941102522114916\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04830210913883315\n",
      "Average test loss: 0.004412384593238433\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04834377141131295\n",
      "Average test loss: 0.004385540302325454\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04823274922039774\n",
      "Average test loss: 0.004416934565123584\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04831887919704119\n",
      "Average test loss: 0.00446027540249957\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04830254717999034\n",
      "Average test loss: 0.0044156751500235665\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04834024611777729\n",
      "Average test loss: 0.0043030819156103664\n",
      "Epoch 292/300\n",
      "Average training loss: 0.048249686694807475\n",
      "Average test loss: 0.004287021411375867\n",
      "Epoch 293/300\n",
      "Average training loss: 0.048172065605719884\n",
      "Average test loss: 0.0042872830451362665\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04815497006310357\n",
      "Average test loss: 0.004333132482237286\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04819039546781116\n",
      "Average test loss: 0.004270454845080773\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04820654073688719\n",
      "Average test loss: 0.004367661150793235\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04819452181458473\n",
      "Average test loss: 0.00445679704989824\n",
      "Epoch 298/300\n",
      "Average training loss: 0.048146089133289126\n",
      "Average test loss: 0.004341033793158001\n",
      "Epoch 299/300\n",
      "Average training loss: 0.048135985099607044\n",
      "Average test loss: 0.004293722342285845\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04805900906523069\n",
      "Average test loss: 0.004327452560265859\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9132466280725268\n",
      "Average test loss: 0.004913513340883785\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2401613357729382\n",
      "Average test loss: 0.0044185957544379765\n",
      "Epoch 3/300\n",
      "Average training loss: 0.14964500670962863\n",
      "Average test loss: 0.004239183376232783\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11146780253118939\n",
      "Average test loss: 0.004120833900653654\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0926714068584972\n",
      "Average test loss: 0.003962354166640176\n",
      "Epoch 6/300\n",
      "Average training loss: 0.0818589331706365\n",
      "Average test loss: 0.003843464566187726\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07467048621177673\n",
      "Average test loss: 0.0037902336496238908\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06967025851541095\n",
      "Average test loss: 0.0037206552798549333\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06602391035689248\n",
      "Average test loss: 0.0036839225010739433\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0632891732222504\n",
      "Average test loss: 0.0036497395880934263\n",
      "Epoch 11/300\n",
      "Average training loss: 0.061087687098317675\n",
      "Average test loss: 0.0035772108143816393\n",
      "Epoch 12/300\n",
      "Average training loss: 0.059339478823873734\n",
      "Average test loss: 0.0038105047887398136\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05805342665976948\n",
      "Average test loss: 0.003494023267593649\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05699794595440229\n",
      "Average test loss: 0.003493766518516673\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05610055771138933\n",
      "Average test loss: 0.0034572216450340218\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05535393808285395\n",
      "Average test loss: 0.0034674159420861137\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05466421380639076\n",
      "Average test loss: 0.0033493976257741452\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05409312726060549\n",
      "Average test loss: 0.0033196423465592993\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05354722957809766\n",
      "Average test loss: 0.0032797437080492576\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05299647070633041\n",
      "Average test loss: 0.0032745421754403247\n",
      "Epoch 21/300\n",
      "Average training loss: 0.052516330156061386\n",
      "Average test loss: 0.0032690281226403183\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05203728291723463\n",
      "Average test loss: 0.003359620443648762\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05165245075358285\n",
      "Average test loss: 0.003218489221607645\n",
      "Epoch 24/300\n",
      "Average training loss: 0.051223899443944294\n",
      "Average test loss: 0.003223672944224543\n",
      "Epoch 25/300\n",
      "Average training loss: 0.050794909470611146\n",
      "Average test loss: 0.003196896799736553\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05041698154144817\n",
      "Average test loss: 0.0031893363249384694\n",
      "Epoch 27/300\n",
      "Average training loss: 0.050098662568463216\n",
      "Average test loss: 0.003142163383257058\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04971098773347007\n",
      "Average test loss: 0.00313154103855292\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04941279348731041\n",
      "Average test loss: 0.0031430390419231523\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04908470827009943\n",
      "Average test loss: 0.0031244814818104107\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04874548030893008\n",
      "Average test loss: 0.0031138246910025675\n",
      "Epoch 32/300\n",
      "Average training loss: 0.048525345395008725\n",
      "Average test loss: 0.0031008237140874068\n",
      "Epoch 33/300\n",
      "Average training loss: 0.048258337295717664\n",
      "Average test loss: 0.003094380803199278\n",
      "Epoch 34/300\n",
      "Average training loss: 0.047992489887608424\n",
      "Average test loss: 0.003081439278398951\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04775896670089828\n",
      "Average test loss: 0.003090945852506492\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04753517700897323\n",
      "Average test loss: 0.003112542542318503\n",
      "Epoch 37/300\n",
      "Average training loss: 0.047237826277812325\n",
      "Average test loss: 0.0030806125371406474\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04701342092620002\n",
      "Average test loss: 0.0030703578882126346\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04686460941698816\n",
      "Average test loss: 0.0031264961568845642\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04664758928616842\n",
      "Average test loss: 0.0030490207587265308\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04651131770014763\n",
      "Average test loss: 0.003033354617241356\n",
      "Epoch 42/300\n",
      "Average training loss: 0.046255061811871\n",
      "Average test loss: 0.0030491672565953597\n",
      "Epoch 43/300\n",
      "Average training loss: 0.046079273909330365\n",
      "Average test loss: 0.003041590306493971\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04592030391097069\n",
      "Average test loss: 0.003036004075159629\n",
      "Epoch 45/300\n",
      "Average training loss: 0.045730830013751986\n",
      "Average test loss: 0.0030939939111057253\n",
      "Epoch 46/300\n",
      "Average training loss: 0.045567820499340696\n",
      "Average test loss: 0.0030266510407543847\n",
      "Epoch 47/300\n",
      "Average training loss: 0.045431448385119436\n",
      "Average test loss: 0.003043475593336754\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04533000706301795\n",
      "Average test loss: 0.0030380401596840885\n",
      "Epoch 49/300\n",
      "Average training loss: 0.045069085773494506\n",
      "Average test loss: 0.0030314848352637554\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04494592074553172\n",
      "Average test loss: 0.003059376892530256\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04483461261457867\n",
      "Average test loss: 0.0030821432889335683\n",
      "Epoch 52/300\n",
      "Average training loss: 0.044669766508870654\n",
      "Average test loss: 0.003028517302332653\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04449624661273426\n",
      "Average test loss: 0.0030140293838663234\n",
      "Epoch 54/300\n",
      "Average training loss: 0.044348081714577145\n",
      "Average test loss: 0.0030179361425754095\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04426010322239664\n",
      "Average test loss: 0.003017834215528435\n",
      "Epoch 56/300\n",
      "Average training loss: 0.044078069186872904\n",
      "Average test loss: 0.0030466735290570392\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04394106197853883\n",
      "Average test loss: 0.0031102144989288514\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0437693368461397\n",
      "Average test loss: 0.0031826778538525105\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04367249313990275\n",
      "Average test loss: 0.0030554729776663915\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04353780619634522\n",
      "Average test loss: 0.00305416299837331\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04332981182138125\n",
      "Average test loss: 0.003012142567170991\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0432601858874162\n",
      "Average test loss: 0.003030467349621985\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04310670050978661\n",
      "Average test loss: 0.0030295893371933037\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04301455172896385\n",
      "Average test loss: 0.0030353757474157546\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04287119730975893\n",
      "Average test loss: 0.003046979272324178\n",
      "Epoch 66/300\n",
      "Average training loss: 0.042721030056476594\n",
      "Average test loss: 0.003046007759248217\n",
      "Epoch 67/300\n",
      "Average training loss: 0.042564998222721945\n",
      "Average test loss: 0.0030398170542385843\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04248412855466207\n",
      "Average test loss: 0.003050194254145026\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04235464739468363\n",
      "Average test loss: 0.0031177322732077704\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04220661771297455\n",
      "Average test loss: 0.0031106433611777094\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04211545059747166\n",
      "Average test loss: 0.003069429551975595\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04204331863588757\n",
      "Average test loss: 0.0030595683716237547\n",
      "Epoch 73/300\n",
      "Average training loss: 0.041880644875268144\n",
      "Average test loss: 0.0031476086845828428\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04177647444936964\n",
      "Average test loss: 0.0031088495266934236\n",
      "Epoch 75/300\n",
      "Average training loss: 0.041607277949651085\n",
      "Average test loss: 0.0030645050507866673\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04150327448878023\n",
      "Average test loss: 0.0031263775808943644\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04138534383310212\n",
      "Average test loss: 0.003115062543294496\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04129659701055951\n",
      "Average test loss: 0.003089924058980412\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04119077239433924\n",
      "Average test loss: 0.003126016693603661\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04108481024371253\n",
      "Average test loss: 0.0031823747739609743\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04099758812785149\n",
      "Average test loss: 0.003166471030977037\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04084435970915688\n",
      "Average test loss: 0.003092937338890301\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04082147474421395\n",
      "Average test loss: 0.003186013234158357\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04070586201548576\n",
      "Average test loss: 0.0031473296117037535\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04062853056854672\n",
      "Average test loss: 0.003154668061269654\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04051888117028607\n",
      "Average test loss: 0.0031644153443889486\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04039272274076939\n",
      "Average test loss: 0.003163070040444533\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04032878158821\n",
      "Average test loss: 0.0031060218947629135\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04030005359318521\n",
      "Average test loss: 0.003355332721852594\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04017906459669272\n",
      "Average test loss: 0.0030950044414235485\n",
      "Epoch 91/300\n",
      "Average training loss: 0.040056536386410393\n",
      "Average test loss: 0.0031399753219965433\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04001018966899978\n",
      "Average test loss: 0.0031307267316927514\n",
      "Epoch 93/300\n",
      "Average training loss: 0.039943970955080454\n",
      "Average test loss: 0.003134025772412618\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0398294036520852\n",
      "Average test loss: 0.003164591291298469\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0398752283387714\n",
      "Average test loss: 0.0031253703733285267\n",
      "Epoch 96/300\n",
      "Average training loss: 0.039635686761803096\n",
      "Average test loss: 0.003116929035840763\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03968636075655619\n",
      "Average test loss: 0.003172943918241395\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03955325072341495\n",
      "Average test loss: 0.003203753439709544\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03944624124301804\n",
      "Average test loss: 0.003149266010978156\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0393943462603622\n",
      "Average test loss: 0.00314319117863973\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03935499408344428\n",
      "Average test loss: 0.003244729105383158\n",
      "Epoch 102/300\n",
      "Average training loss: 0.039198137036628194\n",
      "Average test loss: 0.003167044547696908\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0392600744780567\n",
      "Average test loss: 0.003141346757610639\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03914266661802927\n",
      "Average test loss: 0.003142033934800161\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03903208491868443\n",
      "Average test loss: 0.0031329286073644956\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03904275496469604\n",
      "Average test loss: 0.0032292235422258578\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03893004120058483\n",
      "Average test loss: 0.003109439487465554\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0388588954574532\n",
      "Average test loss: 0.0032138275098469522\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03883296536074744\n",
      "Average test loss: 0.0032045525833964347\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0387247142692407\n",
      "Average test loss: 0.0031915853365013995\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03869967690110206\n",
      "Average test loss: 0.0032064813625895315\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03864435651898384\n",
      "Average test loss: 0.003178172935421268\n",
      "Epoch 113/300\n",
      "Average training loss: 0.038593298325936\n",
      "Average test loss: 0.003142325087140004\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03863828388353189\n",
      "Average test loss: 0.0032256703101512458\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03846302451358901\n",
      "Average test loss: 0.0031613453100952835\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03845015084081226\n",
      "Average test loss: 0.003246824441684617\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03842260945836703\n",
      "Average test loss: 0.003173480526647634\n",
      "Epoch 118/300\n",
      "Average training loss: 0.038318028642071615\n",
      "Average test loss: 0.0031695659898428453\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03832632885376612\n",
      "Average test loss: 0.003183740048772759\n",
      "Epoch 120/300\n",
      "Average training loss: 0.038181463312771585\n",
      "Average test loss: 0.003290870390832424\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03810792239672608\n",
      "Average test loss: 0.00328779896762636\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03814763895008299\n",
      "Average test loss: 0.003242838826444414\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03806822739375962\n",
      "Average test loss: 0.0032039404343813657\n",
      "Epoch 124/300\n",
      "Average training loss: 0.038020198318693375\n",
      "Average test loss: 0.0032956584211852815\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03801399389902751\n",
      "Average test loss: 0.0032011576493581138\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03793636852006117\n",
      "Average test loss: 0.003272787642561727\n",
      "Epoch 127/300\n",
      "Average training loss: 0.037898976408772996\n",
      "Average test loss: 0.0031886415378087098\n",
      "Epoch 128/300\n",
      "Average training loss: 0.037842930297056834\n",
      "Average test loss: 0.003240486531207959\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03783142338693142\n",
      "Average test loss: 0.0032112906558646095\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03773947881658872\n",
      "Average test loss: 0.0032150023236042925\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03774345910880301\n",
      "Average test loss: 0.00317645811330941\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03769609256254302\n",
      "Average test loss: 0.0031642000209540127\n",
      "Epoch 133/300\n",
      "Average training loss: 0.037640674594375825\n",
      "Average test loss: 0.00314002085228761\n",
      "Epoch 134/300\n",
      "Average training loss: 0.037631335304843055\n",
      "Average test loss: 0.0032523367315944697\n",
      "Epoch 135/300\n",
      "Average training loss: 0.037554611566993926\n",
      "Average test loss: 0.003194727603139149\n",
      "Epoch 136/300\n",
      "Average training loss: 0.037430158025688595\n",
      "Average test loss: 0.0034055170890771682\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03747645183404287\n",
      "Average test loss: 0.003222537579221858\n",
      "Epoch 138/300\n",
      "Average training loss: 0.037462436942590605\n",
      "Average test loss: 0.003284153540722198\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03740119753777981\n",
      "Average test loss: 0.0032713416636817984\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03734348638686869\n",
      "Average test loss: 0.00345963462938865\n",
      "Epoch 141/300\n",
      "Average training loss: 0.037307990309264925\n",
      "Average test loss: 0.003237059289796485\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03727406465344959\n",
      "Average test loss: 0.0032049348776539166\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037265008764134515\n",
      "Average test loss: 0.0032544165044609045\n",
      "Epoch 144/300\n",
      "Average training loss: 0.037218587987952766\n",
      "Average test loss: 0.003239249999738402\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03715968836843968\n",
      "Average test loss: 0.0032916682593317494\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03719265115592215\n",
      "Average test loss: 0.00316961680052595\n",
      "Epoch 147/300\n",
      "Average training loss: 0.037072179302573206\n",
      "Average test loss: 0.0032032928994546335\n",
      "Epoch 148/300\n",
      "Average training loss: 0.037012627550297314\n",
      "Average test loss: 0.0032527602275626527\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03701293529735671\n",
      "Average test loss: 0.0032632676845209467\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03702764919400215\n",
      "Average test loss: 0.0032377239759597515\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03706983193092876\n",
      "Average test loss: 0.003403555387424098\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0369677938885159\n",
      "Average test loss: 0.003250643823295832\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03690480182568232\n",
      "Average test loss: 0.0032344674296263193\n",
      "Epoch 154/300\n",
      "Average training loss: 0.036913848620322015\n",
      "Average test loss: 0.0032545499900976816\n",
      "Epoch 155/300\n",
      "Average training loss: 0.036827813920047546\n",
      "Average test loss: 0.003436867901848422\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0368736463089784\n",
      "Average test loss: 0.003275996087325944\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03677408512102233\n",
      "Average test loss: 0.003297341176205211\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03676298557718595\n",
      "Average test loss: 0.0033430597484111787\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03669099415010876\n",
      "Average test loss: 0.003252922389449345\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036714249753289754\n",
      "Average test loss: 0.0032171158373562824\n",
      "Epoch 161/300\n",
      "Average training loss: 0.036645037680864335\n",
      "Average test loss: 0.0032641488646881446\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0366070966720581\n",
      "Average test loss: 0.003250719613292151\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036641187601619295\n",
      "Average test loss: 0.003326442852202389\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03657498985197809\n",
      "Average test loss: 0.0032396410800930526\n",
      "Epoch 165/300\n",
      "Average training loss: 0.036522087567382386\n",
      "Average test loss: 0.003283959777818786\n",
      "Epoch 166/300\n",
      "Average training loss: 0.036499956804845066\n",
      "Average test loss: 0.003282293451536033\n",
      "Epoch 167/300\n",
      "Average training loss: 0.036532558229234485\n",
      "Average test loss: 0.003209736942210131\n",
      "Epoch 168/300\n",
      "Average training loss: 0.036533657181594105\n",
      "Average test loss: 0.0033938742010957666\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03639017442199919\n",
      "Average test loss: 0.0032444095975822872\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03638060510158539\n",
      "Average test loss: 0.003346596360620525\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03636709495385488\n",
      "Average test loss: 0.0033448963419844707\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03633011007805665\n",
      "Average test loss: 0.0033737956856687863\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03629761051469379\n",
      "Average test loss: 0.0032939945525593227\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03639501305421194\n",
      "Average test loss: 0.003290274637647801\n",
      "Epoch 175/300\n",
      "Average training loss: 0.036245370189348855\n",
      "Average test loss: 0.0033688159893370336\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03629870710108015\n",
      "Average test loss: 0.0031953520056688125\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03620790250764953\n",
      "Average test loss: 0.003275280547224813\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03619623042146365\n",
      "Average test loss: 0.0033081472027632927\n",
      "Epoch 179/300\n",
      "Average training loss: 0.036209830267561804\n",
      "Average test loss: 0.0033058431872891054\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03615547878543536\n",
      "Average test loss: 0.0033055326857914526\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0361386398093568\n",
      "Average test loss: 0.0032871955041256216\n",
      "Epoch 182/300\n",
      "Average training loss: 0.036088763571447795\n",
      "Average test loss: 0.0032600753563973637\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03608819022940265\n",
      "Average test loss: 0.0032708846708345745\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03604739406704903\n",
      "Average test loss: 0.003386447336524725\n",
      "Epoch 185/300\n",
      "Average training loss: 0.036061438739299775\n",
      "Average test loss: 0.0033367252993500894\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03597994410494963\n",
      "Average test loss: 0.003285057075528635\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03603431454135312\n",
      "Average test loss: 0.0033503621474115387\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03595481601688597\n",
      "Average test loss: 0.003325649692780442\n",
      "Epoch 189/300\n",
      "Average training loss: 0.035908346861600875\n",
      "Average test loss: 0.003277970106030504\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03587554210093286\n",
      "Average test loss: 0.0032344877190060087\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03590989001426432\n",
      "Average test loss: 0.0033711147980971467\n",
      "Epoch 192/300\n",
      "Average training loss: 0.035891053163343005\n",
      "Average test loss: 0.003376221667147345\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03587162531084485\n",
      "Average test loss: 0.0032555296473825972\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03585566179951032\n",
      "Average test loss: 0.003314815319246716\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03572770089904467\n",
      "Average test loss: 0.003278894753091865\n",
      "Epoch 196/300\n",
      "Average training loss: 0.035761343886454904\n",
      "Average test loss: 0.003346144589698977\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03575565531518724\n",
      "Average test loss: 0.0032521479123582444\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03576026709708902\n",
      "Average test loss: 0.00331974665592942\n",
      "Epoch 199/300\n",
      "Average training loss: 0.035713670363028846\n",
      "Average test loss: 0.003337663433824976\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03571039887269338\n",
      "Average test loss: 0.0032886198934995465\n",
      "Epoch 201/300\n",
      "Average training loss: 0.035657987687322826\n",
      "Average test loss: 0.0033542740057326027\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0356105173031489\n",
      "Average test loss: 0.003233992476844125\n",
      "Epoch 203/300\n",
      "Average training loss: 0.035720793147881826\n",
      "Average test loss: 0.0032724954295489522\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03561779726545016\n",
      "Average test loss: 0.0033035149333170716\n",
      "Epoch 205/300\n",
      "Average training loss: 0.035525095558828775\n",
      "Average test loss: 0.0032537974764903386\n",
      "Epoch 206/300\n",
      "Average training loss: 0.035528537334667314\n",
      "Average test loss: 0.003324438589728541\n",
      "Epoch 207/300\n",
      "Average training loss: 0.035507931076818045\n",
      "Average test loss: 0.0032872842438519\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03555815997719765\n",
      "Average test loss: 0.003338354993818535\n",
      "Epoch 209/300\n",
      "Average training loss: 0.035501218911674286\n",
      "Average test loss: 0.0033586788945313955\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03547492725484901\n",
      "Average test loss: 0.003290604694849915\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03544597240454621\n",
      "Average test loss: 0.0033121161920328934\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03547595530086094\n",
      "Average test loss: 0.0033488269123352235\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03549070801006423\n",
      "Average test loss: 0.0033156936454276243\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03540478548738692\n",
      "Average test loss: 0.003377604468415181\n",
      "Epoch 215/300\n",
      "Average training loss: 0.035334911280208166\n",
      "Average test loss: 0.0033672820495234596\n",
      "Epoch 216/300\n",
      "Average training loss: 0.035369686889979574\n",
      "Average test loss: 0.003395232860619823\n",
      "Epoch 217/300\n",
      "Average training loss: 0.035391404459873835\n",
      "Average test loss: 0.003330714522757464\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03530785981814066\n",
      "Average test loss: 0.0032949972040951254\n",
      "Epoch 219/300\n",
      "Average training loss: 0.035380168401532706\n",
      "Average test loss: 0.0033999214615258906\n",
      "Epoch 220/300\n",
      "Average training loss: 0.035251598886317674\n",
      "Average test loss: 0.003394022836453385\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03532755686508285\n",
      "Average test loss: 0.0033683875871615278\n",
      "Epoch 222/300\n",
      "Average training loss: 0.035219022946225274\n",
      "Average test loss: 0.003311526283621788\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03525428197118971\n",
      "Average test loss: 0.003306906315187613\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0352188453608089\n",
      "Average test loss: 0.0033458978119823668\n",
      "Epoch 225/300\n",
      "Average training loss: 0.035265444931056766\n",
      "Average test loss: 0.0033189154039654466\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03520943236351013\n",
      "Average test loss: 0.0033677681047055456\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03515473264786932\n",
      "Average test loss: 0.0033404270431233777\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0351379490610626\n",
      "Average test loss: 0.003380177875773774\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03501877063843939\n",
      "Average test loss: 0.0033801882724381156\n",
      "Epoch 232/300\n",
      "Average training loss: 0.035106707566314274\n",
      "Average test loss: 0.003440771268680692\n",
      "Epoch 233/300\n",
      "Average training loss: 0.035036774115429985\n",
      "Average test loss: 0.0033151963189658193\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03503199044863383\n",
      "Average test loss: 0.0033271039633287324\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03506975696484248\n",
      "Average test loss: 0.0033275915380153393\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0350198340051704\n",
      "Average test loss: 0.0033008788294262357\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03499654463926951\n",
      "Average test loss: 0.0033413654828651084\n",
      "Epoch 240/300\n",
      "Average training loss: 0.034959727479351894\n",
      "Average test loss: 0.0033550961474991506\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03491245371103287\n",
      "Average test loss: 0.0033619220707979468\n",
      "Epoch 242/300\n",
      "Average training loss: 0.034955665760570104\n",
      "Average test loss: 0.0032750933923655088\n",
      "Epoch 243/300\n",
      "Average training loss: 0.034928061170710456\n",
      "Average test loss: 0.003346757067160474\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03489920152558221\n",
      "Average test loss: 0.0038206539783212875\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03490704824195968\n",
      "Average test loss: 0.0033223199275218777\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03482602535684903\n",
      "Average test loss: 0.003364543451824122\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03487466612789366\n",
      "Average test loss: 0.003298198173029555\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03486176855365435\n",
      "Average test loss: 0.003405839506122801\n",
      "Epoch 249/300\n",
      "Average training loss: 0.034826878269513445\n",
      "Average test loss: 0.0032682607320861684\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03479389241006639\n",
      "Average test loss: 0.003407694225716922\n",
      "Epoch 251/300\n",
      "Average training loss: 0.034739543840289114\n",
      "Average test loss: 0.003523399708999528\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03484051245616542\n",
      "Average test loss: 0.0033344218615028592\n",
      "Epoch 253/300\n",
      "Average training loss: 0.034732171419594025\n",
      "Average test loss: 0.003320398792417513\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03475159323546621\n",
      "Average test loss: 0.0034559823709229627\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03475048314862781\n",
      "Average test loss: 0.0034380361110799844\n",
      "Epoch 256/300\n",
      "Average training loss: 0.034781991273164746\n",
      "Average test loss: 0.0034150650459859105\n",
      "Epoch 257/300\n",
      "Average training loss: 0.034686402675178314\n",
      "Average test loss: 0.003451410243494643\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03464514780375692\n",
      "Average test loss: 0.0033026743171115716\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03467887325584888\n",
      "Average test loss: 0.003414996872552567\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0346313509196043\n",
      "Average test loss: 0.0033606002651568917\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03460448341237174\n",
      "Average test loss: 0.0033644865275257166\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03460087362594075\n",
      "Average test loss: 0.00337070652242336\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03456709940565957\n",
      "Average test loss: 0.0035990981879747575\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034606365780035654\n",
      "Average test loss: 0.003374274837474028\n",
      "Epoch 268/300\n",
      "Average training loss: 0.034556095212697986\n",
      "Average test loss: 0.003331780462215344\n",
      "Epoch 269/300\n",
      "Average training loss: 0.034565028263462916\n",
      "Average test loss: 0.0034510499154114063\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0345080584552553\n",
      "Average test loss: 0.0036524646793388658\n",
      "Epoch 271/300\n",
      "Average training loss: 0.034536117977566186\n",
      "Average test loss: 0.0032871443242248563\n",
      "Epoch 272/300\n",
      "Average training loss: 0.034504658884472315\n",
      "Average test loss: 0.003281333074387577\n",
      "Epoch 273/300\n",
      "Average training loss: 0.034492655974295405\n",
      "Average test loss: 0.0033724342356953358\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03457824195093579\n",
      "Average test loss: 0.0034069056268781423\n",
      "Epoch 275/300\n",
      "Average training loss: 0.034470200437638494\n",
      "Average test loss: 0.0034170399523443646\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03442337117261357\n",
      "Average test loss: 0.003378350498775641\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03445694311790996\n",
      "Average test loss: 0.003315456948760483\n",
      "Epoch 278/300\n",
      "Average training loss: 0.034459161990218694\n",
      "Average test loss: 0.0032908145433498755\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03442323417133755\n",
      "Average test loss: 0.0034119554681496488\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03446678452359306\n",
      "Average test loss: 0.003372951155528426\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03439843634598785\n",
      "Average test loss: 0.0033871785617536967\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03436224080787765\n",
      "Average test loss: 0.003382285073606504\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03439816953241825\n",
      "Average test loss: 0.0033672719622651736\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03437186916338073\n",
      "Average test loss: 0.0033563789745999707\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03434335705306795\n",
      "Average test loss: 0.003319308350690537\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03431819840934541\n",
      "Average test loss: 0.0033610955499526526\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03433346239063475\n",
      "Average test loss: 0.0034831930711451504\n",
      "Epoch 288/300\n",
      "Average training loss: 0.034324245891637274\n",
      "Average test loss: 0.003455997505121761\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03436539874308639\n",
      "Average test loss: 0.003474052909244266\n",
      "Epoch 290/300\n",
      "Average training loss: 0.034260976248317296\n",
      "Average test loss: 0.0032723132734083465\n",
      "Epoch 291/300\n",
      "Average training loss: 0.034248351256052655\n",
      "Average test loss: 0.00334928568204244\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0343084420180983\n",
      "Average test loss: 0.003391856799316075\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03428533780574799\n",
      "Average test loss: 0.003347386055936416\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03418955396943622\n",
      "Average test loss: 0.003279431108592285\n",
      "Epoch 295/300\n",
      "Average training loss: 0.034191708541578714\n",
      "Average test loss: 0.003414098618345128\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03420359119772911\n",
      "Average test loss: 0.003359325170930889\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03415565919379393\n",
      "Average test loss: 0.0033737918871144454\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0341737952745623\n",
      "Average test loss: 0.003353629010832972\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03419563922782739\n",
      "Average test loss: 0.003396161124524143\n",
      "Epoch 300/300\n",
      "Average training loss: 0.034150427447425\n",
      "Average test loss: 0.003342348412817551\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.887831047879325\n",
      "Average test loss: 0.004382247205409738\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0958250929514567\n",
      "Average test loss: 0.003479212509468198\n",
      "Epoch 5/300\n",
      "Average training loss: 0.08060099795791838\n",
      "Average test loss: 0.0033298328307767707\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07126127684116364\n",
      "Average test loss: 0.003226168538754185\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06468429018391503\n",
      "Average test loss: 0.003133419173873133\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05980210246973568\n",
      "Average test loss: 0.0030560000364979107\n",
      "Epoch 9/300\n",
      "Average training loss: 0.0562704732020696\n",
      "Average test loss: 0.003046159545166625\n",
      "Epoch 10/300\n",
      "Average training loss: 0.05374640592932701\n",
      "Average test loss: 0.0029578262101858854\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05177666949894693\n",
      "Average test loss: 0.0028531845158172977\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05013684337337812\n",
      "Average test loss: 0.0029193704411801364\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0487715306977431\n",
      "Average test loss: 0.002730091667941047\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04762092893322309\n",
      "Average test loss: 0.002794251999921269\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04653549731440014\n",
      "Average test loss: 0.0027239226684388188\n",
      "Epoch 16/300\n",
      "Average training loss: 0.045666829145616955\n",
      "Average test loss: 0.0026010687053203583\n",
      "Epoch 17/300\n",
      "Average training loss: 0.044852780958016716\n",
      "Average test loss: 0.00262177807941205\n",
      "Epoch 18/300\n",
      "Average training loss: 0.044136288328303235\n",
      "Average test loss: 0.002557412867123882\n",
      "Epoch 19/300\n",
      "Average training loss: 0.043438200030061935\n",
      "Average test loss: 0.002527979925274849\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04276823117666775\n",
      "Average test loss: 0.002510463055016266\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04224653614560763\n",
      "Average test loss: 0.0025391919472151333\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04171213371223874\n",
      "Average test loss: 0.002481286175135109\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04117907441655795\n",
      "Average test loss: 0.0024563546375268035\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03978863228360812\n",
      "Average test loss: 0.002448608098551631\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03944861852626006\n",
      "Average test loss: 0.002446991585091584\n",
      "Epoch 28/300\n",
      "Average training loss: 0.039009039637115266\n",
      "Average test loss: 0.0024179345251371465\n",
      "Epoch 29/300\n",
      "Average training loss: 0.038680014196369385\n",
      "Average test loss: 0.002402085247966978\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03836995412574874\n",
      "Average test loss: 0.002382869775407016\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03806433999869559\n",
      "Average test loss: 0.002346803108023273\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03773821050259802\n",
      "Average test loss: 0.002352780691658457\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03749226632714271\n",
      "Average test loss: 0.002358696707420879\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03724777616560459\n",
      "Average test loss: 0.0023387880528138744\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0369505690700478\n",
      "Average test loss: 0.0023631455277403194\n",
      "Epoch 36/300\n",
      "Average training loss: 0.036802096479468874\n",
      "Average test loss: 0.002326341999280784\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03651011722286542\n",
      "Average test loss: 0.0023269860877965886\n",
      "Epoch 38/300\n",
      "Average training loss: 0.036356568495432534\n",
      "Average test loss: 0.0023458496692279973\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03614366554054949\n",
      "Average test loss: 0.002361297218956881\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03596600483192338\n",
      "Average test loss: 0.0023157323703376785\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03572700458102756\n",
      "Average test loss: 0.0022879435155126783\n",
      "Epoch 42/300\n",
      "Average training loss: 0.035566645854049256\n",
      "Average test loss: 0.0022860841063989533\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03536407349672582\n",
      "Average test loss: 0.0023156977931244505\n",
      "Epoch 44/300\n",
      "Average training loss: 0.035274674355983734\n",
      "Average test loss: 0.0022751270660923586\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03504953264693419\n",
      "Average test loss: 0.0022928311566097867\n",
      "Epoch 46/300\n",
      "Average training loss: 0.034960108353032\n",
      "Average test loss: 0.0023002033532700606\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03473965937561459\n",
      "Average test loss: 0.002290333979245689\n",
      "Epoch 48/300\n",
      "Average training loss: 0.034259885504841804\n",
      "Average test loss: 0.002299896505040427\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03404459400475025\n",
      "Average test loss: 0.0023119955849316386\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03391843866970804\n",
      "Average test loss: 0.0022713034993244543\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03377685395876567\n",
      "Average test loss: 0.0022800121690250107\n",
      "Epoch 55/300\n",
      "Average training loss: 0.033659475573235086\n",
      "Average test loss: 0.002278560459199879\n",
      "Epoch 56/300\n",
      "Average training loss: 0.033572297622760136\n",
      "Average test loss: 0.0023260093353067836\n",
      "Epoch 57/300\n",
      "Average training loss: 0.033433802452352314\n",
      "Average test loss: 0.002308164983573887\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03336466475824515\n",
      "Average test loss: 0.0023129388716899685\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03321213081975778\n",
      "Average test loss: 0.0022715786159452464\n",
      "Epoch 60/300\n",
      "Average training loss: 0.033035307695468266\n",
      "Average test loss: 0.0022599238008260726\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03291450394690037\n",
      "Average test loss: 0.0023086551019094056\n",
      "Epoch 62/300\n",
      "Average training loss: 0.032791192955440945\n",
      "Average test loss: 0.00233196472004056\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03271041174067391\n",
      "Average test loss: 0.002287235288363364\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03257206948598226\n",
      "Average test loss: 0.0022820037106672923\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03249256170623832\n",
      "Average test loss: 0.002406776826000876\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03232880805598365\n",
      "Average test loss: 0.002366086882021692\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03233286932276355\n",
      "Average test loss: 0.0023387358002364637\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03189880744947327\n",
      "Average test loss: 0.0023025191037191286\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03181706323391861\n",
      "Average test loss: 0.0023389612997157705\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03169730977051788\n",
      "Average test loss: 0.0023165171595497264\n",
      "Epoch 73/300\n",
      "Average training loss: 0.031609790081779165\n",
      "Average test loss: 0.02361776296628846\n",
      "Epoch 74/300\n",
      "Average training loss: 0.031546015466252963\n",
      "Average test loss: 0.0023967725237210593\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03144687483873632\n",
      "Average test loss: 0.002335186826272143\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03142094714277321\n",
      "Average test loss: 0.0023357187170121403\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03127389377355576\n",
      "Average test loss: 0.002356618731800053\n",
      "Epoch 78/300\n",
      "Average training loss: 0.031154231798317698\n",
      "Average test loss: 0.0023292867614784175\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03109627410935031\n",
      "Average test loss: 0.0023240684924854173\n",
      "Epoch 80/300\n",
      "Average training loss: 0.031034416053030225\n",
      "Average test loss: 0.0023401083641995985\n",
      "Epoch 81/300\n",
      "Average training loss: 0.030976727315121227\n",
      "Average test loss: 0.0024087592468907434\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03079512997633881\n",
      "Average test loss: 0.0023373559022115335\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03079088553124004\n",
      "Average test loss: 0.002360595621789495\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0306501914328999\n",
      "Average test loss: 0.002352589082179798\n",
      "Epoch 85/300\n",
      "Average training loss: 0.030640567301048173\n",
      "Average test loss: 0.0023250559189667303\n",
      "Epoch 86/300\n",
      "Average training loss: 0.030526616177625126\n",
      "Average test loss: 0.002305620207140843\n",
      "Epoch 87/300\n",
      "Average training loss: 0.030445711659060584\n",
      "Average test loss: 0.0023448508481184642\n",
      "Epoch 88/300\n",
      "Average training loss: 0.030377293974161147\n",
      "Average test loss: 0.0024108974341717033\n",
      "Epoch 89/300\n",
      "Average training loss: 0.030293310468395552\n",
      "Average test loss: 0.0023554430881308186\n",
      "Epoch 90/300\n",
      "Average training loss: 0.030254283919930458\n",
      "Average test loss: 0.002427887218693892\n",
      "Epoch 91/300\n",
      "Average training loss: 0.030193378700150385\n",
      "Average test loss: 0.002407000257116225\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03010481152931849\n",
      "Average test loss: 0.002350293304046823\n",
      "Epoch 93/300\n",
      "Average training loss: 0.030006372291180822\n",
      "Average test loss: 0.0023707419729067218\n",
      "Epoch 94/300\n",
      "Average training loss: 0.029976012070973713\n",
      "Average test loss: 0.002352989421743486\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02990549296637376\n",
      "Average test loss: 0.002360589808060063\n",
      "Epoch 96/300\n",
      "Average training loss: 0.029943985833062067\n",
      "Average test loss: 0.0023457133640638656\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02976539868613084\n",
      "Average test loss: 0.00238527576211426\n",
      "Epoch 98/300\n",
      "Average training loss: 0.02972689919008149\n",
      "Average test loss: 0.0026966280655728444\n",
      "Epoch 99/300\n",
      "Average training loss: 0.029674975173340903\n",
      "Average test loss: 0.0023313072476949955\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02961354814304246\n",
      "Average test loss: 0.002357414383130769\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02949030730790562\n",
      "Average test loss: 0.0024146261711915336\n",
      "Epoch 102/300\n",
      "Average training loss: 0.029508976231018703\n",
      "Average test loss: 0.0024599338182144694\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02945374867485629\n",
      "Average test loss: 0.002429805953366061\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02939879624048869\n",
      "Average test loss: 0.0023517974817401006\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02928768121699492\n",
      "Average test loss: 0.002379311357624829\n",
      "Epoch 106/300\n",
      "Average training loss: 0.029306876223948265\n",
      "Average test loss: 0.0024509681734359925\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02926516379084852\n",
      "Average test loss: 0.00241060594158868\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02915554904109902\n",
      "Average test loss: 0.0024135774583038355\n",
      "Epoch 109/300\n",
      "Average training loss: 0.029163438040349217\n",
      "Average test loss: 0.002451827298021979\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02897629942993323\n",
      "Average test loss: 0.0023934455890622405\n",
      "Epoch 113/300\n",
      "Average training loss: 0.028916192634238138\n",
      "Average test loss: 0.002406237258679337\n",
      "Epoch 114/300\n",
      "Average training loss: 0.028909495941466756\n",
      "Average test loss: 0.0024151597124420932\n",
      "Epoch 115/300\n",
      "Average training loss: 0.028846704699926905\n",
      "Average test loss: 0.0025105583450446525\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02886427185104953\n",
      "Average test loss: 0.00236331915958888\n",
      "Epoch 117/300\n",
      "Average training loss: 0.028785449432002175\n",
      "Average test loss: 0.0023921224010280437\n",
      "Epoch 118/300\n",
      "Average training loss: 0.028716565888788965\n",
      "Average test loss: 0.002413147680875328\n",
      "Epoch 119/300\n",
      "Average training loss: 0.028684742415944736\n",
      "Average test loss: 0.0024353204030129645\n",
      "Epoch 120/300\n",
      "Average training loss: 0.028645357660121388\n",
      "Average test loss: 0.002405099576546086\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02861306399272548\n",
      "Average test loss: 0.0024527806745221216\n",
      "Epoch 122/300\n",
      "Average training loss: 0.028619221018420325\n",
      "Average test loss: 0.0025230587855395344\n",
      "Epoch 123/300\n",
      "Average training loss: 0.028566735439830356\n",
      "Average test loss: 0.0024181192703545094\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0285528048409356\n",
      "Average test loss: 0.002417740784999397\n",
      "Epoch 125/300\n",
      "Average training loss: 0.028520047762327725\n",
      "Average test loss: 0.0024295414230889744\n",
      "Epoch 126/300\n",
      "Average training loss: 0.028457130955325233\n",
      "Average test loss: 0.002422168964934018\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02837095827857653\n",
      "Average test loss: 0.0024346303341703283\n",
      "Epoch 128/300\n",
      "Average training loss: 0.028375801970561346\n",
      "Average test loss: 0.0024733744520280096\n",
      "Epoch 129/300\n",
      "Average training loss: 0.028297120307882628\n",
      "Average test loss: 0.002415150905234946\n",
      "Epoch 130/300\n",
      "Average training loss: 0.028269496252139408\n",
      "Average test loss: 0.0024374616261985567\n",
      "Epoch 131/300\n",
      "Average training loss: 0.02825526709192329\n",
      "Average test loss: 0.002441156373669704\n",
      "Epoch 132/300\n",
      "Average training loss: 0.02824300736685594\n",
      "Average test loss: 0.002563675254376398\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02824818433324496\n",
      "Average test loss: 0.002452570005837414\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02814578427374363\n",
      "Average test loss: 0.0025976024354911515\n",
      "Epoch 135/300\n",
      "Average training loss: 0.028098429146740173\n",
      "Average test loss: 0.0025261933050221867\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02806429447233677\n",
      "Average test loss: 0.0024901546351611616\n",
      "Epoch 137/300\n",
      "Average training loss: 0.028068109936184353\n",
      "Average test loss: 0.002506821586853928\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02803006294866403\n",
      "Average test loss: 0.002486846850771043\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02799113669163651\n",
      "Average test loss: 0.0024824058049254948\n",
      "Epoch 140/300\n",
      "Average training loss: 0.027904943789045015\n",
      "Average test loss: 0.003093428642178575\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02793026873966058\n",
      "Average test loss: 0.0024524928563170964\n",
      "Epoch 142/300\n",
      "Average training loss: 0.027924266260531214\n",
      "Average test loss: 0.002446999699705177\n",
      "Epoch 143/300\n",
      "Average training loss: 0.027935103916459612\n",
      "Average test loss: 0.0024402211817602317\n",
      "Epoch 144/300\n",
      "Average training loss: 0.027945609451995954\n",
      "Average test loss: 0.0024494371589066253\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02779387325876289\n",
      "Average test loss: 0.002486580090597272\n",
      "Epoch 146/300\n",
      "Average training loss: 0.027809278920292856\n",
      "Average test loss: 0.0026135018710047005\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02774691655072901\n",
      "Average test loss: 0.00251833974590732\n",
      "Epoch 148/300\n",
      "Average training loss: 0.027733541591299905\n",
      "Average training loss: 0.027625262985626856\n",
      "Average test loss: 0.002438858822815948\n",
      "Epoch 152/300\n",
      "Average training loss: 0.027624869831734233\n",
      "Average test loss: 0.0024758193443218867\n",
      "Epoch 153/300\n",
      "Average training loss: 0.02764466600285636\n",
      "Average test loss: 0.0025151242452363175\n",
      "Epoch 154/300\n",
      "Average training loss: 0.027555024289422567\n",
      "Average test loss: 0.0024820463775346675\n",
      "Epoch 155/300\n",
      "Average training loss: 0.027566182356741693\n",
      "Average test loss: 0.0025070854822794595\n",
      "Epoch 156/300\n",
      "Average training loss: 0.027551883584923213\n",
      "Average test loss: 0.0024731026016589666\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02751854769223266\n",
      "Average test loss: 0.002532528935621182\n",
      "Epoch 158/300\n",
      "Average training loss: 0.027473103357685935\n",
      "Average test loss: 0.0024493538327515123\n",
      "Epoch 159/300\n",
      "Average training loss: 0.027452418310774697\n",
      "Average test loss: 0.0025666998635149665\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02747974391281605\n",
      "Average test loss: 0.002447505302934183\n",
      "Epoch 161/300\n",
      "Average training loss: 0.027382071397370762\n",
      "Average test loss: 0.0025432435162365437\n",
      "Epoch 162/300\n",
      "Average training loss: 0.027385776152213416\n",
      "Average test loss: 0.0024759077886119486\n",
      "Epoch 163/300\n",
      "Average training loss: 0.027384511424435508\n",
      "Average test loss: 0.0024507849766976304\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02737010639988714\n",
      "Average test loss: 0.0025064568778293\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02735748519334528\n",
      "Average test loss: 0.002526462142666181\n",
      "Epoch 166/300\n",
      "Average training loss: 0.027255877402093676\n",
      "Average test loss: 0.00241653060581949\n",
      "Epoch 167/300\n",
      "Average training loss: 0.027329650214976735\n",
      "Average test loss: 0.0025033119382957617\n",
      "Epoch 168/300\n",
      "Average training loss: 0.02726377067797714\n",
      "Average test loss: 0.0024583334173593255\n",
      "Epoch 169/300\n",
      "Average training loss: 0.027248394582006667\n",
      "Average test loss: 0.0024628320744054183\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02719709170692497\n",
      "Average test loss: 0.002510221457315816\n",
      "Epoch 173/300\n",
      "Average training loss: 0.027124299001362588\n",
      "Average test loss: 0.0025026021289328735\n",
      "Epoch 174/300\n",
      "Average training loss: 0.02715578872296545\n",
      "Average test loss: 0.002477862785466843\n",
      "Epoch 175/300\n",
      "Average training loss: 0.027109360981318685\n",
      "Average test loss: 0.0025237858620368774\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02707917511794302\n",
      "Average test loss: 0.0025562287446939283\n",
      "Epoch 177/300\n",
      "Average training loss: 0.027101237518919838\n",
      "Average test loss: 0.002450672989918126\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02705389278464847\n",
      "Average test loss: 0.002519384129386809\n",
      "Epoch 179/300\n",
      "Average training loss: 0.027026815538605056\n",
      "Average test loss: 0.00264046579785645\n",
      "Epoch 180/300\n",
      "Average training loss: 0.027060007971194056\n",
      "Average test loss: 0.0026969898148543306\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02705346419579453\n",
      "Average test loss: 0.0025346833442648252\n",
      "Epoch 182/300\n",
      "Average training loss: 0.026936117062966027\n",
      "Average test loss: 0.002480006950183047\n",
      "Epoch 183/300\n",
      "Average training loss: 0.026928086653351783\n",
      "Average test loss: 0.0024712394514224594\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02696949492063787\n",
      "Average test loss: 0.00248474704205162\n",
      "Epoch 185/300\n",
      "Average training loss: 0.026956254695852597\n",
      "Average test loss: 0.0024995490337411564\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02691804224418269\n",
      "Average test loss: 0.002525506870407197\n",
      "Epoch 187/300\n",
      "Average training loss: 0.026880998651186625\n",
      "Average test loss: 0.0024950518042056095\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02687332581480344\n",
      "Average test loss: 0.0025145003222342994\n",
      "Epoch 189/300\n",
      "Average training loss: 0.026904569591085115\n",
      "Average test loss: 0.0026396521400246357\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02677456521987915\n",
      "Average test loss: 0.0025893560459630356\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02681535934905211\n",
      "Average test loss: 0.0024851899362272685\n",
      "Epoch 193/300\n",
      "Average training loss: 0.026762886505987907\n",
      "Average test loss: 0.0024940525715549788\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02674038094613287\n",
      "Average test loss: 0.0025134075668950874\n",
      "Epoch 195/300\n",
      "Average training loss: 0.026756034909023178\n",
      "Average test loss: 0.0025650202389806507\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0267643161714077\n",
      "Average test loss: 0.002511926279299789\n",
      "Epoch 197/300\n",
      "Average training loss: 0.026750847061475118\n",
      "Average test loss: 0.0024773661620501016\n",
      "Epoch 198/300\n",
      "Average training loss: 0.026667554846240414\n",
      "Average test loss: 0.002536117986465494\n",
      "Epoch 199/300\n",
      "Average training loss: 0.026705322821935017\n",
      "Average test loss: 0.0026046475989537107\n",
      "Epoch 200/300\n",
      "Average training loss: 0.026661520901653503\n",
      "Average test loss: 0.002606051727301545\n",
      "Epoch 201/300\n",
      "Average training loss: 0.026695094969537522\n",
      "Average test loss: 0.002535781434012784\n",
      "Epoch 202/300\n",
      "Average training loss: 0.026673010397288536\n",
      "Average test loss: 0.002489672362183531\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02664486164516873\n",
      "Average test loss: 0.002574218259503444\n",
      "Epoch 204/300\n",
      "Average training loss: 0.026594835980070963\n",
      "Average test loss: 0.0025314494723247156\n",
      "Epoch 205/300\n",
      "Average training loss: 0.02661744647887018\n",
      "Average test loss: 0.002531832499222623\n",
      "Epoch 206/300\n",
      "Average training loss: 0.026651728976103996\n",
      "Average test loss: 0.0025409003924578427\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02649771098130279\n",
      "Average test loss: 0.0025393420855204262\n",
      "Epoch 210/300\n",
      "Average training loss: 0.026566872507333757\n",
      "Average test loss: 0.002492692409704129\n",
      "Epoch 211/300\n",
      "Average training loss: 0.026524901035759184\n",
      "Average test loss: 0.0024714437957025237\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02654417951901754\n",
      "Average test loss: 0.0024742438660727605\n",
      "Epoch 213/300\n",
      "Average training loss: 0.026489923766917653\n",
      "Average test loss: 0.002593190897670057\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02643344513575236\n",
      "Average test loss: 0.0025277935143353213\n",
      "Epoch 215/300\n",
      "Average training loss: 0.026416653508941332\n",
      "Average test loss: 0.0025626802878040405\n",
      "Epoch 216/300\n",
      "Average training loss: 0.026427703867355982\n",
      "Average test loss: 0.0025434362435092527\n",
      "Epoch 217/300\n",
      "Average training loss: 0.026410673773951\n",
      "Average test loss: 0.002571396222958962\n",
      "Epoch 218/300\n",
      "Average training loss: 0.026414897825982834\n",
      "Average test loss: 0.002599155454378989\n",
      "Epoch 219/300\n",
      "Average training loss: 0.026399240132835178\n",
      "Average test loss: 0.0025252692215144636\n",
      "Epoch 220/300\n",
      "Average training loss: 0.026382473112808332\n",
      "Average test loss: 0.0025249770407875377\n",
      "Epoch 221/300\n",
      "Average training loss: 0.026427426627940603\n",
      "Average test loss: 0.002502328283049994\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02633495453496774\n",
      "Average test loss: 0.0025460628103464843\n",
      "Epoch 223/300\n",
      "Average training loss: 0.026356428045365546\n",
      "Average test loss: 0.0025525052402582435\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02632645851208104\n",
      "Average test loss: 0.0024935035008109277\n",
      "Epoch 225/300\n",
      "Average training loss: 0.026326776805851194\n",
      "Average test loss: 0.0025856954803069434\n",
      "Epoch 226/300\n",
      "Average training loss: 0.026315959167149332\n",
      "Average test loss: 0.0025079925544559956\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02624996396733655\n",
      "Average test loss: 0.002565968354853491\n",
      "Epoch 229/300\n",
      "Average training loss: 0.026292309501104883\n",
      "Average test loss: 0.0025482924288759627\n",
      "Epoch 230/300\n",
      "Average training loss: 0.026214564328392347\n",
      "Average test loss: 0.002559482887801197\n",
      "Epoch 231/300\n",
      "Average training loss: 0.026257186017102665\n",
      "Average test loss: 0.002559530551855763\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02622353401945697\n",
      "Average test loss: 0.002595950941658682\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02624869821800126\n",
      "Average test loss: 0.002568170719366107\n",
      "Epoch 234/300\n",
      "Average training loss: 0.026242108517222935\n",
      "Average test loss: 0.0025104091652772493\n",
      "Epoch 235/300\n",
      "Average training loss: 0.026170536797907616\n",
      "Average test loss: 0.002556596304186516\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02618444097869926\n",
      "Average test loss: 0.002582018516336878\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02619486237068971\n",
      "Average test loss: 0.0025235475169287785\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02611864208843973\n",
      "Average test loss: 0.002532228657975793\n",
      "Epoch 239/300\n",
      "Average training loss: 0.026195298686623573\n",
      "Average test loss: 0.0025379843580433064\n",
      "Epoch 240/300\n",
      "Average training loss: 0.026168092431293595\n",
      "Average test loss: 0.002481232290880548\n",
      "Epoch 241/300\n",
      "Average training loss: 0.026143010725577672\n",
      "Average test loss: 0.0025373804204993776\n",
      "Epoch 242/300\n",
      "Average training loss: 0.026112557394637\n",
      "Average test loss: 0.002522258417473899\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02608801678650909\n",
      "Average test loss: 0.0025488793386353385\n",
      "Epoch 244/300\n",
      "Average training loss: 0.02611781668331888\n",
      "Average test loss: 0.002586067641981774\n",
      "Epoch 245/300\n",
      "Average training loss: 0.026118899087111156\n",
      "Average test loss: 0.002541554694581363\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02606379806333118\n",
      "Average test loss: 0.0025373958078109557\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02604842918117841\n",
      "Average test loss: 0.0025378309100245436\n",
      "Epoch 248/300\n",
      "Average training loss: 0.026097775949372185\n",
      "Average test loss: 0.0025913480390898054\n",
      "Epoch 249/300\n",
      "Average training loss: 0.026123140949341987\n",
      "Average test loss: 0.0025929619504345787\n",
      "Epoch 250/300\n",
      "Average training loss: 0.025994519455565347\n",
      "Average test loss: 0.0025431096548111072\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02599263822701242\n",
      "Average test loss: 0.0026576101788216167\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025938593632645078\n",
      "Average test loss: 0.002623092270973656\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02590710055331389\n",
      "Average test loss: 0.002583788569809662\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025958914634254242\n",
      "Average test loss: 0.0025781148216790623\n",
      "Epoch 258/300\n",
      "Average training loss: 0.025968239264355767\n",
      "Average test loss: 0.0025391233298513623\n",
      "Epoch 259/300\n",
      "Average training loss: 0.02593143038948377\n",
      "Average test loss: 0.002552652640475167\n",
      "Epoch 260/300\n",
      "Average training loss: 0.025920230504539277\n",
      "Average test loss: 0.0026365456063714293\n",
      "Epoch 261/300\n",
      "Average training loss: 0.025925973993208673\n",
      "Average test loss: 0.0025380299958503907\n",
      "Epoch 262/300\n",
      "Average training loss: 0.025834260180592535\n",
      "Average test loss: 0.002707627759211593\n",
      "Epoch 263/300\n",
      "Average training loss: 0.025869004289309185\n",
      "Average test loss: 0.0025134751556648146\n",
      "Epoch 264/300\n",
      "Average training loss: 0.025885017974509133\n",
      "Average test loss: 0.002582854603520698\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025872717214955224\n",
      "Average test loss: 0.0025098876346730525\n",
      "Epoch 266/300\n",
      "Average training loss: 0.025866990940438376\n",
      "Average test loss: 0.002551844952110615\n",
      "Epoch 267/300\n",
      "Average training loss: 0.025817172217700215\n",
      "Average test loss: 0.0025440330234252745\n",
      "Epoch 268/300\n",
      "Average training loss: 0.025857787494858106\n",
      "Average test loss: 0.0025579640103711022\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025887775832580194\n",
      "Average test loss: 0.002575187559032606\n",
      "Epoch 270/300\n",
      "Average training loss: 0.025833900852335823\n",
      "Average test loss: 0.0025484548987199864\n",
      "Epoch 271/300\n",
      "Average training loss: 0.025797489325205484\n",
      "Average test loss: 0.002499581729165382\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0257806114256382\n",
      "Average test loss: 0.0026272265056355134\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02576855864458614\n",
      "Average test loss: 0.0025979351059844096\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02575182985762755\n",
      "Average test loss: 0.0025983403680018254\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02572289155754778\n",
      "Average test loss: 0.002508868450505866\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02579070837630166\n",
      "Average test loss: 0.0026031920346948835\n",
      "Epoch 280/300\n",
      "Average training loss: 0.025793390502532324\n",
      "Average test loss: 0.0025858534733868305\n",
      "Epoch 281/300\n",
      "Average training loss: 0.025689677117599383\n",
      "Average test loss: 0.0025146051021292808\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02571157406601641\n",
      "Average test loss: 0.0025288251373502945\n",
      "Epoch 283/300\n",
      "Average training loss: 0.025678856564892664\n",
      "Average test loss: 0.002576024202629924\n",
      "Epoch 284/300\n",
      "Average training loss: 0.025632205383645162\n",
      "Average test loss: 0.00250896338518295\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02567266591058837\n",
      "Average test loss: 0.002559369211188621\n",
      "Epoch 288/300\n",
      "Average training loss: 0.025609349110060266\n",
      "Average test loss: 0.0025910333153895205\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02570137582719326\n",
      "Average test loss: 0.0026170168603873913\n",
      "Epoch 290/300\n",
      "Average training loss: 0.025676681185762086\n",
      "Average test loss: 0.0026272987256654437\n",
      "Epoch 291/300\n",
      "Average training loss: 0.025636399512489638\n",
      "Average test loss: 0.0025370606107430324\n",
      "Epoch 292/300\n",
      "Average training loss: 0.025662865463230346\n",
      "Average test loss: 0.0025398039201067553\n",
      "Epoch 293/300\n",
      "Average training loss: 0.025569591181145775\n",
      "Average test loss: 0.0025716784675088194\n",
      "Epoch 294/300\n",
      "Average training loss: 0.025608195803231662\n",
      "Average test loss: 0.0026633324035339884\n",
      "Epoch 295/300\n",
      "Average training loss: 0.025618600474463567\n",
      "Average test loss: 0.002536990074026916\n",
      "Epoch 296/300\n",
      "Average training loss: 0.025580544809500377\n",
      "Average test loss: 0.0026070423281441134\n",
      "Epoch 297/300\n",
      "Average training loss: 0.025595646851592595\n",
      "Average test loss: 0.002585410621431139\n",
      "Epoch 298/300\n",
      "Average training loss: 0.025615134461058512\n",
      "Average test loss: 0.002564431775982181\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02554556741317113\n",
      "Average test loss: 0.002583841828525894\n",
      "Epoch 300/300\n",
      "Average training loss: 0.025581653780407377\n",
      "Average test loss: 0.002592839872671498\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7919467369980282\n",
      "Average test loss: 0.003965869169682264\n",
      "Epoch 2/300\n",
      "Average training loss: 0.1908039730919732\n",
      "Average test loss: 0.0033588652418305477\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11251564122570885\n",
      "Average test loss: 0.0030518935113731356\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08187232983112336\n",
      "Average test loss: 0.0029001821482347117\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06616379729244444\n",
      "Average test loss: 0.002726078853218092\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05723178576429685\n",
      "Average test loss: 0.002657908046411143\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05180094178186523\n",
      "Average test loss: 0.0026153492571579086\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04811592564649052\n",
      "Average test loss: 0.00246028374777072\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04161335509684351\n",
      "Average test loss: 0.002212882412183616\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04020279290941026\n",
      "Average test loss: 0.0021866610593472917\n",
      "Epoch 13/300\n",
      "Average training loss: 0.039058748255173366\n",
      "Average test loss: 0.002103817873944839\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03801756364603837\n",
      "Average test loss: 0.0020932707714123857\n",
      "Epoch 15/300\n",
      "Average training loss: 0.037053690903716616\n",
      "Average test loss: 0.0020229011109719674\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03627066028118134\n",
      "Average test loss: 0.0020061457715928556\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03550119759142399\n",
      "Average test loss: 0.0019494514166274004\n",
      "Epoch 18/300\n",
      "Average training loss: 0.034827126542727155\n",
      "Average test loss: 0.002005259111523628\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03417233109474182\n",
      "Average test loss: 0.0019285203048752413\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03357949235041936\n",
      "Average test loss: 0.0018841485230045186\n",
      "Epoch 21/300\n",
      "Average training loss: 0.033015850469470025\n",
      "Average test loss: 0.001968201939637462\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03251154220104217\n",
      "Average test loss: 0.0018537227128528886\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03200686555107435\n",
      "Average test loss: 0.0018625634053411582\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03152473240759637\n",
      "Average test loss: 0.0018444552494006024\n",
      "Epoch 25/300\n",
      "Average training loss: 0.031096628410948648\n",
      "Average test loss: 0.001795892795547843\n",
      "Epoch 26/300\n",
      "Average training loss: 0.030612134062581592\n",
      "Average test loss: 0.0017964338070402543\n",
      "Epoch 27/300\n",
      "Average training loss: 0.030282250362965796\n",
      "Average test loss: 0.0017826566314324737\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02989788982603285\n",
      "Average test loss: 0.0017797399260517624\n",
      "Epoch 29/300\n",
      "Average training loss: 0.029564982602993648\n",
      "Average test loss: 0.001762297346153193\n",
      "Epoch 30/300\n",
      "Average training loss: 0.029206180196669367\n",
      "Average test loss: 0.0017491522216134601\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02898696347243256\n",
      "Average test loss: 0.0017390607576817274\n",
      "Epoch 32/300\n",
      "Average training loss: 0.028661189675331114\n",
      "Average test loss: 0.0017274093510996965\n",
      "Epoch 33/300\n",
      "Average training loss: 0.028425509633289443\n",
      "Average test loss: 0.0017240607964081897\n",
      "Epoch 34/300\n",
      "Average training loss: 0.028239337427748572\n",
      "Average test loss: 0.0017306838530219263\n",
      "Epoch 35/300\n",
      "Average training loss: 0.027633370220661164\n",
      "Average test loss: 0.0017102444331265159\n",
      "Epoch 38/300\n",
      "Average training loss: 0.027432059852613343\n",
      "Average test loss: 0.0017081310647643275\n",
      "Epoch 39/300\n",
      "Average training loss: 0.027246757975882954\n",
      "Average test loss: 0.0017015315319101015\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02711002864440282\n",
      "Average test loss: 0.0017230800814512703\n",
      "Epoch 41/300\n",
      "Average training loss: 0.026974655618270237\n",
      "Average test loss: 0.0017019392182636593\n",
      "Epoch 42/300\n",
      "Average training loss: 0.02683313753373093\n",
      "Average test loss: 0.0017015900802281167\n",
      "Epoch 43/300\n",
      "Average training loss: 0.026736712916029824\n",
      "Average test loss: 0.0016960701663047075\n",
      "Epoch 44/300\n",
      "Average training loss: 0.026565431813398998\n",
      "Average test loss: 0.0016853610247166621\n",
      "Epoch 45/300\n",
      "Average training loss: 0.026465378476513757\n",
      "Average test loss: 0.0016771834354019828\n",
      "Epoch 46/300\n",
      "Average training loss: 0.026302590310573578\n",
      "Average test loss: 0.001686281443056133\n",
      "Epoch 47/300\n",
      "Average training loss: 0.026146359084380996\n",
      "Average test loss: 0.0016817713499897057\n",
      "Epoch 48/300\n",
      "Average training loss: 0.026079875585105685\n",
      "Average test loss: 0.0016986419783594708\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02596415881605612\n",
      "Average test loss: 0.0017659555862968167\n",
      "Epoch 50/300\n",
      "Average training loss: 0.025832402552167573\n",
      "Average test loss: 0.0017127286673833926\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025722043019202022\n",
      "Average test loss: 0.0016785846221157247\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0257161841077937\n",
      "Average test loss: 0.0017019594471073814\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0255480068475008\n",
      "Average test loss: 0.0017005596889389887\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02542888855934143\n",
      "Average test loss: 0.0016828968624273935\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025327544185850356\n",
      "Average test loss: 0.0016745206804739104\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02524774840970834\n",
      "Average test loss: 0.001693835390628212\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025151808924145168\n",
      "Average test loss: 0.0016949042874491876\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0250685257497761\n",
      "Average test loss: 0.0016991664985608723\n",
      "Epoch 59/300\n",
      "Average training loss: 0.024956456152101358\n",
      "Average test loss: 0.0016931490807069673\n",
      "Epoch 60/300\n",
      "Average training loss: 0.024859274385703935\n",
      "Average test loss: 0.0016799102500081062\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02472984261645211\n",
      "Average test loss: 0.0016833504372172885\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02466038459704982\n",
      "Average test loss: 0.0016880742910628517\n",
      "Epoch 63/300\n",
      "Average training loss: 0.024413803623782263\n",
      "Average test loss: 0.0017307505212310287\n",
      "Epoch 66/300\n",
      "Average training loss: 0.024385686664117705\n",
      "Average test loss: 0.0016911752887277139\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02424830096297794\n",
      "Average test loss: 0.001687168136239052\n",
      "Epoch 68/300\n",
      "Average training loss: 0.024143715194529958\n",
      "Average test loss: 0.0017068247797174586\n",
      "Epoch 69/300\n",
      "Average training loss: 0.023808090153667662\n",
      "Average test loss: 0.0017087547400345404\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02378612803419431\n",
      "Average test loss: 0.0017139756883391075\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02365966841081778\n",
      "Average test loss: 0.0017021655643151867\n",
      "Epoch 75/300\n",
      "Average training loss: 0.023590922469894093\n",
      "Average test loss: 0.001739785178254048\n",
      "Epoch 76/300\n",
      "Average training loss: 0.023561071788271267\n",
      "Average test loss: 0.0017184619242325426\n",
      "Epoch 77/300\n",
      "Average training loss: 0.023451028832130964\n",
      "Average test loss: 0.0017381679331883789\n",
      "Epoch 78/300\n",
      "Average training loss: 0.023352568474080827\n",
      "Average test loss: 0.0017123092981055378\n",
      "Epoch 79/300\n",
      "Average training loss: 0.023372717570927407\n",
      "Average test loss: 0.001732162211400767\n",
      "Epoch 80/300\n",
      "Average training loss: 0.023263302856021458\n",
      "Average test loss: 0.0017135281603162487\n",
      "Epoch 81/300\n",
      "Average training loss: 0.023201869312259885\n",
      "Average test loss: 0.0017035922492957777\n",
      "Epoch 82/300\n",
      "Average training loss: 0.023123900633719233\n",
      "Average test loss: 0.0017131403336922328\n",
      "Epoch 83/300\n",
      "Average training loss: 0.02305808710720804\n",
      "Average test loss: 0.0017204624812843072\n",
      "Epoch 84/300\n",
      "Average training loss: 0.0229653750045432\n",
      "Average test loss: 0.001718967008507914\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02293170779446761\n",
      "Average test loss: 0.001740542507626944\n",
      "Epoch 86/300\n",
      "Average training loss: 0.022880920032660165\n",
      "Average test loss: 0.0017601722923831808\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02278709268487162\n",
      "Average test loss: 0.001744232012735059\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02274904337277015\n",
      "Average test loss: 0.0017539522968646553\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02270993848476145\n",
      "Average test loss: 0.0017475730418745014\n",
      "Epoch 90/300\n",
      "Average training loss: 0.022633544693390528\n",
      "Average test loss: 0.0017356320472641123\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02253779934346676\n",
      "Average test loss: 0.001742067971577247\n",
      "Epoch 94/300\n",
      "Average training loss: 0.022437368233170774\n",
      "Average test loss: 0.0017946027572163278\n",
      "Epoch 95/300\n",
      "Average training loss: 0.022346207555797366\n",
      "Average test loss: 0.0017473636832502153\n",
      "Epoch 96/300\n",
      "Average training loss: 0.022396599252190855\n",
      "Average test loss: 0.0017685917754554086\n",
      "Epoch 97/300\n",
      "Average training loss: 0.022274725009997685\n",
      "Average test loss: 0.0017487070699118905\n",
      "Epoch 98/300\n",
      "Average training loss: 0.022207843053672047\n",
      "Average test loss: 0.0017364571864406269\n",
      "Epoch 99/300\n",
      "Average training loss: 0.022200863354735905\n",
      "Average test loss: 0.0017396477022104794\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02215569116340743\n",
      "Average test loss: 0.0017367552878956\n",
      "Epoch 101/300\n",
      "Average training loss: 0.022096008246143658\n",
      "Average test loss: 0.0017284140427493387\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02206654098381599\n",
      "Average test loss: 0.0017570960136751334\n",
      "Epoch 103/300\n",
      "Average training loss: 0.02204215724931823\n",
      "Average test loss: 0.0017582138555848764\n",
      "Epoch 104/300\n",
      "Average training loss: 0.021983887889318997\n",
      "Average test loss: 0.0017827332001179458\n",
      "Epoch 105/300\n",
      "Average training loss: 0.021944113802578713\n",
      "Average test loss: 0.0017864447627216578\n",
      "Epoch 106/300\n",
      "Average training loss: 0.021900785661405988\n",
      "Average test loss: 0.0017915884328799116\n",
      "Epoch 107/300\n",
      "Average training loss: 0.021842578979002106\n",
      "Average test loss: 0.001759905186895695\n",
      "Epoch 108/300\n",
      "Average training loss: 0.021809846749736204\n",
      "Average test loss: 0.0017876683937178717\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02179450298845768\n",
      "Average test loss: 0.0018108184290015034\n",
      "Epoch 110/300\n",
      "Average training loss: 0.021826479836470552\n",
      "Average test loss: 0.0018216048256597586\n",
      "Epoch 111/300\n",
      "Average training loss: 0.021666739366120763\n",
      "Average test loss: 0.0017706178730974595\n",
      "Epoch 112/300\n",
      "Average training loss: 0.021635260367559063\n",
      "Average test loss: 0.0018330310704186559\n",
      "Epoch 113/300\n",
      "Average training loss: 0.021631200088395013\n",
      "Average test loss: 0.0018035382961647378\n",
      "Epoch 114/300\n",
      "Average training loss: 0.021604775726795197\n",
      "Average test loss: 0.0017565118489373062\n",
      "Epoch 115/300\n",
      "Average training loss: 0.02161335298915704\n",
      "Average test loss: 0.0018216636849360333\n",
      "Epoch 116/300\n",
      "Average training loss: 0.021561427391237682\n",
      "Average test loss: 0.0017852447676575847\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02135639389190409\n",
      "Average test loss: 0.0018157795841495196\n",
      "Epoch 121/300\n",
      "Average training loss: 0.021355288091633055\n",
      "Average test loss: 0.0017645888031563824\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02138219665322039\n",
      "Average test loss: 0.0018243291255914502\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02133606440987852\n",
      "Average test loss: 0.0017849335817413197\n",
      "Epoch 124/300\n",
      "Average training loss: 0.021294313255283567\n",
      "Average test loss: 0.0018011202104389668\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02124199279728863\n",
      "Average test loss: 0.0018279030355107452\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02126408827635977\n",
      "Average test loss: 0.0017796516867561473\n",
      "Epoch 127/300\n",
      "Average training loss: 0.021194250836968423\n",
      "Average test loss: 0.0018268783240475588\n",
      "Epoch 128/300\n",
      "Average training loss: 0.021183061876230768\n",
      "Average test loss: 0.0017854640767392186\n",
      "Epoch 129/300\n",
      "Average training loss: 0.021199568420648573\n",
      "Average test loss: 0.0017910191011097697\n",
      "Epoch 130/300\n",
      "Average training loss: 0.021092948688401118\n",
      "Average test loss: 0.001803542445310288\n",
      "Epoch 131/300\n",
      "Average training loss: 0.021067579264442127\n",
      "Average test loss: 0.0018024809594369598\n",
      "Epoch 132/300\n",
      "Average training loss: 0.021088163662287922\n",
      "Average test loss: 0.001843814310307304\n",
      "Epoch 133/300\n",
      "Average training loss: 0.021048713284234206\n",
      "Average test loss: 0.0017906221076846123\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02100212147914701\n",
      "Average test loss: 0.001836536242833568\n",
      "Epoch 135/300\n",
      "Average training loss: 0.021016365562876064\n",
      "Average test loss: 0.0018378906260348028\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02094683785405424\n",
      "Average test loss: 0.0017931205747235154\n",
      "Epoch 137/300\n",
      "Average training loss: 0.020987297263410358\n",
      "Average test loss: 0.001807963981396622\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020929715832074484\n",
      "Average test loss: 0.0018665155701132283\n",
      "Epoch 139/300\n",
      "Average training loss: 0.020900712539752325\n",
      "Average test loss: 0.0018214185676640935\n",
      "Epoch 140/300\n",
      "Average training loss: 0.020913965980211894\n",
      "Average test loss: 0.0018008655560099417\n",
      "Epoch 141/300\n",
      "Average training loss: 0.020870215447412598\n",
      "Average test loss: 0.0018039659100274245\n",
      "Epoch 142/300\n",
      "Average training loss: 0.020844667903251117\n",
      "Average test loss: 0.001799650171564685\n",
      "Epoch 143/300\n",
      "Average training loss: 0.020784025879369842\n",
      "Average test loss: 0.0018016145593590207\n",
      "Epoch 146/300\n",
      "Average training loss: 0.020767587517698607\n",
      "Average test loss: 0.0018002388633580671\n",
      "Epoch 147/300\n",
      "Average training loss: 0.020714504679044086\n",
      "Average test loss: 0.0017857416766799159\n",
      "Epoch 148/300\n",
      "Average training loss: 0.020663525751895374\n",
      "Average test loss: 0.0018258350911653704\n",
      "Epoch 149/300\n",
      "Average training loss: 0.02069464318619834\n",
      "Average test loss: 0.0018826388049249848\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020654393465982544\n",
      "Average test loss: 0.001855327807056407\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02061196780204773\n",
      "Average test loss: 0.001824427139531407\n",
      "Epoch 154/300\n",
      "Average training loss: 0.020583395721183884\n",
      "Average test loss: 0.001803191017980377\n",
      "Epoch 155/300\n",
      "Average training loss: 0.020562099370691513\n",
      "Average test loss: 0.0018446215557762317\n",
      "Epoch 156/300\n",
      "Average training loss: 0.020542299015654457\n",
      "Average test loss: 0.0018342630089157156\n",
      "Epoch 157/300\n",
      "Average training loss: 0.020533448033862645\n",
      "Average test loss: 0.0018992343260389236\n",
      "Epoch 158/300\n",
      "Average training loss: 0.020505869226323234\n",
      "Average test loss: 0.0018305575371616416\n",
      "Epoch 161/300\n",
      "Average training loss: 0.020402632213301127\n",
      "Average test loss: 0.0018541756007406447\n",
      "Epoch 162/300\n",
      "Average training loss: 0.02045780744486385\n",
      "Average test loss: 0.0018135271147928305\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0204037690129545\n",
      "Average test loss: 0.0018538772378944687\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02039760736293263\n",
      "Average test loss: 0.0018280852648119132\n",
      "Epoch 165/300\n",
      "Average training loss: 0.020373575664228864\n",
      "Average test loss: 0.0018206176689515511\n",
      "Epoch 166/300\n",
      "Average training loss: 0.020387120271722476\n",
      "Average test loss: 0.0018735734054611789\n",
      "Epoch 167/300\n",
      "Average training loss: 0.020346449424823124\n",
      "Average test loss: 0.0018705680432418983\n",
      "Epoch 168/300\n",
      "Average training loss: 0.020276292372081013\n",
      "Average test loss: 0.0018391319962425365\n",
      "Epoch 169/300\n",
      "Average training loss: 0.020357245307829643\n",
      "Average test loss: 0.0018605847758137518\n",
      "Epoch 170/300\n",
      "Average training loss: 0.020285348431931603\n",
      "Average test loss: 0.0018921329835429788\n",
      "Epoch 171/300\n",
      "Average training loss: 0.020290846750968032\n",
      "Average test loss: 0.0019638023597912655\n",
      "Epoch 172/300\n",
      "Average training loss: 0.020305645886394712\n",
      "Average test loss: 0.0018585320221674111\n",
      "Epoch 173/300\n",
      "Average training loss: 0.020282119347817366\n",
      "Average test loss: 0.0018417993864665429\n",
      "Epoch 174/300\n",
      "Average training loss: 0.020265603726108868\n",
      "Average test loss: 0.0018658380718487833\n",
      "Epoch 175/300\n",
      "Average training loss: 0.020247826614313656\n",
      "Average test loss: 0.0018112688190821144\n",
      "Epoch 176/300\n",
      "Average training loss: 0.020168721851375367\n",
      "Average test loss: 0.0018489013887527917\n",
      "Epoch 177/300\n",
      "Average training loss: 0.020203367564413283\n",
      "Average test loss: 0.0018241269441528452\n",
      "Epoch 178/300\n",
      "Average training loss: 0.020210390378203658\n",
      "Average test loss: 0.001887971742492583\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02012925544877847\n",
      "Average test loss: 0.0018248945837840438\n",
      "Epoch 180/300\n",
      "Average training loss: 0.020110638481047417\n",
      "Average test loss: 0.001861082451004121\n",
      "Epoch 181/300\n",
      "Average training loss: 0.020209778108530575\n",
      "Average test loss: 0.0019249484539031983\n",
      "Epoch 182/300\n",
      "Average training loss: 0.020154112845659256\n",
      "Average test loss: 0.0018468878358188603\n",
      "Epoch 183/300\n",
      "Average training loss: 0.020103985115885733\n",
      "Average test loss: 0.001942484456528392\n",
      "Epoch 184/300\n",
      "Average training loss: 0.020119800008005565\n",
      "Average test loss: 0.0018439595510976181\n",
      "Epoch 185/300\n",
      "Average training loss: 0.020064275542895\n",
      "Average test loss: 0.0018569004301809602\n",
      "Epoch 186/300\n",
      "Average training loss: 0.020065100093682606\n",
      "Average test loss: 0.0018654210834453504\n",
      "Epoch 187/300\n",
      "Average training loss: 0.020094229688247044\n",
      "Average test loss: 0.001878710742832886\n",
      "Epoch 188/300\n",
      "Average training loss: 0.020044912308454513\n",
      "Average test loss: 0.0018488805656217866\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02001157942083147\n",
      "Average test loss: 0.0018705898514017462\n",
      "Epoch 192/300\n",
      "Average training loss: 0.019951544551385772\n",
      "Average test loss: 0.001888087984898852\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01996347869767083\n",
      "Average test loss: 0.0019171564454833667\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019980381248725785\n",
      "Average test loss: 0.0018823099961090418\n",
      "Epoch 197/300\n",
      "Average training loss: 0.019963361606001855\n",
      "Average test loss: 0.0018888154327869416\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01985849618083901\n",
      "Average test loss: 0.0018443939002851645\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01989040376411544\n",
      "Average test loss: 0.0018578636778725519\n",
      "Epoch 203/300\n",
      "Average training loss: 0.019873164898819393\n",
      "Average test loss: 0.0018744423534307216\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01987578425804774\n",
      "Average test loss: 0.0018678110982808802\n",
      "Epoch 205/300\n",
      "Average training loss: 0.019852077759802342\n",
      "Average test loss: 0.0018514958354127076\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01982403922577699\n",
      "Average test loss: 0.0018525199107825756\n",
      "Epoch 207/300\n",
      "Average training loss: 0.01986616570336951\n",
      "Average test loss: 0.0018665816387575533\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01981152458737294\n",
      "Average test loss: 0.001935288158348865\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0198064519746436\n",
      "Average test loss: 0.001888930841245585\n",
      "Epoch 210/300\n",
      "Average training loss: 0.019810001035531363\n",
      "Average test loss: 0.0018574697508787115\n",
      "Epoch 211/300\n",
      "Average training loss: 0.019763272465931046\n",
      "Average test loss: 0.0019055238287481997\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01974869430727429\n",
      "Average test loss: 0.0018566577365208004\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01973670654495557\n",
      "Average test loss: 0.0018606971303621927\n",
      "Epoch 214/300\n",
      "Average training loss: 0.019797965962025856\n",
      "Average test loss: 0.0018578026031868325\n",
      "Epoch 215/300\n",
      "Average training loss: 0.019720380210214192\n",
      "Average test loss: 0.0018816853341543011\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01967284132871363\n",
      "Average test loss: 0.0018764264641536607\n",
      "Epoch 217/300\n",
      "Average training loss: 0.019737165316939354\n",
      "Average test loss: 0.0018870775713067916\n",
      "Epoch 218/300\n",
      "Average training loss: 0.019736051773859394\n",
      "Average test loss: 0.0018842279367769757\n",
      "Epoch 219/300\n",
      "Average training loss: 0.019694015459881888\n",
      "Average test loss: 0.001909154201650785\n",
      "Epoch 220/300\n",
      "Average training loss: 0.01967105030020078\n",
      "Average test loss: 0.001865730768069625\n",
      "Epoch 221/300\n",
      "Average training loss: 0.019707952209644847\n",
      "Average test loss: 0.0018792036996326514\n",
      "Epoch 222/300\n",
      "Average training loss: 0.019668114682038624\n",
      "Average test loss: 0.001824663414309422\n",
      "Epoch 223/300\n",
      "Average training loss: 0.019648954255713356\n",
      "Average test loss: 0.0019283985189265675\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019629787569244703\n",
      "Average test loss: 0.0019024696815758944\n",
      "Epoch 227/300\n",
      "Average training loss: 0.01961137232846684\n",
      "Average test loss: 0.0018898647392375603\n",
      "Epoch 228/300\n",
      "Average training loss: 0.01957812525994248\n",
      "Average test loss: 0.0019106201846152543\n",
      "Epoch 229/300\n",
      "Average training loss: 0.019602853866087067\n",
      "Average test loss: 0.0018825535177149706\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01958093660076459\n",
      "Average test loss: 0.0018802846235533555\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01958431107468075\n",
      "Average test loss: 0.001819912815052602\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01958276486893495\n",
      "Average test loss: 0.00189758492136995\n",
      "Epoch 233/300\n",
      "Average training loss: 0.019597352835039297\n",
      "Average test loss: 0.0019160054049765071\n",
      "Epoch 234/300\n",
      "Average training loss: 0.019553335131870377\n",
      "Average test loss: 0.0018836832760522762\n",
      "Epoch 235/300\n",
      "Average training loss: 0.019520711618993016\n",
      "Average test loss: 0.0018696614613549576\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019576198005841837\n",
      "Average test loss: 0.0019056132222629255\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019515848444567788\n",
      "Average test loss: 0.0018642464228388336\n",
      "Epoch 238/300\n",
      "Average training loss: 0.019498867988586424\n",
      "Average test loss: 0.0018456257246434688\n",
      "Epoch 239/300\n",
      "Average training loss: 0.019506916306085056\n",
      "Average test loss: 0.001838933503255248\n",
      "Epoch 240/300\n",
      "Average training loss: 0.019498369024859536\n",
      "Average test loss: 0.0019289460400533345\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01947938307126363\n",
      "Average test loss: 0.0018836984385011924\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019475963646339047\n",
      "Average test loss: 0.0019350050542917517\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01948542564941777\n",
      "Average test loss: 0.0019070958791093694\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019479897915489142\n",
      "Average test loss: 0.001850822492916551\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01944977094978094\n",
      "Average test loss: 0.0019037949024803109\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01947265830470456\n",
      "Average test loss: 0.0018611748615900675\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0194504428025749\n",
      "Average test loss: 0.0019021993857911892\n",
      "Epoch 248/300\n",
      "Average training loss: 0.019443898523847262\n",
      "Average test loss: 0.0018842381816357374\n",
      "Epoch 249/300\n",
      "Average training loss: 0.019404208114577666\n",
      "Average test loss: 0.001918682433457838\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01941825594504674\n",
      "Average test loss: 0.0018823859516220788\n",
      "Epoch 253/300\n",
      "Average training loss: 0.019418496269318793\n",
      "Average test loss: 0.0018439212029592858\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019406795965300667\n",
      "Average test loss: 0.0019059143393403955\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019395022994942137\n",
      "Average test loss: 0.0018977319157371917\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019388418870667615\n",
      "Average test loss: 0.0019185307489501106\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019365503624081612\n",
      "Average test loss: 0.001906453822222021\n",
      "Epoch 258/300\n",
      "Average training loss: 0.019326782120598688\n",
      "Average test loss: 0.0018829247088481983\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019348173096776007\n",
      "Average test loss: 0.0019142911959853437\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01936676904724704\n",
      "Average test loss: 0.0018725313184161981\n",
      "Epoch 261/300\n",
      "Average training loss: 0.01935831699271997\n",
      "Average test loss: 0.0019477053980032602\n",
      "Epoch 262/300\n",
      "Average training loss: 0.019373694338732296\n",
      "Average test loss: 0.001933342023545669\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01929578707781103\n",
      "Average test loss: 0.0018868572615707913\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01930212592913045\n",
      "Average test loss: 0.001899539167061448\n",
      "Epoch 265/300\n",
      "Average training loss: 0.019310435879561636\n",
      "Average test loss: 0.0019112504123606616\n",
      "Epoch 266/300\n",
      "Average training loss: 0.019296278863317435\n",
      "Average test loss: 0.0019115141911639107\n",
      "Epoch 267/300\n",
      "Average training loss: 0.019293193347752095\n",
      "Average test loss: 0.001885340567264292\n",
      "Epoch 268/300\n",
      "Average training loss: 0.019275178820722633\n",
      "Average test loss: 0.0019466007065234913\n",
      "Epoch 269/300\n",
      "Average training loss: 0.019320101492934758\n",
      "Average test loss: 0.001953490766800112\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01923376684718662\n",
      "Average test loss: 0.001886554857198563\n",
      "Epoch 271/300\n",
      "Average training loss: 0.019280344965557257\n",
      "Average test loss: 0.0019271796554740932\n",
      "Epoch 272/300\n",
      "Average training loss: 0.019246293358504772\n",
      "Average test loss: 0.0018680414558491774\n",
      "Epoch 273/300\n",
      "Average training loss: 0.019259023221002685\n",
      "Average test loss: 0.001926727344799373\n",
      "Epoch 274/300\n",
      "Average training loss: 0.019253858702050316\n",
      "Average test loss: 0.0018760160916588373\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01922931089500586\n",
      "Average test loss: 0.0019662361391302612\n",
      "Epoch 276/300\n",
      "Average training loss: 0.019198343303468493\n",
      "Average test loss: 0.001892174530049993\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01920955305132601\n",
      "Average test loss: 0.001932009984842605\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019214643485016294\n",
      "Average test loss: 0.0018704238944790429\n",
      "Epoch 281/300\n",
      "Average training loss: 0.019185049427880182\n",
      "Average test loss: 0.001911967577205764\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019191156132353678\n",
      "Average test loss: 0.0019366987904326782\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01926160991523001\n",
      "Average test loss: 0.0018928040716176232\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019155860064758196\n",
      "Average test loss: 0.0019556360342022445\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019151065070596007\n",
      "Average test loss: 0.0019321340525315868\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01914901897807916\n",
      "Average test loss: 0.0018931628755397268\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019155963470538457\n",
      "Average test loss: 0.001849987258306808\n",
      "Epoch 288/300\n",
      "Average training loss: 0.019149883053368993\n",
      "Average test loss: 0.0018601373951468202\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019117968059248393\n",
      "Average test loss: 0.001922414412515031\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01912685085667504\n",
      "Average test loss: 0.001872746134383811\n",
      "Epoch 291/300\n",
      "Average training loss: 0.019134243971771665\n",
      "Average test loss: 0.0019169261237192486\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01916318913963106\n",
      "Average test loss: 0.0019815046560640137\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01912034179104699\n",
      "Average test loss: 0.0018919550424648657\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01912745909889539\n",
      "Average test loss: 0.0019041238836944104\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01908462154534128\n",
      "Average test loss: 0.0019237961672867337\n",
      "Epoch 296/300\n",
      "Average training loss: 0.019080914997392232\n",
      "Average test loss: 0.0019051979645672771\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019095827826195292\n",
      "Average test loss: 0.0019353138126639857\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019156598346100912\n",
      "Average test loss: 0.001894780734140012\n",
      "Epoch 299/300\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive-.025/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.19\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.84\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.90\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.11\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.20\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.25\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.30\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.008091664208306\n",
      "Average test loss: 0.005673743443770541\n",
      "Epoch 2/300\n",
      "Average training loss: 4.1788097983466255\n",
      "Average test loss: 0.006092961946295368\n",
      "Epoch 3/300\n",
      "Average training loss: 2.8072992691463896\n",
      "Average test loss: 0.004757454319960541\n",
      "Epoch 4/300\n",
      "Average training loss: 2.161188030666775\n",
      "Average test loss: 0.005863694671541453\n",
      "Epoch 5/300\n",
      "Average training loss: 1.6113577755822075\n",
      "Average test loss: 0.004805401674989197\n",
      "Epoch 6/300\n",
      "Average training loss: 1.3631683524449667\n",
      "Average test loss: 0.00447377472743392\n",
      "Epoch 7/300\n",
      "Average training loss: 1.1211808081732857\n",
      "Average test loss: 0.004756431530747149\n",
      "Epoch 8/300\n",
      "Average training loss: 0.9711875529819065\n",
      "Average test loss: 0.008677975511178375\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8281481518745423\n",
      "Average test loss: 0.004440633551941978\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6672527745564779\n",
      "Average test loss: 0.00454171282508307\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5586261930200789\n",
      "Average test loss: 0.004383544448556172\n",
      "Epoch 12/300\n",
      "Average training loss: 0.47488763944307966\n",
      "Average test loss: 0.0042731136851426625\n",
      "Epoch 13/300\n",
      "Average training loss: 0.40427750799391005\n",
      "Average test loss: 0.005335133796764745\n",
      "Epoch 14/300\n",
      "Average training loss: 0.26788154567612543\n",
      "Average test loss: 0.004211136631253692\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2392412236266666\n",
      "Average test loss: 0.004210200288229518\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1988912419743008\n",
      "Average test loss: 0.004177357661227385\n",
      "Epoch 20/300\n",
      "Average training loss: 0.18397525766160752\n",
      "Average test loss: 0.004201863745848338\n",
      "Epoch 21/300\n",
      "Average training loss: 0.17214626235432096\n",
      "Average test loss: 0.004154964170936081\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1621581020090315\n",
      "Average test loss: 0.004139239921338029\n",
      "Epoch 23/300\n",
      "Average training loss: 0.15375082819991642\n",
      "Average test loss: 0.004175970178304447\n",
      "Epoch 24/300\n",
      "Average training loss: 0.14779398894309997\n",
      "Average test loss: 0.0041292685369650525\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1432039345767763\n",
      "Average test loss: 0.004115925562464528\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13964032876491547\n",
      "Average test loss: 0.004115178347668714\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1367528588242001\n",
      "Average test loss: 0.004107255698078209\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13431879864798651\n",
      "Average test loss: 0.004104726463970211\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1322360532482465\n",
      "Average test loss: 0.004098343190633589\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13054529531134498\n",
      "Average test loss: 0.004075146300511228\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12912117556730907\n",
      "Average test loss: 0.004072326072595185\n",
      "Epoch 32/300\n",
      "Average training loss: 0.12708201720317205\n",
      "Average test loss: 0.0040508264154195785\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12624070654975045\n",
      "Average test loss: 0.004077931051866876\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12547712469763225\n",
      "Average test loss: 0.004025501530617475\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12490315486325158\n",
      "Average test loss: 0.00403639577039414\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12433974437581169\n",
      "Average test loss: 0.004024808450498515\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12386332389381197\n",
      "Average test loss: 0.004017850276496675\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12348151786459817\n",
      "Average test loss: 0.00401635576515562\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12304434036546283\n",
      "Average test loss: 0.004026663458802634\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12267890467908647\n",
      "Average test loss: 0.004025158611229724\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12242360725005468\n",
      "Average test loss: 0.004002656464361482\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12189463012748294\n",
      "Average test loss: 0.004008054005602996\n",
      "Epoch 44/300\n",
      "Average training loss: 0.121667902900113\n",
      "Average test loss: 0.004152945501936806\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12136055721839269\n",
      "Average test loss: 0.004033070634222693\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12107022909985649\n",
      "Average test loss: 0.004001407598248787\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1205413202908304\n",
      "Average test loss: 0.003997604802664784\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1202317608859804\n",
      "Average test loss: 0.004168654145465957\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1200870206091139\n",
      "Average test loss: 0.003991327967908648\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11973006025950114\n",
      "Average test loss: 0.003989299778723055\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11954054292705324\n",
      "Average test loss: 0.0040004220876014894\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11923980584409502\n",
      "Average test loss: 0.004029810614469978\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11899298608965343\n",
      "Average test loss: 0.003967457696795464\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11894484726587931\n",
      "Average test loss: 0.003980214058938954\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1185720354616642\n",
      "Average test loss: 0.004034859234674109\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11834231667386161\n",
      "Average test loss: 0.003965238063699669\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11827256550391516\n",
      "Average test loss: 0.003989979072163502\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11796400720543332\n",
      "Average test loss: 0.00397482834984031\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11775399945179621\n",
      "Average test loss: 0.0039721516902661985\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11748404457834032\n",
      "Average training loss: 0.11713041497601404\n",
      "Average test loss: 0.003977823670332631\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11690981080796983\n",
      "Average test loss: 0.003974103091698554\n",
      "Epoch 65/300\n",
      "Average training loss: 0.1167979462146759\n",
      "Average test loss: 0.004015552589669824\n",
      "Epoch 66/300\n",
      "Average training loss: 0.11663030985328886\n",
      "Average test loss: 0.003974672610147132\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11642027652263641\n",
      "Average test loss: 0.003972713128560119\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11629293376869626\n",
      "Average test loss: 0.003987065927435954\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11599186871449152\n",
      "Average test loss: 0.003976484573135773\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11582657157712513\n",
      "Average test loss: 0.004003495062390963\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11568797091643016\n",
      "Average test loss: 0.0039790018887983425\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11545884614520603\n",
      "Average test loss: 0.003975485783484247\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11529158937931061\n",
      "Average test loss: 0.004011192061627904\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11502301675081253\n",
      "Average test loss: 0.003990783699270752\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11472893020179536\n",
      "Average test loss: 0.004001488386756844\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11450517593489754\n",
      "Average test loss: 0.00400584579548902\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11442683848407534\n",
      "Average test loss: 0.003977321196554436\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11407799104849498\n",
      "Average test loss: 0.003984642760207256\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11399550712108612\n",
      "Average test loss: 0.003981249148233069\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11349292511410183\n",
      "Average test loss: 0.004021166250937515\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11345028128226597\n",
      "Average test loss: 0.004005869078346424\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1131189814541075\n",
      "Average test loss: 0.004036082715416948\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11287831201818255\n",
      "Average test loss: 0.004102521892223094\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11274663317203522\n",
      "Average test loss: 0.004017598636448383\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11260896261533102\n",
      "Average test loss: 0.004002145262849\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11232000775469674\n",
      "Average test loss: 0.004094765581604507\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11211181001530754\n",
      "Average test loss: 0.003996591889609893\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11192748530705769\n",
      "Average test loss: 0.004033664759248495\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11164646610286501\n",
      "Average test loss: 0.004047426209681564\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1115774535006947\n",
      "Average test loss: 0.004018590862966246\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11118688380055958\n",
      "Average test loss: 0.004034835924083988\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11111930712726381\n",
      "Average test loss: 0.00400665064735545\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11082811407910453\n",
      "Average test loss: 0.0040949412398040294\n",
      "Epoch 96/300\n",
      "Average training loss: 0.11066825091176563\n",
      "Average test loss: 0.004046827561325497\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10994647553894255\n",
      "Average test loss: 0.004154042674849431\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10979379649957022\n",
      "Average test loss: 0.004028356080046958\n",
      "Epoch 101/300\n",
      "Average training loss: 0.10953643833928638\n",
      "Average test loss: 0.004029478708489073\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10940845256381565\n",
      "Average test loss: 0.0040718173577139775\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10903391175137626\n",
      "Average test loss: 0.004096787581013309\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10896623355812497\n",
      "Average test loss: 0.0041812337363759675\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10882715995444192\n",
      "Average test loss: 0.0040551904626190665\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10876649357212914\n",
      "Average test loss: 0.00416125389147136\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10842972053421868\n",
      "Average test loss: 0.0041288217393060525\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10826203100548851\n",
      "Average test loss: 0.004073219479372104\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10804464985264672\n",
      "Average test loss: 0.004099902926633756\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10771109512779448\n",
      "Average test loss: 0.004096322420777546\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10774160765939289\n",
      "Average test loss: 0.00413356904230184\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1074285123149554\n",
      "Average test loss: 0.004099066518661048\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10706494520107905\n",
      "Average test loss: 0.004164345546729035\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10697189025746451\n",
      "Average test loss: 0.004241618395265606\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10704473810063468\n",
      "Average test loss: 0.004138508972194459\n",
      "Epoch 116/300\n",
      "Average training loss: 0.1065425034960111\n",
      "Average test loss: 0.004093790271009008\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10641305797629887\n",
      "Average test loss: 0.0041847648283259735\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10634747794601652\n",
      "Average test loss: 0.004103461345864667\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10617090202702416\n",
      "Average test loss: 0.0040942232143133875\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10587105198038949\n",
      "Average test loss: 0.004226654421641595\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10589745916922887\n",
      "Average test loss: 0.0040859197498195705\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10544277507066727\n",
      "Average test loss: 0.0041207070375482244\n",
      "Epoch 123/300\n",
      "Average training loss: 0.1053203347325325\n",
      "Average test loss: 0.004091621841407485\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10510943136612574\n",
      "Average test loss: 0.004242586964534389\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10516155036952761\n",
      "Average test loss: 0.00411641084526976\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10504173011912239\n",
      "Average test loss: 0.004180995128220982\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10476484177509944\n",
      "Average test loss: 0.004210704502132204\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10450127055909898\n",
      "Average test loss: 0.004259399885518683\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10430902788374159\n",
      "Average test loss: 0.004242663488826818\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10425628574689229\n",
      "Average test loss: 0.004141416687725319\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10408392680353588\n",
      "Average test loss: 0.004138937616927756\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10390006475978428\n",
      "Average test loss: 0.004168837386286921\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10364308185709847\n",
      "Average test loss: 0.004137402577118741\n",
      "Epoch 134/300\n",
      "Average training loss: 0.1035373863975207\n",
      "Average test loss: 0.0041655285317036845\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10358216467168596\n",
      "Average test loss: 0.004212313282820914\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10331596776511934\n",
      "Average test loss: 0.0042013773055126265\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10332268052630954\n",
      "Average test loss: 0.004235140235887633\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10307622904247708\n",
      "Average test loss: 0.00416629604767594\n",
      "Epoch 139/300\n",
      "Average training loss: 0.1028391907148891\n",
      "Average test loss: 0.004210872070242962\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10275549804170926\n",
      "Average test loss: 0.004285416641996966\n",
      "Epoch 141/300\n",
      "Average training loss: 0.10262918478250503\n",
      "Average test loss: 0.004215283816473352\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10257349993122948\n",
      "Average test loss: 0.00424538931540317\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10244518731037776\n",
      "Average test loss: 0.00416442959714267\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10223346232705646\n",
      "Average test loss: 0.0045185635007090035\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10199317466550403\n",
      "Average test loss: 0.004189163861589299\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10201207189427482\n",
      "Average test loss: 0.004257929568696353\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1016932646897104\n",
      "Average test loss: 0.0041638109092083245\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10175992748472426\n",
      "Average test loss: 0.004198564522796207\n",
      "Epoch 149/300\n",
      "Average training loss: 0.10145831071668202\n",
      "Average test loss: 0.004225501333052913\n",
      "Epoch 150/300\n",
      "Average training loss: 0.1016853729751375\n",
      "Average test loss: 0.004160256064807375\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10150548392865393\n",
      "Average test loss: 0.0042425746449993714\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10118226416243448\n",
      "Average test loss: 0.004207035481515858\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10097962250312169\n",
      "Average test loss: 0.004287928192565838\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10092942559719086\n",
      "Average test loss: 0.004294671897465984\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10084563563929663\n",
      "Average test loss: 0.004368952275357312\n",
      "Epoch 156/300\n",
      "Average training loss: 0.10066302531295353\n",
      "Average test loss: 0.004181125893361039\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10053310573432181\n",
      "Average test loss: 0.004216743929104673\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10052623828583293\n",
      "Average test loss: 0.004384476596282588\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10040985853142209\n",
      "Average test loss: 0.004255528907395071\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10042998726500406\n",
      "Average test loss: 0.00418713173073613\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10013508756955465\n",
      "Average test loss: 0.004394150633157955\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09991600712802676\n",
      "Average test loss: 0.004406580687397056\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09988744048277537\n",
      "Average test loss: 0.004282262684570419\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09985889007647832\n",
      "Average test loss: 0.004261980161484745\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0996257102423244\n",
      "Average test loss: 0.0042984131713294325\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09955863419506285\n",
      "Average test loss: 0.004343818000621266\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09960853238238229\n",
      "Average test loss: 0.004213840825897124\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0994257973631223\n",
      "Average test loss: 0.004324828224670556\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09918458672364552\n",
      "Average test loss: 0.004274657470898496\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09910396854082744\n",
      "Average test loss: 0.004234152604515354\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09921113132105934\n",
      "Average test loss: 0.004236808665924602\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09913579361306296\n",
      "Average test loss: 0.0042787941180997425\n",
      "Epoch 173/300\n",
      "Average training loss: 0.09899749472406176\n",
      "Average test loss: 0.004290546728298068\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09875199737151463\n",
      "Average test loss: 0.004244583230465651\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09857516325182385\n",
      "Average test loss: 0.0043120466371377305\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09868481020132701\n",
      "Average test loss: 0.004248108480746547\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09831883544392056\n",
      "Average test loss: 0.004296238970839315\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09862959260410732\n",
      "Average test loss: 0.004276789934684833\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09840777991215388\n",
      "Average test loss: 0.004184725295752287\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09819944813516406\n",
      "Average test loss: 0.004410775850216548\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09808402419752545\n",
      "Average test loss: 0.004200455682973067\n",
      "Epoch 182/300\n",
      "Average training loss: 0.09833120346400473\n",
      "Average test loss: 0.00434406865739988\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09783447272910012\n",
      "Average test loss: 0.004284810483041737\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09788011521100998\n",
      "Average test loss: 0.004345089092643724\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09771766550673379\n",
      "Average test loss: 0.004377798202137152\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09766430036889182\n",
      "Average test loss: 0.004245537770705091\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09738116455740399\n",
      "Average test loss: 0.00448124085408118\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09749739113118913\n",
      "Average test loss: 0.004253599017030663\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09716575468911065\n",
      "Average test loss: 0.004363899510974685\n",
      "Epoch 193/300\n",
      "Average test loss: 0.004273441400378943\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09687071126699448\n",
      "Average test loss: 0.00432343395985663\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09689887485901515\n",
      "Average test loss: 0.004322420061255495\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09683156478404999\n",
      "Average test loss: 0.004265963454006446\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09675214126374987\n",
      "Average test loss: 0.004254812957925929\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09646694063478047\n",
      "Average test loss: 0.00431963840747873\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09656973329517576\n",
      "Average test loss: 0.004334409850959976\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0966349280807707\n",
      "Average test loss: 0.00424602384492755\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09638050472736359\n",
      "Average test loss: 0.0043200117542097965\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09644965436061223\n",
      "Average test loss: 0.004298040209545029\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09622374214728674\n",
      "Average test loss: 0.004262159340083599\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09615480957428614\n",
      "Average test loss: 0.004488554791443878\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09596085624231232\n",
      "Average test loss: 0.004666474758336941\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09599219775862164\n",
      "Average test loss: 0.004334812641143799\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09587227361069785\n",
      "Average test loss: 0.004227209837072425\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09574172814024819\n",
      "Average test loss: 0.00431904239124722\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09576633279191123\n",
      "Average test loss: 0.004582983027729724\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09557629725005892\n",
      "Average test loss: 0.004442849365373452\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09576291444897651\n",
      "Average test loss: 0.00441302625876334\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09552781130207909\n",
      "Average test loss: 0.0044377961372956635\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09536263589726554\n",
      "Average test loss: 0.0043753236875765855\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09527425227562586\n",
      "Average test loss: 0.0043407473800083\n",
      "Epoch 217/300\n",
      "Average training loss: 0.09538942051596112\n",
      "Average test loss: 0.004482691160092751\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0952604812781016\n",
      "Average test loss: 0.004393392388191488\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09523498519923952\n",
      "Average test loss: 0.004223253805190325\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09515844382842382\n",
      "Average test loss: 0.004305089694965217\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09501830612950855\n",
      "Average test loss: 0.004370646439078782\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09486805556880103\n",
      "Average test loss: 0.004292903350666165\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09493223430050744\n",
      "Average test loss: 0.0044359048816065\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09465473608175913\n",
      "Average test loss: 0.004337805505841971\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0947555781006813\n",
      "Average test loss: 0.004351094000455406\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09460783323314455\n",
      "Average test loss: 0.0042829970698803664\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09471271828810374\n",
      "Average test loss: 0.0042896145896779165\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09506538303030862\n",
      "Average test loss: 0.004381569004721112\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09536265936162737\n",
      "Average test loss: 0.004333951158035133\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09422992878490025\n",
      "Average test loss: 0.004316821849594514\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0941175998184416\n",
      "Average test loss: 0.004334042839705944\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09422269073459837\n",
      "Average test loss: 0.0043199999158581096\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09415984126594332\n",
      "Average test loss: 0.004356145362680157\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09416768169403077\n",
      "Average test loss: 0.004299032047080497\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0941353900498814\n",
      "Average test loss: 0.004405350632551644\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09420609975523418\n",
      "Average test loss: 0.004348017984380325\n",
      "Epoch 237/300\n",
      "Average training loss: 0.09402359605166648\n",
      "Average test loss: 0.004271595979730288\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09398622964488136\n",
      "Average test loss: 0.004401281495475107\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09385225670867496\n",
      "Average test loss: 0.004357424003796445\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09389398427142037\n",
      "Average test loss: 0.004220657815121942\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09367498997847239\n",
      "Average test loss: 0.004377431656751368\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09377413841750887\n",
      "Average test loss: 0.0044172535919480855\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09370566883352067\n",
      "Average test loss: 0.004360335274702973\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09379403490490383\n",
      "Average test loss: 0.004304153573181894\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09353844447930654\n",
      "Average test loss: 0.004376499803322885\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09341963885890113\n",
      "Average test loss: 0.004402397734423478\n",
      "Epoch 247/300\n",
      "Average training loss: 0.09333151612016889\n",
      "Average test loss: 0.004335266874482234\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09342604956361983\n",
      "Average test loss: 0.004310451011690829\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09329726283417808\n",
      "Average test loss: 0.004356689354197847\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09314988358153237\n",
      "Average test loss: 0.004469850393633048\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09328118134869469\n",
      "Average test loss: 0.004363789212786489\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09318186517556508\n",
      "Average test loss: 0.004248866164435943\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09317773152722253\n",
      "Average test loss: 0.004322644712196456\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09303496778342459\n",
      "Average test loss: 0.004325342864212062\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09286214554972119\n",
      "Average test loss: 0.00441122158554693\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09292371486955218\n",
      "Average test loss: 0.004256496827221579\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09288747873571183\n",
      "Average test loss: 0.004280484026504888\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09283059870534473\n",
      "Average test loss: 0.004447125894534919\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09276268227232827\n",
      "Average test loss: 0.004262587591177887\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09282275122404099\n",
      "Average test loss: 0.004347951477600469\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09258625532521142\n",
      "Average test loss: 0.0043313672790924705\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0925750741759936\n",
      "Average test loss: 0.004326911930822664\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09261981767416\n",
      "Average test loss: 0.0042789566529293855\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09259039702680376\n",
      "Average test loss: 0.004351877455910047\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0935002942416403\n",
      "Average test loss: 0.004346508544352319\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0924565212395456\n",
      "Average test loss: 0.004386065730204185\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09225581141312918\n",
      "Average test loss: 0.004377214336974754\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09229118460416794\n",
      "Average test loss: 0.004260802283469174\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09212670323583814\n",
      "Average test loss: 0.004439153563438191\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0922030022210545\n",
      "Average test loss: 0.004363818175469836\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09216657684908973\n",
      "Average test loss: 0.004325244550696677\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09227044629388385\n",
      "Average test loss: 0.004290559707209468\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09203871754142973\n",
      "Average test loss: 0.004380332967473401\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09198245717088381\n",
      "Average test loss: 0.0044767808707224\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09193370077345106\n",
      "Average test loss: 0.004413662970893913\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0920923448006312\n",
      "Average test loss: 0.0043344148198763525\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09183911510970857\n",
      "Average test loss: 0.004352082529622647\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09189468686448203\n",
      "Average test loss: 0.004333084305955304\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0917728226184845\n",
      "Average test loss: 0.004433380484580994\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09178360562854343\n",
      "Average test loss: 0.004524222045722935\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09188940546909968\n",
      "Average test loss: 0.0042718137407468425\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09164234144157833\n",
      "Average test loss: 0.004402432477308644\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09147578385141161\n",
      "Average test loss: 0.004421742579175367\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09160556040207544\n",
      "Average test loss: 0.004382846495343579\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09165176478359434\n",
      "Average test loss: 0.00431368750333786\n",
      "Epoch 286/300\n",
      "Average training loss: 0.09147698393795226\n",
      "Average test loss: 0.0048011028696265485\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09147352067298359\n",
      "Average test loss: 0.004398691904420654\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09136764892604617\n",
      "Average test loss: 0.004249108080648714\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09120013340976503\n",
      "Average test loss: 0.004450072534796265\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09137813853555256\n",
      "Average test loss: 0.004572233294033342\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09123488809665044\n",
      "Average test loss: 0.004398092464026478\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09117523987425698\n",
      "Average test loss: 0.004332264212063617\n",
      "Epoch 293/300\n",
      "Average training loss: 0.09113349615203009\n",
      "Average test loss: 0.004381792977659239\n",
      "Epoch 294/300\n",
      "Average training loss: 0.09108740034037166\n",
      "Average test loss: 0.004373145538692673\n",
      "Epoch 295/300\n",
      "Average training loss: 0.09108379010359446\n",
      "Average test loss: 0.004369984780748685\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09109361958503723\n",
      "Average test loss: 0.004354240093380213\n",
      "Epoch 297/300\n",
      "Average training loss: 0.09099688536259863\n",
      "Average test loss: 0.004322943610035711\n",
      "Epoch 298/300\n",
      "Average training loss: 0.09095851163731682\n",
      "Average test loss: 0.00433523927628994\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0908417522377438\n",
      "Average test loss: 0.0043493441813107995\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09085406019952562\n",
      "Average test loss: 0.004431951003149152\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.255714153713651\n",
      "Average test loss: 0.06177707093333205\n",
      "Epoch 2/300\n",
      "Average training loss: 3.562936302608914\n",
      "Average test loss: 0.004750954547896981\n",
      "Epoch 3/300\n",
      "Average training loss: 2.415419428507487\n",
      "Average test loss: 0.00426299517808689\n",
      "Epoch 4/300\n",
      "Average training loss: 1.9232347366544935\n",
      "Average test loss: 0.004189753016250001\n",
      "Epoch 5/300\n",
      "Average training loss: 1.568763778368632\n",
      "Average test loss: 0.004035796618709961\n",
      "Epoch 6/300\n",
      "Average training loss: 1.384028943909539\n",
      "Average test loss: 0.003991211185438766\n",
      "Epoch 7/300\n",
      "Average training loss: 1.124306369357639\n",
      "Average test loss: 0.00512846457420124\n",
      "Epoch 8/300\n",
      "Average training loss: 0.9108187805281746\n",
      "Average test loss: 0.0038440146094395056\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7610744660695394\n",
      "Average test loss: 0.0037506953991121718\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6120394090546502\n",
      "Average test loss: 0.017836081446458894\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5009411818716261\n",
      "Average test loss: 0.0037968052664978636\n",
      "Epoch 12/300\n",
      "Average training loss: 0.41836705157491894\n",
      "Average test loss: 0.0035985697420934837\n",
      "Epoch 13/300\n",
      "Average training loss: 0.35556648988193934\n",
      "Average test loss: 0.0035509099120067224\n",
      "Epoch 14/300\n",
      "Average training loss: 0.30675125857194263\n",
      "Average test loss: 0.0035467902705487277\n",
      "Epoch 15/300\n",
      "Average training loss: 0.26899025724993814\n",
      "Average test loss: 0.0034500924566139776\n",
      "Epoch 16/300\n",
      "Average training loss: 0.23898068102200826\n",
      "Average test loss: 0.0034894719382541047\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2149265956878662\n",
      "Average test loss: 0.00343446609005332\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1955241502788332\n",
      "Average test loss: 0.0033611404875086414\n",
      "Epoch 19/300\n",
      "Average training loss: 0.17933473141988118\n",
      "Average test loss: 0.0034340738728642463\n",
      "Epoch 20/300\n",
      "Average training loss: 0.16572512103451623\n",
      "Average test loss: 0.003354104758550723\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15392735902468363\n",
      "Average test loss: 0.0032800028965704973\n",
      "Epoch 22/300\n",
      "Average training loss: 0.14366566122902763\n",
      "Average test loss: 0.00329789630530609\n",
      "Epoch 23/300\n",
      "Average training loss: 0.13568097264236875\n",
      "Average test loss: 0.0032650937793983355\n",
      "Epoch 24/300\n",
      "Average training loss: 0.12975987368159825\n",
      "Average test loss: 0.0032250672992732788\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12475059665573968\n",
      "Average test loss: 0.003195739267807868\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12053565709458457\n",
      "Average test loss: 0.0032021809826708503\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11690613312853707\n",
      "Average test loss: 0.0031980840087764795\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11370876084433662\n",
      "Average test loss: 0.00319510533929699\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11090811176432504\n",
      "Average test loss: 0.003131435963842604\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1087642348408699\n",
      "Average test loss: 0.003146139444162448\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1071796539094713\n",
      "Average test loss: 0.003215468270911111\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10576187368565136\n",
      "Average test loss: 0.0031279167214201555\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10462308129999373\n",
      "Average test loss: 0.0031036152032514414\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10344343027141359\n",
      "Average test loss: 0.00309812862591611\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10257125402821435\n",
      "Average test loss: 0.003093187623553806\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10163076871633529\n",
      "Average test loss: 0.0030739952489319774\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1008216278553009\n",
      "Average test loss: 0.0030875902105536725\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09916209257973566\n",
      "Average test loss: 0.0030596960849232145\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09847170960903168\n",
      "Average test loss: 0.0030799726073940594\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09769624921679497\n",
      "Average test loss: 0.0030782742069827185\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09715889506869846\n",
      "Average test loss: 0.0030701589019348223\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09654110207822587\n",
      "Average test loss: 0.0030629146974533797\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09596162094672521\n",
      "Average test loss: 0.003096857546518246\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09544890638854768\n",
      "Average test loss: 0.0030286434930231836\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09475100558333927\n",
      "Average test loss: 0.0030281099390445485\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09427144681082832\n",
      "Average test loss: 0.0030745001296616265\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09285744314392408\n",
      "Average test loss: 0.0030419053381515876\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09205624522103203\n",
      "Average test loss: 0.0030262045154554975\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09151340549521976\n",
      "Average test loss: 0.0030055865947571066\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09075087136692471\n",
      "Average test loss: 0.003013081905328565\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09027651907669174\n",
      "Average test loss: 0.0030378620682491196\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0899238656229443\n",
      "Average test loss: 0.0030083804970814122\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08952171407805548\n",
      "Average test loss: 0.0030104683000180456\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08924375987052917\n",
      "Average test loss: 0.0030439790677693154\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08870835426780913\n",
      "Average test loss: 0.003014828530450662\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0883718574643135\n",
      "Average test loss: 0.00300528186518285\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08799858957860204\n",
      "Average test loss: 0.0030734908551805547\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08759392656882604\n",
      "Average test loss: 0.003001929247958793\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0871064301000701\n",
      "Average test loss: 0.003011922290341722\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08683286862903171\n",
      "Average test loss: 0.003040255103467239\n",
      "Epoch 66/300\n",
      "Average training loss: 0.0865775990817282\n",
      "Average test loss: 0.0031488322363131577\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08608797766764958\n",
      "Average training loss: 0.08523241347074509\n",
      "Average test loss: 0.003017534944332308\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08499874651432038\n",
      "Average test loss: 0.0030049043455057673\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08479377888970904\n",
      "Average test loss: 0.0030349747125680247\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08432930477460225\n",
      "Average test loss: 0.0030208031402693854\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08398798896869024\n",
      "Average test loss: 0.003085669073379702\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08364226539267434\n",
      "Average test loss: 0.0030260260446618\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08320890269676844\n",
      "Average test loss: 0.003058453275718623\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08291523883077834\n",
      "Average test loss: 0.003070582092843122\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08234145149257448\n",
      "Average test loss: 0.0031166966596825256\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0818839103182157\n",
      "Average test loss: 0.003127141575846407\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08166996248563131\n",
      "Average test loss: 0.0031144453678280115\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08132950060897404\n",
      "Average test loss: 0.0030971805434674023\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08100881034798092\n",
      "Average test loss: 0.0030519030578434465\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08073854806025824\n",
      "Average test loss: 0.003189838828725947\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08044695112440321\n",
      "Average test loss: 0.003095491193441881\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08003316411375999\n",
      "Average test loss: 0.003110770583980613\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07999045171340306\n",
      "Average test loss: 0.00312326116187291\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07951750337415271\n",
      "Average test loss: 0.003088056494274901\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07931946430603663\n",
      "Average test loss: 0.003065933196908898\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0783532505830129\n",
      "Average test loss: 0.0031032164353463386\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07817590616146723\n",
      "Average test loss: 0.0032167163176669016\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07807295344935523\n",
      "Average test loss: 0.003084780835029152\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07775013532241186\n",
      "Average test loss: 0.0030921397999756866\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07760820640458001\n",
      "Average test loss: 0.0031914835118999085\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07734288287825054\n",
      "Average test loss: 0.0032179428906076485\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0773820936150021\n",
      "Average test loss: 0.0031256556172544756\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07702142004172008\n",
      "Average test loss: 0.003144201712268922\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07653175907664829\n",
      "Average test loss: 0.0031979723458902703\n",
      "Epoch 100/300\n",
      "Average training loss: 0.076474232825968\n",
      "Average test loss: 0.0031139309799505604\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0761550835635927\n",
      "Average test loss: 0.003111105128294892\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07617977016833094\n",
      "Average test loss: 0.0032540464103221894\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0758882185154491\n",
      "Average test loss: 0.0031416746500051683\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07566638958454132\n",
      "Average test loss: 0.0032156778921683627\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07530107441875669\n",
      "Average test loss: 0.003175626378506422\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07521429293685489\n",
      "Average test loss: 0.0031688256209923163\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07515323651499219\n",
      "Average test loss: 0.0031603448366125425\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07489284655120637\n",
      "Average test loss: 0.0032388510526054435\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07490064348777135\n",
      "Average test loss: 0.003146493437389533\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07462502650419871\n",
      "Average test loss: 0.003225020780538519\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0744793505469958\n",
      "Average test loss: 0.003196452213037345\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07423978485663732\n",
      "Average test loss: 0.0032601227724096842\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0741177263657252\n",
      "Average test loss: 0.0031928333706325953\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07403278388579687\n",
      "Average test loss: 0.0031825673495315845\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0738336111108462\n",
      "Average test loss: 0.0032577763872428073\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07359398315350214\n",
      "Average test loss: 0.003201110669101278\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07370492627223332\n",
      "Average test loss: 0.0031916817741261586\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07333705339829127\n",
      "Average test loss: 0.0032450241210560003\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07320823659168349\n",
      "Average test loss: 0.003233857021563583\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07302657861179776\n",
      "Average test loss: 0.0031838288478967217\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07275735008716583\n",
      "Average test loss: 0.003152376159818636\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07278749179840088\n",
      "Average test loss: 0.0032590663491023912\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07254431369569567\n",
      "Average test loss: 0.0031745154876261948\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07246837570269903\n",
      "Average test loss: 0.0031448494765079684\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07242023090521495\n",
      "Average test loss: 0.003227204901890622\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07247433324654896\n",
      "Average test loss: 0.0031778494303839073\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07201751350031958\n",
      "Average test loss: 0.0032086576340306138\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07209341610802544\n",
      "Average test loss: 0.0032560967972709073\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07171089912785424\n",
      "Average test loss: 0.0031841575706170666\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07164840796920988\n",
      "Average test loss: 0.0033235570498638682\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07202137386798858\n",
      "Average test loss: 0.003207441456202004\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07143512741393514\n",
      "Average test loss: 0.0032366669335299066\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07123992852701082\n",
      "Average test loss: 0.0031923319987124865\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07130687608983782\n",
      "Average test loss: 0.0032937855714311203\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07125266206264495\n",
      "Average test loss: 0.003210591158105267\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07114865845110681\n",
      "Average test loss: 0.0032980510973268087\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07096391531162792\n",
      "Average test loss: 0.0032349212107559045\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07096455421050389\n",
      "Average test loss: 0.0031861726596123644\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0705642855697208\n",
      "Average test loss: 0.003277940501355463\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07051827361186346\n",
      "Average test loss: 0.003255977061473661\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07043403598997328\n",
      "Average test loss: 0.0032712842958668867\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07034490327371491\n",
      "Average test loss: 0.003307683587384721\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07030629177888234\n",
      "Average test loss: 0.003284863170236349\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07023052422867881\n",
      "Average test loss: 0.003205224174178309\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0700931750204828\n",
      "Average test loss: 0.003267558278929856\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06995803887314267\n",
      "Average test loss: 0.0032336731176409454\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06986176913976669\n",
      "Average test loss: 0.003233432893330852\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0698640355931388\n",
      "Average test loss: 0.0033049647766682836\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06969287502103381\n",
      "Average test loss: 0.0034687318336218595\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06976302813159095\n",
      "Average test loss: 0.0032994180212004317\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06952661484479904\n",
      "Average test loss: 0.0032150317343572774\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06960720266567336\n",
      "Average test loss: 0.0034438681834273867\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06941776211394204\n",
      "Average test loss: 0.0033618656249923837\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06937491691443655\n",
      "Average test loss: 0.0032370150219649075\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06927215858962801\n",
      "Average test loss: 0.00322382420880927\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06914235915409193\n",
      "Average test loss: 0.003308721489376492\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06887825367185804\n",
      "Average test loss: 0.0033315394036471844\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06885794725683\n",
      "Average test loss: 0.003264678782266047\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06881418142716089\n",
      "Average test loss: 0.0032373746828072603\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06887930930985345\n",
      "Average test loss: 0.0033503750078380107\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06866653096675873\n",
      "Average test loss: 0.003305226458650496\n",
      "Epoch 162/300\n",
      "Average training loss: 0.06858414782418146\n",
      "Average test loss: 0.003188923525934418\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06843157470226288\n",
      "Average test loss: 0.003268263592074315\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06842576663361655\n",
      "Average test loss: 0.003305108829918835\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06856838946210013\n",
      "Average test loss: 0.0032740308213979005\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06834371943606271\n",
      "Average test loss: 0.0032869199342611764\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06816952633195453\n",
      "Average test loss: 0.0033479914218187334\n",
      "Epoch 168/300\n",
      "Average training loss: 0.068087773376041\n",
      "Average test loss: 0.003249462390421993\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06809147040380371\n",
      "Average test loss: 0.0032876030252211625\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06804435589578417\n",
      "Average test loss: 0.0032830875989877514\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06790000345971849\n",
      "Average test loss: 0.003378089395662149\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06774265819125705\n",
      "Average test loss: 0.003279820747466551\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06792373223106066\n",
      "Average test loss: 0.003288377314599024\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06769304007291793\n",
      "Average test loss: 0.0033711939400268925\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06756549611356523\n",
      "Average test loss: 0.0034039684093246855\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06758203069368998\n",
      "Average test loss: 0.003182717389323645\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06746837525897556\n",
      "Average test loss: 0.0032655867834885917\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0674869082040257\n",
      "Average test loss: 0.0033369372996191185\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06735284795032608\n",
      "Average test loss: 0.003336908094998863\n",
      "Epoch 180/300\n",
      "Average training loss: 0.067414654162195\n",
      "Average test loss: 0.003256043006148603\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06728643447491858\n",
      "Average test loss: 0.003349557083307041\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06715693860914972\n",
      "Average test loss: 0.0033710019178688527\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0670099532339308\n",
      "Average test loss: 0.00422205033691393\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06682538272937139\n",
      "Average test loss: 0.003357050899002287\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06701055719455083\n",
      "Average test loss: 0.0032582093121276963\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0668734917110867\n",
      "Average test loss: 0.003351959202852514\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06678877046042019\n",
      "Average test loss: 0.003327893473621872\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06670553053749932\n",
      "Average test loss: 0.0033451208581940996\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0668007944226265\n",
      "Average test loss: 0.003192379997215337\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06654118519359165\n",
      "Average test loss: 0.003367025004906787\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06653700700071123\n",
      "Average test loss: 0.0032653092096249265\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0665560874707169\n",
      "Average test loss: 0.003285961211141613\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0665290951596366\n",
      "Average test loss: 0.0032590879266046814\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0664051277604368\n",
      "Average test loss: 0.003305877672094438\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06628795375095474\n",
      "Average test loss: 0.0033292079576187662\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06610507735610008\n",
      "Average test loss: 0.0032351907011535434\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06633859732747079\n",
      "Average test loss: 0.0033320325122525294\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06609080133504337\n",
      "Average test loss: 0.0034033510271045896\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06608693422211541\n",
      "Average test loss: 0.003294061122669114\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06601121699478892\n",
      "Average test loss: 0.003464090647175908\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06596663601199786\n",
      "Average test loss: 0.0033491752393957643\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06613868558075693\n",
      "Average test loss: 0.00334669490572479\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06589484787649579\n",
      "Average test loss: 0.0033432695894605583\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06581572309467527\n",
      "Average test loss: 0.0033822979262719552\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06578109246492386\n",
      "Average test loss: 0.0033268402425779236\n",
      "Epoch 206/300\n",
      "Average training loss: 0.06566791015532282\n",
      "Average test loss: 0.0033307191439800794\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06581793647673395\n",
      "Average test loss: 0.0033134139082911943\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06573477811945809\n",
      "Average test loss: 0.003370472433252467\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06550818989674251\n",
      "Average test loss: 0.0033153708341221015\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06543185997671551\n",
      "Average test loss: 0.003318266215216782\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06550112402439118\n",
      "Average test loss: 0.0033712324268288085\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0653857873181502\n",
      "Average test loss: 0.003304981851329406\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0653645625743601\n",
      "Average test loss: 0.0033204033338568276\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06520846032102903\n",
      "Average test loss: 0.0033100475832406016\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06544516493214501\n",
      "Average test loss: 0.0033245405964553356\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06532397847043143\n",
      "Average test loss: 0.0033124183362556827\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06514883601665497\n",
      "Average test loss: 0.0032836108977595964\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06511190905835894\n",
      "Average test loss: 0.0033288990776571964\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06519000615676245\n",
      "Average test loss: 0.0033606012645694943\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06502237698435784\n",
      "Average test loss: 0.003342520836326811\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06498484819134076\n",
      "Average test loss: 0.003419378782932957\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06487759133179982\n",
      "Average test loss: 0.0033186199474665853\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06493854720724954\n",
      "Average test loss: 0.0034980111277351776\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06481072168217765\n",
      "Average test loss: 0.0035439804212914574\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06482929378085667\n",
      "Average test loss: 0.003317597147387763\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06481657324234645\n",
      "Average test loss: 0.003333276855034961\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06476611821519004\n",
      "Average test loss: 0.0034094677579899627\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06465684586763382\n",
      "Average test loss: 0.003418139865208003\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06456385142273373\n",
      "Average test loss: 0.0033578498168951934\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06458599727683598\n",
      "Average test loss: 0.00344888317791952\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0645119470026758\n",
      "Average test loss: 0.003442021379661229\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06447386065125466\n",
      "Average test loss: 0.0034718532791982093\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06453325487176577\n",
      "Average test loss: 0.0033523131927682293\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06437016134791904\n",
      "Average test loss: 0.0033142684371107152\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06433529645866817\n",
      "Average test loss: 0.003361786474991176\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06423963568276829\n",
      "Average test loss: 0.0033899910288552443\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0643708427184158\n",
      "Average test loss: 0.003272723497822881\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0641997767686844\n",
      "Average test loss: 0.003332840412441227\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06416179308626387\n",
      "Average test loss: 0.0033580592742396727\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06417960866954592\n",
      "Average test loss: 0.003255201969916622\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06425213661458758\n",
      "Average test loss: 0.0033741858551899592\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06396352210972044\n",
      "Average test loss: 0.003397544167935848\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06390802609920501\n",
      "Average test loss: 0.003371360599787699\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06387863816155327\n",
      "Average test loss: 0.0033370626324580777\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06392561825447612\n",
      "Average test loss: 0.0033149978456397853\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06395940283272002\n",
      "Average test loss: 0.0033963409589810502\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06383992565009329\n",
      "Average test loss: 0.003383601992908451\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06381143711010615\n",
      "Average test loss: 0.0033413081914186477\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06388538250823815\n",
      "Average test loss: 0.003368282161653042\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06366394432385762\n",
      "Average test loss: 0.0034068795674377017\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06374201211002138\n",
      "Average test loss: 0.0033423045236203406\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06362201996644339\n",
      "Average test loss: 0.0033429393546862735\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0635647492673662\n",
      "Average test loss: 0.0032805848547981845\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06375838073094686\n",
      "Average test loss: 0.0033502562830431595\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06391159665915701\n",
      "Average test loss: 0.0033696993839823536\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06343965201576551\n",
      "Average test loss: 0.0033596103673593867\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06342703210645252\n",
      "Average test loss: 0.003415047030482027\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06333456324868732\n",
      "Average test loss: 0.003344233941493763\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0634664644466506\n",
      "Average test loss: 0.003361636966673864\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06365235632658005\n",
      "Average test loss: 0.003432341569620702\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0634283011621899\n",
      "Average test loss: 0.00353906068764627\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06326063910457823\n",
      "Average test loss: 0.0034046698736233845\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0631969125436412\n",
      "Average test loss: 0.0035124412052747276\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06315693404277166\n",
      "Average test loss: 0.003419919366844826\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0632042712966601\n",
      "Average test loss: 0.0033307221127260064\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06307490021321509\n",
      "Average test loss: 0.003335129057574603\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06311272058884303\n",
      "Average test loss: 0.0033357288280708923\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06306253144807286\n",
      "Average test loss: 0.003306189465026061\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06297307755218612\n",
      "Average test loss: 0.0034574260415716302\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0629623675280147\n",
      "Average test loss: 0.003431271298064126\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06307599582937029\n",
      "Average test loss: 0.0033810359539671077\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06289750150839488\n",
      "Average test loss: 0.003377664723743995\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06284704645143614\n",
      "Average test loss: 0.0034218327709370187\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06276749373144573\n",
      "Average test loss: 0.003398557568589846\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06284668899907006\n",
      "Average test loss: 0.003359871627142032\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06287254456679026\n",
      "Average test loss: 0.003337735876027081\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06273826631241375\n",
      "Average test loss: 0.003352136762605773\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06290817173653179\n",
      "Average test loss: 0.003338216721183724\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0628628723886278\n",
      "Average test loss: 0.003308790605308281\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06258836311764188\n",
      "Average test loss: 0.0034124196546359196\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0626812011566427\n",
      "Average test loss: 0.0034162970321873825\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06267356201012929\n",
      "Average test loss: 0.0033637900482863187\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06265501425001356\n",
      "Average test loss: 0.0033349123866193825\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06239537067545785\n",
      "Average test loss: 0.0034699699539277287\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06241958652933439\n",
      "Average test loss: 0.003410026791402035\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06240566619237264\n",
      "Average test loss: 0.0033542295710908042\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06239731566773521\n",
      "Average test loss: 0.0033282910026609896\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06250673303339216\n",
      "Average test loss: 0.0033793385902212724\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06242800954977671\n",
      "Average test loss: 0.003409456858928833\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06227707521782981\n",
      "Average test loss: 0.0033377578190217417\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06235950257380803\n",
      "Average test loss: 0.003371634738312827\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06236769457989269\n",
      "Average test loss: 0.0034231778915143674\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06236506724026468\n",
      "Average test loss: 0.0034111348837614058\n",
      "Epoch 294/300\n",
      "Average training loss: 0.062322437869177925\n",
      "Average test loss: 0.003309671653020713\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06215216074387232\n",
      "Average test loss: 0.003379633475922876\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06204441339108679\n",
      "Average test loss: 0.003301165351851119\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06220911757151286\n",
      "Average test loss: 0.003399795786374145\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06217424158917533\n",
      "Average test loss: 0.0034319275319576264\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06202991388903724\n",
      "Average test loss: 0.0034024323241578208\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06207271022929085\n",
      "Average test loss: 0.0034027977265003654\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.901780428144667\n",
      "Average test loss: 0.004849317791561286\n",
      "Epoch 2/300\n",
      "Average training loss: 3.4423386340671116\n",
      "Average test loss: 0.005615255410472552\n",
      "Epoch 3/300\n",
      "Average training loss: 2.495663778093126\n",
      "Average test loss: 0.00433089036680758\n",
      "Epoch 4/300\n",
      "Average training loss: 2.0048407540851167\n",
      "Average test loss: 0.003695088025285966\n",
      "Epoch 5/300\n",
      "Average training loss: 1.706544608645969\n",
      "Average test loss: 0.003723390408481161\n",
      "Epoch 6/300\n",
      "Average training loss: 1.4354219110276965\n",
      "Average test loss: 0.003476032173054086\n",
      "Epoch 7/300\n",
      "Average training loss: 1.1997144190470377\n",
      "Average test loss: 0.003373384204796619\n",
      "Epoch 8/300\n",
      "Average training loss: 1.0068357117440965\n",
      "Average test loss: 0.0032340544304913944\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8354095928404066\n",
      "Average test loss: 0.00320667987730768\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7030965230200026\n",
      "Average test loss: 0.0030456087162925136\n",
      "Epoch 11/300\n",
      "Average training loss: 0.588317752096388\n",
      "Average test loss: 0.003019868642298712\n",
      "Epoch 12/300\n",
      "Average training loss: 0.4906838252544403\n",
      "Average test loss: 0.003009572641717063\n",
      "Epoch 13/300\n",
      "Average training loss: 0.41223640865749783\n",
      "Average test loss: 0.002940746175746123\n",
      "Epoch 14/300\n",
      "Average training loss: 0.3499840722878774\n",
      "Average test loss: 0.0028086118031707074\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30119642096095617\n",
      "Average test loss: 0.0028084405213594436\n",
      "Epoch 16/300\n",
      "Average training loss: 0.2629177413913939\n",
      "Average test loss: 0.00268601599521935\n",
      "Epoch 17/300\n",
      "Average training loss: 0.23131958564122518\n",
      "Average test loss: 0.0027436587642878296\n",
      "Epoch 18/300\n",
      "Average training loss: 0.20523550601800283\n",
      "Average test loss: 0.0026865849716381893\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18354842689302234\n",
      "Average test loss: 0.0025982405739939877\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1659892029364904\n",
      "Average test loss: 0.0025383497782879407\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15064651920398076\n",
      "Average test loss: 0.0025687958682990737\n",
      "Epoch 22/300\n",
      "Average training loss: 0.13821507540014055\n",
      "Average test loss: 0.0024996603199591238\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12746430499355\n",
      "Average test loss: 0.002488597167345385\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11823211213615206\n",
      "Average test loss: 0.0025320341378036472\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11049818899234136\n",
      "Average test loss: 0.002434238641212384\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10402718810240427\n",
      "Average test loss: 0.002416246593826347\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09916197346978717\n",
      "Average test loss: 0.0024696244748516217\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0957900904085901\n",
      "Average test loss: 0.0024045258950855996\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09304884890715281\n",
      "Average test loss: 0.0023893620346983275\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09085095415181584\n",
      "Average test loss: 0.00243756478631662\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08900146761205462\n",
      "Average test loss: 0.002413190427339739\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08740776610374451\n",
      "Average test loss: 0.0023663038674535024\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08585860450400247\n",
      "Average test loss: 0.002365481663288342\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08462705869144864\n",
      "Average test loss: 0.0023626584449989926\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08340050221151776\n",
      "Average test loss: 0.0023132265106671386\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08225460190905465\n",
      "Average test loss: 0.0023243474666443135\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08120545089907116\n",
      "Average test loss: 0.0023221834077396327\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0802936174472173\n",
      "Average test loss: 0.0023274485477142864\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07941374948951933\n",
      "Average test loss: 0.002312941652826137\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07850159909327825\n",
      "Average test loss: 0.00232305853224049\n",
      "Epoch 41/300\n",
      "Average training loss: 0.0777295437918769\n",
      "Average test loss: 0.002317229267106288\n",
      "Epoch 42/300\n",
      "Average training loss: 0.07678676968150669\n",
      "Average test loss: 0.002310843214806583\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07623629963397979\n",
      "Average test loss: 0.002280549059311549\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07550291507111655\n",
      "Average test loss: 0.0026381845163802307\n",
      "Epoch 45/300\n",
      "Average training loss: 0.07508218554324574\n",
      "Average test loss: 0.002294518060154385\n",
      "Epoch 46/300\n",
      "Average training loss: 0.07423133847448561\n",
      "Average test loss: 0.002258927575002114\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0736427568131023\n",
      "Average test loss: 0.002255775186336703\n",
      "Epoch 48/300\n",
      "Average training loss: 0.07325249832868576\n",
      "Average test loss: 0.0022755535880310666\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0727104079193539\n",
      "Average test loss: 0.002247829761149155\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0719912216729588\n",
      "Average test loss: 0.002306824898140298\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0715311151014434\n",
      "Average test loss: 0.002298783564940095\n",
      "Epoch 52/300\n",
      "Average training loss: 0.07101820591423247\n",
      "Average test loss: 0.0022474864390161302\n",
      "Epoch 53/300\n",
      "Average training loss: 0.07083762581149737\n",
      "Average test loss: 0.002264432230239941\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07012404106391801\n",
      "Average test loss: 0.002295517312776711\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06972234156065517\n",
      "Average test loss: 0.00226032821337382\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06941548003090753\n",
      "Average test loss: 0.002280506618010501\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06894382148981094\n",
      "Average test loss: 0.0022736325841397047\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0684334131942855\n",
      "Average test loss: 0.00223786088410351\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0681704081065125\n",
      "Average test loss: 0.002252493116590712\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06770377284950681\n",
      "Average test loss: 0.002242783398160504\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06746675997310214\n",
      "Average test loss: 0.0022551936383048694\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0671132571068075\n",
      "Average test loss: 0.0022453364388396345\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06650547486874793\n",
      "Average test loss: 0.0022678683005894224\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06612023395962185\n",
      "Average test loss: 0.0022783749885857104\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06585938610302078\n",
      "Average test loss: 0.002265241088759568\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06549210435483191\n",
      "Average test loss: 0.0022732513265477285\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06517031332188182\n",
      "Average test loss: 0.0022540238636235394\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0647289901971817\n",
      "Average test loss: 0.0022679087379947303\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06455182478162977\n",
      "Average test loss: 0.002258194138399429\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06419306869970427\n",
      "Average test loss: 0.002836910463869572\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06387815713220173\n",
      "Average test loss: 0.0023049128852370713\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06350275100602044\n",
      "Average test loss: 0.0023011502355544102\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0631546211971177\n",
      "Average test loss: 0.002258141086747249\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0629054434299469\n",
      "Average test loss: 0.002288437049421999\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06261466884281901\n",
      "Average test loss: 0.0022733534144030675\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0622579895026154\n",
      "Average test loss: 0.0023370707451055447\n",
      "Epoch 77/300\n",
      "Average training loss: 0.061935908724864326\n",
      "Average test loss: 0.002531067676221331\n",
      "Epoch 78/300\n",
      "Average training loss: 0.061863570047749415\n",
      "Average test loss: 0.0023080041924905446\n",
      "Epoch 79/300\n",
      "Average training loss: 0.061342050304015475\n",
      "Average test loss: 0.002278621046907372\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06118069362309244\n",
      "Average test loss: 0.00228939038183954\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06076291843255361\n",
      "Average test loss: 0.0023129012503971657\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06051679097943836\n",
      "Average test loss: 0.0023135625437522927\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06021180716488096\n",
      "Average test loss: 0.0022894774737457434\n",
      "Epoch 84/300\n",
      "Average training loss: 0.060156564735704\n",
      "Average test loss: 0.0023369169311804904\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0597349038388994\n",
      "Average test loss: 0.0023729307372123004\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05969934960868623\n",
      "Average test loss: 0.002417944073677063\n",
      "Epoch 87/300\n",
      "Average training loss: 0.059265925973653794\n",
      "Average test loss: 0.0022926600547507406\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05900114134616322\n",
      "Average test loss: 0.0023394541988770168\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05885583486159642\n",
      "Average test loss: 0.002359430434803168\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05877486861083243\n",
      "Average test loss: 0.0023178102638986378\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05834440104166667\n",
      "Average test loss: 0.002373719480095638\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05831743938393063\n",
      "Average test loss: 0.0023170538744371797\n",
      "Epoch 93/300\n",
      "Average training loss: 0.05796601558062765\n",
      "Average test loss: 0.002399533374235034\n",
      "Epoch 94/300\n",
      "Average training loss: 0.057838654392295415\n",
      "Average test loss: 0.0023713516087995636\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05752758139371872\n",
      "Average test loss: 0.0023523835256281825\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05733463802933693\n",
      "Average test loss: 0.002342821611505416\n",
      "Epoch 97/300\n",
      "Average training loss: 0.057152671565612156\n",
      "Average test loss: 0.0024241788434899514\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05693256794744068\n",
      "Average test loss: 0.0023731921314158375\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05672152674529288\n",
      "Average test loss: 0.0023624628049631912\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05653842198517588\n",
      "Average test loss: 0.0023997276922067007\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05647149992982547\n",
      "Average test loss: 0.002360427729992403\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05616173169016838\n",
      "Average test loss: 0.002415768778158559\n",
      "Epoch 103/300\n",
      "Average training loss: 0.056068688869476316\n",
      "Average test loss: 0.002389475672195355\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05592683773901728\n",
      "Average test loss: 0.002367807335116797\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05576429918408394\n",
      "Average test loss: 0.0023629389034791125\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05553155379162894\n",
      "Average test loss: 0.0024931859336793424\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05540543421109517\n",
      "Average test loss: 0.0023652088306844233\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05516491546895769\n",
      "Average test loss: 0.002353416523171796\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05508617420660125\n",
      "Average test loss: 0.0023932969043445255\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05498814924226867\n",
      "Average test loss: 0.002338755789730284\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05484304768509335\n",
      "Average test loss: 0.0024066656730655166\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05461579165856043\n",
      "Average test loss: 0.002413824342191219\n",
      "Epoch 113/300\n",
      "Average training loss: 0.054504259589645596\n",
      "Average test loss: 0.0023685881928023366\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05458914761410819\n",
      "Average test loss: 0.0023798930420436794\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0541971984439426\n",
      "Average test loss: 0.00241822042192022\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05408937612175942\n",
      "Average test loss: 0.002426037213454644\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0539527650475502\n",
      "Average test loss: 0.002373254387329022\n",
      "Epoch 118/300\n",
      "Average training loss: 0.053811769402689405\n",
      "Average test loss: 0.002385746594104502\n",
      "Epoch 119/300\n",
      "Average training loss: 0.053747605241007274\n",
      "Average test loss: 0.0023674384566644826\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05354110529025396\n",
      "Average test loss: 0.00241539028328326\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05353085054953893\n",
      "Average test loss: 0.0024948080668432846\n",
      "Epoch 122/300\n",
      "Average training loss: 0.053433645360999635\n",
      "Average test loss: 0.0024261992352290285\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05325287735131052\n",
      "Average test loss: 0.00247548826245798\n",
      "Epoch 124/300\n",
      "Average training loss: 0.053160028782155776\n",
      "Average test loss: 0.0024876534634580214\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05301170121298896\n",
      "Average test loss: 0.002412780340347025\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05301418368352784\n",
      "Average test loss: 0.0024186136699799036\n",
      "Epoch 127/300\n",
      "Average training loss: 0.052810004072056875\n",
      "Average test loss: 0.0024833969784279664\n",
      "Epoch 128/300\n",
      "Average training loss: 0.052673837241199284\n",
      "Average test loss: 0.0024314982957310146\n",
      "Epoch 129/300\n",
      "Average training loss: 0.052586567242940266\n",
      "Average test loss: 0.0024451211747816867\n",
      "Epoch 130/300\n",
      "Average training loss: 0.052534814490212335\n",
      "Average test loss: 0.0025080304214109977\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05242714530064\n",
      "Average test loss: 0.002469176567883955\n",
      "Epoch 132/300\n",
      "Average training loss: 0.052200829638375176\n",
      "Average test loss: 0.002452821912864844\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05228251259856754\n",
      "Average test loss: 0.002453648465789027\n",
      "Epoch 134/300\n",
      "Average training loss: 0.052180378556251523\n",
      "Average test loss: 0.0024486329522397784\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05222321680519316\n",
      "Average test loss: 0.0024272769764065744\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05187617537048128\n",
      "Average test loss: 0.002602877532649371\n",
      "Epoch 137/300\n",
      "Average training loss: 0.051886247009038926\n",
      "Average test loss: 0.0025188839016482235\n",
      "Epoch 138/300\n",
      "Average training loss: 0.051758451021379895\n",
      "Average test loss: 0.0025449065303223\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05166085050172276\n",
      "Average test loss: 0.002475962488187684\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05169598985380597\n",
      "Average test loss: 0.002479511745274067\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05162049321002431\n",
      "Average test loss: 0.002464129657484591\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05144657249914275\n",
      "Average test loss: 0.002524307457109292\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05133332216408518\n",
      "Average test loss: 0.002447624764508671\n",
      "Epoch 144/300\n",
      "Average training loss: 0.051209605909056134\n",
      "Average test loss: 0.0025011622064436477\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0512666989200645\n",
      "Average test loss: 0.002456724953941173\n",
      "Epoch 146/300\n",
      "Average training loss: 0.051100308113627965\n",
      "Average test loss: 0.0025368188172578813\n",
      "Epoch 147/300\n",
      "Average training loss: 0.05095875142349137\n",
      "Average test loss: 0.002450484224077728\n",
      "Epoch 148/300\n",
      "Average training loss: 0.051032268497678966\n",
      "Average test loss: 0.002440544239555796\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05088309213519096\n",
      "Average test loss: 0.002443297526695662\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0508561022248533\n",
      "Average test loss: 0.0024627991229709653\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05093521159887314\n",
      "Average test loss: 0.0024450592984341913\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05066346777147717\n",
      "Average test loss: 0.0024101572200242016\n",
      "Epoch 153/300\n",
      "Average training loss: 0.050627674397495055\n",
      "Average test loss: 0.0024585492271516057\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05038512690530883\n",
      "Average test loss: 0.002464581934321258\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05034608562787374\n",
      "Average test loss: 0.0024715534216827816\n",
      "Epoch 157/300\n",
      "Average training loss: 0.050385235922204126\n",
      "Average test loss: 0.0024413165191395415\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05027646110455195\n",
      "Average test loss: 0.002465140771741668\n",
      "Epoch 160/300\n",
      "Average training loss: 0.050191063808070285\n",
      "Average test loss: 0.002457615045002765\n",
      "Epoch 161/300\n",
      "Average training loss: 0.049902206639448804\n",
      "Average test loss: 0.002524185900265972\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05006494854225053\n",
      "Average test loss: 0.002526595425171157\n",
      "Epoch 163/300\n",
      "Average training loss: 0.049891553027762305\n",
      "Average test loss: 0.002468871200974617\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04984014349513584\n",
      "Average test loss: 0.0024735246518005926\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04984468584756056\n",
      "Average test loss: 0.002537718565099769\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04974937520093388\n",
      "Average test loss: 0.0024996585787998306\n",
      "Epoch 167/300\n",
      "Average training loss: 0.049691147926780914\n",
      "Average test loss: 0.002484796050729023\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04965699877672725\n",
      "Average test loss: 0.0028221538066864013\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04969961408442921\n",
      "Average test loss: 0.0025166954812076356\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04950921612315708\n",
      "Average test loss: 0.002502617796882987\n",
      "Epoch 171/300\n",
      "Average training loss: 0.049397574441300496\n",
      "Average test loss: 0.0024504323803509276\n",
      "Epoch 173/300\n",
      "Average training loss: 0.049381509953074984\n",
      "Average test loss: 0.0025813393416917988\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04924764334493213\n",
      "Average test loss: 0.0024668884654011993\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0492587999370363\n",
      "Average test loss: 0.002549693265722858\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04928398985995187\n",
      "Average test loss: 0.0026765194065454935\n",
      "Epoch 177/300\n",
      "Average training loss: 0.049162348257170786\n",
      "Average test loss: 0.002427740482199523\n",
      "Epoch 178/300\n",
      "Average training loss: 0.049050020651684866\n",
      "Average test loss: 0.00256825718180173\n",
      "Epoch 179/300\n",
      "Average training loss: 0.049067389087544544\n",
      "Average test loss: 0.0025167658668425346\n",
      "Epoch 180/300\n",
      "Average training loss: 0.049036395801438225\n",
      "Average test loss: 0.002709911933996611\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04892845101488961\n",
      "Average test loss: 0.0024983939619527924\n",
      "Epoch 182/300\n",
      "Average training loss: 0.048852581451336544\n",
      "Average test loss: 0.0025644806544813845\n",
      "Epoch 183/300\n",
      "Average training loss: 0.048898247414165076\n",
      "Average test loss: 0.002496693351616462\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04871955148379008\n",
      "Average test loss: 0.0024768113018944857\n",
      "Epoch 185/300\n",
      "Average training loss: 0.048711804227696526\n",
      "Average test loss: 0.0024560419544577597\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04867816240919961\n",
      "Average test loss: 0.0025285030284689534\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04865300298730532\n",
      "Average test loss: 0.0047750240624364875\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04852951068679492\n",
      "Average test loss: 0.002538478983255724\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04856317336360613\n",
      "Average test loss: 0.002496210315471722\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04855534706513087\n",
      "Average test loss: 0.0024565178831625315\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04840283579627673\n",
      "Average test loss: 0.0024570376655707756\n",
      "Epoch 192/300\n",
      "Average training loss: 0.048401445637146635\n",
      "Average test loss: 0.0024982853811234234\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04846374034053749\n",
      "Average test loss: 0.0025815011051793894\n",
      "Epoch 194/300\n",
      "Average training loss: 0.048259489549530875\n",
      "Average test loss: 0.0025117141512326067\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04825911708672841\n",
      "Average test loss: 0.002661861878923244\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04822774788075023\n",
      "Average test loss: 0.002460758046867947\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04814341831207276\n",
      "Average test loss: 0.0025512814414170052\n",
      "Epoch 198/300\n",
      "Average training loss: 0.048084716048505574\n",
      "Average test loss: 0.0025767183756041858\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04806177947918574\n",
      "Average test loss: 0.0024850450532717836\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04808898202909364\n",
      "Average test loss: 0.00253961684451335\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04799444303578801\n",
      "Average test loss: 0.00254829414602783\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04793235699666871\n",
      "Average test loss: 0.0025197393261931007\n",
      "Epoch 203/300\n",
      "Average training loss: 0.047863337874412534\n",
      "Average test loss: 0.0025007898768203128\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04783452983034982\n",
      "Average test loss: 0.0025203000669264133\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04775044085913234\n",
      "Average test loss: 0.0025722681555069155\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04786529865860939\n",
      "Average test loss: 0.002494639650401142\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04770131426056226\n",
      "Average test loss: 0.002576842834138208\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04759594698415862\n",
      "Average test loss: 0.00256960879245566\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04769748828808466\n",
      "Average test loss: 0.002497310386142797\n",
      "Epoch 210/300\n",
      "Average training loss: 0.047582599149809945\n",
      "Average test loss: 0.002590641766993536\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04751737314959367\n",
      "Average test loss: 0.0027266459040757684\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04745198628968662\n",
      "Average test loss: 0.0025284260465866988\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04747603868775897\n",
      "Average test loss: 0.002505087057335509\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04743142756488588\n",
      "Average test loss: 0.002581042778160837\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04738471844792366\n",
      "Average test loss: 0.0024981005562262405\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04734545229209794\n",
      "Average test loss: 0.0025289291008892987\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04733163951171769\n",
      "Average test loss: 0.002541662947378225\n",
      "Epoch 220/300\n",
      "Average training loss: 0.047230750928322476\n",
      "Average test loss: 0.002461973279921545\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04722140527433819\n",
      "Average test loss: 0.002508883929500977\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04717682028479046\n",
      "Average test loss: 0.002544803056245049\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04702782171964645\n",
      "Average test loss: 0.0024986733543790047\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04716534286406305\n",
      "Average test loss: 0.002593533523587717\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04707549396819539\n",
      "Average test loss: 0.0025202551397184533\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04691332710120413\n",
      "Average test loss: 0.002572521915038427\n",
      "Epoch 229/300\n",
      "Average training loss: 0.047027256621254815\n",
      "Average test loss: 0.002506971314135525\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04692155973116557\n",
      "Average test loss: 0.0025305235410730043\n",
      "Epoch 231/300\n",
      "Average training loss: 0.046901444219880635\n",
      "Average test loss: 0.002532911066793733\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04682389525241322\n",
      "Average test loss: 0.0025555451640652286\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04680460305346383\n",
      "Average test loss: 0.00256105513104962\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04676475057999293\n",
      "Average test loss: 0.0025366808221571977\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04673966891566912\n",
      "Average test loss: 0.0025127998940232725\n",
      "Epoch 236/300\n",
      "Average training loss: 0.046726788265837566\n",
      "Average test loss: 0.00255780393195649\n",
      "Epoch 237/300\n",
      "Average training loss: 0.046708392997582754\n",
      "Average test loss: 0.002504174160357151\n",
      "Epoch 238/300\n",
      "Average training loss: 0.046610352317492165\n",
      "Average test loss: 0.0024918915898435646\n",
      "Epoch 240/300\n",
      "Average training loss: 0.046614019354184466\n",
      "Average test loss: 0.0025096924605054988\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0465120833184984\n",
      "Average test loss: 0.002528742271578974\n",
      "Epoch 242/300\n",
      "Average training loss: 0.046500494887431465\n",
      "Average test loss: 0.0025413926146510574\n",
      "Epoch 243/300\n",
      "Average training loss: 0.046579415775007675\n",
      "Average test loss: 0.0024941904419619174\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04647296131981744\n",
      "Average test loss: 0.0025933588314801456\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04634772586491373\n",
      "Average test loss: 0.0025303173181083468\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04642398612697919\n",
      "Average test loss: 0.002552974701238175\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04636236375901434\n",
      "Average test loss: 0.0025351603999733924\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04627801772289806\n",
      "Average test loss: 0.0025139071175621615\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04622530264655749\n",
      "Average test loss: 0.0025218632781050273\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04628574162721634\n",
      "Average test loss: 0.002570465975130598\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04616604190402561\n",
      "Average test loss: 0.002547706023686462\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04611895809239811\n",
      "Average test loss: 0.0026089219175693063\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04609980121917195\n",
      "Average test loss: 0.0025702988002449276\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04604474397169219\n",
      "Average test loss: 0.002577081626074182\n",
      "Epoch 257/300\n",
      "Average training loss: 0.046147079606850945\n",
      "Average test loss: 0.0026034912949221\n",
      "Epoch 258/300\n",
      "Average training loss: 0.046036942823065655\n",
      "Average test loss: 0.0025762331719613738\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04599229618906975\n",
      "Average test loss: 0.002564714611404472\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04598843372033702\n",
      "Average test loss: 0.0025409718662914305\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04598590305778715\n",
      "Average test loss: 0.0025977785715626347\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04584281034602059\n",
      "Average test loss: 0.0025355559111469323\n",
      "Epoch 264/300\n",
      "Average training loss: 0.045875136382049986\n",
      "Average test loss: 0.002547479961067438\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04590240387121836\n",
      "Average test loss: 0.002634746702387929\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04580160887704955\n",
      "Average test loss: 0.0025188127872016693\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04585957105292214\n",
      "Average test loss: 0.0025517178715931043\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04585964851578077\n",
      "Average test loss: 0.002542514707893133\n",
      "Epoch 269/300\n",
      "Average training loss: 0.045753961109452776\n",
      "Average test loss: 0.0026573409712356\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04568114680714078\n",
      "Average test loss: 0.0025985849491424033\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04567414241035779\n",
      "Average test loss: 0.0025380241591483354\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04559486368298531\n",
      "Average test loss: 0.002559610275758637\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04568110101090537\n",
      "Average test loss: 0.0025727261441449325\n",
      "Epoch 274/300\n",
      "Average training loss: 0.045756067719724444\n",
      "Average test loss: 0.002553788576895992\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04559541498290168\n",
      "Average test loss: 0.0025608453549858597\n",
      "Epoch 276/300\n",
      "Average training loss: 0.045564167741272185\n",
      "Average test loss: 0.002557876332352559\n",
      "Epoch 277/300\n",
      "Average training loss: 0.045588448021146984\n",
      "Average test loss: 0.0026336403195228843\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04555765304134952\n",
      "Average test loss: 0.002541330876863665\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04546086239483622\n",
      "Average test loss: 0.00248897656539662\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04548142558005121\n",
      "Average test loss: 0.0025573153437839615\n",
      "Epoch 282/300\n",
      "Average training loss: 0.045534162127309374\n",
      "Average test loss: 0.002585847094241116\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04537100977036688\n",
      "Average test loss: 0.0025541967989669906\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04536338240239355\n",
      "Average test loss: 0.002560260609827108\n",
      "Epoch 285/300\n",
      "Average training loss: 0.045427155796024535\n",
      "Average test loss: 0.0025509056537929508\n",
      "Epoch 286/300\n",
      "Average training loss: 0.045391976283656224\n",
      "Average test loss: 0.0026272638336651854\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04533091098401282\n",
      "Average test loss: 0.002594764766593774\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04524252369999886\n",
      "Average test loss: 0.0025878089159313174\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0454174968832069\n",
      "Average test loss: 0.002541366830468178\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04528430149621434\n",
      "Average test loss: 0.0025904618625839553\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04525282880332735\n",
      "Average test loss: 0.002660805626668864\n",
      "Epoch 292/300\n",
      "Average training loss: 0.045138090044260025\n",
      "Average test loss: 0.00257117169836743\n",
      "Epoch 293/300\n",
      "Average test loss: 0.002525034601800144\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04514020121428702\n",
      "Average test loss: 0.0025312917116615506\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04509339562389586\n",
      "Average test loss: 0.002532814317589833\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0450575302640597\n",
      "Average test loss: 0.0025907964973400036\n",
      "Epoch 298/300\n",
      "Average training loss: 0.045116171813673445\n",
      "Average test loss: 0.0025531979569544394\n",
      "Epoch 299/300\n",
      "Average training loss: 0.044995087636841666\n",
      "Average test loss: 0.0025915417859537736\n",
      "Epoch 300/300\n",
      "Average training loss: 0.045021315331260366\n",
      "Average test loss: 0.002555527202785015\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.477533100552029\n",
      "Average test loss: 0.004646475078745021\n",
      "Epoch 2/300\n",
      "Average training loss: 3.368523953543769\n",
      "Average test loss: 0.0034694652687758207\n",
      "Epoch 3/300\n",
      "Average training loss: 2.516750420888265\n",
      "Average test loss: 0.00318200013310545\n",
      "Epoch 4/300\n",
      "Average training loss: 2.0575579030778672\n",
      "Average test loss: 0.0030215098520533904\n",
      "Epoch 5/300\n",
      "Average training loss: 1.7352901701397365\n",
      "Average test loss: 0.0030240797048641578\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1325425669352214\n",
      "Average test loss: 0.002622866874560714\n",
      "Epoch 9/300\n",
      "Average training loss: 0.9939190004666646\n",
      "Average test loss: 0.0024776055957708093\n",
      "Epoch 10/300\n",
      "Average training loss: 0.8729383041593763\n",
      "Average test loss: 0.0024034112758106654\n",
      "Epoch 11/300\n",
      "Average training loss: 0.7598177963892618\n",
      "Average test loss: 0.002363614465834366\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6555988895628188\n",
      "Average test loss: 0.0022311739871899287\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5612855531904433\n",
      "Average test loss: 0.0021862268331978058\n",
      "Epoch 14/300\n",
      "Average training loss: 0.47673899499575295\n",
      "Average test loss: 0.002130500661416186\n",
      "Epoch 15/300\n",
      "Average training loss: 0.40080338292651707\n",
      "Average test loss: 0.002075860373675823\n",
      "Epoch 16/300\n",
      "Average training loss: 0.337455304145813\n",
      "Average test loss: 0.0021317305409659943\n",
      "Epoch 17/300\n",
      "Average training loss: 0.28524783174196877\n",
      "Average test loss: 0.001977555367474755\n",
      "Epoch 18/300\n",
      "Average training loss: 0.24210917958948347\n",
      "Average test loss: 0.0019647258302817744\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18073821602927315\n",
      "Average test loss: 0.0019195313262235787\n",
      "Epoch 21/300\n",
      "Average training loss: 0.15965600305133396\n",
      "Average test loss: 0.0018855000022384855\n",
      "Epoch 22/300\n",
      "Average training loss: 0.14300801707638636\n",
      "Average test loss: 0.001891197978415423\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12937414619657728\n",
      "Average test loss: 0.001887329228843252\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11813085535499784\n",
      "Average test loss: 0.0018412550083465047\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10900838131374783\n",
      "Average test loss: 0.0018410832894345125\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1011730777223905\n",
      "Average test loss: 0.0018355066388224563\n",
      "Epoch 27/300\n",
      "Average training loss: 0.094685764392217\n",
      "Average test loss: 0.0018153959464074837\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08915254407458835\n",
      "Average test loss: 0.0017853686823509633\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08464388589726554\n",
      "Average test loss: 0.0017919657921625508\n",
      "Epoch 30/300\n",
      "Average training loss: 0.08076052459081014\n",
      "Average test loss: 0.0017784613964872227\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07750526789161893\n",
      "Average test loss: 0.00181902107451525\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07461264912949668\n",
      "Average test loss: 0.0017811117872802748\n",
      "Epoch 33/300\n",
      "Average training loss: 0.072146087427934\n",
      "Average test loss: 0.0017500814326728384\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06811862768067253\n",
      "Average test loss: 0.0017485961601552036\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06645022051533063\n",
      "Average test loss: 0.001715936952787969\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06410600634747081\n",
      "Average test loss: 0.0017301788785391383\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06279502170615726\n",
      "Average test loss: 0.00171060692694866\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06178591291109721\n",
      "Average test loss: 0.0017276598433446553\n",
      "Epoch 41/300\n",
      "Average training loss: 0.060911019610034096\n",
      "Average test loss: 0.0016995591137351261\n",
      "Epoch 42/300\n",
      "Average training loss: 0.060121138013071486\n",
      "Average test loss: 0.001765092108398676\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05933636273940404\n",
      "Average test loss: 0.0017157126437458727\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05853268779979812\n",
      "Average test loss: 0.0016780923928858505\n",
      "Epoch 45/300\n",
      "Average training loss: 0.057849667704767654\n",
      "Average test loss: 0.0016847243816074397\n",
      "Epoch 46/300\n",
      "Average training loss: 0.057209364283416005\n",
      "Average test loss: 0.0017100966467211644\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05664576535423597\n",
      "Average test loss: 0.0016948548754056295\n",
      "Epoch 48/300\n",
      "Average training loss: 0.056029849860403276\n",
      "Average test loss: 0.0016610381367305914\n",
      "Epoch 49/300\n",
      "Average training loss: 0.05500887092616823\n",
      "Average test loss: 0.0017038791653596693\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05448617142107751\n",
      "Average test loss: 0.001664490648234884\n",
      "Epoch 52/300\n",
      "Average training loss: 0.054193096131086346\n",
      "Average test loss: 0.0017318936247482067\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05378176762329207\n",
      "Average test loss: 0.001686631115877794\n",
      "Epoch 54/300\n",
      "Average training loss: 0.053259093473354974\n",
      "Average test loss: 0.001679023599666026\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05285833944545852\n",
      "Average test loss: 0.001692576687472562\n",
      "Epoch 56/300\n",
      "Average training loss: 0.052479254487488004\n",
      "Average test loss: 0.0016697644053233994\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05226083786620034\n",
      "Average test loss: 0.001956099540823036\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05177141793900066\n",
      "Average test loss: 0.0016975069853166738\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05142697831988335\n",
      "Average test loss: 0.001662137642192344\n",
      "Epoch 60/300\n",
      "Average training loss: 0.051125886532995435\n",
      "Average test loss: 0.0016528372240977155\n",
      "Epoch 61/300\n",
      "Average training loss: 0.051044651130835214\n",
      "Average test loss: 0.0016615745798788137\n",
      "Epoch 62/300\n",
      "Average training loss: 0.050448028435309725\n",
      "Average test loss: 0.0016520386817347672\n",
      "Epoch 63/300\n",
      "Average training loss: 0.050093053307798174\n",
      "Average test loss: 0.0016589986166606347\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0496435747510857\n",
      "Average test loss: 0.0016797284034805165\n",
      "Epoch 66/300\n",
      "Average training loss: 0.049284125129381816\n",
      "Average test loss: 0.0016763843121007085\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04894082403182983\n",
      "Average test loss: 0.0016526377689507273\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04925335267848439\n",
      "Average test loss: 0.0016815359148507317\n",
      "Epoch 69/300\n",
      "Average training loss: 0.048501760688092976\n",
      "Average test loss: 0.0016545271550615628\n",
      "Epoch 70/300\n",
      "Average training loss: 0.048087821506791646\n",
      "Average test loss: 0.0016696289853296345\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04806065009699927\n",
      "Average test loss: 0.001683044669425322\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04761295028196441\n",
      "Average test loss: 0.0016638787291530107\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0473560444381502\n",
      "Average test loss: 0.0016755216093733906\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04712672738565339\n",
      "Average test loss: 0.001676368368168672\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04685089022914569\n",
      "Average test loss: 0.0016951297522625989\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0467210300664107\n",
      "Average test loss: 0.0017104398641838796\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0464800393515163\n",
      "Average test loss: 0.0016750073386356235\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04600776272018751\n",
      "Average test loss: 0.0017011077375047737\n",
      "Epoch 79/300\n",
      "Average training loss: 0.045914035823610096\n",
      "Average test loss: 0.0016873356520922648\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04541000782781177\n",
      "Average test loss: 0.0016904790265899565\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04532468429538939\n",
      "Average test loss: 0.0017144551930121249\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04476801557342212\n",
      "Average test loss: 0.0017112259260482258\n",
      "Epoch 85/300\n",
      "Average training loss: 0.044470149868064454\n",
      "Average test loss: 0.0016849654714266458\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04448384459482299\n",
      "Average test loss: 0.0017078438121825457\n",
      "Epoch 87/300\n",
      "Average training loss: 0.044106859263446596\n",
      "Average test loss: 0.001789132131677535\n",
      "Epoch 88/300\n",
      "Average training loss: 0.043882974260383185\n",
      "Average test loss: 0.0017768349654765593\n",
      "Epoch 89/300\n",
      "Average training loss: 0.043734664767980574\n",
      "Average test loss: 0.0017394020232475466\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04355025859673818\n",
      "Average test loss: 0.0017135801812012991\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04335756504204538\n",
      "Average test loss: 0.0017452816938360533\n",
      "Epoch 92/300\n",
      "Average training loss: 0.043194552289115055\n",
      "Average test loss: 0.001747011474126743\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04305415357483758\n",
      "Average test loss: 0.0017080723212824927\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04288210493988461\n",
      "Average test loss: 0.001943997808628612\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04226995970474349\n",
      "Average test loss: 0.0017340073805923264\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04214659565024906\n",
      "Average test loss: 0.001815191548731592\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04200307633810573\n",
      "Average test loss: 0.001743939891250597\n",
      "Epoch 100/300\n",
      "Average training loss: 0.041860778278774685\n",
      "Average test loss: 0.0017377294236794114\n",
      "Epoch 101/300\n",
      "Average training loss: 0.041743527813090216\n",
      "Average test loss: 0.001758809333667159\n",
      "Epoch 102/300\n",
      "Average training loss: 0.041599522560834885\n",
      "Average test loss: 0.001771778224243058\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0414412202835083\n",
      "Average test loss: 0.0017628119316779906\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04129129127661387\n",
      "Average test loss: 0.0017818973731353052\n",
      "Epoch 105/300\n",
      "Average training loss: 0.041186696916818616\n",
      "Average test loss: 0.001803843366706537\n",
      "Epoch 106/300\n",
      "Average training loss: 0.040993030968639585\n",
      "Average test loss: 0.0017435228640420568\n",
      "Epoch 107/300\n",
      "Average training loss: 0.040900488959418405\n",
      "Average test loss: 0.0017491328595206142\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04083213253815969\n",
      "Average test loss: 0.0017669675070792438\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04075028839872943\n",
      "Average test loss: 0.0017784410191492902\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0405332864522934\n",
      "Average test loss: 0.0017800350371334288\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04049625311129623\n",
      "Average test loss: 0.001816408592586716\n",
      "Epoch 112/300\n",
      "Average training loss: 0.040379359010193086\n",
      "Average test loss: 0.0017774170520198013\n",
      "Epoch 113/300\n",
      "Average training loss: 0.040199214812782075\n",
      "Average test loss: 0.001762949478191634\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03999684798055225\n",
      "Average test loss: 0.001753248857230776\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0399704606698619\n",
      "Average test loss: 0.0017548939575337701\n",
      "Epoch 116/300\n",
      "Average training loss: 0.039919419798586105\n",
      "Average test loss: 0.0017690327141640914\n",
      "Epoch 117/300\n",
      "Average training loss: 0.039709016657537885\n",
      "Average test loss: 0.0023808416788362796\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03961831967367066\n",
      "Average test loss: 0.0017709182632259196\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03955782482359144\n",
      "Average test loss: 0.0018187055333207052\n",
      "Epoch 120/300\n",
      "Average training loss: 0.039427013536294304\n",
      "Average test loss: 0.0018040877939719292\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03932184847361512\n",
      "Average test loss: 0.0018701342230455743\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03932471729483869\n",
      "Average test loss: 0.0018358319060256083\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03923270521561305\n",
      "Average test loss: 0.0017896391867349545\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03904634815454483\n",
      "Average test loss: 0.001791579709905717\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03892738717794418\n",
      "Average test loss: 0.0017827052399516105\n",
      "Epoch 126/300\n",
      "Average training loss: 0.03883495870563719\n",
      "Average test loss: 0.0018034347568949064\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03883961412641737\n",
      "Average test loss: 0.0017839571227216057\n",
      "Epoch 128/300\n",
      "Average training loss: 0.038750169701046416\n",
      "Average test loss: 0.0018168481246878703\n",
      "Epoch 129/300\n",
      "Average training loss: 0.038699194418059454\n",
      "Average test loss: 0.0018308348660874697\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03857747165693177\n",
      "Average test loss: 0.0018299265145841571\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0386890398396386\n",
      "Average test loss: 0.0018128860871204071\n",
      "Epoch 132/300\n",
      "Average training loss: 0.038379051403866875\n",
      "Average test loss: 0.0017699442882504728\n",
      "Epoch 133/300\n",
      "Average training loss: 0.038299905740552476\n",
      "Average test loss: 0.0018416146894709932\n",
      "Epoch 134/300\n",
      "Average training loss: 0.038258246984746724\n",
      "Average test loss: 0.0017988950187961261\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03816979312565592\n",
      "Average test loss: 0.0018284469697003563\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03815748608443472\n",
      "Average test loss: 0.0018621170901589924\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03802894357509083\n",
      "Average test loss: 0.0018643473469548756\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03798763530453046\n",
      "Average test loss: 0.001831807379093435\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03788406966129939\n",
      "Average test loss: 0.0018058624534557264\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03782281809548537\n",
      "Average test loss: 0.001842301067378786\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03777328543861707\n",
      "Average test loss: 0.0017945241594894065\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0377089679290851\n",
      "Average test loss: 0.001946074582843317\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037623403641912675\n",
      "Average test loss: 0.0018052612981862492\n",
      "Epoch 144/300\n",
      "Average training loss: 0.037629955255322985\n",
      "Average test loss: 0.0017812585724103782\n",
      "Epoch 145/300\n",
      "Average training loss: 0.037602174782090714\n",
      "Average test loss: 0.0018090047095384862\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03753868282834689\n",
      "Average test loss: 0.0018159501457379924\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03736422414580981\n",
      "Average test loss: 0.0017967106438138418\n",
      "Epoch 148/300\n",
      "Average training loss: 0.037316070682472655\n",
      "Average test loss: 0.0018268976679278745\n",
      "Epoch 149/300\n",
      "Average training loss: 0.037220501396391124\n",
      "Average test loss: 0.0018503031285686626\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03734613602028953\n",
      "Average test loss: 0.0017772305504315429\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03717764442496829\n",
      "Average test loss: 0.0018379306204410063\n",
      "Epoch 152/300\n",
      "Average training loss: 0.037064229435390894\n",
      "Average test loss: 0.0017996994871646167\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03709797453549173\n",
      "Average test loss: 0.0018432677112933662\n",
      "Epoch 154/300\n",
      "Average training loss: 0.037052733873327574\n",
      "Average test loss: 0.0018220181880104873\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03686786921819051\n",
      "Average test loss: 0.0018706191635380189\n",
      "Epoch 156/300\n",
      "Average training loss: 0.036930680645836725\n",
      "Average test loss: 0.0018073284859872526\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03682206226554182\n",
      "Average test loss: 0.0018381449480851491\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03687682478129864\n",
      "Average test loss: 0.0018465721596860223\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03672589683698283\n",
      "Average test loss: 0.0018868675453381405\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036799397584464814\n",
      "Average test loss: 0.0025416565040747323\n",
      "Epoch 161/300\n",
      "Average training loss: 0.036553897400697075\n",
      "Average test loss: 0.0018230265009527406\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03653238977657424\n",
      "Average test loss: 0.0018312500116104882\n",
      "Epoch 163/300\n",
      "Average training loss: 0.03655826314290365\n",
      "Average test loss: 0.0018078195703112416\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03648136293556955\n",
      "Average test loss: 0.0019309572488483456\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03645204515920745\n",
      "Average test loss: 0.0018166052932954498\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03636177289485931\n",
      "Average test loss: 0.0018519994965237048\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03634577275150352\n",
      "Average test loss: 0.0018441613702517416\n",
      "Epoch 168/300\n",
      "Average training loss: 0.036333830611573324\n",
      "Average test loss: 0.001826725306403306\n",
      "Epoch 169/300\n",
      "Average training loss: 0.036385857441359096\n",
      "Average test loss: 0.0018470712807029486\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03628341824478573\n",
      "Average test loss: 0.0018194431772248613\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03609657239582804\n",
      "Average test loss: 0.001869952446160217\n",
      "Epoch 172/300\n",
      "Average training loss: 0.03606325199537807\n",
      "Average test loss: 0.0018633909400345551\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03611061781644821\n",
      "Average test loss: 0.0018024261398758325\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03602654964062903\n",
      "Average test loss: 0.0018517851320405802\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03604931080341339\n",
      "Average test loss: 0.0018498275275859568\n",
      "Epoch 176/300\n",
      "Average training loss: 0.035906406846311356\n",
      "Average test loss: 0.0018550711460411549\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03600426288114654\n",
      "Average test loss: 0.0019142113375580972\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0358824359940158\n",
      "Average test loss: 0.0018329123775992129\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0358147990355889\n",
      "Average test loss: 0.0018604290876537562\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03582072802715831\n",
      "Average test loss: 0.0018163760006427764\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03576236147681872\n",
      "Average test loss: 0.0018191892254269784\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03571704778737492\n",
      "Average test loss: 0.0018439167191584905\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03566629327668084\n",
      "Average test loss: 0.0023443617363356883\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03564679174290763\n",
      "Average test loss: 0.0018561708424240351\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03559706348843045\n",
      "Average test loss: 0.0018865373289833466\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03560836911201477\n",
      "Average test loss: 0.0018801550831024846\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03560917512906922\n",
      "Average test loss: 0.0018621370362945729\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03542430268228054\n",
      "Average test loss: 0.0018729562394114005\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03541538227432304\n",
      "Average test loss: 0.001916970145371225\n",
      "Epoch 190/300\n",
      "Average training loss: 0.035428907540109425\n",
      "Average test loss: 0.0019074401738536026\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03536231960521804\n",
      "Average test loss: 0.0018384198144906097\n",
      "Epoch 192/300\n",
      "Average training loss: 0.035323384877708225\n",
      "Average test loss: 0.001997923186255826\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03540182731548945\n",
      "Average test loss: 0.0018840702122284306\n",
      "Epoch 194/300\n",
      "Average training loss: 0.035293593204683726\n",
      "Average test loss: 0.001911765091949039\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03519553508526749\n",
      "Average test loss: 0.0018701725308265952\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0352525103588899\n",
      "Average test loss: 0.0018748806522538265\n",
      "Epoch 197/300\n",
      "Average training loss: 0.035207465685076184\n",
      "Average test loss: 0.0018373161332888735\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03516293378174305\n",
      "Average test loss: 0.0018589809470706516\n",
      "Epoch 199/300\n",
      "Average training loss: 0.035191716220643786\n",
      "Average test loss: 0.0018859394101632967\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03517133300834232\n",
      "Average test loss: 0.0018817268065694306\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03505546730094486\n",
      "Average test loss: 0.0018892197311959334\n",
      "Epoch 202/300\n",
      "Average training loss: 0.035117140339480504\n",
      "Average test loss: 0.0021396825576925444\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03500030486616824\n",
      "Average test loss: 0.0018776791227153606\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03490171498391363\n",
      "Average test loss: 0.0018698266975374685\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0349845387985309\n",
      "Average test loss: 0.0018739837645035651\n",
      "Epoch 206/300\n",
      "Average training loss: 0.034961296700769\n",
      "Average test loss: 0.0019195618029269907\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03489100507232878\n",
      "Average test loss: 0.00186342916249608\n",
      "Epoch 208/300\n",
      "Average training loss: 0.034867355750666726\n",
      "Average test loss: 0.0018382190213435225\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03485916627777947\n",
      "Average test loss: 0.0018310993812564347\n",
      "Epoch 210/300\n",
      "Average training loss: 0.034802232537832524\n",
      "Average test loss: 0.0019373220081130664\n",
      "Epoch 211/300\n",
      "Average training loss: 0.034753907832834455\n",
      "Average test loss: 0.0018607447719615367\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0347369770589802\n",
      "Average test loss: 0.001898841617939373\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03470753816101286\n",
      "Average test loss: 0.002439538326114416\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03461655169725418\n",
      "Average test loss: 0.0018423174400296477\n",
      "Epoch 215/300\n",
      "Average training loss: 0.034603710217608344\n",
      "Average test loss: 0.0018540105920078025\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03464816201064322\n",
      "Average test loss: 0.0018649692952425944\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03464342611696985\n",
      "Average test loss: 0.0018721334028782117\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03455919000506401\n",
      "Average test loss: 0.001884561515516705\n",
      "Epoch 219/300\n",
      "Average training loss: 0.034496519570549326\n",
      "Average test loss: 0.0018669486600491735\n",
      "Epoch 220/300\n",
      "Average training loss: 0.034542621581090824\n",
      "Average test loss: 0.0019931123871356248\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03451644547118081\n",
      "Average test loss: 0.0018784591749103532\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03446992462873459\n",
      "Average test loss: 0.0019146580584347248\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03442930718925264\n",
      "Average test loss: 0.0019329335087289413\n",
      "Epoch 224/300\n",
      "Average training loss: 0.034410239242845114\n",
      "Average test loss: 0.0018914759358805086\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03436337569024828\n",
      "Average test loss: 0.001883188704235686\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03434073093202379\n",
      "Average test loss: 0.0022645044695172047\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0344189703365167\n",
      "Average test loss: 0.0018668285741781194\n",
      "Epoch 228/300\n",
      "Average training loss: 0.034335979448424446\n",
      "Average test loss: 0.0018566123351661694\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03435631611612108\n",
      "Average test loss: 0.0018452288415282965\n",
      "Epoch 230/300\n",
      "Average training loss: 0.034282788323031534\n",
      "Average test loss: 0.001873106642677966\n",
      "Epoch 231/300\n",
      "Average training loss: 0.034184173494577405\n",
      "Average test loss: 0.0018638243502419856\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03420750485360623\n",
      "Average test loss: 0.0018905392991792824\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03415354566441642\n",
      "Average test loss: 0.0018777540297143988\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03416633473171128\n",
      "Average test loss: 0.0018968063135527903\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03420683455136087\n",
      "Average test loss: 0.0018933777320716116\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03418190339869923\n",
      "Average test loss: 0.0018991496697482136\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03408286888069577\n",
      "Average test loss: 0.0018452414898201823\n",
      "Epoch 238/300\n",
      "Average training loss: 0.034090645025173824\n",
      "Average test loss: 0.0019167057317164208\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03405859039392736\n",
      "Average test loss: 0.0019342197999358177\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03400109941429562\n",
      "Average test loss: 0.0018691397110621134\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03396938359737396\n",
      "Average test loss: 0.0019439407914049096\n",
      "Epoch 242/300\n",
      "Average training loss: 0.033948170766234395\n",
      "Average test loss: 0.0018731945277088219\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03402712799774276\n",
      "Average test loss: 0.0019388583099676504\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03388745818369918\n",
      "Average test loss: 0.0019125643172818753\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03386961552831862\n",
      "Average test loss: 0.0018627312870489227\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03391034775310092\n",
      "Average test loss: 0.002307263035948078\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03396534759137366\n",
      "Average test loss: 0.0018799258468465672\n",
      "Epoch 248/300\n",
      "Average training loss: 0.033848138471444446\n",
      "Average test loss: 0.0019371395392550363\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03377010520961549\n",
      "Average test loss: 0.0019032530794954963\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03381794384453032\n",
      "Average test loss: 0.0019422042996933063\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03376296105318599\n",
      "Average test loss: 0.001905314087867737\n",
      "Epoch 252/300\n",
      "Average training loss: 0.033755910307168964\n",
      "Average test loss: 0.0018616535804337926\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03375932003392114\n",
      "Average test loss: 0.0019168023311843475\n",
      "Epoch 254/300\n",
      "Average training loss: 0.033748324755165315\n",
      "Average test loss: 0.0019138352918542094\n",
      "Epoch 255/300\n",
      "Average training loss: 0.033660596625672445\n",
      "Average test loss: 0.0018993092331414422\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03370823096897867\n",
      "Average test loss: 0.001948237762476007\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03367749869823456\n",
      "Average test loss: 0.0019225407715679872\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03363959434794055\n",
      "Average test loss: 0.0018971781227737664\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03363271774186028\n",
      "Average test loss: 0.0018910846207290887\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03364812718828519\n",
      "Average test loss: 0.0019049654577134383\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03355960228376918\n",
      "Average test loss: 0.0020654327432728476\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03350073863400353\n",
      "Average test loss: 0.0018816409616006745\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03349180634154214\n",
      "Average test loss: 0.0019438894414860342\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03348859344919523\n",
      "Average test loss: 0.0019014042903048296\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03352923247052564\n",
      "Average test loss: 0.0018644772599347764\n",
      "Epoch 268/300\n",
      "Average training loss: 0.033452011396487555\n",
      "Average test loss: 0.0018817951337744792\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03342725189195739\n",
      "Average test loss: 0.001924399540759623\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0334826439652178\n",
      "Average test loss: 0.0021249538564847574\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03337166422605514\n",
      "Average test loss: 0.0018954213007042805\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03333618242210812\n",
      "Average test loss: 0.0018956565644799007\n",
      "Epoch 273/300\n",
      "Average training loss: 0.033447621961434686\n",
      "Average test loss: 0.002002098506006102\n",
      "Epoch 274/300\n",
      "Average training loss: 0.033398526198334166\n",
      "Average test loss: 0.001897702434617612\n",
      "Epoch 275/300\n",
      "Average training loss: 0.033324454251262875\n",
      "Average test loss: 0.0020767425287307965\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03323855420947075\n",
      "Average test loss: 0.0019265555076094136\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03331836414337158\n",
      "Average test loss: 0.001859112263760633\n",
      "Epoch 278/300\n",
      "Average training loss: 0.033350652605295183\n",
      "Average test loss: 0.0018756201198945443\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03329717747039265\n",
      "Average test loss: 0.0018460384722178182\n",
      "Epoch 280/300\n",
      "Average training loss: 0.033329852806197276\n",
      "Average test loss: 0.001925603016383118\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03318325736456447\n",
      "Average test loss: 0.0019640555342452393\n",
      "Epoch 282/300\n",
      "Average training loss: 0.033144941606455376\n",
      "Average test loss: 0.002211373987918099\n",
      "Epoch 283/300\n",
      "Average training loss: 0.033244468682342106\n",
      "Average test loss: 0.0018924920682070983\n",
      "Epoch 284/300\n",
      "Average training loss: 0.033145220514800816\n",
      "Average test loss: 0.0018945025118688743\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0331876752310329\n",
      "Average test loss: 0.001914631840876407\n",
      "Epoch 286/300\n",
      "Average training loss: 0.033177703112363816\n",
      "Average test loss: 0.0018855829699378874\n",
      "Epoch 287/300\n",
      "Average training loss: 0.033093825750880775\n",
      "Average test loss: 0.0018935252910272943\n",
      "Epoch 288/300\n",
      "Average training loss: 0.033083715054723954\n",
      "Average test loss: 0.001901053372770548\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03306510758399964\n",
      "Average test loss: 0.0019231522534456519\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03308855466710196\n",
      "Average test loss: 0.0018940806104284194\n",
      "Epoch 291/300\n",
      "Average training loss: 0.033158946827054024\n",
      "Average test loss: 0.0019452898419565624\n",
      "Epoch 292/300\n",
      "Average training loss: 0.033003478427728015\n",
      "Average test loss: 0.0019131284546520975\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03296318657696247\n",
      "Average test loss: 0.0019311731548772918\n",
      "Epoch 294/300\n",
      "Average training loss: 0.033094451526800794\n",
      "Average test loss: 0.0019530076620479424\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03300587482584847\n",
      "Average test loss: 0.001996237895037565\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03299778176347415\n",
      "Average test loss: 0.001862499633597003\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03291119948691792\n",
      "Average test loss: 0.0019078221703983016\n",
      "Epoch 298/300\n",
      "Average training loss: 0.032936005994677546\n",
      "Average test loss: 0.0019887435133051543\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03292593081792196\n",
      "Average test loss: 0.0019445443223747943\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03296438682410452\n",
      "Average test loss: 0.0018663963191211223\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive-.025/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.08\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.48\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.49\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.55\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.56\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.74\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.79\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.80\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.61\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.23\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.42\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.62\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.95\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.00\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.05\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.03\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.66\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.62\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.82\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.85\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.86\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.04\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.16\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.24\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.31\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.30\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.59\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.18\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.12\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.70\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.73\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.78\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.77\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.79\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.92\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.95\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.13\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.23\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.26\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.32\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.33\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
