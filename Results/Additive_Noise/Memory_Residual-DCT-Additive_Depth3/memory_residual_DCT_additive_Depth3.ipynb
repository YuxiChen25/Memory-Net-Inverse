{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.05)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.05)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.05)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.05)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.061238383756743536\n",
      "Average test loss: 0.005278842308041122\n",
      "Epoch 2/300\n",
      "Average training loss: 0.0247159014062749\n",
      "Average test loss: 0.004841528691765335\n",
      "Epoch 3/300\n",
      "Average training loss: 0.023666922903723187\n",
      "Average test loss: 0.00468388780247834\n",
      "Epoch 4/300\n",
      "Average training loss: 0.023207552442948025\n",
      "Average test loss: 0.004686050111841825\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02294374974237548\n",
      "Average test loss: 0.004708688240912226\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02274118476609389\n",
      "Average test loss: 0.00456055819367369\n",
      "Epoch 7/300\n",
      "Average training loss: 0.022592178504500123\n",
      "Average test loss: 0.0045376916064156425\n",
      "Epoch 8/300\n",
      "Average training loss: 0.022462713504830995\n",
      "Average test loss: 0.00454188402576579\n",
      "Epoch 9/300\n",
      "Average training loss: 0.022351375334792666\n",
      "Average test loss: 0.0044975822220246\n",
      "Epoch 10/300\n",
      "Average training loss: 0.022254821707804996\n",
      "Average test loss: 0.004504474656449424\n",
      "Epoch 11/300\n",
      "Average training loss: 0.022181051421496602\n",
      "Average test loss: 0.004466903338001834\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02210048081477483\n",
      "Average test loss: 0.004438401605519984\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02204887391626835\n",
      "Average test loss: 0.004426521262360944\n",
      "Epoch 14/300\n",
      "Average training loss: 0.021981022314892875\n",
      "Average test loss: 0.004431839136199819\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021931015458371905\n",
      "Average test loss: 0.004413102536979649\n",
      "Epoch 16/300\n",
      "Average training loss: 0.021881273044480218\n",
      "Average test loss: 0.00438204530833496\n",
      "Epoch 17/300\n",
      "Average training loss: 0.021819368567731645\n",
      "Average test loss: 0.004374487313131491\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02178461485935582\n",
      "Average test loss: 0.0043574160983165106\n",
      "Epoch 19/300\n",
      "Average training loss: 0.021715460057059925\n",
      "Average test loss: 0.004386029758387142\n",
      "Epoch 20/300\n",
      "Average training loss: 0.021682401287886833\n",
      "Average test loss: 0.004363073766438497\n",
      "Epoch 21/300\n",
      "Average training loss: 0.021649856853816245\n",
      "Average test loss: 0.00436028683020009\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021611953167451754\n",
      "Average test loss: 0.004338080580656727\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02156192005177339\n",
      "Average test loss: 0.004338567132751147\n",
      "Epoch 24/300\n",
      "Average training loss: 0.02154128921031952\n",
      "Average test loss: 0.004374482870929771\n",
      "Epoch 25/300\n",
      "Average training loss: 0.021515301355885134\n",
      "Average test loss: 0.004310142756336265\n",
      "Epoch 26/300\n",
      "Average training loss: 0.021483712527487014\n",
      "Average test loss: 0.004306154087599781\n",
      "Epoch 27/300\n",
      "Average training loss: 0.021455088434947863\n",
      "Average test loss: 0.004302650037325091\n",
      "Epoch 28/300\n",
      "Average training loss: 0.021419061173995335\n",
      "Average test loss: 0.0042976686735120085\n",
      "Epoch 29/300\n",
      "Average training loss: 0.021394085549645954\n",
      "Average test loss: 0.00429045176713003\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02137581825421916\n",
      "Average test loss: 0.004366111926320526\n",
      "Epoch 31/300\n",
      "Average training loss: 0.02135255789756775\n",
      "Average test loss: 0.004292649702686403\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02132651455203692\n",
      "Average test loss: 0.004280939171711603\n",
      "Epoch 33/300\n",
      "Average training loss: 0.021314639599786866\n",
      "Average test loss: 0.004279708891279168\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0212797055542469\n",
      "Average test loss: 0.0042799880103104645\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0212685126695368\n",
      "Average test loss: 0.004281352036115196\n",
      "Epoch 36/300\n",
      "Average training loss: 0.02124525045355161\n",
      "Average test loss: 0.004273113646854957\n",
      "Epoch 37/300\n",
      "Average training loss: 0.021250020841757457\n",
      "Average test loss: 0.004287580299915539\n",
      "Epoch 38/300\n",
      "Average training loss: 0.02122314834760295\n",
      "Average test loss: 0.004265286383529504\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02119940652118789\n",
      "Average test loss: 0.004296936602642139\n",
      "Epoch 40/300\n",
      "Average training loss: 0.021179024731119475\n",
      "Average test loss: 0.004254273305336635\n",
      "Epoch 41/300\n",
      "Average training loss: 0.021171151475773916\n",
      "Average test loss: 0.004268969328039223\n",
      "Epoch 42/300\n",
      "Average training loss: 0.021156992519895234\n",
      "Average test loss: 0.004254021730687883\n",
      "Epoch 43/300\n",
      "Average training loss: 0.02114601491888364\n",
      "Average test loss: 0.004245293351925082\n",
      "Epoch 44/300\n",
      "Average training loss: 0.021130160247286162\n",
      "Average test loss: 0.004247910334418218\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02112552668319808\n",
      "Average test loss: 0.004252871736470196\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02110465285844273\n",
      "Average test loss: 0.004239061263700326\n",
      "Epoch 47/300\n",
      "Average training loss: 0.021094954210850926\n",
      "Average test loss: 0.004253290304086274\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02108033810224798\n",
      "Average test loss: 0.004236453408168422\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02106460962196191\n",
      "Average test loss: 0.004236107719855176\n",
      "Epoch 50/300\n",
      "Average training loss: 0.021051276033951176\n",
      "Average test loss: 0.0042411643113527036\n",
      "Epoch 51/300\n",
      "Average training loss: 0.021049345586034985\n",
      "Average test loss: 0.00423485293197963\n",
      "Epoch 52/300\n",
      "Average training loss: 0.021040879575742615\n",
      "Average test loss: 0.0042332491667734255\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021026527151465416\n",
      "Average test loss: 0.004234271131041977\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0210094609591696\n",
      "Average test loss: 0.0042268698372774655\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021004372013939752\n",
      "Average test loss: 0.004235396401749717\n",
      "Epoch 56/300\n",
      "Average training loss: 0.020992933217022156\n",
      "Average test loss: 0.004235185697053869\n",
      "Epoch 57/300\n",
      "Average training loss: 0.020983620569109917\n",
      "Average test loss: 0.0042369341223190225\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020977724047170746\n",
      "Average test loss: 0.0042288784984913135\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02096630403233899\n",
      "Average test loss: 0.004225910959144433\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020957351335220867\n",
      "Average test loss: 0.004230476008107265\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02094757777121332\n",
      "Average test loss: 0.004226795491452018\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020931880306866435\n",
      "Average test loss: 0.004230322884188758\n",
      "Epoch 63/300\n",
      "Average training loss: 0.020923076930973266\n",
      "Average test loss: 0.004220244387785593\n",
      "Epoch 64/300\n",
      "Average training loss: 0.020921850227647358\n",
      "Average test loss: 0.004259381866703431\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02091168943875366\n",
      "Average test loss: 0.004227236813969082\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02090359292096562\n",
      "Average test loss: 0.004233319184018506\n",
      "Epoch 67/300\n",
      "Average training loss: 0.020891161481539407\n",
      "Average test loss: 0.004217137184407976\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020884022762378055\n",
      "Average test loss: 0.0042169782668352125\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02088294899960359\n",
      "Average test loss: 0.004241902898790108\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020873857044511373\n",
      "Average test loss: 0.004216482654213905\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020860516329606373\n",
      "Average test loss: 0.004243256961305936\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020850775602791045\n",
      "Average test loss: 0.004217985549734698\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020847441176573434\n",
      "Average test loss: 0.004210975144472387\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020841250595119265\n",
      "Average test loss: 0.004211606635194686\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020832338384456105\n",
      "Average test loss: 0.004211076325012578\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020825680666499667\n",
      "Average test loss: 0.004220816580992606\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020815506584114497\n",
      "Average test loss: 0.004214159895769424\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020806901277767288\n",
      "Average test loss: 0.004213761040113039\n",
      "Epoch 79/300\n",
      "Average training loss: 0.020794888350698683\n",
      "Average test loss: 0.004215947386291292\n",
      "Epoch 80/300\n",
      "Average training loss: 0.020791623244682946\n",
      "Average test loss: 0.004214423204461733\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0207851755734947\n",
      "Average test loss: 0.0042168476881666314\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020788796835475496\n",
      "Average test loss: 0.0042099079866376185\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020774694964289665\n",
      "Average test loss: 0.0042094039399590754\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020769549335042634\n",
      "Average test loss: 0.004220931828642885\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020756981280114915\n",
      "Average test loss: 0.004208028613072303\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02075274683535099\n",
      "Average test loss: 0.004212335312945975\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0207454677571853\n",
      "Average test loss: 0.004211103539293011\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020741967893309062\n",
      "Average test loss: 0.004213909829242362\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02074013988673687\n",
      "Average test loss: 0.004218906370715963\n",
      "Epoch 90/300\n",
      "Average training loss: 0.02072711457643244\n",
      "Average test loss: 0.004211003943449921\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020713146654268107\n",
      "Average test loss: 0.004213595392803351\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02071692572037379\n",
      "Average test loss: 0.004223161199440559\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02070870294753048\n",
      "Average test loss: 0.004210966743942764\n",
      "Epoch 94/300\n",
      "Average training loss: 0.020703952913482985\n",
      "Average test loss: 0.004242503147572279\n",
      "Epoch 95/300\n",
      "Average training loss: 0.020697711376680267\n",
      "Average test loss: 0.0042099944021966726\n",
      "Epoch 96/300\n",
      "Average training loss: 0.020685643316970932\n",
      "Average test loss: 0.004220080952677461\n",
      "Epoch 97/300\n",
      "Average training loss: 0.020684591982099744\n",
      "Average test loss: 0.004219117485400703\n",
      "Epoch 98/300\n",
      "Average training loss: 0.020682501451836692\n",
      "Average test loss: 0.004228305927788218\n",
      "Epoch 99/300\n",
      "Average training loss: 0.02067183882329199\n",
      "Average test loss: 0.0042206964625252615\n",
      "Epoch 100/300\n",
      "Average training loss: 0.020664809005128012\n",
      "Average test loss: 0.004212170828340782\n",
      "Epoch 101/300\n",
      "Average training loss: 0.020656058078838718\n",
      "Average test loss: 0.0042122116581433345\n",
      "Epoch 102/300\n",
      "Average training loss: 0.020641191702749993\n",
      "Average test loss: 0.004211591013189819\n",
      "Epoch 103/300\n",
      "Average training loss: 0.020643460921115346\n",
      "Average test loss: 0.004233280577593379\n",
      "Epoch 104/300\n",
      "Average training loss: 0.020637030055125554\n",
      "Average test loss: 0.004210590930034717\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02063030295405123\n",
      "Average test loss: 0.004206786507947578\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02062326648251878\n",
      "Average test loss: 0.0042155874001069205\n",
      "Epoch 107/300\n",
      "Average training loss: 0.020617070911659135\n",
      "Average test loss: 0.004239914259148969\n",
      "Epoch 108/300\n",
      "Average training loss: 0.020609560016128754\n",
      "Average test loss: 0.004240047673591309\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02060590100536744\n",
      "Average test loss: 0.004215411044864191\n",
      "Epoch 110/300\n",
      "Average training loss: 0.020600435074832703\n",
      "Average test loss: 0.00420562261218826\n",
      "Epoch 111/300\n",
      "Average training loss: 0.020597564508517584\n",
      "Average test loss: 0.00423247980285022\n",
      "Epoch 112/300\n",
      "Average training loss: 0.020587557799286314\n",
      "Average test loss: 0.00420208693564766\n",
      "Epoch 113/300\n",
      "Average training loss: 0.02058270617822806\n",
      "Average test loss: 0.004234561093565491\n",
      "Epoch 114/300\n",
      "Average training loss: 0.020572358217504288\n",
      "Average test loss: 0.004242685895413161\n",
      "Epoch 115/300\n",
      "Average training loss: 0.020567729444967375\n",
      "Average test loss: 0.004244815449333853\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02057272699971994\n",
      "Average test loss: 0.0042211217164165446\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02056175917552577\n",
      "Average test loss: 0.004211287320488029\n",
      "Epoch 118/300\n",
      "Average training loss: 0.020557030429442725\n",
      "Average test loss: 0.004239492810434765\n",
      "Epoch 119/300\n",
      "Average training loss: 0.020547261340750588\n",
      "Average test loss: 0.004211124976269073\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02054563405778673\n",
      "Average test loss: 0.004207940867377652\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02053957260813978\n",
      "Average test loss: 0.004213582329865959\n",
      "Epoch 122/300\n",
      "Average training loss: 0.020529052153229713\n",
      "Average test loss: 0.0042847367852098415\n",
      "Epoch 123/300\n",
      "Average training loss: 0.02052186439765824\n",
      "Average test loss: 0.004230461655391587\n",
      "Epoch 124/300\n",
      "Average training loss: 0.02052092685798804\n",
      "Average test loss: 0.004268439210744368\n",
      "Epoch 125/300\n",
      "Average training loss: 0.02051484908329116\n",
      "Average test loss: 0.004244445333878199\n",
      "Epoch 126/300\n",
      "Average training loss: 0.020511341904600463\n",
      "Average test loss: 0.004224417174441947\n",
      "Epoch 127/300\n",
      "Average training loss: 0.020499624374012152\n",
      "Average test loss: 0.0042115809275872175\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02049820996324221\n",
      "Average test loss: 0.004216951952626308\n",
      "Epoch 129/300\n",
      "Average training loss: 0.020487436780499087\n",
      "Average test loss: 0.004252305611140198\n",
      "Epoch 130/300\n",
      "Average training loss: 0.020488215362032255\n",
      "Average test loss: 0.00421799321886566\n",
      "Epoch 131/300\n",
      "Average training loss: 0.020476360802849133\n",
      "Average test loss: 0.00420097501989868\n",
      "Epoch 132/300\n",
      "Average training loss: 0.020473861537045904\n",
      "Average test loss: 0.004212616392307811\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02046442363328404\n",
      "Average test loss: 0.004214214107642571\n",
      "Epoch 134/300\n",
      "Average training loss: 0.020462152947982152\n",
      "Average test loss: 0.004219026396671931\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02046293464054664\n",
      "Average test loss: 0.0042450028401282095\n",
      "Epoch 136/300\n",
      "Average training loss: 0.020454556667142444\n",
      "Average test loss: 0.004217993021011352\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02044416535562939\n",
      "Average test loss: 0.004205899092886183\n",
      "Epoch 138/300\n",
      "Average training loss: 0.020442190210024514\n",
      "Average test loss: 0.004237251629639003\n",
      "Epoch 139/300\n",
      "Average training loss: 0.020436334568593235\n",
      "Average test loss: 0.004242376210374965\n",
      "Epoch 140/300\n",
      "Average training loss: 0.020430515235496893\n",
      "Average test loss: 0.004241719187340802\n",
      "Epoch 141/300\n",
      "Average training loss: 0.020423811251918476\n",
      "Average test loss: 0.004224813858668009\n",
      "Epoch 142/300\n",
      "Average training loss: 0.020421103419529067\n",
      "Average test loss: 0.004240016096168094\n",
      "Epoch 143/300\n",
      "Average training loss: 0.02041069891386562\n",
      "Average test loss: 0.004232242901499073\n",
      "Epoch 144/300\n",
      "Average training loss: 0.020410552647378708\n",
      "Average test loss: 0.004217450100928545\n",
      "Epoch 145/300\n",
      "Average training loss: 0.020396670285198423\n",
      "Average test loss: 0.004233924579289224\n",
      "Epoch 146/300\n",
      "Average training loss: 0.020408551280697188\n",
      "Average test loss: 0.004214281378106938\n",
      "Epoch 147/300\n",
      "Average training loss: 0.02039321703877714\n",
      "Average test loss: 0.0042199693198005355\n",
      "Epoch 148/300\n",
      "Average training loss: 0.020392912707395024\n",
      "Average test loss: 0.004211396886656681\n",
      "Epoch 149/300\n",
      "Average training loss: 0.020372381708688207\n",
      "Average test loss: 0.004232289302473267\n",
      "Epoch 150/300\n",
      "Average training loss: 0.020371917261017693\n",
      "Average test loss: 0.004227327964992987\n",
      "Epoch 151/300\n",
      "Average training loss: 0.020376683973603777\n",
      "Average test loss: 0.004224626777072748\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02036629047162003\n",
      "Average test loss: 0.004216935060090489\n",
      "Epoch 153/300\n",
      "Average training loss: 0.020351943449841606\n",
      "Average test loss: 0.004232185521473487\n",
      "Epoch 154/300\n",
      "Average training loss: 0.02036415131224526\n",
      "Average test loss: 0.004234482135416733\n",
      "Epoch 155/300\n",
      "Average training loss: 0.020348408960633807\n",
      "Average test loss: 0.004231434616156751\n",
      "Epoch 156/300\n",
      "Average training loss: 0.020343319768706958\n",
      "Average test loss: 0.004218983814120293\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02033887738486131\n",
      "Average test loss: 0.004301104175547759\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02033576889667246\n",
      "Average test loss: 0.004228960008670886\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02033031980196635\n",
      "Average test loss: 0.004237419101099173\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02031731176541911\n",
      "Average test loss: 0.004217995493362347\n",
      "Epoch 161/300\n",
      "Average training loss: 0.020312808139456642\n",
      "Average test loss: 0.0042835226023776665\n",
      "Epoch 162/300\n",
      "Average training loss: 0.020315485977464253\n",
      "Average test loss: 0.004223771994726525\n",
      "Epoch 163/300\n",
      "Average training loss: 0.02030914122114579\n",
      "Average test loss: 0.004248704103132089\n",
      "Epoch 164/300\n",
      "Average training loss: 0.020302045901616415\n",
      "Average test loss: 0.004220836179537906\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02029296720855766\n",
      "Average test loss: 0.004682657132132186\n",
      "Epoch 166/300\n",
      "Average training loss: 0.02028686711523268\n",
      "Average test loss: 0.004227021779658066\n",
      "Epoch 167/300\n",
      "Average training loss: 0.020287147699130904\n",
      "Average test loss: 0.004236057622565163\n",
      "Epoch 168/300\n",
      "Average training loss: 0.020275989358623823\n",
      "Average test loss: 0.004218456567575534\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0202719755868117\n",
      "Average test loss: 0.004222605479467246\n",
      "Epoch 170/300\n",
      "Average training loss: 0.020262223389413622\n",
      "Average test loss: 0.004230776639448272\n",
      "Epoch 171/300\n",
      "Average training loss: 0.020262507680389615\n",
      "Average test loss: 0.004307871310247315\n",
      "Epoch 172/300\n",
      "Average training loss: 0.020258373805218274\n",
      "Average test loss: 0.004243290621373389\n",
      "Epoch 173/300\n",
      "Average training loss: 0.020254498498307336\n",
      "Average test loss: 0.004230290339224868\n",
      "Epoch 174/300\n",
      "Average training loss: 0.020248925808403227\n",
      "Average test loss: 0.00424277142352528\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02024218901164002\n",
      "Average test loss: 0.00422317284428411\n",
      "Epoch 176/300\n",
      "Average training loss: 0.020238066176573435\n",
      "Average test loss: 0.004239836965584093\n",
      "Epoch 177/300\n",
      "Average training loss: 0.020230064461628595\n",
      "Average test loss: 0.004251586129681932\n",
      "Epoch 178/300\n",
      "Average training loss: 0.020229556164807743\n",
      "Average test loss: 0.0042372408852809\n",
      "Epoch 179/300\n",
      "Average training loss: 0.020222251092394193\n",
      "Average test loss: 0.004233380696839756\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02021461097399394\n",
      "Average test loss: 0.004253070003456539\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02021206944849756\n",
      "Average test loss: 0.00423658448147277\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02020666114654806\n",
      "Average test loss: 0.004248598754819897\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02020255591803127\n",
      "Average test loss: 0.004269533840318521\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0201951529044244\n",
      "Average test loss: 0.00430752402337061\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0201792873226934\n",
      "Average test loss: 0.0042652492593559955\n",
      "Epoch 186/300\n",
      "Average training loss: 0.020191699392265745\n",
      "Average test loss: 0.0042610290127082\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02017941179871559\n",
      "Average test loss: 0.004216708739598592\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02017420663105117\n",
      "Average test loss: 0.004259080229534043\n",
      "Epoch 189/300\n",
      "Average training loss: 0.020176584119598072\n",
      "Average test loss: 0.004263155070443948\n",
      "Epoch 190/300\n",
      "Average training loss: 0.020167981881234382\n",
      "Average test loss: 0.004249303975452979\n",
      "Epoch 191/300\n",
      "Average training loss: 0.020159194325407345\n",
      "Average test loss: 0.00425547637583481\n",
      "Epoch 192/300\n",
      "Average training loss: 0.020161893899242084\n",
      "Average test loss: 0.004255962290904588\n",
      "Epoch 193/300\n",
      "Average training loss: 0.0201490064031548\n",
      "Average test loss: 0.004276441366308266\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02014327616824044\n",
      "Average test loss: 0.004270360954519775\n",
      "Epoch 195/300\n",
      "Average training loss: 0.020133444688386386\n",
      "Average test loss: 0.004235629007427228\n",
      "Epoch 196/300\n",
      "Average training loss: 0.020139404793580375\n",
      "Average test loss: 0.004263339761851562\n",
      "Epoch 197/300\n",
      "Average training loss: 0.020132820922467445\n",
      "Average test loss: 0.004250186059416996\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02012461729016569\n",
      "Average test loss: 0.004256920530564255\n",
      "Epoch 199/300\n",
      "Average training loss: 0.020122559560669794\n",
      "Average test loss: 0.004242740159233411\n",
      "Epoch 200/300\n",
      "Average training loss: 0.020114848375320436\n",
      "Average test loss: 0.004276961005810234\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02010825647248162\n",
      "Average test loss: 0.004259616321780615\n",
      "Epoch 202/300\n",
      "Average training loss: 0.020112372619410356\n",
      "Average test loss: 0.004284355910287963\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0200999763160944\n",
      "Average test loss: 0.0042604214511811735\n",
      "Epoch 204/300\n",
      "Average training loss: 0.020091176624099413\n",
      "Average test loss: 0.004278000516196092\n",
      "Epoch 205/300\n",
      "Average training loss: 0.020091873097750877\n",
      "Average test loss: 0.00428486529406574\n",
      "Epoch 206/300\n",
      "Average training loss: 0.020088305067684914\n",
      "Average test loss: 0.004259490515622828\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02008177484737502\n",
      "Average test loss: 0.004263179896399379\n",
      "Epoch 208/300\n",
      "Average training loss: 0.020078062255349423\n",
      "Average test loss: 0.00428231502779656\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02006836155553659\n",
      "Average test loss: 0.004329784885048866\n",
      "Epoch 210/300\n",
      "Average training loss: 0.02006228092395597\n",
      "Average test loss: 0.0042651162418640325\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02006561132437653\n",
      "Average test loss: 0.004328140314668417\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0200642412064804\n",
      "Average test loss: 0.004249056627766953\n",
      "Epoch 213/300\n",
      "Average training loss: 0.020053473389810984\n",
      "Average test loss: 0.0042835305444896225\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02005452162027359\n",
      "Average test loss: 0.004271958147899972\n",
      "Epoch 215/300\n",
      "Average training loss: 0.020045765745970938\n",
      "Average test loss: 0.004288027683479918\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02004008983572324\n",
      "Average test loss: 0.0042929974099000295\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02002755173461305\n",
      "Average test loss: 0.004391757515569528\n",
      "Epoch 218/300\n",
      "Average training loss: 0.020030276036924786\n",
      "Average test loss: 0.004321462362176842\n",
      "Epoch 219/300\n",
      "Average training loss: 0.020019168991181585\n",
      "Average test loss: 0.004296719991912445\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0200170476651854\n",
      "Average test loss: 0.004313879539155298\n",
      "Epoch 221/300\n",
      "Average training loss: 0.020013281154963706\n",
      "Average test loss: 0.0042893181898527675\n",
      "Epoch 222/300\n",
      "Average training loss: 0.02001230845683151\n",
      "Average test loss: 0.004306939945866665\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02001242320322328\n",
      "Average test loss: 0.004324353664699528\n",
      "Epoch 224/300\n",
      "Average training loss: 0.019997574258181786\n",
      "Average test loss: 0.0042810025099251\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01999879634877046\n",
      "Average test loss: 0.004284377759115564\n",
      "Epoch 226/300\n",
      "Average training loss: 0.019993818220165042\n",
      "Average test loss: 0.004302249585588773\n",
      "Epoch 227/300\n",
      "Average training loss: 0.019974911351998647\n",
      "Average test loss: 0.004309271381960975\n",
      "Epoch 228/300\n",
      "Average training loss: 0.019985278331571155\n",
      "Average test loss: 0.004261948626695408\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01998328762832615\n",
      "Average test loss: 0.004314689704941379\n",
      "Epoch 230/300\n",
      "Average training loss: 0.019976734139853052\n",
      "Average test loss: 0.004263073533773422\n",
      "Epoch 231/300\n",
      "Average training loss: 0.019968478807144696\n",
      "Average test loss: 0.004261101343565517\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01996894614232911\n",
      "Average test loss: 0.004286314344240559\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01995721279250251\n",
      "Average test loss: 0.004288282935404115\n",
      "Epoch 234/300\n",
      "Average training loss: 0.019949455978141892\n",
      "Average test loss: 0.004285579293552372\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01995181444949574\n",
      "Average test loss: 0.004260917740563552\n",
      "Epoch 236/300\n",
      "Average training loss: 0.019942901154773103\n",
      "Average test loss: 0.004394481648380558\n",
      "Epoch 237/300\n",
      "Average training loss: 0.019933044673668014\n",
      "Average test loss: 0.004273325639466445\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01994387611415651\n",
      "Average test loss: 0.004278262381338411\n",
      "Epoch 239/300\n",
      "Average training loss: 0.019928548456894027\n",
      "Average test loss: 0.004313925926677055\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01993027585413721\n",
      "Average test loss: 0.004308388784113858\n",
      "Epoch 241/300\n",
      "Average training loss: 0.019931746436489952\n",
      "Average test loss: 0.004298414646337429\n",
      "Epoch 242/300\n",
      "Average training loss: 0.019915747770004804\n",
      "Average test loss: 0.0043042019493877886\n",
      "Epoch 243/300\n",
      "Average training loss: 0.019916405126452447\n",
      "Average test loss: 0.0042609761680165925\n",
      "Epoch 244/300\n",
      "Average training loss: 0.019907890102101696\n",
      "Average test loss: 0.004268131751360165\n",
      "Epoch 245/300\n",
      "Average training loss: 0.019907462817099358\n",
      "Average test loss: 0.004337049508674277\n",
      "Epoch 246/300\n",
      "Average training loss: 0.019908969456950823\n",
      "Average test loss: 0.004289828604294194\n",
      "Epoch 247/300\n",
      "Average training loss: 0.019907452270388603\n",
      "Average test loss: 0.004294423804514938\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01990172925757037\n",
      "Average test loss: 0.004291724181009663\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01988870869576931\n",
      "Average test loss: 0.004323345973259873\n",
      "Epoch 250/300\n",
      "Average training loss: 0.019898220625188614\n",
      "Average test loss: 0.004311462624205484\n",
      "Epoch 251/300\n",
      "Average training loss: 0.019887936568922467\n",
      "Average test loss: 0.00430529428128567\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01987804401252005\n",
      "Average test loss: 0.004251518149963684\n",
      "Epoch 253/300\n",
      "Average training loss: 0.019875714078545572\n",
      "Average test loss: 0.004350620718465911\n",
      "Epoch 254/300\n",
      "Average training loss: 0.019878751680254936\n",
      "Average test loss: 0.004272824895464712\n",
      "Epoch 255/300\n",
      "Average training loss: 0.019862726245489386\n",
      "Average test loss: 0.004300179449013538\n",
      "Epoch 256/300\n",
      "Average training loss: 0.019865305599239137\n",
      "Average test loss: 0.004296982436958287\n",
      "Epoch 257/300\n",
      "Average training loss: 0.019856365299059285\n",
      "Average test loss: 0.004312866318556998\n",
      "Epoch 258/300\n",
      "Average training loss: 0.019854801216059263\n",
      "Average test loss: 0.004307223216527038\n",
      "Epoch 259/300\n",
      "Average training loss: 0.019848050392336315\n",
      "Average test loss: 0.004338228069659737\n",
      "Epoch 260/300\n",
      "Average training loss: 0.019846237671044136\n",
      "Average test loss: 0.004321398404012951\n",
      "Epoch 261/300\n",
      "Average training loss: 0.019840017128321858\n",
      "Average test loss: 0.00435482952495416\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01984152283271154\n",
      "Average test loss: 0.004315414922518863\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01983024966220061\n",
      "Average test loss: 0.004375889810836977\n",
      "Epoch 264/300\n",
      "Average training loss: 0.019830362680885526\n",
      "Average test loss: 0.00433295471014248\n",
      "Epoch 265/300\n",
      "Average training loss: 0.019826192097531423\n",
      "Average test loss: 0.004335780875550376\n",
      "Epoch 266/300\n",
      "Average training loss: 0.019820640701386665\n",
      "Average test loss: 0.00465245526118411\n",
      "Epoch 267/300\n",
      "Average training loss: 0.019817274327079457\n",
      "Average test loss: 0.004294540788564417\n",
      "Epoch 268/300\n",
      "Average training loss: 0.019811126006974114\n",
      "Average test loss: 0.004365235390969449\n",
      "Epoch 269/300\n",
      "Average training loss: 0.01980247257153193\n",
      "Average test loss: 0.004299072037554449\n",
      "Epoch 270/300\n",
      "Average training loss: 0.019818570915195677\n",
      "Average test loss: 0.00430328025834428\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01979830243024561\n",
      "Average test loss: 0.004347695282349984\n",
      "Epoch 272/300\n",
      "Average training loss: 0.019792745951149198\n",
      "Average test loss: 0.004309171435733636\n",
      "Epoch 273/300\n",
      "Average training loss: 0.019795566476053662\n",
      "Average test loss: 0.004332083803912004\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01978757289548715\n",
      "Average test loss: 0.004336280285484261\n",
      "Epoch 275/300\n",
      "Average training loss: 0.019788231399324206\n",
      "Average test loss: 0.0043767316606309675\n",
      "Epoch 276/300\n",
      "Average training loss: 0.019781859540277058\n",
      "Average test loss: 0.004321121791584624\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01978064853284094\n",
      "Average test loss: 0.00434369208291173\n",
      "Epoch 278/300\n",
      "Average training loss: 0.019782220658328797\n",
      "Average test loss: 0.004426428201090959\n",
      "Epoch 279/300\n",
      "Average training loss: 0.019778889518645076\n",
      "Average test loss: 0.0043285592682659624\n",
      "Epoch 280/300\n",
      "Average training loss: 0.019775757763120863\n",
      "Average test loss: 0.004333558643443717\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01976560376915667\n",
      "Average test loss: 0.004491807678507434\n",
      "Epoch 282/300\n",
      "Average training loss: 0.019771909922361375\n",
      "Average test loss: 0.004296834991830919\n",
      "Epoch 283/300\n",
      "Average training loss: 0.019764545590513282\n",
      "Average test loss: 0.004351187016814947\n",
      "Epoch 284/300\n",
      "Average training loss: 0.019748851635389857\n",
      "Average test loss: 0.004380660614205731\n",
      "Epoch 285/300\n",
      "Average training loss: 0.019749420719014275\n",
      "Average test loss: 0.004319696677641736\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01974662896576855\n",
      "Average test loss: 0.004346928568349944\n",
      "Epoch 287/300\n",
      "Average training loss: 0.019743244912889268\n",
      "Average test loss: 0.0043429727314247026\n",
      "Epoch 288/300\n",
      "Average training loss: 0.019735542117721504\n",
      "Average test loss: 0.0044243120695981715\n",
      "Epoch 289/300\n",
      "Average training loss: 0.019741780039336947\n",
      "Average test loss: 0.004333835392776463\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01973131070865525\n",
      "Average test loss: 0.0043461785548263125\n",
      "Epoch 291/300\n",
      "Average training loss: 0.019731668204069137\n",
      "Average test loss: 0.00435422741745909\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01972737382683489\n",
      "Average test loss: 0.004359221611171961\n",
      "Epoch 293/300\n",
      "Average training loss: 0.019716628548999627\n",
      "Average test loss: 0.004369628372809125\n",
      "Epoch 294/300\n",
      "Average training loss: 0.019714994569619497\n",
      "Average test loss: 0.004340966570294566\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01971673505173789\n",
      "Average test loss: 0.0043395139310095044\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01971645727256934\n",
      "Average test loss: 0.004300398927181959\n",
      "Epoch 297/300\n",
      "Average training loss: 0.019707055176297825\n",
      "Average test loss: 0.004463036094895668\n",
      "Epoch 298/300\n",
      "Average training loss: 0.019716411429974767\n",
      "Average test loss: 0.004383646412442128\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01969732729262776\n",
      "Average test loss: 0.004458737916416592\n",
      "Epoch 300/300\n",
      "Average training loss: 0.019688949391245842\n",
      "Average test loss: 0.0043452430727581185\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.061316243239574963\n",
      "Average test loss: 0.0047876512224061625\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02281217245923148\n",
      "Average test loss: 0.004439969985021486\n",
      "Epoch 3/300\n",
      "Average training loss: 0.02160832314358817\n",
      "Average test loss: 0.0042377462664412125\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02103136764301194\n",
      "Average test loss: 0.004178559723620613\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02063258741133743\n",
      "Average test loss: 0.004102005903919538\n",
      "Epoch 6/300\n",
      "Average training loss: 0.020329531853397687\n",
      "Average test loss: 0.004053714007553127\n",
      "Epoch 7/300\n",
      "Average training loss: 0.02006363495190938\n",
      "Average test loss: 0.004032924543652269\n",
      "Epoch 8/300\n",
      "Average training loss: 0.019841577058037123\n",
      "Average test loss: 0.0039535250187748006\n",
      "Epoch 9/300\n",
      "Average training loss: 0.019649316350619\n",
      "Average test loss: 0.0039108958610643944\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0194950445741415\n",
      "Average test loss: 0.003924150692092048\n",
      "Epoch 11/300\n",
      "Average training loss: 0.019347265043192438\n",
      "Average test loss: 0.0038718964571340217\n",
      "Epoch 12/300\n",
      "Average training loss: 0.019219503240452872\n",
      "Average test loss: 0.003858871420845389\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01910089300572872\n",
      "Average test loss: 0.0037870362483792834\n",
      "Epoch 14/300\n",
      "Average training loss: 0.019004411233796013\n",
      "Average test loss: 0.0037813615769975717\n",
      "Epoch 15/300\n",
      "Average training loss: 0.018147948612769443\n",
      "Average test loss: 0.0035914541975491576\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01811253231929408\n",
      "Average test loss: 0.003609401133739286\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01805748303234577\n",
      "Average test loss: 0.003599453996659981\n",
      "Epoch 30/300\n",
      "Average training loss: 0.018032905815376175\n",
      "Average test loss: 0.003601648383256462\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01799116197062863\n",
      "Average test loss: 0.003566629217730628\n",
      "Epoch 32/300\n",
      "Average training loss: 0.017961934665838876\n",
      "Average test loss: 0.0035824274838798576\n",
      "Epoch 33/300\n",
      "Average training loss: 0.01793679311540392\n",
      "Average test loss: 0.003553333561660515\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01789556206597222\n",
      "Average test loss: 0.0035491089725659952\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017870570508970153\n",
      "Average test loss: 0.003542471639605032\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01784485671752029\n",
      "Average test loss: 0.003533040445918838\n",
      "Epoch 37/300\n",
      "Average training loss: 0.017822534726725684\n",
      "Average test loss: 0.0035378125303735337\n",
      "Epoch 38/300\n",
      "Average training loss: 0.017788972264362705\n",
      "Average test loss: 0.0035235137660056355\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017770861582623587\n",
      "Average test loss: 0.0035234934650361536\n",
      "Epoch 40/300\n",
      "Average training loss: 0.017747463832298915\n",
      "Average test loss: 0.0035699738665587373\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01773315014441808\n",
      "Average test loss: 0.0035169640524933734\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017704495598872504\n",
      "Average test loss: 0.003518284943782621\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017684024729662472\n",
      "Average test loss: 0.003511377859653698\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01766247804628478\n",
      "Average test loss: 0.0035200803958707386\n",
      "Epoch 45/300\n",
      "Average training loss: 0.017641152900126245\n",
      "Average test loss: 0.00351879191170964\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017629710141983296\n",
      "Average test loss: 0.0034938831557002332\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017609578181472088\n",
      "Average test loss: 0.003509766714233491\n",
      "Epoch 48/300\n",
      "Average training loss: 0.017586134898993706\n",
      "Average test loss: 0.003485106281936169\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017525670210520428\n",
      "Average test loss: 0.0035136381830606195\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017507620756824813\n",
      "Average test loss: 0.003491272318901287\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01749320949945185\n",
      "Average test loss: 0.0035001776054915455\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01748396353258027\n",
      "Average test loss: 0.0034831119204560917\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01746133347352346\n",
      "Average test loss: 0.0034774932054181893\n",
      "Epoch 57/300\n",
      "Average training loss: 0.01744239275240236\n",
      "Average test loss: 0.0035053111443089113\n",
      "Epoch 58/300\n",
      "Average training loss: 0.017432842826677692\n",
      "Average test loss: 0.0035098882739742596\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01742382475319836\n",
      "Average test loss: 0.003470979932902588\n",
      "Epoch 60/300\n",
      "Average training loss: 0.017414589491155413\n",
      "Average test loss: 0.0034937889869842264\n",
      "Epoch 61/300\n",
      "Average training loss: 0.017392353820304077\n",
      "Average test loss: 0.0034936745611743795\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01738522445741627\n",
      "Average test loss: 0.0034757139016356733\n",
      "Epoch 63/300\n",
      "Average training loss: 0.017250291039546332\n",
      "Average test loss: 0.0034584364079766803\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01723520188364718\n",
      "Average test loss: 0.0034547591002451047\n",
      "Epoch 75/300\n",
      "Average training loss: 0.017226203135318228\n",
      "Average test loss: 0.0034693877576953834\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01721869212554561\n",
      "Average test loss: 0.0034906935662859014\n",
      "Epoch 77/300\n",
      "Average training loss: 0.01720696283545759\n",
      "Average test loss: 0.0034601554092433717\n",
      "Epoch 78/300\n",
      "Average training loss: 0.017193896742330655\n",
      "Average test loss: 0.0034760698156638277\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01718326849076483\n",
      "Average test loss: 0.003460170282257928\n",
      "Epoch 80/300\n",
      "Average training loss: 0.017175261442859968\n",
      "Average test loss: 0.003460621681685249\n",
      "Epoch 81/300\n",
      "Average training loss: 0.017161744544075597\n",
      "Average test loss: 0.003463465114020639\n",
      "Epoch 82/300\n",
      "Average training loss: 0.017157029506233004\n",
      "Average test loss: 0.003486644797027111\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01714539443122016\n",
      "Average test loss: 0.0034624348032391734\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01712967140807046\n",
      "Average test loss: 0.0034509736775524087\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01712738593419393\n",
      "Average test loss: 0.003461180516001251\n",
      "Epoch 86/300\n",
      "Average training loss: 0.017115209938751326\n",
      "Average test loss: 0.003449540793688761\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01710629284216298\n",
      "Average test loss: 0.0034651789007087547\n",
      "Epoch 88/300\n",
      "Average training loss: 0.017098876105414496\n",
      "Average test loss: 0.003539938560790486\n",
      "Epoch 89/300\n",
      "Average training loss: 0.017087556813326146\n",
      "Average test loss: 0.003460463707438774\n",
      "Epoch 90/300\n",
      "Average training loss: 0.017076278501086766\n",
      "Average test loss: 0.003456443302333355\n",
      "Epoch 91/300\n",
      "Average training loss: 0.017074820721315012\n",
      "Average test loss: 0.0034592193352679414\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01705234663022889\n",
      "Average test loss: 0.0034760664560728603\n",
      "Epoch 93/300\n",
      "Average training loss: 0.017046553942064443\n",
      "Average test loss: 0.0034804347968763775\n",
      "Epoch 94/300\n",
      "Average training loss: 0.017040127724409104\n",
      "Average test loss: 0.0034674487163623173\n",
      "Epoch 95/300\n",
      "Average training loss: 0.017030613478687076\n",
      "Average test loss: 0.003462548714958959\n",
      "Epoch 96/300\n",
      "Average training loss: 0.017025720478759873\n",
      "Average test loss: 0.0034635500013828276\n",
      "Epoch 97/300\n",
      "Average training loss: 0.017015433760152923\n",
      "Average test loss: 0.003497032000372807\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01699964975482888\n",
      "Average test loss: 0.003463683222937915\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01699384341140588\n",
      "Average test loss: 0.0035542401435474553\n",
      "Epoch 100/300\n",
      "Average training loss: 0.016992460352679095\n",
      "Average test loss: 0.0034727474914656746\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01698566567608052\n",
      "Average test loss: 0.003499113627192047\n",
      "Epoch 102/300\n",
      "Average training loss: 0.016969231020245286\n",
      "Average test loss: 0.003459291935380962\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016965974966684977\n",
      "Average test loss: 0.0034649315528157685\n",
      "Epoch 104/300\n",
      "Average training loss: 0.016952803804642626\n",
      "Average test loss: 0.003572729834665855\n",
      "Epoch 105/300\n",
      "Average training loss: 0.016956004089779323\n",
      "Average test loss: 0.0034765260016752612\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01694835530469815\n",
      "Average test loss: 0.003480551282564799\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016929467075400884\n",
      "Average test loss: 0.0034776055450654693\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016912778610984485\n",
      "Average test loss: 0.0034897439498454332\n",
      "Epoch 109/300\n",
      "Average training loss: 0.016914401383863555\n",
      "Average test loss: 0.003488489012958275\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016907061849203375\n",
      "Average test loss: 0.0035210281875398424\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01689968482322163\n",
      "Average test loss: 0.0034676882839865153\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01688541227910254\n",
      "Average test loss: 0.003476899131304688\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01688064441581567\n",
      "Average test loss: 0.0034658960921482907\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016875796084602674\n",
      "Average test loss: 0.0034949216283857824\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016875751026802593\n",
      "Average test loss: 0.003484547880788644\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01685444221480025\n",
      "Average test loss: 0.0034738231425484024\n",
      "Epoch 117/300\n",
      "Average training loss: 0.016850030893252956\n",
      "Average test loss: 0.00346855261694226\n",
      "Epoch 118/300\n",
      "Average training loss: 0.016840130877991518\n",
      "Average test loss: 0.0034678768378992877\n",
      "Epoch 119/300\n",
      "Average training loss: 0.016833961602714328\n",
      "Average test loss: 0.0034736328425092828\n",
      "Epoch 120/300\n",
      "Average training loss: 0.016827536745203864\n",
      "Average test loss: 0.00350856157971753\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016813994011945194\n",
      "Average test loss: 0.0034619220443483857\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01680858928296301\n",
      "Average test loss: 0.0034642154251535735\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016801054226027595\n",
      "Average test loss: 0.003481559935336312\n",
      "Epoch 124/300\n",
      "Average training loss: 0.016797726456489828\n",
      "Average test loss: 0.0034875173069950608\n",
      "Epoch 125/300\n",
      "Average training loss: 0.016783798625071844\n",
      "Average test loss: 0.00348839335454007\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016778472322556706\n",
      "Average test loss: 0.0034647281632448237\n",
      "Epoch 127/300\n",
      "Average training loss: 0.0167786857345038\n",
      "Average test loss: 0.0035070791737072996\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01676830789198478\n",
      "Average test loss: 0.003489632630927695\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016764515790674422\n",
      "Average test loss: 0.00347586463433173\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016757113714185028\n",
      "Average test loss: 0.0034959553113828104\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0167460873035921\n",
      "Average test loss: 0.0034894571457472112\n",
      "Epoch 132/300\n",
      "Average training loss: 0.016732715911335416\n",
      "Average test loss: 0.0035182691754566298\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0167300835053126\n",
      "Average test loss: 0.003484905688299073\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0167249860904283\n",
      "Average test loss: 0.003477769614300794\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016722672754691706\n",
      "Average test loss: 0.0035754529697199664\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016709062444667022\n",
      "Average test loss: 0.003497844894002709\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01671181611220042\n",
      "Average test loss: 0.0034915161157647767\n",
      "Epoch 138/300\n",
      "Average training loss: 0.016699640259146692\n",
      "Average test loss: 0.003508739491303762\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016683449843691456\n",
      "Average test loss: 0.003584246516227722\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01668796345591545\n",
      "Average test loss: 0.003502818189457887\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016678531616926194\n",
      "Average test loss: 0.0035326456899444263\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01666910024732351\n",
      "Average test loss: 0.0034582744890617\n",
      "Epoch 143/300\n",
      "Average training loss: 0.016660827907423178\n",
      "Average test loss: 0.0035005901791155336\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01666187146306038\n",
      "Average test loss: 0.0034972658631288343\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016652866517504056\n",
      "Average test loss: 0.003478139850207501\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0166533282995224\n",
      "Average test loss: 0.0034675656898568072\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01664120446311103\n",
      "Average test loss: 0.003502971873515182\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016629021094905005\n",
      "Average test loss: 0.0036182670200036633\n",
      "Epoch 149/300\n",
      "Average training loss: 0.016630715678135555\n",
      "Average test loss: 0.003493968859521879\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016614331281847423\n",
      "Average test loss: 0.0034807036788099344\n",
      "Epoch 151/300\n",
      "Average training loss: 0.016621485363278125\n",
      "Average test loss: 0.0035231395094758935\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016606854386627675\n",
      "Average test loss: 0.0034903980495615138\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016597600918677118\n",
      "Average test loss: 0.003471925822810994\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016593395089937582\n",
      "Average test loss: 0.0036730710644688872\n",
      "Epoch 155/300\n",
      "Average training loss: 0.016594877334104644\n",
      "Average test loss: 0.0034648759907318486\n",
      "Epoch 156/300\n",
      "Average training loss: 0.016582967199385165\n",
      "Average test loss: 0.003481448917132285\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01658647313548459\n",
      "Average test loss: 0.003516765379243427\n",
      "Epoch 158/300\n",
      "Average training loss: 0.016579829042156536\n",
      "Average test loss: 0.003476315536432796\n",
      "Epoch 159/300\n",
      "Average training loss: 0.016567843159867656\n",
      "Average test loss: 0.0035088818023602166\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01656287001901203\n",
      "Average test loss: 0.0035038469831148783\n",
      "Epoch 161/300\n",
      "Average training loss: 0.016492684183849228\n",
      "Average test loss: 0.0039031734452065494\n",
      "Epoch 173/300\n",
      "Average training loss: 0.016494444457193214\n",
      "Average test loss: 0.0035388740822672846\n",
      "Epoch 174/300\n",
      "Average training loss: 0.016486942246556283\n",
      "Average test loss: 0.003514636298020681\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01647188790473673\n",
      "Average test loss: 0.003533834789155258\n",
      "Epoch 176/300\n",
      "Average training loss: 0.016472993555996154\n",
      "Average test loss: 0.0035510216371880637\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01646884938991732\n",
      "Average test loss: 0.003535427993784348\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01646105656524499\n",
      "Average test loss: 0.003540771766876181\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0164579562726948\n",
      "Average test loss: 0.003510116529547506\n",
      "Epoch 180/300\n",
      "Average training loss: 0.016450581772459876\n",
      "Average test loss: 0.0035014355116420324\n",
      "Epoch 181/300\n",
      "Average training loss: 0.016444871087869007\n",
      "Average test loss: 0.0035023417236904305\n",
      "Epoch 182/300\n",
      "Average training loss: 0.016437114281786812\n",
      "Average test loss: 0.0035051046717498036\n",
      "Epoch 183/300\n",
      "Average training loss: 0.016434064977698858\n",
      "Average test loss: 0.0035084042797485985\n",
      "Epoch 184/300\n",
      "Average training loss: 0.016430808352099525\n",
      "Average test loss: 0.00348992374435895\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01642996518727806\n",
      "Average test loss: 0.003516410167846415\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0164152849101358\n",
      "Average test loss: 0.0035211863908916714\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01641401394125488\n",
      "Average test loss: 0.003516601813543174\n",
      "Epoch 188/300\n",
      "Average training loss: 0.016405389739407434\n",
      "Average test loss: 0.0034932762988739543\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01640914769222339\n",
      "Average test loss: 0.0035714878220525054\n",
      "Epoch 190/300\n",
      "Average training loss: 0.016403589312401084\n",
      "Average test loss: 0.003560175365044011\n",
      "Epoch 191/300\n",
      "Average training loss: 0.016402497961289352\n",
      "Average test loss: 0.003562109297969275\n",
      "Epoch 192/300\n",
      "Average training loss: 0.016387877302037345\n",
      "Average test loss: 0.0035350010943495567\n",
      "Epoch 193/300\n",
      "Average training loss: 0.016385131238235366\n",
      "Average test loss: 0.003529002506285906\n",
      "Epoch 194/300\n",
      "Average training loss: 0.016379145360655253\n",
      "Average test loss: 0.0035605948480467\n",
      "Epoch 195/300\n",
      "Average training loss: 0.016376038138237264\n",
      "Average test loss: 0.003557730815269881\n",
      "Epoch 196/300\n",
      "Average training loss: 0.016381218204067814\n",
      "Average test loss: 0.0035889915600419045\n",
      "Epoch 197/300\n",
      "Average training loss: 0.01637255015472571\n",
      "Average test loss: 0.003514277331531048\n",
      "Epoch 198/300\n",
      "Average training loss: 0.016370228935447005\n",
      "Average test loss: 0.003500478412749039\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01636125546031528\n",
      "Average test loss: 0.0035204669806278414\n",
      "Epoch 200/300\n",
      "Average training loss: 0.016357932249705\n",
      "Average test loss: 0.00351760741447409\n",
      "Epoch 201/300\n",
      "Average training loss: 0.016350464156104458\n",
      "Average test loss: 0.003554805413302448\n",
      "Epoch 202/300\n",
      "Average training loss: 0.016343939582506815\n",
      "Average test loss: 0.003541924667234222\n",
      "Epoch 203/300\n",
      "Average training loss: 0.016337341407934823\n",
      "Average test loss: 0.0035143569469865826\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01632567724171612\n",
      "Average test loss: 0.0035315754790272978\n",
      "Epoch 205/300\n",
      "Average training loss: 0.016329762623541886\n",
      "Average test loss: 0.0035431207180437116\n",
      "Epoch 206/300\n",
      "Average training loss: 0.016337381534278394\n",
      "Average test loss: 0.0035273290909826754\n",
      "Epoch 207/300\n",
      "Average training loss: 0.016320530883140033\n",
      "Average test loss: 0.0035196843687444923\n",
      "Epoch 208/300\n",
      "Average training loss: 0.016316864829096528\n",
      "Average test loss: 0.0035018883293701543\n",
      "Epoch 209/300\n",
      "Average training loss: 0.016314922495848604\n",
      "Average test loss: 0.003523673632492622\n",
      "Epoch 210/300\n",
      "Average training loss: 0.016314390783508617\n",
      "Average test loss: 0.0035260329830149807\n",
      "Epoch 211/300\n",
      "Average training loss: 0.016303372242384487\n",
      "Average test loss: 0.0035098255148364437\n",
      "Epoch 212/300\n",
      "Average training loss: 0.016292561860548126\n",
      "Average test loss: 0.0035445628348324035\n",
      "Epoch 213/300\n",
      "Average training loss: 0.016302613814671834\n",
      "Average test loss: 0.003570701175679763\n",
      "Epoch 214/300\n",
      "Average training loss: 0.016287108562886714\n",
      "Average test loss: 0.0035583369309703508\n",
      "Epoch 215/300\n",
      "Average training loss: 0.016285287871128984\n",
      "Average test loss: 0.0035513791172868676\n",
      "Epoch 216/300\n",
      "Average training loss: 0.01627642855213748\n",
      "Average test loss: 0.003538901009493404\n",
      "Epoch 217/300\n",
      "Average training loss: 0.016284894842240546\n",
      "Average test loss: 0.003547543836136659\n",
      "Epoch 218/300\n",
      "Average training loss: 0.016279700275096627\n",
      "Average test loss: 0.0035423208718291586\n",
      "Epoch 219/300\n",
      "Average training loss: 0.016275866207149294\n",
      "Average test loss: 0.0035179238555332025\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016265241117941007\n",
      "Average test loss: 0.003488945696502924\n",
      "Epoch 221/300\n",
      "Average training loss: 0.016260455279714533\n",
      "Average test loss: 0.0036029447013926174\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016268945950600838\n",
      "Average test loss: 0.003567885203493966\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016249128019644156\n",
      "Average test loss: 0.0036378379648344386\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016251022383570672\n",
      "Average test loss: 0.0035226362258609797\n",
      "Epoch 225/300\n",
      "Average training loss: 0.016241581675079133\n",
      "Average test loss: 0.0036370951771322224\n",
      "Epoch 226/300\n",
      "Average training loss: 0.016252106809781657\n",
      "Average test loss: 0.003558589430525899\n",
      "Epoch 227/300\n",
      "Average training loss: 0.016240468277699417\n",
      "Average test loss: 0.0035590319890114997\n",
      "Epoch 228/300\n",
      "Average training loss: 0.016231959100398752\n",
      "Average test loss: 0.003581368796527386\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016232039912707275\n",
      "Average test loss: 0.0035329921547737385\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01623133162988557\n",
      "Average test loss: 0.00353818642679188\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016225942671298982\n",
      "Average test loss: 0.0035102945363356007\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01622261831578281\n",
      "Average test loss: 0.0035762845861415067\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01621924279961321\n",
      "Average test loss: 0.0036153528394384515\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016217676640384728\n",
      "Average test loss: 0.0035828291612366833\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016204817682504653\n",
      "Average test loss: 0.0036088660235206287\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016213451772100394\n",
      "Average test loss: 0.0036032968374590077\n",
      "Epoch 237/300\n",
      "Average training loss: 0.016207464173436166\n",
      "Average test loss: 0.0035084677578674423\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016202721080846257\n",
      "Average test loss: 0.003537989406329062\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01619077928778198\n",
      "Average test loss: 0.0035201360318395827\n",
      "Epoch 240/300\n",
      "Average training loss: 0.016190354734659195\n",
      "Average test loss: 0.003547843095742994\n",
      "Epoch 241/300\n",
      "Average training loss: 0.016185159262683657\n",
      "Average test loss: 0.003522382337393032\n",
      "Epoch 242/300\n",
      "Average training loss: 0.016181793849501344\n",
      "Average test loss: 0.0035454017772442764\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016193391307360597\n",
      "Average test loss: 0.0036516308991445434\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01617331449025207\n",
      "Average test loss: 0.0035729935756987994\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01616778966287772\n",
      "Average test loss: 0.003531445821126302\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016166120712955794\n",
      "Average test loss: 0.0035663748358686766\n",
      "Epoch 247/300\n",
      "Average training loss: 0.016160308692190383\n",
      "Average test loss: 0.0035890643284138706\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0161718803031577\n",
      "Average test loss: 0.003644009249491824\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016156383263568084\n",
      "Average test loss: 0.0035793710413078465\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01615227478577031\n",
      "Average test loss: 0.0035388162413405046\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016154605204032526\n",
      "Average test loss: 0.003558910380014115\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01614585934413804\n",
      "Average test loss: 0.0035644816348536145\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016155550241470338\n",
      "Average test loss: 0.003534568126416869\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016139984502560562\n",
      "Average test loss: 0.00365753083386355\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01614321664141284\n",
      "Average test loss: 0.003660222736083799\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0161377886177765\n",
      "Average test loss: 0.0035103590190410615\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01613485915710529\n",
      "Average test loss: 0.0035722274273220036\n",
      "Epoch 258/300\n",
      "Average training loss: 0.016127090757919682\n",
      "Average test loss: 0.0036436803697918853\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01613434797939327\n",
      "Average test loss: 0.003561506857888566\n",
      "Epoch 260/300\n",
      "Average training loss: 0.016115664205617376\n",
      "Average test loss: 0.003526824512829383\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016118621673848893\n",
      "Average test loss: 0.0035170798028508823\n",
      "Epoch 262/300\n",
      "Average training loss: 0.016119232318467566\n",
      "Average test loss: 0.0035707025734914672\n",
      "Epoch 263/300\n",
      "Average training loss: 0.016113429655631385\n",
      "Average test loss: 0.003538545758359962\n",
      "Epoch 264/300\n",
      "Average training loss: 0.016105037750469314\n",
      "Average test loss: 0.003576071705876125\n",
      "Epoch 265/300\n",
      "Average training loss: 0.016115424444278083\n",
      "Average test loss: 0.0035485862625969783\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01609213580439488\n",
      "Average test loss: 0.003624003838540779\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016101875618928008\n",
      "Average test loss: 0.0035804223844574557\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016100960430999598\n",
      "Average test loss: 0.0036953813723391957\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016108103044331072\n",
      "Average test loss: 0.003646973983074228\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01609916932384173\n",
      "Average test loss: 0.003587666844949126\n",
      "Epoch 271/300\n",
      "Average training loss: 0.016090040703614552\n",
      "Average test loss: 0.003608403839170933\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016085532863934836\n",
      "Average test loss: 0.003596557521985637\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01608262042535676\n",
      "Average test loss: 0.0035690221635417806\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016077760871085856\n",
      "Average test loss: 0.0035180529612633916\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016070796873006554\n",
      "Average test loss: 0.003634701968481143\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01607998222940498\n",
      "Average test loss: 0.0035887874683572184\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01606934068020847\n",
      "Average test loss: 0.003598659224394295\n",
      "Epoch 278/300\n",
      "Average training loss: 0.016068718445797762\n",
      "Average test loss: 0.003556017143651843\n",
      "Epoch 279/300\n",
      "Average training loss: 0.016064042918384074\n",
      "Average test loss: 0.0036188902130557432\n",
      "Epoch 280/300\n",
      "Average training loss: 0.01606358977324433\n",
      "Average test loss: 0.003620088092982769\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01605654883550273\n",
      "Average test loss: 0.003555154776200652\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01604979855650001\n",
      "Average test loss: 0.0035360028919660383\n",
      "Epoch 283/300\n",
      "Average training loss: 0.016054810710251333\n",
      "Average test loss: 0.0036410451817015807\n",
      "Epoch 284/300\n",
      "Average training loss: 0.016049369982547232\n",
      "Average test loss: 0.0035462148479289478\n",
      "Epoch 285/300\n",
      "Average training loss: 0.016048248761230045\n",
      "Average test loss: 0.0036279690019372438\n",
      "Epoch 286/300\n",
      "Average training loss: 0.016043225071496435\n",
      "Average test loss: 0.0035877752204736073\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01604573379043076\n",
      "Average test loss: 0.0035437287700672945\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016041682125793565\n",
      "Average test loss: 0.0035247837830748824\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01602834341592259\n",
      "Average test loss: 0.003569501260916392\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016039151729808913\n",
      "Average test loss: 0.003548730567097664\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01601917863719993\n",
      "Average test loss: 0.003543619660039743\n",
      "Epoch 292/300\n",
      "Average training loss: 0.016034507433573406\n",
      "Average test loss: 0.0035286112576723098\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0160299197435379\n",
      "Average test loss: 0.0035684725851234463\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016023141551348898\n",
      "Average test loss: 0.0035623939516436724\n",
      "Epoch 295/300\n",
      "Average training loss: 0.016011845886707305\n",
      "Average test loss: 0.0035772921031133995\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016010542559954856\n",
      "Average test loss: 0.0035948728594101136\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016022093751364284\n",
      "Average test loss: 0.003662045540702012\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01600123996618721\n",
      "Average test loss: 0.003583281504818135\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016004153482615947\n",
      "Average test loss: 0.0035448568891733884\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016007671443952456\n",
      "Average test loss: 0.0035608448680076333\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05866298845741484\n",
      "Average test loss: 0.004407614718708727\n",
      "Epoch 2/300\n",
      "Average training loss: 0.020756775269905724\n",
      "Average test loss: 0.0040721158016886975\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019437264907691213\n",
      "Average test loss: 0.0038692891552216477\n",
      "Epoch 4/300\n",
      "Average training loss: 0.018728113869826\n",
      "Average test loss: 0.003692154329063164\n",
      "Epoch 5/300\n",
      "Average training loss: 0.01821501216954655\n",
      "Average test loss: 0.003651711627634035\n",
      "Epoch 6/300\n",
      "Average training loss: 0.017812944547997582\n",
      "Average test loss: 0.0035226323439015283\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01748574475778474\n",
      "Average test loss: 0.003447382042805354\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01720011535783609\n",
      "Average test loss: 0.003361961154267192\n",
      "Epoch 9/300\n",
      "Average training loss: 0.016952198858062425\n",
      "Average test loss: 0.003326702070525951\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01675071775996023\n",
      "Average test loss: 0.003280804948053426\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01656716208242708\n",
      "Average test loss: 0.003226062173437741\n",
      "Epoch 12/300\n",
      "Average training loss: 0.016403041490250164\n",
      "Average test loss: 0.0031957494219144187\n",
      "Epoch 13/300\n",
      "Average training loss: 0.01625831100013521\n",
      "Average test loss: 0.003164816223084927\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01614584095693297\n",
      "Average test loss: 0.003129429997669326\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016022932173477278\n",
      "Average test loss: 0.0031050723126779\n",
      "Epoch 16/300\n",
      "Average training loss: 0.015919276941153738\n",
      "Average test loss: 0.0030889373183664347\n",
      "Epoch 17/300\n",
      "Average training loss: 0.01582937208728658\n",
      "Average test loss: 0.0030838564361135167\n",
      "Epoch 18/300\n",
      "Average training loss: 0.015754382019655573\n",
      "Average test loss: 0.003059464799033271\n",
      "Epoch 19/300\n",
      "Average training loss: 0.015659027932418718\n",
      "Average test loss: 0.003022165656917625\n",
      "Epoch 20/300\n",
      "Average training loss: 0.015591202348470688\n",
      "Average test loss: 0.003007071260155903\n",
      "Epoch 21/300\n",
      "Average training loss: 0.015520214933488104\n",
      "Average test loss: 0.003017673010834389\n",
      "Epoch 22/300\n",
      "Average training loss: 0.015448538874586424\n",
      "Average test loss: 0.0029881603150731986\n",
      "Epoch 23/300\n",
      "Average training loss: 0.015390896807114283\n",
      "Average test loss: 0.0029729136429313156\n",
      "Epoch 24/300\n",
      "Average training loss: 0.015340112928715018\n",
      "Average test loss: 0.0029638484919236765\n",
      "Epoch 25/300\n",
      "Average training loss: 0.015295892664955723\n",
      "Average test loss: 0.0029504632597996128\n",
      "Epoch 26/300\n",
      "Average training loss: 0.015242194440629747\n",
      "Average test loss: 0.002951678517171078\n",
      "Epoch 27/300\n",
      "Average training loss: 0.015196072071790695\n",
      "Average test loss: 0.0029548903776125774\n",
      "Epoch 28/300\n",
      "Average training loss: 0.015164083826045196\n",
      "Average test loss: 0.002932100897654891\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015119062210122745\n",
      "Average test loss: 0.0029213161137368942\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015081943743758732\n",
      "Average test loss: 0.0029335856756402385\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015037885341379377\n",
      "Average test loss: 0.002911870087393456\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015014593150880601\n",
      "Average test loss: 0.0029075306869215437\n",
      "Epoch 33/300\n",
      "Average training loss: 0.014986173174447483\n",
      "Average test loss: 0.0029039923395547604\n",
      "Epoch 34/300\n",
      "Average training loss: 0.014947335675358771\n",
      "Average test loss: 0.0028940099873062636\n",
      "Epoch 35/300\n",
      "Average training loss: 0.014929753237300449\n",
      "Average test loss: 0.0029529943325453333\n",
      "Epoch 36/300\n",
      "Average training loss: 0.014893246108459102\n",
      "Average test loss: 0.002905833671490351\n",
      "Epoch 37/300\n",
      "Average training loss: 0.014876881284018357\n",
      "Average test loss: 0.0029072548382812076\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01485110930270619\n",
      "Average test loss: 0.0028743420377787617\n",
      "Epoch 39/300\n",
      "Average training loss: 0.014819650494390064\n",
      "Average test loss: 0.002883385102575024\n",
      "Epoch 40/300\n",
      "Average training loss: 0.014801162794232369\n",
      "Average test loss: 0.0028673879934681787\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014773702024585672\n",
      "Average test loss: 0.0028680361405842834\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014748566745056046\n",
      "Average test loss: 0.0028621323050724134\n",
      "Epoch 43/300\n",
      "Average training loss: 0.014741230078869396\n",
      "Average test loss: 0.0028521110868702333\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014721322238445283\n",
      "Average test loss: 0.0028583778941796887\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014691338744428423\n",
      "Average test loss: 0.0028670526894016397\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014675991968148285\n",
      "Average test loss: 0.002863017082421316\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014654592129091423\n",
      "Average test loss: 0.0028587249691287677\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014640143170124955\n",
      "Average test loss: 0.0028474470285905734\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014628823998073737\n",
      "Average test loss: 0.0028468249620248872\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014612209252185291\n",
      "Average test loss: 0.0028426838672409457\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014595002479851247\n",
      "Average test loss: 0.002844128800349103\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014581900565988487\n",
      "Average test loss: 0.0028425833123425642\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014558112869660059\n",
      "Average test loss: 0.0028507867941839826\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014550694206522571\n",
      "Average test loss: 0.0028416380651502147\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014532624253796207\n",
      "Average test loss: 0.002837880962424808\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014509886111650203\n",
      "Average test loss: 0.00283453505506946\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014499654275675615\n",
      "Average test loss: 0.0028333520956544413\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014488794123133024\n",
      "Average test loss: 0.002828082610335615\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014469422461258041\n",
      "Average test loss: 0.0028320590019639993\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014466378539800644\n",
      "Average test loss: 0.0028181967122687234\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014449340492486954\n",
      "Average test loss: 0.002825674100882477\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014429135467443201\n",
      "Average test loss: 0.002853153232898977\n",
      "Epoch 63/300\n",
      "Average training loss: 0.014423403222527768\n",
      "Average test loss: 0.0028570064161386756\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014406775766776668\n",
      "Average test loss: 0.002819039184600115\n",
      "Epoch 65/300\n",
      "Average training loss: 0.014401399725013309\n",
      "Average test loss: 0.0028178253076556656\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014387626720799341\n",
      "Average test loss: 0.0028336208789712853\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01437451884150505\n",
      "Average test loss: 0.0028175233631498285\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01436992314375109\n",
      "Average test loss: 0.0028247969047062926\n",
      "Epoch 69/300\n",
      "Average training loss: 0.01435411923709843\n",
      "Average test loss: 0.0028098058727466396\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0143451985369126\n",
      "Average test loss: 0.0028128128496723043\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01433469075544013\n",
      "Average test loss: 0.002882916348468926\n",
      "Epoch 72/300\n",
      "Average training loss: 0.01431512148016029\n",
      "Average test loss: 0.0028101394270650215\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014307510553134813\n",
      "Average test loss: 0.0028114282875839206\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014297575101256371\n",
      "Average test loss: 0.002805428902515107\n",
      "Epoch 75/300\n",
      "Average training loss: 0.014283325058718523\n",
      "Average test loss: 0.002837033644732502\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014287228205137783\n",
      "Average test loss: 0.0028052363886187474\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014276953304807345\n",
      "Average test loss: 0.0028057144468443262\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014258777713610067\n",
      "Average test loss: 0.0028011344936158923\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014255295823017756\n",
      "Average test loss: 0.002800369209299485\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014238107052114275\n",
      "Average test loss: 0.0028273104714850584\n",
      "Epoch 81/300\n",
      "Average training loss: 0.01423043756104178\n",
      "Average test loss: 0.002829126369415058\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014219478526877032\n",
      "Average test loss: 0.002809327641295062\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014211266871127818\n",
      "Average test loss: 0.0029309664925353395\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014203598856925964\n",
      "Average test loss: 0.0028141695068528254\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014195506567756335\n",
      "Average test loss: 0.0028489078225360977\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014183989923861292\n",
      "Average test loss: 0.0028233032140673863\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014176029507484702\n",
      "Average test loss: 0.0028251211584235233\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014157279358969795\n",
      "Average test loss: 0.002795475435960624\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014162814485530058\n",
      "Average test loss: 0.002804722505932053\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01414372725950347\n",
      "Average test loss: 0.002806175068434742\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014139205507934094\n",
      "Average test loss: 0.0027985017862584855\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01413374179436101\n",
      "Average test loss: 0.0028153733565575547\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014122875006662474\n",
      "Average test loss: 0.0028210210231029327\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014113854558931457\n",
      "Average test loss: 0.0028281993855618767\n",
      "Epoch 95/300\n",
      "Average training loss: 0.014114861426254113\n",
      "Average test loss: 0.0027944273779996567\n",
      "Epoch 96/300\n",
      "Average training loss: 0.014100099668734604\n",
      "Average test loss: 0.0028206163150154883\n",
      "Epoch 97/300\n",
      "Average training loss: 0.014089258708059788\n",
      "Average test loss: 0.0027970181099242633\n",
      "Epoch 98/300\n",
      "Average training loss: 0.014085704875489076\n",
      "Average test loss: 0.002810978260719114\n",
      "Epoch 99/300\n",
      "Average training loss: 0.014078836547003853\n",
      "Average test loss: 0.002852880872372124\n",
      "Epoch 100/300\n",
      "Average training loss: 0.014069708567526606\n",
      "Average test loss: 0.0028040854955712954\n",
      "Epoch 101/300\n",
      "Average training loss: 0.014059776138100359\n",
      "Average test loss: 0.0027974788049856822\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01405885106821855\n",
      "Average test loss: 0.0028249400986565484\n",
      "Epoch 103/300\n",
      "Average training loss: 0.014045492002533543\n",
      "Average test loss: 0.002834202350427707\n",
      "Epoch 104/300\n",
      "Average training loss: 0.014051709258721935\n",
      "Average test loss: 0.0028206490735626882\n",
      "Epoch 105/300\n",
      "Average training loss: 0.014039733898308543\n",
      "Average test loss: 0.0028032229671047793\n",
      "Epoch 106/300\n",
      "Average training loss: 0.014025354141990343\n",
      "Average test loss: 0.0028212862821916738\n",
      "Epoch 107/300\n",
      "Average training loss: 0.01401685144421127\n",
      "Average test loss: 0.0028192122555855247\n",
      "Epoch 108/300\n",
      "Average training loss: 0.014010817463199298\n",
      "Average test loss: 0.0027990042852858703\n",
      "Epoch 109/300\n",
      "Average training loss: 0.014003085868226158\n",
      "Average test loss: 0.002815138747294744\n",
      "Epoch 110/300\n",
      "Average training loss: 0.014003122490313318\n",
      "Average test loss: 0.0028181152600381108\n",
      "Epoch 111/300\n",
      "Average training loss: 0.013986428660237128\n",
      "Average test loss: 0.0028086008646835883\n",
      "Epoch 112/300\n",
      "Average training loss: 0.013979649976723725\n",
      "Average test loss: 0.0027965594687395625\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013980439016388522\n",
      "Average test loss: 0.0030908048376441003\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013967048408256636\n",
      "Average test loss: 0.0028465635093549886\n",
      "Epoch 115/300\n",
      "Average training loss: 0.013963561515013377\n",
      "Average test loss: 0.0028009870768421225\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013957623784740766\n",
      "Average test loss: 0.00281981952695383\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013956357796986897\n",
      "Average test loss: 0.002834169071374668\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013942753930886587\n",
      "Average test loss: 0.0028105494141992594\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013941858851247364\n",
      "Average test loss: 0.0028032625400357778\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013929354811708133\n",
      "Average test loss: 0.0028488773258609906\n",
      "Epoch 121/300\n",
      "Average training loss: 0.013922234679261843\n",
      "Average test loss: 0.002801506653221117\n",
      "Epoch 122/300\n",
      "Average training loss: 0.013918149725430541\n",
      "Average test loss: 0.0028271656499968636\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013915525170663993\n",
      "Average test loss: 0.0028028432509551444\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01391111797756619\n",
      "Average test loss: 0.002792892599064443\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013904060831500425\n",
      "Average test loss: 0.002816532381499807\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013893677745428351\n",
      "Average test loss: 0.002810379289297594\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013887960188090801\n",
      "Average test loss: 0.002833863853166501\n",
      "Epoch 128/300\n",
      "Average training loss: 0.013881315485470825\n",
      "Average test loss: 0.00281766775291827\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013875589141415226\n",
      "Average test loss: 0.002796128422435787\n",
      "Epoch 130/300\n",
      "Average training loss: 0.013881486187378566\n",
      "Average test loss: 0.0028144145876997048\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01386677088919613\n",
      "Average test loss: 0.002820363606636723\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01387830358164178\n",
      "Average test loss: 0.002817373480233881\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013855627118713326\n",
      "Average test loss: 0.0028270563108639584\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01384776110533211\n",
      "Average test loss: 0.0027974266289836833\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013838620542652077\n",
      "Average test loss: 0.0028248994178656076\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013834154485828346\n",
      "Average test loss: 0.0028215030362415644\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013829995069238874\n",
      "Average test loss: 0.0028181837973081402\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013823055666353968\n",
      "Average test loss: 0.0028442768764992553\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013813891116943625\n",
      "Average test loss: 0.0028121557376450963\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013813104494578309\n",
      "Average test loss: 0.0028262544394367273\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013806579002075724\n",
      "Average test loss: 0.0028323775033156076\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013808257235421074\n",
      "Average test loss: 0.0028175410311669113\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01379571921709511\n",
      "Average test loss: 0.0028024357888433668\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013791115706993474\n",
      "Average test loss: 0.0028252280253089138\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013794704145855374\n",
      "Average test loss: 0.002809949795818991\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01378596011797587\n",
      "Average test loss: 0.002837072768766019\n",
      "Epoch 147/300\n",
      "Average training loss: 0.013778411732779609\n",
      "Average test loss: 0.0028087784088113243\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013775095082819462\n",
      "Average test loss: 0.0028075955404589575\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013770089238054223\n",
      "Average test loss: 0.00281253826038705\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013756868053641584\n",
      "Average test loss: 0.002822132893320587\n",
      "Epoch 151/300\n",
      "Average training loss: 0.013758870215051703\n",
      "Average test loss: 0.002838965674655305\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013754401189585527\n",
      "Average test loss: 0.0028258256086458764\n",
      "Epoch 153/300\n",
      "Average training loss: 0.0137519553311997\n",
      "Average test loss: 0.002860817987057898\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013737264091769855\n",
      "Average test loss: 0.0028103376637316413\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013730995857053332\n",
      "Average test loss: 0.0028140512274371253\n",
      "Epoch 156/300\n",
      "Average training loss: 0.01372834256456958\n",
      "Average test loss: 0.0028360136987434495\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013735591844552093\n",
      "Average test loss: 0.0028327718497150474\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013720864213175244\n",
      "Average test loss: 0.0028078556531626318\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013726620000269679\n",
      "Average test loss: 0.0028228784408420326\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013713529459304279\n",
      "Average test loss: 0.0028670940478849744\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013704959558116065\n",
      "Average test loss: 0.0028161884035087293\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01370520172516505\n",
      "Average test loss: 0.002804391759965155\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013696546954413255\n",
      "Average test loss: 0.0028197098827610415\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013697080963187747\n",
      "Average test loss: 0.0028477746424161727\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013687043858071169\n",
      "Average test loss: 0.002822747519860665\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013687797032296658\n",
      "Average test loss: 0.002809976167562935\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013680243065787686\n",
      "Average test loss: 0.0028586206707275578\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013674686736530727\n",
      "Average test loss: 0.002833187500221862\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013671968737410174\n",
      "Average test loss: 0.0028311123483710818\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01366249611808194\n",
      "Average test loss: 0.002829760839127832\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013658093079924584\n",
      "Average test loss: 0.002805067870972885\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013657297708921962\n",
      "Average test loss: 0.0028184441961348057\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013658528396652804\n",
      "Average test loss: 0.002812473537400365\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013651326471732722\n",
      "Average test loss: 0.002820528006181121\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013641137056052684\n",
      "Average test loss: 0.002845944106268386\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013637089334428311\n",
      "Average test loss: 0.002845321930737959\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013633736483752728\n",
      "Average test loss: 0.002835760993676053\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01363452649447653\n",
      "Average test loss: 0.002822787092274262\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01362881702101893\n",
      "Average test loss: 0.0028985029140280354\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013619016745024257\n",
      "Average test loss: 0.0028343442361801864\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01362078638209237\n",
      "Average test loss: 0.0029038451611995697\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013617850015560786\n",
      "Average test loss: 0.002846520646371775\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013612565448714626\n",
      "Average test loss: 0.0028301509283483028\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013609768665499157\n",
      "Average test loss: 0.0028399776669426097\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013603948741323419\n",
      "Average test loss: 0.002830840180731482\n",
      "Epoch 186/300\n",
      "Average training loss: 0.013602506885098087\n",
      "Average test loss: 0.0028220069010017647\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013599173100458252\n",
      "Average test loss: 0.002844192899349663\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013590969455738863\n",
      "Average test loss: 0.002822805783193972\n",
      "Epoch 189/300\n",
      "Average training loss: 0.013596163406968117\n",
      "Average test loss: 0.0028929006024781202\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013585808001458645\n",
      "Average test loss: 0.002824522596680456\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013581505693495274\n",
      "Average test loss: 0.0028632364442778956\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013581670193208589\n",
      "Average test loss: 0.002821436015682088\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013576714235875342\n",
      "Average test loss: 0.0028257516353494593\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013565525368683868\n",
      "Average test loss: 0.002844022925529215\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013563803490665223\n",
      "Average test loss: 0.0028267445030311744\n",
      "Epoch 196/300\n",
      "Average training loss: 0.01356348424239291\n",
      "Average test loss: 0.002846204867379533\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013556936401459906\n",
      "Average test loss: 0.002837096855872207\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013553314129511515\n",
      "Average test loss: 0.0028655335654815036\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013550684718622101\n",
      "Average test loss: 0.0028518168270174\n",
      "Epoch 200/300\n",
      "Average training loss: 0.013541550075842275\n",
      "Average test loss: 0.0028158029704872106\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01354287911620405\n",
      "Average test loss: 0.0028389506031655603\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013541619752844175\n",
      "Average test loss: 0.0028352203908272914\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01353228129612075\n",
      "Average test loss: 0.0028720493476010033\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013524371167023977\n",
      "Average test loss: 0.0028379196073446008\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01352874815546804\n",
      "Average test loss: 0.00283566001119713\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013521471670104398\n",
      "Average test loss: 0.0028567186502946747\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013537747922870848\n",
      "Average test loss: 0.0028939587662203444\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013524399417142074\n",
      "Average test loss: 0.0028424529592610067\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013528721051911514\n",
      "Average test loss: 0.0028433416572709877\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013502900062335862\n",
      "Average test loss: 0.0028360445931967762\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01349503361268176\n",
      "Average test loss: 0.0028777948431670665\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013502495285537508\n",
      "Average test loss: 0.0028716510790917607\n",
      "Epoch 213/300\n",
      "Average training loss: 0.013501164303885566\n",
      "Average test loss: 0.002825251200857262\n",
      "Epoch 214/300\n",
      "Average training loss: 0.013499206035501428\n",
      "Average test loss: 0.0028437664409478505\n",
      "Epoch 215/300\n",
      "Average training loss: 0.013487431325018406\n",
      "Average test loss: 0.0028712613652977677\n",
      "Epoch 216/300\n",
      "Average training loss: 0.013491092152893543\n",
      "Average test loss: 0.0028487080132795703\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01349232385141982\n",
      "Average test loss: 0.0028635791103459067\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013490770380530093\n",
      "Average test loss: 0.002866416143460406\n",
      "Epoch 219/300\n",
      "Average training loss: 0.013471775576472283\n",
      "Average test loss: 0.0028503133557322953\n",
      "Epoch 220/300\n",
      "Average training loss: 0.013477615496350659\n",
      "Average test loss: 0.0029540568821960025\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01348017170611355\n",
      "Average test loss: 0.0029030539692483015\n",
      "Epoch 222/300\n",
      "Average training loss: 0.013470128063526418\n",
      "Average test loss: 0.0028905493910941814\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01346682505061229\n",
      "Average test loss: 0.002856276451713509\n",
      "Epoch 224/300\n",
      "Average training loss: 0.013466144856479434\n",
      "Average test loss: 0.0028637100234627723\n",
      "Epoch 225/300\n",
      "Average training loss: 0.013473621597720517\n",
      "Average test loss: 0.0028518378604203464\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01345275669793288\n",
      "Average test loss: 0.0028843157585296364\n",
      "Epoch 227/300\n",
      "Average training loss: 0.013447550230556065\n",
      "Average test loss: 0.0028390981588098736\n",
      "Epoch 228/300\n",
      "Average training loss: 0.013453930616378784\n",
      "Average test loss: 0.002876102628393306\n",
      "Epoch 229/300\n",
      "Average training loss: 0.013446640505558915\n",
      "Average test loss: 0.0028546917068047657\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01343462023884058\n",
      "Average test loss: 0.0030089044622662996\n",
      "Epoch 231/300\n",
      "Average training loss: 0.013440873381992182\n",
      "Average test loss: 0.0028468347715420853\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01343603195829524\n",
      "Average test loss: 0.00300516001921561\n",
      "Epoch 233/300\n",
      "Average training loss: 0.013438821624550554\n",
      "Average test loss: 0.0028329315673973825\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013439792529576354\n",
      "Average test loss: 0.0029052522395633987\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013423257545464569\n",
      "Average test loss: 0.0029027785691950058\n",
      "Epoch 236/300\n",
      "Average training loss: 0.013442151460382674\n",
      "Average test loss: 0.0030876929126679897\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01342451644440492\n",
      "Average test loss: 0.00288561835202078\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013419586819079187\n",
      "Average test loss: 0.0028730382172183857\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013424390819337634\n",
      "Average test loss: 0.0028715713324232233\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01342449977911181\n",
      "Average test loss: 0.0029119197606212563\n",
      "Epoch 241/300\n",
      "Average training loss: 0.0134062881047527\n",
      "Average test loss: 0.0028697992047915857\n",
      "Epoch 242/300\n",
      "Average training loss: 0.013412885938253667\n",
      "Average test loss: 0.00284988650213927\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013404217754801115\n",
      "Average test loss: 0.0028403724394738673\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013406966447002358\n",
      "Average test loss: 0.002898559419024322\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013401797049575382\n",
      "Average test loss: 0.0028541111337641875\n",
      "Epoch 246/300\n",
      "Average training loss: 0.013401871440311273\n",
      "Average test loss: 0.0028767331764101983\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013399446944395701\n",
      "Average test loss: 0.0028825442673017583\n",
      "Epoch 248/300\n",
      "Average training loss: 0.013395014379587438\n",
      "Average test loss: 0.0028753333857489956\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013391027660005623\n",
      "Average test loss: 0.0028911261966245044\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013388669083515803\n",
      "Average test loss: 0.0028957014839268392\n",
      "Epoch 251/300\n",
      "Average training loss: 0.013379160268439187\n",
      "Average test loss: 0.002860706236006485\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013385375868943002\n",
      "Average test loss: 0.0028669914785358643\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013380288307037618\n",
      "Average test loss: 0.002905121071574589\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01337784520867798\n",
      "Average test loss: 0.00287614862434566\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013373202624420325\n",
      "Average test loss: 0.002849019121792581\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013374596122238371\n",
      "Average test loss: 0.0028640216843535504\n",
      "Epoch 257/300\n",
      "Average training loss: 0.013366389171116882\n",
      "Average test loss: 0.0028547926387853095\n",
      "Epoch 258/300\n",
      "Average training loss: 0.013373864508337444\n",
      "Average test loss: 0.002875287987705734\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013370853414138158\n",
      "Average test loss: 0.002917953774746921\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01336669775015778\n",
      "Average test loss: 0.0028699599765241147\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013360873012906975\n",
      "Average test loss: 0.0028474905194921627\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013353799930877156\n",
      "Average test loss: 0.0028668256731082995\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013356408707797527\n",
      "Average test loss: 0.002929287114284105\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0133546863877111\n",
      "Average test loss: 0.002885740712698963\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013347395170893934\n",
      "Average test loss: 0.002887695031447543\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013364660897188716\n",
      "Average test loss: 0.002847647617881497\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013355763491657045\n",
      "Average test loss: 0.002865996560495761\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013337141044437886\n",
      "Average test loss: 0.002874291397010287\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013336997026370631\n",
      "Average test loss: 0.002883235918151008\n",
      "Epoch 270/300\n",
      "Average training loss: 0.013339487971531021\n",
      "Average test loss: 0.002912625918785731\n",
      "Epoch 271/300\n",
      "Average training loss: 0.013341911448372734\n",
      "Average test loss: 0.0028594516830311883\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01333667710920175\n",
      "Average test loss: 0.002848188138463431\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013333183760444324\n",
      "Average test loss: 0.0029536428617106545\n",
      "Epoch 274/300\n",
      "Average training loss: 0.013325761782626311\n",
      "Average test loss: 0.0028983771717175843\n",
      "Epoch 275/300\n",
      "Average training loss: 0.013342105489638116\n",
      "Average test loss: 0.002874423284497526\n",
      "Epoch 276/300\n",
      "Average training loss: 0.013327124228907956\n",
      "Average test loss: 0.0029417241339882213\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013312877168258032\n",
      "Average test loss: 0.0029084707678606113\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01331999345537689\n",
      "Average test loss: 0.002860578474485212\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013315352189044158\n",
      "Average test loss: 0.0028606579514841237\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013312891316082742\n",
      "Average test loss: 0.002882432032376528\n",
      "Epoch 281/300\n",
      "Average training loss: 0.013315372016694811\n",
      "Average test loss: 0.002878006084718638\n",
      "Epoch 282/300\n",
      "Average training loss: 0.013303019473950068\n",
      "Average test loss: 0.0028903688219272428\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013307588453094165\n",
      "Average test loss: 0.0028765631191846395\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013299456930822797\n",
      "Average test loss: 0.0028836865381648142\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013298830345273018\n",
      "Average test loss: 0.0028620076131903463\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013306083893610371\n",
      "Average test loss: 0.0028779943589535024\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01330157214568721\n",
      "Average test loss: 0.0028824499593012864\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013299767251229949\n",
      "Average test loss: 0.00287249250875579\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01330234864105781\n",
      "Average test loss: 0.002924967389760746\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013290309314098622\n",
      "Average test loss: 0.0028817343819472524\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013291231769654487\n",
      "Average test loss: 0.002892551620387369\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013296129962636365\n",
      "Average test loss: 0.0028732681665569544\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013286575309932231\n",
      "Average test loss: 0.0029389765128079387\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01329131041881111\n",
      "Average test loss: 0.002896035917517212\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013275912832054827\n",
      "Average test loss: 0.002903989482877983\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013272384769386716\n",
      "Average test loss: 0.002885276929785808\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013269858895904488\n",
      "Average test loss: 0.002877385532276498\n",
      "Epoch 298/300\n",
      "Average training loss: 0.013271057308548027\n",
      "Average test loss: 0.002962528631505039\n",
      "Epoch 299/300\n",
      "Average training loss: 0.013267530491782559\n",
      "Average test loss: 0.0029581079714828067\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01326164974272251\n",
      "Average test loss: 0.002935444652620289\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05470521753695276\n",
      "Average test loss: 0.0038676139704055257\n",
      "Epoch 2/300\n",
      "Average training loss: 0.018232977579037347\n",
      "Average test loss: 0.0035868801756037605\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016915752172470093\n",
      "Average test loss: 0.003316649593412876\n",
      "Epoch 4/300\n",
      "Average training loss: 0.016169386461377142\n",
      "Average test loss: 0.003254428924371799\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015618418004777696\n",
      "Average test loss: 0.0030751875429931613\n",
      "Epoch 6/300\n",
      "Average training loss: 0.015157685415612326\n",
      "Average test loss: 0.003009912086650729\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014789962217211723\n",
      "Average test loss: 0.002880485633595122\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014475246602462398\n",
      "Average test loss: 0.0029038075036886667\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014179308246407243\n",
      "Average test loss: 0.002736891810471813\n",
      "Epoch 10/300\n",
      "Average training loss: 0.013947461648119821\n",
      "Average test loss: 0.002693171816153659\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01374239687124888\n",
      "Average test loss: 0.0026446720716646974\n",
      "Epoch 12/300\n",
      "Average training loss: 0.013566481806337833\n",
      "Average test loss: 0.002604430502280593\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013435416157874797\n",
      "Average test loss: 0.0025574805128077667\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013317003312210241\n",
      "Average test loss: 0.0025548193839689095\n",
      "Epoch 15/300\n",
      "Average training loss: 0.01318585916277435\n",
      "Average test loss: 0.002623384239152074\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013110578014618821\n",
      "Average test loss: 0.0025101985149085524\n",
      "Epoch 17/300\n",
      "Average training loss: 0.013006134847799937\n",
      "Average test loss: 0.0026135519159336887\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012922403170830673\n",
      "Average test loss: 0.0025001429979585937\n",
      "Epoch 19/300\n",
      "Average training loss: 0.012849554050299857\n",
      "Average test loss: 0.0024524503151575725\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012771995086636808\n",
      "Average test loss: 0.0024695321269747285\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012707290947437286\n",
      "Average test loss: 0.002437757889015807\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012655113640758726\n",
      "Average test loss: 0.0024205593506081238\n",
      "Epoch 23/300\n",
      "Average training loss: 0.012595427077677515\n",
      "Average test loss: 0.002408298868789441\n",
      "Epoch 24/300\n",
      "Average training loss: 0.01255532207671139\n",
      "Average test loss: 0.002411424597725272\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012494148617817296\n",
      "Average test loss: 0.0023852245409248605\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012460902105602953\n",
      "Average test loss: 0.0023921105340123177\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012423335670183102\n",
      "Average test loss: 0.0023655630896488827\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012378956509961023\n",
      "Average test loss: 0.002365888881083164\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012357266907890638\n",
      "Average test loss: 0.002360287668597367\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01231329492893484\n",
      "Average test loss: 0.002380773344387611\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01229840975254774\n",
      "Average test loss: 0.0023736097427705925\n",
      "Epoch 32/300\n",
      "Average training loss: 0.012267362894283401\n",
      "Average test loss: 0.0024056728625049192\n",
      "Epoch 33/300\n",
      "Average training loss: 0.012226824282772011\n",
      "Average test loss: 0.002342811370889346\n",
      "Epoch 34/300\n",
      "Average training loss: 0.012208532594972187\n",
      "Average test loss: 0.0023332449644804\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01218107685363955\n",
      "Average test loss: 0.002332551630006896\n",
      "Epoch 36/300\n",
      "Average training loss: 0.012156213975614972\n",
      "Average test loss: 0.0023148045270807212\n",
      "Epoch 37/300\n",
      "Average training loss: 0.012133367027673456\n",
      "Average test loss: 0.002311453752530118\n",
      "Epoch 38/300\n",
      "Average training loss: 0.012107260531021489\n",
      "Average test loss: 0.002336136437745558\n",
      "Epoch 39/300\n",
      "Average training loss: 0.012090985423160923\n",
      "Average test loss: 0.002304971737580167\n",
      "Epoch 40/300\n",
      "Average training loss: 0.012069181204670005\n",
      "Average test loss: 0.0023083567368901436\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01205826850483815\n",
      "Average test loss: 0.0023042303470687735\n",
      "Epoch 42/300\n",
      "Average training loss: 0.012032989472150802\n",
      "Average test loss: 0.002310531167520417\n",
      "Epoch 43/300\n",
      "Average training loss: 0.012016230131189028\n",
      "Average test loss: 0.0024254222695405283\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01199148734493388\n",
      "Average test loss: 0.00230163625213835\n",
      "Epoch 45/300\n",
      "Average training loss: 0.011974397136933273\n",
      "Average test loss: 0.0022930488638165923\n",
      "Epoch 46/300\n",
      "Average training loss: 0.011968581184744835\n",
      "Average test loss: 0.0022972648959192965\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01194885310365094\n",
      "Average test loss: 0.0022847246256553462\n",
      "Epoch 48/300\n",
      "Average training loss: 0.01192752544913027\n",
      "Average test loss: 0.0022994407295352883\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011920654076668952\n",
      "Average test loss: 0.002291991133035885\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011900918113688628\n",
      "Average test loss: 0.00229685595114198\n",
      "Epoch 51/300\n",
      "Average training loss: 0.011882889991005261\n",
      "Average test loss: 0.0022795789876124926\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011866757960783111\n",
      "Average test loss: 0.0022839324360506403\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011860750306811598\n",
      "Average test loss: 0.002295393775527676\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011840833337770569\n",
      "Average test loss: 0.002270417717181974\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011828976755340894\n",
      "Average test loss: 0.0022970132370375923\n",
      "Epoch 56/300\n",
      "Average training loss: 0.011816543500456545\n",
      "Average test loss: 0.0022841966034223638\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011804820994536083\n",
      "Average test loss: 0.0022765123690995904\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011796909365389081\n",
      "Average test loss: 0.0022745317606669333\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011788667407300737\n",
      "Average test loss: 0.0022639707836012045\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011772061308224996\n",
      "Average test loss: 0.0022741576842963697\n",
      "Epoch 61/300\n",
      "Average training loss: 0.01176136741704411\n",
      "Average test loss: 0.0022598050000766914\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011754633298350706\n",
      "Average test loss: 0.0022835250647945536\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011746107473141617\n",
      "Average test loss: 0.0022592829258905518\n",
      "Epoch 64/300\n",
      "Average training loss: 0.011725509452323118\n",
      "Average test loss: 0.002265592569485307\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011723434454037084\n",
      "Average test loss: 0.0022732753180381327\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011707489087349839\n",
      "Average test loss: 0.0022540427676091593\n",
      "Epoch 67/300\n",
      "Average training loss: 0.011706735219392512\n",
      "Average test loss: 0.002255792607449823\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011688463871677716\n",
      "Average test loss: 0.00226312358946436\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011680649605062272\n",
      "Average test loss: 0.0022564289847181902\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01166421186675628\n",
      "Average test loss: 0.002262709494887127\n",
      "Epoch 71/300\n",
      "Average training loss: 0.011661961132039626\n",
      "Average test loss: 0.002260055766337448\n",
      "Epoch 72/300\n",
      "Average training loss: 0.011660482334593932\n",
      "Average test loss: 0.002253338805089394\n",
      "Epoch 73/300\n",
      "Average training loss: 0.011643455718954403\n",
      "Average test loss: 0.0022492313995543453\n",
      "Epoch 74/300\n",
      "Average training loss: 0.011634568114247587\n",
      "Average test loss: 0.00225778605726858\n",
      "Epoch 75/300\n",
      "Average training loss: 0.011629750996000237\n",
      "Average test loss: 0.0022518226291156477\n",
      "Epoch 76/300\n",
      "Average training loss: 0.011621361548701923\n",
      "Average test loss: 0.0022530506420880554\n",
      "Epoch 77/300\n",
      "Average training loss: 0.011602429332832495\n",
      "Average test loss: 0.002250162958064013\n",
      "Epoch 78/300\n",
      "Average training loss: 0.011605690920518504\n",
      "Average test loss: 0.0025857506771054532\n",
      "Epoch 79/300\n",
      "Average training loss: 0.011584392954077986\n",
      "Average test loss: 0.0022451131169994674\n",
      "Epoch 80/300\n",
      "Average training loss: 0.011582140881982115\n",
      "Average test loss: 0.002271182739072376\n",
      "Epoch 81/300\n",
      "Average training loss: 0.011577226120564672\n",
      "Average test loss: 0.002244510276243091\n",
      "Epoch 82/300\n",
      "Average training loss: 0.011570828852554163\n",
      "Average test loss: 0.002250112643879321\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01155694838364919\n",
      "Average test loss: 0.002246128463703725\n",
      "Epoch 84/300\n",
      "Average training loss: 0.011552771434187889\n",
      "Average test loss: 0.0022560362461954355\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01154998425145944\n",
      "Average test loss: 0.002243956842356258\n",
      "Epoch 86/300\n",
      "Average training loss: 0.011542584968937768\n",
      "Average test loss: 0.0022399526931759384\n",
      "Epoch 87/300\n",
      "Average training loss: 0.011534400646057394\n",
      "Average test loss: 0.0023082878401296004\n",
      "Epoch 88/300\n",
      "Average training loss: 0.011523639565540684\n",
      "Average test loss: 0.0022751064068741267\n",
      "Epoch 89/300\n",
      "Average training loss: 0.011520568855934672\n",
      "Average test loss: 0.0022424958396909966\n",
      "Epoch 90/300\n",
      "Average training loss: 0.011514985406564341\n",
      "Average test loss: 0.002251692985184491\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01150460050503413\n",
      "Average test loss: 0.0022642429175062312\n",
      "Epoch 92/300\n",
      "Average training loss: 0.011501504502362675\n",
      "Average test loss: 0.002247146237745053\n",
      "Epoch 93/300\n",
      "Average training loss: 0.011497407808899879\n",
      "Average test loss: 0.0022411614217691953\n",
      "Epoch 94/300\n",
      "Average training loss: 0.011487667712072531\n",
      "Average test loss: 0.002249385983372728\n",
      "Epoch 95/300\n",
      "Average training loss: 0.011482913797100385\n",
      "Average test loss: 0.002257283435306615\n",
      "Epoch 96/300\n",
      "Average training loss: 0.011469042924543222\n",
      "Average test loss: 0.0022451489214888876\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01146861276196109\n",
      "Average test loss: 0.0022588482583976452\n",
      "Epoch 98/300\n",
      "Average training loss: 0.011465810797280736\n",
      "Average test loss: 0.002242561348610454\n",
      "Epoch 99/300\n",
      "Average training loss: 0.011452777312033707\n",
      "Average test loss: 0.002250284973325001\n",
      "Epoch 100/300\n",
      "Average training loss: 0.011453683958285384\n",
      "Average test loss: 0.0022637473413099846\n",
      "Epoch 101/300\n",
      "Average training loss: 0.011449071726865238\n",
      "Average test loss: 0.0022421347045650086\n",
      "Epoch 102/300\n",
      "Average training loss: 0.011438473516040378\n",
      "Average test loss: 0.0022570741259389454\n",
      "Epoch 103/300\n",
      "Average training loss: 0.011433222913493713\n",
      "Average test loss: 0.0022517967294487687\n",
      "Epoch 104/300\n",
      "Average training loss: 0.0114327482216888\n",
      "Average test loss: 0.002239535569627252\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01142233460313744\n",
      "Average test loss: 0.0027509836016429796\n",
      "Epoch 106/300\n",
      "Average training loss: 0.011412824392318726\n",
      "Average test loss: 0.0022473452526869044\n",
      "Epoch 107/300\n",
      "Average training loss: 0.011418279118835925\n",
      "Average test loss: 0.0022484823737500444\n",
      "Epoch 108/300\n",
      "Average training loss: 0.011401747967633937\n",
      "Average test loss: 0.0022318963582317036\n",
      "Epoch 109/300\n",
      "Average training loss: 0.011396139536466864\n",
      "Average test loss: 0.00224590779778858\n",
      "Epoch 110/300\n",
      "Average training loss: 0.011390814668602414\n",
      "Average test loss: 0.002239535404265755\n",
      "Epoch 111/300\n",
      "Average training loss: 0.011389590615199672\n",
      "Average test loss: 0.002237072659139004\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01138125936107503\n",
      "Average test loss: 0.0022434778851974342\n",
      "Epoch 113/300\n",
      "Average training loss: 0.011380395938952764\n",
      "Average test loss: 0.0022400063879580963\n",
      "Epoch 114/300\n",
      "Average training loss: 0.011367865133616659\n",
      "Average test loss: 0.002254875316388077\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01137447643197245\n",
      "Average test loss: 0.002238075689516134\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01137107186516126\n",
      "Average test loss: 0.0022595665380358695\n",
      "Epoch 117/300\n",
      "Average training loss: 0.011363617295192347\n",
      "Average test loss: 0.0022469316613343026\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01135516604118877\n",
      "Average test loss: 0.0022417656394342583\n",
      "Epoch 119/300\n",
      "Average training loss: 0.011340162252800332\n",
      "Average test loss: 0.002243009721032447\n",
      "Epoch 120/300\n",
      "Average training loss: 0.011339015930063194\n",
      "Average test loss: 0.0022423371080723072\n",
      "Epoch 121/300\n",
      "Average training loss: 0.011334972718523609\n",
      "Average test loss: 0.0022673980206665066\n",
      "Epoch 122/300\n",
      "Average training loss: 0.011331372230417198\n",
      "Average test loss: 0.0023785340796328254\n",
      "Epoch 123/300\n",
      "Average training loss: 0.011322941722141372\n",
      "Average test loss: 0.002236979665234685\n",
      "Epoch 124/300\n",
      "Average training loss: 0.011327591066559155\n",
      "Average test loss: 0.0022460103817284107\n",
      "Epoch 125/300\n",
      "Average training loss: 0.011330561416844527\n",
      "Average test loss: 0.0028543630310644704\n",
      "Epoch 126/300\n",
      "Average training loss: 0.011306559729079405\n",
      "Average test loss: 0.002254341987686025\n",
      "Epoch 127/300\n",
      "Average training loss: 0.011309141869346302\n",
      "Average test loss: 0.002265700313780043\n",
      "Epoch 128/300\n",
      "Average training loss: 0.011305330432123608\n",
      "Average test loss: 0.002290928533197277\n",
      "Epoch 129/300\n",
      "Average training loss: 0.011297823717196783\n",
      "Average test loss: 0.002253920865141683\n",
      "Epoch 130/300\n",
      "Average training loss: 0.011290745043920147\n",
      "Average test loss: 0.0022383045657641356\n",
      "Epoch 131/300\n",
      "Average training loss: 0.011288123007449838\n",
      "Average test loss: 0.002276284748274419\n",
      "Epoch 132/300\n",
      "Average training loss: 0.011289058499866061\n",
      "Average test loss: 0.002234407095445527\n",
      "Epoch 133/300\n",
      "Average training loss: 0.011279608057604897\n",
      "Average test loss: 0.0022546646555678712\n",
      "Epoch 134/300\n",
      "Average training loss: 0.011279498918188942\n",
      "Average test loss: 0.002239456444564793\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01126942751225498\n",
      "Average test loss: 0.002249173356530567\n",
      "Epoch 136/300\n",
      "Average training loss: 0.011267740266190636\n",
      "Average test loss: 0.0022401943940462336\n",
      "Epoch 137/300\n",
      "Average training loss: 0.011256433287428485\n",
      "Average test loss: 0.0022529090928534667\n",
      "Epoch 138/300\n",
      "Average training loss: 0.011258399377266566\n",
      "Average test loss: 0.0022477411530498\n",
      "Epoch 139/300\n",
      "Average training loss: 0.011258897821108501\n",
      "Average test loss: 0.0022516747446109852\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01124801135642661\n",
      "Average test loss: 0.002260564652995931\n",
      "Epoch 141/300\n",
      "Average training loss: 0.011243393512235748\n",
      "Average test loss: 0.0022565336030804448\n",
      "Epoch 142/300\n",
      "Average training loss: 0.011240955457091332\n",
      "Average test loss: 0.0022530962203939756\n",
      "Epoch 143/300\n",
      "Average training loss: 0.011247295416891574\n",
      "Average test loss: 0.0022691655609135826\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01123132980035411\n",
      "Average test loss: 0.0022524048034101725\n",
      "Epoch 145/300\n",
      "Average training loss: 0.011225140976409117\n",
      "Average test loss: 0.002247263222725855\n",
      "Epoch 146/300\n",
      "Average training loss: 0.011227367971506384\n",
      "Average test loss: 0.002245503220914139\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01122384483449989\n",
      "Average test loss: 0.0022508339159604575\n",
      "Epoch 148/300\n",
      "Average training loss: 0.011218694620662266\n",
      "Average test loss: 0.002297777795129352\n",
      "Epoch 149/300\n",
      "Average training loss: 0.011216825811399354\n",
      "Average test loss: 0.002243991471413109\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01120983833571275\n",
      "Average test loss: 0.00225043294702967\n",
      "Epoch 151/300\n",
      "Average training loss: 0.011211046232117546\n",
      "Average test loss: 0.002280904621506731\n",
      "Epoch 152/300\n",
      "Average training loss: 0.011204596177571349\n",
      "Average test loss: 0.0022573588780230944\n",
      "Epoch 153/300\n",
      "Average training loss: 0.011201548997726705\n",
      "Average test loss: 0.002252502425470286\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011191891418562996\n",
      "Average test loss: 0.0022471407569117016\n",
      "Epoch 155/300\n",
      "Average training loss: 0.011196087530089749\n",
      "Average test loss: 0.0022647635495911043\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011191323622233337\n",
      "Average test loss: 0.002265053214298354\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011182276414500342\n",
      "Average test loss: 0.0022527418788522483\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011180425149699053\n",
      "Average test loss: 0.002264462425270014\n",
      "Epoch 159/300\n",
      "Average training loss: 0.011175171438190672\n",
      "Average test loss: 0.002239377328298158\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011172890315453212\n",
      "Average test loss: 0.0022853147232284148\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011166547762023077\n",
      "Average test loss: 0.0022505571127144825\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01116284397823943\n",
      "Average test loss: 0.002252866129701336\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011162196492155394\n",
      "Average test loss: 0.0022551168478611443\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011160132477680842\n",
      "Average test loss: 0.002243596183963948\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0111560425096088\n",
      "Average test loss: 0.002266057290550735\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01115908415781127\n",
      "Average test loss: 0.0022493736272056896\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011143629422618283\n",
      "Average test loss: 0.0022514833060817588\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011151850114266077\n",
      "Average test loss: 0.0022631629523303773\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01114143480029371\n",
      "Average test loss: 0.002265963743958208\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011131957891086737\n",
      "Average test loss: 0.0022963226936343643\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011130129142767854\n",
      "Average test loss: 0.002277777960523963\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011133764625423485\n",
      "Average test loss: 0.002252024781786733\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011132531968255838\n",
      "Average test loss: 0.0022742169407299825\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011120165275202857\n",
      "Average test loss: 0.0022600029032263493\n",
      "Epoch 175/300\n",
      "Average training loss: 0.011128102623754077\n",
      "Average test loss: 0.002258033927737011\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011118926296217574\n",
      "Average test loss: 0.002261343624028895\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011113608334627417\n",
      "Average test loss: 0.002256948141588105\n",
      "Epoch 178/300\n",
      "Average training loss: 0.011110445876088407\n",
      "Average test loss: 0.002254617257664601\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01110330650375949\n",
      "Average test loss: 0.002255093079060316\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01111110429548555\n",
      "Average test loss: 0.002246455781575706\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011102750688791276\n",
      "Average test loss: 0.0022611250782178507\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011103901250494852\n",
      "Average test loss: 0.0022440971315114033\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011102391799290975\n",
      "Average test loss: 0.002279861650326186\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011088333966831366\n",
      "Average test loss: 0.00225451147577001\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011084905587964588\n",
      "Average test loss: 0.00226681391398112\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011086841098964214\n",
      "Average test loss: 0.0022397088640265994\n",
      "Epoch 187/300\n",
      "Average training loss: 0.011088851096729437\n",
      "Average test loss: 0.002253392827179697\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011082207574612564\n",
      "Average test loss: 0.0022754327445808384\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011082005129092269\n",
      "Average test loss: 0.0025092912109361755\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011077096856302685\n",
      "Average test loss: 0.0022763303155079483\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011071939954327212\n",
      "Average test loss: 0.0022477420447394253\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01107147710935937\n",
      "Average test loss: 0.0022567348496781454\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011064984672599368\n",
      "Average test loss: 0.0022603938091132376\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011065118485026889\n",
      "Average test loss: 0.002277192989157306\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011061951757305198\n",
      "Average test loss: 0.002247454232018855\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011066194979680909\n",
      "Average test loss: 0.0022709043874508806\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011053044577439625\n",
      "Average test loss: 0.0022598314327705238\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011058871645894315\n",
      "Average test loss: 0.002292701616262396\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011044579253428511\n",
      "Average test loss: 0.0022780469856742355\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011046914663579729\n",
      "Average test loss: 0.002250346194849246\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011043592267566257\n",
      "Average test loss: 0.002249269767767853\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011050685067143705\n",
      "Average test loss: 0.002261896617917551\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011037407918108835\n",
      "Average test loss: 0.0022532630322708025\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01103213053693374\n",
      "Average test loss: 0.0022648250026007493\n",
      "Epoch 205/300\n",
      "Average training loss: 0.01103596685330073\n",
      "Average test loss: 0.0022533725547707745\n",
      "Epoch 206/300\n",
      "Average training loss: 0.01103422801113791\n",
      "Average test loss: 0.002293112598152624\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011027378963927428\n",
      "Average test loss: 0.002280702698147959\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011025569741096761\n",
      "Average test loss: 0.0022663540569030577\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011023368657463126\n",
      "Average test loss: 0.002551489069643948\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011022612458301915\n",
      "Average test loss: 0.002255515147621433\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0110136346668005\n",
      "Average test loss: 0.0022863559929860965\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011011291174425019\n",
      "Average test loss: 0.002274983922019601\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011012010779645708\n",
      "Average test loss: 0.0023083507772535088\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011010704651888874\n",
      "Average test loss: 0.002299427585055431\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011008489517701997\n",
      "Average test loss: 0.002260489086310069\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011003631864156987\n",
      "Average test loss: 0.003918839431057373\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011001087899009387\n",
      "Average test loss: 0.00225826873174972\n",
      "Epoch 218/300\n",
      "Average training loss: 0.01100363589823246\n",
      "Average test loss: 0.0022918972639987866\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01099725384099616\n",
      "Average test loss: 0.00229302008346551\n",
      "Epoch 220/300\n",
      "Average training loss: 0.010995672281417582\n",
      "Average test loss: 0.0022648601479207477\n",
      "Epoch 221/300\n",
      "Average training loss: 0.010992723080019156\n",
      "Average test loss: 0.0022616541613307262\n",
      "Epoch 222/300\n",
      "Average training loss: 0.010997880321409968\n",
      "Average test loss: 0.0022969840564247635\n",
      "Epoch 223/300\n",
      "Average training loss: 0.010981539493633641\n",
      "Average test loss: 0.0022601080746907326\n",
      "Epoch 224/300\n",
      "Average training loss: 0.010979386453827223\n",
      "Average test loss: 0.0022680803396635586\n",
      "Epoch 225/300\n",
      "Average training loss: 0.010979774395624796\n",
      "Average test loss: 0.0022670425904086893\n",
      "Epoch 226/300\n",
      "Average training loss: 0.010979815406103929\n",
      "Average test loss: 0.00225555779847006\n",
      "Epoch 227/300\n",
      "Average training loss: 0.010975963887241152\n",
      "Average test loss: 0.002310277411714196\n",
      "Epoch 228/300\n",
      "Average training loss: 0.010978699397709634\n",
      "Average test loss: 0.0022563168740727837\n",
      "Epoch 229/300\n",
      "Average training loss: 0.010976818839708965\n",
      "Average test loss: 0.0022980887813286647\n",
      "Epoch 230/300\n",
      "Average training loss: 0.010973707093960708\n",
      "Average test loss: 0.0022716787471953365\n",
      "Epoch 231/300\n",
      "Average training loss: 0.010966491436792744\n",
      "Average test loss: 0.0022802493582583135\n",
      "Epoch 232/300\n",
      "Average training loss: 0.010963866339789496\n",
      "Average test loss: 0.0023318290248927143\n",
      "Epoch 233/300\n",
      "Average training loss: 0.010965347025957372\n",
      "Average test loss: 0.002271581718801624\n",
      "Epoch 234/300\n",
      "Average training loss: 0.010958235586682956\n",
      "Average test loss: 0.0022740214582946563\n",
      "Epoch 235/300\n",
      "Average training loss: 0.010960488890608152\n",
      "Average test loss: 0.0022890500653949047\n",
      "Epoch 236/300\n",
      "Average training loss: 0.010963221076462004\n",
      "Average test loss: 0.0022764548632419773\n",
      "Epoch 237/300\n",
      "Average training loss: 0.010956230899526014\n",
      "Average test loss: 0.0023260947950184347\n",
      "Epoch 238/300\n",
      "Average training loss: 0.010951755974027845\n",
      "Average test loss: 0.0022699310527079636\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01095288403166665\n",
      "Average test loss: 0.0022787111417079966\n",
      "Epoch 240/300\n",
      "Average training loss: 0.010954911946422523\n",
      "Average test loss: 0.002267871033400297\n",
      "Epoch 241/300\n",
      "Average training loss: 0.010945936904185348\n",
      "Average test loss: 0.0022843507857372362\n",
      "Epoch 242/300\n",
      "Average training loss: 0.010944103665649891\n",
      "Average test loss: 0.002272423915978935\n",
      "Epoch 243/300\n",
      "Average training loss: 0.010944939668807718\n",
      "Average test loss: 0.0022680120254970257\n",
      "Epoch 244/300\n",
      "Average training loss: 0.010936443332168791\n",
      "Average test loss: 0.0022707513928827312\n",
      "Epoch 245/300\n",
      "Average training loss: 0.01093664911803272\n",
      "Average test loss: 0.0022746590980225138\n",
      "Epoch 246/300\n",
      "Average training loss: 0.010940168858402306\n",
      "Average test loss: 0.002275139170595341\n",
      "Epoch 247/300\n",
      "Average training loss: 0.010937951274216175\n",
      "Average test loss: 0.002264946635088159\n",
      "Epoch 248/300\n",
      "Average training loss: 0.010950573494864835\n",
      "Average test loss: 0.0023656397114197414\n",
      "Epoch 249/300\n",
      "Average training loss: 0.010931549381050799\n",
      "Average test loss: 0.0022641588631603453\n",
      "Epoch 250/300\n",
      "Average training loss: 0.010915945072140958\n",
      "Average test loss: 0.0022731070761672326\n",
      "Epoch 251/300\n",
      "Average training loss: 0.010932050812575552\n",
      "Average test loss: 0.0022802974780400595\n",
      "Epoch 252/300\n",
      "Average training loss: 0.010924077571266228\n",
      "Average test loss: 0.002296393443726831\n",
      "Epoch 253/300\n",
      "Average training loss: 0.010919776397446792\n",
      "Average test loss: 0.0023238427489995954\n",
      "Epoch 254/300\n",
      "Average training loss: 0.010920275508529611\n",
      "Average test loss: 0.002313455045637157\n",
      "Epoch 255/300\n",
      "Average training loss: 0.010921525442765818\n",
      "Average test loss: 0.002311677493361963\n",
      "Epoch 256/300\n",
      "Average training loss: 0.010914267677399848\n",
      "Average test loss: 0.0022961436881580284\n",
      "Epoch 257/300\n",
      "Average training loss: 0.010914234024782976\n",
      "Average test loss: 0.0022734619699832466\n",
      "Epoch 258/300\n",
      "Average training loss: 0.010924189153644773\n",
      "Average test loss: 0.0022701193141854473\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01091393705457449\n",
      "Average test loss: 0.002293494487181306\n",
      "Epoch 260/300\n",
      "Average training loss: 0.010907344075540702\n",
      "Average test loss: 0.002341908316852318\n",
      "Epoch 261/300\n",
      "Average training loss: 0.010905007084210714\n",
      "Average test loss: 0.0022753002698429757\n",
      "Epoch 262/300\n",
      "Average training loss: 0.010905145613683595\n",
      "Average test loss: 0.002303912054747343\n",
      "Epoch 263/300\n",
      "Average training loss: 0.010898139351771937\n",
      "Average test loss: 0.0022703541368246077\n",
      "Epoch 264/300\n",
      "Average training loss: 0.010893619540664885\n",
      "Average test loss: 0.002267621535807848\n",
      "Epoch 265/300\n",
      "Average training loss: 0.010894629747503333\n",
      "Average test loss: 0.002310843590853943\n",
      "Epoch 266/300\n",
      "Average training loss: 0.010892613665925132\n",
      "Average test loss: 0.00227815173069636\n",
      "Epoch 267/300\n",
      "Average training loss: 0.010900728546082974\n",
      "Average test loss: 0.002274515204338564\n",
      "Epoch 268/300\n",
      "Average training loss: 0.010886153982331356\n",
      "Average test loss: 0.0023073100960916944\n",
      "Epoch 269/300\n",
      "Average training loss: 0.010892826954523722\n",
      "Average test loss: 0.0022802404074205294\n",
      "Epoch 270/300\n",
      "Average training loss: 0.010891665619280603\n",
      "Average test loss: 0.0022742213853117494\n",
      "Epoch 271/300\n",
      "Average training loss: 0.010892013198385637\n",
      "Average test loss: 0.0023061265624645683\n",
      "Epoch 272/300\n",
      "Average training loss: 0.010888823467824195\n",
      "Average test loss: 0.002270148589896659\n",
      "Epoch 273/300\n",
      "Average training loss: 0.010886050189534823\n",
      "Average test loss: 0.0023010374042722916\n",
      "Epoch 274/300\n",
      "Average training loss: 0.010883081342611048\n",
      "Average test loss: 0.0023289701725459763\n",
      "Epoch 275/300\n",
      "Average training loss: 0.010881646945244737\n",
      "Average test loss: 0.0022916697102288407\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01087199480500486\n",
      "Average test loss: 0.0022860542649610176\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01087186732971006\n",
      "Average test loss: 0.0022728250737612445\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0108745465974013\n",
      "Average test loss: 0.002282842700680097\n",
      "Epoch 279/300\n",
      "Average training loss: 0.010874828153186375\n",
      "Average test loss: 0.0022943457356757587\n",
      "Epoch 280/300\n",
      "Average training loss: 0.010874815648628605\n",
      "Average test loss: 0.002268268147690429\n",
      "Epoch 281/300\n",
      "Average training loss: 0.010868529113630454\n",
      "Average test loss: 0.0022967444225731825\n",
      "Epoch 282/300\n",
      "Average training loss: 0.010870471522212028\n",
      "Average test loss: 0.002358163669705391\n",
      "Epoch 283/300\n",
      "Average training loss: 0.010870549698670705\n",
      "Average test loss: 0.0022829699779136313\n",
      "Epoch 284/300\n",
      "Average training loss: 0.010863972560399109\n",
      "Average test loss: 0.0022867619360072747\n",
      "Epoch 285/300\n",
      "Average training loss: 0.010857136799229516\n",
      "Average test loss: 0.0022818184652262266\n",
      "Epoch 286/300\n",
      "Average training loss: 0.010854201353258557\n",
      "Average test loss: 0.0022740928816298643\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01086018813153108\n",
      "Average test loss: 0.0023118860305597383\n",
      "Epoch 288/300\n",
      "Average training loss: 0.010861839449240102\n",
      "Average test loss: 0.002287038140412834\n",
      "Epoch 289/300\n",
      "Average training loss: 0.010858424572481049\n",
      "Average test loss: 0.002293212448867659\n",
      "Epoch 290/300\n",
      "Average training loss: 0.010850908358063963\n",
      "Average test loss: 0.0022743814210924836\n",
      "Epoch 291/300\n",
      "Average training loss: 0.010859159234497283\n",
      "Average test loss: 0.002321315833263927\n",
      "Epoch 292/300\n",
      "Average training loss: 0.010858824554416868\n",
      "Average test loss: 0.00229237475235843\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01084638212290075\n",
      "Average test loss: 0.002296552770046724\n",
      "Epoch 294/300\n",
      "Average training loss: 0.010849477796918815\n",
      "Average test loss: 0.0023115113602123327\n",
      "Epoch 295/300\n",
      "Average training loss: 0.010843860099713007\n",
      "Average test loss: 0.0022973718690789406\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01084745136068927\n",
      "Average test loss: 0.002267391896703177\n",
      "Epoch 297/300\n",
      "Average training loss: 0.010843072993887795\n",
      "Average test loss: 0.0022699329217688905\n",
      "Epoch 298/300\n",
      "Average training loss: 0.010843843151297834\n",
      "Average test loss: 0.002286052570057412\n",
      "Epoch 299/300\n",
      "Average training loss: 0.010839227441698313\n",
      "Average test loss: 0.0023017451787988347\n",
      "Epoch 300/300\n",
      "Average training loss: 0.010839011373619239\n",
      "Average test loss: 0.0023405452754555476\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive_Depth3/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.54\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.94\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.55\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.82\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.27\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.38\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7521181116501491\n",
      "Average test loss: 0.005660811312910584\n",
      "Epoch 2/300\n",
      "Average training loss: 0.11475997622807821\n",
      "Average test loss: 0.005311882080303298\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08599159907632403\n",
      "Average test loss: 0.00492602802771661\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07895877858002981\n",
      "Average test loss: 0.004807000327441428\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07563083941406674\n",
      "Average test loss: 0.004731182500720024\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07353039296468099\n",
      "Average test loss: 0.004725118663575914\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07204718017578125\n",
      "Average test loss: 0.004624259297632508\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07092402279045847\n",
      "Average test loss: 0.004614567860133118\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06998348840077719\n",
      "Average test loss: 0.00465303976005978\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0692577649752299\n",
      "Average test loss: 0.004528787720534537\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06867469225327173\n",
      "Average test loss: 0.004510338883019156\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06815007825361358\n",
      "Average test loss: 0.00450408476880855\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0677004325720999\n",
      "Average test loss: 0.004467821195100745\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06729925047357876\n",
      "Average test loss: 0.004476824560099178\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06694225166241329\n",
      "Average test loss: 0.00445304264769786\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06664875841803021\n",
      "Average test loss: 0.004410163346264097\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06635194507241249\n",
      "Average test loss: 0.004422594017038743\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06608626246783468\n",
      "Average test loss: 0.004430260717868805\n",
      "Epoch 19/300\n",
      "Average training loss: 0.06584721949365403\n",
      "Average test loss: 0.004386861113210519\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06560608379046122\n",
      "Average test loss: 0.004361604154523876\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06549887903200255\n",
      "Average test loss: 0.00435798757771651\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06530017141169972\n",
      "Average test loss: 0.004424731965487202\n",
      "Epoch 23/300\n",
      "Average training loss: 0.065119581606653\n",
      "Average test loss: 0.004346101739754279\n",
      "Epoch 24/300\n",
      "Average training loss: 0.06492386062277689\n",
      "Average test loss: 0.004330015692032046\n",
      "Epoch 25/300\n",
      "Average training loss: 0.06478404872616132\n",
      "Average test loss: 0.0043463576895495255\n",
      "Epoch 26/300\n",
      "Average training loss: 0.06465415879090627\n",
      "Average test loss: 0.004317096237921052\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06449416058262189\n",
      "Average test loss: 0.004311577984028392\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06434643246730169\n",
      "Average test loss: 0.004298300159060292\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0642265326778094\n",
      "Average test loss: 0.00429372802335355\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0640748730202516\n",
      "Average test loss: 0.004299418524942465\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06398882672521804\n",
      "Average test loss: 0.004304379964040385\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0638617163333628\n",
      "Average test loss: 0.0042794979549944405\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06375708216759894\n",
      "Average test loss: 0.004275804052543309\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06368482471174663\n",
      "Average test loss: 0.004314937129616737\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06353723325663142\n",
      "Average test loss: 0.004257839697930548\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06349456370207998\n",
      "Average test loss: 0.004246079866256979\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06339076345496708\n",
      "Average test loss: 0.004256011507991288\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06332122996118333\n",
      "Average test loss: 0.004251042550429702\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06322152420547274\n",
      "Average test loss: 0.004236581332153744\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06318723802765211\n",
      "Average test loss: 0.004243267408675618\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06305819387899504\n",
      "Average test loss: 0.0042538497183058\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06303188127941556\n",
      "Average test loss: 0.004240320763654179\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06295330486363834\n",
      "Average test loss: 0.004224033760113849\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06289566606614325\n",
      "Average test loss: 0.004216291565034125\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0628502827319834\n",
      "Average test loss: 0.004240738759852118\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06280913985768954\n",
      "Average test loss: 0.0042365642930898405\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06272454659475221\n",
      "Average test loss: 0.0042266478559209245\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0626865395738019\n",
      "Average test loss: 0.004220702360798087\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06260428254471885\n",
      "Average test loss: 0.0042194600254297256\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06255176311400201\n",
      "Average test loss: 0.004221935934904549\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06251601701478163\n",
      "Average test loss: 0.004204052680275507\n",
      "Epoch 52/300\n",
      "Average training loss: 0.062446725421481664\n",
      "Average test loss: 0.004240762989140219\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06242183917760849\n",
      "Average test loss: 0.004210871155891154\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06236489153901736\n",
      "Average test loss: 0.004221890807151794\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06230773426426781\n",
      "Average test loss: 0.004213130760937929\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06230451690157254\n",
      "Average test loss: 0.004188982986327675\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06222761126690441\n",
      "Average test loss: 0.004196333708655503\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06220120753182305\n",
      "Average test loss: 0.0042043249437378515\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06214756471580929\n",
      "Average test loss: 0.004208490082787143\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06210982017384635\n",
      "Average test loss: 0.004204427621430821\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06211338141229417\n",
      "Average test loss: 0.004203595707399977\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06200921749737528\n",
      "Average test loss: 0.004198677440484365\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06197273721959856\n",
      "Average test loss: 0.004198574046707816\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06199624972873264\n",
      "Average test loss: 0.0041864826194942\n",
      "Epoch 65/300\n",
      "Average training loss: 0.061906257606214946\n",
      "Average test loss: 0.004727381176004807\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06189590895175934\n",
      "Average test loss: 0.004197173152325882\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06185525132218997\n",
      "Average test loss: 0.004191333532747295\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06178302393356959\n",
      "Average test loss: 0.004211777096407281\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0617555410861969\n",
      "Average test loss: 0.00418138746254974\n",
      "Epoch 70/300\n",
      "Average training loss: 0.061743619826104906\n",
      "Average test loss: 0.004179480978391237\n",
      "Epoch 71/300\n",
      "Average training loss: 0.061718809445699054\n",
      "Average test loss: 0.004186635960307386\n",
      "Epoch 72/300\n",
      "Average training loss: 0.061668224089675476\n",
      "Average test loss: 0.004194260603644782\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06163203539119826\n",
      "Average test loss: 0.004193346814148956\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06159227987792757\n",
      "Average test loss: 0.004205340207864841\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06154501262969441\n",
      "Average test loss: 0.004220615590198172\n",
      "Epoch 76/300\n",
      "Average training loss: 0.061545870294173556\n",
      "Average test loss: 0.004194539674868186\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06147906198104223\n",
      "Average test loss: 0.004214578829705715\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06142275196644995\n",
      "Average test loss: 0.0041892872982554966\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06141109916236666\n",
      "Average test loss: 0.004215753681543801\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06139553663465712\n",
      "Average test loss: 0.004188133975697888\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06133475745717684\n",
      "Average test loss: 0.004179455254226923\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06132654905650351\n",
      "Average test loss: 0.004195381643664506\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06130181122157309\n",
      "Average test loss: 0.004201110498358806\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06126249317990409\n",
      "Average test loss: 0.0043786391102605396\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06120257464382384\n",
      "Average test loss: 0.004341467292358478\n",
      "Epoch 86/300\n",
      "Average training loss: 0.061186861604452136\n",
      "Average test loss: 0.004176978244549699\n",
      "Epoch 87/300\n",
      "Average training loss: 0.061099533018138676\n",
      "Average test loss: 0.004277490658064684\n",
      "Epoch 88/300\n",
      "Average training loss: 0.0610874471200837\n",
      "Average test loss: 0.004217736266553402\n",
      "Epoch 89/300\n",
      "Average training loss: 0.061073067595561345\n",
      "Average test loss: 0.004216023641741938\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06102212589316898\n",
      "Average test loss: 0.0041969851375454\n",
      "Epoch 91/300\n",
      "Average training loss: 0.060983735505077574\n",
      "Average test loss: 0.004190049735208352\n",
      "Epoch 92/300\n",
      "Average training loss: 0.060958820939064025\n",
      "Average test loss: 0.004184740265210469\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06090484711527824\n",
      "Average test loss: 0.0041840208876464105\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06090560222334332\n",
      "Average test loss: 0.004178451888677146\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06082843022214042\n",
      "Average test loss: 0.004198113319360547\n",
      "Epoch 96/300\n",
      "Average training loss: 0.060688941283358465\n",
      "Average test loss: 0.0041842948819200195\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06066972705721855\n",
      "Average test loss: 0.004220846915410624\n",
      "Epoch 102/300\n",
      "Average training loss: 0.06062155182825194\n",
      "Average test loss: 0.004196800770031081\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06057646034823524\n",
      "Average test loss: 0.004193634417321947\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06054451932840877\n",
      "Average test loss: 0.004246531417179439\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06049664150675138\n",
      "Average test loss: 0.004202179472272595\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06046442780892054\n",
      "Average test loss: 0.004186038885472549\n",
      "Epoch 107/300\n",
      "Average training loss: 0.060427056865559686\n",
      "Average test loss: 0.0042155312034818865\n",
      "Epoch 108/300\n",
      "Average training loss: 0.06038828428586324\n",
      "Average test loss: 0.004201778466916746\n",
      "Epoch 109/300\n",
      "Average training loss: 0.06037771528297001\n",
      "Average test loss: 0.004206619439439641\n",
      "Epoch 110/300\n",
      "Average training loss: 0.06033235180046823\n",
      "Average test loss: 0.004189412938223945\n",
      "Epoch 111/300\n",
      "Average training loss: 0.06030138525366783\n",
      "Average test loss: 0.00421936379911171\n",
      "Epoch 112/300\n",
      "Average training loss: 0.060277655439244376\n",
      "Average test loss: 0.004199935649211208\n",
      "Epoch 113/300\n",
      "Average training loss: 0.060221710751454036\n",
      "Average test loss: 0.00418265019191636\n",
      "Epoch 114/300\n",
      "Average training loss: 0.060214168167776534\n",
      "Average test loss: 0.004192736235550708\n",
      "Epoch 115/300\n",
      "Average training loss: 0.060156212591462664\n",
      "Average test loss: 0.0042076419124172796\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0601150970823235\n",
      "Average test loss: 0.004210826496283213\n",
      "Epoch 117/300\n",
      "Average training loss: 0.06007161601715618\n",
      "Average test loss: 0.004180901674346791\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06004377170072662\n",
      "Average test loss: 0.004189061633000772\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06002072203821606\n",
      "Average test loss: 0.004255374269146058\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05994784327016937\n",
      "Average test loss: 0.004207907512370083\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05990604212217861\n",
      "Average test loss: 0.004227544720388121\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05992144113779068\n",
      "Average test loss: 0.0042421366257799995\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05986471551325586\n",
      "Average test loss: 0.004193869838284122\n",
      "Epoch 124/300\n",
      "Average training loss: 0.059816695872280334\n",
      "Average test loss: 0.004221425093296502\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05979059649838342\n",
      "Average test loss: 0.0041981797727445765\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05964679668015904\n",
      "Average test loss: 0.004236510425392124\n",
      "Epoch 130/300\n",
      "Average training loss: 0.059571737769577235\n",
      "Average test loss: 0.004251694154822164\n",
      "Epoch 131/300\n",
      "Average training loss: 0.059564115424950916\n",
      "Average test loss: 0.004216840881647335\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05950529886947738\n",
      "Average test loss: 0.004217393725696537\n",
      "Epoch 133/300\n",
      "Average training loss: 0.059454086661338806\n",
      "Average test loss: 0.00422987282793555\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05947821773423089\n",
      "Average test loss: 0.0042204264069183005\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05938296627667215\n",
      "Average test loss: 0.0042532672360539436\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05937080481648445\n",
      "Average test loss: 0.004263460499958859\n",
      "Epoch 137/300\n",
      "Average training loss: 0.059345518502924174\n",
      "Average test loss: 0.004265707111193074\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05926464050014814\n",
      "Average test loss: 0.0042360130374630295\n",
      "Epoch 139/300\n",
      "Average training loss: 0.059262071784999634\n",
      "Average test loss: 0.004243676213754548\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0592325114177333\n",
      "Average test loss: 0.0043047178350389004\n",
      "Epoch 141/300\n",
      "Average training loss: 0.059161553015311556\n",
      "Average test loss: 0.004219000173111756\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05917914001478089\n",
      "Average test loss: 0.004254546320272817\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05914437903298272\n",
      "Average test loss: 0.0042699115375677745\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05909662993417846\n",
      "Average test loss: 0.004223314793573485\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05903959851132499\n",
      "Average test loss: 0.004254249476310279\n",
      "Epoch 146/300\n",
      "Average training loss: 0.058966691030396354\n",
      "Average test loss: 0.0042391745212177435\n",
      "Epoch 147/300\n",
      "Average training loss: 0.058966104679637484\n",
      "Average test loss: 0.004226514344414075\n",
      "Epoch 148/300\n",
      "Average training loss: 0.058914868745538924\n",
      "Average test loss: 0.004252988802062141\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05890438618262609\n",
      "Average test loss: 0.0042478862814605235\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05884650402267774\n",
      "Average test loss: 0.004284728063477411\n",
      "Epoch 151/300\n",
      "Average training loss: 0.058804170787334444\n",
      "Average test loss: 0.004268626932468679\n",
      "Epoch 152/300\n",
      "Average training loss: 0.058810640272166995\n",
      "Average test loss: 0.004245422059877052\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05873315834005674\n",
      "Average test loss: 0.004359937234471241\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05869983496930864\n",
      "Average test loss: 0.004270776142676671\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05863712467418777\n",
      "Average test loss: 0.004251315926926003\n",
      "Epoch 156/300\n",
      "Average training loss: 0.058632616562975776\n",
      "Average test loss: 0.004342198691848252\n",
      "Epoch 157/300\n",
      "Average training loss: 0.058612967285845016\n",
      "Average test loss: 0.004307183844347795\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0585840796497133\n",
      "Average test loss: 0.004285466910029451\n",
      "Epoch 159/300\n",
      "Average training loss: 0.058535555710395176\n",
      "Average test loss: 0.0042578858871840765\n",
      "Epoch 160/300\n",
      "Average training loss: 0.058539402931928636\n",
      "Average test loss: 0.004301935815355844\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05847206276655197\n",
      "Average test loss: 0.004326407734097706\n",
      "Epoch 162/300\n",
      "Average training loss: 0.058455038802491296\n",
      "Average test loss: 0.004268991326085395\n",
      "Epoch 163/300\n",
      "Average training loss: 0.058425208975871404\n",
      "Average test loss: 0.004277046977542341\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05833465953005685\n",
      "Average test loss: 0.004290195080969068\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05833418659369151\n",
      "Average test loss: 0.004345530236967736\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05827295315596792\n",
      "Average test loss: 0.004252921783261829\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05826979820926984\n",
      "Average test loss: 0.004414758123043511\n",
      "Epoch 168/300\n",
      "Average training loss: 0.058224462625053194\n",
      "Average test loss: 0.004621735128677554\n",
      "Epoch 169/300\n",
      "Average training loss: 0.058206548853052986\n",
      "Average test loss: 0.004284361231244272\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05815360965993669\n",
      "Average test loss: 0.004292304137514697\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0581651424003972\n",
      "Average test loss: 0.0042701063719060685\n",
      "Epoch 172/300\n",
      "Average training loss: 0.058079342246055606\n",
      "Average test loss: 0.004285925781561268\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0580503011710114\n",
      "Average test loss: 0.0042795379762020375\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05803694197866652\n",
      "Average test loss: 0.004309720871763097\n",
      "Epoch 175/300\n",
      "Average training loss: 0.057981120995349356\n",
      "Average test loss: 0.004332757413387299\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05784014389912288\n",
      "Average test loss: 0.004335091672423813\n",
      "Epoch 181/300\n",
      "Average training loss: 0.057824233028623793\n",
      "Average test loss: 0.004353038762592607\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0577412930727005\n",
      "Average test loss: 0.004265626650924484\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05767051046093305\n",
      "Average test loss: 0.004347354139718745\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05771273110310236\n",
      "Average test loss: 0.004333094763259093\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05769423334466087\n",
      "Average test loss: 0.004416054602505432\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05760865405864186\n",
      "Average test loss: 0.004306747756898403\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05773796250091659\n",
      "Average test loss: 0.004274676088450683\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0576174583070808\n",
      "Average test loss: 0.004278298712439007\n",
      "Epoch 189/300\n",
      "Average training loss: 0.057517879664897915\n",
      "Average test loss: 0.00431880453767048\n",
      "Epoch 190/300\n",
      "Average training loss: 0.057510450710852944\n",
      "Average test loss: 0.004300446495413781\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05751697258485688\n",
      "Average test loss: 0.00433354264125228\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05744736203882429\n",
      "Average test loss: 0.004301700838324096\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05743544417288568\n",
      "Average test loss: 0.004307408350209395\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05738782984349463\n",
      "Average test loss: 0.004321713669846456\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05736513192786111\n",
      "Average test loss: 0.004339100989409619\n",
      "Epoch 196/300\n",
      "Average training loss: 0.057377128092779055\n",
      "Average test loss: 0.004370892459733619\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05730074402689934\n",
      "Average test loss: 0.004329593357526594\n",
      "Epoch 198/300\n",
      "Average training loss: 0.057367267863618\n",
      "Average test loss: 0.004385369413842758\n",
      "Epoch 199/300\n",
      "Average training loss: 0.057242817706531945\n",
      "Average test loss: 0.004288855137924354\n",
      "Epoch 200/300\n",
      "Average training loss: 0.057206053031815425\n",
      "Average test loss: 0.004339381915620632\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0571443134645621\n",
      "Average test loss: 0.004384908384333054\n",
      "Epoch 205/300\n",
      "Average training loss: 0.057049797455469765\n",
      "Average test loss: 0.004362514081100623\n",
      "Epoch 206/300\n",
      "Average training loss: 0.057079498370488486\n",
      "Average test loss: 0.00434721432502071\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05703543073270056\n",
      "Average test loss: 0.004385902585461736\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05703035168846448\n",
      "Average test loss: 0.0043261057128094966\n",
      "Epoch 209/300\n",
      "Average training loss: 0.056991957637998795\n",
      "Average test loss: 0.0043346237676839035\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05702375508679284\n",
      "Average test loss: 0.0042892736461427475\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05695519306593471\n",
      "Average test loss: 0.004345888930269414\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05692359190517002\n",
      "Average test loss: 0.004367957391258743\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0569120023018784\n",
      "Average test loss: 0.004365302753531271\n",
      "Epoch 214/300\n",
      "Average training loss: 0.05682542888323466\n",
      "Average test loss: 0.004355791152765354\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05682375970813963\n",
      "Average test loss: 0.004410125631011195\n",
      "Epoch 216/300\n",
      "Average training loss: 0.05679320158561071\n",
      "Average test loss: 0.004369877213405238\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05677546887927585\n",
      "Average test loss: 0.004311913036223915\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05671896766953998\n",
      "Average test loss: 0.004341315960304605\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05671020809809367\n",
      "Average test loss: 0.004322742843793498\n",
      "Epoch 220/300\n",
      "Average training loss: 0.0566690537598398\n",
      "Average test loss: 0.004375057508548101\n",
      "Epoch 221/300\n",
      "Average training loss: 0.056714246365759106\n",
      "Average test loss: 0.004331584042766028\n",
      "Epoch 222/300\n",
      "Average training loss: 0.056663143634796145\n",
      "Average test loss: 0.0043209887490504315\n",
      "Epoch 223/300\n",
      "Average training loss: 0.05667321548528141\n",
      "Average test loss: 0.004411970876571205\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05659369230601523\n",
      "Average test loss: 0.004331129339420133\n",
      "Epoch 225/300\n",
      "Average training loss: 0.056533019489712184\n",
      "Average test loss: 0.004331351586307089\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05657782537407345\n",
      "Average test loss: 0.004329616676602099\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05649666743808322\n",
      "Average test loss: 0.004369997682670752\n",
      "Epoch 228/300\n",
      "Average training loss: 0.056558455036746134\n",
      "Average test loss: 0.00440239723233713\n",
      "Epoch 229/300\n",
      "Average training loss: 0.056498490495814215\n",
      "Average test loss: 0.0044174995575514105\n",
      "Epoch 230/300\n",
      "Average training loss: 0.056442370103465186\n",
      "Average test loss: 0.004378610856417152\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05647345716423458\n",
      "Average test loss: 0.004372327062404818\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05641119303968218\n",
      "Average test loss: 0.0044869030604345935\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05640471723675728\n",
      "Average test loss: 0.004331914684838719\n",
      "Epoch 234/300\n",
      "Average training loss: 0.05630644169449806\n",
      "Average test loss: 0.004336333663720223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05630568743083212\n",
      "Average test loss: 0.004416488562193181\n",
      "Epoch 238/300\n",
      "Average training loss: 0.056292857286002904\n",
      "Average test loss: 0.004368088942021131\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05623189420832528\n",
      "Average test loss: 0.004399452243828111\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05624769576390584\n",
      "Average test loss: 0.004412090247703923\n",
      "Epoch 241/300\n",
      "Average training loss: 0.05624825563033422\n",
      "Average test loss: 0.004413319990866714\n",
      "Epoch 242/300\n",
      "Average training loss: 0.056209442691670525\n",
      "Average test loss: 0.0044208844867017535\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05616806760099199\n",
      "Average test loss: 0.004400275449372\n",
      "Epoch 244/300\n",
      "Average training loss: 0.056198997067080605\n",
      "Average test loss: 0.004513596140262153\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05608944567044576\n",
      "Average test loss: 0.0044056664941211545\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05607081679171986\n",
      "Average test loss: 0.004460984001557033\n",
      "Epoch 247/300\n",
      "Average training loss: 0.056229227503140765\n",
      "Average test loss: 0.004368311212708553\n",
      "Epoch 248/300\n",
      "Average training loss: 0.056019341160853706\n",
      "Average test loss: 0.004373082115418381\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05607303125990762\n",
      "Average test loss: 0.004493801221251488\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05600780027442508\n",
      "Average test loss: 0.00435411159776979\n",
      "Epoch 251/300\n",
      "Average training loss: 0.056063880072699655\n",
      "Average test loss: 0.00432866603260239\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05596109899878502\n",
      "Average test loss: 0.004398618932399485\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05594188372294108\n",
      "Average test loss: 0.004448657305704223\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05592991046441926\n",
      "Average test loss: 0.029610997539427544\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0559484684434202\n",
      "Average test loss: 0.004372282785053055\n",
      "Epoch 256/300\n",
      "Average training loss: 0.055921881361140145\n",
      "Average test loss: 0.004446499111751715\n",
      "Epoch 257/300\n",
      "Average training loss: 0.055815625382794276\n",
      "Average test loss: 0.004394919726583693\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05592827710178163\n",
      "Average test loss: 0.004509228281676769\n",
      "Epoch 259/300\n",
      "Average training loss: 0.055824027912484274\n",
      "Average test loss: 0.004855514092577828\n",
      "Epoch 260/300\n",
      "Average training loss: 0.055852463589774234\n",
      "Average test loss: 0.0044458798468112945\n",
      "Epoch 261/300\n",
      "Average training loss: 0.055823015415006214\n",
      "Average test loss: 0.0043716603608595\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05574102036158244\n",
      "Average test loss: 0.00434400943480432\n",
      "Epoch 263/300\n",
      "Average training loss: 0.055775850736432604\n",
      "Average test loss: 0.004372979188130961\n",
      "Epoch 264/300\n",
      "Average training loss: 0.055755178680022555\n",
      "Average test loss: 0.004438100741555293\n",
      "Epoch 265/300\n",
      "Average training loss: 0.055731694638729096\n",
      "Average test loss: 0.004462432832767566\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0556727757718828\n",
      "Average test loss: 0.004507989352362024\n",
      "Epoch 267/300\n",
      "Average training loss: 0.055652187327543894\n",
      "Average test loss: 0.00434967810039719\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05564999039636718\n",
      "Average test loss: 0.004439755972888735\n",
      "Epoch 269/300\n",
      "Average training loss: 0.055717713753382365\n",
      "Average test loss: 0.004529143027133412\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05562384773625268\n",
      "Average test loss: 0.004441270846666561\n",
      "Epoch 271/300\n",
      "Average training loss: 0.055631414973073536\n",
      "Average test loss: 0.004362375656349791\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05563740335570441\n",
      "Average test loss: 0.004458601004961464\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05560445539487733\n",
      "Average test loss: 0.0044511803359621105\n",
      "Epoch 274/300\n",
      "Average training loss: 0.055512229644589954\n",
      "Average test loss: 0.004358390920485059\n",
      "Epoch 275/300\n",
      "Average training loss: 0.055491079956293106\n",
      "Average test loss: 0.004457164214717017\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05562040543556213\n",
      "Average test loss: 0.00443353638280597\n",
      "Epoch 277/300\n",
      "Average training loss: 0.055479222228129704\n",
      "Average test loss: 0.004365280090106858\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05549301755759451\n",
      "Average test loss: 0.004443158869114187\n",
      "Epoch 279/300\n",
      "Average training loss: 0.055447325597206755\n",
      "Average test loss: 0.0044579602326783865\n",
      "Epoch 280/300\n",
      "Average training loss: 0.055481372465689974\n",
      "Average test loss: 0.0044028327191869415\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05547633330027262\n",
      "Average test loss: 0.0044349136756112175\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05540692412853241\n",
      "Average test loss: 0.004419569690608316\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05536575929986106\n",
      "Average test loss: 0.004491251055565145\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05535609612862269\n",
      "Average test loss: 0.004440830166969035\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05535320327679316\n",
      "Average test loss: 0.004401108703679509\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05536556904845768\n",
      "Average test loss: 0.004394195687025785\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05534061941173342\n",
      "Average test loss: 0.004478418704950147\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05530645610557662\n",
      "Average test loss: 0.004388184210078584\n",
      "Epoch 289/300\n",
      "Average training loss: 0.055267606404092576\n",
      "Average test loss: 0.004446101546287537\n",
      "Epoch 290/300\n",
      "Average training loss: 0.055335005902581745\n",
      "Average test loss: 0.004361315990073813\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05525063427289327\n",
      "Average test loss: 0.004376450046896935\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05526815938287311\n",
      "Average test loss: 0.004458373671604527\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05519528117113643\n",
      "Average test loss: 0.004545801629001895\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05521248269081116\n",
      "Average test loss: 0.004470730980237325\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05521842689977752\n",
      "Average test loss: 0.004402155831042263\n",
      "Epoch 296/300\n",
      "Average training loss: 0.055177930785550014\n",
      "Average test loss: 0.004438522383984592\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0551792262825701\n",
      "Average test loss: 0.004537258231391509\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05517310571670532\n",
      "Average test loss: 0.004465557882355319\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05509608671731419\n",
      "Average test loss: 0.004478593635476298\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05514148222737842\n",
      "Average test loss: 0.0044448112429430085\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7795074118640688\n",
      "Average test loss: 0.00562299045547843\n",
      "Epoch 2/300\n",
      "Average training loss: 0.10344298013713625\n",
      "Average test loss: 0.004912710566073656\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08279539241393408\n",
      "Average test loss: 0.004716319310582346\n",
      "Epoch 4/300\n",
      "Average training loss: 0.07534393335051007\n",
      "Average test loss: 0.004519013713217444\n",
      "Epoch 5/300\n",
      "Average training loss: 0.07116844205061594\n",
      "Average test loss: 0.004427592122720348\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06848399435811572\n",
      "Average test loss: 0.004348621008296807\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06654998542533981\n",
      "Average test loss: 0.004335492950346735\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06509730819861094\n",
      "Average test loss: 0.004165345264184806\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06388501917322477\n",
      "Average test loss: 0.0041412965688440535\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06282818077339067\n",
      "Average test loss: 0.004060441079239051\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06191169872548845\n",
      "Average test loss: 0.004008974730140633\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0610551402370135\n",
      "Average test loss: 0.003959236757622824\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0603240478667948\n",
      "Average test loss: 0.0039132478005356255\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05962716641359859\n",
      "Average test loss: 0.0038700366736286215\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05905099834336175\n",
      "Average test loss: 0.003887395153443019\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05841151240136888\n",
      "Average test loss: 0.003839821827494436\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05793080130881734\n",
      "Average test loss: 0.0037596335998839802\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05749395585060119\n",
      "Average test loss: 0.003743795720446441\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05697090732720163\n",
      "Average test loss: 0.003748707203815381\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05658494570685758\n",
      "Average test loss: 0.0036881165601727033\n",
      "Epoch 21/300\n",
      "Average training loss: 0.056214856485525766\n",
      "Average test loss: 0.0037076357623769176\n",
      "Epoch 22/300\n",
      "Average training loss: 0.055865986267725626\n",
      "Average test loss: 0.0037425945951706833\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0555257145497534\n",
      "Average test loss: 0.0036440899078216816\n",
      "Epoch 24/300\n",
      "Average training loss: 0.05520707045661079\n",
      "Average test loss: 0.0036630973449597757\n",
      "Epoch 25/300\n",
      "Average training loss: 0.054945975446038776\n",
      "Average test loss: 0.0036318529264794458\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05469537312620216\n",
      "Average test loss: 0.003645633922682868\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05438094035453266\n",
      "Average test loss: 0.003565461983283361\n",
      "Epoch 28/300\n",
      "Average training loss: 0.054190983768966465\n",
      "Average test loss: 0.003589187314733863\n",
      "Epoch 29/300\n",
      "Average training loss: 0.05396787134806315\n",
      "Average test loss: 0.0035900439044667617\n",
      "Epoch 30/300\n",
      "Average training loss: 0.05379472932881779\n",
      "Average test loss: 0.003563696878030896\n",
      "Epoch 31/300\n",
      "Average training loss: 0.0535767140686512\n",
      "Average test loss: 0.003606341107024087\n",
      "Epoch 32/300\n",
      "Average training loss: 0.05341143040855725\n",
      "Average test loss: 0.003515042259875271\n",
      "Epoch 33/300\n",
      "Average training loss: 0.053234816686974634\n",
      "Average test loss: 0.0035244088363316326\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05307112504045169\n",
      "Average test loss: 0.0035091741799066463\n",
      "Epoch 35/300\n",
      "Average training loss: 0.05290317902300093\n",
      "Average test loss: 0.0035089917559590605\n",
      "Epoch 36/300\n",
      "Average training loss: 0.052766889648305046\n",
      "Average test loss: 0.003529537515093883\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05264495000574324\n",
      "Average test loss: 0.003480611169503795\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05250413467817836\n",
      "Average test loss: 0.003499359080154035\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05239918686118391\n",
      "Average test loss: 0.0034894172770695555\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05227771296766069\n",
      "Average test loss: 0.0034889543135133054\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05216243391235669\n",
      "Average test loss: 0.003487140331003401\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05207032122876909\n",
      "Average test loss: 0.0034632188042418824\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05194359238611327\n",
      "Average test loss: 0.0034573876841200724\n",
      "Epoch 44/300\n",
      "Average training loss: 0.05184862879580922\n",
      "Average test loss: 0.003488938478132089\n",
      "Epoch 45/300\n",
      "Average training loss: 0.051734468460083005\n",
      "Average test loss: 0.0034462965834471913\n",
      "Epoch 46/300\n",
      "Average training loss: 0.051613213863637715\n",
      "Average test loss: 0.0034516271946744784\n",
      "Epoch 47/300\n",
      "Average training loss: 0.05155713406867451\n",
      "Average test loss: 0.003455880009672708\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05144656792283058\n",
      "Average test loss: 0.0034485977391401925\n",
      "Epoch 49/300\n",
      "Average training loss: 0.0513839505745305\n",
      "Average test loss: 0.003444087867314617\n",
      "Epoch 50/300\n",
      "Average training loss: 0.05126963746547699\n",
      "Average test loss: 0.0034222451104886\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05122256002492375\n",
      "Average test loss: 0.0034246309304402935\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05112264671590593\n",
      "Average test loss: 0.003454050261941221\n",
      "Epoch 53/300\n",
      "Average training loss: 0.051047788590192794\n",
      "Average test loss: 0.0034264355040051872\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0509865114291509\n",
      "Average test loss: 0.0034147859785912766\n",
      "Epoch 55/300\n",
      "Average training loss: 0.05087346621354421\n",
      "Average test loss: 0.003468992324339019\n",
      "Epoch 56/300\n",
      "Average training loss: 0.05082997399899695\n",
      "Average test loss: 0.0034207629348254865\n",
      "Epoch 57/300\n",
      "Average training loss: 0.05073172128531668\n",
      "Average test loss: 0.0034422719629688394\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05070596373246776\n",
      "Average test loss: 0.0034328102643291156\n",
      "Epoch 59/300\n",
      "Average training loss: 0.05059764230913586\n",
      "Average test loss: 0.003437671264012655\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05053792878985405\n",
      "Average test loss: 0.003441707570519712\n",
      "Epoch 61/300\n",
      "Average training loss: 0.050460636536280315\n",
      "Average test loss: 0.003410443007118172\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05039332266979747\n",
      "Average test loss: 0.0034092317519502507\n",
      "Epoch 63/300\n",
      "Average training loss: 0.05031624395979775\n",
      "Average test loss: 0.0034816699632340005\n",
      "Epoch 64/300\n",
      "Average training loss: 0.050273530417018465\n",
      "Average test loss: 0.0034221881040268475\n",
      "Epoch 65/300\n",
      "Average training loss: 0.05016441171367963\n",
      "Average test loss: 0.003409328871924016\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05007384362154537\n",
      "Average test loss: 0.003405369486245844\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0500519051750501\n",
      "Average test loss: 0.0034183459389540882\n",
      "Epoch 68/300\n",
      "Average training loss: 0.0499973733789391\n",
      "Average test loss: 0.0034478833847161796\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04991555598874887\n",
      "Average test loss: 0.0034258417557511064\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0499116739432017\n",
      "Average test loss: 0.0034214250499175654\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04976858458254072\n",
      "Average test loss: 0.003439200527345141\n",
      "Epoch 72/300\n",
      "Average training loss: 0.04971508916384644\n",
      "Average test loss: 0.0034255306880093283\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0497067611515522\n",
      "Average test loss: 0.0034098996536599266\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04958340489367644\n",
      "Average test loss: 0.0034146196676625145\n",
      "Epoch 75/300\n",
      "Average training loss: 0.04951031223270628\n",
      "Average test loss: 0.003411040014069941\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04948891466856003\n",
      "Average test loss: 0.0034419832124064366\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04942969031797515\n",
      "Average test loss: 0.0034096874528461033\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04934243435660998\n",
      "Average test loss: 0.0034365575061076218\n",
      "Epoch 79/300\n",
      "Average training loss: 0.049292773968643615\n",
      "Average test loss: 0.0034522909969091414\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0492480598754353\n",
      "Average test loss: 0.0034475022529562313\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04915740467442407\n",
      "Average test loss: 0.0034055748298350307\n",
      "Epoch 82/300\n",
      "Average training loss: 0.04917583145035638\n",
      "Average test loss: 0.003402993578463793\n",
      "Epoch 83/300\n",
      "Average training loss: 0.04903464940521452\n",
      "Average test loss: 0.0034248275481578378\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04898408845398161\n",
      "Average test loss: 0.003397719596202175\n",
      "Epoch 85/300\n",
      "Average training loss: 0.048900620612833234\n",
      "Average test loss: 0.0034224088229238988\n",
      "Epoch 86/300\n",
      "Average training loss: 0.04882439191142718\n",
      "Average test loss: 0.0034083585786736674\n",
      "Epoch 87/300\n",
      "Average training loss: 0.04884208576546775\n",
      "Average test loss: 0.003407190590269036\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04873564394315084\n",
      "Average test loss: 0.0034123211916950013\n",
      "Epoch 89/300\n",
      "Average training loss: 0.048680430395735635\n",
      "Average test loss: 0.0034242919192959864\n",
      "Epoch 90/300\n",
      "Average training loss: 0.0486393174595303\n",
      "Average test loss: 0.003423910658289161\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04859991072283851\n",
      "Average test loss: 0.003416998558367292\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04852929267949528\n",
      "Average test loss: 0.003476973524938027\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04846196496155527\n",
      "Average test loss: 0.003404108105227351\n",
      "Epoch 94/300\n",
      "Average training loss: 0.048411313964260946\n",
      "Average test loss: 0.0034945851419534947\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04835146771536933\n",
      "Average test loss: 0.0034432626598411135\n",
      "Epoch 96/300\n",
      "Average training loss: 0.048315402537584304\n",
      "Average test loss: 0.003406900784621636\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04823831343981955\n",
      "Average test loss: 0.003455630574789312\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04819500697321362\n",
      "Average test loss: 0.0034190911017358302\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04810810842116674\n",
      "Average test loss: 0.003422668074361152\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04805449938111835\n",
      "Average test loss: 0.0034150776469873056\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04804066925578647\n",
      "Average test loss: 0.0034133954864616197\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04796913355257776\n",
      "Average test loss: 0.003431988974412282\n",
      "Epoch 103/300\n",
      "Average training loss: 0.047855746113591724\n",
      "Average test loss: 0.0034077098545514874\n",
      "Epoch 104/300\n",
      "Average training loss: 0.04786786695321401\n",
      "Average test loss: 0.0034331039641466404\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04783085235622194\n",
      "Average test loss: 0.0034393318773557743\n",
      "Epoch 106/300\n",
      "Average training loss: 0.04775095964140362\n",
      "Average test loss: 0.0034809585731062622\n",
      "Epoch 107/300\n",
      "Average training loss: 0.047691674490769706\n",
      "Average test loss: 0.003436671645277076\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04760142707824707\n",
      "Average test loss: 0.0034320300086918805\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04760527049832874\n",
      "Average test loss: 0.0035741413682699205\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04755813854932785\n",
      "Average test loss: 0.003425442350924843\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04745555334621006\n",
      "Average test loss: 0.0035350168264574474\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04742600962850783\n",
      "Average test loss: 0.0034421781595382426\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04739945748779509\n",
      "Average test loss: 0.0034499760419130325\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04735594612360001\n",
      "Average test loss: 0.003440567263919446\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04731336484021611\n",
      "Average test loss: 0.0034623897112905978\n",
      "Epoch 116/300\n",
      "Average training loss: 0.047236397415399554\n",
      "Average test loss: 0.0034501901037163204\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04718505280878809\n",
      "Average test loss: 0.0035627167709171774\n",
      "Epoch 118/300\n",
      "Average training loss: 0.047167384564876555\n",
      "Average test loss: 0.0034732829874588385\n",
      "Epoch 119/300\n",
      "Average training loss: 0.047113785008589426\n",
      "Average test loss: 0.003453517041893469\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04707822827829255\n",
      "Average test loss: 0.0034602372044076524\n",
      "Epoch 121/300\n",
      "Average training loss: 0.047028661257690854\n",
      "Average test loss: 0.003508683182299137\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04693601719207234\n",
      "Average test loss: 0.0034622630071308876\n",
      "Epoch 123/300\n",
      "Average training loss: 0.046904784993992914\n",
      "Average test loss: 0.0035058740261528225\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04688962902956539\n",
      "Average test loss: 0.0034703979345245494\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04679867276880476\n",
      "Average test loss: 0.003552405432694488\n",
      "Epoch 126/300\n",
      "Average training loss: 0.04678045381771194\n",
      "Average test loss: 0.0034769479311588737\n",
      "Epoch 127/300\n",
      "Average training loss: 0.046741275201241175\n",
      "Average test loss: 0.0034595754084487756\n",
      "Epoch 128/300\n",
      "Average training loss: 0.046699684825208455\n",
      "Average test loss: 0.0035361512642767693\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04665211919943492\n",
      "Average test loss: 0.003454978715421425\n",
      "Epoch 130/300\n",
      "Average training loss: 0.046611715624729794\n",
      "Average test loss: 0.0034848499811357923\n",
      "Epoch 131/300\n",
      "Average training loss: 0.046562785502937104\n",
      "Average test loss: 0.0034906728278017705\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04652260676688618\n",
      "Average test loss: 0.0034933121237489912\n",
      "Epoch 133/300\n",
      "Average training loss: 0.046487331012884776\n",
      "Average test loss: 0.0035289810448884966\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04645273072520892\n",
      "Average test loss: 0.0035014068039341106\n",
      "Epoch 135/300\n",
      "Average training loss: 0.046383411178986235\n",
      "Average test loss: 0.0034494548978077043\n",
      "Epoch 136/300\n",
      "Average training loss: 0.046331506099965836\n",
      "Average test loss: 0.003496551377077897\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04631516687075297\n",
      "Average test loss: 0.0034812891802026165\n",
      "Epoch 138/300\n",
      "Average training loss: 0.046245866778824064\n",
      "Average test loss: 0.0035321172641383275\n",
      "Epoch 139/300\n",
      "Average training loss: 0.046244767299956746\n",
      "Average test loss: 0.003537077385932207\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04620964822504255\n",
      "Average test loss: 0.0034922239325112766\n",
      "Epoch 141/300\n",
      "Average training loss: 0.046148946513732275\n",
      "Average test loss: 0.0034622032565789092\n",
      "Epoch 142/300\n",
      "Average training loss: 0.046151229805416534\n",
      "Average test loss: 0.0035238979475365746\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04607178952627712\n",
      "Average test loss: 0.0035137195396754478\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04606632779373063\n",
      "Average test loss: 0.0036765443943440916\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04601305551330249\n",
      "Average test loss: 0.0035363814876311355\n",
      "Epoch 146/300\n",
      "Average training loss: 0.046005228598912554\n",
      "Average test loss: 0.003519049373144905\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04596437353226874\n",
      "Average test loss: 0.0035843164845266277\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04590949896309111\n",
      "Average test loss: 0.003514112345667349\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04587569340732363\n",
      "Average test loss: 0.0034842734492073458\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04581062581307358\n",
      "Average test loss: 0.0035359854561587173\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04578647043307622\n",
      "Average test loss: 0.003584852820676234\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04574072134825918\n",
      "Average test loss: 0.0034758958980027174\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04571406435304218\n",
      "Average test loss: 0.003556474344804883\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04572121054927508\n",
      "Average test loss: 0.0035257935118344096\n",
      "Epoch 155/300\n",
      "Average training loss: 0.045666235198577244\n",
      "Average test loss: 0.003489313093531463\n",
      "Epoch 156/300\n",
      "Average training loss: 0.045673304968410065\n",
      "Average test loss: 0.0035727866796983615\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04561392617887921\n",
      "Average test loss: 0.003592270551352865\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04559596629606353\n",
      "Average test loss: 0.0035042821255822977\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04556137013766501\n",
      "Average test loss: 0.0035163173151926863\n",
      "Epoch 160/300\n",
      "Average training loss: 0.045534048438072204\n",
      "Average test loss: 0.0035327496567947995\n",
      "Epoch 161/300\n",
      "Average training loss: 0.045519513620270624\n",
      "Average test loss: 0.003572083292528987\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04548406120472484\n",
      "Average test loss: 0.003577580584006177\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04536797494358487\n",
      "Average test loss: 0.0035986143458220693\n",
      "Epoch 164/300\n",
      "Average training loss: 0.045420895361238056\n",
      "Average test loss: 0.0035105720892962483\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04534430267744594\n",
      "Average test loss: 0.0035519869178533554\n",
      "Epoch 166/300\n",
      "Average training loss: 0.045328976697391936\n",
      "Average test loss: 0.0035441785289181604\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04527448351515664\n",
      "Average test loss: 0.0035139388839403787\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04527361271116469\n",
      "Average test loss: 0.0035921564429170555\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04525226859086089\n",
      "Average test loss: 0.00353688043811255\n",
      "Epoch 170/300\n",
      "Average training loss: 0.045203044020467334\n",
      "Average test loss: 0.003550099530981647\n",
      "Epoch 171/300\n",
      "Average training loss: 0.045163179132673474\n",
      "Average test loss: 0.003486376091423962\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04512499953309695\n",
      "Average test loss: 0.003563519129736556\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04511897458963924\n",
      "Average test loss: 0.0035300822713308865\n",
      "Epoch 174/300\n",
      "Average training loss: 0.045060705671707786\n",
      "Average test loss: 0.003527219214373165\n",
      "Epoch 175/300\n",
      "Average training loss: 0.045045940508445105\n",
      "Average test loss: 0.0035536449199749364\n",
      "Epoch 176/300\n",
      "Average training loss: 0.045033791691064834\n",
      "Average test loss: 0.0035300089160187377\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04503085184428427\n",
      "Average test loss: 0.003526938479807642\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04497588906023237\n",
      "Average test loss: 0.0035401519901222655\n",
      "Epoch 179/300\n",
      "Average training loss: 0.044967457607388496\n",
      "Average test loss: 0.0035859764928205147\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04495939186215401\n",
      "Average test loss: 0.003593134263323413\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04493341467115614\n",
      "Average test loss: 0.0036111396476626394\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04488910925057199\n",
      "Average test loss: 0.0035182043504383828\n",
      "Epoch 183/300\n",
      "Average training loss: 0.044835909303691655\n",
      "Average test loss: 0.0035830369707610873\n",
      "Epoch 184/300\n",
      "Average training loss: 0.044888339297638996\n",
      "Average test loss: 0.003554059375905328\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04485829966432518\n",
      "Average test loss: 0.0035385939981788395\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04478812406460444\n",
      "Average test loss: 0.003558219346528252\n",
      "Epoch 187/300\n",
      "Average training loss: 0.044731051822503407\n",
      "Average test loss: 0.0037120153597659535\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04472015550732612\n",
      "Average test loss: 0.003550552292416493\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04475815944208039\n",
      "Average test loss: 0.0035764168645772668\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04468645305434863\n",
      "Average test loss: 0.0035286905401282842\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04461468633678224\n",
      "Average test loss: 0.0035820877448552186\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0446531632343928\n",
      "Average test loss: 0.003589467102040847\n",
      "Epoch 193/300\n",
      "Average training loss: 0.044591858314143286\n",
      "Average test loss: 0.0036110568353906274\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04459483502639665\n",
      "Average test loss: 0.003601154625415802\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04461452284124162\n",
      "Average test loss: 0.003544326767946283\n",
      "Epoch 196/300\n",
      "Average training loss: 0.044568673137161466\n",
      "Average test loss: 0.0035836580387420126\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04450847980048921\n",
      "Average test loss: 0.0036014829261435404\n",
      "Epoch 198/300\n",
      "Average training loss: 0.044546798057026334\n",
      "Average test loss: 0.003565357026954492\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04447912543018659\n",
      "Average test loss: 0.003538309011608362\n",
      "Epoch 200/300\n",
      "Average training loss: 0.044405015485154256\n",
      "Average test loss: 0.0035959842604481513\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04448140659266048\n",
      "Average test loss: 0.0035952894480692017\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04440585909287135\n",
      "Average test loss: 0.003564047946077254\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04437866699033313\n",
      "Average test loss: 0.003603264676200019\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04432883773247401\n",
      "Average test loss: 0.0035755396626061865\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04436927106976509\n",
      "Average test loss: 0.0036320682995849185\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04431498627199067\n",
      "Average test loss: 0.003564724571795927\n",
      "Epoch 207/300\n",
      "Average training loss: 0.044311004843976765\n",
      "Average test loss: 0.003596559983988603\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04425020678506957\n",
      "Average test loss: 0.0035260801364978152\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04425999962952402\n",
      "Average test loss: 0.0035816149219042724\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04428025087051921\n",
      "Average test loss: 0.003619191074122985\n",
      "Epoch 211/300\n",
      "Average training loss: 0.0442218933933311\n",
      "Average test loss: 0.003605621693034967\n",
      "Epoch 212/300\n",
      "Average training loss: 0.044297451585531236\n",
      "Average test loss: 0.0035610559373680088\n",
      "Epoch 213/300\n",
      "Average training loss: 0.044184520930051804\n",
      "Average test loss: 0.0035348849735326237\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04416673064231873\n",
      "Average test loss: 0.0035869210267232524\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04418708310193486\n",
      "Average test loss: 0.003630138956010342\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04412293831838502\n",
      "Average test loss: 0.0038031867808765835\n",
      "Epoch 217/300\n",
      "Average training loss: 0.044115075482262504\n",
      "Average test loss: 0.0036213648253017\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04410536667373445\n",
      "Average test loss: 0.003567487513025602\n",
      "Epoch 219/300\n",
      "Average training loss: 0.044039924707677626\n",
      "Average test loss: 0.00358077600867384\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04405106375614802\n",
      "Average test loss: 0.003539105365673701\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04401549487974909\n",
      "Average test loss: 0.0036043278051333294\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04398668431242307\n",
      "Average test loss: 0.0035552884158160953\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04403684949874878\n",
      "Average test loss: 0.0037093247671922047\n",
      "Epoch 224/300\n",
      "Average training loss: 0.043993598500887555\n",
      "Average test loss: 0.005366629591832558\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04399534697333972\n",
      "Average test loss: 0.0036297147940430375\n",
      "Epoch 226/300\n",
      "Average training loss: 0.043956590897507135\n",
      "Average test loss: 0.0037372074243095187\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04398420688509941\n",
      "Average test loss: 0.0035885762588845358\n",
      "Epoch 228/300\n",
      "Average training loss: 0.04389984801411629\n",
      "Average test loss: 0.0035623499657958745\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04392262098855442\n",
      "Average test loss: 0.003936104589866267\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04389733483393987\n",
      "Average test loss: 0.0035791052898599043\n",
      "Epoch 231/300\n",
      "Average training loss: 0.043869290487633814\n",
      "Average test loss: 0.003563422514125705\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04381552150183254\n",
      "Average test loss: 0.003583879748566283\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04382845818665292\n",
      "Average test loss: 0.0036548614129424096\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04402255470222897\n",
      "Average test loss: 0.0036706241872161626\n",
      "Epoch 235/300\n",
      "Average training loss: 0.043823985434240764\n",
      "Average test loss: 0.003632460446200437\n",
      "Epoch 236/300\n",
      "Average training loss: 0.043780014362600116\n",
      "Average test loss: 0.003636728298953838\n",
      "Epoch 237/300\n",
      "Average training loss: 0.043726401636997855\n",
      "Average test loss: 0.0035562624745070935\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04371931349900034\n",
      "Average test loss: 0.003574433287191722\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04374452345238791\n",
      "Average test loss: 0.003622144634524981\n",
      "Epoch 240/300\n",
      "Average training loss: 0.043689825634161635\n",
      "Average test loss: 0.003618969404242105\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04368509793612692\n",
      "Average test loss: 0.0036235389121704632\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04370551599396599\n",
      "Average test loss: 0.0036135827102180985\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04373031019833353\n",
      "Average test loss: 0.0035994541686442164\n",
      "Epoch 244/300\n",
      "Average training loss: 0.043606019069751104\n",
      "Average test loss: 0.00369770379902588\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04368952503138118\n",
      "Average test loss: 0.0035650482889678743\n",
      "Epoch 246/300\n",
      "Average training loss: 0.043582595576842624\n",
      "Average test loss: 0.0036716565767096147\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04354412222570843\n",
      "Average test loss: 0.003606833000977834\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04361044689350658\n",
      "Average test loss: 0.003585961199055115\n",
      "Epoch 249/300\n",
      "Average training loss: 0.043545391508274606\n",
      "Average test loss: 0.0037387062706467178\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04356670166386498\n",
      "Average test loss: 0.0035832125461763807\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04352004072401259\n",
      "Average test loss: 0.003594977411131064\n",
      "Epoch 252/300\n",
      "Average training loss: 0.043493873433934314\n",
      "Average test loss: 0.0035793519611987804\n",
      "Epoch 253/300\n",
      "Average training loss: 0.043527967350350485\n",
      "Average test loss: 0.003746032032701704\n",
      "Epoch 254/300\n",
      "Average training loss: 0.043543263302909\n",
      "Average test loss: 0.003567334359097812\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04346457646621598\n",
      "Average test loss: 0.00357372688398593\n",
      "Epoch 256/300\n",
      "Average training loss: 0.043433489526311556\n",
      "Average test loss: 0.0035753629501495097\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04344128939178255\n",
      "Average test loss: 0.003606164078745577\n",
      "Epoch 258/300\n",
      "Average training loss: 0.043479029874006904\n",
      "Average test loss: 0.0036292165499180557\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04339416150583161\n",
      "Average test loss: 0.0036711498954229886\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04340939496954282\n",
      "Average test loss: 0.0038115247260365223\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04342202249831623\n",
      "Average test loss: 0.00366384489234123\n",
      "Epoch 262/300\n",
      "Average training loss: 0.043359502640035415\n",
      "Average test loss: 0.003619106894152032\n",
      "Epoch 263/300\n",
      "Average training loss: 0.043481122010284\n",
      "Average test loss: 0.0036230786330997944\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04341554297341241\n",
      "Average test loss: 0.0036441389413343537\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04334902678926786\n",
      "Average test loss: 0.003634426966102587\n",
      "Epoch 266/300\n",
      "Average training loss: 0.043324824137820135\n",
      "Average test loss: 0.0038708946839388875\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04326014356149568\n",
      "Average test loss: 0.00362062636969818\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04331980289684401\n",
      "Average test loss: 0.003603523414582014\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04328009198109309\n",
      "Average test loss: 0.0036499392228821915\n",
      "Epoch 270/300\n",
      "Average training loss: 0.043319361236360335\n",
      "Average test loss: 0.003634848638334208\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04325498132242097\n",
      "Average test loss: 0.0035737912257512413\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0432246238456832\n",
      "Average test loss: 0.0036116026766184303\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04321611082553863\n",
      "Average test loss: 0.003609300662452976\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04321910395887163\n",
      "Average test loss: 0.0036787112978183562\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04319931963086128\n",
      "Average test loss: 0.0036689524377385777\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04315709603163931\n",
      "Average test loss: 0.003676041474772824\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04324776324629784\n",
      "Average test loss: 0.003603874159562919\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04319026480780708\n",
      "Average test loss: 0.003614922544194592\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04316087751918369\n",
      "Average test loss: 0.0036364601792560685\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04316040121515592\n",
      "Average test loss: 0.0036431034372912514\n",
      "Epoch 281/300\n",
      "Average training loss: 0.043119494991170036\n",
      "Average test loss: 0.003639901518821716\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04315132143762376\n",
      "Average test loss: 0.0036129337321552965\n",
      "Epoch 283/300\n",
      "Average training loss: 0.04307290505038367\n",
      "Average test loss: 0.0036181933956427707\n",
      "Epoch 284/300\n",
      "Average training loss: 0.04313230714201927\n",
      "Average test loss: 0.0036412175635082856\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04313254377908177\n",
      "Average test loss: 0.003618426166060898\n",
      "Epoch 286/300\n",
      "Average training loss: 0.043090564247634675\n",
      "Average test loss: 0.003841193744705783\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04313622296187613\n",
      "Average test loss: 0.0036648447047919037\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0431562839017974\n",
      "Average test loss: 0.003693262824167808\n",
      "Epoch 289/300\n",
      "Average training loss: 0.042989032881127466\n",
      "Average test loss: 0.003659743659198284\n",
      "Epoch 290/300\n",
      "Average training loss: 0.043040084699789684\n",
      "Average test loss: 0.00360662393251227\n",
      "Epoch 291/300\n",
      "Average training loss: 0.043011362976498074\n",
      "Average test loss: 0.003632445523308383\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04298746587501632\n",
      "Average test loss: 0.003713037044637733\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04301467885242568\n",
      "Average test loss: 0.0036233774409112005\n",
      "Epoch 294/300\n",
      "Average training loss: 0.043028412345382905\n",
      "Average test loss: 0.0035964315032793416\n",
      "Epoch 295/300\n",
      "Average training loss: 0.042955322212643095\n",
      "Average test loss: 0.0036782611215280163\n",
      "Epoch 296/300\n",
      "Average training loss: 0.043008476585149766\n",
      "Average test loss: 0.0036776881358689733\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04299948952595393\n",
      "Average test loss: 0.00366592096682224\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04296808111170928\n",
      "Average test loss: 0.0036181200782044068\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04290657424595621\n",
      "Average test loss: 0.00368670900269515\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04293154418799612\n",
      "Average test loss: 0.0036550668782244127\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.747454478363196\n",
      "Average test loss: 0.005296426361633672\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09732962972588009\n",
      "Average test loss: 0.004551836596387956\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07731595728794734\n",
      "Average test loss: 0.0044311670178754465\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0695912273923556\n",
      "Average test loss: 0.004093269493016932\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06519032557143105\n",
      "Average test loss: 0.004082780770129628\n",
      "Epoch 6/300\n",
      "Average training loss: 0.062293279627958935\n",
      "Average test loss: 0.003871192193279664\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06016155694590675\n",
      "Average test loss: 0.0038691962491720915\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05845328485634592\n",
      "Average test loss: 0.003750059047920836\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05704647234082222\n",
      "Average test loss: 0.003606994923618105\n",
      "Epoch 10/300\n",
      "Average training loss: 0.055749351256423524\n",
      "Average test loss: 0.003515453129178948\n",
      "Epoch 11/300\n",
      "Average training loss: 0.05455885479185316\n",
      "Average test loss: 0.0034320785784059103\n",
      "Epoch 12/300\n",
      "Average training loss: 0.053500728991296556\n",
      "Average test loss: 0.0034726863193015258\n",
      "Epoch 13/300\n",
      "Average training loss: 0.05255755357278718\n",
      "Average test loss: 0.0033710431878765422\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05159348733557595\n",
      "Average test loss: 0.003264355759239859\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05077967747052511\n",
      "Average test loss: 0.003210652405396104\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05000466504693031\n",
      "Average test loss: 0.003196954224465622\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04929342278175884\n",
      "Average test loss: 0.0031986308344122438\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04867174159818225\n",
      "Average test loss: 0.0031774523922552663\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0480731428861618\n",
      "Average test loss: 0.0030810702563159997\n",
      "Epoch 20/300\n",
      "Average training loss: 0.047681896588868566\n",
      "Average test loss: 0.003031114220619202\n",
      "Epoch 21/300\n",
      "Average training loss: 0.047155064456992676\n",
      "Average test loss: 0.003029853633708424\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04671959645880593\n",
      "Average test loss: 0.0029989944882690906\n",
      "Epoch 23/300\n",
      "Average training loss: 0.046408455544047884\n",
      "Average test loss: 0.003032509393679599\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04606239855455028\n",
      "Average test loss: 0.0029743136252380078\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04573988656534089\n",
      "Average test loss: 0.0029616616438660356\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04558441163102786\n",
      "Average test loss: 0.002962591212656763\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04521223840448591\n",
      "Average test loss: 0.0029305301254822147\n",
      "Epoch 28/300\n",
      "Average training loss: 0.044957790821790695\n",
      "Average test loss: 0.002911589583588971\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0447268274989393\n",
      "Average test loss: 0.0029082028843048545\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04453432422876358\n",
      "Average test loss: 0.00294698818317718\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04441486144065857\n",
      "Average test loss: 0.002926041847715775\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04417613366246224\n",
      "Average test loss: 0.002894270693572859\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04397723787691858\n",
      "Average test loss: 0.002883839659185873\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04380421482523282\n",
      "Average test loss: 0.00287401318591502\n",
      "Epoch 35/300\n",
      "Average training loss: 0.04365194539229075\n",
      "Average test loss: 0.0028740618899464605\n",
      "Epoch 36/300\n",
      "Average training loss: 0.043553046100669436\n",
      "Average test loss: 0.0028366902576138575\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04335551110241148\n",
      "Average test loss: 0.0028210910434524217\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04325139447715547\n",
      "Average test loss: 0.002828076051030722\n",
      "Epoch 39/300\n",
      "Average training loss: 0.043125784715016685\n",
      "Average test loss: 0.0028414520592325264\n",
      "Epoch 40/300\n",
      "Average training loss: 0.043024853103690676\n",
      "Average test loss: 0.0028475742596719\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04293082449502415\n",
      "Average test loss: 0.0027992154523316356\n",
      "Epoch 42/300\n",
      "Average training loss: 0.042794929461346734\n",
      "Average test loss: 0.002825990749316083\n",
      "Epoch 43/300\n",
      "Average training loss: 0.04268876716825697\n",
      "Average test loss: 0.0028582896932752597\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04260895222094324\n",
      "Average test loss: 0.002825210206210613\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04249242604109976\n",
      "Average test loss: 0.0028239202507668072\n",
      "Epoch 46/300\n",
      "Average training loss: 0.042410830514298545\n",
      "Average test loss: 0.002778019388102823\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04231306981046994\n",
      "Average test loss: 0.002809663353073928\n",
      "Epoch 48/300\n",
      "Average training loss: 0.042217744708061215\n",
      "Average test loss: 0.0027822581380605696\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04210781817303764\n",
      "Average test loss: 0.0027815869664773343\n",
      "Epoch 50/300\n",
      "Average training loss: 0.042057648728291194\n",
      "Average test loss: 0.0027754487186256384\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04195151061978605\n",
      "Average test loss: 0.002766897367520465\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0418457608487871\n",
      "Average test loss: 0.0028116176581631106\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04187359930740463\n",
      "Average test loss: 0.0027847230028775\n",
      "Epoch 54/300\n",
      "Average training loss: 0.041734857506222196\n",
      "Average test loss: 0.0027637042343202564\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04167841680182351\n",
      "Average test loss: 0.0027632522034562296\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04159662193722195\n",
      "Average test loss: 0.002789363276006447\n",
      "Epoch 57/300\n",
      "Average training loss: 0.041530583010779484\n",
      "Average test loss: 0.0027915271882795623\n",
      "Epoch 58/300\n",
      "Average training loss: 0.041405444976356294\n",
      "Average test loss: 0.0027604404124948714\n",
      "Epoch 59/300\n",
      "Average training loss: 0.04135381435354551\n",
      "Average test loss: 0.00276043808605108\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04133775141835213\n",
      "Average test loss: 0.0027630565052645076\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04123225289583206\n",
      "Average test loss: 0.0027551232234885297\n",
      "Epoch 62/300\n",
      "Average training loss: 0.041194002661440104\n",
      "Average test loss: 0.0027488899181286495\n",
      "Epoch 63/300\n",
      "Average training loss: 0.041076808859904605\n",
      "Average test loss: 0.0027613589701553187\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04104624567429225\n",
      "Average test loss: 0.002757855166784591\n",
      "Epoch 65/300\n",
      "Average training loss: 0.040991522120104894\n",
      "Average test loss: 0.002755243966769841\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04090339073207643\n",
      "Average test loss: 0.002753204240360194\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04088092721833123\n",
      "Average test loss: 0.002746779525652528\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04079198081294696\n",
      "Average test loss: 0.0027555806253933245\n",
      "Epoch 69/300\n",
      "Average training loss: 0.040713528917895425\n",
      "Average test loss: 0.002745011917832825\n",
      "Epoch 70/300\n",
      "Average training loss: 0.04067968119515313\n",
      "Average test loss: 0.0027268561269674038\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04060229845841726\n",
      "Average test loss: 0.0027486214523928035\n",
      "Epoch 72/300\n",
      "Average training loss: 0.040531230015887154\n",
      "Average test loss: 0.0027481619690855343\n",
      "Epoch 73/300\n",
      "Average training loss: 0.040470665040943356\n",
      "Average test loss: 0.0027477897748144136\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04041577348940902\n",
      "Average test loss: 0.002782797560716669\n",
      "Epoch 75/300\n",
      "Average training loss: 0.040384553574853474\n",
      "Average test loss: 0.0027275285977456307\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04033938980433676\n",
      "Average test loss: 0.0027514929527209867\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04025787928700447\n",
      "Average test loss: 0.002795074632184373\n",
      "Epoch 78/300\n",
      "Average training loss: 0.04020420736736721\n",
      "Average test loss: 0.0027508015719552837\n",
      "Epoch 79/300\n",
      "Average training loss: 0.040111365371280246\n",
      "Average test loss: 0.002820337776508596\n",
      "Epoch 80/300\n",
      "Average training loss: 0.040154839668009014\n",
      "Average test loss: 0.0027361274181554713\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04005805822213491\n",
      "Average test loss: 0.00274721963931289\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03995456922385428\n",
      "Average test loss: 0.002849506340093083\n",
      "Epoch 83/300\n",
      "Average training loss: 0.039941799448596105\n",
      "Average test loss: 0.002767865682641665\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03988308509522014\n",
      "Average test loss: 0.002737083659403854\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0398857263740566\n",
      "Average test loss: 0.002845992155166136\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03973660222027037\n",
      "Average test loss: 0.002778120548567838\n",
      "Epoch 87/300\n",
      "Average training loss: 0.039693187041415105\n",
      "Average test loss: 0.0027488415024967657\n",
      "Epoch 88/300\n",
      "Average training loss: 0.039643771366940604\n",
      "Average test loss: 0.0027548087982253895\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03956645975841416\n",
      "Average test loss: 0.002740127898131808\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03955098458793428\n",
      "Average test loss: 0.0027438611237125266\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03951368782917659\n",
      "Average test loss: 0.0027428347073081465\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03944733598166042\n",
      "Average test loss: 0.0027535495373109974\n",
      "Epoch 93/300\n",
      "Average training loss: 0.039395205706357954\n",
      "Average test loss: 0.0027588534411042928\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03938528692060047\n",
      "Average test loss: 0.0027466058851116234\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03931685064236323\n",
      "Average test loss: 0.002765407204834951\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03927545244826211\n",
      "Average test loss: 0.002761554879032903\n",
      "Epoch 97/300\n",
      "Average training loss: 0.039187930679983565\n",
      "Average test loss: 0.002735008975594408\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03913955103026496\n",
      "Average test loss: 0.0028236508265965517\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03913464098837641\n",
      "Average test loss: 0.0027477801595297124\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0391172546380096\n",
      "Average test loss: 0.0027691011362605627\n",
      "Epoch 101/300\n",
      "Average training loss: 0.039008257089389695\n",
      "Average test loss: 0.0027752505818174943\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03893731604019801\n",
      "Average test loss: 0.002816219068442782\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03897158574395709\n",
      "Average test loss: 0.0027575019800828563\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03885545005732113\n",
      "Average test loss: 0.0027647669650614263\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03881464209655921\n",
      "Average test loss: 0.002772437827868594\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03877577875388993\n",
      "Average test loss: 0.002794031387194991\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03876163477367825\n",
      "Average test loss: 0.0027891652997997073\n",
      "Epoch 108/300\n",
      "Average training loss: 0.038684363226095836\n",
      "Average test loss: 0.0027663896820611425\n",
      "Epoch 109/300\n",
      "Average training loss: 0.038652486850818\n",
      "Average test loss: 0.002752858632761571\n",
      "Epoch 110/300\n",
      "Average training loss: 0.038621611744165424\n",
      "Average test loss: 0.0027766531467851665\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03856552645564079\n",
      "Average test loss: 0.002758356092704667\n",
      "Epoch 112/300\n",
      "Average training loss: 0.038554628395371965\n",
      "Average test loss: 0.0027746585882786248\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03848079597949982\n",
      "Average test loss: 0.0027862213313993483\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03843722571598159\n",
      "Average test loss: 0.0027607248926328288\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03841091484493679\n",
      "Average test loss: 0.002765940568720301\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03835971854130427\n",
      "Average test loss: 0.00279872243768639\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03834073984954092\n",
      "Average test loss: 0.002768195138623317\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03827061792876985\n",
      "Average test loss: 0.0027653752201133304\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03824013436834017\n",
      "Average test loss: 0.0027701936945733097\n",
      "Epoch 120/300\n",
      "Average training loss: 0.038199105968077976\n",
      "Average test loss: 0.0027971245435376964\n",
      "Epoch 121/300\n",
      "Average training loss: 0.038144841389523614\n",
      "Average test loss: 0.002808304782335957\n",
      "Epoch 122/300\n",
      "Average training loss: 0.038138601965374413\n",
      "Average test loss: 0.002788633301957614\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03810123043590122\n",
      "Average test loss: 0.002775116567603416\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03804679114619891\n",
      "Average test loss: 0.0028353654427660836\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03806090151601368\n",
      "Average test loss: 0.0027891735173761843\n",
      "Epoch 126/300\n",
      "Average training loss: 0.037943634774949817\n",
      "Average test loss: 0.0027973239690893227\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03795602598786354\n",
      "Average test loss: 0.0027867937578509253\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03789397304919031\n",
      "Average test loss: 0.002784034664432208\n",
      "Epoch 129/300\n",
      "Average training loss: 0.037853292697005804\n",
      "Average test loss: 0.00284060820688804\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03783359767496586\n",
      "Average test loss: 0.00302312583848834\n",
      "Epoch 131/300\n",
      "Average training loss: 0.037798833380142845\n",
      "Average test loss: 0.0028202385124233035\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03776132927338282\n",
      "Average test loss: 0.0028009214212910997\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03769366222288874\n",
      "Average test loss: 0.0027989929653704166\n",
      "Epoch 134/300\n",
      "Average training loss: 0.037701003703806135\n",
      "Average test loss: 0.0027797542261994547\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03765498205688265\n",
      "Average test loss: 0.0030005660034302206\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03771004155609343\n",
      "Average test loss: 0.0028021475672721863\n",
      "Epoch 137/300\n",
      "Average training loss: 0.037559294501940406\n",
      "Average test loss: 0.002820528511578838\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0375223457051648\n",
      "Average test loss: 0.0028115774562789335\n",
      "Epoch 139/300\n",
      "Average training loss: 0.037503426033589575\n",
      "Average test loss: 0.00281538068730798\n",
      "Epoch 140/300\n",
      "Average training loss: 0.037474085489908854\n",
      "Average test loss: 0.0027867818768653606\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03745509282251199\n",
      "Average test loss: 0.002837645905299319\n",
      "Epoch 142/300\n",
      "Average training loss: 0.037461051629649265\n",
      "Average test loss: 0.0027773330646256606\n",
      "Epoch 143/300\n",
      "Average training loss: 0.037395802703168654\n",
      "Average test loss: 0.002789384136804276\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03734148599704107\n",
      "Average test loss: 0.0027891863892889686\n",
      "Epoch 145/300\n",
      "Average training loss: 0.037313717977868185\n",
      "Average test loss: 0.002816895406279299\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03728059666686588\n",
      "Average test loss: 0.0028002710272040632\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03721944806310866\n",
      "Average test loss: 0.002785040528823932\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03720117875105805\n",
      "Average test loss: 0.0027974333678268725\n",
      "Epoch 149/300\n",
      "Average training loss: 0.037203744143247605\n",
      "Average test loss: 0.002798774408176541\n",
      "Epoch 150/300\n",
      "Average training loss: 0.037179188864098656\n",
      "Average test loss: 0.002841605678200722\n",
      "Epoch 151/300\n",
      "Average training loss: 0.037119052819079824\n",
      "Average test loss: 0.002813481456082728\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03709261881642872\n",
      "Average test loss: 0.002793867699594961\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03714310300681326\n",
      "Average test loss: 0.0028186277978950076\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03704816558294826\n",
      "Average test loss: 0.002866076229347123\n",
      "Epoch 155/300\n",
      "Average training loss: 0.037057543380392924\n",
      "Average test loss: 0.002842892895763119\n",
      "Epoch 156/300\n",
      "Average training loss: 0.037002076966894996\n",
      "Average test loss: 0.002805302410489983\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03696903700298733\n",
      "Average test loss: 0.0028300744390322105\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03688563266893228\n",
      "Average test loss: 0.0028346262296868696\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03690854614476363\n",
      "Average test loss: 0.0028344139344990255\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036860416839520134\n",
      "Average test loss: 0.0028188694541653\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03684147684607241\n",
      "Average test loss: 0.002831464783185058\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0368758524954319\n",
      "Average test loss: 0.002843062651033203\n",
      "Epoch 163/300\n",
      "Average training loss: 0.036848527050680584\n",
      "Average test loss: 0.0028203941349767975\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03676883501476712\n",
      "Average test loss: 0.002812279465297858\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03674246051907539\n",
      "Average test loss: 0.0028544438268161484\n",
      "Epoch 166/300\n",
      "Average training loss: 0.036720495738916926\n",
      "Average test loss: 0.002868947344728642\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03669462865425481\n",
      "Average test loss: 0.0028720219965196318\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03664067309101423\n",
      "Average test loss: 0.0028038887134235768\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03667303677399953\n",
      "Average test loss: 0.0028196615705059634\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03662425817714797\n",
      "Average test loss: 0.0028573428818749057\n",
      "Epoch 171/300\n",
      "Average training loss: 0.036632223036554126\n",
      "Average test loss: 0.0029320289676802026\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0365895405544175\n",
      "Average test loss: 0.0028536484121448463\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03655168429348204\n",
      "Average test loss: 0.0028189754157016676\n",
      "Epoch 174/300\n",
      "Average training loss: 0.036558323283990224\n",
      "Average test loss: 0.0028491557964848147\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03658594025505914\n",
      "Average test loss: 0.002838932974057065\n",
      "Epoch 176/300\n",
      "Average training loss: 0.036499042905039254\n",
      "Average test loss: 0.0028606469213134714\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03652077368895213\n",
      "Average test loss: 0.002838473490335875\n",
      "Epoch 178/300\n",
      "Average training loss: 0.036466957948274085\n",
      "Average test loss: 0.002846654151255886\n",
      "Epoch 179/300\n",
      "Average training loss: 0.036402462290392984\n",
      "Average test loss: 0.002911884290062719\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03637226945161819\n",
      "Average test loss: 0.002882523430718316\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03635218898786439\n",
      "Average test loss: 0.0029241270447770756\n",
      "Epoch 182/300\n",
      "Average training loss: 0.03631414026353094\n",
      "Average test loss: 0.0028476474699046877\n",
      "Epoch 183/300\n",
      "Average training loss: 0.036468444249696204\n",
      "Average test loss: 0.0028518790952447386\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0363330066634549\n",
      "Average test loss: 0.0028352466302199496\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0362970281591018\n",
      "Average test loss: 0.0028863408416509627\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03624579937259356\n",
      "Average test loss: 0.0028269633588691554\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03622734082241853\n",
      "Average test loss: 0.003003071613609791\n",
      "Epoch 188/300\n",
      "Average training loss: 0.03629168321357833\n",
      "Average test loss: 0.0028659671033836073\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0362287728091081\n",
      "Average test loss: 0.00289058953213195\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03619106380475892\n",
      "Average test loss: 0.0028199209889603984\n",
      "Epoch 191/300\n",
      "Average training loss: 0.036155158604184785\n",
      "Average test loss: 0.0028800895284447404\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0361724934776624\n",
      "Average test loss: 0.0028977481017096176\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03612695586681366\n",
      "Average test loss: 0.0028768546452952754\n",
      "Epoch 194/300\n",
      "Average training loss: 0.036171295331584086\n",
      "Average test loss: 0.002868994529876444\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03615995598832766\n",
      "Average test loss: 0.002862066514997019\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03602335089113977\n",
      "Average test loss: 0.0028829384508232275\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03605818071961403\n",
      "Average test loss: 0.00283659822638664\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03603647622466087\n",
      "Average test loss: 0.00288474526297715\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0359995792971717\n",
      "Average test loss: 0.0028430680547737413\n",
      "Epoch 200/300\n",
      "Average training loss: 0.035984536651108\n",
      "Average test loss: 0.0028428813214931223\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0360446208483643\n",
      "Average test loss: 0.0029337533528192177\n",
      "Epoch 202/300\n",
      "Average training loss: 0.036049902978870604\n",
      "Average test loss: 0.0028574415230088763\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03590832400156392\n",
      "Average test loss: 0.002952551505838831\n",
      "Epoch 204/300\n",
      "Average training loss: 0.035920121658179496\n",
      "Average test loss: 0.0028643850315776135\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03587967913349469\n",
      "Average test loss: 0.0028548368494957684\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0358618513925208\n",
      "Average test loss: 0.0028579366283698216\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03586979338195589\n",
      "Average test loss: 0.0028755075751493372\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03580803210867776\n",
      "Average test loss: 0.0028589892393598953\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03588087652789222\n",
      "Average test loss: 0.002857256210098664\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03586922687954373\n",
      "Average test loss: 0.0028676268534941806\n",
      "Epoch 211/300\n",
      "Average training loss: 0.035814938785301315\n",
      "Average test loss: 0.0028714481712215478\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0357786307003763\n",
      "Average test loss: 0.002859372990619805\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03576660637391938\n",
      "Average test loss: 0.0029252308611240653\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03575408388177554\n",
      "Average test loss: 0.002876540177398258\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03576150777273708\n",
      "Average test loss: 0.002933299229790767\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03574151691959964\n",
      "Average test loss: 0.0028718681409955027\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03576343887382084\n",
      "Average test loss: 0.0028831251371237965\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03576031071113216\n",
      "Average test loss: 0.0029226176581448977\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03563261411587397\n",
      "Average test loss: 0.002870799277598659\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03566016219556332\n",
      "Average test loss: 0.002899927435649766\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0357189667440123\n",
      "Average test loss: 0.0028969789238439667\n",
      "Epoch 222/300\n",
      "Average training loss: 0.035676206323835585\n",
      "Average test loss: 0.0028918457937737304\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03564746695094639\n",
      "Average test loss: 0.00291517051267955\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03560973694258266\n",
      "Average test loss: 0.002904079772118065\n",
      "Epoch 225/300\n",
      "Average training loss: 0.035567117869853976\n",
      "Average test loss: 0.005126728270616796\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03561342915892601\n",
      "Average test loss: 0.002931028925503294\n",
      "Epoch 227/300\n",
      "Average training loss: 0.035600211386879285\n",
      "Average test loss: 0.0028886360132859812\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03559986130727662\n",
      "Average test loss: 0.0028748165778815747\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03548341147436036\n",
      "Average test loss: 0.0029568174303405814\n",
      "Epoch 230/300\n",
      "Average training loss: 0.035515041516886815\n",
      "Average test loss: 0.0029466313210626443\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03552408878008525\n",
      "Average test loss: 0.002886616079136729\n",
      "Epoch 232/300\n",
      "Average training loss: 0.035464995387527676\n",
      "Average test loss: 0.002933133640223079\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03549689875377549\n",
      "Average test loss: 0.0028629264521102113\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03547985253234704\n",
      "Average test loss: 0.002915481471353107\n",
      "Epoch 235/300\n",
      "Average training loss: 0.035441186591982844\n",
      "Average test loss: 0.0029316913709044458\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03544697704414527\n",
      "Average test loss: 0.0030354046809176606\n",
      "Epoch 237/300\n",
      "Average training loss: 0.035417811882164744\n",
      "Average test loss: 0.0028738963086571956\n",
      "Epoch 238/300\n",
      "Average training loss: 0.035399497634834715\n",
      "Average test loss: 0.0030868950558619367\n",
      "Epoch 239/300\n",
      "Average training loss: 0.035436146517594656\n",
      "Average test loss: 0.0029822056866768334\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03537148820194933\n",
      "Average test loss: 0.0029138583332921067\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03538036818305652\n",
      "Average test loss: 0.00289628489750127\n",
      "Epoch 242/300\n",
      "Average training loss: 0.035370200374060204\n",
      "Average test loss: 0.00292235752236512\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03538212677505281\n",
      "Average test loss: 0.0028573540691286324\n",
      "Epoch 244/300\n",
      "Average training loss: 0.035313016664650705\n",
      "Average test loss: 0.002884119769765271\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03536245553029908\n",
      "Average test loss: 0.0028839881668488183\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03529229733016756\n",
      "Average test loss: 0.0028664802730911307\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03527816788686646\n",
      "Average test loss: 0.0028985003241234356\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03522741448879242\n",
      "Average test loss: 0.0028944417970875897\n",
      "Epoch 249/300\n",
      "Average training loss: 0.035307238805625175\n",
      "Average test loss: 0.0029351447721322377\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03526776270237234\n",
      "Average test loss: 0.002913508137067159\n",
      "Epoch 251/300\n",
      "Average training loss: 0.035270679417583675\n",
      "Average test loss: 0.002906359142727322\n",
      "Epoch 252/300\n",
      "Average training loss: 0.035199538985888165\n",
      "Average test loss: 0.0030149239856335852\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03524889743328095\n",
      "Average test loss: 0.0029037540275603534\n",
      "Epoch 254/300\n",
      "Average training loss: 0.035228640341096455\n",
      "Average test loss: 0.0029885263620979255\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03524377889434496\n",
      "Average test loss: 0.002943455983988113\n",
      "Epoch 256/300\n",
      "Average training loss: 0.035196640418635475\n",
      "Average test loss: 0.0029261700076361496\n",
      "Epoch 257/300\n",
      "Average training loss: 0.035143306420909036\n",
      "Average test loss: 0.002885873895759384\n",
      "Epoch 258/300\n",
      "Average training loss: 0.035129700697130624\n",
      "Average test loss: 0.00287440550679134\n",
      "Epoch 259/300\n",
      "Average training loss: 0.035168504155344435\n",
      "Average test loss: 0.0028832276459369393\n",
      "Epoch 260/300\n",
      "Average training loss: 0.035109331644243666\n",
      "Average test loss: 0.0029214583622912565\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0351181896229585\n",
      "Average test loss: 0.0029335487646361194\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03510048408806324\n",
      "Average test loss: 0.002833180578218566\n",
      "Epoch 263/300\n",
      "Average training loss: 0.035101707869105866\n",
      "Average test loss: 0.0029059050012793805\n",
      "Epoch 264/300\n",
      "Average training loss: 0.035108048919174406\n",
      "Average test loss: 0.0029271506385670767\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0350793916715516\n",
      "Average test loss: 0.0029348920153246987\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03511603852775362\n",
      "Average test loss: 0.0028430598830390306\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0350250589383973\n",
      "Average test loss: 0.002933745027830203\n",
      "Epoch 268/300\n",
      "Average training loss: 0.035042823255062104\n",
      "Average test loss: 0.0028943056157893603\n",
      "Epoch 269/300\n",
      "Average training loss: 0.035059198084804746\n",
      "Average test loss: 0.002951527369519075\n",
      "Epoch 270/300\n",
      "Average training loss: 0.035046727889113956\n",
      "Average test loss: 0.0028903760705143214\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03502330167591572\n",
      "Average test loss: 0.002947551215067506\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03503316476278835\n",
      "Average test loss: 0.002876666632998321\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03501195760567983\n",
      "Average test loss: 0.0029446131195873023\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03505372409688102\n",
      "Average test loss: 0.0029427715916600494\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03495915252632565\n",
      "Average test loss: 0.002896856312950452\n",
      "Epoch 276/300\n",
      "Average training loss: 0.034997266428338156\n",
      "Average test loss: 0.003007436116432978\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03498910728096962\n",
      "Average test loss: 0.002923755449553331\n",
      "Epoch 278/300\n",
      "Average training loss: 0.034968640570839246\n",
      "Average test loss: 0.0030207603182643653\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03498261638482412\n",
      "Average test loss: 0.0029299311103920143\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0349139021800624\n",
      "Average test loss: 0.003008475614918603\n",
      "Epoch 281/300\n",
      "Average training loss: 0.034939787434207066\n",
      "Average test loss: 0.002943860078851382\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03488864556948344\n",
      "Average test loss: 0.0029112854653762445\n",
      "Epoch 283/300\n",
      "Average training loss: 0.034908311115370856\n",
      "Average test loss: 0.002939107251457042\n",
      "Epoch 284/300\n",
      "Average training loss: 0.034870891615748405\n",
      "Average test loss: 0.0028914067302313116\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03487623288234075\n",
      "Average test loss: 0.002909508133100139\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0348894560303953\n",
      "Average test loss: 0.0029835537117388513\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03485791168941392\n",
      "Average test loss: 0.0029278455393181905\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03487071476048893\n",
      "Average test loss: 0.0028930391545097032\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03484675633245044\n",
      "Average test loss: 0.002913878907966945\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03484033117029402\n",
      "Average test loss: 0.0029099323271463313\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03478275304536025\n",
      "Average test loss: 0.002968143867432243\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03479705555240313\n",
      "Average test loss: 0.0029405468845119078\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03480163727535142\n",
      "Average test loss: 0.0029690623440676266\n",
      "Epoch 294/300\n",
      "Average training loss: 0.034781170517206195\n",
      "Average test loss: 0.0030255898924337493\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03476555012663205\n",
      "Average test loss: 0.002891755663065447\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03484199476573202\n",
      "Average test loss: 0.002880349456229144\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0347220813996262\n",
      "Average test loss: 0.002932868637972408\n",
      "Epoch 298/300\n",
      "Average training loss: 0.034742631279759933\n",
      "Average test loss: 0.0029115246089382305\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03473498217264811\n",
      "Average test loss: 0.0028811230270398987\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03471366262104776\n",
      "Average test loss: 0.002920522266171045\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7273425569534302\n",
      "Average test loss: 0.004977215490821335\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09622884304655922\n",
      "Average test loss: 0.004203961891018682\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07238706033097374\n",
      "Average test loss: 0.003880391728132963\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06356198551588588\n",
      "Average test loss: 0.004008428124917878\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05856873009602229\n",
      "Average test loss: 0.003561373546719551\n",
      "Epoch 6/300\n",
      "Average training loss: 0.055407064528928865\n",
      "Average test loss: 0.003395604578571187\n",
      "Epoch 7/300\n",
      "Average training loss: 0.053092373722129396\n",
      "Average test loss: 0.0033206811661107673\n",
      "Epoch 8/300\n",
      "Average training loss: 0.051239796976248426\n",
      "Average test loss: 0.003281798181227512\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04964579693145222\n",
      "Average test loss: 0.0031302287239167426\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04810178131858508\n",
      "Average test loss: 0.00301525321478645\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04674947087632285\n",
      "Average test loss: 0.003034282610234287\n",
      "Epoch 12/300\n",
      "Average training loss: 0.045530394822359085\n",
      "Average test loss: 0.002871150327225526\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04436249564091364\n",
      "Average test loss: 0.0028008247328301272\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04333873174919022\n",
      "Average test loss: 0.00268613211872677\n",
      "Epoch 15/300\n",
      "Average training loss: 0.042433745705419114\n",
      "Average test loss: 0.002655180219974783\n",
      "Epoch 16/300\n",
      "Average training loss: 0.041606813409262235\n",
      "Average test loss: 0.0026156505299732087\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04087783327367571\n",
      "Average test loss: 0.00263365279065652\n",
      "Epoch 18/300\n",
      "Average training loss: 0.040231521126296785\n",
      "Average test loss: 0.0025382861786832412\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03960590961575508\n",
      "Average test loss: 0.0025240559675213364\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0391230224205388\n",
      "Average test loss: 0.0024707575942286186\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03870242740379439\n",
      "Average test loss: 0.0024561603100349506\n",
      "Epoch 22/300\n",
      "Average training loss: 0.03830189952916569\n",
      "Average test loss: 0.0024640530643777717\n",
      "Epoch 23/300\n",
      "Average training loss: 0.03795608531766467\n",
      "Average test loss: 0.0024105635328839224\n",
      "Epoch 24/300\n",
      "Average training loss: 0.037603112482362325\n",
      "Average test loss: 0.0024045029326031607\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03731404516266452\n",
      "Average test loss: 0.0024248494448968106\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03705145065652\n",
      "Average test loss: 0.0023722793372968833\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03680770735939344\n",
      "Average test loss: 0.002422091772572862\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03661322075128555\n",
      "Average test loss: 0.002355501006874773\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03638523322178258\n",
      "Average test loss: 0.0023284943872648807\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03618096421493424\n",
      "Average test loss: 0.002331627331053217\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03601265441543526\n",
      "Average test loss: 0.0023258854808906712\n",
      "Epoch 32/300\n",
      "Average training loss: 0.035844747056563694\n",
      "Average test loss: 0.0023220234676781626\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03567044945226775\n",
      "Average test loss: 0.0023022974810252587\n",
      "Epoch 34/300\n",
      "Average training loss: 0.035533850047323436\n",
      "Average test loss: 0.002302409476393627\n",
      "Epoch 35/300\n",
      "Average training loss: 0.035374945210085974\n",
      "Average test loss: 0.00230000430242055\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03529552838206291\n",
      "Average test loss: 0.0023039894595535265\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03516505993571546\n",
      "Average test loss: 0.002277943209020628\n",
      "Epoch 38/300\n",
      "Average training loss: 0.035017598731650244\n",
      "Average test loss: 0.0022808975455247695\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03489765383137597\n",
      "Average test loss: 0.002283086015739375\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03484654407368766\n",
      "Average test loss: 0.0022742960571000975\n",
      "Epoch 41/300\n",
      "Average training loss: 0.034650728619760936\n",
      "Average test loss: 0.002269123039725754\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03460193391640981\n",
      "Average test loss: 0.0022429138586545984\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03450480377342966\n",
      "Average test loss: 0.002249601115265654\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03446981517804994\n",
      "Average test loss: 0.002248531369906333\n",
      "Epoch 45/300\n",
      "Average training loss: 0.0343423896531264\n",
      "Average test loss: 0.0022492887605395583\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03423760257826911\n",
      "Average test loss: 0.0022365077077928517\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03416744814316432\n",
      "Average test loss: 0.002380221824368669\n",
      "Epoch 48/300\n",
      "Average training loss: 0.034070395055744385\n",
      "Average test loss: 0.0022333705253485174\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03401939435137643\n",
      "Average test loss: 0.0022392820645537642\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03398273716701401\n",
      "Average test loss: 0.002236035114568141\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03387303783827358\n",
      "Average test loss: 0.002226380893546674\n",
      "Epoch 52/300\n",
      "Average training loss: 0.033827075958251955\n",
      "Average test loss: 0.00224576519181331\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03374293218056361\n",
      "Average test loss: 0.0022205794466038545\n",
      "Epoch 54/300\n",
      "Average training loss: 0.033714607563283705\n",
      "Average test loss: 0.0022256952271693282\n",
      "Epoch 55/300\n",
      "Average training loss: 0.033597953385776945\n",
      "Average test loss: 0.002215278728554646\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03355438355108102\n",
      "Average test loss: 0.0022238740264955495\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0334799641403887\n",
      "Average test loss: 0.0022120445211314494\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03341937332517571\n",
      "Average test loss: 0.0022045699825717345\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03337703195876545\n",
      "Average test loss: 0.00225921437330544\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03333268413941066\n",
      "Average test loss: 0.002204896224869622\n",
      "Epoch 61/300\n",
      "Average training loss: 0.033264491617679595\n",
      "Average test loss: 0.0022204183090685143\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03316962434185876\n",
      "Average test loss: 0.0022153005523400174\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0331548093209664\n",
      "Average test loss: 0.0022181451194402246\n",
      "Epoch 64/300\n",
      "Average training loss: 0.033148664171497026\n",
      "Average test loss: 0.002198880693771773\n",
      "Epoch 65/300\n",
      "Average training loss: 0.03306240740418434\n",
      "Average test loss: 0.002207471038939224\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03298321721951167\n",
      "Average test loss: 0.002215314807577266\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03294576656487253\n",
      "Average test loss: 0.0022072534275551637\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03287988445162773\n",
      "Average test loss: 0.0021952415963427886\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03282473195758131\n",
      "Average test loss: 0.0022021653697722487\n",
      "Epoch 70/300\n",
      "Average training loss: 0.032802705897225275\n",
      "Average test loss: 0.0021945602564762035\n",
      "Epoch 71/300\n",
      "Average training loss: 0.032735473692417145\n",
      "Average test loss: 0.0021953366469177936\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03270275741153293\n",
      "Average test loss: 0.002209240406130751\n",
      "Epoch 73/300\n",
      "Average training loss: 0.032623666793107985\n",
      "Average test loss: 0.0022471033407168257\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03257141165269746\n",
      "Average test loss: 0.002202145747323003\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03257535668876436\n",
      "Average test loss: 0.0021943887507336005\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0324908956438303\n",
      "Average test loss: 0.0022406925732890767\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03243139132857323\n",
      "Average test loss: 0.002195216802983648\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0323785791794459\n",
      "Average test loss: 0.002204352768758933\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03235090704096688\n",
      "Average test loss: 0.0021998545583337544\n",
      "Epoch 80/300\n",
      "Average training loss: 0.032300362510813606\n",
      "Average test loss: 0.002207263315303458\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03224441172348128\n",
      "Average test loss: 0.0021977868249846827\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03221437772446208\n",
      "Average test loss: 0.002220086444997125\n",
      "Epoch 83/300\n",
      "Average training loss: 0.032178280298908554\n",
      "Average test loss: 0.0021971289614836375\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03213118242224058\n",
      "Average test loss: 0.0021982715689680644\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03208919895854261\n",
      "Average test loss: 0.0022011264560537207\n",
      "Epoch 86/300\n",
      "Average training loss: 0.032074515200323526\n",
      "Average test loss: 0.0021893949690792295\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03203076391584343\n",
      "Average test loss: 0.002190937258510126\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03199606317281723\n",
      "Average test loss: 0.0021980440854612324\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03193959487146802\n",
      "Average test loss: 0.0022021713722497225\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03188170595963796\n",
      "Average test loss: 0.0022021288333667647\n",
      "Epoch 91/300\n",
      "Average training loss: 0.031843325058619185\n",
      "Average test loss: 0.0021821214546346003\n",
      "Epoch 92/300\n",
      "Average training loss: 0.031780680023961595\n",
      "Average test loss: 0.002199444441435238\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03176280732287301\n",
      "Average test loss: 0.0022135476482411224\n",
      "Epoch 94/300\n",
      "Average training loss: 0.031735322381059326\n",
      "Average test loss: 0.002196076362911198\n",
      "Epoch 95/300\n",
      "Average training loss: 0.031668684820334116\n",
      "Average test loss: 0.002217386220478349\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03167375052306387\n",
      "Average test loss: 0.002208584749036365\n",
      "Epoch 97/300\n",
      "Average training loss: 0.031608354495631326\n",
      "Average test loss: 0.0022495247611982955\n",
      "Epoch 98/300\n",
      "Average training loss: 0.031571148753166196\n",
      "Average test loss: 0.0022059790400995147\n",
      "Epoch 99/300\n",
      "Average training loss: 0.031579144462943076\n",
      "Average test loss: 0.0022273913628111284\n",
      "Epoch 100/300\n",
      "Average training loss: 0.03154186052746243\n",
      "Average test loss: 0.0022151278387755155\n",
      "Epoch 101/300\n",
      "Average training loss: 0.031525125248564614\n",
      "Average test loss: 0.0021985722279383078\n",
      "Epoch 102/300\n",
      "Average training loss: 0.031422958236601614\n",
      "Average test loss: 0.0021993906404823064\n",
      "Epoch 103/300\n",
      "Average training loss: 0.031381223761373096\n",
      "Average test loss: 0.0022064959458592865\n",
      "Epoch 104/300\n",
      "Average training loss: 0.031376173708173964\n",
      "Average test loss: 0.002201313142147329\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03133499359091123\n",
      "Average test loss: 0.0022099658656451436\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03125077818499671\n",
      "Average test loss: 0.002205140664138728\n",
      "Epoch 107/300\n",
      "Average training loss: 0.031265226436985864\n",
      "Average test loss: 0.0022149920964406595\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03122597585287359\n",
      "Average test loss: 0.002266346491148902\n",
      "Epoch 109/300\n",
      "Average training loss: 0.031190037808484502\n",
      "Average test loss: 0.0022134038543121684\n",
      "Epoch 110/300\n",
      "Average training loss: 0.031153563343816332\n",
      "Average test loss: 0.002228577898401353\n",
      "Epoch 111/300\n",
      "Average training loss: 0.031135238758391803\n",
      "Average test loss: 0.002212221704216467\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03109084657828013\n",
      "Average test loss: 0.002214199984446168\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03107677036523819\n",
      "Average test loss: 0.0022032748135841556\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03102836475107405\n",
      "Average test loss: 0.00221208655120184\n",
      "Epoch 115/300\n",
      "Average training loss: 0.030982145364085834\n",
      "Average test loss: 0.0022172543153994614\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03095058074924681\n",
      "Average test loss: 0.002215260087305473\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03092105560335848\n",
      "Average test loss: 0.0022606993592861626\n",
      "Epoch 118/300\n",
      "Average training loss: 0.030908860401974783\n",
      "Average test loss: 0.002217098272922966\n",
      "Epoch 119/300\n",
      "Average training loss: 0.030872054449386066\n",
      "Average test loss: 0.002236993679569827\n",
      "Epoch 120/300\n",
      "Average training loss: 0.030821959674358367\n",
      "Average test loss: 0.0022047497639432548\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03078050812582175\n",
      "Average test loss: 0.002217164765422543\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03076990803413921\n",
      "Average test loss: 0.0022015711727241677\n",
      "Epoch 123/300\n",
      "Average training loss: 0.030756188372770946\n",
      "Average test loss: 0.0022046919367793533\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0306872828801473\n",
      "Average test loss: 0.002259948367977308\n",
      "Epoch 125/300\n",
      "Average training loss: 0.030710440556208293\n",
      "Average test loss: 0.0022100310052434605\n",
      "Epoch 126/300\n",
      "Average training loss: 0.030616648385922113\n",
      "Average test loss: 0.002226874244502849\n",
      "Epoch 127/300\n",
      "Average training loss: 0.030622626254955928\n",
      "Average test loss: 0.0022418369435601763\n",
      "Epoch 128/300\n",
      "Average training loss: 0.030602455109357832\n",
      "Average test loss: 0.002287551099020574\n",
      "Epoch 129/300\n",
      "Average training loss: 0.030599163432916004\n",
      "Average test loss: 0.002233957113698125\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03070063992175791\n",
      "Average test loss: 0.0022412223267472453\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03049045211904579\n",
      "Average test loss: 0.0022175582675263285\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03047435162630346\n",
      "Average test loss: 0.002230915025807917\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03044739032122824\n",
      "Average test loss: 0.002230214645775656\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03043335646390915\n",
      "Average test loss: 0.0022101298099797633\n",
      "Epoch 135/300\n",
      "Average training loss: 0.030396317943930627\n",
      "Average test loss: 0.0022217608479162057\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03037441772719224\n",
      "Average test loss: 0.0022397367399599817\n",
      "Epoch 137/300\n",
      "Average training loss: 0.030340240814619595\n",
      "Average test loss: 0.0022269126681817904\n",
      "Epoch 138/300\n",
      "Average training loss: 0.030319659216536416\n",
      "Average test loss: 0.002234396046648423\n",
      "Epoch 139/300\n",
      "Average training loss: 0.030377213908566368\n",
      "Average test loss: 0.0022385909996098944\n",
      "Epoch 140/300\n",
      "Average training loss: 0.030286716298924553\n",
      "Average test loss: 0.002240181340318587\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030228624602158864\n",
      "Average test loss: 0.002248631824221876\n",
      "Epoch 142/300\n",
      "Average training loss: 0.030217809634076224\n",
      "Average test loss: 0.0022474577486928968\n",
      "Epoch 143/300\n",
      "Average training loss: 0.030228566808833016\n",
      "Average test loss: 0.002253565836076935\n",
      "Epoch 144/300\n",
      "Average training loss: 0.030190925503770512\n",
      "Average test loss: 0.002247224005767041\n",
      "Epoch 145/300\n",
      "Average training loss: 0.030202725996573768\n",
      "Average test loss: 0.0022377919449160497\n",
      "Epoch 146/300\n",
      "Average training loss: 0.030161038297745916\n",
      "Average test loss: 0.0022403866592794657\n",
      "Epoch 147/300\n",
      "Average training loss: 0.030117127580775154\n",
      "Average test loss: 0.002222032646752066\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03009849115543895\n",
      "Average test loss: 0.0022283464492195183\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030029566723439427\n",
      "Average test loss: 0.002250717807147238\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030035958129498695\n",
      "Average test loss: 0.012684358606735864\n",
      "Epoch 151/300\n",
      "Average training loss: 0.030052423447370528\n",
      "Average test loss: 0.0023239589680193198\n",
      "Epoch 152/300\n",
      "Average training loss: 0.029996696280108558\n",
      "Average test loss: 0.0022173229531488483\n",
      "Epoch 153/300\n",
      "Average training loss: 0.029948867443535063\n",
      "Average test loss: 0.002279537584218714\n",
      "Epoch 154/300\n",
      "Average training loss: 0.029972041565510962\n",
      "Average test loss: 0.002241421302159627\n",
      "Epoch 155/300\n",
      "Average training loss: 0.029930385981996856\n",
      "Average test loss: 0.002266510349801845\n",
      "Epoch 156/300\n",
      "Average training loss: 0.029904507994651795\n",
      "Average test loss: 0.002261885948272215\n",
      "Epoch 157/300\n",
      "Average training loss: 0.029877368102471033\n",
      "Average test loss: 0.002266051853593025\n",
      "Epoch 158/300\n",
      "Average training loss: 0.029851024427347713\n",
      "Average test loss: 0.0022633460003675687\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02985073126355807\n",
      "Average test loss: 0.002260181507923537\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02984889382123947\n",
      "Average test loss: 0.002225609404138393\n",
      "Epoch 161/300\n",
      "Average training loss: 0.029832446545362473\n",
      "Average test loss: 0.002267303684312436\n",
      "Epoch 162/300\n",
      "Average training loss: 0.029801917552947997\n",
      "Average test loss: 0.00226540070399642\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029808667769034704\n",
      "Average test loss: 0.002254624745498101\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029741182618670994\n",
      "Average test loss: 0.0022638812193440066\n",
      "Epoch 165/300\n",
      "Average training loss: 0.029700266768534978\n",
      "Average test loss: 0.0022650208197948006\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029704344279236263\n",
      "Average test loss: 0.0022339056401203075\n",
      "Epoch 167/300\n",
      "Average training loss: 0.029685432000292673\n",
      "Average test loss: 0.002261047845085462\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029675269628564517\n",
      "Average test loss: 0.0022745760856196285\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02965624407099353\n",
      "Average test loss: 0.0022678851122036577\n",
      "Epoch 170/300\n",
      "Average training loss: 0.029635079302721554\n",
      "Average test loss: 0.0022543187133140033\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02961553076075183\n",
      "Average test loss: 0.0022462256842603287\n",
      "Epoch 172/300\n",
      "Average training loss: 0.029571699826253784\n",
      "Average test loss: 0.002250500909984112\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029587706103920936\n",
      "Average test loss: 0.0022587188264975946\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029573162626889016\n",
      "Average test loss: 0.002246458531667789\n",
      "Epoch 175/300\n",
      "Average training loss: 0.029518040984869005\n",
      "Average test loss: 0.002261669745047887\n",
      "Epoch 176/300\n",
      "Average training loss: 0.029495592612359258\n",
      "Average test loss: 0.0022842009495943785\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029489566576149727\n",
      "Average test loss: 0.002289676905609667\n",
      "Epoch 178/300\n",
      "Average training loss: 0.029508319566647213\n",
      "Average test loss: 0.00224736174237397\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02947801983853181\n",
      "Average test loss: 0.0022658364085687533\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02950031832853953\n",
      "Average test loss: 0.002271236928593781\n",
      "Epoch 181/300\n",
      "Average training loss: 0.02943348265853193\n",
      "Average test loss: 0.0022890501150654424\n",
      "Epoch 182/300\n",
      "Average training loss: 0.029407205485635333\n",
      "Average test loss: 0.002851883247701658\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02945587800277604\n",
      "Average test loss: 0.00225541078671813\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02940802101790905\n",
      "Average test loss: 0.002278887545896901\n",
      "Epoch 185/300\n",
      "Average training loss: 0.029389343326290448\n",
      "Average test loss: 0.002300509456338154\n",
      "Epoch 186/300\n",
      "Average training loss: 0.029349951992432278\n",
      "Average test loss: 0.0022786560418705144\n",
      "Epoch 187/300\n",
      "Average training loss: 0.029334294974803925\n",
      "Average test loss: 0.002262076328198115\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02932328435447481\n",
      "Average test loss: 0.002342274252532257\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02927898339099354\n",
      "Average test loss: 0.0023588862367388275\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02929855152633455\n",
      "Average test loss: 0.002317251520127886\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02929793839653333\n",
      "Average test loss: 0.0022662005403803454\n",
      "Epoch 192/300\n",
      "Average training loss: 0.02928092924091551\n",
      "Average test loss: 0.0023065708567284876\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029232693433761597\n",
      "Average test loss: 0.002271582095987267\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02923001170158386\n",
      "Average test loss: 0.002272763692256477\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029267132045494186\n",
      "Average test loss: 0.0023548137717362906\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029185701423221164\n",
      "Average test loss: 0.0022861301416738167\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02920022530025906\n",
      "Average test loss: 0.0023006958549635277\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029174106063114272\n",
      "Average test loss: 0.0022623271039790577\n",
      "Epoch 199/300\n",
      "Average training loss: 0.029135225928492017\n",
      "Average test loss: 0.002281894011422992\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029124765656060644\n",
      "Average test loss: 0.0022661843851415646\n",
      "Epoch 201/300\n",
      "Average training loss: 0.029165883685151738\n",
      "Average test loss: 0.0022611288231694036\n",
      "Epoch 202/300\n",
      "Average training loss: 0.029122794886430103\n",
      "Average test loss: 0.0023019420887447064\n",
      "Epoch 203/300\n",
      "Average training loss: 0.029125894788238738\n",
      "Average test loss: 0.0023012218726798893\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02907851670847999\n",
      "Average test loss: 0.0023446002782632905\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029063660790522892\n",
      "Average test loss: 0.0022938069419728385\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02905160086022483\n",
      "Average test loss: 0.0022857668807523116\n",
      "Epoch 207/300\n",
      "Average training loss: 0.029037098033560648\n",
      "Average test loss: 0.002279857979880439\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029036347021659217\n",
      "Average test loss: 0.0022783686220645906\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02901295304629538\n",
      "Average test loss: 0.0022860034389628305\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028983533667193518\n",
      "Average test loss: 0.0022958589837782915\n",
      "Epoch 211/300\n",
      "Average training loss: 0.029006339020199248\n",
      "Average test loss: 0.0022861259093301164\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02899639460775587\n",
      "Average test loss: 0.002428042787644598\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0290200785216358\n",
      "Average test loss: 0.002292755078731312\n",
      "Epoch 214/300\n",
      "Average training loss: 0.028965217555562656\n",
      "Average test loss: 0.0022802184135135677\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02894605227973726\n",
      "Average test loss: 0.002292083776866396\n",
      "Epoch 216/300\n",
      "Average training loss: 0.028937939654621814\n",
      "Average test loss: 0.002339151678710348\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028901974687973658\n",
      "Average test loss: 0.0022721045702281924\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028927465175588925\n",
      "Average test loss: 0.0022972916445384425\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028886214377151595\n",
      "Average test loss: 0.0022910971339378094\n",
      "Epoch 220/300\n",
      "Average training loss: 0.028877479798263973\n",
      "Average test loss: 0.002286699029513531\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028844268822007708\n",
      "Average test loss: 0.0022872987415434587\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028894438952207564\n",
      "Average test loss: 0.002291721302912467\n",
      "Epoch 223/300\n",
      "Average training loss: 0.028830910349885622\n",
      "Average test loss: 0.0022914309824506444\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02880585812860065\n",
      "Average test loss: 0.002328505316335294\n",
      "Epoch 225/300\n",
      "Average training loss: 0.028792307906680636\n",
      "Average test loss: 0.0022730959529678025\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02884577547841602\n",
      "Average test loss: 0.002290518687210149\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02881240560942226\n",
      "Average test loss: 0.0022894213414854474\n",
      "Epoch 228/300\n",
      "Average training loss: 0.028766853915320502\n",
      "Average test loss: 0.002323190165580147\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028778565494550597\n",
      "Average test loss: 0.0023247611394359006\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02875759431388643\n",
      "Average test loss: 0.0022970839503945574\n",
      "Epoch 231/300\n",
      "Average training loss: 0.028766478543480238\n",
      "Average test loss: 0.002326485209994846\n",
      "Epoch 232/300\n",
      "Average training loss: 0.028723169434401723\n",
      "Average test loss: 0.002321076545036501\n",
      "Epoch 233/300\n",
      "Average training loss: 0.028719280544254516\n",
      "Average test loss: 0.0022866724224554166\n",
      "Epoch 234/300\n",
      "Average training loss: 0.028731015609370338\n",
      "Average test loss: 0.0023103917400456135\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02871744477086597\n",
      "Average test loss: 0.0023047544350847603\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02869284139573574\n",
      "Average test loss: 0.0022757647978141906\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028700782704684468\n",
      "Average test loss: 0.0023054861372543704\n",
      "Epoch 238/300\n",
      "Average training loss: 0.028668559986684057\n",
      "Average test loss: 0.0023430620630582174\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02865771578749021\n",
      "Average test loss: 0.0024838289856496786\n",
      "Epoch 240/300\n",
      "Average training loss: 0.028710201657480665\n",
      "Average test loss: 0.0022910087561855714\n",
      "Epoch 241/300\n",
      "Average training loss: 0.028647197508149678\n",
      "Average test loss: 0.0022764187480012574\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02861826157901022\n",
      "Average test loss: 0.002331464418830971\n",
      "Epoch 243/300\n",
      "Average training loss: 0.028629690969983735\n",
      "Average test loss: 0.0023111479758388467\n",
      "Epoch 244/300\n",
      "Average training loss: 0.028637834793991512\n",
      "Average test loss: 0.0022936516306880446\n",
      "Epoch 245/300\n",
      "Average training loss: 0.028592692593733468\n",
      "Average test loss: 0.002372530569632848\n",
      "Epoch 246/300\n",
      "Average training loss: 0.028579656960235702\n",
      "Average test loss: 0.0023010579831898212\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02858945357137256\n",
      "Average test loss: 0.00229884340096679\n",
      "Epoch 248/300\n",
      "Average training loss: 0.028564572720064057\n",
      "Average test loss: 0.0022875231749688585\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02856540720992618\n",
      "Average test loss: 0.002293387127419313\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02856361883547571\n",
      "Average test loss: 0.00232963192411181\n",
      "Epoch 251/300\n",
      "Average training loss: 0.028577335094412168\n",
      "Average test loss: 0.002323763493862417\n",
      "Epoch 252/300\n",
      "Average training loss: 0.028540056043201024\n",
      "Average test loss: 0.002280179689741797\n",
      "Epoch 253/300\n",
      "Average training loss: 0.028529911801218985\n",
      "Average test loss: 0.002380868767077724\n",
      "Epoch 254/300\n",
      "Average training loss: 0.028509509459137918\n",
      "Average test loss: 0.0023277668113716775\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028494561993413502\n",
      "Average test loss: 0.0023389507461753158\n",
      "Epoch 256/300\n",
      "Average training loss: 0.028501379188564088\n",
      "Average test loss: 0.002323294396615691\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02848365086813768\n",
      "Average test loss: 0.002329350565456682\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02848217053876983\n",
      "Average test loss: 0.002317616878077388\n",
      "Epoch 259/300\n",
      "Average training loss: 0.028464857420987554\n",
      "Average test loss: 0.0023156807103918656\n",
      "Epoch 260/300\n",
      "Average training loss: 0.028435065724783475\n",
      "Average test loss: 0.0023839286545084583\n",
      "Epoch 261/300\n",
      "Average training loss: 0.028483666787544885\n",
      "Average test loss: 0.002274611974135041\n",
      "Epoch 262/300\n",
      "Average training loss: 0.028445837759309346\n",
      "Average test loss: 0.002321797002831267\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02842343262500233\n",
      "Average test loss: 0.002320568246353004\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02842674027880033\n",
      "Average test loss: 0.0023028560678164164\n",
      "Epoch 265/300\n",
      "Average training loss: 0.02841809641652637\n",
      "Average test loss: 0.002333818959693114\n",
      "Epoch 266/300\n",
      "Average training loss: 0.028390863057639864\n",
      "Average test loss: 0.0023258023305485647\n",
      "Epoch 267/300\n",
      "Average training loss: 0.0284112162788709\n",
      "Average test loss: 0.0023242844152781697\n",
      "Epoch 268/300\n",
      "Average training loss: 0.028399474145637617\n",
      "Average test loss: 0.002338794524470965\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02840304212603304\n",
      "Average test loss: 0.0023476827579240003\n",
      "Epoch 270/300\n",
      "Average training loss: 0.028365767704115974\n",
      "Average test loss: 0.00230924238016208\n",
      "Epoch 271/300\n",
      "Average training loss: 0.028372998639941215\n",
      "Average test loss: 0.0023081583248244393\n",
      "Epoch 272/300\n",
      "Average training loss: 0.028345402368240888\n",
      "Average test loss: 0.0023338416629574365\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02834490411480268\n",
      "Average test loss: 0.002332152036846512\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02836188045144081\n",
      "Average test loss: 0.002346211933841308\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02833095215426551\n",
      "Average test loss: 0.002301351032737229\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02834875671068827\n",
      "Average test loss: 0.002316841798213621\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02828367413745986\n",
      "Average test loss: 0.002439272726989455\n",
      "Epoch 278/300\n",
      "Average training loss: 0.028298341592152915\n",
      "Average test loss: 0.0023248287544896207\n",
      "Epoch 279/300\n",
      "Average training loss: 0.02828093327416314\n",
      "Average test loss: 0.002328457131464448\n",
      "Epoch 280/300\n",
      "Average training loss: 0.028300190875927608\n",
      "Average test loss: 0.0022928701198349395\n",
      "Epoch 281/300\n",
      "Average training loss: 0.028298446893692018\n",
      "Average test loss: 0.0023001768213386335\n",
      "Epoch 282/300\n",
      "Average training loss: 0.028281840261485842\n",
      "Average test loss: 0.003892426953961452\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02834043472343021\n",
      "Average test loss: 0.002432064096753796\n",
      "Epoch 284/300\n",
      "Average training loss: 0.02824024687376287\n",
      "Average test loss: 0.002327974477575885\n",
      "Epoch 286/300\n",
      "Average training loss: 0.028256935679250292\n",
      "Average test loss: 0.0023297547532452477\n",
      "Epoch 287/300\n",
      "Average training loss: 0.028237143229279255\n",
      "Average test loss: 0.002326903705795606\n",
      "Epoch 288/300\n",
      "Average training loss: 0.028275220420625476\n",
      "Average test loss: 0.002302499647459222\n",
      "Epoch 289/300\n",
      "Average training loss: 0.028186909375919236\n",
      "Average test loss: 0.002359126446561681\n",
      "Epoch 293/300\n",
      "Average training loss: 0.028234436851408747\n",
      "Average test loss: 0.0023019287428922126\n",
      "Epoch 294/300\n",
      "Average training loss: 0.028175083266364205\n",
      "Average test loss: 0.0023159269495970675\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02821217008266184\n",
      "Average test loss: 0.002312033131925596\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02818929495414098\n",
      "Average test loss: 0.002322875471578704\n",
      "Epoch 297/300\n",
      "Average training loss: 0.028156131113568943\n",
      "Average test loss: 0.002381128045523332\n",
      "Epoch 298/300\n",
      "Average training loss: 0.028162586200568412\n",
      "Average test loss: 0.0023097853594356114\n",
      "Epoch 299/300\n",
      "Average training loss: 0.028134302236967616\n",
      "Average test loss: 0.0023114010985526776\n",
      "Epoch 300/300\n",
      "Average training loss: 0.028153175153666072\n",
      "Average test loss: 0.0023207992796475687\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive_Depth3/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 5.970088803503248\n",
      "Average test loss: 0.006133404284715652\n",
      "Epoch 2/300\n",
      "Average training loss: 0.8949944408204821\n",
      "Average test loss: 0.005223700669076708\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4734283582899306\n",
      "Average test loss: 0.005132389575656917\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3400723957750532\n",
      "Average test loss: 0.005108829723464118\n",
      "Epoch 5/300\n",
      "Average training loss: 0.2740502936972512\n",
      "Average test loss: 0.004905618528525035\n",
      "Epoch 6/300\n",
      "Average training loss: 0.23046938835250005\n",
      "Average test loss: 0.004796699899973141\n",
      "Epoch 7/300\n",
      "Average training loss: 0.20036481636100345\n",
      "Average test loss: 0.004679514270689753\n",
      "Epoch 8/300\n",
      "Average training loss: 0.18210542874866062\n",
      "Average test loss: 0.004664321718116602\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1686955431037479\n",
      "Average test loss: 0.004633397144989835\n",
      "Epoch 10/300\n",
      "Average training loss: 0.1591323946184582\n",
      "Average test loss: 0.0046268137254648735\n",
      "Epoch 11/300\n",
      "Average training loss: 0.15317934968074162\n",
      "Average test loss: 0.004607920800232225\n",
      "Epoch 12/300\n",
      "Average training loss: 0.1491065242158042\n",
      "Average test loss: 0.004539598606526852\n",
      "Epoch 13/300\n",
      "Average training loss: 0.14607369718286725\n",
      "Average test loss: 0.004687348646836148\n",
      "Epoch 14/300\n",
      "Average training loss: 0.14361004159847895\n",
      "Average test loss: 0.00452661348755161\n",
      "Epoch 15/300\n",
      "Average training loss: 0.14160217000378503\n",
      "Average test loss: 0.004503242518338892\n",
      "Epoch 16/300\n",
      "Average training loss: 0.13853061109118991\n",
      "Average test loss: 0.004427222805718581\n",
      "Epoch 18/300\n",
      "Average training loss: 0.13741651368141175\n",
      "Average test loss: 0.004406905954082807\n",
      "Epoch 19/300\n",
      "Average training loss: 0.13624816974666384\n",
      "Average test loss: 0.004402403407212761\n",
      "Epoch 20/300\n",
      "Average training loss: 0.13544613923629126\n",
      "Average test loss: 0.004429035765843259\n",
      "Epoch 21/300\n",
      "Average training loss: 0.13459074637624951\n",
      "Average test loss: 0.0043890157457855015\n",
      "Epoch 22/300\n",
      "Average training loss: 0.13385548981030782\n",
      "Average test loss: 0.0043925131948457825\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1332881827685568\n",
      "Average test loss: 0.004383545331656933\n",
      "Epoch 24/300\n",
      "Average training loss: 0.13278529858589172\n",
      "Average test loss: 0.0043523581417070495\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1321879020465745\n",
      "Average test loss: 0.004349552443044053\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13169296183188756\n",
      "Average test loss: 0.004361747759911749\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1312864317893982\n",
      "Average test loss: 0.0043371873959485024\n",
      "Epoch 28/300\n",
      "Average training loss: 0.13081460540162193\n",
      "Average test loss: 0.004329972385532326\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13056917162073983\n",
      "Average test loss: 0.004299324195418093\n",
      "Epoch 30/300\n",
      "Average training loss: 0.13021591719653872\n",
      "Average test loss: 0.004316317038817538\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12978741629918417\n",
      "Average test loss: 0.004301484091414345\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1294949301050769\n",
      "Average test loss: 0.0042798264279133745\n",
      "Epoch 33/300\n",
      "Average training loss: 0.12923152950406075\n",
      "Average test loss: 0.004305432529706094\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12887879424624973\n",
      "Average test loss: 0.004343941923851768\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12858968395657008\n",
      "Average test loss: 0.00427293869935804\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12829442006349565\n",
      "Average test loss: 0.004267914736022552\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12801864192883175\n",
      "Average test loss: 0.004279158852166599\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12784633268912635\n",
      "Average test loss: 0.004258160600645674\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1276106453206804\n",
      "Average test loss: 0.004281882311941849\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12743924367427825\n",
      "Average test loss: 0.004241855991383394\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1271138954228825\n",
      "Average test loss: 0.004256697648515304\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12693772547774845\n",
      "Average test loss: 0.004226757348825534\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12669347467687395\n",
      "Average test loss: 0.0042579177183409535\n",
      "Epoch 44/300\n",
      "Average training loss: 0.12653541054990555\n",
      "Average test loss: 0.004258004888271292\n",
      "Epoch 45/300\n",
      "Average training loss: 0.12638875926203197\n",
      "Average test loss: 0.0042518990197115475\n",
      "Epoch 46/300\n",
      "Average training loss: 0.12615318893724017\n",
      "Average test loss: 0.004269188421260979\n",
      "Epoch 47/300\n",
      "Average training loss: 0.12597464037603803\n",
      "Average test loss: 0.004225696401256654\n",
      "Epoch 48/300\n",
      "Average training loss: 0.12580516063504749\n",
      "Average test loss: 0.004217081410603391\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12559253284666275\n",
      "Average test loss: 0.004215927022198836\n",
      "Epoch 50/300\n",
      "Average training loss: 0.125411194410589\n",
      "Average test loss: 0.004219324409961701\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1253084551029735\n",
      "Average test loss: 0.004205057971179486\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1251230037742191\n",
      "Average test loss: 0.004203583893262678\n",
      "Epoch 53/300\n",
      "Average training loss: 0.125035534646776\n",
      "Average test loss: 0.004232693610712886\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12490011327134239\n",
      "Average test loss: 0.004215294978684849\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12483163146177928\n",
      "Average test loss: 0.004202176260037555\n",
      "Epoch 56/300\n",
      "Average training loss: 0.12469412542051739\n",
      "Average test loss: 0.0042905924208462235\n",
      "Epoch 57/300\n",
      "Average training loss: 0.12458418000406689\n",
      "Average test loss: 0.004213992769726449\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12444526562425826\n",
      "Average test loss: 0.0043083355406092275\n",
      "Epoch 59/300\n",
      "Average training loss: 0.12435224194659127\n",
      "Average test loss: 0.004185214672237635\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12417440368731816\n",
      "Average test loss: 0.004192625515576866\n",
      "Epoch 61/300\n",
      "Average training loss: 0.12410752548774083\n",
      "Average test loss: 0.004192891455358929\n",
      "Epoch 62/300\n",
      "Average training loss: 0.12403295225567287\n",
      "Average test loss: 0.004192667113824023\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1238180228140619\n",
      "Average test loss: 0.004187305287768443\n",
      "Epoch 64/300\n",
      "Average training loss: 0.12383024519019657\n",
      "Average test loss: 0.0041894160248339175\n",
      "Epoch 65/300\n",
      "Average training loss: 0.12370183834764692\n",
      "Average test loss: 0.004186553446162078\n",
      "Epoch 66/300\n",
      "Average training loss: 0.12351658183336257\n",
      "Average test loss: 0.004183739037977325\n",
      "Epoch 67/300\n",
      "Average training loss: 0.12348646825551987\n",
      "Average test loss: 0.004182200396640433\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12315908647908105\n",
      "Average test loss: 0.004242061965581444\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12314189035362667\n",
      "Average test loss: 0.00417611991489927\n",
      "Epoch 72/300\n",
      "Average training loss: 0.12296832648913066\n",
      "Average test loss: 0.004187667766792907\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12292102522320218\n",
      "Average test loss: 0.004188320157014661\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12280199813842774\n",
      "Average test loss: 0.00417045455508762\n",
      "Epoch 75/300\n",
      "Average training loss: 0.12272105534871419\n",
      "Average test loss: 0.004171534450931682\n",
      "Epoch 76/300\n",
      "Average training loss: 0.12264413241545359\n",
      "Average test loss: 0.004171740121725533\n",
      "Epoch 77/300\n",
      "Average training loss: 0.12254486179351806\n",
      "Average test loss: 0.0041759482199947035\n",
      "Epoch 78/300\n",
      "Average training loss: 0.12243319057093727\n",
      "Average test loss: 0.0041700150440964435\n",
      "Epoch 79/300\n",
      "Average training loss: 0.1223233329322603\n",
      "Average test loss: 0.004176992761799031\n",
      "Epoch 80/300\n",
      "Average training loss: 0.12216552235682805\n",
      "Average test loss: 0.004170080969731013\n",
      "Epoch 81/300\n",
      "Average training loss: 0.12210668541987738\n",
      "Average test loss: 0.004163275873081552\n",
      "Epoch 82/300\n",
      "Average training loss: 0.12202952222029369\n",
      "Average test loss: 0.0041780058193124\n",
      "Epoch 83/300\n",
      "Average training loss: 0.12191800764534208\n",
      "Average test loss: 0.004163046403374109\n",
      "Epoch 84/300\n",
      "Average training loss: 0.12190603133704928\n",
      "Average test loss: 0.004163811566929023\n",
      "Epoch 85/300\n",
      "Average training loss: 0.12176613649394777\n",
      "Average test loss: 0.0041906789752344294\n",
      "Epoch 86/300\n",
      "Average training loss: 0.12173432961437437\n",
      "Average test loss: 0.00418328111535973\n",
      "Epoch 87/300\n",
      "Average training loss: 0.12162853615151512\n",
      "Average test loss: 0.004170488081044621\n",
      "Epoch 88/300\n",
      "Average training loss: 0.12156813440057966\n",
      "Average test loss: 0.0041646691809097925\n",
      "Epoch 89/300\n",
      "Average training loss: 0.12122595857249366\n",
      "Average test loss: 0.004188509024679661\n",
      "Epoch 92/300\n",
      "Average training loss: 0.12108447133832508\n",
      "Average test loss: 0.004189912912953231\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1211105524301529\n",
      "Average test loss: 0.004168555087099473\n",
      "Epoch 94/300\n",
      "Average training loss: 0.12091924440860748\n",
      "Average test loss: 0.004207369974710875\n",
      "Epoch 95/300\n",
      "Average training loss: 0.1208551510307524\n",
      "Average test loss: 0.004180585931572649\n",
      "Epoch 96/300\n",
      "Average training loss: 0.12073235369390911\n",
      "Average test loss: 0.0041895384126239355\n",
      "Epoch 97/300\n",
      "Average training loss: 0.12063433866368399\n",
      "Average test loss: 0.004164417318171925\n",
      "Epoch 98/300\n",
      "Average training loss: 0.12061919181876712\n",
      "Average test loss: 0.00419329489974512\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1203739702767796\n",
      "Average test loss: 0.004187603870613708\n",
      "Epoch 100/300\n",
      "Average training loss: 0.12039329829480913\n",
      "Average test loss: 0.0041652126097016864\n",
      "Epoch 101/300\n",
      "Average training loss: 0.12032009428077274\n",
      "Average test loss: 0.004172895871516731\n",
      "Epoch 102/300\n",
      "Average training loss: 0.1201763355533282\n",
      "Average test loss: 0.0041832009793983565\n",
      "Epoch 103/300\n",
      "Average training loss: 0.12005692476034165\n",
      "Average test loss: 0.00417730037909415\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11995557963848114\n",
      "Average test loss: 0.0042039082315233015\n",
      "Epoch 105/300\n",
      "Average training loss: 0.11961860497792562\n",
      "Average test loss: 0.004204665652993653\n",
      "Epoch 108/300\n",
      "Average training loss: 0.11961898610989252\n",
      "Average test loss: 0.004164858286993371\n",
      "Epoch 109/300\n",
      "Average training loss: 0.11947123516930475\n",
      "Average test loss: 0.004307807395234704\n",
      "Epoch 110/300\n",
      "Average training loss: 0.11934885062111748\n",
      "Average test loss: 0.004182444004548921\n",
      "Epoch 111/300\n",
      "Average training loss: 0.11922864914602703\n",
      "Average test loss: 0.0041974781565368175\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1189468047287729\n",
      "Average test loss: 0.004174420362959306\n",
      "Epoch 115/300\n",
      "Average training loss: 0.11878533523612553\n",
      "Average test loss: 0.004185945614758465\n",
      "Epoch 116/300\n",
      "Average training loss: 0.1187843802637524\n",
      "Average test loss: 0.005041804232945045\n",
      "Epoch 117/300\n",
      "Average training loss: 0.11863559778531392\n",
      "Average test loss: 0.004215122015111976\n",
      "Epoch 119/300\n",
      "Average training loss: 0.11836560389730666\n",
      "Average test loss: 0.004271113275653786\n",
      "Epoch 120/300\n",
      "Average training loss: 0.11833864133225547\n",
      "Average test loss: 0.0042067503573166\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11827811118629243\n",
      "Average test loss: 0.004191649337195688\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11813077718019485\n",
      "Average test loss: 0.004186718118687471\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11799657116333644\n",
      "Average test loss: 0.0042362527990092835\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11799704933166504\n",
      "Average test loss: 0.004686620295461681\n",
      "Epoch 125/300\n",
      "Average training loss: 0.11777712372938792\n",
      "Average test loss: 0.00660574257084065\n",
      "Epoch 126/300\n",
      "Average training loss: 0.1177695666750272\n",
      "Average test loss: 0.004296991529150141\n",
      "Epoch 127/300\n",
      "Average training loss: 0.11750751711262597\n",
      "Average test loss: 0.004217841875428955\n",
      "Epoch 128/300\n",
      "Average training loss: 0.11742069598701266\n",
      "Average test loss: 0.004219561676184336\n",
      "Epoch 129/300\n",
      "Average training loss: 0.11736839282512665\n",
      "Average test loss: 0.004507357584105597\n",
      "Epoch 130/300\n",
      "Average training loss: 0.11717505946424273\n",
      "Average test loss: 0.004207109200457732\n",
      "Epoch 131/300\n",
      "Average training loss: 0.11705985479222404\n",
      "Average test loss: 0.004670039378106594\n",
      "Epoch 132/300\n",
      "Average training loss: 0.11707142211331262\n",
      "Average test loss: 0.004266576105107864\n",
      "Epoch 133/300\n",
      "Average training loss: 0.11696340358919567\n",
      "Average test loss: 0.0042291230745613575\n",
      "Epoch 134/300\n",
      "Average training loss: 0.11682342598173354\n",
      "Average test loss: 0.0042610665437661935\n",
      "Epoch 135/300\n",
      "Average training loss: 0.11672879352834489\n",
      "Average test loss: 0.004217387361658944\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11660097419553332\n",
      "Average test loss: 0.004205382356006238\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11617513171831767\n",
      "Average test loss: 0.004318576880420247\n",
      "Epoch 140/300\n",
      "Average training loss: 0.11616718655162388\n",
      "Average test loss: 0.004241956286546257\n",
      "Epoch 141/300\n",
      "Average training loss: 0.116012401037746\n",
      "Average test loss: 0.004259027983993292\n",
      "Epoch 142/300\n",
      "Average training loss: 0.1159375947382715\n",
      "Average test loss: 0.004212551492369837\n",
      "Epoch 143/300\n",
      "Average training loss: 0.11581702261500888\n",
      "Average test loss: 0.0042641352166732155\n",
      "Epoch 144/300\n",
      "Average training loss: 0.11570266483227412\n",
      "Average test loss: 0.004210934737490283\n",
      "Epoch 145/300\n",
      "Average training loss: 0.11564875445100996\n",
      "Average test loss: 0.004295084156923824\n",
      "Epoch 146/300\n",
      "Average training loss: 0.11558117124769422\n",
      "Average test loss: 0.00424716298861636\n",
      "Epoch 147/300\n",
      "Average training loss: 0.11558341177966859\n",
      "Average test loss: 0.0042762365469502074\n",
      "Epoch 148/300\n",
      "Average training loss: 0.11528936150338914\n",
      "Average test loss: 0.0042438462004065515\n",
      "Epoch 149/300\n",
      "Average training loss: 0.1152026270892885\n",
      "Average test loss: 0.004258708818298247\n",
      "Epoch 150/300\n",
      "Average training loss: 0.11509478572342131\n",
      "Average test loss: 0.004300752066903644\n",
      "Epoch 151/300\n",
      "Average training loss: 0.11513775315549639\n",
      "Average test loss: 0.004340491068032052\n",
      "Epoch 152/300\n",
      "Average training loss: 0.1148843987054295\n",
      "Average test loss: 0.004255882278498676\n",
      "Epoch 153/300\n",
      "Average training loss: 0.11481548778216044\n",
      "Average test loss: 0.0042326954685979416\n",
      "Epoch 154/300\n",
      "Average training loss: 0.11478974175453185\n",
      "Average test loss: 0.0042884943265881804\n",
      "Epoch 155/300\n",
      "Average training loss: 0.11466679023371802\n",
      "Average test loss: 0.004418856737514337\n",
      "Epoch 156/300\n",
      "Average training loss: 0.11463393956422806\n",
      "Average training loss: 0.11429134418567022\n",
      "Average test loss: 0.00433944773591227\n",
      "Epoch 159/300\n",
      "Average training loss: 0.11402623854080836\n",
      "Average test loss: 0.0042486071137504446\n",
      "Epoch 160/300\n",
      "Average training loss: 0.11416727732287513\n",
      "Average test loss: 0.00440595169737935\n",
      "Epoch 161/300\n",
      "Average training loss: 0.11397644835048251\n",
      "Average test loss: 0.004325508446329169\n",
      "Epoch 162/300\n",
      "Average training loss: 0.1139638597799672\n",
      "Average test loss: 0.00430699085816741\n",
      "Epoch 163/300\n",
      "Average training loss: 0.1138418086634742\n",
      "Average test loss: 0.004285270327081283\n",
      "Epoch 164/300\n",
      "Average training loss: 0.11387328757842381\n",
      "Average test loss: 0.004321723798703816\n",
      "Epoch 165/300\n",
      "Average training loss: 0.11359769917196698\n",
      "Average test loss: 0.00432790877421697\n",
      "Epoch 166/300\n",
      "Average training loss: 0.11357710907194349\n",
      "Average test loss: 0.004267994057801035\n",
      "Epoch 167/300\n",
      "Average training loss: 0.113470504866706\n",
      "Average test loss: 0.0043775536569042334\n",
      "Epoch 168/300\n",
      "Average training loss: 0.1133544570075141\n",
      "Average test loss: 0.004384148424284326\n",
      "Epoch 169/300\n",
      "Average training loss: 0.11348711038298077\n",
      "Average test loss: 0.004289405373649465\n",
      "Epoch 170/300\n",
      "Average training loss: 0.11310767105552885\n",
      "Average test loss: 0.004403100500711137\n",
      "Epoch 171/300\n",
      "Average training loss: 0.11316702561908298\n",
      "Average test loss: 0.004308711505598492\n",
      "Epoch 172/300\n",
      "Average training loss: 0.11289512904485066\n",
      "Average test loss: 0.00433260214039021\n",
      "Epoch 173/300\n",
      "Average training loss: 0.11298723825481202\n",
      "Average test loss: 0.004289717530210813\n",
      "Epoch 174/300\n",
      "Average training loss: 0.11286095713244544\n",
      "Average test loss: 0.004329129801442226\n",
      "Epoch 175/300\n",
      "Average training loss: 0.11269135261244244\n",
      "Average test loss: 0.0043258584878510895\n",
      "Epoch 176/300\n",
      "Average training loss: 0.11270044498973422\n",
      "Average test loss: 0.004281850698093574\n",
      "Epoch 177/300\n",
      "Average training loss: 0.11237167565690147\n",
      "Average test loss: 0.004352672375738621\n",
      "Epoch 180/300\n",
      "Average training loss: 0.11229063571161695\n",
      "Average test loss: 0.004289831949190961\n",
      "Epoch 181/300\n",
      "Average training loss: 0.1121933862235811\n",
      "Average test loss: 0.004348373180876175\n",
      "Epoch 182/300\n",
      "Average training loss: 0.11216967568794886\n",
      "Average test loss: 0.0043021117138365905\n",
      "Epoch 183/300\n",
      "Average training loss: 0.11210231557819578\n",
      "Average test loss: 0.004335518188774585\n",
      "Epoch 184/300\n",
      "Average training loss: 0.11187821174992456\n",
      "Average test loss: 0.004330854860858785\n",
      "Epoch 185/300\n",
      "Average training loss: 0.11188432082864973\n",
      "Average test loss: 0.004326672737797101\n",
      "Epoch 186/300\n",
      "Average training loss: 0.11170399673117531\n",
      "Average test loss: 0.0043882987077037494\n",
      "Epoch 187/300\n",
      "Average training loss: 0.11170022467109893\n",
      "Average test loss: 0.004339972379720873\n",
      "Epoch 188/300\n",
      "Average training loss: 0.11179133258925544\n",
      "Average test loss: 0.004360130344414049\n",
      "Epoch 189/300\n",
      "Average training loss: 0.11159978399674098\n",
      "Average test loss: 0.00428324043419626\n",
      "Epoch 190/300\n",
      "Average training loss: 0.11144134877125422\n",
      "Average test loss: 0.004326611862000492\n",
      "Epoch 191/300\n",
      "Average training loss: 0.11153902445899115\n",
      "Average test loss: 0.004390304538731774\n",
      "Epoch 192/300\n",
      "Average training loss: 0.11135591727495193\n",
      "Average test loss: 0.004373431857675314\n",
      "Epoch 193/300\n",
      "Average training loss: 0.11128431345356836\n",
      "Average test loss: 0.004408297560281224\n",
      "Epoch 194/300\n",
      "Average training loss: 0.11119734495215945\n",
      "Average test loss: 0.004333182034393151\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1110914846526252\n",
      "Average test loss: 0.004378315426202284\n",
      "Epoch 198/300\n",
      "Average training loss: 0.11097883184750874\n",
      "Average test loss: 0.004379008932453063\n",
      "Epoch 199/300\n",
      "Average training loss: 0.11073218759563234\n",
      "Average test loss: 0.004419350338478883\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1106431375278367\n",
      "Average test loss: 0.004382817779564195\n",
      "Epoch 201/300\n",
      "Average training loss: 0.1106996225118637\n",
      "Average test loss: 0.004347804432734847\n",
      "Epoch 202/300\n",
      "Average training loss: 0.11057002341747284\n",
      "Average test loss: 0.004423341238664256\n",
      "Epoch 203/300\n",
      "Average training loss: 0.11044443801376555\n",
      "Average test loss: 0.004364108190768295\n",
      "Epoch 204/300\n",
      "Average training loss: 0.11069175826178657\n",
      "Average test loss: 0.004373611917305324\n",
      "Epoch 205/300\n",
      "Average training loss: 0.11041940912273195\n",
      "Average test loss: 0.0044072706956002445\n",
      "Epoch 206/300\n",
      "Average training loss: 0.11033048199945025\n",
      "Average test loss: 0.004352167039695713\n",
      "Epoch 207/300\n",
      "Average training loss: 0.11034084604183833\n",
      "Average test loss: 0.007762683387431833\n",
      "Epoch 208/300\n",
      "Average training loss: 0.11032877386278576\n",
      "Average test loss: 0.0043240121648543415\n",
      "Epoch 209/300\n",
      "Average training loss: 0.11021652529637019\n",
      "Average test loss: 0.004518777092918753\n",
      "Epoch 210/300\n",
      "Average training loss: 0.10990373979012172\n",
      "Average test loss: 0.0044133437871932985\n",
      "Epoch 211/300\n",
      "Average training loss: 0.11002704286575317\n",
      "Average test loss: 0.0044563300816549195\n",
      "Epoch 212/300\n",
      "Average training loss: 0.10973260595401128\n",
      "Average test loss: 0.0044156429353687495\n",
      "Epoch 214/300\n",
      "Average training loss: 0.10990987573729621\n",
      "Average test loss: 0.004326587065019542\n",
      "Epoch 215/300\n",
      "Average training loss: 0.10966514935758379\n",
      "Average test loss: 0.004343570747309261\n",
      "Epoch 216/300\n",
      "Average training loss: 0.10965859107838737\n",
      "Average test loss: 0.004415839094254706\n",
      "Epoch 217/300\n",
      "Average training loss: 0.1097459589044253\n",
      "Average test loss: 0.004427924219932821\n",
      "Epoch 218/300\n",
      "Average training loss: 0.10963613753186331\n",
      "Average test loss: 0.004456233678592576\n",
      "Epoch 219/300\n",
      "Average training loss: 0.1093224105834961\n",
      "Average test loss: 0.004555815986461109\n",
      "Epoch 220/300\n",
      "Average training loss: 0.10932640384965472\n",
      "Average test loss: 0.004422354121175078\n",
      "Epoch 221/300\n",
      "Average training loss: 0.1094946504301495\n",
      "Average test loss: 0.004401277017676168\n",
      "Epoch 222/300\n",
      "Average training loss: 0.10930024577511681\n",
      "Average test loss: 0.004431300450530317\n",
      "Epoch 223/300\n",
      "Average training loss: 0.10914301794105107\n",
      "Average test loss: 0.004372789930552244\n",
      "Epoch 224/300\n",
      "Average training loss: 0.10908152711391449\n",
      "Average test loss: 0.004386172822780079\n",
      "Epoch 225/300\n",
      "Average training loss: 0.10902354114585452\n",
      "Average test loss: 0.004341679981599252\n",
      "Epoch 226/300\n",
      "Average training loss: 0.10924009577433268\n",
      "Average test loss: 0.004345873813248343\n",
      "Epoch 227/300\n",
      "Average training loss: 0.1088585378991233\n",
      "Average test loss: 0.004379870170520411\n",
      "Epoch 228/300\n",
      "Average training loss: 0.10912554674016105\n",
      "Average test loss: 0.00436824273918238\n",
      "Epoch 229/300\n",
      "Average training loss: 0.10877059559027354\n",
      "Average test loss: 0.0044327575311892565\n",
      "Epoch 230/300\n",
      "Average training loss: 0.1088741719060474\n",
      "Average test loss: 0.004695275224331353\n",
      "Epoch 231/300\n",
      "Average training loss: 0.10880825510289933\n",
      "Average test loss: 0.004409644896371497\n",
      "Epoch 232/300\n",
      "Average training loss: 0.10868111523654726\n",
      "Average test loss: 0.004371745195860664\n",
      "Epoch 233/300\n",
      "Average training loss: 0.10852637235323588\n",
      "Average test loss: 0.004322320430643028\n",
      "Epoch 234/300\n",
      "Average training loss: 0.10861168583234151\n",
      "Average test loss: 0.004463482885311047\n",
      "Epoch 235/300\n",
      "Average training loss: 0.10844650912947125\n",
      "Average test loss: 0.00437593168247905\n",
      "Epoch 236/300\n",
      "Average training loss: 0.10848468524217605\n",
      "Average test loss: 0.004375702059517304\n",
      "Epoch 238/300\n",
      "Average training loss: 0.10835683283872075\n",
      "Average test loss: 0.0044032043909860985\n",
      "Epoch 239/300\n",
      "Average training loss: 0.10816872071557575\n",
      "Average test loss: 0.0044018097842733065\n",
      "Epoch 240/300\n",
      "Average training loss: 0.10822186184591717\n",
      "Average test loss: 0.004399601333257225\n",
      "Epoch 241/300\n",
      "Average training loss: 0.1082155009508133\n",
      "Average test loss: 0.004534003549979793\n",
      "Epoch 242/300\n",
      "Average training loss: 0.10815505598651039\n",
      "Average test loss: 0.0044099892891115615\n",
      "Epoch 243/300\n",
      "Average training loss: 0.10801438677310944\n",
      "Average test loss: 0.0043739997506555585\n",
      "Epoch 244/300\n",
      "Average training loss: 0.10803617152902815\n",
      "Average test loss: 0.004443878098080556\n",
      "Epoch 245/300\n",
      "Average training loss: 0.10791066686974632\n",
      "Average test loss: 0.004444734756731325\n",
      "Epoch 246/300\n",
      "Average training loss: 0.10813613866435157\n",
      "Average test loss: 0.004427689827978611\n",
      "Epoch 247/300\n",
      "Average training loss: 0.10780869256125557\n",
      "Average test loss: 0.004570887107402086\n",
      "Epoch 248/300\n",
      "Average training loss: 0.10791521739959717\n",
      "Average test loss: 0.004488532552288638\n",
      "Epoch 249/300\n",
      "Average training loss: 0.10807937235302395\n",
      "Average test loss: 0.004362330767015616\n",
      "Epoch 250/300\n",
      "Average training loss: 0.10763212506638632\n",
      "Average test loss: 0.004390343010839489\n",
      "Epoch 251/300\n",
      "Average training loss: 0.10753292014201482\n",
      "Average test loss: 0.004412682241449754\n",
      "Epoch 252/300\n",
      "Average training loss: 0.10763155115975274\n",
      "Average test loss: 0.004453017084548871\n",
      "Epoch 253/300\n",
      "Average training loss: 0.10770177303420173\n",
      "Average test loss: 0.004413677194880115\n",
      "Epoch 255/300\n",
      "Average training loss: 0.1074783403078715\n",
      "Average test loss: 0.004466103847035103\n",
      "Epoch 256/300\n",
      "Average training loss: 0.10775879657930798\n",
      "Average test loss: 0.004365555577600996\n",
      "Epoch 257/300\n",
      "Average training loss: 0.10736724223030938\n",
      "Average test loss: 32.144678525712756\n",
      "Epoch 258/300\n",
      "Average training loss: 0.1073660070962376\n",
      "Average test loss: 0.004427253395318985\n",
      "Epoch 259/300\n",
      "Average training loss: 0.10726637697882123\n",
      "Average test loss: 0.004472156491130591\n",
      "Epoch 260/300\n",
      "Average training loss: 0.10721739645136727\n",
      "Average test loss: 0.00456401110564669\n",
      "Epoch 261/300\n",
      "Average training loss: 0.1071781662636333\n",
      "Average test loss: 0.004440946764830086\n",
      "Epoch 262/300\n",
      "Average training loss: 0.10701659381389618\n",
      "Average test loss: 0.0044878897437204916\n",
      "Epoch 263/300\n",
      "Average training loss: 0.10706557298368878\n",
      "Average test loss: 0.004492341469973325\n",
      "Epoch 264/300\n",
      "Average training loss: 0.10682917360464732\n",
      "Average test loss: 0.004406702226855689\n",
      "Epoch 265/300\n",
      "Average training loss: 0.10694721259673437\n",
      "Average test loss: 0.004405587642971012\n",
      "Epoch 266/300\n",
      "Average training loss: 0.10700736993551255\n",
      "Average test loss: 0.004373914817555083\n",
      "Epoch 267/300\n",
      "Average training loss: 0.10702533609999551\n",
      "Average test loss: 0.00458208810703622\n",
      "Epoch 268/300\n",
      "Average training loss: 0.10668561530775494\n",
      "Average test loss: 0.004463897964192762\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10676717264784708\n",
      "Average test loss: 0.0044677792052841845\n",
      "Epoch 270/300\n",
      "Average training loss: 0.10671247167719736\n",
      "Average test loss: 0.004546743703799115\n",
      "Epoch 271/300\n",
      "Average training loss: 0.10665709711445702\n",
      "Average test loss: 0.004442229047003719\n",
      "Epoch 272/300\n",
      "Average training loss: 0.1065365572836664\n",
      "Average test loss: 0.004493789565645986\n",
      "Epoch 274/300\n",
      "Average training loss: 0.1065972362657388\n",
      "Average test loss: 0.004493722511041496\n",
      "Epoch 275/300\n",
      "Average training loss: 0.10659565011660257\n",
      "Average test loss: 0.0044911750517785546\n",
      "Epoch 276/300\n",
      "Average training loss: 0.10669053698910608\n",
      "Average test loss: 0.004448001548647881\n",
      "Epoch 277/300\n",
      "Average training loss: 0.10640969008869595\n",
      "Average test loss: 0.004480717027766837\n",
      "Epoch 278/300\n",
      "Average training loss: 0.10628695588310559\n",
      "Average test loss: 0.004461055672417084\n",
      "Epoch 279/300\n",
      "Average training loss: 0.10629870250489977\n",
      "Average test loss: 0.0044757205421725905\n",
      "Epoch 280/300\n",
      "Average training loss: 0.10633390018675062\n",
      "Average test loss: 0.004550903499954277\n",
      "Epoch 281/300\n",
      "Average training loss: 0.10627546104457643\n",
      "Average test loss: 0.004527163943896691\n",
      "Epoch 282/300\n",
      "Average training loss: 0.10634252455499438\n",
      "Average test loss: 0.004534204500313434\n",
      "Epoch 283/300\n",
      "Average training loss: 0.10625723469257355\n",
      "Average test loss: 0.004381854264686505\n",
      "Epoch 284/300\n",
      "Average training loss: 0.10600393364164565\n",
      "Average test loss: 0.004614180244919327\n",
      "Epoch 285/300\n",
      "Average training loss: 0.10608482084671657\n",
      "Average test loss: 0.004420206698692507\n",
      "Epoch 286/300\n",
      "Average training loss: 0.10608785737885369\n",
      "Average test loss: 0.004492299900286727\n",
      "Epoch 287/300\n",
      "Average training loss: 0.106021218571398\n",
      "Average test loss: 0.004477278704858489\n",
      "Epoch 288/300\n",
      "Average training loss: 0.10603364545769162\n",
      "Average test loss: 0.004500774589263731\n",
      "Epoch 289/300\n",
      "Average training loss: 0.10596159658829371\n",
      "Average test loss: 0.004471750346736776\n",
      "Epoch 290/300\n",
      "Average training loss: 0.10579836737447315\n",
      "Average test loss: 0.004549472550965018\n",
      "Epoch 291/300\n",
      "Average training loss: 0.10580874777502484\n",
      "Average test loss: 0.004501041806406445\n",
      "Epoch 292/300\n",
      "Average training loss: 0.1057593342198266\n",
      "Average test loss: 0.00447304194751713\n",
      "Epoch 293/300\n",
      "Average training loss: 0.10588927962382634\n",
      "Average test loss: 0.00451908683238758\n",
      "Epoch 294/300\n",
      "Average training loss: 0.10570986534489525\n",
      "Average test loss: 0.004478345585573051\n",
      "Epoch 295/300\n",
      "Average training loss: 0.10562723231315613\n",
      "Average test loss: 0.004457033455578817\n",
      "Epoch 296/300\n",
      "Average training loss: 0.10564409629503886\n",
      "Average test loss: 0.004533181760460138\n",
      "Epoch 299/300\n",
      "Average training loss: 0.10561563495132659\n",
      "Average test loss: 0.004387324000812239\n",
      "Epoch 300/300\n",
      "Average training loss: 0.10558120565944248\n",
      "Average test loss: 0.004505976672387785\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 6.648538884162903\n",
      "Average test loss: 0.006532698285248544\n",
      "Epoch 2/300\n",
      "Average training loss: 1.0585228312280444\n",
      "Average test loss: 0.005251195907178852\n",
      "Epoch 3/300\n",
      "Average training loss: 0.5316947427855597\n",
      "Average test loss: 0.0049235087057782545\n",
      "Epoch 4/300\n",
      "Average training loss: 0.358344451268514\n",
      "Average test loss: 0.004736675945834981\n",
      "Epoch 5/300\n",
      "Average training loss: 0.27560736231009164\n",
      "Average test loss: 0.004649431390480863\n",
      "Epoch 6/300\n",
      "Average training loss: 0.2260674424701267\n",
      "Average test loss: 0.004722381578551398\n",
      "Epoch 7/300\n",
      "Average training loss: 0.19444621413283877\n",
      "Average test loss: 0.00455254154610965\n",
      "Epoch 8/300\n",
      "Average training loss: 0.17415448820590973\n",
      "Average test loss: 0.00458717567846179\n",
      "Epoch 9/300\n",
      "Average training loss: 0.16172784468862744\n",
      "Average test loss: 0.00429556105658412\n",
      "Epoch 10/300\n",
      "Average training loss: 0.15278709126181073\n",
      "Average test loss: 0.004234986498124069\n",
      "Epoch 11/300\n",
      "Average training loss: 0.14634848231739467\n",
      "Average test loss: 0.004142254570706023\n",
      "Epoch 12/300\n",
      "Average training loss: 0.14141528373294407\n",
      "Average test loss: 0.004087980214092467\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13082685646745892\n",
      "Average test loss: 0.0039343040655884474\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1283880354563395\n",
      "Average test loss: 0.003932959022207393\n",
      "Epoch 17/300\n",
      "Average training loss: 0.12613914095693166\n",
      "Average test loss: 0.0038499340472949875\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12426528041892582\n",
      "Average test loss: 0.00382605671013395\n",
      "Epoch 19/300\n",
      "Average training loss: 0.12258130701382955\n",
      "Average test loss: 0.0038532860804763105\n",
      "Epoch 20/300\n",
      "Average training loss: 0.12108869765864479\n",
      "Average test loss: 0.0037723658970660635\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1196071066790157\n",
      "Average test loss: 0.003802588431371583\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11837279366784625\n",
      "Average test loss: 0.0037126572337001563\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1173399334218767\n",
      "Average test loss: 0.003703591922505034\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1164233366780811\n",
      "Average test loss: 0.0036777812290108864\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11538944529824786\n",
      "Average test loss: 0.003686696297592587\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11437482926580642\n",
      "Average test loss: 0.0036858646880007454\n",
      "Epoch 27/300\n",
      "Average training loss: 0.11386597725417878\n",
      "Average test loss: 0.0036101686565412415\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11290623101260927\n",
      "Average test loss: 0.003617291011951036\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1120922973950704\n",
      "Average test loss: 0.0036230408404436375\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11143619577089946\n",
      "Average test loss: 0.003595910602559646\n",
      "Epoch 31/300\n",
      "Average training loss: 0.11067004901833004\n",
      "Average test loss: 0.003551251674691836\n",
      "Epoch 32/300\n",
      "Average training loss: 0.11015900238355\n",
      "Average test loss: 0.00358939511825641\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10940039615498649\n",
      "Average test loss: 0.003538995135575533\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10888809371656842\n",
      "Average test loss: 0.0035241936134795347\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10842597903807957\n",
      "Average test loss: 0.0035366893164399597\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10774440812402301\n",
      "Average test loss: 0.003511131554428074\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10746062079403136\n",
      "Average test loss: 0.003504451432161861\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10687104672193527\n",
      "Average test loss: 0.0036042672995891837\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1065124910208914\n",
      "Average test loss: 0.0035250908254335325\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10607979847987493\n",
      "Average test loss: 0.0034747949288123185\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10568907978137335\n",
      "Average test loss: 0.003483079126311673\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10526092608769734\n",
      "Average test loss: 0.003570705228588647\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10487397234969668\n",
      "Average test loss: 0.0034598307179080114\n",
      "Epoch 44/300\n",
      "Average training loss: 0.10465657156705856\n",
      "Average test loss: 0.003488153327670362\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10427827154927784\n",
      "Average test loss: 0.003455904141896301\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10398177874088288\n",
      "Average test loss: 0.0034371809162613417\n",
      "Epoch 47/300\n",
      "Average training loss: 0.1037017622590065\n",
      "Average test loss: 0.0034374765652335354\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1032829004128774\n",
      "Average test loss: 0.0034866005124317275\n",
      "Epoch 49/300\n",
      "Average training loss: 0.10311356218324767\n",
      "Average test loss: 0.003439978285175231\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10279153762261073\n",
      "Average test loss: 0.003432568964237968\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10256419385141796\n",
      "Average test loss: 0.003434999342179961\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10227810947100321\n",
      "Average test loss: 0.0034161161709990767\n",
      "Epoch 53/300\n",
      "Average training loss: 0.10207772461573283\n",
      "Average test loss: 0.003458196567164527\n",
      "Epoch 54/300\n",
      "Average training loss: 0.10192336676518123\n",
      "Average test loss: 0.003412635211729341\n",
      "Epoch 55/300\n",
      "Average training loss: 0.10166723765598402\n",
      "Average test loss: 0.00339205909644564\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10142542154259152\n",
      "Average test loss: 0.0034209988562183247\n",
      "Epoch 57/300\n",
      "Average training loss: 0.10123946346839269\n",
      "Average test loss: 0.0034424460913158126\n",
      "Epoch 58/300\n",
      "Average training loss: 0.10099953344133165\n",
      "Average test loss: 0.0034523000698536634\n",
      "Epoch 59/300\n",
      "Average training loss: 0.10081869069072935\n",
      "Average test loss: 0.0033872059997585085\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10058366829156876\n",
      "Average test loss: 0.0034067957705507675\n",
      "Epoch 61/300\n",
      "Average training loss: 0.1004262370798323\n",
      "Average test loss: 0.0033752295354174244\n",
      "Epoch 62/300\n",
      "Average training loss: 0.10013895168569353\n",
      "Average test loss: 0.003394018025034004\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09994416848818462\n",
      "Average test loss: 0.003378813095597757\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09982902740107642\n",
      "Average test loss: 0.003370395924896002\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09953118724955452\n",
      "Average test loss: 0.003410748830479052\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09937678909964032\n",
      "Average test loss: 0.0033817395961119067\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09920414644148615\n",
      "Average test loss: 0.003417220272330774\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09904994289742576\n",
      "Average test loss: 0.003378465008197559\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0987736349105835\n",
      "Average test loss: 0.0033891757289982506\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0986528840661049\n",
      "Average test loss: 0.003371307356076108\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09839751097228792\n",
      "Average test loss: 0.0034016579426825047\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09829112311204274\n",
      "Average test loss: 0.0033960492116295628\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09812479271822505\n",
      "Average test loss: 0.0033906664181914594\n",
      "Epoch 74/300\n",
      "Average training loss: 0.09796050171719657\n",
      "Average test loss: 0.0033754618672860994\n",
      "Epoch 75/300\n",
      "Average training loss: 0.09767137177122964\n",
      "Average test loss: 0.003390188621150123\n",
      "Epoch 76/300\n",
      "Average training loss: 0.09751495103041331\n",
      "Average test loss: 0.003375971505832341\n",
      "Epoch 77/300\n",
      "Average training loss: 0.09729379849963718\n",
      "Average test loss: 0.003380982983029551\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09711385924286313\n",
      "Average test loss: 0.003424419567609827\n",
      "Epoch 79/300\n",
      "Average training loss: 0.09710780566930771\n",
      "Average test loss: 0.0033989643442134065\n",
      "Epoch 80/300\n",
      "Average training loss: 0.09685509473747678\n",
      "Average test loss: 0.003367387004610565\n",
      "Epoch 81/300\n",
      "Average training loss: 0.09660750472545623\n",
      "Average test loss: 0.0033982458389881583\n",
      "Epoch 82/300\n",
      "Average training loss: 0.09641111942794588\n",
      "Average test loss: 0.00338594980041186\n",
      "Epoch 83/300\n",
      "Average training loss: 0.09627055861552557\n",
      "Average test loss: 0.003397835118489133\n",
      "Epoch 84/300\n",
      "Average training loss: 0.09608929426140256\n",
      "Average test loss: 0.003403119052035941\n",
      "Epoch 85/300\n",
      "Average training loss: 0.09592923522657819\n",
      "Average test loss: 0.0034047147771343587\n",
      "Epoch 86/300\n",
      "Average training loss: 0.09578122405211131\n",
      "Average test loss: 0.003410396024170849\n",
      "Epoch 87/300\n",
      "Average training loss: 0.09550277403328154\n",
      "Average test loss: 0.0033830321662955813\n",
      "Epoch 88/300\n",
      "Average training loss: 0.09538548119862875\n",
      "Average test loss: 0.004309186152699921\n",
      "Epoch 89/300\n",
      "Average training loss: 0.09520138732592265\n",
      "Average test loss: 0.003439135832298133\n",
      "Epoch 90/300\n",
      "Average training loss: 0.09517738727728525\n",
      "Average test loss: 0.0038654258880350327\n",
      "Epoch 91/300\n",
      "Average training loss: 0.09495017819934422\n",
      "Average test loss: 0.0033767871138536266\n",
      "Epoch 92/300\n",
      "Average training loss: 0.09471413227584627\n",
      "Average test loss: 0.003386287383321259\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09451002373960284\n",
      "Average test loss: 0.0033941548872325157\n",
      "Epoch 94/300\n",
      "Average training loss: 0.09429407124386893\n",
      "Average test loss: 0.003489713408673803\n",
      "Epoch 95/300\n",
      "Average training loss: 0.09419714978668425\n",
      "Average test loss: 0.00339307058064474\n",
      "Epoch 96/300\n",
      "Average training loss: 0.09404825456937155\n",
      "Average test loss: 0.0034007645305246114\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09381832603613535\n",
      "Average test loss: 0.003427257598274284\n",
      "Epoch 98/300\n",
      "Average training loss: 0.09372787655062145\n",
      "Average test loss: 0.0033966476950380536\n",
      "Epoch 99/300\n",
      "Average training loss: 0.093484581609567\n",
      "Average test loss: 0.003398018305707309\n",
      "Epoch 100/300\n",
      "Average training loss: 0.09340051240391202\n",
      "Average test loss: 0.0034108655701080956\n",
      "Epoch 101/300\n",
      "Average training loss: 0.09325477935870488\n",
      "Average test loss: 0.0034136583885798853\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09298118305868573\n",
      "Average test loss: 0.003502483872696757\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09300045855177773\n",
      "Average test loss: 0.0034299031175259085\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09273882432116402\n",
      "Average test loss: 0.0034352404632502134\n",
      "Epoch 105/300\n",
      "Average training loss: 0.09248847464058134\n",
      "Average test loss: 0.0035467317398223614\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09243020447757509\n",
      "Average test loss: 0.003401441512422429\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09222903233766556\n",
      "Average test loss: 0.003409335053215424\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09212417540947596\n",
      "Average test loss: 0.0035315526630729436\n",
      "Epoch 109/300\n",
      "Average training loss: 0.09196480328506894\n",
      "Average test loss: 0.0034940787706938055\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09182816845840878\n",
      "Average test loss: 0.003447573655181461\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09159213497241338\n",
      "Average test loss: 0.00340474292760094\n",
      "Epoch 112/300\n",
      "Average training loss: 0.09146891477372911\n",
      "Average test loss: 0.0034112119802998174\n",
      "Epoch 113/300\n",
      "Average training loss: 0.09133663272857666\n",
      "Average test loss: 0.0035175438473622005\n",
      "Epoch 114/300\n",
      "Average training loss: 0.09118592370880975\n",
      "Average test loss: 0.003413340735145741\n",
      "Epoch 115/300\n",
      "Average training loss: 0.09111538689335187\n",
      "Average test loss: 0.0034512016222708755\n",
      "Epoch 116/300\n",
      "Average training loss: 0.09105549230178198\n",
      "Average test loss: 0.0034542536549270154\n",
      "Epoch 117/300\n",
      "Average training loss: 0.09080684121449789\n",
      "Average test loss: 0.0034853793827609885\n",
      "Epoch 118/300\n",
      "Average training loss: 0.09071258970101674\n",
      "Average test loss: 0.0034489155978792243\n",
      "Epoch 119/300\n",
      "Average training loss: 0.09041300478908751\n",
      "Average test loss: 0.0034555659691492715\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0904894521302647\n",
      "Average test loss: 0.003471048889060815\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0905003037850062\n",
      "Average test loss: 0.003444589844180478\n",
      "Epoch 122/300\n",
      "Average training loss: 0.09016947244935565\n",
      "Average test loss: 0.003455706833137406\n",
      "Epoch 123/300\n",
      "Average training loss: 0.08998070483075248\n",
      "Average test loss: 0.0034278560512595708\n",
      "Epoch 124/300\n",
      "Average training loss: 0.08981975648138259\n",
      "Average test loss: 0.0034631022674342\n",
      "Epoch 125/300\n",
      "Average training loss: 0.08988907874955071\n",
      "Average test loss: 0.0034653150733146404\n",
      "Epoch 126/300\n",
      "Average training loss: 0.0897616418103377\n",
      "Average test loss: 0.003548320430641373\n",
      "Epoch 127/300\n",
      "Average training loss: 0.08952236477533976\n",
      "Average test loss: 0.003524438933365875\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0894242447151078\n",
      "Average test loss: 0.0035007860279745525\n",
      "Epoch 129/300\n",
      "Average training loss: 0.08946385828653972\n",
      "Average test loss: 0.003556185330781672\n",
      "Epoch 130/300\n",
      "Average training loss: 0.08950144756502576\n",
      "Average test loss: 0.003481047803329097\n",
      "Epoch 131/300\n",
      "Average training loss: 0.08900921977228589\n",
      "Average test loss: 0.0034817568651503987\n",
      "Epoch 132/300\n",
      "Average training loss: 0.08883584337433179\n",
      "Average test loss: 0.003467247055222591\n",
      "Epoch 133/300\n",
      "Average training loss: 0.08884496607383092\n",
      "Average test loss: 0.0036156806347684726\n",
      "Epoch 134/300\n",
      "Average training loss: 0.08881329256296158\n",
      "Average test loss: 0.003490975638023681\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0887520206173261\n",
      "Average test loss: 0.003494686814645926\n",
      "Epoch 136/300\n",
      "Average training loss: 0.08858535466591518\n",
      "Average test loss: 0.003462883587098784\n",
      "Epoch 137/300\n",
      "Average training loss: 0.08840576301018398\n",
      "Average test loss: 0.0035087630103031796\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0883341690103213\n",
      "Average test loss: 0.003560520484836565\n",
      "Epoch 139/300\n",
      "Average training loss: 0.08825268774562411\n",
      "Average test loss: 0.003523255235205094\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0880934466322263\n",
      "Average test loss: 0.003647610826624764\n",
      "Epoch 141/300\n",
      "Average training loss: 0.08794089255730311\n",
      "Average test loss: 0.003515760213964515\n",
      "Epoch 142/300\n",
      "Average training loss: 0.08802976622846391\n",
      "Average test loss: 0.003545554944831464\n",
      "Epoch 143/300\n",
      "Average training loss: 0.08786329803201888\n",
      "Average test loss: 0.0035211571041080685\n",
      "Epoch 144/300\n",
      "Average training loss: 0.08781532381309404\n",
      "Average test loss: 0.0035584833272215392\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0874865007797877\n",
      "Average test loss: 0.0035238199796941544\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08757854003376431\n",
      "Average test loss: 0.0035317122654782403\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08739894385470284\n",
      "Average test loss: 0.003541820855397317\n",
      "Epoch 148/300\n",
      "Average training loss: 0.08740628996491433\n",
      "Average test loss: 0.003517140372759766\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08730004506640963\n",
      "Average test loss: 0.00352831630077627\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08725854247808457\n",
      "Average test loss: 0.0035036608891354667\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08698475999302334\n",
      "Average test loss: 0.003551535933175021\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08697297752565808\n",
      "Average test loss: 0.0034803035360657505\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08692893547481961\n",
      "Average test loss: 0.003622903391304943\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08669735178682539\n",
      "Average test loss: 0.0035663721238573393\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08672112036413616\n",
      "Average test loss: 0.0035098479888919327\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08657437519232432\n",
      "Average test loss: 0.003555390789277024\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08652801248762343\n",
      "Average test loss: 0.003500181854185131\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08655396131012175\n",
      "Average test loss: 0.0034900608166224425\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08636836617191633\n",
      "Average test loss: 0.003589180891091625\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08638594338628981\n",
      "Average test loss: 0.003488986564800143\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0859914005531205\n",
      "Average test loss: 0.003531583228872882\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08613673031330109\n",
      "Average test loss: 0.0035507894733713733\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08615781350268258\n",
      "Average test loss: 0.0035030817639910513\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08601192994250191\n",
      "Average test loss: 0.0035357607261588177\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08592018175125123\n",
      "Average test loss: 0.003514291601255536\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08583956646256977\n",
      "Average test loss: 0.003560255588756667\n",
      "Epoch 167/300\n",
      "Average training loss: 0.08583268617921405\n",
      "Average test loss: 0.0035038689981318183\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0857113668984837\n",
      "Average test loss: 0.003580618811564313\n",
      "Epoch 169/300\n",
      "Average training loss: 0.08576552747355567\n",
      "Average test loss: 0.00351521077627937\n",
      "Epoch 170/300\n",
      "Average training loss: 0.08559520004855262\n",
      "Average test loss: 0.0035826036230557495\n",
      "Epoch 171/300\n",
      "Average training loss: 0.08536887698703342\n",
      "Average test loss: 0.0035203859775016703\n",
      "Epoch 172/300\n",
      "Average training loss: 0.08547269452942742\n",
      "Average test loss: 0.0035235322014325193\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08525869469510185\n",
      "Average test loss: 0.003713062802950541\n",
      "Epoch 174/300\n",
      "Average training loss: 0.08531992751028802\n",
      "Average test loss: 0.003591154008689854\n",
      "Epoch 175/300\n",
      "Average training loss: 0.08522905425230662\n",
      "Average test loss: 0.0036034431198818818\n",
      "Epoch 176/300\n",
      "Average training loss: 0.08512152865197924\n",
      "Average test loss: 0.0036017058483428423\n",
      "Epoch 177/300\n",
      "Average training loss: 0.08499727551142375\n",
      "Average test loss: 0.0035921672400501038\n",
      "Epoch 178/300\n",
      "Average training loss: 0.08506217839320501\n",
      "Average test loss: 0.0035513253836996026\n",
      "Epoch 179/300\n",
      "Average training loss: 0.08494116099675496\n",
      "Average test loss: 0.003569953854713175\n",
      "Epoch 180/300\n",
      "Average training loss: 0.08476271762450537\n",
      "Average test loss: 0.0035993228434688515\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08461891791555616\n",
      "Average test loss: 0.0036296353961030644\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0847570468319787\n",
      "Average test loss: 0.003555074003421598\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08476761944426431\n",
      "Average test loss: 0.0036000017734865346\n",
      "Epoch 184/300\n",
      "Average training loss: 0.08476615185870065\n",
      "Average test loss: 0.003544496735350953\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08461055003272162\n",
      "Average test loss: 0.0036411504815849994\n",
      "Epoch 186/300\n",
      "Average training loss: 0.08447927631934483\n",
      "Average test loss: 0.0035178375575277542\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0844196724096934\n",
      "Average test loss: 0.003517271938837237\n",
      "Epoch 188/300\n",
      "Average training loss: 0.08427822698487177\n",
      "Average test loss: 0.0035674487290283043\n",
      "Epoch 189/300\n",
      "Average training loss: 0.08437330400281483\n",
      "Average test loss: 0.0035786581995586554\n",
      "Epoch 190/300\n",
      "Average training loss: 0.08415113598770566\n",
      "Average test loss: 0.003559827965373794\n",
      "Epoch 191/300\n",
      "Average training loss: 0.08414227108822929\n",
      "Average test loss: 0.0035775249970869884\n",
      "Epoch 192/300\n",
      "Average training loss: 0.08406933143734932\n",
      "Average test loss: 0.0035847369676662816\n",
      "Epoch 193/300\n",
      "Average training loss: 0.08390740125046836\n",
      "Average test loss: 0.0035761361006233427\n",
      "Epoch 194/300\n",
      "Average training loss: 0.08402603356043498\n",
      "Average test loss: 0.003588646254605717\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08418052517705493\n",
      "Average test loss: 0.0036079806559201743\n",
      "Epoch 196/300\n",
      "Average training loss: 0.08392293614811368\n",
      "Average test loss: 0.004122556424389283\n",
      "Epoch 197/300\n",
      "Average training loss: 0.08375234960185157\n",
      "Average test loss: 0.003623273353609774\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0836629553106096\n",
      "Average test loss: 0.003591695992483033\n",
      "Epoch 199/300\n",
      "Average training loss: 0.08365347216526667\n",
      "Average test loss: 0.003530518854657809\n",
      "Epoch 200/300\n",
      "Average training loss: 0.08362176724274953\n",
      "Average test loss: 0.0036150751629223424\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08341794145107269\n",
      "Average test loss: 0.003590835781561004\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08351400668753518\n",
      "Average test loss: 0.0036107737087748117\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08334992995527056\n",
      "Average test loss: 0.0035999074065023\n",
      "Epoch 204/300\n",
      "Average training loss: 0.08340641018417147\n",
      "Average test loss: 0.0036429554960793918\n",
      "Epoch 205/300\n",
      "Average training loss: 0.08352563434176975\n",
      "Average test loss: 0.0035893945364902416\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08323779957824283\n",
      "Average test loss: 0.0035453837035844724\n",
      "Epoch 207/300\n",
      "Average training loss: 0.083500187350644\n",
      "Average test loss: 0.0035834441524412895\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08319534463352628\n",
      "Average test loss: 0.0035669290158483716\n",
      "Epoch 209/300\n",
      "Average training loss: 0.08306262852085962\n",
      "Average test loss: 0.0036111342861420577\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08314411670631833\n",
      "Average test loss: 0.0036250057597127226\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08299901503324508\n",
      "Average test loss: 0.003624847133954366\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0828435052368376\n",
      "Average test loss: 0.0035920555473615725\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08290135339895884\n",
      "Average test loss: 0.0036596793954571088\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08313564485973782\n",
      "Average test loss: 0.003645785877067182\n",
      "Epoch 215/300\n",
      "Average training loss: 0.08294384176532428\n",
      "Average test loss: 0.003632330882880423\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08269961206780539\n",
      "Average test loss: 0.00359474749242266\n",
      "Epoch 217/300\n",
      "Average training loss: 0.08270025355617205\n",
      "Average test loss: 0.0036092699308776194\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08270084536737866\n",
      "Average test loss: 0.0036329674563474125\n",
      "Epoch 219/300\n",
      "Average training loss: 0.08267829314867656\n",
      "Average test loss: 0.003605109685824977\n",
      "Epoch 220/300\n",
      "Average training loss: 0.08259052549468146\n",
      "Average test loss: 0.0036520361838241417\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0826833714445432\n",
      "Average test loss: 0.003628198973006672\n",
      "Epoch 222/300\n",
      "Average training loss: 0.08259320525328319\n",
      "Average test loss: 0.0036679694884353214\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08249147182703018\n",
      "Average test loss: 0.003589451171043846\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0823419934047593\n",
      "Average test loss: 0.0037048454731702803\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0822286084095637\n",
      "Average test loss: 0.0036412127380155853\n",
      "Epoch 226/300\n",
      "Average training loss: 0.08239496687385771\n",
      "Average test loss: 0.0035639239591028957\n",
      "Epoch 227/300\n",
      "Average training loss: 0.08229547416501574\n",
      "Average test loss: 0.003584801459271047\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08200238290760252\n",
      "Average test loss: 0.003679735056642029\n",
      "Epoch 231/300\n",
      "Average training loss: 0.08221063753962517\n",
      "Average test loss: 0.003621992532370819\n",
      "Epoch 232/300\n",
      "Average training loss: 0.08196454898516337\n",
      "Average test loss: 0.0036579981374864775\n",
      "Epoch 233/300\n",
      "Average training loss: 0.08191805554760827\n",
      "Average test loss: 0.0036113420555161104\n",
      "Epoch 234/300\n",
      "Average training loss: 0.08206012468205558\n",
      "Average test loss: 0.003748230066564348\n",
      "Epoch 235/300\n",
      "Average training loss: 0.08193701090415319\n",
      "Average test loss: 0.0036461553890258075\n",
      "Epoch 236/300\n",
      "Average training loss: 0.08184817171096802\n",
      "Average test loss: 0.003589564829443892\n",
      "Epoch 237/300\n",
      "Average training loss: 0.08184537045823204\n",
      "Average test loss: 0.0036279562215010327\n",
      "Epoch 238/300\n",
      "Average training loss: 0.08166045727994707\n",
      "Average test loss: 0.0036387722533610133\n",
      "Epoch 239/300\n",
      "Average training loss: 0.08185213106208378\n",
      "Average test loss: 0.0035953612079222996\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08163495037953059\n",
      "Average test loss: 0.0036206653614838918\n",
      "Epoch 241/300\n",
      "Average training loss: 0.08166609736945894\n",
      "Average test loss: 0.0036220218514402707\n",
      "Epoch 242/300\n",
      "Average training loss: 0.08163866702053282\n",
      "Average test loss: 0.003628375662283765\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08158647329277463\n",
      "Average test loss: 0.00365242418853773\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08165221424897512\n",
      "Average test loss: 0.0036153831161144707\n",
      "Epoch 245/300\n",
      "Average training loss: 0.08149804321262571\n",
      "Average test loss: 0.0036318840937068064\n",
      "Epoch 246/300\n",
      "Average training loss: 0.08138710348473654\n",
      "Average test loss: 0.003628603246890836\n",
      "Epoch 247/300\n",
      "Average training loss: 0.08160614774955643\n",
      "Average test loss: 0.003681272024495734\n",
      "Epoch 248/300\n",
      "Average training loss: 0.08154528862237931\n",
      "Average test loss: 0.003621460062969062\n",
      "Epoch 249/300\n",
      "Average training loss: 0.08127372932434082\n",
      "Average test loss: 0.0036594617302632996\n",
      "Epoch 250/300\n",
      "Average training loss: 0.08119276696443557\n",
      "Average test loss: 0.0037262333650141953\n",
      "Epoch 251/300\n",
      "Average training loss: 0.08128739804691738\n",
      "Average test loss: 0.003580788662450181\n",
      "Epoch 252/300\n",
      "Average training loss: 0.08115766497453054\n",
      "Average test loss: 0.0037004500184622076\n",
      "Epoch 253/300\n",
      "Average training loss: 0.08120604893896315\n",
      "Average test loss: 0.0036277661491185428\n",
      "Epoch 254/300\n",
      "Average training loss: 0.08115259324179755\n",
      "Average test loss: 0.0036749132950272827\n",
      "Epoch 255/300\n",
      "Average training loss: 0.08114294261402553\n",
      "Average test loss: 0.0037466743679510223\n",
      "Epoch 256/300\n",
      "Average training loss: 0.08104762711127599\n",
      "Average test loss: 0.00369270139767064\n",
      "Epoch 257/300\n",
      "Average training loss: 0.08108031280173196\n",
      "Average test loss: 0.0036059124221404395\n",
      "Epoch 258/300\n",
      "Average training loss: 0.08098210565249125\n",
      "Average test loss: 0.003784225873235199\n",
      "Epoch 259/300\n",
      "Average training loss: 0.08103334261973699\n",
      "Average test loss: 0.0036830481820636324\n",
      "Epoch 260/300\n",
      "Average training loss: 0.08091606182522244\n",
      "Average test loss: 0.0036817524751855267\n",
      "Epoch 261/300\n",
      "Average training loss: 0.08090109271473354\n",
      "Average test loss: 0.0036061865946071013\n",
      "Epoch 262/300\n",
      "Average training loss: 0.0807119777434402\n",
      "Average test loss: 0.0037539317111174265\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0808144884771771\n",
      "Average test loss: 0.0036271116762525506\n",
      "Epoch 264/300\n",
      "Average training loss: 0.08069479447603226\n",
      "Average test loss: 0.003649252231336302\n",
      "Epoch 265/300\n",
      "Average training loss: 0.08077256595426135\n",
      "Average test loss: 0.0036229846911090945\n",
      "Epoch 266/300\n",
      "Average training loss: 0.08074876821372244\n",
      "Average test loss: 0.0036847082782122823\n",
      "Epoch 267/300\n",
      "Average training loss: 0.08074812822209464\n",
      "Average test loss: 0.003654462000148164\n",
      "Epoch 268/300\n",
      "Average training loss: 0.08067018687725067\n",
      "Average test loss: 0.003725646612751815\n",
      "Epoch 269/300\n",
      "Average training loss: 0.08059765591224034\n",
      "Average test loss: 0.003621650827427705\n",
      "Epoch 270/300\n",
      "Average training loss: 0.08064735804663764\n",
      "Average test loss: 0.0036330093095699946\n",
      "Epoch 271/300\n",
      "Average training loss: 0.08057041356960933\n",
      "Average test loss: 0.0036703346400625177\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08061346854103936\n",
      "Average test loss: 0.0036159792323079373\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08043158928553264\n",
      "Average test loss: 0.003666909581878119\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08052467254135344\n",
      "Average test loss: 0.0036370144540237054\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08048880472448137\n",
      "Average test loss: 0.0036947227248715028\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08032848999897639\n",
      "Average test loss: 0.0037192261777818205\n",
      "Epoch 277/300\n",
      "Average training loss: 0.08042057226101558\n",
      "Average test loss: 0.003660327862120337\n",
      "Epoch 278/300\n",
      "Average training loss: 0.08045879426267412\n",
      "Average test loss: 0.0038763750203781658\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08018923276000553\n",
      "Average test loss: 0.0036534585915505885\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08023973383175002\n",
      "Average test loss: 0.0036479592070811323\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0802898709376653\n",
      "Average test loss: 0.0036576776117500332\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08019157426224814\n",
      "Average test loss: 0.003649513246284591\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08020359943310419\n",
      "Average test loss: 0.0036086910503606\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08006195640563965\n",
      "Average test loss: 0.0037034412510693073\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0802097425626384\n",
      "Average test loss: 0.003681193677915467\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0800944013926718\n",
      "Average test loss: 0.0036671135268277594\n",
      "Epoch 287/300\n",
      "Average training loss: 0.08007443717453215\n",
      "Average test loss: 0.003629149897230996\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07998068114121755\n",
      "Average test loss: 0.003704127560887072\n",
      "Epoch 289/300\n",
      "Average training loss: 0.07988169876072142\n",
      "Average test loss: 0.0036922591254115104\n",
      "Epoch 290/300\n",
      "Average training loss: 0.07982027665111753\n",
      "Average test loss: 0.003586028816592362\n",
      "Epoch 291/300\n",
      "Average training loss: 0.07991261468993292\n",
      "Average test loss: 0.003696741159177489\n",
      "Epoch 292/300\n",
      "Average training loss: 0.07989785268571642\n",
      "Average test loss: 0.0036386894430551265\n",
      "Epoch 293/300\n",
      "Average training loss: 0.07993607954184215\n",
      "Average test loss: 0.0036645378820184204\n",
      "Epoch 294/300\n",
      "Average training loss: 0.07982435317834219\n",
      "Average test loss: 0.0036424829761187234\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07984976007541021\n",
      "Average test loss: 0.0036141581210411256\n",
      "Epoch 296/300\n",
      "Average training loss: 0.07984520131349564\n",
      "Average test loss: 0.0037261857986450194\n",
      "Epoch 297/300\n",
      "Average training loss: 0.07981703535053465\n",
      "Average test loss: 0.0036204215611020723\n",
      "Epoch 298/300\n",
      "Average training loss: 0.079650701754623\n",
      "Average test loss: 0.0035963238314208057\n",
      "Epoch 299/300\n",
      "Average training loss: 0.07969345329205195\n",
      "Average test loss: 0.0036549075001643763\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0795540128019121\n",
      "Average test loss: 0.003744306306458182\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.730877173847622\n",
      "Average test loss: 0.0063255306676858\n",
      "Epoch 2/300\n",
      "Average training loss: 0.9249041134516398\n",
      "Average test loss: 0.005163485953791274\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4794961653550466\n",
      "Average test loss: 0.00468967812259992\n",
      "Epoch 4/300\n",
      "Average training loss: 0.3157669965955946\n",
      "Average test loss: 0.004363140403810474\n",
      "Epoch 5/300\n",
      "Average training loss: 0.24014902500311533\n",
      "Average test loss: 0.00420583170817958\n",
      "Epoch 6/300\n",
      "Average training loss: 0.19994551412264505\n",
      "Average test loss: 0.004083632587558694\n",
      "Epoch 7/300\n",
      "Average training loss: 0.17476285666889615\n",
      "Average test loss: 0.00465516123154925\n",
      "Epoch 8/300\n",
      "Average training loss: 0.15779374958409204\n",
      "Average test loss: 0.003904565898494588\n",
      "Epoch 9/300\n",
      "Average training loss: 0.14627677286333507\n",
      "Average test loss: 0.0038576967362314463\n",
      "Epoch 10/300\n",
      "Average training loss: 0.13806777256064945\n",
      "Average test loss: 0.0037751917261630297\n",
      "Epoch 11/300\n",
      "Average training loss: 0.13182545034090679\n",
      "Average test loss: 0.003628478832128975\n",
      "Epoch 12/300\n",
      "Average training loss: 0.12677674584918552\n",
      "Average test loss: 0.0036114904727372857\n",
      "Epoch 13/300\n",
      "Average training loss: 0.12243247909016079\n",
      "Average test loss: 0.0035752516821440723\n",
      "Epoch 14/300\n",
      "Average training loss: 0.11893145671817991\n",
      "Average test loss: 0.003410963536550601\n",
      "Epoch 15/300\n",
      "Average training loss: 0.11574864639838536\n",
      "Average test loss: 0.003376676781516936\n",
      "Epoch 16/300\n",
      "Average training loss: 0.11299707771672143\n",
      "Average test loss: 0.003345846916238467\n",
      "Epoch 17/300\n",
      "Average training loss: 0.11056746841139263\n",
      "Average test loss: 0.0032444351793577274\n",
      "Epoch 18/300\n",
      "Average training loss: 0.10845176631212235\n",
      "Average test loss: 0.003216121164874898\n",
      "Epoch 19/300\n",
      "Average training loss: 0.10638486323753993\n",
      "Average test loss: 0.0031723475615597434\n",
      "Epoch 20/300\n",
      "Average training loss: 0.10465275949570868\n",
      "Average test loss: 0.003122186169649164\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10289837171302901\n",
      "Average test loss: 0.0031415755036804413\n",
      "Epoch 22/300\n",
      "Average training loss: 0.10133470237255096\n",
      "Average test loss: 0.0030543006513681675\n",
      "Epoch 23/300\n",
      "Average training loss: 0.09998137008481556\n",
      "Average test loss: 0.0030330740337570507\n",
      "Epoch 24/300\n",
      "Average training loss: 0.09870240693622165\n",
      "Average test loss: 0.0030158001182393895\n",
      "Epoch 25/300\n",
      "Average training loss: 0.09737579675184356\n",
      "Average test loss: 0.0030126803640483156\n",
      "Epoch 26/300\n",
      "Average training loss: 0.09628563641177283\n",
      "Average test loss: 0.002979818348876304\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09531009708510504\n",
      "Average test loss: 0.0029571375170101724\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09432341576284832\n",
      "Average test loss: 0.0029672202358229293\n",
      "Epoch 29/300\n",
      "Average training loss: 0.09346865158610873\n",
      "Average test loss: 0.002975743578539954\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09265485154920154\n",
      "Average test loss: 0.002989912473079231\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09180236347516378\n",
      "Average test loss: 0.0029057796044896045\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09122964382833905\n",
      "Average test loss: 0.0028940628717343013\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09058514102962283\n",
      "Average test loss: 0.002879391312599182\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09008978554937574\n",
      "Average test loss: 0.0028802577919430205\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08932815723286734\n",
      "Average test loss: 0.0028680109060886835\n",
      "Epoch 36/300\n",
      "Average training loss: 0.08893588515122731\n",
      "Average test loss: 0.0029259404693212773\n",
      "Epoch 37/300\n",
      "Average training loss: 0.08835925440655815\n",
      "Average test loss: 0.0028525090298304953\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0878737239042918\n",
      "Average test loss: 0.002843112387797899\n",
      "Epoch 39/300\n",
      "Average training loss: 0.08730626548661126\n",
      "Average test loss: 0.0028167280854864253\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08696874959601296\n",
      "Average test loss: 0.002842595767436756\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08651426372263167\n",
      "Average test loss: 0.002820033763638801\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08609739928775363\n",
      "Average test loss: 0.002806063272472885\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08587682686249415\n",
      "Average test loss: 0.00280404218327668\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08557042341099845\n",
      "Average test loss: 0.0027788817547261713\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08507377427154117\n",
      "Average test loss: 0.0027662533602366844\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08479732802841398\n",
      "Average test loss: 0.002808190541755822\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08449486105309592\n",
      "Average test loss: 0.0027822370193898677\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08432654082112842\n",
      "Average test loss: 0.0027501736390921805\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08391945272021824\n",
      "Average test loss: 0.0027545313853770494\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08369942225350274\n",
      "Average test loss: 0.0027550240606069565\n",
      "Epoch 51/300\n",
      "Average training loss: 0.08343496633238262\n",
      "Average test loss: 0.0027625375491463476\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08322012390030754\n",
      "Average test loss: 0.0027324717073804804\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08288891461822721\n",
      "Average test loss: 0.002754978987077872\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08274926481644312\n",
      "Average test loss: 0.0027274292945447896\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0823562358584669\n",
      "Average test loss: 0.0027648430094122886\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08219493920935525\n",
      "Average test loss: 0.0027235952439821427\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08205826761987474\n",
      "Average test loss: 0.002733932683037387\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08177954026725558\n",
      "Average test loss: 0.002720638148072693\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08162464143832525\n",
      "Average test loss: 0.0027623352939262985\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08137940169705285\n",
      "Average test loss: 0.0027290443811151715\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08121920382976532\n",
      "Average test loss: 0.002741680029572712\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08106157516108618\n",
      "Average test loss: 0.0027722817772171562\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08079601777262158\n",
      "Average test loss: 0.0027245971757090754\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08060802768336402\n",
      "Average test loss: 0.0027259613000270395\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0803980622722043\n",
      "Average test loss: 0.0027366212562968334\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08024936186605029\n",
      "Average test loss: 0.0027164487945329813\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08006733476453358\n",
      "Average test loss: 0.0027443849405066833\n",
      "Epoch 68/300\n",
      "Average training loss: 0.07987092941999435\n",
      "Average test loss: 0.002703887940280967\n",
      "Epoch 69/300\n",
      "Average training loss: 0.07978667898310556\n",
      "Average test loss: 0.0027404267177399662\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07960053555170694\n",
      "Average test loss: 0.0031611604938904443\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07931644621160296\n",
      "Average test loss: 0.002735644339480334\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07916793627540271\n",
      "Average test loss: 0.002713892857958045\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0790123399760988\n",
      "Average test loss: 0.0027308494405200085\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07880033889081743\n",
      "Average test loss: 0.002707648785991801\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07860067566235861\n",
      "Average test loss: 0.002731310128337807\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07843501006232367\n",
      "Average test loss: 0.0027112104735440676\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07846048341194789\n",
      "Average test loss: 0.0027091066737969715\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0780361663268672\n",
      "Average test loss: 0.002706221316734122\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07794599408573574\n",
      "Average test loss: 0.0027118364063402015\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07779022492965063\n",
      "Average test loss: 0.0027285010704977647\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07758900021182166\n",
      "Average test loss: 0.0027246071986026235\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07744327124622133\n",
      "Average test loss: 0.00270179124093718\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07727234256267547\n",
      "Average test loss: 0.0027125290744006636\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07707536081473032\n",
      "Average test loss: 0.0027101289867940874\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07700431560807758\n",
      "Average test loss: 0.002718498463018073\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07682923132843442\n",
      "Average test loss: 0.0027468252192354866\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07662230297260814\n",
      "Average test loss: 0.0027655440020478432\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07642865849865807\n",
      "Average test loss: 0.0027150216897328695\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07625451424386766\n",
      "Average test loss: 0.0027705238527721827\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07613490364948909\n",
      "Average test loss: 0.0027363594399972095\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07592316799693638\n",
      "Average test loss: 0.0027243229698182807\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07588693687650892\n",
      "Average test loss: 0.0028154376819729803\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07564797349770865\n",
      "Average test loss: 0.0027295672972169186\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07551493277814653\n",
      "Average test loss: 0.0027239885485420626\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07534319176938799\n",
      "Average test loss: 0.002718605508820878\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07525701091024611\n",
      "Average test loss: 0.002752904352835483\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07523036623332235\n",
      "Average test loss: 0.0027389583055757814\n",
      "Epoch 98/300\n",
      "Average training loss: 0.0748832934498787\n",
      "Average test loss: 0.0027508240288330447\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07471113347344928\n",
      "Average test loss: 0.00274795429677599\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07461476308107376\n",
      "Average test loss: 0.0027456602009220256\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07456791452566783\n",
      "Average test loss: 0.0027339998926553463\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07438294667667812\n",
      "Average test loss: 0.0027501815458138784\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07422899316416846\n",
      "Average test loss: 0.0027754125294999943\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07411783649524054\n",
      "Average test loss: 0.0027539974469691516\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07391927679379781\n",
      "Average test loss: 0.0027622636682871314\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07388747546407912\n",
      "Average test loss: 0.002755487331499656\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07368735597862137\n",
      "Average test loss: 0.0027711550156689353\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0735424703558286\n",
      "Average test loss: 0.0027446846779849796\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07338373351759381\n",
      "Average test loss: 0.002748207569950157\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07347250112560061\n",
      "Average test loss: 0.002759055897179577\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0731693496770329\n",
      "Average test loss: 0.0027719628806743358\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07313650716675653\n",
      "Average test loss: 0.0027417140147752233\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07309862654076682\n",
      "Average test loss: 0.0027719963232262267\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07286643825305833\n",
      "Average test loss: 0.0027692832394192617\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07266823784510294\n",
      "Average test loss: 0.0027749557099822496\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07259700600968468\n",
      "Average test loss: 0.0028755363139013447\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07248079425096512\n",
      "Average test loss: 0.0027756879474553795\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07229454120000203\n",
      "Average test loss: 0.00277836715326541\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07230536378092237\n",
      "Average test loss: 0.0027563762712395855\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07210283620489968\n",
      "Average test loss: 0.002777743661776185\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07200207449330225\n",
      "Average test loss: 0.002795908391268717\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07179726195335388\n",
      "Average test loss: 0.002772661690082815\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07183368194434378\n",
      "Average test loss: 0.0027956353028615317\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07165303818384806\n",
      "Average test loss: 0.002834147036075592\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07155275648501185\n",
      "Average test loss: 0.00280312817171216\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07151848960585064\n",
      "Average test loss: 0.0028347634927680097\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07140874132182863\n",
      "Average test loss: 0.0027773981183353397\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07124799173408085\n",
      "Average test loss: 0.0028033896856423882\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07106610347827276\n",
      "Average test loss: 0.0028074915272494156\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0711841292447514\n",
      "Average test loss: 0.0028033222189793984\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07090971434116364\n",
      "Average test loss: 0.0028671167890230815\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07119832830958896\n",
      "Average test loss: 0.0028087014853954316\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0708317865398195\n",
      "Average test loss: 0.002791988993063569\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07057576962974337\n",
      "Average test loss: 0.0028045475954810778\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07046170767148335\n",
      "Average test loss: 0.0027908907665146723\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07045291848315133\n",
      "Average test loss: 0.0027890316038909884\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07037692906459173\n",
      "Average test loss: 0.0028061760695030293\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07019442791740099\n",
      "Average test loss: 0.002856826199632552\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07027194324466918\n",
      "Average test loss: 0.0029309182572695943\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06998323761092291\n",
      "Average test loss: 0.0027717628615597885\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07000266006257799\n",
      "Average test loss: 0.0028467904925346373\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07012668120861053\n",
      "Average test loss: 0.0028337186228276955\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06989192613628176\n",
      "Average test loss: 0.0028329310301277374\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06977901593844096\n",
      "Average test loss: 0.0028142926152795555\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06960787813531027\n",
      "Average test loss: 0.0028032036183608905\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06958162442180846\n",
      "Average test loss: 0.002803837141022086\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06949303385284211\n",
      "Average test loss: 0.0028133581580801143\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06933438050084643\n",
      "Average test loss: 0.002822705433393518\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06947600391176012\n",
      "Average test loss: 0.002818117602210906\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06917819780111313\n",
      "Average test loss: 0.0028219102968772255\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06923964152071212\n",
      "Average test loss: 0.0028239797575192317\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06914765118890338\n",
      "Average test loss: 0.0028489419662704072\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06902094734377331\n",
      "Average test loss: 0.002837658816948533\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06900062767995728\n",
      "Average test loss: 0.0028561571025186114\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06893829053309229\n",
      "Average test loss: 0.0028188766038252247\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06876515109671487\n",
      "Average test loss: 0.002876400363528066\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06873613241646025\n",
      "Average test loss: 0.002959813634554545\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06864981521831619\n",
      "Average test loss: 0.002875439797838529\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06867019282446968\n",
      "Average test loss: 0.0028392729439462223\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06857541611128383\n",
      "Average test loss: 0.0028803917898072135\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06857784817285008\n",
      "Average test loss: 0.002918128454229898\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0684873485730754\n",
      "Average test loss: 0.0028615092074291575\n",
      "Epoch 163/300\n",
      "Average training loss: 0.068346949034267\n",
      "Average test loss: 0.0028224828475051458\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06821151090992822\n",
      "Average test loss: 0.0028702644500881434\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06818717715144157\n",
      "Average test loss: 0.0028425634279847147\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06819280509154002\n",
      "Average test loss: 0.002851836918335822\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06831599802441067\n",
      "Average test loss: 0.002854724722603957\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06801626730627484\n",
      "Average test loss: 0.002917559835025006\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06796456753545337\n",
      "Average test loss: 0.0028770264856931237\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06801073863771227\n",
      "Average test loss: 0.0028601910869280496\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06778487397233646\n",
      "Average test loss: 0.002959201842960384\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06790760285324521\n",
      "Average test loss: 0.002843776099797752\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06774355571137534\n",
      "Average test loss: 0.0028854227643460035\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06766010387076273\n",
      "Average test loss: 0.0028498765944192806\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06767631188366148\n",
      "Average test loss: 0.002879732323396537\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06743973367081749\n",
      "Average test loss: 0.0028241810722069607\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06751679717501005\n",
      "Average test loss: 0.002874916669395235\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0674045746922493\n",
      "Average test loss: 0.002886892000420226\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06748061100641886\n",
      "Average test loss: 0.0028379622720595865\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06729783487319946\n",
      "Average test loss: 0.002865094676613808\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06720447413788902\n",
      "Average test loss: 0.0029378845043894317\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06725021280182733\n",
      "Average test loss: 0.0028845202786227067\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06708214148547914\n",
      "Average test loss: 0.0029711911781794497\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06707862381802665\n",
      "Average test loss: 0.002865522807670964\n",
      "Epoch 185/300\n",
      "Average training loss: 0.067121565176381\n",
      "Average test loss: 0.0028598477743152113\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06695023317469491\n",
      "Average test loss: 0.002819343921417991\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06697887359725105\n",
      "Average test loss: 0.002835543970680899\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06690361599789725\n",
      "Average test loss: 0.0034402675980495082\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06695640904373593\n",
      "Average test loss: 0.0029212552031709087\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06676991849475437\n",
      "Average test loss: 0.002864446268727382\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06683059311244223\n",
      "Average test loss: 0.00303313117639886\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06679045740763347\n",
      "Average test loss: 0.0028554580045036144\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06669266266292996\n",
      "Average test loss: 0.002954915936001473\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06668903565075662\n",
      "Average test loss: 0.0028650545407500534\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06663488471508026\n",
      "Average test loss: 0.0028764764265053803\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06649670051866108\n",
      "Average test loss: 0.002867230726405978\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06644970050123003\n",
      "Average test loss: 0.0028440550174564125\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0667227658993668\n",
      "Average test loss: 0.0028923418370799888\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06630634723769294\n",
      "Average test loss: 0.0029141616916490928\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06631642037630081\n",
      "Average test loss: 0.0028756634613706007\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06621908369329241\n",
      "Average test loss: 0.002941182023742133\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06635549232032564\n",
      "Average test loss: 0.0029114682991057635\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06637121021747588\n",
      "Average test loss: 0.0029180875546816324\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06615435531404283\n",
      "Average test loss: 0.0028710640981379483\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06604647321833504\n",
      "Average test loss: 0.002964227235565583\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0661125575767623\n",
      "Average test loss: 0.0030076222421808376\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06613125750753615\n",
      "Average test loss: 0.0028703667401439614\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06610578809844123\n",
      "Average test loss: 0.0028963818254156247\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06594512246714698\n",
      "Average test loss: 0.003000830956010355\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06590239282449087\n",
      "Average test loss: 0.0028984213057491512\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06585430818133883\n",
      "Average test loss: 0.0028797060530632735\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06596085337797801\n",
      "Average test loss: 0.002900198993169599\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06586784534984165\n",
      "Average test loss: 0.0028854849477195077\n",
      "Epoch 214/300\n",
      "Average training loss: 0.06565846949153477\n",
      "Average test loss: 0.0028861344328357116\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06584570152892007\n",
      "Average test loss: 0.0029069984168228176\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06577591234445572\n",
      "Average test loss: 0.0028946399446576833\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06566976907849312\n",
      "Average test loss: 0.002923010694069995\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06561754333310657\n",
      "Average test loss: 0.0029270052812579604\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06555867318974601\n",
      "Average test loss: 0.0029739706937430633\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06550953585240576\n",
      "Average test loss: 0.0029507742484824527\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06546476756864124\n",
      "Average test loss: 0.003740029693891605\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06549299160639445\n",
      "Average test loss: 0.0029013979942020445\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06537224104669359\n",
      "Average test loss: 0.0029038478864563838\n",
      "Epoch 224/300\n",
      "Average training loss: 0.06547553704182307\n",
      "Average test loss: 0.0028941695609440406\n",
      "Epoch 225/300\n",
      "Average training loss: 0.06531764654318492\n",
      "Average test loss: 0.002945038998913434\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06533951307667626\n",
      "Average test loss: 0.0029066356987588935\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0653534561196963\n",
      "Average test loss: 0.0029380196276017363\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06525608699851566\n",
      "Average test loss: 0.003137312641367316\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0653008791870541\n",
      "Average test loss: 0.002912664599509703\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06512465151151021\n",
      "Average test loss: 0.0029415781052990093\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06521132906940248\n",
      "Average test loss: 0.003049882515644034\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06505574108163516\n",
      "Average test loss: 0.002939458944938249\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06497022196650505\n",
      "Average test loss: 0.0028883496816787457\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0649393087029457\n",
      "Average test loss: 0.0029374332773602673\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06508741129106946\n",
      "Average test loss: 0.0029630955590142145\n",
      "Epoch 236/300\n",
      "Average training loss: 0.06498366608222325\n",
      "Average test loss: 0.0029329764088615773\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06495257718364397\n",
      "Average test loss: 0.0028957241320361695\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06493876415159967\n",
      "Average test loss: 0.002932033188951512\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06491952333185408\n",
      "Average test loss: 0.002947641668220361\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06484705077939563\n",
      "Average test loss: 0.0030687976516783236\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06488621638218561\n",
      "Average test loss: 0.0029333395411570867\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06469187595446904\n",
      "Average test loss: 0.002925705104652378\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06484019249015384\n",
      "Average test loss: 0.0029739610991544193\n",
      "Epoch 244/300\n",
      "Average training loss: 0.064804374674956\n",
      "Average test loss: 0.002886031212906043\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06463482993178897\n",
      "Average test loss: 0.0029573226927055254\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06468923237257533\n",
      "Average test loss: 0.0029802664373483924\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0645056215027968\n",
      "Average test loss: 0.002903208643818895\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06473300305671162\n",
      "Average test loss: 0.0029343893310676016\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06461789684825474\n",
      "Average test loss: 0.0028878794031010732\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06456842979457643\n",
      "Average test loss: 0.0029193840716034173\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06456552583641476\n",
      "Average test loss: 0.0029535222604042954\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06439702948596743\n",
      "Average test loss: 0.002987232773254315\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0644483929740058\n",
      "Average test loss: 0.0029400601910634172\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0644748779601521\n",
      "Average test loss: 0.0029425018415268926\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06432561797565885\n",
      "Average test loss: 0.0029553384435259634\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06437201078401672\n",
      "Average test loss: 0.003080595193637742\n",
      "Epoch 257/300\n",
      "Average training loss: 0.0643935352894995\n",
      "Average test loss: 0.002979979459817211\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06439722664488687\n",
      "Average test loss: 0.0029428141475137735\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06420479016171561\n",
      "Average test loss: 0.002932022627443075\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06429437715477414\n",
      "Average test loss: 0.0028693393626146846\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06426347542471356\n",
      "Average test loss: 0.002952757212643822\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06410713541838858\n",
      "Average test loss: 0.0030330407958891656\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06418683911363283\n",
      "Average test loss: 0.002936672075341145\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06407919086350335\n",
      "Average test loss: 0.0029563739405324063\n",
      "Epoch 265/300\n",
      "Average training loss: 0.0641693196925852\n",
      "Average test loss: 0.0029755550532912216\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06409124993284543\n",
      "Average test loss: 0.0029351089648488493\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06401792645454407\n",
      "Average test loss: 0.0028858494853807822\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06411702325608995\n",
      "Average test loss: 0.0029891832216332355\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0639186976518896\n",
      "Average test loss: 0.0029419961569623816\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06403313763439655\n",
      "Average test loss: 0.0029333932133805423\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06394945874810219\n",
      "Average test loss: 0.002969574266837703\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06389958084954156\n",
      "Average test loss: 0.0029391440281437502\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06392976137664583\n",
      "Average test loss: 0.0029962244253191684\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06377043387624952\n",
      "Average test loss: 0.0029908635065787367\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06383171649773915\n",
      "Average test loss: 0.0029526135228160355\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06378026461601258\n",
      "Average test loss: 0.0029358160292936694\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06373562900225321\n",
      "Average test loss: 0.002936635348118014\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06377125034067366\n",
      "Average test loss: 0.0029112122986051773\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06376991620990966\n",
      "Average test loss: 0.002937210084663497\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06379007403055827\n",
      "Average test loss: 0.0034140119527777037\n",
      "Epoch 281/300\n",
      "Average training loss: 0.063662891222371\n",
      "Average test loss: 0.002955780656180448\n",
      "Epoch 282/300\n",
      "Average training loss: 0.0635869586666425\n",
      "Average test loss: 0.0029818275740577114\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0636884656118022\n",
      "Average test loss: 0.0029243336588972144\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06361911826994684\n",
      "Average test loss: 0.0029546894455949467\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0635295813481013\n",
      "Average test loss: 0.0029687634063884616\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06366572748290168\n",
      "Average test loss: 0.003154117384718524\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06353326247798072\n",
      "Average test loss: 0.002974874584004283\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06348267681731118\n",
      "Average test loss: 0.0029584179282602335\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06357758835951487\n",
      "Average test loss: 0.00297251687799063\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06352927398019367\n",
      "Average test loss: 0.0029260027135411897\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0635085077020857\n",
      "Average test loss: 0.003002056018759807\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06347250600655874\n",
      "Average test loss: 0.002993662618100643\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0634413302342097\n",
      "Average test loss: 0.0030338573836618\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06338193922572666\n",
      "Average test loss: 0.0038427049883951745\n",
      "Epoch 295/300\n",
      "Average training loss: 0.06327319112751219\n",
      "Average test loss: 0.002962692410374681\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06342178535461426\n",
      "Average test loss: 0.002973697380059295\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06332595430811246\n",
      "Average test loss: 0.003322428176179528\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0632166036301189\n",
      "Average test loss: 0.002938996101833052\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06328463364972009\n",
      "Average test loss: 0.0029173847662491933\n",
      "Epoch 300/300\n",
      "Average training loss: 0.06332481998205185\n",
      "Average test loss: 0.0029439961713635257\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.443151860025194\n",
      "Average test loss: 0.006121853834225072\n",
      "Epoch 2/300\n",
      "Average training loss: 0.7883742131127252\n",
      "Average test loss: 0.004722095844646295\n",
      "Epoch 3/300\n",
      "Average training loss: 0.4231114580896166\n",
      "Average test loss: 0.004208797757824262\n",
      "Epoch 4/300\n",
      "Average training loss: 0.2940569872326321\n",
      "Average test loss: 0.004086847452446818\n",
      "Epoch 5/300\n",
      "Average training loss: 0.22302443220880297\n",
      "Average test loss: 0.003754097392782569\n",
      "Epoch 6/300\n",
      "Average training loss: 0.1810149150027169\n",
      "Average test loss: 0.0036472618327372604\n",
      "Epoch 7/300\n",
      "Average training loss: 0.15661405684550603\n",
      "Average test loss: 0.00360512500628829\n",
      "Epoch 8/300\n",
      "Average training loss: 0.14109136456913418\n",
      "Average test loss: 0.0034083562588526142\n",
      "Epoch 9/300\n",
      "Average training loss: 0.12997780328326755\n",
      "Average test loss: 0.00334417230842842\n",
      "Epoch 10/300\n",
      "Average training loss: 0.12212516600555844\n",
      "Average test loss: 0.003331126469083958\n",
      "Epoch 11/300\n",
      "Average training loss: 0.11589137008455065\n",
      "Average test loss: 0.00331959979256822\n",
      "Epoch 12/300\n",
      "Average training loss: 0.11091059554285473\n",
      "Average test loss: 0.0030340085153778393\n",
      "Epoch 13/300\n",
      "Average training loss: 0.10659059089422226\n",
      "Average test loss: 0.0029412496969517736\n",
      "Epoch 14/300\n",
      "Average training loss: 0.10296842722760306\n",
      "Average test loss: 0.0028704747069213127\n",
      "Epoch 15/300\n",
      "Average training loss: 0.09951526311702198\n",
      "Average test loss: 0.0027958096305115356\n",
      "Epoch 16/300\n",
      "Average training loss: 0.09659274348285463\n",
      "Average test loss: 0.0027155970450904636\n",
      "Epoch 17/300\n",
      "Average training loss: 0.09397257553206549\n",
      "Average test loss: 0.0026804086276226574\n",
      "Epoch 18/300\n",
      "Average training loss: 0.09159626158740786\n",
      "Average test loss: 0.0026127698542550205\n",
      "Epoch 19/300\n",
      "Average training loss: 0.08934662283129162\n",
      "Average test loss: 0.002629354272451666\n",
      "Epoch 20/300\n",
      "Average training loss: 0.08737147793504926\n",
      "Average test loss: 0.002544564467544357\n",
      "Epoch 21/300\n",
      "Average training loss: 0.08545591013961368\n",
      "Average test loss: 0.0025612192950728867\n",
      "Epoch 22/300\n",
      "Average training loss: 0.08384358529249827\n",
      "Average test loss: 0.0024848378787024152\n",
      "Epoch 23/300\n",
      "Average training loss: 0.08218173685338762\n",
      "Average test loss: 0.00245660036491851\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08085906737049421\n",
      "Average test loss: 0.002471040912386444\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07966331007083258\n",
      "Average test loss: 0.0024544195654905504\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07841311034229066\n",
      "Average test loss: 0.002398427675788601\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07755221892065473\n",
      "Average test loss: 0.0023862969786342648\n",
      "Epoch 28/300\n",
      "Average training loss: 0.07653595453500747\n",
      "Average test loss: 0.0023663547181834777\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0756096930305163\n",
      "Average test loss: 0.002369894728478458\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07502346259355545\n",
      "Average test loss: 0.0023551520452731187\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07448613040977055\n",
      "Average test loss: 0.002345830939296219\n",
      "Epoch 32/300\n",
      "Average training loss: 0.07362085038423538\n",
      "Average test loss: 0.0023367545675072403\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07300891217920515\n",
      "Average test loss: 0.0023155440346648297\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07250977814859814\n",
      "Average test loss: 0.002335550609148211\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07199199367231793\n",
      "Average test loss: 0.0023311374158495003\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07164866913689508\n",
      "Average test loss: 0.002347508413096269\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0711696288353867\n",
      "Average test loss: 0.0023216031274447837\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07072435657845604\n",
      "Average test loss: 0.0022811450875467724\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07033571126725939\n",
      "Average test loss: 0.0022698408234864474\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06987782284617423\n",
      "Average test loss: 0.0022440300567282572\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06957540565398004\n",
      "Average test loss: 0.0022378071534136933\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06933813759353426\n",
      "Average test loss: 0.0022528814755173195\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06908469423320558\n",
      "Average test loss: 0.0022441841883377896\n",
      "Epoch 44/300\n",
      "Average training loss: 0.06868689332405727\n",
      "Average test loss: 0.002249114263078405\n",
      "Epoch 45/300\n",
      "Average training loss: 0.068470427552859\n",
      "Average test loss: 0.002243062594698535\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06813042813539505\n",
      "Average test loss: 0.0022537831246025033\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06792426299055418\n",
      "Average test loss: 0.002229730139279531\n",
      "Epoch 48/300\n",
      "Average training loss: 0.06786348628335528\n",
      "Average test loss: 0.00221654757598622\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06745392844743199\n",
      "Average test loss: 0.0022265827850335173\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06716537616650263\n",
      "Average test loss: 0.00221012476252185\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06698744651344088\n",
      "Average test loss: 0.0022099977862089872\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06668575980265935\n",
      "Average test loss: 0.00220806968325956\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06658426665597492\n",
      "Average test loss: 0.00221352393200828\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06630439752340317\n",
      "Average test loss: 0.0022000120647458566\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06629166038168802\n",
      "Average test loss: 0.0021907893603460658\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06595574354463153\n",
      "Average test loss: 0.0022011481591810785\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06578098781241311\n",
      "Average test loss: 0.002200909601317512\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06563127760754692\n",
      "Average test loss: 0.00220256321111487\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06543052972025341\n",
      "Average test loss: 0.0021819067702939115\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06518681736124886\n",
      "Average test loss: 0.0021845137440702983\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06508229711320665\n",
      "Average test loss: 0.0021874902672651742\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06495086880856091\n",
      "Average test loss: 0.00219206196287026\n",
      "Epoch 63/300\n",
      "Average training loss: 0.06477319465412033\n",
      "Average test loss: 0.0021883706407000623\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0645871632926994\n",
      "Average test loss: 0.0022174936518487005\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06443300151824952\n",
      "Average test loss: 0.002177998140764733\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06428456105788549\n",
      "Average test loss: 0.0021849147617403002\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0641121843457222\n",
      "Average test loss: 0.002182135156666239\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06398012476497227\n",
      "Average test loss: 0.0022322967658854193\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06406358875168694\n",
      "Average test loss: 0.0022750665659291877\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06369054981072744\n",
      "Average test loss: 0.002224705688034495\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06346801649861866\n",
      "Average test loss: 0.0021713250973779295\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06330387543969684\n",
      "Average test loss: 0.002171214830854701\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06333855842550595\n",
      "Average test loss: 0.0021769929515818754\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06300134299198787\n",
      "Average test loss: 0.0021696511877493728\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0628395648698012\n",
      "Average test loss: 0.002198101645335555\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06246126859386762\n",
      "Average test loss: 0.0021784209268581535\n",
      "Epoch 79/300\n",
      "Average training loss: 0.062346577018499376\n",
      "Average test loss: 0.002172382723333107\n",
      "Epoch 80/300\n",
      "Average training loss: 0.06220784678061803\n",
      "Average test loss: 0.0021683731554076077\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06210336931215392\n",
      "Average test loss: 0.002177697632668747\n",
      "Epoch 82/300\n",
      "Average training loss: 0.06189747640159395\n",
      "Average test loss: 0.0021656475296864906\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06184956008195877\n",
      "Average test loss: 0.002166500957061847\n",
      "Epoch 84/300\n",
      "Average training loss: 0.061702600101629895\n",
      "Average test loss: 0.0021841707482106155\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0615093765159448\n",
      "Average test loss: 0.0021771181490686203\n",
      "Epoch 86/300\n",
      "Average training loss: 0.061413373940520814\n",
      "Average test loss: 0.0021705029915190404\n",
      "Epoch 87/300\n",
      "Average training loss: 0.061369233661227755\n",
      "Average test loss: 0.002187381687056687\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06114087674021721\n",
      "Average test loss: 0.0021800858362888294\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06117690857251485\n",
      "Average test loss: 0.0021845746567058896\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06095851463410589\n",
      "Average test loss: 0.002199719298630953\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06069923635654979\n",
      "Average test loss: 0.0021662627123296262\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06074296424123976\n",
      "Average test loss: 0.002177593266384469\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06039249131745762\n",
      "Average test loss: 0.002189171195237173\n",
      "Epoch 95/300\n",
      "Average training loss: 0.060388048715061614\n",
      "Average test loss: 0.0021811182188491026\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06027905121776793\n",
      "Average test loss: 0.002185750602227118\n",
      "Epoch 97/300\n",
      "Average training loss: 0.060134534663624235\n",
      "Average test loss: 0.0021894364077597855\n",
      "Epoch 98/300\n",
      "Average training loss: 0.059921417815817724\n",
      "Average test loss: 0.0022017244005368814\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05986682596471574\n",
      "Average test loss: 0.0021846158140235478\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05980285906791687\n",
      "Average test loss: 0.00218795402886139\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05964160311222076\n",
      "Average test loss: 0.002186214397040506\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05956072200006909\n",
      "Average test loss: 0.002194819905898637\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05945732915070322\n",
      "Average test loss: 0.0027896435047603317\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05919062949551476\n",
      "Average test loss: 0.0021833942226237722\n",
      "Epoch 105/300\n",
      "Average training loss: 0.059152791996796926\n",
      "Average test loss: 0.0022041464615613223\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05908649864461687\n",
      "Average test loss: 0.002196428019967344\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05898499752415551\n",
      "Average test loss: 0.0022122456511068675\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05877547382315\n",
      "Average test loss: 0.0022151066476686133\n",
      "Epoch 109/300\n",
      "Average training loss: 0.058640441636244454\n",
      "Average test loss: 0.002211418838136726\n",
      "Epoch 111/300\n",
      "Average training loss: 0.058579767094718085\n",
      "Average test loss: 0.002236764506333404\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05850709949268235\n",
      "Average test loss: 0.0021958875291877324\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05837867573897044\n",
      "Average test loss: 0.0022142404461693434\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05822850172387229\n",
      "Average test loss: 0.0022333512802918754\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05816911483473248\n",
      "Average test loss: 0.0022247437178674672\n",
      "Epoch 116/300\n",
      "Average training loss: 0.058061623341507385\n",
      "Average test loss: 0.0022001525613789758\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05792204488648309\n",
      "Average test loss: 0.0021975630211333434\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05790274740258853\n",
      "Average test loss: 0.002219784027689861\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05783303869432873\n",
      "Average test loss: 0.0022225293370170727\n",
      "Epoch 120/300\n",
      "Average training loss: 0.057690815769963794\n",
      "Average test loss: 0.0022497477390699917\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05761140881313218\n",
      "Average test loss: 0.0022078167863397134\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05756908556984531\n",
      "Average test loss: 0.0022527868680448995\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05740543502569199\n",
      "Average test loss: 0.0022238860871228905\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05734316974878311\n",
      "Average test loss: 0.0022493736137532525\n",
      "Epoch 125/300\n",
      "Average training loss: 0.057264480163653694\n",
      "Average test loss: 0.0022140872799273994\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05717300103108088\n",
      "Average test loss: 0.0022151112587501607\n",
      "Epoch 127/300\n",
      "Average training loss: 0.057045364462667045\n",
      "Average test loss: 0.0022282355175250105\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05696916819943322\n",
      "Average test loss: 0.0022295822927521337\n",
      "Epoch 129/300\n",
      "Average training loss: 0.056935192353195614\n",
      "Average test loss: 0.0022574868268436855\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05687945889764362\n",
      "Average test loss: 0.0022591360381080043\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05678893459836642\n",
      "Average test loss: 0.0022431549886241554\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05675037521455023\n",
      "Average test loss: 0.002244630889346202\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05667329819334878\n",
      "Average test loss: 0.0022289192537880607\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05655528624852498\n",
      "Average test loss: 0.0022254199707466694\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05647678843471739\n",
      "Average test loss: 0.0022257908032172254\n",
      "Epoch 136/300\n",
      "Average training loss: 0.056573096851507826\n",
      "Average test loss: 0.0022499419297609064\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05640710117088424\n",
      "Average test loss: 0.0022528642949958644\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05628331421150101\n",
      "Average test loss: 0.002305821618479159\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05618698262506061\n",
      "Average test loss: 0.0023252746604589952\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05612391876512104\n",
      "Average test loss: 0.0022533737110594907\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05601838783091969\n",
      "Average test loss: 0.0022409473483761154\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05593553894758224\n",
      "Average test loss: 0.0022798311385429566\n",
      "Epoch 143/300\n",
      "Average training loss: 0.055924791279766295\n",
      "Average test loss: 0.0022707039515177407\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05580975246098306\n",
      "Average test loss: 0.002292855353405078\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05612487777074178\n",
      "Average test loss: 0.0022511884866075384\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05568732644451989\n",
      "Average test loss: 0.002238988358941343\n",
      "Epoch 147/300\n",
      "Average training loss: 0.055574851751327516\n",
      "Average test loss: 0.0022440473403160773\n",
      "Epoch 148/300\n",
      "Average training loss: 0.055619207792811924\n",
      "Average test loss: 0.0023217023933927218\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05549632186359829\n",
      "Average test loss: 0.002276987092776431\n",
      "Epoch 150/300\n",
      "Average training loss: 0.055436898138788014\n",
      "Average test loss: 0.0022498338357028035\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05536033885015382\n",
      "Average test loss: 0.002231504028249118\n",
      "Epoch 152/300\n",
      "Average training loss: 0.055321245729923245\n",
      "Average test loss: 0.002254644546036919\n",
      "Epoch 153/300\n",
      "Average training loss: 0.055284955094257994\n",
      "Average test loss: 0.002290644997730851\n",
      "Epoch 154/300\n",
      "Average training loss: 0.055226631419526204\n",
      "Average test loss: 0.002245718373192681\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05511858229504691\n",
      "Average test loss: 0.00228414958425694\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05516809438996845\n",
      "Average test loss: 0.002299908107560542\n",
      "Epoch 157/300\n",
      "Average training loss: 0.0550357411702474\n",
      "Average test loss: 0.0022555769700556994\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05497975965672069\n",
      "Average test loss: 0.0023383605546421476\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05488908598489232\n",
      "Average test loss: 0.0022492990263013376\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05483998682101567\n",
      "Average test loss: 0.002247416627282898\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05481184935238626\n",
      "Average test loss: 0.002605829247376985\n",
      "Epoch 162/300\n",
      "Average training loss: 0.054751000987158883\n",
      "Average test loss: 0.002295715821079082\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05477789454989963\n",
      "Average test loss: 0.002283541736089521\n",
      "Epoch 164/300\n",
      "Average training loss: 0.054723781486352284\n",
      "Average test loss: 0.0022742413005067243\n",
      "Epoch 165/300\n",
      "Average training loss: 0.054614520046446055\n",
      "Average test loss: 0.002262764856322772\n",
      "Epoch 166/300\n",
      "Average training loss: 0.054553057418929206\n",
      "Average test loss: 0.0022585099999689394\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05454889639880922\n",
      "Average test loss: 0.0022887936176525223\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05442838101916843\n",
      "Average test loss: 0.0022817770226134194\n",
      "Epoch 169/300\n",
      "Average training loss: 0.05437414103084141\n",
      "Average test loss: 0.0023071260220474668\n",
      "Epoch 170/300\n",
      "Average training loss: 0.054399880372815665\n",
      "Average test loss: 0.0022980952044535014\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05441710477405124\n",
      "Average test loss: 0.0022927991485016215\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0541761822005113\n",
      "Average test loss: 0.0022759234240899485\n",
      "Epoch 173/300\n",
      "Average training loss: 0.054221581088172065\n",
      "Average test loss: 0.002330418690832125\n",
      "Epoch 174/300\n",
      "Average training loss: 0.0541546605527401\n",
      "Average test loss: 0.0022787340166461134\n",
      "Epoch 175/300\n",
      "Average training loss: 0.054227732674943074\n",
      "Average test loss: 0.00234583250226246\n",
      "Epoch 176/300\n",
      "Average training loss: 0.054063290321164664\n",
      "Average test loss: 0.0022677677863587936\n",
      "Epoch 177/300\n",
      "Average training loss: 0.054036391466856\n",
      "Average test loss: 0.002273506035717825\n",
      "Epoch 178/300\n",
      "Average training loss: 0.053993488676018185\n",
      "Average test loss: 0.00232486597303715\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05393334916234017\n",
      "Average test loss: 0.0022804680228647256\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05385114030705558\n",
      "Average test loss: 0.0022704284032806755\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05391902783513069\n",
      "Average test loss: 0.0022976600807160137\n",
      "Epoch 182/300\n",
      "Average training loss: 0.053869360827737384\n",
      "Average test loss: 0.002323122548870742\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05376133412785\n",
      "Average test loss: 0.0023036101886795626\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05375443537698852\n",
      "Average test loss: 0.0022982758945888945\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05363102703624301\n",
      "Average test loss: 0.002329290882166889\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05364936972988976\n",
      "Average test loss: 0.0022919809505757356\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05363757832845052\n",
      "Average test loss: 0.0022892337784998947\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05358928722805447\n",
      "Average test loss: 0.002324018239043653\n",
      "Epoch 189/300\n",
      "Average training loss: 0.053503875351614424\n",
      "Average test loss: 0.0022967333835032253\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05352877070837551\n",
      "Average test loss: 0.0022578848745259976\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05347410478856828\n",
      "Average test loss: 0.002279222146090534\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05344240086608463\n",
      "Average test loss: 0.002339062199824386\n",
      "Epoch 193/300\n",
      "Average training loss: 0.05346364390850067\n",
      "Average test loss: 0.0023017992878125774\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05346081049243609\n",
      "Average test loss: 0.002313910664783584\n",
      "Epoch 195/300\n",
      "Average training loss: 0.053354314078887306\n",
      "Average test loss: 0.0023305829322586456\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05326922511723307\n",
      "Average test loss: 0.002306026648937\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05325409954455164\n",
      "Average test loss: 0.002377540037449863\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0531902315583494\n",
      "Average test loss: 0.002326095063239336\n",
      "Epoch 199/300\n",
      "Average training loss: 0.05315056282944149\n",
      "Average test loss: 0.0023337508458644153\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05317524466580815\n",
      "Average test loss: 0.00230632325882713\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05307075106766489\n",
      "Average test loss: 0.0023235206334955164\n",
      "Epoch 202/300\n",
      "Average training loss: 0.053048441506094404\n",
      "Average test loss: 0.002322345813115438\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05306449952390459\n",
      "Average test loss: 0.00233118799680637\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05306870764659511\n",
      "Average test loss: 0.002314644343530138\n",
      "Epoch 205/300\n",
      "Average training loss: 0.052897185458077325\n",
      "Average test loss: 0.0022960977003806168\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05292920144730144\n",
      "Average test loss: 0.0023359172446653247\n",
      "Epoch 207/300\n",
      "Average training loss: 0.053006247368123795\n",
      "Average test loss: 0.0022913500590042934\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05296281902988752\n",
      "Average test loss: 0.0023065811563283203\n",
      "Epoch 209/300\n",
      "Average training loss: 0.052792337172561224\n",
      "Average test loss: 0.0023239296856853698\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05278557739655177\n",
      "Average test loss: 0.00241447776204182\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052817723711331685\n",
      "Average test loss: 0.002322555311852031\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05270748651358816\n",
      "Average test loss: 0.0023345531723979445\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05280850799216164\n",
      "Average test loss: 0.0023095857861141363\n",
      "Epoch 214/300\n",
      "Average training loss: 0.052763728976249695\n",
      "Average test loss: 0.0023085284955385657\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05266574376159244\n",
      "Average test loss: 0.0023336741522782378\n",
      "Epoch 216/300\n",
      "Average training loss: 0.052626833071311316\n",
      "Average test loss: 0.002383581547066569\n",
      "Epoch 217/300\n",
      "Average training loss: 0.05259945823086633\n",
      "Average test loss: 0.0023135357077957853\n",
      "Epoch 218/300\n",
      "Average training loss: 0.052509723911682765\n",
      "Average test loss: 0.002404332285746932\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0525402341220114\n",
      "Average test loss: 0.0023024319048143096\n",
      "Epoch 220/300\n",
      "Average training loss: 0.052489364677005344\n",
      "Average test loss: 0.014562553842862447\n",
      "Epoch 221/300\n",
      "Average training loss: 0.052349070138401455\n",
      "Average test loss: 0.0023259531646553015\n",
      "Epoch 224/300\n",
      "Average training loss: 0.05237214223047097\n",
      "Average test loss: 0.0023331584106716843\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05233159702354007\n",
      "Average test loss: 0.002338651853096154\n",
      "Epoch 226/300\n",
      "Average training loss: 0.05232205195228259\n",
      "Average test loss: 0.0023671898995008735\n",
      "Epoch 227/300\n",
      "Average training loss: 0.052318423397011225\n",
      "Average test loss: 0.0025738498212562666\n",
      "Epoch 228/300\n",
      "Average training loss: 0.05233578210075696\n",
      "Average test loss: 0.002301888078244196\n",
      "Epoch 229/300\n",
      "Average training loss: 0.052256851120127575\n",
      "Average test loss: 0.002301563584763143\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05216068419482973\n",
      "Average test loss: 0.002318499880739384\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05211739851699935\n",
      "Average test loss: 0.0022904127484394445\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05216351329949167\n",
      "Average test loss: 0.0023249355673583016\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05216750016146236\n",
      "Average test loss: 0.0023259558350675635\n",
      "Epoch 234/300\n",
      "Average training loss: 0.052070897943443724\n",
      "Average test loss: 0.0023743483968493013\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05203858367270894\n",
      "Average test loss: 0.002327881153052052\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05207321046789487\n",
      "Average test loss: 0.002327715271876918\n",
      "Epoch 237/300\n",
      "Average training loss: 0.052048033873240154\n",
      "Average test loss: 0.0027669594892197184\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05205213882194625\n",
      "Average test loss: 0.0023239910101724996\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05197504604193899\n",
      "Average test loss: 0.002319014099943969\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0519620875981119\n",
      "Average test loss: 0.002360185709885425\n",
      "Epoch 243/300\n",
      "Average training loss: 0.051790968153211804\n",
      "Average test loss: 0.0023503806582755513\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0518005745642715\n",
      "Average test loss: 0.0023481603796697325\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05180961852272352\n",
      "Average test loss: 0.0023663940214448506\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05180017389853795\n",
      "Average test loss: 0.0023244428882996243\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05179250248935487\n",
      "Average test loss: 0.0024098786376416685\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05177385661999385\n",
      "Average test loss: 0.0024271560196454326\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05174137991335657\n",
      "Average test loss: 0.002401728404375414\n",
      "Epoch 250/300\n",
      "Average training loss: 0.05170823585324817\n",
      "Average test loss: 0.0023047100303487645\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05168612263931169\n",
      "Average test loss: 0.002400978928224908\n",
      "Epoch 252/300\n",
      "Average training loss: 0.051747630847824945\n",
      "Average test loss: 0.002409399079158902\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05158555479513274\n",
      "Average test loss: 0.0023813477526936264\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05152501007252269\n",
      "Average test loss: 0.002349074665663971\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05157205501529905\n",
      "Average test loss: 0.002338180186226964\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05155387756559584\n",
      "Average test loss: 0.002360156944849425\n",
      "Epoch 259/300\n",
      "Average training loss: 0.051421916143761744\n",
      "Average test loss: 0.0023538625268265606\n",
      "Epoch 260/300\n",
      "Average training loss: 0.051546198503838646\n",
      "Average test loss: 0.0023053328288305136\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05149980135758718\n",
      "Average test loss: 0.0023388991001993416\n",
      "Epoch 262/300\n",
      "Average training loss: 0.051450097027752134\n",
      "Average test loss: 0.002357888313734697\n",
      "Epoch 263/300\n",
      "Average training loss: 0.051412400358253056\n",
      "Average test loss: 0.002365255094029837\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05136878988146782\n",
      "Average test loss: 0.002362227650359273\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05140095909767681\n",
      "Average test loss: 0.0023689074901243052\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05135291551550229\n",
      "Average test loss: 0.002362821582911743\n",
      "Epoch 267/300\n",
      "Average training loss: 0.051347618218925264\n",
      "Average test loss: 0.0023920066168324817\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05126645214358966\n",
      "Average test loss: 0.002350033742893073\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05122251484460301\n",
      "Average test loss: 0.0023241946914543707\n",
      "Epoch 270/300\n",
      "Average training loss: 0.051304505040248236\n",
      "Average test loss: 0.002318046444820033\n",
      "Epoch 271/300\n",
      "Average training loss: 0.051241672823826474\n",
      "Average test loss: 0.0023785724325312506\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05114345952868462\n",
      "Average test loss: 0.0023382970243692397\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05117790719866753\n",
      "Average test loss: 0.0023161366492923763\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05122846434182591\n",
      "Average test loss: 0.002344534957781434\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05116487937503391\n",
      "Average test loss: 0.0023617457571542926\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05101817519466082\n",
      "Average test loss: 0.0023497133867608176\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05112182198299302\n",
      "Average test loss: 0.002383074276459714\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05108854971660508\n",
      "Average test loss: 0.0023459775855557787\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05104839298460219\n",
      "Average test loss: 0.002325785436770982\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05108455721537272\n",
      "Average test loss: 0.0023285340230084126\n",
      "Epoch 283/300\n",
      "Average training loss: 0.051060042467382216\n",
      "Average test loss: 0.0023488909616652463\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05097792554232809\n",
      "Average test loss: 0.002393264419916603\n",
      "Epoch 285/300\n",
      "Average training loss: 0.051063591917355856\n",
      "Average test loss: 0.0023850171843336687\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05094190365076065\n",
      "Average test loss: 0.0024122767578810453\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05094253511561288\n",
      "Average test loss: 0.0023444412700417967\n",
      "Epoch 288/300\n",
      "Average training loss: 0.0510069199833605\n",
      "Average test loss: 0.002613577950745821\n",
      "Epoch 289/300\n",
      "Average training loss: 0.050874468720621534\n",
      "Average test loss: 0.0023964919487221375\n",
      "Epoch 290/300\n",
      "Average training loss: 0.050870081027348836\n",
      "Average test loss: 0.0023560510134945315\n",
      "Epoch 291/300\n",
      "Average training loss: 0.050892735269334584\n",
      "Average test loss: 0.0024446169866455926\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05086814406514168\n",
      "Average test loss: 0.0023804244918541775\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05074256501264042\n",
      "Average test loss: 0.0023707456288652286\n",
      "Epoch 295/300\n",
      "Average training loss: 0.050766229997078575\n",
      "Average test loss: 0.002359624081601699\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05079651658733686\n",
      "Average test loss: 0.0023849954524387917\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05075810542371538\n",
      "Average test loss: 0.002372153949406412\n",
      "Epoch 298/300\n",
      "Average training loss: 0.050771473785241446\n",
      "Average test loss: 0.002347167579457164\n",
      "Epoch 299/300\n",
      "Average training loss: 0.050701917383405896\n",
      "Average test loss: 0.0023617596911887327\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05064715354641278\n",
      "Average test loss: 0.002374822527791063\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive_Depth3/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 26.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.70\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.02\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.06\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.07\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.11\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.16\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.12\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 27.28\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 27.20\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.29\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 27.27\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 27.33\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 26.64\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.31\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.44\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.76\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.72\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.77\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.82\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.90\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 27.97\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.01\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.07\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.26\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.50\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.93\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.35\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.43\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.48\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.52\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.54\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.66\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.74\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.76\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.83\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.88\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.47\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.64\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.81\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.99\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.37\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.39\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.51\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.48\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.57\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
