{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_10.pkl', 'rb') as f:\n",
    "    DCT_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_20.pkl', 'rb') as f:\n",
    "    DCT_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_30.pkl', 'rb') as f:\n",
    "    DCT_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/DCT_40.pkl', 'rb') as f:\n",
    "    DCT_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCT_10_normalized = normalize_max_row_norm(DCT_10)\n",
    "DCT_20_normalized = normalize_max_row_norm(DCT_20)\n",
    "DCT_30_normalized = normalize_max_row_norm(DCT_30)\n",
    "DCT_40_normalized = normalize_max_row_norm(DCT_40)\n",
    "\n",
    "DCT_10_observations = generate_observations(img_dataset, DCT_10_normalized, 0.01)\n",
    "DCT_20_observations = generate_observations(img_dataset, DCT_20_normalized, 0.01)\n",
    "DCT_30_observations = generate_observations(img_dataset, DCT_30_normalized, 0.01)\n",
    "DCT_40_observations = generate_observations(img_dataset, DCT_40_normalized, 0.01)\n",
    "\n",
    "DCT_10_train, DCT_10_test = split_dataset(DCT_10_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_20_train, DCT_20_test = split_dataset(DCT_20_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_30_train, DCT_30_test = split_dataset(DCT_30_observations, train_ratio = 0.9, seed = 0)\n",
    "DCT_40_train, DCT_40_test = split_dataset(DCT_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 5)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05902849623726474\n",
      "Average test loss: 0.004884370194748044\n",
      "Epoch 2/300\n",
      "Average training loss: 0.02366755380398697\n",
      "Average test loss: 0.004539730022557907\n",
      "Epoch 3/300\n",
      "Average training loss: 0.022466762815912564\n",
      "Average test loss: 0.004439681791183021\n",
      "Epoch 4/300\n",
      "Average training loss: 0.02202801962527964\n",
      "Average test loss: 0.004357402675474684\n",
      "Epoch 5/300\n",
      "Average training loss: 0.021763175699445938\n",
      "Average test loss: 0.004324013712091578\n",
      "Epoch 6/300\n",
      "Average training loss: 0.021563353659378157\n",
      "Average test loss: 0.004307577130281263\n",
      "Epoch 7/300\n",
      "Average training loss: 0.021395914943681822\n",
      "Average test loss: 0.004253671514905162\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02124631341629558\n",
      "Average test loss: 0.004239228192923798\n",
      "Epoch 9/300\n",
      "Average training loss: 0.021135766895280943\n",
      "Average test loss: 0.004258535899635818\n",
      "Epoch 10/300\n",
      "Average training loss: 0.021032551382978756\n",
      "Average test loss: 0.004189273080064191\n",
      "Epoch 11/300\n",
      "Average training loss: 0.020924322166376644\n",
      "Average test loss: 0.004169755965057346\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02084648105005423\n",
      "Average test loss: 0.004168003827540411\n",
      "Epoch 13/300\n",
      "Average training loss: 0.020760207487477197\n",
      "Average test loss: 0.004138507623432411\n",
      "Epoch 14/300\n",
      "Average training loss: 0.020672590860890017\n",
      "Average test loss: 0.004141746276161737\n",
      "Epoch 15/300\n",
      "Average training loss: 0.020613827867640388\n",
      "Average test loss: 0.004116946710066663\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02053342730138037\n",
      "Average test loss: 0.004110580637223191\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02049084966381391\n",
      "Average test loss: 0.004101042113784287\n",
      "Epoch 18/300\n",
      "Average training loss: 0.02042930620080895\n",
      "Average test loss: 0.004108782325767809\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02037047649257713\n",
      "Average test loss: 0.004085953766066167\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02032770879069964\n",
      "Average test loss: 0.004076353894546628\n",
      "Epoch 21/300\n",
      "Average training loss: 0.020275516976912815\n",
      "Average test loss: 0.004078186384091775\n",
      "Epoch 22/300\n",
      "Average training loss: 0.020234376928872533\n",
      "Average test loss: 0.0040480605906082524\n",
      "Epoch 23/300\n",
      "Average training loss: 0.02018576465547085\n",
      "Average test loss: 0.004039453325172265\n",
      "Epoch 24/300\n",
      "Average training loss: 0.020144466883606382\n",
      "Average test loss: 0.004042141298866935\n",
      "Epoch 25/300\n",
      "Average training loss: 0.02011609127289719\n",
      "Average test loss: 0.004034625797222058\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02007000761727492\n",
      "Average test loss: 0.004027840187980069\n",
      "Epoch 27/300\n",
      "Average training loss: 0.020037728374203045\n",
      "Average test loss: 0.004033541403503881\n",
      "Epoch 28/300\n",
      "Average training loss: 0.019999833802382152\n",
      "Average test loss: 0.004011095740314987\n",
      "Epoch 29/300\n",
      "Average training loss: 0.01996099492410819\n",
      "Average test loss: 0.004019157521426678\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019936791689859495\n",
      "Average test loss: 0.004051297158209814\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019913893098632493\n",
      "Average test loss: 0.004004322190665537\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01987652090191841\n",
      "Average test loss: 0.003998275971454051\n",
      "Epoch 33/300\n",
      "Average training loss: 0.019856580747498406\n",
      "Average test loss: 0.003989926820413934\n",
      "Epoch 34/300\n",
      "Average training loss: 0.019830972394181624\n",
      "Average test loss: 0.004005006606380145\n",
      "Epoch 35/300\n",
      "Average training loss: 0.019806057847208448\n",
      "Average test loss: 0.003992394206424554\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0197873066994879\n",
      "Average test loss: 0.004005469827188386\n",
      "Epoch 37/300\n",
      "Average training loss: 0.019755301920076213\n",
      "Average test loss: 0.003986112876484792\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01973098794205321\n",
      "Average test loss: 0.0039772575236856934\n",
      "Epoch 39/300\n",
      "Average training loss: 0.019714314899510806\n",
      "Average test loss: 0.003996038275046481\n",
      "Epoch 40/300\n",
      "Average training loss: 0.019690403199858136\n",
      "Average test loss: 0.004008613120557533\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01966984423995018\n",
      "Average test loss: 0.0039737897983027825\n",
      "Epoch 42/300\n",
      "Average training loss: 0.019647084000209966\n",
      "Average test loss: 0.003969757944759396\n",
      "Epoch 43/300\n",
      "Average training loss: 0.01963621440033118\n",
      "Average test loss: 0.003969936140709453\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01961073746614986\n",
      "Average test loss: 0.0039784269721971615\n",
      "Epoch 45/300\n",
      "Average training loss: 0.01959503176808357\n",
      "Average test loss: 0.003962159233374728\n",
      "Epoch 46/300\n",
      "Average training loss: 0.019581351209017966\n",
      "Average test loss: 0.003976450610905886\n",
      "Epoch 47/300\n",
      "Average training loss: 0.01956120409568151\n",
      "Average test loss: 0.003970715869838993\n",
      "Epoch 48/300\n",
      "Average training loss: 0.019541306059393616\n",
      "Average test loss: 0.003999375295307901\n",
      "Epoch 49/300\n",
      "Average training loss: 0.01952411700122886\n",
      "Average test loss: 0.003968049330429898\n",
      "Epoch 50/300\n",
      "Average training loss: 0.01950401064256827\n",
      "Average test loss: 0.003978820940893558\n",
      "Epoch 51/300\n",
      "Average training loss: 0.019485646875368225\n",
      "Average test loss: 0.003978933113109735\n",
      "Epoch 52/300\n",
      "Average training loss: 0.019478924989700316\n",
      "Average test loss: 0.003990744624079929\n",
      "Epoch 53/300\n",
      "Average training loss: 0.019455687304337818\n",
      "Average test loss: 0.00398617458778123\n",
      "Epoch 54/300\n",
      "Average training loss: 0.019435609553423192\n",
      "Average test loss: 0.003981081202626228\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01942517520652877\n",
      "Average test loss: 0.003970400504353973\n",
      "Epoch 56/300\n",
      "Average training loss: 0.019413954514596196\n",
      "Average test loss: 0.003956104540162616\n",
      "Epoch 57/300\n",
      "Average training loss: 0.019392496867312325\n",
      "Average test loss: 0.003962185591459274\n",
      "Epoch 58/300\n",
      "Average training loss: 0.019371588584449557\n",
      "Average test loss: 0.003978766923356388\n",
      "Epoch 59/300\n",
      "Average training loss: 0.01936295669939783\n",
      "Average test loss: 0.003947326895677381\n",
      "Epoch 60/300\n",
      "Average training loss: 0.019337939835257\n",
      "Average test loss: 0.0039797148377531105\n",
      "Epoch 61/300\n",
      "Average training loss: 0.019329754134847058\n",
      "Average test loss: 0.00396996480309301\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01931105102101962\n",
      "Average test loss: 0.003981842300544183\n",
      "Epoch 63/300\n",
      "Average training loss: 0.019300440521703825\n",
      "Average test loss: 0.0039821580749832925\n",
      "Epoch 64/300\n",
      "Average training loss: 0.01928435981604788\n",
      "Average test loss: 0.003956791202848156\n",
      "Epoch 65/300\n",
      "Average training loss: 0.019271835962931316\n",
      "Average test loss: 0.00420138136876954\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01926385300192568\n",
      "Average test loss: 0.003996096825434102\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01924026396373908\n",
      "Average test loss: 0.003971784024602837\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01921685155067179\n",
      "Average test loss: 0.003955144022901853\n",
      "Epoch 69/300\n",
      "Average training loss: 0.019213049752844706\n",
      "Average test loss: 0.003961421647626493\n",
      "Epoch 70/300\n",
      "Average training loss: 0.019202755966948138\n",
      "Average test loss: 0.003997034296393394\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01917672204805745\n",
      "Average test loss: 0.003970659837954574\n",
      "Epoch 72/300\n",
      "Average training loss: 0.019169507713781463\n",
      "Average test loss: 0.003981262049327294\n",
      "Epoch 73/300\n",
      "Average training loss: 0.019163617779811223\n",
      "Average test loss: 0.003977123864408997\n",
      "Epoch 74/300\n",
      "Average training loss: 0.019142246655291982\n",
      "Average test loss: 0.003972587800274293\n",
      "Epoch 75/300\n",
      "Average training loss: 0.019114594618479412\n",
      "Average test loss: 0.003965495442143745\n",
      "Epoch 76/300\n",
      "Average training loss: 0.019102268459068403\n",
      "Average test loss: 0.003959886273369193\n",
      "Epoch 77/300\n",
      "Average training loss: 0.019105214562680987\n",
      "Average test loss: 0.00396230069299539\n",
      "Epoch 78/300\n",
      "Average training loss: 0.019086984674135842\n",
      "Average test loss: 0.003954605692790614\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019060416163669693\n",
      "Average test loss: 0.004004030643237962\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019042084339592192\n",
      "Average test loss: 0.004001763847553068\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019032033907042608\n",
      "Average test loss: 0.004006619566016727\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019020567634867296\n",
      "Average test loss: 0.004079611794816123\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01901835071378284\n",
      "Average test loss: 0.003962329813382692\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01898537086115943\n",
      "Average test loss: 0.003980832761360539\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01897792745878299\n",
      "Average test loss: 0.003972301012112035\n",
      "Epoch 86/300\n",
      "Average training loss: 0.018953870173957613\n",
      "Average test loss: 0.003998940955433581\n",
      "Epoch 87/300\n",
      "Average training loss: 0.018955326721072197\n",
      "Average test loss: 0.003980684574279521\n",
      "Epoch 88/300\n",
      "Average training loss: 0.018932112503382895\n",
      "Average test loss: 0.003983936517188947\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01892038549317254\n",
      "Average test loss: 0.004001047543353504\n",
      "Epoch 90/300\n",
      "Average training loss: 0.018905635234382417\n",
      "Average test loss: 0.004005693275481463\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01889625543438726\n",
      "Average test loss: 0.004037338884340392\n",
      "Epoch 92/300\n",
      "Average training loss: 0.018883530510796442\n",
      "Average test loss: 0.003988022587779495\n",
      "Epoch 93/300\n",
      "Average training loss: 0.018852195596529377\n",
      "Average test loss: 0.00402556347888377\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01885600155757533\n",
      "Average test loss: 0.003993047070586019\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01883012098322312\n",
      "Average test loss: 0.004001348593375749\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01882141209807661\n",
      "Average test loss: 0.004027846999052498\n",
      "Epoch 97/300\n",
      "Average training loss: 0.018804370466205807\n",
      "Average test loss: 0.003978133969008923\n",
      "Epoch 98/300\n",
      "Average training loss: 0.018783420691059694\n",
      "Average test loss: 0.003995113210959567\n",
      "Epoch 99/300\n",
      "Average training loss: 0.018769784086280397\n",
      "Average test loss: 0.004077188158200847\n",
      "Epoch 100/300\n",
      "Average training loss: 0.018766581451727285\n",
      "Average test loss: 0.004024282194260094\n",
      "Epoch 101/300\n",
      "Average training loss: 0.018750982184376982\n",
      "Average test loss: 0.004004464189625448\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01874331804447704\n",
      "Average test loss: 0.003999287548164527\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01872474090423849\n",
      "Average test loss: 0.0040029338169842955\n",
      "Epoch 104/300\n",
      "Average training loss: 0.018718368937571842\n",
      "Average test loss: 0.004052611703673999\n",
      "Epoch 105/300\n",
      "Average training loss: 0.018695942733022903\n",
      "Average test loss: 0.003975527873676684\n",
      "Epoch 106/300\n",
      "Average training loss: 0.018692134829858938\n",
      "Average test loss: 0.004155661864206195\n",
      "Epoch 107/300\n",
      "Average training loss: 0.018662369300921758\n",
      "Average test loss: 0.00409086780084504\n",
      "Epoch 108/300\n",
      "Average training loss: 0.018649666743146047\n",
      "Average test loss: 0.004059186455897159\n",
      "Epoch 109/300\n",
      "Average training loss: 0.018644199851486416\n",
      "Average test loss: 0.004138727405418952\n",
      "Epoch 110/300\n",
      "Average training loss: 0.018633730507559246\n",
      "Average test loss: 0.0040294531761772105\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01862315501438247\n",
      "Average test loss: 0.004028882119183739\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018611317078272502\n",
      "Average test loss: 0.004034902113179366\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0185857735623916\n",
      "Average test loss: 0.004049604252187742\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018579521425068378\n",
      "Average test loss: 0.004115118125246631\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01856297641992569\n",
      "Average test loss: 0.004025803222631415\n",
      "Epoch 116/300\n",
      "Average training loss: 0.0185636771288183\n",
      "Average test loss: 0.0040499742084907165\n",
      "Epoch 117/300\n",
      "Average training loss: 0.018540177719460593\n",
      "Average test loss: 0.004089582410330574\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018527340076035924\n",
      "Average test loss: 0.0040789815636558665\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018509869042370054\n",
      "Average test loss: 0.004090442835042874\n",
      "Epoch 120/300\n",
      "Average training loss: 0.018497865713304945\n",
      "Average test loss: 0.004078798847066032\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01848451104346249\n",
      "Average test loss: 0.0040578991218159595\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01847388076616658\n",
      "Average test loss: 0.0040083346913258235\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018462417720092668\n",
      "Average test loss: 0.00402634814961089\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01844563883377446\n",
      "Average test loss: 0.004030492637720373\n",
      "Epoch 125/300\n",
      "Average training loss: 0.018448238434063062\n",
      "Average test loss: 0.004200403711034192\n",
      "Epoch 126/300\n",
      "Average training loss: 0.018432200203339258\n",
      "Average test loss: 0.004051588158226676\n",
      "Epoch 127/300\n",
      "Average training loss: 0.018433621774117153\n",
      "Average test loss: 0.004065213553814425\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01838804577787717\n",
      "Average test loss: 0.004101235658344296\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018396772146224975\n",
      "Average test loss: 0.004074417530248562\n",
      "Epoch 130/300\n",
      "Average training loss: 0.018391609635618\n",
      "Average test loss: 0.004018148451629612\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018388929057452413\n",
      "Average test loss: 0.004092731804069546\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018357996983660592\n",
      "Average test loss: 0.004051700295466516\n",
      "Epoch 133/300\n",
      "Average training loss: 0.018341990091734463\n",
      "Average test loss: 0.00404311619947354\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01833478924218151\n",
      "Average test loss: 0.004045564754141702\n",
      "Epoch 135/300\n",
      "Average training loss: 0.018335596112741364\n",
      "Average test loss: 0.00406834546642171\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018320126962330607\n",
      "Average test loss: 0.004116298196837306\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01829464870525731\n",
      "Average test loss: 0.00421909390276091\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01828842149178187\n",
      "Average test loss: 0.004049977198036181\n",
      "Epoch 139/300\n",
      "Average training loss: 0.018284805261426503\n",
      "Average test loss: 0.004119363925937149\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01827175502644645\n",
      "Average test loss: 0.004181414248214828\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018261196093426812\n",
      "Average test loss: 0.004133284841146734\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01825044085747666\n",
      "Average test loss: 0.0041332500309993825\n",
      "Epoch 143/300\n",
      "Average training loss: 0.018235125213861465\n",
      "Average test loss: 0.0040872724184559455\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018232234939932822\n",
      "Average test loss: 0.004138276265313228\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018209035240113736\n",
      "Average test loss: 0.004132007904764679\n",
      "Epoch 146/300\n",
      "Average training loss: 0.018208347007632256\n",
      "Average test loss: 0.00409159283609026\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01819216514296002\n",
      "Average test loss: 0.0040497058052569625\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01820097276651197\n",
      "Average test loss: 0.004199106903953685\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018176239085694153\n",
      "Average test loss: 0.0040391883210589485\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018166902736657196\n",
      "Average test loss: 0.0041073769128157035\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018152804538607598\n",
      "Average test loss: 0.004245416987273428\n",
      "Epoch 152/300\n",
      "Average training loss: 0.018147457568181886\n",
      "Average test loss: 0.004143321748202046\n",
      "Epoch 153/300\n",
      "Average training loss: 0.018148437571194435\n",
      "Average test loss: 0.004119178748379151\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018125691324472427\n",
      "Average test loss: 0.004109176702176531\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01810642116268476\n",
      "Average test loss: 0.004116314166949855\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018106701085136997\n",
      "Average test loss: 0.00409299355900536\n",
      "Epoch 157/300\n",
      "Average training loss: 0.018101007314191925\n",
      "Average test loss: 0.004097098658689194\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018094919357862737\n",
      "Average test loss: 0.004134120402236779\n",
      "Epoch 159/300\n",
      "Average training loss: 0.018081999018788337\n",
      "Average test loss: 0.004100522361488806\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018071539238923125\n",
      "Average test loss: 0.004110606585318844\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018064025892151725\n",
      "Average test loss: 0.004132079056981537\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01806419760154353\n",
      "Average test loss: 0.004049202991028626\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018043960201243558\n",
      "Average test loss: 0.004111799597326252\n",
      "Epoch 164/300\n",
      "Average training loss: 0.0180488857568966\n",
      "Average test loss: 0.004196600241172645\n",
      "Epoch 165/300\n",
      "Average training loss: 0.01802239309748014\n",
      "Average test loss: 0.004063545035612252\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01801643752803405\n",
      "Average test loss: 0.0041718654077914025\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01800493857016166\n",
      "Average test loss: 0.004208083662721846\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01802146650933557\n",
      "Average test loss: 0.0042736472781333655\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01799486023849911\n",
      "Average test loss: 0.004152041502710846\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01798523019336992\n",
      "Average test loss: 0.004139696203793088\n",
      "Epoch 171/300\n",
      "Average training loss: 0.017976855453517702\n",
      "Average test loss: 0.004114044640627172\n",
      "Epoch 172/300\n",
      "Average training loss: 0.017979546901252533\n",
      "Average test loss: 0.004115380058065057\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017957084442178407\n",
      "Average test loss: 0.0041852037993570165\n",
      "Epoch 174/300\n",
      "Average training loss: 0.017955106258392334\n",
      "Average test loss: 0.004344527146261599\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017946614253852102\n",
      "Average test loss: 0.004115399652471145\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017927942963110077\n",
      "Average test loss: 0.004361557556937138\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01792018275376823\n",
      "Average test loss: 0.004219063231100639\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017919972317086325\n",
      "Average test loss: 0.004168677744662596\n",
      "Epoch 179/300\n",
      "Average training loss: 0.017901888276139893\n",
      "Average test loss: 0.004063161325123575\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017901098413599863\n",
      "Average test loss: 0.0041169436598817504\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01791229381908973\n",
      "Average test loss: 0.004248308077247606\n",
      "Epoch 182/300\n",
      "Average training loss: 0.017887098751134343\n",
      "Average test loss: 0.004168554081788493\n",
      "Epoch 183/300\n",
      "Average training loss: 0.017877674346996678\n",
      "Average test loss: 0.004122250617585249\n",
      "Epoch 184/300\n",
      "Average training loss: 0.017885325281156435\n",
      "Average test loss: 0.004207336482074526\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01787620606852902\n",
      "Average test loss: 0.00411687530659967\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017861158650782374\n",
      "Average test loss: 0.00419740903377533\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01784617296606302\n",
      "Average test loss: 0.004193371809605095\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01784040712979105\n",
      "Average test loss: 0.004127866516510646\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017843439810805852\n",
      "Average test loss: 0.0042209864900343945\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01783105046964354\n",
      "Average test loss: 0.0042037829171038335\n",
      "Epoch 191/300\n",
      "Average training loss: 0.017810536715719433\n",
      "Average test loss: 0.004125291765564018\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01782031755811638\n",
      "Average test loss: 0.004258915714919567\n",
      "Epoch 193/300\n",
      "Average training loss: 0.01779932971133126\n",
      "Average test loss: 0.004159042159302367\n",
      "Epoch 194/300\n",
      "Average training loss: 0.017802808210253717\n",
      "Average test loss: 0.004197439145710733\n",
      "Epoch 195/300\n",
      "Average training loss: 0.017793660369184283\n",
      "Average test loss: 0.004209283595490787\n",
      "Epoch 196/300\n",
      "Average training loss: 0.017785233115156493\n",
      "Average test loss: 0.004162380740667383\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017774867359134885\n",
      "Average test loss: 0.004092185494593448\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01777721928887897\n",
      "Average test loss: 0.004215660062515073\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01777069095439381\n",
      "Average test loss: 0.004201297625485394\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017772735707461833\n",
      "Average test loss: 0.004159521037091812\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017756418953339258\n",
      "Average test loss: 0.004159280959930685\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017745212656756243\n",
      "Average test loss: 0.004207889210018846\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017757125692235098\n",
      "Average test loss: 0.004228200031651391\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01773292431400882\n",
      "Average test loss: 0.004170644619812568\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017719725450707807\n",
      "Average test loss: 0.004217136006802321\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017723296807044084\n",
      "Average test loss: 0.0041529833198421535\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017708422023389073\n",
      "Average test loss: 0.004163319395648108\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017700994316902426\n",
      "Average test loss: 0.004205738524802857\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017696031431357066\n",
      "Average test loss: 0.004151994965763556\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01770087907380528\n",
      "Average test loss: 0.004224477270825042\n",
      "Epoch 211/300\n",
      "Average training loss: 0.017688697167568736\n",
      "Average test loss: 0.0043117632493376735\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0176709225061867\n",
      "Average test loss: 0.0042908747937116356\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0176738914495541\n",
      "Average test loss: 0.004310253508388996\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01766780736380153\n",
      "Average test loss: 0.00413507386110723\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017656421084370877\n",
      "Average test loss: 0.004226688199159172\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017662049683431786\n",
      "Average test loss: 0.004334843061657416\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0176483424504598\n",
      "Average test loss: 0.00423202289102806\n",
      "Epoch 218/300\n",
      "Average training loss: 0.017647485999597444\n",
      "Average test loss: 0.004180359590384695\n",
      "Epoch 219/300\n",
      "Average training loss: 0.01763981362680594\n",
      "Average test loss: 0.00413776421174407\n",
      "Epoch 220/300\n",
      "Average training loss: 0.017638330722848573\n",
      "Average test loss: 0.0042803827666987974\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017628046956327227\n",
      "Average test loss: 0.004133183361548516\n",
      "Epoch 222/300\n",
      "Average training loss: 0.017620804402563306\n",
      "Average test loss: 0.004275062993168831\n",
      "Epoch 223/300\n",
      "Average training loss: 0.017618778898484178\n",
      "Average test loss: 0.004245110487358437\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01762271951138973\n",
      "Average test loss: 0.004220567104303174\n",
      "Epoch 225/300\n",
      "Average training loss: 0.017623497820562786\n",
      "Average test loss: 0.00420736549836066\n",
      "Epoch 226/300\n",
      "Average training loss: 0.01759461754726039\n",
      "Average test loss: 0.004191723085112042\n",
      "Epoch 227/300\n",
      "Average training loss: 0.017589452770021226\n",
      "Average test loss: 0.004237585053675705\n",
      "Epoch 228/300\n",
      "Average training loss: 0.017590169252620802\n",
      "Average test loss: 0.0043242046824759905\n",
      "Epoch 229/300\n",
      "Average training loss: 0.017578429286678632\n",
      "Average test loss: 0.004129084918234083\n",
      "Epoch 230/300\n",
      "Average training loss: 0.017576612394717004\n",
      "Average test loss: 0.004241093755596214\n",
      "Epoch 231/300\n",
      "Average training loss: 0.017586444423430495\n",
      "Average test loss: 0.004248650744971302\n",
      "Epoch 232/300\n",
      "Average training loss: 0.01757480506102244\n",
      "Average test loss: 0.0041907408427861005\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017565905809402465\n",
      "Average test loss: 0.004241332992911339\n",
      "Epoch 234/300\n",
      "Average training loss: 0.017560342934396532\n",
      "Average test loss: 0.004248222607084446\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01755507321490182\n",
      "Average test loss: 0.004160628179709117\n",
      "Epoch 236/300\n",
      "Average training loss: 0.017545286728276148\n",
      "Average test loss: 0.004114682152039475\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017547167552842035\n",
      "Average test loss: 0.004216512759940492\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017538421413964694\n",
      "Average test loss: 0.004263774520821041\n",
      "Epoch 239/300\n",
      "Average training loss: 0.01755390371216668\n",
      "Average test loss: 0.004158565890457895\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017527360171079635\n",
      "Average test loss: 0.00428899024385545\n",
      "Epoch 241/300\n",
      "Average training loss: 0.017508989315893914\n",
      "Average test loss: 0.0041856934399240545\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017511891038881407\n",
      "Average test loss: 0.00412654065920247\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01751611361818181\n",
      "Average test loss: 0.0042459919899702075\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01751268884042899\n",
      "Average test loss: 0.004158037407323718\n",
      "Epoch 245/300\n",
      "Average training loss: 0.017498360640472835\n",
      "Average test loss: 0.004215170831109087\n",
      "Epoch 246/300\n",
      "Average training loss: 0.017497301912970012\n",
      "Average test loss: 0.0042679288801219725\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017487456311782203\n",
      "Average test loss: 0.004214625123888254\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01748507028983699\n",
      "Average test loss: 0.004201585126005941\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01748681005007691\n",
      "Average test loss: 0.004245264454434315\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01748600445687771\n",
      "Average test loss: 0.004299087614234951\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01747887091545595\n",
      "Average test loss: 0.004238578982651234\n",
      "Epoch 252/300\n",
      "Average training loss: 0.017470445356435247\n",
      "Average test loss: 0.004367740640209781\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01746266255941656\n",
      "Average test loss: 0.004238327925817834\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01746253430015511\n",
      "Average test loss: 0.00415227898913953\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017457769415444798\n",
      "Average test loss: 0.004253423900033037\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017442245208554797\n",
      "Average test loss: 0.004349964985416995\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01744184875819418\n",
      "Average test loss: 0.004303144639357924\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01744957873887486\n",
      "Average test loss: 0.004281013363765345\n",
      "Epoch 259/300\n",
      "Average training loss: 0.017435189712378715\n",
      "Average test loss: 0.004369299585206641\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01742992478609085\n",
      "Average test loss: 0.004259938623342249\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017432658248477513\n",
      "Average test loss: 0.004205978729865618\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017438293107681805\n",
      "Average test loss: 0.00426432189800673\n",
      "Epoch 263/300\n",
      "Average training loss: 0.017407916863759358\n",
      "Average test loss: 0.00428439859011107\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01741133817533652\n",
      "Average test loss: 0.00435292388084862\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01741983143157429\n",
      "Average test loss: 0.004356529919637574\n",
      "Epoch 266/300\n",
      "Average training loss: 0.01740740453451872\n",
      "Average test loss: 0.0042521657353887955\n",
      "Epoch 267/300\n",
      "Average training loss: 0.017413914589418305\n",
      "Average test loss: 0.004203895897708005\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0174132495207919\n",
      "Average test loss: 0.004236595647616519\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017387913995318943\n",
      "Average test loss: 0.004301488903247648\n",
      "Epoch 270/300\n",
      "Average training loss: 0.017389074818955526\n",
      "Average test loss: 0.004263304243692093\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01738491046594249\n",
      "Average test loss: 0.004321725528273318\n",
      "Epoch 272/300\n",
      "Average training loss: 0.017389661648207242\n",
      "Average test loss: 0.004303216604929831\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01736949078283376\n",
      "Average test loss: 0.004237998393674692\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01737009122139878\n",
      "Average test loss: 0.004229364666673873\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0173652712504069\n",
      "Average test loss: 0.004223245071868101\n",
      "Epoch 276/300\n",
      "Average training loss: 0.017357589377297294\n",
      "Average test loss: 0.004182227775868443\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01736832551161448\n",
      "Average test loss: 0.004223597424725691\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017352471144662963\n",
      "Average test loss: 0.004188145459319155\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01735142503016525\n",
      "Average test loss: 0.004427719466388226\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017355035833186573\n",
      "Average test loss: 0.004133877383545041\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01734761336776945\n",
      "Average test loss: 0.004304816096814142\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017326006399260626\n",
      "Average test loss: 0.004225372018085586\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01732992988328139\n",
      "Average test loss: 0.004249606536908282\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01733169920080238\n",
      "Average test loss: 0.0042044457520047825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017334240799148876\n",
      "Average test loss: 0.004147238496690989\n",
      "Epoch 286/300\n",
      "Average training loss: 0.017309918999671936\n",
      "Average test loss: 0.004346164025573267\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017326056498620245\n",
      "Average test loss: 0.004362417990548743\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017320696685049267\n",
      "Average test loss: 0.004345938040771418\n",
      "Epoch 289/300\n",
      "Average training loss: 0.017312093830770918\n",
      "Average test loss: 0.004238770118604103\n",
      "Epoch 290/300\n",
      "Average training loss: 0.017305070786012542\n",
      "Average test loss: 0.004269709140476253\n",
      "Epoch 291/300\n",
      "Average training loss: 0.017294367031918632\n",
      "Average test loss: 0.004272740196850565\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01730742187466886\n",
      "Average test loss: 0.004308727542145385\n",
      "Epoch 293/300\n",
      "Average training loss: 0.017304469777478113\n",
      "Average test loss: 0.004311471031564805\n",
      "Epoch 294/300\n",
      "Average training loss: 0.01729531110988723\n",
      "Average test loss: 0.004253081129656898\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017300926481684048\n",
      "Average test loss: 0.004242155407451921\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017286558660368125\n",
      "Average test loss: 0.004294687573694521\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01729010839925872\n",
      "Average test loss: 0.004270372360944748\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017265949200424884\n",
      "Average test loss: 0.004138293472843038\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017264138440291087\n",
      "Average test loss: 0.004230497931854592\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017275328291787042\n",
      "Average test loss: 0.004232530455622408\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.05345621352725559\n",
      "Average test loss: 0.004199565538929568\n",
      "Epoch 2/300\n",
      "Average training loss: 0.020604673919578393\n",
      "Average test loss: 0.004179587843517463\n",
      "Epoch 3/300\n",
      "Average training loss: 0.019589858758780693\n",
      "Average test loss: 0.003857640000060201\n",
      "Epoch 4/300\n",
      "Average training loss: 0.019096934237413935\n",
      "Average test loss: 0.00375842556088335\n",
      "Epoch 5/300\n",
      "Average training loss: 0.018737006748716038\n",
      "Average test loss: 0.0036693508302172023\n",
      "Epoch 6/300\n",
      "Average training loss: 0.018439830868608422\n",
      "Average test loss: 0.0036084363265997833\n",
      "Epoch 7/300\n",
      "Average training loss: 0.01816366947028372\n",
      "Average test loss: 0.003600353282358911\n",
      "Epoch 8/300\n",
      "Average training loss: 0.01791515230635802\n",
      "Average test loss: 0.0035503070631788835\n",
      "Epoch 9/300\n",
      "Average training loss: 0.017700924891564582\n",
      "Average test loss: 0.0034839223312834898\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01750376816921764\n",
      "Average test loss: 0.003424446145693461\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01730402318553792\n",
      "Average test loss: 0.0033842047208713162\n",
      "Epoch 12/300\n",
      "Average training loss: 0.017134864889913136\n",
      "Average test loss: 0.0033842688045567938\n",
      "Epoch 13/300\n",
      "Average training loss: 0.016980567146506573\n",
      "Average test loss: 0.003319723356101248\n",
      "Epoch 14/300\n",
      "Average training loss: 0.01683663874036736\n",
      "Average test loss: 0.0033035937692556115\n",
      "Epoch 15/300\n",
      "Average training loss: 0.016708853498101233\n",
      "Average test loss: 0.0032751191556453707\n",
      "Epoch 16/300\n",
      "Average training loss: 0.01659318236468567\n",
      "Average test loss: 0.003240235923892922\n",
      "Epoch 17/300\n",
      "Average training loss: 0.016479530974394745\n",
      "Average test loss: 0.0032279699471675686\n",
      "Epoch 18/300\n",
      "Average training loss: 0.016383445642060703\n",
      "Average test loss: 0.0032192099909815522\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01627560371905565\n",
      "Average test loss: 0.0033127049441552823\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01619839370581839\n",
      "Average test loss: 0.0031789030651044516\n",
      "Epoch 21/300\n",
      "Average training loss: 0.016111063054038418\n",
      "Average test loss: 0.0031572948123017946\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01604344445466995\n",
      "Average test loss: 0.003119873037147853\n",
      "Epoch 23/300\n",
      "Average training loss: 0.015952865001228122\n",
      "Average test loss: 0.00313453006464988\n",
      "Epoch 24/300\n",
      "Average training loss: 0.015891726966533397\n",
      "Average test loss: 0.003130973733961582\n",
      "Epoch 25/300\n",
      "Average training loss: 0.015838321164250373\n",
      "Average test loss: 0.003135282922536135\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0157713689174917\n",
      "Average test loss: 0.003074276346092423\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01571726499746243\n",
      "Average test loss: 0.0030921417818301255\n",
      "Epoch 28/300\n",
      "Average training loss: 0.015670556348231102\n",
      "Average test loss: 0.0030832687757081454\n",
      "Epoch 29/300\n",
      "Average training loss: 0.015607854732208782\n",
      "Average test loss: 0.003075878908443782\n",
      "Epoch 30/300\n",
      "Average training loss: 0.015569997433986928\n",
      "Average test loss: 0.0030552580102036395\n",
      "Epoch 31/300\n",
      "Average training loss: 0.015523203584055106\n",
      "Average test loss: 0.0030471377464839156\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01548050215923124\n",
      "Average test loss: 0.0030382043866233692\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015445331626468235\n",
      "Average test loss: 0.003039372390550044\n",
      "Epoch 34/300\n",
      "Average training loss: 0.015408009245163865\n",
      "Average test loss: 0.0031032708827406167\n",
      "Epoch 35/300\n",
      "Average training loss: 0.015363354269829061\n",
      "Average test loss: 0.0030301001291308137\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015329430853327115\n",
      "Average test loss: 0.0030889974327550996\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015298091082937188\n",
      "Average test loss: 0.0030311402411510547\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015250135902729299\n",
      "Average test loss: 0.003018047773072289\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015225949419869317\n",
      "Average test loss: 0.0030113653995924527\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015193217971258693\n",
      "Average test loss: 0.0030304317246708603\n",
      "Epoch 41/300\n",
      "Average training loss: 0.015160770922899245\n",
      "Average test loss: 0.0029995081029418444\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01513344882014725\n",
      "Average test loss: 0.0029933231284634933\n",
      "Epoch 43/300\n",
      "Average training loss: 0.015107168151272667\n",
      "Average test loss: 0.0030058461893349886\n",
      "Epoch 44/300\n",
      "Average training loss: 0.015075590323242877\n",
      "Average test loss: 0.0030102110457503133\n",
      "Epoch 45/300\n",
      "Average training loss: 0.015053779048638211\n",
      "Average test loss: 0.0030131530536131727\n",
      "Epoch 46/300\n",
      "Average training loss: 0.015009753687514199\n",
      "Average test loss: 0.0029773552895834047\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014995721497469479\n",
      "Average test loss: 0.0030006017714945808\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014955807483858533\n",
      "Average test loss: 0.003015930089685652\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014930776471892993\n",
      "Average test loss: 0.003016624777060416\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014912704472740491\n",
      "Average test loss: 0.0030003961090826325\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014880409491558869\n",
      "Average test loss: 0.0030011285315785143\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014857036800848112\n",
      "Average test loss: 0.0029751933682709932\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01482884174088637\n",
      "Average test loss: 0.0029847541995760467\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01480879586107201\n",
      "Average test loss: 0.003012714464010464\n",
      "Epoch 55/300\n",
      "Average training loss: 0.014789207423726717\n",
      "Average test loss: 0.0030408906526863573\n",
      "Epoch 56/300\n",
      "Average training loss: 0.014758103672001097\n",
      "Average test loss: 0.0030177495665848256\n",
      "Epoch 57/300\n",
      "Average training loss: 0.014750199933018949\n",
      "Average test loss: 0.0030467003639787437\n",
      "Epoch 58/300\n",
      "Average training loss: 0.014698408054808775\n",
      "Average test loss: 0.0029993749523742333\n",
      "Epoch 59/300\n",
      "Average training loss: 0.014691358409821987\n",
      "Average test loss: 0.00300248825425903\n",
      "Epoch 60/300\n",
      "Average training loss: 0.014663393068644736\n",
      "Average test loss: 0.002994172227051523\n",
      "Epoch 61/300\n",
      "Average training loss: 0.014642569434311655\n",
      "Average test loss: 0.0029743586279865767\n",
      "Epoch 62/300\n",
      "Average training loss: 0.014625322865943114\n",
      "Average test loss: 0.0029946780933274163\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01459936408781343\n",
      "Average test loss: 0.0030532304060955843\n",
      "Epoch 64/300\n",
      "Average training loss: 0.014584903115199672\n",
      "Average test loss: 0.0030751236182534033\n",
      "Epoch 65/300\n",
      "Average training loss: 0.01454393400831355\n",
      "Average test loss: 0.002975131401999129\n",
      "Epoch 66/300\n",
      "Average training loss: 0.014533474112550418\n",
      "Average test loss: 0.003042790723757611\n",
      "Epoch 67/300\n",
      "Average training loss: 0.014507120714419418\n",
      "Average test loss: 0.003048066675662994\n",
      "Epoch 68/300\n",
      "Average training loss: 0.014484441039462885\n",
      "Average test loss: 0.0029960825039694705\n",
      "Epoch 69/300\n",
      "Average training loss: 0.014471919844547907\n",
      "Average test loss: 0.0030577721525397567\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01444756876428922\n",
      "Average test loss: 0.003063880693581369\n",
      "Epoch 71/300\n",
      "Average training loss: 0.014425906702048248\n",
      "Average test loss: 0.0030241367078075805\n",
      "Epoch 72/300\n",
      "Average training loss: 0.014413010996248986\n",
      "Average test loss: 0.0030301302572091422\n",
      "Epoch 73/300\n",
      "Average training loss: 0.014381658441490598\n",
      "Average test loss: 0.003020348548061318\n",
      "Epoch 74/300\n",
      "Average training loss: 0.014360958806342548\n",
      "Average test loss: 0.00305540727203091\n",
      "Epoch 75/300\n",
      "Average training loss: 0.01434401265780131\n",
      "Average test loss: 0.0029936384951902762\n",
      "Epoch 76/300\n",
      "Average training loss: 0.014325476056999631\n",
      "Average test loss: 0.0030051400719417467\n",
      "Epoch 77/300\n",
      "Average training loss: 0.014309935689800315\n",
      "Average test loss: 0.003019736042039262\n",
      "Epoch 78/300\n",
      "Average training loss: 0.014296994822720687\n",
      "Average test loss: 0.0029951883231600124\n",
      "Epoch 79/300\n",
      "Average training loss: 0.014293642487376928\n",
      "Average test loss: 0.0030740725327697066\n",
      "Epoch 80/300\n",
      "Average training loss: 0.014252744478483995\n",
      "Average test loss: 0.003041202396568325\n",
      "Epoch 81/300\n",
      "Average training loss: 0.014233131555219491\n",
      "Average test loss: 0.0030392518813411396\n",
      "Epoch 82/300\n",
      "Average training loss: 0.014225825818876426\n",
      "Average test loss: 0.003134812800006734\n",
      "Epoch 83/300\n",
      "Average training loss: 0.014199279654357168\n",
      "Average test loss: 0.0030038894398344887\n",
      "Epoch 84/300\n",
      "Average training loss: 0.014183137090669738\n",
      "Average test loss: 0.003025524011709624\n",
      "Epoch 85/300\n",
      "Average training loss: 0.014167083568043179\n",
      "Average test loss: 0.0030358358770608902\n",
      "Epoch 86/300\n",
      "Average training loss: 0.014153345492151049\n",
      "Average test loss: 0.00303657558788028\n",
      "Epoch 87/300\n",
      "Average training loss: 0.014126139076219665\n",
      "Average test loss: 0.003056556660061081\n",
      "Epoch 88/300\n",
      "Average training loss: 0.014115691802567906\n",
      "Average test loss: 0.00314450055712627\n",
      "Epoch 89/300\n",
      "Average training loss: 0.014097183260652754\n",
      "Average test loss: 0.0031461656507518554\n",
      "Epoch 90/300\n",
      "Average training loss: 0.014069493750317229\n",
      "Average test loss: 0.003088941262000137\n",
      "Epoch 91/300\n",
      "Average training loss: 0.014073776194618809\n",
      "Average test loss: 0.0030165324518457055\n",
      "Epoch 92/300\n",
      "Average training loss: 0.014056772926615344\n",
      "Average test loss: 0.0030349927585985926\n",
      "Epoch 93/300\n",
      "Average training loss: 0.014043067437079218\n",
      "Average test loss: 0.0031437437335650126\n",
      "Epoch 94/300\n",
      "Average training loss: 0.014017005933655633\n",
      "Average test loss: 0.003027832206338644\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01401023756712675\n",
      "Average test loss: 0.0030480704663528335\n",
      "Epoch 96/300\n",
      "Average training loss: 0.01398963631855117\n",
      "Average test loss: 0.003069139702556034\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0139760767146945\n",
      "Average test loss: 0.003032951427416669\n",
      "Epoch 98/300\n",
      "Average training loss: 0.013969064488179153\n",
      "Average test loss: 0.0030225312320722474\n",
      "Epoch 99/300\n",
      "Average training loss: 0.013941830693847603\n",
      "Average test loss: 0.003119245298206806\n",
      "Epoch 100/300\n",
      "Average training loss: 0.013935570109221671\n",
      "Average test loss: 0.003063445826785432\n",
      "Epoch 101/300\n",
      "Average training loss: 0.013908962997297446\n",
      "Average test loss: 0.0030513974118770823\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01389655981047286\n",
      "Average test loss: 0.003091038524897562\n",
      "Epoch 103/300\n",
      "Average training loss: 0.013895073742502265\n",
      "Average test loss: 0.0030051080859783623\n",
      "Epoch 104/300\n",
      "Average training loss: 0.013871469027466244\n",
      "Average test loss: 0.0030412746554033623\n",
      "Epoch 105/300\n",
      "Average training loss: 0.013870873650742901\n",
      "Average test loss: 0.003112324505837427\n",
      "Epoch 106/300\n",
      "Average training loss: 0.013849209943579303\n",
      "Average test loss: 0.003037215010573467\n",
      "Epoch 107/300\n",
      "Average training loss: 0.013821613316734632\n",
      "Average test loss: 0.0031291601926916176\n",
      "Epoch 108/300\n",
      "Average training loss: 0.013823479923109214\n",
      "Average test loss: 0.003116989016946819\n",
      "Epoch 109/300\n",
      "Average training loss: 0.013818840809994274\n",
      "Average test loss: 0.0030517410499354206\n",
      "Epoch 110/300\n",
      "Average training loss: 0.013783336004449262\n",
      "Average test loss: 0.003050695157920321\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01377747348199288\n",
      "Average test loss: 0.0031169888967027266\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01375741102712022\n",
      "Average test loss: 0.003077564444185959\n",
      "Epoch 113/300\n",
      "Average training loss: 0.013758238108621703\n",
      "Average test loss: 0.003130982713566886\n",
      "Epoch 114/300\n",
      "Average training loss: 0.013743601695530945\n",
      "Average test loss: 0.0030274824736018975\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01374038349174791\n",
      "Average test loss: 0.003130916537096103\n",
      "Epoch 116/300\n",
      "Average training loss: 0.013722655113372537\n",
      "Average test loss: 0.0031009819996025826\n",
      "Epoch 117/300\n",
      "Average training loss: 0.013704921883841356\n",
      "Average test loss: 0.003140860169298119\n",
      "Epoch 118/300\n",
      "Average training loss: 0.013696119753022989\n",
      "Average test loss: 0.00309870318406158\n",
      "Epoch 119/300\n",
      "Average training loss: 0.013683878007034462\n",
      "Average test loss: 0.003197180566481418\n",
      "Epoch 120/300\n",
      "Average training loss: 0.013684977375798756\n",
      "Average test loss: 0.0031073753034902944\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01366559465809001\n",
      "Average test loss: 0.003231700430934628\n",
      "Epoch 122/300\n",
      "Average training loss: 0.01364559304051929\n",
      "Average test loss: 0.003115445638696353\n",
      "Epoch 123/300\n",
      "Average training loss: 0.013639196635120445\n",
      "Average test loss: 0.003211589835584164\n",
      "Epoch 124/300\n",
      "Average training loss: 0.013625644437140889\n",
      "Average test loss: 0.0031014511444502407\n",
      "Epoch 125/300\n",
      "Average training loss: 0.013632965460419654\n",
      "Average test loss: 0.003074348503930701\n",
      "Epoch 126/300\n",
      "Average training loss: 0.013601946234703064\n",
      "Average test loss: 0.003090095301469167\n",
      "Epoch 127/300\n",
      "Average training loss: 0.013597549239794414\n",
      "Average test loss: 0.003179761532901062\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01358769783957137\n",
      "Average test loss: 0.0031278742742207317\n",
      "Epoch 129/300\n",
      "Average training loss: 0.013575721537073454\n",
      "Average test loss: 0.0031154973451048137\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01357021727744076\n",
      "Average test loss: 0.003122888529880179\n",
      "Epoch 131/300\n",
      "Average training loss: 0.013546933965550529\n",
      "Average test loss: 0.0031277422437237367\n",
      "Epoch 132/300\n",
      "Average training loss: 0.013550236216021909\n",
      "Average test loss: 0.0032417438622150156\n",
      "Epoch 133/300\n",
      "Average training loss: 0.013556459084981017\n",
      "Average test loss: 0.0032033269881374307\n",
      "Epoch 134/300\n",
      "Average training loss: 0.013518040055202114\n",
      "Average test loss: 0.003168829501606524\n",
      "Epoch 135/300\n",
      "Average training loss: 0.013509248297247622\n",
      "Average test loss: 0.0030954844252102905\n",
      "Epoch 136/300\n",
      "Average training loss: 0.013508960078159968\n",
      "Average test loss: 0.0031578087343109977\n",
      "Epoch 137/300\n",
      "Average training loss: 0.013495925980309645\n",
      "Average test loss: 0.003105683559551835\n",
      "Epoch 138/300\n",
      "Average training loss: 0.013487644457154804\n",
      "Average test loss: 0.003226788359383742\n",
      "Epoch 139/300\n",
      "Average training loss: 0.013495508040818904\n",
      "Average test loss: 0.0032038460015836687\n",
      "Epoch 140/300\n",
      "Average training loss: 0.013474810543159644\n",
      "Average test loss: 0.003471999579005771\n",
      "Epoch 141/300\n",
      "Average training loss: 0.013460489610830942\n",
      "Average test loss: 0.0031884979342834815\n",
      "Epoch 142/300\n",
      "Average training loss: 0.013459060040613015\n",
      "Average test loss: 0.0032282055349399648\n",
      "Epoch 143/300\n",
      "Average training loss: 0.013441084652311272\n",
      "Average test loss: 0.0031293960944232015\n",
      "Epoch 144/300\n",
      "Average training loss: 0.013429300759401587\n",
      "Average test loss: 0.0030587935938189426\n",
      "Epoch 145/300\n",
      "Average training loss: 0.013428780131042003\n",
      "Average test loss: 0.003203827723239859\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01340937337693241\n",
      "Average test loss: 0.0031225838193462956\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01341611737675137\n",
      "Average test loss: 0.003153713342630201\n",
      "Epoch 148/300\n",
      "Average training loss: 0.013409326846400896\n",
      "Average test loss: 0.0031444978811260727\n",
      "Epoch 149/300\n",
      "Average training loss: 0.013388841528859403\n",
      "Average test loss: 0.003209469067553679\n",
      "Epoch 150/300\n",
      "Average training loss: 0.013381336233682102\n",
      "Average test loss: 0.003251859757014447\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01338052487042215\n",
      "Average test loss: 0.0031387651384704642\n",
      "Epoch 152/300\n",
      "Average training loss: 0.013362045329478052\n",
      "Average test loss: 0.0031320820099984604\n",
      "Epoch 153/300\n",
      "Average training loss: 0.013373224992718962\n",
      "Average test loss: 0.003113825978090366\n",
      "Epoch 154/300\n",
      "Average training loss: 0.013352857959767183\n",
      "Average test loss: 0.0032431720451762277\n",
      "Epoch 155/300\n",
      "Average training loss: 0.013351499854690498\n",
      "Average test loss: 0.0031296709184017446\n",
      "Epoch 156/300\n",
      "Average training loss: 0.013338411167263985\n",
      "Average test loss: 0.0033054829510963625\n",
      "Epoch 157/300\n",
      "Average training loss: 0.013345395488043626\n",
      "Average test loss: 0.003143652840413981\n",
      "Epoch 158/300\n",
      "Average training loss: 0.013316914879613452\n",
      "Average test loss: 0.003154555948244201\n",
      "Epoch 159/300\n",
      "Average training loss: 0.013315122109320428\n",
      "Average test loss: 0.003136166885081265\n",
      "Epoch 160/300\n",
      "Average training loss: 0.013310328466196855\n",
      "Average test loss: 0.0031342417891654702\n",
      "Epoch 161/300\n",
      "Average training loss: 0.013304585090941854\n",
      "Average test loss: 0.003217121268105176\n",
      "Epoch 162/300\n",
      "Average training loss: 0.013273930552932951\n",
      "Average test loss: 0.003104386874164144\n",
      "Epoch 163/300\n",
      "Average training loss: 0.013291976816124386\n",
      "Average test loss: 0.003207336019931568\n",
      "Epoch 164/300\n",
      "Average training loss: 0.013284565461178621\n",
      "Average test loss: 0.003119503579619858\n",
      "Epoch 165/300\n",
      "Average training loss: 0.013283490022851362\n",
      "Average test loss: 0.0032451921473774644\n",
      "Epoch 166/300\n",
      "Average training loss: 0.013258571359018485\n",
      "Average test loss: 0.003181180402636528\n",
      "Epoch 167/300\n",
      "Average training loss: 0.013267720992366472\n",
      "Average test loss: 0.0032032369997145402\n",
      "Epoch 168/300\n",
      "Average training loss: 0.013257649978001912\n",
      "Average test loss: 0.0032234651220755446\n",
      "Epoch 169/300\n",
      "Average training loss: 0.013247356431351767\n",
      "Average test loss: 0.003239538727949063\n",
      "Epoch 170/300\n",
      "Average training loss: 0.013231219444837836\n",
      "Average test loss: 0.003127139811300569\n",
      "Epoch 171/300\n",
      "Average training loss: 0.013235910633371937\n",
      "Average test loss: 0.0031999470601893135\n",
      "Epoch 172/300\n",
      "Average training loss: 0.013229395656122103\n",
      "Average test loss: 0.003270485073534979\n",
      "Epoch 173/300\n",
      "Average training loss: 0.013224183809426096\n",
      "Average test loss: 0.0032552340595672527\n",
      "Epoch 174/300\n",
      "Average training loss: 0.013216171491477224\n",
      "Average test loss: 0.0031542536781893836\n",
      "Epoch 175/300\n",
      "Average training loss: 0.013204743439952532\n",
      "Average test loss: 0.0031674122244326605\n",
      "Epoch 176/300\n",
      "Average training loss: 0.013208893796635998\n",
      "Average test loss: 0.0032427070583734246\n",
      "Epoch 177/300\n",
      "Average training loss: 0.013188987203770213\n",
      "Average test loss: 0.0031466586262815528\n",
      "Epoch 178/300\n",
      "Average training loss: 0.013191861607962185\n",
      "Average test loss: 0.0032892071283939814\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01318615529851781\n",
      "Average test loss: 0.003229195911851194\n",
      "Epoch 180/300\n",
      "Average training loss: 0.013185534449915092\n",
      "Average test loss: 0.003144523966540065\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01318219326933225\n",
      "Average test loss: 0.003244575023651123\n",
      "Epoch 182/300\n",
      "Average training loss: 0.013158448571960132\n",
      "Average test loss: 0.0031623033659739625\n",
      "Epoch 183/300\n",
      "Average training loss: 0.013167574532330036\n",
      "Average test loss: 0.0032184387863510183\n",
      "Epoch 184/300\n",
      "Average training loss: 0.013148069572945436\n",
      "Average test loss: 0.0032510581360095077\n",
      "Epoch 185/300\n",
      "Average training loss: 0.013130644406709406\n",
      "Average test loss: 0.0031580969627118772\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01314016218814585\n",
      "Average test loss: 0.0032574164933628505\n",
      "Epoch 187/300\n",
      "Average training loss: 0.013129443812701437\n",
      "Average test loss: 0.0031805457207891674\n",
      "Epoch 188/300\n",
      "Average training loss: 0.013137310115413534\n",
      "Average test loss: 0.0032439431322531568\n",
      "Epoch 189/300\n",
      "Average training loss: 0.01312581918305821\n",
      "Average test loss: 0.003192083458519644\n",
      "Epoch 190/300\n",
      "Average training loss: 0.013106403790414334\n",
      "Average test loss: 0.003210598744245039\n",
      "Epoch 191/300\n",
      "Average training loss: 0.013113341828187308\n",
      "Average test loss: 0.0031604799458550084\n",
      "Epoch 192/300\n",
      "Average training loss: 0.013099787500997384\n",
      "Average test loss: 0.0031345744885297286\n",
      "Epoch 193/300\n",
      "Average training loss: 0.013110069330367777\n",
      "Average test loss: 0.003314738422218296\n",
      "Epoch 194/300\n",
      "Average training loss: 0.013080268402894338\n",
      "Average test loss: 0.0032103271627177796\n",
      "Epoch 195/300\n",
      "Average training loss: 0.013098941788905197\n",
      "Average test loss: 0.0031536243052946197\n",
      "Epoch 196/300\n",
      "Average training loss: 0.013086304293738471\n",
      "Average test loss: 0.0032183315526280137\n",
      "Epoch 197/300\n",
      "Average training loss: 0.013071266459921996\n",
      "Average test loss: 0.003414120089262724\n",
      "Epoch 198/300\n",
      "Average training loss: 0.013079615163306395\n",
      "Average test loss: 0.0032142376403013866\n",
      "Epoch 199/300\n",
      "Average training loss: 0.013084024826685588\n",
      "Average test loss: 0.003235118128359318\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01304987221211195\n",
      "Average test loss: 0.0032323472551587557\n",
      "Epoch 201/300\n",
      "Average training loss: 0.013055555312997765\n",
      "Average test loss: 0.0032008169961886272\n",
      "Epoch 202/300\n",
      "Average training loss: 0.013042564081648986\n",
      "Average test loss: 0.0032090112751142845\n",
      "Epoch 203/300\n",
      "Average training loss: 0.013047012928873301\n",
      "Average test loss: 0.003212985184871488\n",
      "Epoch 204/300\n",
      "Average training loss: 0.013044822659757403\n",
      "Average test loss: 0.003284936871793535\n",
      "Epoch 205/300\n",
      "Average training loss: 0.013046825038890044\n",
      "Average test loss: 0.0031827401655415695\n",
      "Epoch 206/300\n",
      "Average training loss: 0.013054711210230985\n",
      "Average test loss: 0.0031681192898088033\n",
      "Epoch 207/300\n",
      "Average training loss: 0.013031549574600326\n",
      "Average test loss: 0.0031501417234539985\n",
      "Epoch 208/300\n",
      "Average training loss: 0.013016292345192698\n",
      "Average test loss: 0.0033996734900607\n",
      "Epoch 209/300\n",
      "Average training loss: 0.013007636711415317\n",
      "Average test loss: 0.0032137691405498318\n",
      "Epoch 210/300\n",
      "Average training loss: 0.013037162154912948\n",
      "Average test loss: 0.0032649873157756196\n",
      "Epoch 211/300\n",
      "Average training loss: 0.013016322322189808\n",
      "Average test loss: 0.003099360965192318\n",
      "Epoch 212/300\n",
      "Average training loss: 0.013007065194348494\n",
      "Average test loss: 0.0031524766631838346\n",
      "Epoch 213/300\n",
      "Average training loss: 0.012996028474635548\n",
      "Average test loss: 0.003238013249718481\n",
      "Epoch 214/300\n",
      "Average training loss: 0.012993710128797424\n",
      "Average test loss: 0.003187378397418393\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01298678553269969\n",
      "Average test loss: 0.0033603589218109847\n",
      "Epoch 216/300\n",
      "Average training loss: 0.012994521539658308\n",
      "Average test loss: 0.0033113384224060507\n",
      "Epoch 217/300\n",
      "Average training loss: 0.01300229481773244\n",
      "Average test loss: 0.003241120219955014\n",
      "Epoch 218/300\n",
      "Average training loss: 0.013007944440262185\n",
      "Average test loss: 0.0033554107532319096\n",
      "Epoch 219/300\n",
      "Average training loss: 0.012959516047603554\n",
      "Average test loss: 0.003273414107246531\n",
      "Epoch 220/300\n",
      "Average training loss: 0.012957897298038005\n",
      "Average test loss: 0.0031423890213999484\n",
      "Epoch 221/300\n",
      "Average training loss: 0.012959446147084237\n",
      "Average test loss: 0.0031308799791667195\n",
      "Epoch 222/300\n",
      "Average training loss: 0.012952780157327651\n",
      "Average test loss: 0.0032410730429821544\n",
      "Epoch 223/300\n",
      "Average training loss: 0.012941340737044811\n",
      "Average test loss: 0.003274998641676373\n",
      "Epoch 224/300\n",
      "Average training loss: 0.012959111383391751\n",
      "Average test loss: 0.0031885575198878843\n",
      "Epoch 225/300\n",
      "Average training loss: 0.012971138616402944\n",
      "Average test loss: 0.0031360624192489518\n",
      "Epoch 226/300\n",
      "Average training loss: 0.012943017318844795\n",
      "Average test loss: 0.0032660055057042174\n",
      "Epoch 227/300\n",
      "Average training loss: 0.012924106336302227\n",
      "Average test loss: 0.003219732695362634\n",
      "Epoch 228/300\n",
      "Average training loss: 0.012932624397178492\n",
      "Average test loss: 0.0032141212361554306\n",
      "Epoch 229/300\n",
      "Average training loss: 0.012938082462383642\n",
      "Average test loss: 0.0031923636481579806\n",
      "Epoch 230/300\n",
      "Average training loss: 0.012928249697718355\n",
      "Average test loss: 0.003182991075846884\n",
      "Epoch 231/300\n",
      "Average training loss: 0.012923170174989436\n",
      "Average test loss: 0.0033236403848148056\n",
      "Epoch 232/300\n",
      "Average training loss: 0.012914779813753234\n",
      "Average test loss: 0.0031832582164141865\n",
      "Epoch 233/300\n",
      "Average training loss: 0.012918676336606344\n",
      "Average test loss: 0.003306154703100522\n",
      "Epoch 234/300\n",
      "Average training loss: 0.012920176155865193\n",
      "Average test loss: 0.003194801277998421\n",
      "Epoch 235/300\n",
      "Average training loss: 0.012896695755422115\n",
      "Average test loss: 0.003213452558964491\n",
      "Epoch 236/300\n",
      "Average training loss: 0.012903727227614986\n",
      "Average test loss: 0.0033064172079579697\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01289622512128618\n",
      "Average test loss: 0.0032797846402972935\n",
      "Epoch 238/300\n",
      "Average training loss: 0.012894643355574872\n",
      "Average test loss: 0.003282327029440138\n",
      "Epoch 239/300\n",
      "Average training loss: 0.012897293344140052\n",
      "Average test loss: 0.0031805508935617076\n",
      "Epoch 240/300\n",
      "Average training loss: 0.012881447071830431\n",
      "Average test loss: 0.0032029128490636744\n",
      "Epoch 241/300\n",
      "Average training loss: 0.012889961606098546\n",
      "Average test loss: 0.0032407040745019914\n",
      "Epoch 242/300\n",
      "Average training loss: 0.012882491735120615\n",
      "Average test loss: 0.0032441984025968445\n",
      "Epoch 243/300\n",
      "Average training loss: 0.012880202886958916\n",
      "Average test loss: 0.0031613375892241796\n",
      "Epoch 244/300\n",
      "Average training loss: 0.012880120496782992\n",
      "Average test loss: 0.0032736053553720315\n",
      "Epoch 245/300\n",
      "Average training loss: 0.012873327275945081\n",
      "Average test loss: 0.0032304756068107154\n",
      "Epoch 246/300\n",
      "Average training loss: 0.012880050622754626\n",
      "Average test loss: 0.0031891263619893127\n",
      "Epoch 247/300\n",
      "Average training loss: 0.012843208561340969\n",
      "Average test loss: 0.003212573545674483\n",
      "Epoch 248/300\n",
      "Average training loss: 0.012864132112926908\n",
      "Average test loss: 0.0032792924305217134\n",
      "Epoch 249/300\n",
      "Average training loss: 0.012846332831515206\n",
      "Average test loss: 0.0032591883037239315\n",
      "Epoch 250/300\n",
      "Average training loss: 0.012861696552899149\n",
      "Average test loss: 0.0032491328509317505\n",
      "Epoch 251/300\n",
      "Average training loss: 0.012835129796630807\n",
      "Average test loss: 0.0031510928103493318\n",
      "Epoch 252/300\n",
      "Average training loss: 0.012847616113722324\n",
      "Average test loss: 0.0033175616707238884\n",
      "Epoch 253/300\n",
      "Average training loss: 0.01283439497401317\n",
      "Average test loss: 0.003264975926735335\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01284100748433007\n",
      "Average test loss: 0.0032311554004748663\n",
      "Epoch 255/300\n",
      "Average training loss: 0.012827001134554546\n",
      "Average test loss: 0.0032860824772053296\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01282429939839575\n",
      "Average test loss: 0.0033818104159500863\n",
      "Epoch 257/300\n",
      "Average training loss: 0.012823357292347484\n",
      "Average test loss: 0.003248029881467422\n",
      "Epoch 258/300\n",
      "Average training loss: 0.012825280613369412\n",
      "Average test loss: 0.0033133307914767\n",
      "Epoch 259/300\n",
      "Average training loss: 0.012819858656989204\n",
      "Average test loss: 0.0032820369352897006\n",
      "Epoch 260/300\n",
      "Average training loss: 0.012802119607726732\n",
      "Average test loss: 0.0033306200293203195\n",
      "Epoch 261/300\n",
      "Average training loss: 0.012801235784259107\n",
      "Average test loss: 0.0033728600868748295\n",
      "Epoch 262/300\n",
      "Average training loss: 0.012805187265906069\n",
      "Average test loss: 0.003280316774422924\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01281738155335188\n",
      "Average test loss: 0.0031956905906813013\n",
      "Epoch 264/300\n",
      "Average training loss: 0.012791873940163189\n",
      "Average test loss: 0.003299426544871595\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01281207662075758\n",
      "Average test loss: 0.0032539822583397228\n",
      "Epoch 266/300\n",
      "Average training loss: 0.012792857656876246\n",
      "Average test loss: 0.003162258427590132\n",
      "Epoch 267/300\n",
      "Average training loss: 0.012799588878949483\n",
      "Average test loss: 0.003231324158194992\n",
      "Epoch 268/300\n",
      "Average training loss: 0.012793214159707229\n",
      "Average test loss: 0.0032983750872727898\n",
      "Epoch 269/300\n",
      "Average training loss: 0.012782885460389986\n",
      "Average test loss: 0.0033002619517760144\n",
      "Epoch 270/300\n",
      "Average training loss: 0.012777852084901597\n",
      "Average test loss: 0.0033076111376285555\n",
      "Epoch 271/300\n",
      "Average training loss: 0.012782415634228123\n",
      "Average test loss: 0.0033354662989990578\n",
      "Epoch 272/300\n",
      "Average training loss: 0.012793978663782278\n",
      "Average test loss: 0.0033495029925058287\n",
      "Epoch 273/300\n",
      "Average training loss: 0.012768749727971023\n",
      "Average test loss: 0.0033821625374257564\n",
      "Epoch 274/300\n",
      "Average training loss: 0.012777782043649091\n",
      "Average test loss: 0.0032055457875960402\n",
      "Epoch 275/300\n",
      "Average training loss: 0.012776906653410858\n",
      "Average test loss: 0.003187763558700681\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01277205717149708\n",
      "Average test loss: 0.003392905240464542\n",
      "Epoch 277/300\n",
      "Average training loss: 0.012748509303563171\n",
      "Average test loss: 0.003241576210078266\n",
      "Epoch 278/300\n",
      "Average training loss: 0.012758410754303137\n",
      "Average test loss: 0.0032435563436398903\n",
      "Epoch 279/300\n",
      "Average training loss: 0.012761746808058685\n",
      "Average test loss: 0.003227884913277295\n",
      "Epoch 280/300\n",
      "Average training loss: 0.012750976321597895\n",
      "Average test loss: 0.0032837977843979994\n",
      "Epoch 281/300\n",
      "Average training loss: 0.012761000905599859\n",
      "Average test loss: 0.0032165944739762278\n",
      "Epoch 282/300\n",
      "Average training loss: 0.012735665070099962\n",
      "Average test loss: 0.003239217737482654\n",
      "Epoch 283/300\n",
      "Average training loss: 0.012743720880813068\n",
      "Average test loss: 0.0035133899820761548\n",
      "Epoch 284/300\n",
      "Average training loss: 0.012749121484657129\n",
      "Average test loss: 0.0032578810550686387\n",
      "Epoch 285/300\n",
      "Average training loss: 0.012743310511112212\n",
      "Average test loss: 0.0032864637946089107\n",
      "Epoch 286/300\n",
      "Average training loss: 0.012724394362833765\n",
      "Average test loss: 0.003258599137680398\n",
      "Epoch 287/300\n",
      "Average training loss: 0.012727084480226039\n",
      "Average test loss: 0.003260561976167891\n",
      "Epoch 288/300\n",
      "Average training loss: 0.01272867326769564\n",
      "Average test loss: 0.003185739954105682\n",
      "Epoch 289/300\n",
      "Average training loss: 0.012713872022926807\n",
      "Average test loss: 0.0033437310219224955\n",
      "Epoch 290/300\n",
      "Average training loss: 0.012727299865749147\n",
      "Average test loss: 0.003252675694723924\n",
      "Epoch 291/300\n",
      "Average training loss: 0.012721547522478633\n",
      "Average test loss: 0.0032373185468216738\n",
      "Epoch 292/300\n",
      "Average training loss: 0.012711927648219797\n",
      "Average test loss: 0.0033958501619183357\n",
      "Epoch 293/300\n",
      "Average training loss: 0.012705842124091255\n",
      "Average test loss: 0.003229114820145898\n",
      "Epoch 294/300\n",
      "Average training loss: 0.012709113872713514\n",
      "Average test loss: 0.003311683623120189\n",
      "Epoch 295/300\n",
      "Average training loss: 0.012712557024425931\n",
      "Average test loss: 0.0033470160456167326\n",
      "Epoch 296/300\n",
      "Average training loss: 0.012715129243002998\n",
      "Average test loss: 0.0033187660783943205\n",
      "Epoch 297/300\n",
      "Average training loss: 0.012702054952581724\n",
      "Average test loss: 0.0032756755076762704\n",
      "Epoch 298/300\n",
      "Average training loss: 0.012716274325218466\n",
      "Average test loss: 0.0032931984240810075\n",
      "Epoch 299/300\n",
      "Average training loss: 0.012708224016759131\n",
      "Average test loss: 0.0032377833051400054\n",
      "Epoch 300/300\n",
      "Average training loss: 0.012690030021799935\n",
      "Average test loss: 0.003201706510451105\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.04906874292426639\n",
      "Average test loss: 0.003605820080058442\n",
      "Epoch 2/300\n",
      "Average training loss: 0.017744096470375857\n",
      "Average test loss: 0.0033961461736924116\n",
      "Epoch 3/300\n",
      "Average training loss: 0.016693564378552968\n",
      "Average test loss: 0.0031886972586313883\n",
      "Epoch 4/300\n",
      "Average training loss: 0.01610516814308034\n",
      "Average test loss: 0.0030667880636950336\n",
      "Epoch 5/300\n",
      "Average training loss: 0.015655086672968336\n",
      "Average test loss: 0.0030343063442657392\n",
      "Epoch 6/300\n",
      "Average training loss: 0.015266625877883699\n",
      "Average test loss: 0.0028992621074948044\n",
      "Epoch 7/300\n",
      "Average training loss: 0.014922843261725372\n",
      "Average test loss: 0.0028397631098826728\n",
      "Epoch 8/300\n",
      "Average training loss: 0.014627182087136639\n",
      "Average test loss: 0.0027791353920474647\n",
      "Epoch 9/300\n",
      "Average training loss: 0.014351284210052755\n",
      "Average test loss: 0.002803025908043815\n",
      "Epoch 10/300\n",
      "Average training loss: 0.01410034810503324\n",
      "Average test loss: 0.0027386080080436335\n",
      "Epoch 11/300\n",
      "Average training loss: 0.01387755566338698\n",
      "Average test loss: 0.002691049008940657\n",
      "Epoch 12/300\n",
      "Average training loss: 0.01366498921232091\n",
      "Average test loss: 0.0027044099910805623\n",
      "Epoch 13/300\n",
      "Average training loss: 0.013492061365809705\n",
      "Average test loss: 0.00252142447936866\n",
      "Epoch 14/300\n",
      "Average training loss: 0.013315773446526792\n",
      "Average test loss: 0.0025201544145950013\n",
      "Epoch 15/300\n",
      "Average training loss: 0.013163210874630345\n",
      "Average test loss: 0.002614993111954795\n",
      "Epoch 16/300\n",
      "Average training loss: 0.013029729411833816\n",
      "Average test loss: 0.0024690167850090396\n",
      "Epoch 17/300\n",
      "Average training loss: 0.012912508028248946\n",
      "Average test loss: 0.0024292784832004046\n",
      "Epoch 18/300\n",
      "Average training loss: 0.012807951831155353\n",
      "Average test loss: 0.002408286175897552\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01270299618691206\n",
      "Average test loss: 0.00237663455783493\n",
      "Epoch 20/300\n",
      "Average training loss: 0.012599893406033517\n",
      "Average test loss: 0.0023755949654926858\n",
      "Epoch 21/300\n",
      "Average training loss: 0.012537828359338973\n",
      "Average test loss: 0.002345381612992949\n",
      "Epoch 22/300\n",
      "Average training loss: 0.012455479819741514\n",
      "Average test loss: 0.0023878776743594143\n",
      "Epoch 23/300\n",
      "Average training loss: 0.01239019893275367\n",
      "Average test loss: 0.0023279793641219535\n",
      "Epoch 24/300\n",
      "Average training loss: 0.012311638782421749\n",
      "Average test loss: 0.0023314249128517177\n",
      "Epoch 25/300\n",
      "Average training loss: 0.012270077804724375\n",
      "Average test loss: 0.0023089124366848005\n",
      "Epoch 26/300\n",
      "Average training loss: 0.012198802741865317\n",
      "Average test loss: 0.002306674966795577\n",
      "Epoch 27/300\n",
      "Average training loss: 0.012145198099315167\n",
      "Average test loss: 0.002289889745828178\n",
      "Epoch 28/300\n",
      "Average training loss: 0.012108463310533099\n",
      "Average test loss: 0.002270664364306463\n",
      "Epoch 29/300\n",
      "Average training loss: 0.012051723309689098\n",
      "Average test loss: 0.002297428233342038\n",
      "Epoch 30/300\n",
      "Average training loss: 0.012017345430122481\n",
      "Average test loss: 0.00225626930490964\n",
      "Epoch 31/300\n",
      "Average training loss: 0.011954328970776664\n",
      "Average test loss: 0.0022413381774806316\n",
      "Epoch 32/300\n",
      "Average training loss: 0.011936462218562762\n",
      "Average test loss: 0.0022642011353746056\n",
      "Epoch 33/300\n",
      "Average training loss: 0.011895157624449995\n",
      "Average test loss: 0.0022491085150589544\n",
      "Epoch 34/300\n",
      "Average training loss: 0.011838880049685637\n",
      "Average test loss: 0.002232333543193009\n",
      "Epoch 35/300\n",
      "Average training loss: 0.011811229973203606\n",
      "Average test loss: 0.0022468205508258607\n",
      "Epoch 36/300\n",
      "Average training loss: 0.011785471265928613\n",
      "Average test loss: 0.002231182648696833\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01174434127824174\n",
      "Average test loss: 0.002250788419197003\n",
      "Epoch 38/300\n",
      "Average training loss: 0.011721420582797792\n",
      "Average test loss: 0.0022213281854573225\n",
      "Epoch 39/300\n",
      "Average training loss: 0.011682853810489177\n",
      "Average test loss: 0.002236161976845728\n",
      "Epoch 40/300\n",
      "Average training loss: 0.011669290864633189\n",
      "Average test loss: 0.002203037137683067\n",
      "Epoch 41/300\n",
      "Average training loss: 0.011633381294707458\n",
      "Average test loss: 0.0021950705459134446\n",
      "Epoch 42/300\n",
      "Average training loss: 0.011593018944064776\n",
      "Average test loss: 0.002229263569228351\n",
      "Epoch 43/300\n",
      "Average training loss: 0.011588032287855944\n",
      "Average test loss: 0.0022072895618362558\n",
      "Epoch 44/300\n",
      "Average training loss: 0.01154903596225712\n",
      "Average test loss: 0.0022156347682078677\n",
      "Epoch 45/300\n",
      "Average training loss: 0.011532365475470822\n",
      "Average test loss: 0.002281874098504583\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01149648688154088\n",
      "Average test loss: 0.0022118263995895785\n",
      "Epoch 47/300\n",
      "Average training loss: 0.011472082941896385\n",
      "Average test loss: 0.002228770172016488\n",
      "Epoch 48/300\n",
      "Average training loss: 0.011464544543789493\n",
      "Average test loss: 0.0021866520781897835\n",
      "Epoch 49/300\n",
      "Average training loss: 0.011430636886921194\n",
      "Average test loss: 0.0022041803799155687\n",
      "Epoch 50/300\n",
      "Average training loss: 0.011416285234193007\n",
      "Average test loss: 0.0022030135452126465\n",
      "Epoch 51/300\n",
      "Average training loss: 0.01138702171544234\n",
      "Average test loss: 0.002201404231807424\n",
      "Epoch 52/300\n",
      "Average training loss: 0.011366868163148563\n",
      "Average test loss: 0.002210669471157922\n",
      "Epoch 53/300\n",
      "Average training loss: 0.011352537551687823\n",
      "Average test loss: 0.0021834097204522953\n",
      "Epoch 54/300\n",
      "Average training loss: 0.011321154952877098\n",
      "Average test loss: 0.0022012158690227403\n",
      "Epoch 55/300\n",
      "Average training loss: 0.011300699171920617\n",
      "Average test loss: 0.00221695635571248\n",
      "Epoch 56/300\n",
      "Average training loss: 0.01127935341745615\n",
      "Average test loss: 0.002212360482869877\n",
      "Epoch 57/300\n",
      "Average training loss: 0.011273259822279214\n",
      "Average test loss: 0.002203322731889784\n",
      "Epoch 58/300\n",
      "Average training loss: 0.011245843788815869\n",
      "Average test loss: 0.002191810514984859\n",
      "Epoch 59/300\n",
      "Average training loss: 0.011223462008767658\n",
      "Average test loss: 0.0022126864581886265\n",
      "Epoch 60/300\n",
      "Average training loss: 0.011214391013814343\n",
      "Average test loss: 0.002233329065557983\n",
      "Epoch 61/300\n",
      "Average training loss: 0.011189393510007197\n",
      "Average test loss: 0.002187405749327607\n",
      "Epoch 62/300\n",
      "Average training loss: 0.011163690640694566\n",
      "Average test loss: 0.0022388353736864197\n",
      "Epoch 63/300\n",
      "Average training loss: 0.011154292375677161\n",
      "Average test loss: 0.0021885969443246724\n",
      "Epoch 64/300\n",
      "Average training loss: 0.011130935985181067\n",
      "Average test loss: 0.0022119589472810426\n",
      "Epoch 65/300\n",
      "Average training loss: 0.011118002138617966\n",
      "Average test loss: 0.0022186697338604264\n",
      "Epoch 66/300\n",
      "Average training loss: 0.011107744104332394\n",
      "Average test loss: 0.002205762648748027\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01107306245714426\n",
      "Average test loss: 0.0022118368599977757\n",
      "Epoch 68/300\n",
      "Average training loss: 0.011061065949499608\n",
      "Average test loss: 0.0022007156784335773\n",
      "Epoch 69/300\n",
      "Average training loss: 0.011053170086608992\n",
      "Average test loss: 0.002183302469862004\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01103399662176768\n",
      "Average test loss: 0.002195875949743721\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01101843287381861\n",
      "Average test loss: 0.002203669297612376\n",
      "Epoch 72/300\n",
      "Average training loss: 0.011012434540523423\n",
      "Average test loss: 0.0022389689253436195\n",
      "Epoch 73/300\n",
      "Average training loss: 0.010975042812526226\n",
      "Average test loss: 0.002260261693348487\n",
      "Epoch 74/300\n",
      "Average training loss: 0.010972780304650466\n",
      "Average test loss: 0.002191167093606459\n",
      "Epoch 75/300\n",
      "Average training loss: 0.010967931077712111\n",
      "Average test loss: 0.002193731589863698\n",
      "Epoch 76/300\n",
      "Average training loss: 0.010929710485868984\n",
      "Average test loss: 0.0022103557917806838\n",
      "Epoch 77/300\n",
      "Average training loss: 0.010924017178515594\n",
      "Average test loss: 0.0022168499374141297\n",
      "Epoch 78/300\n",
      "Average training loss: 0.010908888630568981\n",
      "Average test loss: 0.0022139387894421817\n",
      "Epoch 79/300\n",
      "Average training loss: 0.010897298753261566\n",
      "Average test loss: 0.002228985217192935\n",
      "Epoch 80/300\n",
      "Average training loss: 0.010878700705038177\n",
      "Average test loss: 0.002254956068367594\n",
      "Epoch 81/300\n",
      "Average training loss: 0.010869228610975874\n",
      "Average test loss: 0.002229224783471889\n",
      "Epoch 82/300\n",
      "Average training loss: 0.010840446889400482\n",
      "Average test loss: 0.0021991715921709934\n",
      "Epoch 83/300\n",
      "Average training loss: 0.010839109112405114\n",
      "Average test loss: 0.002280445093392498\n",
      "Epoch 84/300\n",
      "Average training loss: 0.010818879619240761\n",
      "Average test loss: 0.002227868308416671\n",
      "Epoch 85/300\n",
      "Average training loss: 0.010812983135382334\n",
      "Average test loss: 0.0022864550339678925\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01079233879264858\n",
      "Average test loss: 0.0022495286566101844\n",
      "Epoch 87/300\n",
      "Average training loss: 0.010792328314234814\n",
      "Average test loss: 0.002221163389790389\n",
      "Epoch 88/300\n",
      "Average training loss: 0.010764428084095319\n",
      "Average test loss: 0.002192731936979625\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01076487690789832\n",
      "Average test loss: 0.002246879407101207\n",
      "Epoch 90/300\n",
      "Average training loss: 0.010751169990334247\n",
      "Average test loss: 0.002244996242639091\n",
      "Epoch 91/300\n",
      "Average training loss: 0.010732083786692884\n",
      "Average test loss: 0.00224718997006615\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01072124400238196\n",
      "Average test loss: 0.002210809489712119\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01071370285211338\n",
      "Average test loss: 0.002246113230370813\n",
      "Epoch 94/300\n",
      "Average training loss: 0.010692928069995509\n",
      "Average test loss: 0.002217403149232268\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01070077872607443\n",
      "Average test loss: 0.0022715325167195666\n",
      "Epoch 96/300\n",
      "Average training loss: 0.010672763363354735\n",
      "Average test loss: 0.0022134672335038584\n",
      "Epoch 97/300\n",
      "Average training loss: 0.010661442825363743\n",
      "Average test loss: 0.002258088858384225\n",
      "Epoch 98/300\n",
      "Average training loss: 0.010649374447762967\n",
      "Average test loss: 0.0022845121189537976\n",
      "Epoch 99/300\n",
      "Average training loss: 0.010645325031545427\n",
      "Average test loss: 0.0022172191652158895\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01062658481631014\n",
      "Average test loss: 0.002228416615890132\n",
      "Epoch 101/300\n",
      "Average training loss: 0.010616463551090824\n",
      "Average test loss: 0.0022499292727766766\n",
      "Epoch 102/300\n",
      "Average training loss: 0.010598750319745805\n",
      "Average test loss: 0.002255947451935046\n",
      "Epoch 103/300\n",
      "Average training loss: 0.01059941135181321\n",
      "Average test loss: 0.0022265187916863296\n",
      "Epoch 104/300\n",
      "Average training loss: 0.010585472467872831\n",
      "Average test loss: 0.0022009536172780727\n",
      "Epoch 105/300\n",
      "Average training loss: 0.010587276495993137\n",
      "Average test loss: 0.002273931822636061\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01056733334561189\n",
      "Average test loss: 0.0022760648601171042\n",
      "Epoch 107/300\n",
      "Average training loss: 0.010561678181091945\n",
      "Average test loss: 0.0022414778595169385\n",
      "Epoch 108/300\n",
      "Average training loss: 0.0105442799727122\n",
      "Average test loss: 0.002272888900919093\n",
      "Epoch 109/300\n",
      "Average training loss: 0.010532541708813773\n",
      "Average test loss: 0.0022260697583357495\n",
      "Epoch 110/300\n",
      "Average training loss: 0.010520188106430902\n",
      "Average test loss: 0.002228486753896707\n",
      "Epoch 111/300\n",
      "Average training loss: 0.010509919948875904\n",
      "Average test loss: 0.0022490052146216235\n",
      "Epoch 112/300\n",
      "Average training loss: 0.010512915410101414\n",
      "Average test loss: 0.0022714882029427424\n",
      "Epoch 113/300\n",
      "Average training loss: 0.010486045787731806\n",
      "Average test loss: 0.002296855883258912\n",
      "Epoch 114/300\n",
      "Average training loss: 0.010496730779608091\n",
      "Average test loss: 0.002292609997300638\n",
      "Epoch 115/300\n",
      "Average training loss: 0.010464059203035302\n",
      "Average test loss: 0.0022685522216682632\n",
      "Epoch 116/300\n",
      "Average training loss: 0.010470834805733627\n",
      "Average test loss: 0.002245030908121003\n",
      "Epoch 117/300\n",
      "Average training loss: 0.010458644913302527\n",
      "Average test loss: 0.002270759061496291\n",
      "Epoch 118/300\n",
      "Average training loss: 0.010466487922602229\n",
      "Average test loss: 0.0022584541416209606\n",
      "Epoch 119/300\n",
      "Average training loss: 0.010457503826253944\n",
      "Average test loss: 0.00225368672878378\n",
      "Epoch 120/300\n",
      "Average training loss: 0.010434685131741894\n",
      "Average test loss: 0.0022835054480367235\n",
      "Epoch 121/300\n",
      "Average training loss: 0.010425070308976703\n",
      "Average test loss: 0.0022899230470259982\n",
      "Epoch 122/300\n",
      "Average training loss: 0.010403577036741708\n",
      "Average test loss: 0.0023477841942674584\n",
      "Epoch 123/300\n",
      "Average training loss: 0.010402416179991431\n",
      "Average test loss: 0.0022937094562480017\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01039812361697356\n",
      "Average test loss: 0.002219759474818905\n",
      "Epoch 125/300\n",
      "Average training loss: 0.010389497468868891\n",
      "Average test loss: 0.0022264285951645838\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01037209034131633\n",
      "Average test loss: 0.002227250041440129\n",
      "Epoch 127/300\n",
      "Average training loss: 0.010378227598137326\n",
      "Average test loss: 0.002267727308182253\n",
      "Epoch 128/300\n",
      "Average training loss: 0.01037794642150402\n",
      "Average test loss: 0.002301908737462428\n",
      "Epoch 129/300\n",
      "Average training loss: 0.010364466619160441\n",
      "Average test loss: 0.0022983887733684645\n",
      "Epoch 130/300\n",
      "Average training loss: 0.010342652542309629\n",
      "Average test loss: 0.00235590261945294\n",
      "Epoch 131/300\n",
      "Average training loss: 0.010328886639740732\n",
      "Average test loss: 0.002323716268564264\n",
      "Epoch 132/300\n",
      "Average training loss: 0.010327043760567903\n",
      "Average test loss: 0.002276367314573791\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0103251441485352\n",
      "Average test loss: 0.0023728689578258327\n",
      "Epoch 134/300\n",
      "Average training loss: 0.010322363125781218\n",
      "Average test loss: 0.002262676695154773\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01031350980202357\n",
      "Average test loss: 0.0023708703389598265\n",
      "Epoch 136/300\n",
      "Average training loss: 0.010304802332487371\n",
      "Average test loss: 0.002483742637766732\n",
      "Epoch 137/300\n",
      "Average training loss: 0.010289170022639963\n",
      "Average test loss: 0.0022409967409653796\n",
      "Epoch 138/300\n",
      "Average training loss: 0.010291178761257066\n",
      "Average test loss: 0.002288736776345306\n",
      "Epoch 139/300\n",
      "Average training loss: 0.010279048291345438\n",
      "Average test loss: 0.002330086150724027\n",
      "Epoch 140/300\n",
      "Average training loss: 0.010282199499921667\n",
      "Average test loss: 0.0023149532992392777\n",
      "Epoch 141/300\n",
      "Average training loss: 0.010261220042076375\n",
      "Average test loss: 0.0022469667848199607\n",
      "Epoch 142/300\n",
      "Average training loss: 0.010259126013351811\n",
      "Average test loss: 0.0022708712031857836\n",
      "Epoch 143/300\n",
      "Average training loss: 0.010252517106632391\n",
      "Average test loss: 0.002296173364544908\n",
      "Epoch 144/300\n",
      "Average training loss: 0.010251605917182233\n",
      "Average test loss: 0.0022622995124095017\n",
      "Epoch 145/300\n",
      "Average training loss: 0.010248323967059454\n",
      "Average test loss: 0.002248009838991695\n",
      "Epoch 146/300\n",
      "Average training loss: 0.010241401698440313\n",
      "Average test loss: 0.002344495861066712\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01022072882950306\n",
      "Average test loss: 0.002290767011129194\n",
      "Epoch 148/300\n",
      "Average training loss: 0.010210255677915281\n",
      "Average test loss: 0.002317002431386047\n",
      "Epoch 149/300\n",
      "Average training loss: 0.010213627287083202\n",
      "Average test loss: 0.0023024151799165542\n",
      "Epoch 150/300\n",
      "Average training loss: 0.010222615537544092\n",
      "Average test loss: 0.0023257346594085294\n",
      "Epoch 151/300\n",
      "Average training loss: 0.010195315877182616\n",
      "Average test loss: 0.002299970681572126\n",
      "Epoch 152/300\n",
      "Average training loss: 0.010193043077157604\n",
      "Average test loss: 0.0022642804755725796\n",
      "Epoch 153/300\n",
      "Average training loss: 0.010210602738791041\n",
      "Average test loss: 0.0023328502596252493\n",
      "Epoch 154/300\n",
      "Average training loss: 0.010183525619407495\n",
      "Average test loss: 0.0023381040218389696\n",
      "Epoch 155/300\n",
      "Average training loss: 0.010167391564283106\n",
      "Average test loss: 0.0022885356748269664\n",
      "Epoch 156/300\n",
      "Average training loss: 0.010184082354108493\n",
      "Average test loss: 0.002287294805774258\n",
      "Epoch 157/300\n",
      "Average training loss: 0.010165018391278055\n",
      "Average test loss: 0.002273062714479036\n",
      "Epoch 158/300\n",
      "Average training loss: 0.010156499620113108\n",
      "Average test loss: 0.0022863842925677697\n",
      "Epoch 159/300\n",
      "Average training loss: 0.010156916667189863\n",
      "Average test loss: 0.0022727105845179825\n",
      "Epoch 160/300\n",
      "Average training loss: 0.010145716234213776\n",
      "Average test loss: 0.0023493822568820583\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01014889658573601\n",
      "Average test loss: 0.002363577276468277\n",
      "Epoch 162/300\n",
      "Average training loss: 0.010134611757265197\n",
      "Average test loss: 0.0022790512593670025\n",
      "Epoch 163/300\n",
      "Average training loss: 0.010135349001321528\n",
      "Average test loss: 0.0023737722034048703\n",
      "Epoch 164/300\n",
      "Average training loss: 0.010115907605323526\n",
      "Average test loss: 0.0022976728967494435\n",
      "Epoch 165/300\n",
      "Average training loss: 0.010136589521335232\n",
      "Average test loss: 0.0022927435180172323\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01012423598849111\n",
      "Average test loss: 0.0023071491898347934\n",
      "Epoch 167/300\n",
      "Average training loss: 0.010106833333770434\n",
      "Average test loss: 0.0023083398224165042\n",
      "Epoch 168/300\n",
      "Average training loss: 0.010096528270178371\n",
      "Average test loss: 0.0024281309532622497\n",
      "Epoch 169/300\n",
      "Average training loss: 0.010108807594411903\n",
      "Average test loss: 0.0023259704315827952\n",
      "Epoch 170/300\n",
      "Average training loss: 0.010088292782505353\n",
      "Average test loss: 0.0025012065873791776\n",
      "Epoch 171/300\n",
      "Average training loss: 0.010092302676704195\n",
      "Average test loss: 0.002291242361896568\n",
      "Epoch 172/300\n",
      "Average training loss: 0.010079507316152255\n",
      "Average test loss: 0.0023372731405413814\n",
      "Epoch 173/300\n",
      "Average training loss: 0.010077205907139513\n",
      "Average test loss: 0.002321447325870395\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01008476539949576\n",
      "Average test loss: 0.00232383972644392\n",
      "Epoch 175/300\n",
      "Average training loss: 0.010061577470766173\n",
      "Average test loss: 0.0023979955541176927\n",
      "Epoch 176/300\n",
      "Average training loss: 0.01006487892402543\n",
      "Average test loss: 0.0023003215366560552\n",
      "Epoch 177/300\n",
      "Average training loss: 0.010054928835067483\n",
      "Average test loss: 0.0023033159634926253\n",
      "Epoch 178/300\n",
      "Average training loss: 0.010048532590270042\n",
      "Average test loss: 0.00231477674924665\n",
      "Epoch 179/300\n",
      "Average training loss: 0.010055131423804495\n",
      "Average test loss: 0.0023255163066916995\n",
      "Epoch 180/300\n",
      "Average training loss: 0.010033929139375687\n",
      "Average test loss: 0.0023193227710823217\n",
      "Epoch 181/300\n",
      "Average training loss: 0.010050335061219003\n",
      "Average test loss: 0.0024409156433410116\n",
      "Epoch 182/300\n",
      "Average training loss: 0.010031674502624406\n",
      "Average test loss: 0.002360629908326599\n",
      "Epoch 183/300\n",
      "Average training loss: 0.010019064024090767\n",
      "Average test loss: 0.00224245339528554\n",
      "Epoch 184/300\n",
      "Average training loss: 0.010033868202732669\n",
      "Average test loss: 0.002327087433180875\n",
      "Epoch 185/300\n",
      "Average training loss: 0.010020523982743423\n",
      "Average test loss: 0.0023223928345574275\n",
      "Epoch 186/300\n",
      "Average training loss: 0.010007339392271306\n",
      "Average test loss: 0.0022892168118721907\n",
      "Epoch 187/300\n",
      "Average training loss: 0.010016704612308079\n",
      "Average test loss: 0.0022978848504523436\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01000837921185626\n",
      "Average test loss: 0.0023190226326179173\n",
      "Epoch 189/300\n",
      "Average training loss: 0.00998969200750192\n",
      "Average test loss: 0.002375800906163123\n",
      "Epoch 190/300\n",
      "Average training loss: 0.009999306278096304\n",
      "Average test loss: 0.0023253189898613425\n",
      "Epoch 191/300\n",
      "Average training loss: 0.009995549058748616\n",
      "Average test loss: 0.0023613927697555886\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0099950271083249\n",
      "Average test loss: 0.002303156678357886\n",
      "Epoch 193/300\n",
      "Average training loss: 0.009978230915135807\n",
      "Average test loss: 0.0022966090289668903\n",
      "Epoch 194/300\n",
      "Average training loss: 0.009971367286311256\n",
      "Average test loss: 0.002327776273505555\n",
      "Epoch 195/300\n",
      "Average training loss: 0.009985983615948095\n",
      "Average test loss: 0.0022721079039490885\n",
      "Epoch 196/300\n",
      "Average training loss: 0.009964202959504393\n",
      "Average test loss: 0.0023155701719224453\n",
      "Epoch 197/300\n",
      "Average training loss: 0.00994891371205449\n",
      "Average test loss: 0.0023185135672489803\n",
      "Epoch 198/300\n",
      "Average training loss: 0.009963416825979949\n",
      "Average test loss: 0.002343432354844279\n",
      "Epoch 199/300\n",
      "Average training loss: 0.009964385245409277\n",
      "Average test loss: 0.0027873863016979563\n",
      "Epoch 200/300\n",
      "Average training loss: 0.009956433514753978\n",
      "Average test loss: 0.0023520826692175535\n",
      "Epoch 201/300\n",
      "Average training loss: 0.009946179101450577\n",
      "Average test loss: 0.0023524050580130684\n",
      "Epoch 202/300\n",
      "Average training loss: 0.00994217616568009\n",
      "Average test loss: 0.0023792579118162393\n",
      "Epoch 203/300\n",
      "Average training loss: 0.00995461434374253\n",
      "Average test loss: 0.0023341975710872145\n",
      "Epoch 204/300\n",
      "Average training loss: 0.009940254750351112\n",
      "Average test loss: 0.0023843111362722184\n",
      "Epoch 205/300\n",
      "Average training loss: 0.009951006797452767\n",
      "Average test loss: 0.0022958050629951886\n",
      "Epoch 206/300\n",
      "Average training loss: 0.009920270981887976\n",
      "Average test loss: 0.0023242044027687774\n",
      "Epoch 207/300\n",
      "Average training loss: 0.009938082227276431\n",
      "Average test loss: 0.0023375397566705943\n",
      "Epoch 208/300\n",
      "Average training loss: 0.009927477057609293\n",
      "Average test loss: 0.002350237862310476\n",
      "Epoch 209/300\n",
      "Average training loss: 0.009917422487503953\n",
      "Average test loss: 0.0023136417836778695\n",
      "Epoch 210/300\n",
      "Average training loss: 0.009923738824824491\n",
      "Average test loss: 0.0023304815758019687\n",
      "Epoch 211/300\n",
      "Average training loss: 0.00991595520120528\n",
      "Average test loss: 0.0023911854666140344\n",
      "Epoch 212/300\n",
      "Average training loss: 0.009905675912896792\n",
      "Average test loss: 0.002331887281396323\n",
      "Epoch 213/300\n",
      "Average training loss: 0.009905364242692788\n",
      "Average test loss: 0.0024389592349115343\n",
      "Epoch 214/300\n",
      "Average training loss: 0.009904126848611567\n",
      "Average test loss: 0.0024087667144421076\n",
      "Epoch 215/300\n",
      "Average training loss: 0.009893555769489871\n",
      "Average test loss: 0.0023225444282094637\n",
      "Epoch 216/300\n",
      "Average training loss: 0.009891051176521512\n",
      "Average test loss: 0.0023562354791081615\n",
      "Epoch 217/300\n",
      "Average training loss: 0.009896550577547816\n",
      "Average test loss: 0.002355558436881337\n",
      "Epoch 218/300\n",
      "Average training loss: 0.009883627014855545\n",
      "Average test loss: 0.0024003922864794733\n",
      "Epoch 219/300\n",
      "Average training loss: 0.009889261037111283\n",
      "Average test loss: 0.0023503179119692907\n",
      "Epoch 220/300\n",
      "Average training loss: 0.009888331886794832\n",
      "Average test loss: 0.0023023810421841013\n",
      "Epoch 221/300\n",
      "Average training loss: 0.009883969864083662\n",
      "Average test loss: 0.002391889405540294\n",
      "Epoch 222/300\n",
      "Average training loss: 0.009871746106694142\n",
      "Average test loss: 0.002428235803316865\n",
      "Epoch 223/300\n",
      "Average training loss: 0.009875981495612198\n",
      "Average test loss: 0.002321819360885355\n",
      "Epoch 224/300\n",
      "Average training loss: 0.009872673161327838\n",
      "Average test loss: 0.0023990191210889154\n",
      "Epoch 225/300\n",
      "Average training loss: 0.009859909124672413\n",
      "Average test loss: 0.0023804544198016324\n",
      "Epoch 226/300\n",
      "Average training loss: 0.009870448087652525\n",
      "Average test loss: 0.002380204456133975\n",
      "Epoch 227/300\n",
      "Average training loss: 0.009873397241036096\n",
      "Average test loss: 0.0023405912649921245\n",
      "Epoch 228/300\n",
      "Average training loss: 0.009860730316489935\n",
      "Average test loss: 0.0024026208085318407\n",
      "Epoch 229/300\n",
      "Average training loss: 0.009856605156842205\n",
      "Average test loss: 0.0024746138414161074\n",
      "Epoch 230/300\n",
      "Average training loss: 0.009846244387328625\n",
      "Average test loss: 0.0023246770132746963\n",
      "Epoch 231/300\n",
      "Average training loss: 0.009852422561082576\n",
      "Average test loss: 0.002347183292938603\n",
      "Epoch 232/300\n",
      "Average training loss: 0.009835409810145695\n",
      "Average test loss: 0.0022916344420777428\n",
      "Epoch 233/300\n",
      "Average training loss: 0.009847132437345054\n",
      "Average test loss: 0.0024469575490802527\n",
      "Epoch 234/300\n",
      "Average training loss: 0.009830541215837001\n",
      "Average test loss: 0.002402351275086403\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0098404880033599\n",
      "Average test loss: 0.0023653637679914635\n",
      "Epoch 236/300\n",
      "Average training loss: 0.009823406180573833\n",
      "Average test loss: 0.0023514161670787465\n",
      "Epoch 237/300\n",
      "Average training loss: 0.00982126959744427\n",
      "Average test loss: 0.002426062238092224\n",
      "Epoch 238/300\n",
      "Average training loss: 0.009825035502513249\n",
      "Average test loss: 0.00242939448936118\n",
      "Epoch 239/300\n",
      "Average training loss: 0.009827816867993937\n",
      "Average test loss: 0.0023847193756244248\n",
      "Epoch 240/300\n",
      "Average training loss: 0.009813964414099853\n",
      "Average test loss: 0.002410158012579713\n",
      "Epoch 241/300\n",
      "Average training loss: 0.009814039530025588\n",
      "Average test loss: 0.0023069552061044506\n",
      "Epoch 242/300\n",
      "Average training loss: 0.009821813020441267\n",
      "Average test loss: 0.0023733268831339148\n",
      "Epoch 243/300\n",
      "Average training loss: 0.009816747104128202\n",
      "Average test loss: 0.002398033469294508\n",
      "Epoch 244/300\n",
      "Average training loss: 0.00980304812350207\n",
      "Average test loss: 0.0024194921443445815\n",
      "Epoch 245/300\n",
      "Average training loss: 0.00981570605850882\n",
      "Average test loss: 0.0023722065402608777\n",
      "Epoch 246/300\n",
      "Average training loss: 0.00979991098286377\n",
      "Average test loss: 0.0023880041792160933\n",
      "Epoch 247/300\n",
      "Average training loss: 0.00979813947280248\n",
      "Average test loss: 0.0025203793215461904\n",
      "Epoch 248/300\n",
      "Average training loss: 0.009786688610497448\n",
      "Average test loss: 0.0023863334715780284\n",
      "Epoch 249/300\n",
      "Average training loss: 0.009792397691971726\n",
      "Average test loss: 0.002316034061834216\n",
      "Epoch 250/300\n",
      "Average training loss: 0.009795105343891514\n",
      "Average test loss: 0.0023970550389753446\n",
      "Epoch 251/300\n",
      "Average training loss: 0.009796189429031477\n",
      "Average test loss: 0.0023018394814183314\n",
      "Epoch 252/300\n",
      "Average training loss: 0.009785393957462576\n",
      "Average test loss: 0.002369252387434244\n",
      "Epoch 253/300\n",
      "Average training loss: 0.009784554379681747\n",
      "Average test loss: 0.0023371487606523764\n",
      "Epoch 254/300\n",
      "Average training loss: 0.009786084106398953\n",
      "Average test loss: 0.002372196506087979\n",
      "Epoch 255/300\n",
      "Average training loss: 0.009786193533904022\n",
      "Average test loss: 0.0023508049454540013\n",
      "Epoch 256/300\n",
      "Average training loss: 0.009780940311236514\n",
      "Average test loss: 0.00233840348592235\n",
      "Epoch 257/300\n",
      "Average training loss: 0.009767891479035219\n",
      "Average test loss: 0.002310215887096193\n",
      "Epoch 258/300\n",
      "Average training loss: 0.009770099830296305\n",
      "Average test loss: 0.0023632745372338428\n",
      "Epoch 259/300\n",
      "Average training loss: 0.009763834108908971\n",
      "Average test loss: 0.0023208356864957346\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00976687073128091\n",
      "Average test loss: 0.002353762836299009\n",
      "Epoch 261/300\n",
      "Average training loss: 0.009764411118295457\n",
      "Average test loss: 0.002371054634451866\n",
      "Epoch 262/300\n",
      "Average training loss: 0.009762502999769317\n",
      "Average test loss: 0.002396387608204451\n",
      "Epoch 263/300\n",
      "Average training loss: 0.009752351202898555\n",
      "Average test loss: 0.002354460915964511\n",
      "Epoch 264/300\n",
      "Average training loss: 0.00976733278731505\n",
      "Average test loss: 0.0023836990739736293\n",
      "Epoch 265/300\n",
      "Average training loss: 0.009749934614532524\n",
      "Average test loss: 0.002351283739424414\n",
      "Epoch 266/300\n",
      "Average training loss: 0.009749057311978605\n",
      "Average test loss: 0.0023584650854269664\n",
      "Epoch 267/300\n",
      "Average training loss: 0.009752242707957824\n",
      "Average test loss: 0.0024037060163294276\n",
      "Epoch 268/300\n",
      "Average training loss: 0.009739463223351372\n",
      "Average test loss: 0.002341223904449079\n",
      "Epoch 269/300\n",
      "Average training loss: 0.009747661908467611\n",
      "Average test loss: 0.002460689989022083\n",
      "Epoch 270/300\n",
      "Average training loss: 0.009744550112220977\n",
      "Average test loss: 0.002367281359206471\n",
      "Epoch 271/300\n",
      "Average training loss: 0.009739462229112783\n",
      "Average test loss: 0.0024051488590323264\n",
      "Epoch 272/300\n",
      "Average training loss: 0.009728267369998825\n",
      "Average test loss: 0.0023752529189611476\n",
      "Epoch 273/300\n",
      "Average training loss: 0.009728215196894275\n",
      "Average test loss: 0.0023957403858916627\n",
      "Epoch 274/300\n",
      "Average training loss: 0.009734186373651027\n",
      "Average test loss: 0.00246323688638707\n",
      "Epoch 275/300\n",
      "Average training loss: 0.00973330360568232\n",
      "Average test loss: 0.0023365981730942924\n",
      "Epoch 276/300\n",
      "Average training loss: 0.009728985650671852\n",
      "Average test loss: 0.0023912510758058894\n",
      "Epoch 277/300\n",
      "Average training loss: 0.009714874611960517\n",
      "Average test loss: 0.0024120613522827625\n",
      "Epoch 278/300\n",
      "Average training loss: 0.009731398628817665\n",
      "Average test loss: 0.0024565420446710454\n",
      "Epoch 279/300\n",
      "Average training loss: 0.00972092322881023\n",
      "Average test loss: 0.0024261429260174433\n",
      "Epoch 280/300\n",
      "Average training loss: 0.009711912736296653\n",
      "Average test loss: 0.0023408535127010612\n",
      "Epoch 281/300\n",
      "Average training loss: 0.009709820660452048\n",
      "Average test loss: 0.0024235193812184863\n",
      "Epoch 282/300\n",
      "Average training loss: 0.009722923618223932\n",
      "Average test loss: 0.0024354437397172053\n",
      "Epoch 283/300\n",
      "Average training loss: 0.009702375791966915\n",
      "Average test loss: 0.0024514982988023096\n",
      "Epoch 284/300\n",
      "Average training loss: 0.00970897933592399\n",
      "Average test loss: 0.0023358165674532455\n",
      "Epoch 285/300\n",
      "Average training loss: 0.009706828986605008\n",
      "Average test loss: 0.002351505644619465\n",
      "Epoch 286/300\n",
      "Average training loss: 0.009705376322070757\n",
      "Average test loss: 0.002408346803651916\n",
      "Epoch 287/300\n",
      "Average training loss: 0.009700841040247016\n",
      "Average test loss: 0.0023772103709893094\n",
      "Epoch 288/300\n",
      "Average training loss: 0.009693655245420006\n",
      "Average test loss: 0.002370559576484892\n",
      "Epoch 289/300\n",
      "Average training loss: 0.009704969737264845\n",
      "Average test loss: 0.0023996678054746656\n",
      "Epoch 290/300\n",
      "Average training loss: 0.009693306951887078\n",
      "Average test loss: 0.0023687319989419645\n",
      "Epoch 291/300\n",
      "Average training loss: 0.009695973712537024\n",
      "Average test loss: 0.0023879540966202815\n",
      "Epoch 292/300\n",
      "Average training loss: 0.009681913900706503\n",
      "Average test loss: 0.002386666422709823\n",
      "Epoch 293/300\n",
      "Average training loss: 0.009681234566701783\n",
      "Average test loss: 0.002403946709922618\n",
      "Epoch 294/300\n",
      "Average training loss: 0.009685273956093523\n",
      "Average test loss: 0.002493505751093229\n",
      "Epoch 295/300\n",
      "Average training loss: 0.00968123297020793\n",
      "Average test loss: 0.0024337653855068817\n",
      "Epoch 296/300\n",
      "Average training loss: 0.009689489141934448\n",
      "Average test loss: 0.0024293216003312005\n",
      "Epoch 297/300\n",
      "Average training loss: 0.00968575910727183\n",
      "Average test loss: 0.002893194496217701\n",
      "Epoch 298/300\n",
      "Average training loss: 0.009683273389935493\n",
      "Average test loss: 0.002325757717817194\n",
      "Epoch 299/300\n",
      "Average training loss: 0.009669013085464636\n",
      "Average test loss: 0.0023808279641800456\n",
      "Epoch 300/300\n",
      "Average training loss: 0.009670336563140154\n",
      "Average test loss: 0.0023704711604449484\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.045772113733821444\n",
      "Average test loss: 0.003349629493844178\n",
      "Epoch 2/300\n",
      "Average training loss: 0.01492613077743186\n",
      "Average test loss: 0.0028001910323897997\n",
      "Epoch 3/300\n",
      "Average training loss: 0.01374457022216585\n",
      "Average test loss: 0.002738406352802283\n",
      "Epoch 4/300\n",
      "Average training loss: 0.013052738488548331\n",
      "Average test loss: 0.0024993974771350624\n",
      "Epoch 5/300\n",
      "Average training loss: 0.012532747027774653\n",
      "Average test loss: 0.002331404970958829\n",
      "Epoch 6/300\n",
      "Average training loss: 0.012090556516415543\n",
      "Average test loss: 0.0022557404769791498\n",
      "Epoch 7/300\n",
      "Average training loss: 0.011737993410064115\n",
      "Average test loss: 0.0021813372380824554\n",
      "Epoch 8/300\n",
      "Average training loss: 0.011400072485208511\n",
      "Average test loss: 0.002129981377058559\n",
      "Epoch 9/300\n",
      "Average training loss: 0.011118371640642484\n",
      "Average test loss: 0.002057648330512974\n",
      "Epoch 10/300\n",
      "Average training loss: 0.010861820095115238\n",
      "Average test loss: 0.001996822541579604\n",
      "Epoch 11/300\n",
      "Average training loss: 0.010649413929217392\n",
      "Average test loss: 0.0019424582540781962\n",
      "Epoch 12/300\n",
      "Average training loss: 0.010452881500124932\n",
      "Average test loss: 0.0019795996786819564\n",
      "Epoch 13/300\n",
      "Average training loss: 0.010278183380762735\n",
      "Average test loss: 0.0018685964805384477\n",
      "Epoch 14/300\n",
      "Average training loss: 0.010133760005235672\n",
      "Average test loss: 0.001863709327661329\n",
      "Epoch 15/300\n",
      "Average training loss: 0.010007815199593702\n",
      "Average test loss: 0.0018339063674211503\n",
      "Epoch 16/300\n",
      "Average training loss: 0.009876487471991114\n",
      "Average test loss: 0.0017785865090166528\n",
      "Epoch 17/300\n",
      "Average training loss: 0.009781801063981322\n",
      "Average test loss: 0.001778750499089559\n",
      "Epoch 18/300\n",
      "Average training loss: 0.00968016068968508\n",
      "Average test loss: 0.0017479254287771052\n",
      "Epoch 19/300\n",
      "Average training loss: 0.009597944684326648\n",
      "Average test loss: 0.0017555623264569374\n",
      "Epoch 20/300\n",
      "Average training loss: 0.009521160640650325\n",
      "Average test loss: 0.0017067597691590587\n",
      "Epoch 21/300\n",
      "Average training loss: 0.009445153240528371\n",
      "Average test loss: 0.001701486853365269\n",
      "Epoch 22/300\n",
      "Average training loss: 0.009384015983177556\n",
      "Average test loss: 0.0017032416096578041\n",
      "Epoch 23/300\n",
      "Average training loss: 0.009321440173519982\n",
      "Average test loss: 0.0016762457084324625\n",
      "Epoch 24/300\n",
      "Average training loss: 0.009267562209732003\n",
      "Average test loss: 0.001701039482322004\n",
      "Epoch 25/300\n",
      "Average training loss: 0.00921399940467543\n",
      "Average test loss: 0.0016763156547935473\n",
      "Epoch 26/300\n",
      "Average training loss: 0.009170994949009683\n",
      "Average test loss: 0.001652451825224691\n",
      "Epoch 27/300\n",
      "Average training loss: 0.009120541884667344\n",
      "Average test loss: 0.0016440707063302398\n",
      "Epoch 28/300\n",
      "Average training loss: 0.009080875132646826\n",
      "Average test loss: 0.0016633410172330009\n",
      "Epoch 29/300\n",
      "Average training loss: 0.009040453632258706\n",
      "Average test loss: 0.0016304357283645206\n",
      "Epoch 30/300\n",
      "Average training loss: 0.009008810816539658\n",
      "Average test loss: 0.001618515466650327\n",
      "Epoch 31/300\n",
      "Average training loss: 0.008968211443059974\n",
      "Average test loss: 0.0015941661211351555\n",
      "Epoch 32/300\n",
      "Average training loss: 0.008935990571975708\n",
      "Average test loss: 0.0016104091046791938\n",
      "Epoch 33/300\n",
      "Average training loss: 0.008911436291204559\n",
      "Average test loss: 0.0016057187498857578\n",
      "Epoch 34/300\n",
      "Average training loss: 0.008889646443227928\n",
      "Average test loss: 0.001609340789831347\n",
      "Epoch 35/300\n",
      "Average training loss: 0.008852180939167738\n",
      "Average test loss: 0.0016468366988831096\n",
      "Epoch 36/300\n",
      "Average training loss: 0.008816490369538466\n",
      "Average test loss: 0.0015890535844696892\n",
      "Epoch 37/300\n",
      "Average training loss: 0.00879443807610207\n",
      "Average test loss: 0.0015917816762295034\n",
      "Epoch 38/300\n",
      "Average training loss: 0.008773218740191725\n",
      "Average test loss: 0.001568383067432377\n",
      "Epoch 39/300\n",
      "Average training loss: 0.008742999521394571\n",
      "Average test loss: 0.001583311379990644\n",
      "Epoch 40/300\n",
      "Average training loss: 0.008728664303819338\n",
      "Average test loss: 0.0015765070786906614\n",
      "Epoch 41/300\n",
      "Average training loss: 0.008704511166032818\n",
      "Average test loss: 0.0015633618263527751\n",
      "Epoch 42/300\n",
      "Average training loss: 0.008671211644179291\n",
      "Average test loss: 0.0015790875758975745\n",
      "Epoch 43/300\n",
      "Average training loss: 0.008663451592541404\n",
      "Average test loss: 0.0015660649348671239\n",
      "Epoch 44/300\n",
      "Average training loss: 0.008635620160235299\n",
      "Average test loss: 0.0015604189528773229\n",
      "Epoch 45/300\n",
      "Average training loss: 0.008618705208516783\n",
      "Average test loss: 0.001576373312001427\n",
      "Epoch 46/300\n",
      "Average training loss: 0.00860678694314427\n",
      "Average test loss: 0.0015894122185806433\n",
      "Epoch 47/300\n",
      "Average training loss: 0.008581148212982549\n",
      "Average test loss: 0.0015637386985537078\n",
      "Epoch 48/300\n",
      "Average training loss: 0.00856139895816644\n",
      "Average test loss: 0.0015547449626028538\n",
      "Epoch 49/300\n",
      "Average training loss: 0.008546558977001243\n",
      "Average test loss: 0.001562754018128746\n",
      "Epoch 50/300\n",
      "Average training loss: 0.008535895914253262\n",
      "Average test loss: 0.001557029917422268\n",
      "Epoch 51/300\n",
      "Average training loss: 0.008514020199577014\n",
      "Average test loss: 0.0015773971776167552\n",
      "Epoch 52/300\n",
      "Average training loss: 0.008493990882817241\n",
      "Average test loss: 0.0015712130133890443\n",
      "Epoch 53/300\n",
      "Average training loss: 0.008478752094424433\n",
      "Average test loss: 0.001543076288058526\n",
      "Epoch 54/300\n",
      "Average training loss: 0.00845012001776033\n",
      "Average test loss: 0.0016731810642199383\n",
      "Epoch 55/300\n",
      "Average training loss: 0.008446531080537372\n",
      "Average test loss: 0.0016082713943388726\n",
      "Epoch 56/300\n",
      "Average training loss: 0.008431305178337628\n",
      "Average test loss: 0.0015690790601074695\n",
      "Epoch 57/300\n",
      "Average training loss: 0.00842014001434048\n",
      "Average test loss: 0.0015487337378888495\n",
      "Epoch 58/300\n",
      "Average training loss: 0.00839077503234148\n",
      "Average test loss: 0.0015646444933695926\n",
      "Epoch 59/300\n",
      "Average training loss: 0.008388070323814948\n",
      "Average test loss: 0.0015564668387588528\n",
      "Epoch 60/300\n",
      "Average training loss: 0.008382473070795338\n",
      "Average test loss: 0.0015949517538150152\n",
      "Epoch 61/300\n",
      "Average training loss: 0.008352878247284227\n",
      "Average test loss: 0.0015645383952392472\n",
      "Epoch 62/300\n",
      "Average training loss: 0.008340810222758187\n",
      "Average test loss: 0.0016771629839721653\n",
      "Epoch 63/300\n",
      "Average training loss: 0.008331064742058516\n",
      "Average test loss: 0.0015430460452723006\n",
      "Epoch 64/300\n",
      "Average training loss: 0.008311221549080478\n",
      "Average test loss: 0.00156934943381283\n",
      "Epoch 65/300\n",
      "Average training loss: 0.008294400525589784\n",
      "Average test loss: 0.001546260603910519\n",
      "Epoch 66/300\n",
      "Average training loss: 0.008287592579921087\n",
      "Average test loss: 0.0015421302187153035\n",
      "Epoch 67/300\n",
      "Average training loss: 0.008271434939983818\n",
      "Average test loss: 0.0015690948294682634\n",
      "Epoch 68/300\n",
      "Average training loss: 0.008263632367468543\n",
      "Average test loss: 0.0015509168782995807\n",
      "Epoch 69/300\n",
      "Average training loss: 0.008248062664849891\n",
      "Average test loss: 0.001560095745863186\n",
      "Epoch 70/300\n",
      "Average training loss: 0.00823830815570222\n",
      "Average test loss: 0.001549993045731551\n",
      "Epoch 71/300\n",
      "Average training loss: 0.008222785954674084\n",
      "Average test loss: 0.0015496031608846453\n",
      "Epoch 72/300\n",
      "Average training loss: 0.008216549126638306\n",
      "Average test loss: 0.0015733975526980228\n",
      "Epoch 73/300\n",
      "Average training loss: 0.008198010145790047\n",
      "Average test loss: 0.001541276279785153\n",
      "Epoch 74/300\n",
      "Average training loss: 0.008190207687103086\n",
      "Average test loss: 0.0016228619729065233\n",
      "Epoch 75/300\n",
      "Average training loss: 0.008178006476412217\n",
      "Average test loss: 0.0015851190752453274\n",
      "Epoch 76/300\n",
      "Average training loss: 0.008172252114034361\n",
      "Average test loss: 0.0015653641798223058\n",
      "Epoch 77/300\n",
      "Average training loss: 0.008167626510063808\n",
      "Average test loss: 0.0015669271957336201\n",
      "Epoch 78/300\n",
      "Average training loss: 0.008152377954787678\n",
      "Average test loss: 0.0015545423922853337\n",
      "Epoch 79/300\n",
      "Average training loss: 0.008133812175442776\n",
      "Average test loss: 0.0015644630021933053\n",
      "Epoch 80/300\n",
      "Average training loss: 0.008123910844326019\n",
      "Average test loss: 0.001609817879481448\n",
      "Epoch 81/300\n",
      "Average training loss: 0.008108383973439534\n",
      "Average test loss: 0.0016037434852785534\n",
      "Epoch 82/300\n",
      "Average training loss: 0.008099589246842596\n",
      "Average test loss: 0.0015490716635766957\n",
      "Epoch 83/300\n",
      "Average training loss: 0.008108301351881689\n",
      "Average test loss: 0.0015841407823479839\n",
      "Epoch 84/300\n",
      "Average training loss: 0.008086689385688968\n",
      "Average test loss: 0.0015888990622220768\n",
      "Epoch 85/300\n",
      "Average training loss: 0.008072878568950627\n",
      "Average test loss: 0.0015420231222071582\n",
      "Epoch 86/300\n",
      "Average training loss: 0.008057202200094859\n",
      "Average test loss: 0.0015560950906947256\n",
      "Epoch 87/300\n",
      "Average training loss: 0.008048197098904185\n",
      "Average test loss: 0.0015829589640100796\n",
      "Epoch 88/300\n",
      "Average training loss: 0.00804211625084281\n",
      "Average test loss: 0.0015484059517168337\n",
      "Epoch 89/300\n",
      "Average training loss: 0.008035025827586651\n",
      "Average test loss: 0.00160750798549917\n",
      "Epoch 90/300\n",
      "Average training loss: 0.00802259656538566\n",
      "Average test loss: 0.001601958360729946\n",
      "Epoch 91/300\n",
      "Average training loss: 0.008012441037015783\n",
      "Average test loss: 0.0015695764554871452\n",
      "Epoch 92/300\n",
      "Average training loss: 0.008008733963800802\n",
      "Average test loss: 0.001559675379242334\n",
      "Epoch 93/300\n",
      "Average training loss: 0.007999084479692909\n",
      "Average test loss: 0.0015535050206300284\n",
      "Epoch 94/300\n",
      "Average training loss: 0.007983154527842998\n",
      "Average test loss: 0.0015845504144413605\n",
      "Epoch 95/300\n",
      "Average training loss: 0.007981173629355099\n",
      "Average test loss: 0.0016023776311841276\n",
      "Epoch 96/300\n",
      "Average training loss: 0.007974394777582751\n",
      "Average test loss: 0.0015913942113725676\n",
      "Epoch 97/300\n",
      "Average training loss: 0.007962620698743397\n",
      "Average test loss: 0.001579137094422347\n",
      "Epoch 98/300\n",
      "Average training loss: 0.007945200622909598\n",
      "Average test loss: 0.0015913437956737147\n",
      "Epoch 99/300\n",
      "Average training loss: 0.007945915978401899\n",
      "Average test loss: 0.0016078365489633547\n",
      "Epoch 100/300\n",
      "Average training loss: 0.007927852573494116\n",
      "Average test loss: 0.0015848253993317484\n",
      "Epoch 101/300\n",
      "Average training loss: 0.007931592878782086\n",
      "Average test loss: 0.001565799053861863\n",
      "Epoch 102/300\n",
      "Average training loss: 0.007922802206542756\n",
      "Average test loss: 0.0016045274720009831\n",
      "Epoch 103/300\n",
      "Average training loss: 0.007915355160832405\n",
      "Average test loss: 0.0015701833308363953\n",
      "Epoch 104/300\n",
      "Average training loss: 0.007910375824819009\n",
      "Average test loss: 0.0015814761901274323\n",
      "Epoch 105/300\n",
      "Average training loss: 0.00789540444024735\n",
      "Average test loss: 0.0015496785817667841\n",
      "Epoch 106/300\n",
      "Average training loss: 0.00789134510109822\n",
      "Average test loss: 0.001593865550433596\n",
      "Epoch 107/300\n",
      "Average training loss: 0.007883462623175647\n",
      "Average test loss: 0.0015630772748134202\n",
      "Epoch 108/300\n",
      "Average training loss: 0.007874484116004573\n",
      "Average test loss: 0.0015994229794790346\n",
      "Epoch 109/300\n",
      "Average training loss: 0.007867731392383575\n",
      "Average test loss: 0.0015634558345708582\n",
      "Epoch 110/300\n",
      "Average training loss: 0.007863448169910246\n",
      "Average test loss: 0.0016054382817302312\n",
      "Epoch 111/300\n",
      "Average training loss: 0.007849927225046687\n",
      "Average test loss: 0.0016739446419394679\n",
      "Epoch 112/300\n",
      "Average training loss: 0.007844498332175943\n",
      "Average test loss: 0.0016189840830759042\n",
      "Epoch 113/300\n",
      "Average training loss: 0.007844706998931037\n",
      "Average test loss: 0.0015723642262940605\n",
      "Epoch 114/300\n",
      "Average training loss: 0.007828320134845045\n",
      "Average test loss: 0.001570466950846215\n",
      "Epoch 115/300\n",
      "Average training loss: 0.007825384120560355\n",
      "Average test loss: 0.0016020501414313912\n",
      "Epoch 116/300\n",
      "Average training loss: 0.007818966512050893\n",
      "Average test loss: 0.0016049826867464516\n",
      "Epoch 117/300\n",
      "Average training loss: 0.007808436046044032\n",
      "Average test loss: 0.0016024746584395568\n",
      "Epoch 118/300\n",
      "Average training loss: 0.007804707867817747\n",
      "Average test loss: 0.0015858720874206886\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0078097191510929\n",
      "Average test loss: 0.0015950011861407094\n",
      "Epoch 120/300\n",
      "Average training loss: 0.007797010174228085\n",
      "Average test loss: 0.001561875498543183\n",
      "Epoch 121/300\n",
      "Average training loss: 0.007788063778231541\n",
      "Average test loss: 0.001609736327858021\n",
      "Epoch 122/300\n",
      "Average training loss: 0.007777845496104823\n",
      "Average test loss: 0.0016751923346892\n",
      "Epoch 123/300\n",
      "Average training loss: 0.007773178417649534\n",
      "Average test loss: 0.0016163280732515786\n",
      "Epoch 124/300\n",
      "Average training loss: 0.007761807724005646\n",
      "Average test loss: 0.0015767109161242843\n",
      "Epoch 125/300\n",
      "Average training loss: 0.007758877652386824\n",
      "Average test loss: 0.0015793313574459817\n",
      "Epoch 126/300\n",
      "Average training loss: 0.007756357516679499\n",
      "Average test loss: 0.0016126775292472707\n",
      "Epoch 127/300\n",
      "Average training loss: 0.007755030019001828\n",
      "Average test loss: 0.001588398694164223\n",
      "Epoch 128/300\n",
      "Average training loss: 0.007731626345465581\n",
      "Average test loss: 0.0015843400867759354\n",
      "Epoch 129/300\n",
      "Average training loss: 0.007741105629752079\n",
      "Average test loss: 0.0016269337260681723\n",
      "Epoch 130/300\n",
      "Average training loss: 0.007733515592912833\n",
      "Average test loss: 0.001597713749234875\n",
      "Epoch 131/300\n",
      "Average training loss: 0.007717640432218711\n",
      "Average test loss: 0.0015796262699489792\n",
      "Epoch 132/300\n",
      "Average training loss: 0.007716480036162668\n",
      "Average test loss: 0.0016008280315953824\n",
      "Epoch 133/300\n",
      "Average training loss: 0.007710027319689592\n",
      "Average test loss: 0.0015794680987795194\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0077143945693969726\n",
      "Average test loss: 0.0016267133149214918\n",
      "Epoch 135/300\n",
      "Average training loss: 0.007707023271669944\n",
      "Average test loss: 0.001644422894447214\n",
      "Epoch 136/300\n",
      "Average training loss: 0.007702567490852541\n",
      "Average test loss: 0.00160869713463924\n",
      "Epoch 137/300\n",
      "Average training loss: 0.007688675204084979\n",
      "Average test loss: 0.001610189283370144\n",
      "Epoch 138/300\n",
      "Average training loss: 0.007685263638695081\n",
      "Average test loss: 0.0015980840759972732\n",
      "Epoch 139/300\n",
      "Average training loss: 0.00767521923325128\n",
      "Average test loss: 0.0016495767953909105\n",
      "Epoch 140/300\n",
      "Average training loss: 0.007669913833340009\n",
      "Average test loss: 0.0016091821192660265\n",
      "Epoch 141/300\n",
      "Average training loss: 0.007671880719148451\n",
      "Average test loss: 0.0016213883424384726\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0076664685946371824\n",
      "Average test loss: 0.0016003290634188387\n",
      "Epoch 143/300\n",
      "Average training loss: 0.007656510572880506\n",
      "Average test loss: 0.0015932392788430055\n",
      "Epoch 144/300\n",
      "Average training loss: 0.007652234784844849\n",
      "Average test loss: 0.001569893409187595\n",
      "Epoch 145/300\n",
      "Average training loss: 0.007649158083730274\n",
      "Average test loss: 0.001605857643816206\n",
      "Epoch 146/300\n",
      "Average training loss: 0.007640691124316719\n",
      "Average test loss: 0.0016401791417350372\n",
      "Epoch 147/300\n",
      "Average training loss: 0.007637029453698132\n",
      "Average test loss: 0.0016068430957901809\n",
      "Epoch 148/300\n",
      "Average training loss: 0.007642856770091587\n",
      "Average test loss: 0.001590187058887548\n",
      "Epoch 149/300\n",
      "Average training loss: 0.0076254537722302805\n",
      "Average test loss: 0.0016045851156943375\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0076272002470990025\n",
      "Average test loss: 0.0015879253333227503\n",
      "Epoch 151/300\n",
      "Average training loss: 0.007611443811820613\n",
      "Average test loss: 0.0016043481434591942\n",
      "Epoch 152/300\n",
      "Average training loss: 0.007613692060940796\n",
      "Average test loss: 0.0015759720580859316\n",
      "Epoch 153/300\n",
      "Average training loss: 0.007617142715387874\n",
      "Average test loss: 0.0016184081165120006\n",
      "Epoch 154/300\n",
      "Average training loss: 0.007606910352905592\n",
      "Average test loss: 0.0015786332109322151\n",
      "Epoch 155/300\n",
      "Average training loss: 0.007603253174987105\n",
      "Average test loss: 0.0018004887855301301\n",
      "Epoch 156/300\n",
      "Average training loss: 0.007601418525808388\n",
      "Average test loss: 0.0016180807571444247\n",
      "Epoch 157/300\n",
      "Average training loss: 0.007593585840115945\n",
      "Average test loss: 0.0016223379940622383\n",
      "Epoch 158/300\n",
      "Average training loss: 0.007579511780291795\n",
      "Average test loss: 0.001623180313139326\n",
      "Epoch 159/300\n",
      "Average training loss: 0.00757630480453372\n",
      "Average test loss: 0.00163943509840303\n",
      "Epoch 160/300\n",
      "Average training loss: 0.007575432033588489\n",
      "Average test loss: 0.0016398315791868502\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0075845073229736755\n",
      "Average test loss: 0.001603383895734118\n",
      "Epoch 162/300\n",
      "Average training loss: 0.007574668039464288\n",
      "Average test loss: 0.0015940837342705992\n",
      "Epoch 163/300\n",
      "Average training loss: 0.0075630963742733\n",
      "Average test loss: 0.0016013974783321222\n",
      "Epoch 164/300\n",
      "Average training loss: 0.00756821796629164\n",
      "Average test loss: 0.0016138296855303148\n",
      "Epoch 165/300\n",
      "Average training loss: 0.007545886209441556\n",
      "Average test loss: 0.0016224385011527273\n",
      "Epoch 166/300\n",
      "Average training loss: 0.00755416050967243\n",
      "Average test loss: 0.0016263663000944588\n",
      "Epoch 167/300\n",
      "Average training loss: 0.0075474819925924145\n",
      "Average test loss: 0.001599996160922779\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0075487299515969225\n",
      "Average test loss: 0.0016387764145102766\n",
      "Epoch 169/300\n",
      "Average training loss: 0.007541186003635327\n",
      "Average test loss: 0.0016107200489689907\n",
      "Epoch 170/300\n",
      "Average training loss: 0.007538683840798007\n",
      "Average test loss: 0.0016199335212715798\n",
      "Epoch 171/300\n",
      "Average training loss: 0.007527549519307083\n",
      "Average test loss: 0.0015898340315971937\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0075276989017923675\n",
      "Average test loss: 0.0016700461835910878\n",
      "Epoch 173/300\n",
      "Average training loss: 0.007526670150872734\n",
      "Average test loss: 0.001632909532222483\n",
      "Epoch 174/300\n",
      "Average training loss: 0.007517957409222921\n",
      "Average test loss: 0.0016377615115294854\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0075110774673521515\n",
      "Average test loss: 0.0016564564992570215\n",
      "Epoch 176/300\n",
      "Average training loss: 0.007515942126512527\n",
      "Average test loss: 0.0015956329307001498\n",
      "Epoch 177/300\n",
      "Average training loss: 0.007513671886175871\n",
      "Average test loss: 0.0016162982489913702\n",
      "Epoch 178/300\n",
      "Average training loss: 0.007506785041342179\n",
      "Average test loss: 0.001648561171359486\n",
      "Epoch 179/300\n",
      "Average training loss: 0.007505246021681362\n",
      "Average test loss: 0.0016584518543548055\n",
      "Epoch 180/300\n",
      "Average training loss: 0.007499873930381404\n",
      "Average test loss: 0.0016291892169974745\n",
      "Epoch 181/300\n",
      "Average training loss: 0.007495420355349779\n",
      "Average test loss: 0.0016172318936636051\n",
      "Epoch 182/300\n",
      "Average training loss: 0.007496925649957524\n",
      "Average test loss: 0.001597619232411186\n",
      "Epoch 183/300\n",
      "Average training loss: 0.007485962491482496\n",
      "Average test loss: 0.0016012701822651757\n",
      "Epoch 184/300\n",
      "Average training loss: 0.007483754189478027\n",
      "Average test loss: 0.0016473463420859642\n",
      "Epoch 185/300\n",
      "Average training loss: 0.0074788196401463615\n",
      "Average test loss: 0.0017107833866029979\n",
      "Epoch 186/300\n",
      "Average training loss: 0.007487745808644427\n",
      "Average test loss: 0.0016867526036997637\n",
      "Epoch 187/300\n",
      "Average training loss: 0.007475634182906813\n",
      "Average test loss: 0.0016297569000679585\n",
      "Epoch 188/300\n",
      "Average training loss: 0.007474240174310075\n",
      "Average test loss: 0.0016007529582517843\n",
      "Epoch 189/300\n",
      "Average training loss: 0.007468854481147395\n",
      "Average test loss: 0.0016367574348631832\n",
      "Epoch 190/300\n",
      "Average training loss: 0.00746266703804334\n",
      "Average test loss: 0.0016715036691683862\n",
      "Epoch 191/300\n",
      "Average training loss: 0.007459740736832221\n",
      "Average test loss: 0.0016351611473494107\n",
      "Epoch 192/300\n",
      "Average training loss: 0.007449769910011027\n",
      "Average test loss: 0.0016383345218168365\n",
      "Epoch 193/300\n",
      "Average training loss: 0.007455253285252385\n",
      "Average test loss: 0.0016225321791652177\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0074483050153487254\n",
      "Average test loss: 0.001651671083850993\n",
      "Epoch 195/300\n",
      "Average training loss: 0.007447206797699134\n",
      "Average test loss: 0.001659251286751694\n",
      "Epoch 196/300\n",
      "Average training loss: 0.007441353886491722\n",
      "Average test loss: 0.0016377746401768593\n",
      "Epoch 197/300\n",
      "Average training loss: 0.007439741192178594\n",
      "Average test loss: 0.001622010750333882\n",
      "Epoch 198/300\n",
      "Average training loss: 0.007437175847176049\n",
      "Average test loss: 0.0016544356416496966\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0074412364405062464\n",
      "Average test loss: 0.003333079230454233\n",
      "Epoch 200/300\n",
      "Average training loss: 0.007436455130577087\n",
      "Average test loss: 0.0016481550843454897\n",
      "Epoch 201/300\n",
      "Average training loss: 0.007423670277827316\n",
      "Average test loss: 0.001653327561966661\n",
      "Epoch 202/300\n",
      "Average training loss: 0.007420165402193864\n",
      "Average test loss: 0.001623923269721369\n",
      "Epoch 203/300\n",
      "Average training loss: 0.007419022975696458\n",
      "Average test loss: 0.0016350936961049836\n",
      "Epoch 204/300\n",
      "Average training loss: 0.007416685579551591\n",
      "Average test loss: 0.0016443988454217713\n",
      "Epoch 205/300\n",
      "Average training loss: 0.007416231499363979\n",
      "Average test loss: 0.001656003635790613\n",
      "Epoch 206/300\n",
      "Average training loss: 0.007414446919742558\n",
      "Average test loss: 0.0016326718309687244\n",
      "Epoch 207/300\n",
      "Average training loss: 0.007422993849135108\n",
      "Average test loss: 0.0016462654309968153\n",
      "Epoch 208/300\n",
      "Average training loss: 0.007404296330693695\n",
      "Average test loss: 0.0016434509134333995\n",
      "Epoch 209/300\n",
      "Average training loss: 0.007412905108597543\n",
      "Average test loss: 0.0016244541785369318\n",
      "Epoch 210/300\n",
      "Average training loss: 0.007408127300027344\n",
      "Average test loss: 0.0016381223334206475\n",
      "Epoch 211/300\n",
      "Average training loss: 0.007403433908190992\n",
      "Average test loss: 0.0016847989530199103\n",
      "Epoch 212/300\n",
      "Average training loss: 0.0073967116926279336\n",
      "Average test loss: 0.0016466842731460928\n",
      "Epoch 213/300\n",
      "Average training loss: 0.007399243893308772\n",
      "Average test loss: 0.0016501565454527735\n",
      "Epoch 214/300\n",
      "Average training loss: 0.007391503606819444\n",
      "Average test loss: 0.0016128073455765842\n",
      "Epoch 215/300\n",
      "Average training loss: 0.007395505671699842\n",
      "Average test loss: 0.0016434595560034116\n",
      "Epoch 216/300\n",
      "Average training loss: 0.007390726062986586\n",
      "Average test loss: 0.0016905452753934595\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0073832710824078985\n",
      "Average test loss: 0.0016708747811822427\n",
      "Epoch 218/300\n",
      "Average training loss: 0.007379796555472745\n",
      "Average test loss: 0.001633768144943234\n",
      "Epoch 219/300\n",
      "Average training loss: 0.007378772158589628\n",
      "Average test loss: 0.0016662443021519316\n",
      "Epoch 220/300\n",
      "Average training loss: 0.007379434081829257\n",
      "Average test loss: 0.001679313121570481\n",
      "Epoch 221/300\n",
      "Average training loss: 0.007376509380837282\n",
      "Average test loss: 0.001647357584287723\n",
      "Epoch 222/300\n",
      "Average training loss: 0.007366079277876351\n",
      "Average test loss: 0.001654683783546918\n",
      "Epoch 223/300\n",
      "Average training loss: 0.007369761653244496\n",
      "Average test loss: 0.0016400804865277475\n",
      "Epoch 224/300\n",
      "Average training loss: 0.007372349061071873\n",
      "Average test loss: 0.001671832723232607\n",
      "Epoch 225/300\n",
      "Average training loss: 0.007366013093541066\n",
      "Average test loss: 0.0016731175428463354\n",
      "Epoch 226/300\n",
      "Average training loss: 0.007359605597125159\n",
      "Average test loss: 0.0016468026758068137\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0073611769096718896\n",
      "Average test loss: 0.0016927369428384636\n",
      "Epoch 228/300\n",
      "Average training loss: 0.007361226387321949\n",
      "Average test loss: 0.001638736048920287\n",
      "Epoch 229/300\n",
      "Average training loss: 0.007359550358520614\n",
      "Average test loss: 0.001619841615565949\n",
      "Epoch 230/300\n",
      "Average training loss: 0.007351440192510684\n",
      "Average test loss: 0.0016703938699016967\n",
      "Epoch 231/300\n",
      "Average training loss: 0.007351633126123084\n",
      "Average test loss: 0.0016351851460834343\n",
      "Epoch 232/300\n",
      "Average training loss: 0.007343769874837663\n",
      "Average test loss: 0.001687495228006608\n",
      "Epoch 233/300\n",
      "Average training loss: 0.007348420660942793\n",
      "Average test loss: 0.0016351000807351536\n",
      "Epoch 234/300\n",
      "Average training loss: 0.007347053253816234\n",
      "Average test loss: 0.0016647211242881085\n",
      "Epoch 235/300\n",
      "Average training loss: 0.007344294883724716\n",
      "Average test loss: 0.001674875649313132\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0073343713291817245\n",
      "Average test loss: 0.0016716429492872622\n",
      "Epoch 237/300\n",
      "Average training loss: 0.007338438048958778\n",
      "Average test loss: 0.0016684897697220246\n",
      "Epoch 238/300\n",
      "Average training loss: 0.007327853552997112\n",
      "Average test loss: 0.001653113693723248\n",
      "Epoch 239/300\n",
      "Average training loss: 0.007327543743368652\n",
      "Average test loss: 0.0016468651656889252\n",
      "Epoch 240/300\n",
      "Average training loss: 0.007339557761947314\n",
      "Average test loss: 0.001639933123356766\n",
      "Epoch 241/300\n",
      "Average training loss: 0.007331611838191748\n",
      "Average test loss: 0.0017001413486173584\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0073207577077878845\n",
      "Average test loss: 0.0017429173404557837\n",
      "Epoch 243/300\n",
      "Average training loss: 0.007328515339642763\n",
      "Average test loss: 0.0016545550601763858\n",
      "Epoch 244/300\n",
      "Average training loss: 0.007314895662168662\n",
      "Average test loss: 0.0016796411987808017\n",
      "Epoch 245/300\n",
      "Average training loss: 0.007316068864117066\n",
      "Average test loss: 0.001664706634150611\n",
      "Epoch 246/300\n",
      "Average training loss: 0.007318494244582123\n",
      "Average test loss: 0.0017008345459277431\n",
      "Epoch 247/300\n",
      "Average training loss: 0.007320321813639667\n",
      "Average test loss: 0.0016744306046101783\n",
      "Epoch 248/300\n",
      "Average training loss: 0.007313424337241385\n",
      "Average test loss: 0.0016852289211625854\n",
      "Epoch 249/300\n",
      "Average training loss: 0.007311047659566005\n",
      "Average test loss: 0.0016340150634447733\n",
      "Epoch 250/300\n",
      "Average training loss: 0.007313236890567673\n",
      "Average test loss: 0.0017486016164119872\n",
      "Epoch 251/300\n",
      "Average training loss: 0.007308035591824187\n",
      "Average test loss: 0.0016632035691291093\n",
      "Epoch 252/300\n",
      "Average training loss: 0.007305309272888634\n",
      "Average test loss: 0.001655269734044042\n",
      "Epoch 253/300\n",
      "Average training loss: 0.007290947185622321\n",
      "Average test loss: 0.0016926547824922535\n",
      "Epoch 254/300\n",
      "Average training loss: 0.00729923594246308\n",
      "Average test loss: 0.0016692660379533967\n",
      "Epoch 255/300\n",
      "Average training loss: 0.007298417126552926\n",
      "Average test loss: 0.0016465840311720968\n",
      "Epoch 256/300\n",
      "Average training loss: 0.007294824245903227\n",
      "Average test loss: 0.001651228826544765\n",
      "Epoch 257/300\n",
      "Average training loss: 0.007292513608104653\n",
      "Average test loss: 0.0016721839809583294\n",
      "Epoch 258/300\n",
      "Average training loss: 0.007285227322330078\n",
      "Average test loss: 0.0016850218845324385\n",
      "Epoch 259/300\n",
      "Average training loss: 0.007289867593596379\n",
      "Average test loss: 0.0016727632341078586\n",
      "Epoch 260/300\n",
      "Average training loss: 0.00729174201687177\n",
      "Average test loss: 0.0016586390311519306\n",
      "Epoch 261/300\n",
      "Average training loss: 0.0072886655318240325\n",
      "Average test loss: 0.0016792643958081803\n",
      "Epoch 262/300\n",
      "Average training loss: 0.007279492774771319\n",
      "Average test loss: 0.0016786081081566712\n",
      "Epoch 263/300\n",
      "Average training loss: 0.007281654511060979\n",
      "Average test loss: 0.0016368079200490481\n",
      "Epoch 264/300\n",
      "Average training loss: 0.007276315282616351\n",
      "Average test loss: 0.001673190116468403\n",
      "Epoch 265/300\n",
      "Average training loss: 0.007277067093385591\n",
      "Average test loss: 0.0016955059193488624\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0072790858323375385\n",
      "Average test loss: 0.0016678015143714018\n",
      "Epoch 267/300\n",
      "Average training loss: 0.007280367540402545\n",
      "Average test loss: 0.0017037961058732536\n",
      "Epoch 268/300\n",
      "Average training loss: 0.00727145436199175\n",
      "Average test loss: 0.0017050717693443099\n",
      "Epoch 269/300\n",
      "Average training loss: 0.007265384061468972\n",
      "Average test loss: 0.001648185717407614\n",
      "Epoch 270/300\n",
      "Average training loss: 0.007267137166940504\n",
      "Average test loss: 0.0016432969158308374\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0072737722901834384\n",
      "Average test loss: 0.0016440953292573492\n",
      "Epoch 272/300\n",
      "Average training loss: 0.0072692796236111055\n",
      "Average test loss: 0.001657595091395908\n",
      "Epoch 273/300\n",
      "Average training loss: 0.00726277410859863\n",
      "Average test loss: 0.001641125087884979\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0072531126038067875\n",
      "Average test loss: 0.0016717724465868538\n",
      "Epoch 275/300\n",
      "Average training loss: 0.007259183540526364\n",
      "Average test loss: 0.0016707534617226986\n",
      "Epoch 276/300\n",
      "Average training loss: 0.007263332441035244\n",
      "Average test loss: 0.002398883626485864\n",
      "Epoch 277/300\n",
      "Average training loss: 0.007261349289781518\n",
      "Average test loss: 0.0016927690901276137\n",
      "Epoch 278/300\n",
      "Average training loss: 0.007248357102688816\n",
      "Average test loss: 0.0016533743997828828\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0072475073221657015\n",
      "Average test loss: 0.0016714774845167995\n",
      "Epoch 280/300\n",
      "Average training loss: 0.007251120609955655\n",
      "Average test loss: 0.0016703192052534884\n",
      "Epoch 281/300\n",
      "Average training loss: 0.007248468951218658\n",
      "Average test loss: 0.001785006771588491\n",
      "Epoch 282/300\n",
      "Average training loss: 0.007250626484553019\n",
      "Average test loss: 0.001755786116131478\n",
      "Epoch 283/300\n",
      "Average training loss: 0.007240845171941651\n",
      "Average test loss: 0.0017070685527804826\n",
      "Epoch 284/300\n",
      "Average training loss: 0.0072475797099371755\n",
      "Average test loss: 0.0016516473125148979\n",
      "Epoch 285/300\n",
      "Average training loss: 0.007238950618853172\n",
      "Average test loss: 0.001661608262401488\n",
      "Epoch 286/300\n",
      "Average training loss: 0.007234432105802827\n",
      "Average test loss: 0.0016747015004770624\n",
      "Epoch 287/300\n",
      "Average training loss: 0.007238443751715951\n",
      "Average test loss: 0.0016639713152415223\n",
      "Epoch 288/300\n",
      "Average training loss: 0.007236698818289571\n",
      "Average test loss: 0.0017241592415504986\n",
      "Epoch 289/300\n",
      "Average training loss: 0.007234324315769805\n",
      "Average test loss: 0.0016525669395923614\n",
      "Epoch 290/300\n",
      "Average training loss: 0.007244671921763155\n",
      "Average test loss: 0.0016989164795312617\n",
      "Epoch 291/300\n",
      "Average training loss: 0.007231753213538064\n",
      "Average test loss: 0.0016489070187219316\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0072263500545587805\n",
      "Average test loss: 0.001699670632990698\n",
      "Epoch 293/300\n",
      "Average training loss: 0.007227242053796848\n",
      "Average test loss: 0.0017124225040897726\n",
      "Epoch 294/300\n",
      "Average training loss: 0.007219508009652297\n",
      "Average test loss: 0.0016642064857814048\n",
      "Epoch 295/300\n",
      "Average training loss: 0.007223584794335895\n",
      "Average test loss: 0.001660144434000055\n",
      "Epoch 296/300\n",
      "Average training loss: 0.007231368751575549\n",
      "Average test loss: 0.0017207911509192653\n",
      "Epoch 297/300\n",
      "Average training loss: 0.007215601416097747\n",
      "Average test loss: 0.0017935728840529918\n",
      "Epoch 298/300\n",
      "Average training loss: 0.00722236646256513\n",
      "Average test loss: 0.0017125455976153413\n",
      "Epoch 299/300\n",
      "Average training loss: 0.007219440891096989\n",
      "Average test loss: 0.001700648702474104\n",
      "Epoch 300/300\n",
      "Average training loss: 0.007217780578881502\n",
      "Average test loss: 0.001640080257008473\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-DCT-Additive-.01/5 Projections'\n",
    "\n",
    "DCT_10_proj5_weights, DCT_10_proj5_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj5', display=True)\n",
    "\n",
    "DCT_20_proj5_weights, DCT_20_proj5_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj5', display=True)\n",
    "\n",
    "DCT_30_proj5_weights, DCT_30_proj5_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj5', display=True)\n",
    "\n",
    "DCT_40_proj5_weights, DCT_40_proj5_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, DCT_10_proj5_weights, DCT_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, DCT_20_proj5_weights, DCT_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, DCT_30_proj5_weights, DCT_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, DCT_40_proj5_weights, DCT_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.88\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.71\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.07\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.97\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.69\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 32.21\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 32.54\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj5_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj5_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj5_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj5_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj5_psnr = average_PSNR(DCT_10_proj5_model, DCT_10_proj5_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj5_psnr = average_PSNR(DCT_20_proj5_model, DCT_20_proj5_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj5_psnr = average_PSNR(DCT_30_proj5_model, DCT_30_proj5_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj5_psnr = average_PSNR(DCT_40_proj5_model, DCT_40_proj5_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9663716490003798\n",
      "Average test loss: 0.01151183373729388\n",
      "Epoch 2/300\n",
      "Average training loss: 0.22490478936831157\n",
      "Average test loss: 0.004803920553376278\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1508086237245136\n",
      "Average test loss: 0.004559377003461123\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11931679924329122\n",
      "Average test loss: 0.004522634130799108\n",
      "Epoch 5/300\n",
      "Average training loss: 0.10171570284499062\n",
      "Average test loss: 0.00552945401519537\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09123710010449092\n",
      "Average test loss: 0.07312521750314369\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08388221639725897\n",
      "Average test loss: 0.004330900097058879\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07854150754875606\n",
      "Average test loss: 0.004305230692028999\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07448253419001898\n",
      "Average test loss: 0.004260182168748644\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07164878352483113\n",
      "Average test loss: 0.004230148122749395\n",
      "Epoch 11/300\n",
      "Average training loss: 0.06946075637473001\n",
      "Average test loss: 0.004235152007804977\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06777454753716787\n",
      "Average test loss: 0.004203288357704878\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06644341260857052\n",
      "Average test loss: 0.004176259883369009\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06551854897207685\n",
      "Average test loss: 0.0041532327698336705\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0647540972398387\n",
      "Average test loss: 0.00414440834066934\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0641378293302324\n",
      "Average test loss: 0.004133835870151719\n",
      "Epoch 17/300\n",
      "Average training loss: 0.0636398527820905\n",
      "Average test loss: 0.004137862694760163\n",
      "Epoch 18/300\n",
      "Average training loss: 0.06323142439126968\n",
      "Average test loss: 0.004241320772303475\n",
      "Epoch 19/300\n",
      "Average training loss: 0.0628771713938978\n",
      "Average test loss: 0.004113984876829717\n",
      "Epoch 20/300\n",
      "Average training loss: 0.06253082200553682\n",
      "Average test loss: 0.004078788561125596\n",
      "Epoch 21/300\n",
      "Average training loss: 0.06224782628483243\n",
      "Average test loss: 0.004057127898765935\n",
      "Epoch 22/300\n",
      "Average training loss: 0.06196681578954061\n",
      "Average test loss: 0.004055412446459135\n",
      "Epoch 23/300\n",
      "Average training loss: 0.06173234077625805\n",
      "Average test loss: 0.004085881574700276\n",
      "Epoch 24/300\n",
      "Average training loss: 0.061511448466115524\n",
      "Average test loss: 0.0040647824481129646\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0612805438472165\n",
      "Average test loss: 0.0040469249362746875\n",
      "Epoch 26/300\n",
      "Average training loss: 0.061087672587898045\n",
      "Average test loss: 0.0040255256394545235\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06091613150967492\n",
      "Average test loss: 0.004020557187083695\n",
      "Epoch 28/300\n",
      "Average training loss: 0.060703645229339596\n",
      "Average test loss: 0.00402666687592864\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06057342367039786\n",
      "Average test loss: 0.004002949544539054\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06034921282198694\n",
      "Average test loss: 0.004021278301460875\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06018225809931755\n",
      "Average test loss: 0.003985352803021669\n",
      "Epoch 32/300\n",
      "Average training loss: 0.060064620524644854\n",
      "Average test loss: 0.004009547514634\n",
      "Epoch 33/300\n",
      "Average training loss: 0.059907472097211416\n",
      "Average test loss: 0.003979312906869583\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05974996089935303\n",
      "Average test loss: 0.003962365204261409\n",
      "Epoch 35/300\n",
      "Average training loss: 0.059622083932161334\n",
      "Average test loss: 0.003983609833030237\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05948890809218089\n",
      "Average test loss: 0.003973267893410392\n",
      "Epoch 37/300\n",
      "Average training loss: 0.059344476809104284\n",
      "Average test loss: 0.003960655232063598\n",
      "Epoch 38/300\n",
      "Average training loss: 0.059238193972243204\n",
      "Average test loss: 0.003948347326575054\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05912388711836603\n",
      "Average test loss: 0.003948890873955356\n",
      "Epoch 40/300\n",
      "Average training loss: 0.058959276103311115\n",
      "Average test loss: 0.003978200385140048\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05886176128519906\n",
      "Average test loss: 0.003940428376197815\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05877863082455264\n",
      "Average test loss: 0.003936970650942789\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05863922374447187\n",
      "Average test loss: 0.003944423787709739\n",
      "Epoch 44/300\n",
      "Average training loss: 0.058566732418206\n",
      "Average test loss: 0.003948146979014079\n",
      "Epoch 45/300\n",
      "Average training loss: 0.058430959694915345\n",
      "Average test loss: 0.0039442030642595554\n",
      "Epoch 46/300\n",
      "Average training loss: 0.058345779842800566\n",
      "Average test loss: 0.003918569347924656\n",
      "Epoch 47/300\n",
      "Average training loss: 0.058255241870880124\n",
      "Average test loss: 0.003936682919661204\n",
      "Epoch 48/300\n",
      "Average training loss: 0.05818160616026984\n",
      "Average test loss: 0.003969117946922779\n",
      "Epoch 49/300\n",
      "Average training loss: 0.058072490202056036\n",
      "Average test loss: 0.003922826467288865\n",
      "Epoch 50/300\n",
      "Average training loss: 0.0579764292438825\n",
      "Average test loss: 0.003937081534829405\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05792965357502301\n",
      "Average test loss: 0.003918315901110569\n",
      "Epoch 52/300\n",
      "Average training loss: 0.05782339265942574\n",
      "Average test loss: 0.00391724849326743\n",
      "Epoch 53/300\n",
      "Average training loss: 0.05771258885661761\n",
      "Average test loss: 0.003949375410046843\n",
      "Epoch 54/300\n",
      "Average training loss: 0.05765889114472601\n",
      "Average test loss: 0.00391846779278583\n",
      "Epoch 55/300\n",
      "Average training loss: 0.057604104455974366\n",
      "Average test loss: 0.0039043263029307127\n",
      "Epoch 56/300\n",
      "Average training loss: 0.057509260833263395\n",
      "Average test loss: 0.003934189084089465\n",
      "Epoch 57/300\n",
      "Average training loss: 0.057373982502354516\n",
      "Average test loss: 0.0039227808519370026\n",
      "Epoch 58/300\n",
      "Average training loss: 0.05732295874754588\n",
      "Average test loss: 0.003916714694764879\n",
      "Epoch 59/300\n",
      "Average training loss: 0.057236653520001304\n",
      "Average test loss: 0.003927847028192546\n",
      "Epoch 60/300\n",
      "Average training loss: 0.057121990256839325\n",
      "Average test loss: 0.003918297182147702\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05706321057014995\n",
      "Average test loss: 0.003905649147927761\n",
      "Epoch 62/300\n",
      "Average training loss: 0.05701217069559627\n",
      "Average test loss: 0.003918032625483142\n",
      "Epoch 63/300\n",
      "Average training loss: 0.056907551404502654\n",
      "Average test loss: 0.003932961233374145\n",
      "Epoch 64/300\n",
      "Average training loss: 0.056831699053446455\n",
      "Average test loss: 0.003908600962824292\n",
      "Epoch 65/300\n",
      "Average training loss: 0.056705444594224295\n",
      "Average test loss: 0.0039047391967227062\n",
      "Epoch 66/300\n",
      "Average training loss: 0.05671047529909346\n",
      "Average test loss: 0.003941795601612992\n",
      "Epoch 67/300\n",
      "Average training loss: 0.056593373311890495\n",
      "Average test loss: 0.003983812961727381\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05652980418668853\n",
      "Average test loss: 0.003920218781050709\n",
      "Epoch 69/300\n",
      "Average training loss: 0.056437059193849566\n",
      "Average test loss: 0.003918525970851382\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05630417248606682\n",
      "Average test loss: 0.003930362552404404\n",
      "Epoch 71/300\n",
      "Average training loss: 0.056305995944473476\n",
      "Average test loss: 0.0039532015464372105\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05617866856190894\n",
      "Average test loss: 0.003929083856650525\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05608266982767317\n",
      "Average test loss: 0.003950413788565331\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05597811967465613\n",
      "Average test loss: 0.003941650228160951\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0558841684891118\n",
      "Average test loss: 0.00391957586672571\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05582785372270478\n",
      "Average test loss: 0.003924631759524346\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05575834408071306\n",
      "Average test loss: 0.003968469671077199\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05571845689747069\n",
      "Average test loss: 0.003931262320528428\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05559246399667528\n",
      "Average test loss: 0.003936566411621041\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05548171778519948\n",
      "Average test loss: 0.003986337655534347\n",
      "Epoch 81/300\n",
      "Average training loss: 0.055396046333842805\n",
      "Average test loss: 0.003949104704376724\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05527961289551523\n",
      "Average test loss: 0.003976316999023159\n",
      "Epoch 83/300\n",
      "Average training loss: 0.055239236070050136\n",
      "Average test loss: 0.004002744995471504\n",
      "Epoch 84/300\n",
      "Average training loss: 0.055171079675356546\n",
      "Average test loss: 0.003942050457000732\n",
      "Epoch 85/300\n",
      "Average training loss: 0.055092515842782123\n",
      "Average test loss: 0.0039314138320171175\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05499744989143478\n",
      "Average test loss: 0.004007638538049327\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05491400498814053\n",
      "Average test loss: 0.0039569535578290625\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05479574565092723\n",
      "Average test loss: 0.004054122054742443\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05477041646507051\n",
      "Average test loss: 0.004018054414126608\n",
      "Epoch 90/300\n",
      "Average training loss: 0.054614786982536315\n",
      "Average test loss: 0.0039678285713824965\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05462984677818086\n",
      "Average test loss: 0.0039514361787587405\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05455461600422859\n",
      "Average test loss: 0.003994637961188952\n",
      "Epoch 93/300\n",
      "Average training loss: 0.054413471943802305\n",
      "Average test loss: 0.0039762772894981835\n",
      "Epoch 94/300\n",
      "Average training loss: 0.054333642972840206\n",
      "Average test loss: 0.004005214915093448\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0543021392888493\n",
      "Average test loss: 0.0039656074543794\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05421199847592248\n",
      "Average test loss: 0.003994875402500232\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05410954385333591\n",
      "Average test loss: 0.004000173679242531\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05400016484326786\n",
      "Average test loss: 0.003970552651004659\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05396156393819385\n",
      "Average test loss: 0.0040026346722410785\n",
      "Epoch 100/300\n",
      "Average training loss: 0.05390466726819674\n",
      "Average test loss: 0.004036339170816872\n",
      "Epoch 101/300\n",
      "Average training loss: 0.053825740496317546\n",
      "Average test loss: 0.004175267341650195\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0537719898753696\n",
      "Average test loss: 0.0040889559876587656\n",
      "Epoch 103/300\n",
      "Average training loss: 0.05361871851815118\n",
      "Average test loss: 0.004048595572925276\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05355805003643036\n",
      "Average test loss: 0.004060743998529182\n",
      "Epoch 105/300\n",
      "Average training loss: 0.053506160812245476\n",
      "Average test loss: 0.004072875883844164\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05353439539339807\n",
      "Average test loss: 0.0040760777534710035\n",
      "Epoch 107/300\n",
      "Average training loss: 0.053367128670215605\n",
      "Average test loss: 0.003998311824268765\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05328964229590363\n",
      "Average test loss: 0.004063031416593327\n",
      "Epoch 109/300\n",
      "Average training loss: 0.053279134568240905\n",
      "Average test loss: 0.004071037818160322\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05321106206708484\n",
      "Average test loss: 0.00404916397192412\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05306949080692397\n",
      "Average test loss: 0.00414076630729768\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0530255453950829\n",
      "Average test loss: 0.004010053563449118\n",
      "Epoch 113/300\n",
      "Average training loss: 0.052961087458663514\n",
      "Average test loss: 0.004149159322803219\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05291482079194652\n",
      "Average test loss: 0.004101979398892986\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05280406347910563\n",
      "Average test loss: 0.004168537602242496\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05275368600752619\n",
      "Average test loss: 0.004110089284264379\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0526671667959955\n",
      "Average test loss: 0.004050343172831668\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05261353325512674\n",
      "Average test loss: 0.004109335388160414\n",
      "Epoch 119/300\n",
      "Average training loss: 0.052584147820870085\n",
      "Average test loss: 0.004073615093405048\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05251704181234042\n",
      "Average test loss: 0.004107389631163743\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05247492789891031\n",
      "Average test loss: 0.00418783670808706\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05242708418104384\n",
      "Average test loss: 0.004139956252856387\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05230447036690182\n",
      "Average test loss: 0.004114468358043168\n",
      "Epoch 124/300\n",
      "Average training loss: 0.052256659825642905\n",
      "Average test loss: 0.004185255521287521\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05218066536055671\n",
      "Average test loss: 0.004088567174971104\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05221523190538088\n",
      "Average test loss: 0.004132596284771959\n",
      "Epoch 127/300\n",
      "Average training loss: 0.052143187158637576\n",
      "Average test loss: 0.004080301620480087\n",
      "Epoch 128/300\n",
      "Average training loss: 0.052060337858067615\n",
      "Average test loss: 0.0041553167485528526\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05199443138639132\n",
      "Average test loss: 0.004104605347332028\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05192624226874775\n",
      "Average test loss: 0.005841351279368003\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05186629561583201\n",
      "Average test loss: 0.004095576611244016\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05185014808509085\n",
      "Average test loss: 0.004263267689694961\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05177892116043303\n",
      "Average test loss: 0.004071875078396665\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05171547062860595\n",
      "Average test loss: 0.00405660216944913\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05162447531024615\n",
      "Average test loss: 0.004115701299160719\n",
      "Epoch 136/300\n",
      "Average training loss: 0.051618181659115685\n",
      "Average test loss: 0.004134228746924136\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05154068535897467\n",
      "Average test loss: 0.004081850511539313\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05149175831344392\n",
      "Average test loss: 0.004163826513621542\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05141024138530095\n",
      "Average test loss: 0.004158189883662595\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05143110583225886\n",
      "Average test loss: 0.004079321568624841\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05142892137500975\n",
      "Average test loss: 0.004072587543477614\n",
      "Epoch 142/300\n",
      "Average training loss: 0.05125038882096609\n",
      "Average test loss: 0.004112145885411236\n",
      "Epoch 143/300\n",
      "Average training loss: 0.05127538481354713\n",
      "Average test loss: 0.004077183070489102\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05121159756183624\n",
      "Average test loss: 0.004120084054975046\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05113179516957866\n",
      "Average test loss: 0.004147251659797298\n",
      "Epoch 146/300\n",
      "Average training loss: 0.051167882399426566\n",
      "Average test loss: 0.004086282532248232\n",
      "Epoch 147/300\n",
      "Average training loss: 0.051095495363076525\n",
      "Average test loss: 0.004112386063983043\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05105162840419345\n",
      "Average test loss: 0.004074155200686719\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05104570189449522\n",
      "Average test loss: 0.004092614323107733\n",
      "Epoch 150/300\n",
      "Average training loss: 0.050936809602710936\n",
      "Average test loss: 0.004134262998691863\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05084203008148405\n",
      "Average test loss: 0.0041670912895351644\n",
      "Epoch 152/300\n",
      "Average training loss: 0.05083525213930342\n",
      "Average test loss: 0.004117293174068133\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05076760821044445\n",
      "Average test loss: 0.004162607938879066\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05076789191365242\n",
      "Average test loss: 0.004082595337389244\n",
      "Epoch 155/300\n",
      "Average training loss: 0.050689864503012765\n",
      "Average test loss: 0.0040910898962368564\n",
      "Epoch 156/300\n",
      "Average training loss: 0.050675483183728325\n",
      "Average test loss: 0.004235904020981656\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05061385359697872\n",
      "Average test loss: 0.00417929749418464\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05054292321536276\n",
      "Average test loss: 0.004221549545725187\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0505207784374555\n",
      "Average test loss: 0.004188768391807874\n",
      "Epoch 160/300\n",
      "Average training loss: 0.050546468913555145\n",
      "Average test loss: 0.004407423321157694\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05047044916616546\n",
      "Average test loss: 0.004106543861743477\n",
      "Epoch 162/300\n",
      "Average training loss: 0.050398504541979895\n",
      "Average test loss: 0.0041862464596827825\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05036957386467192\n",
      "Average test loss: 0.004117887301991383\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05033427295751042\n",
      "Average test loss: 0.004202013654220436\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05031726309988234\n",
      "Average test loss: 0.004246216780609555\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05022038830320041\n",
      "Average test loss: 0.0042883577785558175\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05029695453246435\n",
      "Average test loss: 0.00415442509121365\n",
      "Epoch 168/300\n",
      "Average training loss: 0.050193111581934825\n",
      "Average test loss: 0.004228532805624935\n",
      "Epoch 169/300\n",
      "Average training loss: 0.050116734723250074\n",
      "Average test loss: 0.004136840481725004\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05007820009191831\n",
      "Average test loss: 0.004217535976113545\n",
      "Epoch 171/300\n",
      "Average training loss: 0.050070076339774665\n",
      "Average test loss: 0.004143547898779313\n",
      "Epoch 172/300\n",
      "Average training loss: 0.050092866914139854\n",
      "Average test loss: 0.004166395511478186\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05001649572451909\n",
      "Average test loss: 0.004137914329353306\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04996780354446835\n",
      "Average test loss: 0.004192739844322204\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04991559264726109\n",
      "Average test loss: 0.004167797030260166\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04989063882165485\n",
      "Average test loss: 0.004169547057400147\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04989672259820832\n",
      "Average test loss: 0.004171162163631783\n",
      "Epoch 178/300\n",
      "Average training loss: 0.049813854621516336\n",
      "Average test loss: 0.004254444990307093\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04976709459225337\n",
      "Average test loss: 0.004107689976278278\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04981542647547192\n",
      "Average test loss: 0.004167298713905944\n",
      "Epoch 181/300\n",
      "Average training loss: 0.049772405518425836\n",
      "Average test loss: 0.004129855508191718\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04969245417581664\n",
      "Average test loss: 0.004144944899198082\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04968888025482496\n",
      "Average test loss: 0.004216882772329781\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04960663675434059\n",
      "Average test loss: 0.004246116585408648\n",
      "Epoch 185/300\n",
      "Average training loss: 0.049576324307256274\n",
      "Average test loss: 0.004156581651833322\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04958133609427346\n",
      "Average test loss: 0.004152893235699998\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04951461213495996\n",
      "Average test loss: 0.0042095592916011814\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04950328744782342\n",
      "Average test loss: 0.004132246917320622\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0494874284432994\n",
      "Average test loss: 0.004192674846284919\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04947599503397942\n",
      "Average test loss: 0.004172416565318902\n",
      "Epoch 191/300\n",
      "Average training loss: 0.049414043383465875\n",
      "Average test loss: 0.0041674821416123045\n",
      "Epoch 192/300\n",
      "Average training loss: 0.049351236565245524\n",
      "Average test loss: 0.0043030581495000254\n",
      "Epoch 193/300\n",
      "Average training loss: 0.049312379873461196\n",
      "Average test loss: 0.004306188838349448\n",
      "Epoch 194/300\n",
      "Average training loss: 0.049383206493324706\n",
      "Average test loss: 0.0042697607889357544\n",
      "Epoch 195/300\n",
      "Average training loss: 0.049360093037287396\n",
      "Average test loss: 0.004211498025804758\n",
      "Epoch 196/300\n",
      "Average training loss: 0.049269841122958395\n",
      "Average test loss: 0.004177441368086471\n",
      "Epoch 197/300\n",
      "Average training loss: 0.049209289769331616\n",
      "Average test loss: 0.004313450681252612\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04919130152463913\n",
      "Average test loss: 0.00427339581027627\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04912931819094552\n",
      "Average test loss: 0.004186781785968277\n",
      "Epoch 200/300\n",
      "Average training loss: 0.049139156950844656\n",
      "Average test loss: 0.0041169351302087305\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04916793765955501\n",
      "Average test loss: 0.004304616263343228\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04905680838227272\n",
      "Average test loss: 0.004244114446143309\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04907073796126578\n",
      "Average test loss: 0.0042490020555754505\n",
      "Epoch 204/300\n",
      "Average training loss: 0.049023973024553726\n",
      "Average test loss: 0.00421982926916745\n",
      "Epoch 205/300\n",
      "Average training loss: 0.048955908404456246\n",
      "Average test loss: 0.00420715581273867\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04895133894019657\n",
      "Average test loss: 0.004211913502671652\n",
      "Epoch 207/300\n",
      "Average training loss: 0.048972463740242854\n",
      "Average test loss: 0.0042585785701457\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04895186546444893\n",
      "Average test loss: 0.0042239354343877895\n",
      "Epoch 209/300\n",
      "Average training loss: 0.048880966593821844\n",
      "Average test loss: 0.004170991990715266\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0488900966876083\n",
      "Average test loss: 0.004203075691643688\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04886302762230237\n",
      "Average test loss: 0.004197563091913859\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04884451185994678\n",
      "Average test loss: 0.004296578383694093\n",
      "Epoch 213/300\n",
      "Average training loss: 0.048712377856175104\n",
      "Average test loss: 0.004341024172802766\n",
      "Epoch 214/300\n",
      "Average training loss: 0.048762613892555234\n",
      "Average test loss: 0.0041603587358776065\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04868648974928591\n",
      "Average test loss: 0.004279055104073551\n",
      "Epoch 216/300\n",
      "Average training loss: 0.048712372601032256\n",
      "Average test loss: 0.004184239192141427\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04875144061777327\n",
      "Average test loss: 0.004215806603017781\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04863977143168449\n",
      "Average test loss: 0.004217476526896159\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0486219446890884\n",
      "Average test loss: 0.0041653670490615895\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04862013771798875\n",
      "Average test loss: 0.004124234270925323\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04860994488663144\n",
      "Average test loss: 0.004230477396812704\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04858162694507175\n",
      "Average test loss: 0.004308696906185812\n",
      "Epoch 223/300\n",
      "Average training loss: 0.048522054983509914\n",
      "Average test loss: 0.004208858461222715\n",
      "Epoch 224/300\n",
      "Average training loss: 0.04853145248691241\n",
      "Average test loss: 0.004214846252567238\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04845856829153167\n",
      "Average test loss: 0.004194744119213687\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04844584245814217\n",
      "Average test loss: 0.004296617712825536\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04845357632637024\n",
      "Average test loss: 0.004256173181451029\n",
      "Epoch 228/300\n",
      "Average training loss: 0.048414230624834693\n",
      "Average test loss: 0.004221012728702691\n",
      "Epoch 229/300\n",
      "Average training loss: 0.048436716473764845\n",
      "Average test loss: 0.004364078984078434\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04837268116076787\n",
      "Average test loss: 0.004160068131776319\n",
      "Epoch 231/300\n",
      "Average training loss: 0.048324665013286805\n",
      "Average test loss: 0.004296273289455308\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04835418140888214\n",
      "Average test loss: 0.0042394079089992575\n",
      "Epoch 233/300\n",
      "Average training loss: 0.04827626518242889\n",
      "Average test loss: 0.0043172216874857745\n",
      "Epoch 234/300\n",
      "Average training loss: 0.048258051888810265\n",
      "Average test loss: 0.0042028506327834395\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04827170491549704\n",
      "Average test loss: 0.00421097811890973\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04824375846982002\n",
      "Average test loss: 0.004302646845579148\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04815488796101676\n",
      "Average test loss: 0.004236759742101034\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04826342233022054\n",
      "Average test loss: 0.004217079499529468\n",
      "Epoch 239/300\n",
      "Average training loss: 0.048179048260053\n",
      "Average test loss: 0.004319643277054031\n",
      "Epoch 240/300\n",
      "Average training loss: 0.048127230054802364\n",
      "Average test loss: 0.004305496440372533\n",
      "Epoch 241/300\n",
      "Average training loss: 0.048141602635383605\n",
      "Average test loss: 0.0043344737957749105\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04807214915090137\n",
      "Average test loss: 0.004262555915862322\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04807982515295347\n",
      "Average test loss: 0.004343837081144253\n",
      "Epoch 244/300\n",
      "Average training loss: 0.048077921711736256\n",
      "Average test loss: 0.004191401125863195\n",
      "Epoch 245/300\n",
      "Average training loss: 0.048097528802023996\n",
      "Average test loss: 0.00423618358746171\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04798116689754857\n",
      "Average test loss: 0.004170535295787784\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0479888888961739\n",
      "Average test loss: 0.004307006811930074\n",
      "Epoch 248/300\n",
      "Average training loss: 0.048001576946841346\n",
      "Average test loss: 0.0042578704199857185\n",
      "Epoch 249/300\n",
      "Average training loss: 0.047995480219523115\n",
      "Average test loss: 0.004137692352963819\n",
      "Epoch 250/300\n",
      "Average training loss: 0.04793458005123668\n",
      "Average test loss: 0.004190452624319328\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04787705903251966\n",
      "Average test loss: 0.004252256645510594\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04792024085587925\n",
      "Average test loss: 0.004415908859835731\n",
      "Epoch 253/300\n",
      "Average training loss: 0.047925047308206556\n",
      "Average test loss: 0.004310187889469994\n",
      "Epoch 254/300\n",
      "Average training loss: 0.0478509075542291\n",
      "Average test loss: 0.004291387328671084\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04786446894870864\n",
      "Average test loss: 0.00428813368992673\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04783599724372228\n",
      "Average test loss: 0.004164334000812637\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04780133847395579\n",
      "Average test loss: 0.00429438214459353\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04774333053827286\n",
      "Average test loss: 0.004283798047651847\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04774122391144434\n",
      "Average test loss: 0.004245127914887336\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04769298507769903\n",
      "Average test loss: 0.004254953331624468\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04772055739164353\n",
      "Average test loss: 0.004306139020663169\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04772148822744687\n",
      "Average test loss: 0.004338807177626424\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04773917636275291\n",
      "Average test loss: 0.004328592831682828\n",
      "Epoch 264/300\n",
      "Average training loss: 0.047627028465271\n",
      "Average test loss: 0.004269212542308701\n",
      "Epoch 265/300\n",
      "Average training loss: 0.047702685084607864\n",
      "Average test loss: 0.004259020823778378\n",
      "Epoch 266/300\n",
      "Average training loss: 0.047653361873494254\n",
      "Average test loss: 0.004213104581253396\n",
      "Epoch 267/300\n",
      "Average training loss: 0.047617726372347936\n",
      "Average test loss: 0.004263965490170651\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0475723772181405\n",
      "Average test loss: 0.004373450207213561\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0475924483637015\n",
      "Average test loss: 0.004328233351310094\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04754642230934567\n",
      "Average test loss: 0.0042490682519144484\n",
      "Epoch 271/300\n",
      "Average training loss: 0.047523596846395066\n",
      "Average test loss: 0.004166193048987124\n",
      "Epoch 272/300\n",
      "Average training loss: 0.04748477997051345\n",
      "Average test loss: 0.004253636568991674\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04749145664771398\n",
      "Average test loss: 0.004349971104620231\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04749384980069266\n",
      "Average test loss: 0.0042534819290869766\n",
      "Epoch 275/300\n",
      "Average training loss: 0.04751876129375564\n",
      "Average test loss: 0.0042585673795806035\n",
      "Epoch 276/300\n",
      "Average training loss: 0.047426112921701534\n",
      "Average test loss: 0.00434045352621211\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04744918434487449\n",
      "Average test loss: 0.004293159703827567\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04738665629757775\n",
      "Average test loss: 0.004332138334711392\n",
      "Epoch 279/300\n",
      "Average training loss: 0.04739697060485681\n",
      "Average test loss: 0.00434591636620462\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04740500856108135\n",
      "Average test loss: 0.004274440253360404\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04729114930166139\n",
      "Average test loss: 0.004294486561583148\n",
      "Epoch 282/300\n",
      "Average training loss: 0.04736733010411263\n",
      "Average test loss: 0.004234999469377929\n",
      "Epoch 283/300\n",
      "Average training loss: 0.047337018602424195\n",
      "Average test loss: 0.004314449289606677\n",
      "Epoch 284/300\n",
      "Average training loss: 0.047367414734429784\n",
      "Average test loss: 0.004608616464460889\n",
      "Epoch 285/300\n",
      "Average training loss: 0.04730929584966766\n",
      "Average test loss: 0.004252008685635196\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04723934447434214\n",
      "Average test loss: 0.004378389687173897\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04724082217613856\n",
      "Average test loss: 0.004285996412961847\n",
      "Epoch 288/300\n",
      "Average training loss: 0.04721229643291897\n",
      "Average test loss: 0.004366841891573535\n",
      "Epoch 289/300\n",
      "Average training loss: 0.04722347484694587\n",
      "Average test loss: 0.004390064602924718\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0472181561589241\n",
      "Average test loss: 0.00446373626175854\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04724729846252335\n",
      "Average test loss: 0.004234942567845186\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04713922033045027\n",
      "Average test loss: 0.004212651912950807\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04714180502295494\n",
      "Average test loss: 0.004227487573193179\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04706944027543068\n",
      "Average test loss: 0.004229991297134095\n",
      "Epoch 295/300\n",
      "Average training loss: 0.047147355592913096\n",
      "Average test loss: 0.0042324016468806396\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04713003372483783\n",
      "Average test loss: 0.004312551856454875\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04711340994967355\n",
      "Average test loss: 0.0044550889618694784\n",
      "Epoch 298/300\n",
      "Average training loss: 0.047081638392474916\n",
      "Average test loss: 0.004236088215890858\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04708797301517593\n",
      "Average test loss: 0.004215462549279133\n",
      "Epoch 300/300\n",
      "Average training loss: 0.04703037249379688\n",
      "Average test loss: 0.00428451663793789\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.9569021731747521\n",
      "Average test loss: 0.004825064831309849\n",
      "Epoch 2/300\n",
      "Average training loss: 0.26888586571481493\n",
      "Average test loss: 0.004347666612515847\n",
      "Epoch 3/300\n",
      "Average training loss: 0.15964583567115995\n",
      "Average test loss: 0.004174183583921857\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1138803099989891\n",
      "Average test loss: 0.004041574860612552\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09234937025441063\n",
      "Average test loss: 0.003877494565728638\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08036692423290677\n",
      "Average test loss: 0.0037583650632037058\n",
      "Epoch 7/300\n",
      "Average training loss: 0.0727820429470804\n",
      "Average test loss: 0.00370557001274493\n",
      "Epoch 8/300\n",
      "Average training loss: 0.06791820705599255\n",
      "Average test loss: 0.003627251729162203\n",
      "Epoch 9/300\n",
      "Average training loss: 0.06450491579373678\n",
      "Average test loss: 0.0035888163368735047\n",
      "Epoch 10/300\n",
      "Average training loss: 0.06196995290782716\n",
      "Average test loss: 0.0035542954071942304\n",
      "Epoch 11/300\n",
      "Average training loss: 0.060014516214529676\n",
      "Average test loss: 0.003459768373105261\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05841041778855854\n",
      "Average test loss: 0.003788507098539008\n",
      "Epoch 13/300\n",
      "Average training loss: 0.057125321219364804\n",
      "Average test loss: 0.003389213673977388\n",
      "Epoch 14/300\n",
      "Average training loss: 0.056006117989619575\n",
      "Average test loss: 0.0033997533747719393\n",
      "Epoch 15/300\n",
      "Average training loss: 0.0550815919107861\n",
      "Average test loss: 0.0033466256445066796\n",
      "Epoch 16/300\n",
      "Average training loss: 0.054263604942295283\n",
      "Average test loss: 0.0033623325619846585\n",
      "Epoch 17/300\n",
      "Average training loss: 0.05352060140834914\n",
      "Average test loss: 0.003230343370801873\n",
      "Epoch 18/300\n",
      "Average training loss: 0.052896263702048196\n",
      "Average test loss: 0.0031924893535259695\n",
      "Epoch 19/300\n",
      "Average training loss: 0.052293415286474756\n",
      "Average test loss: 0.0031576370963205896\n",
      "Epoch 20/300\n",
      "Average training loss: 0.05170793960491816\n",
      "Average test loss: 0.0031478930993212592\n",
      "Epoch 21/300\n",
      "Average training loss: 0.05120137751102447\n",
      "Average test loss: 0.003146686073806551\n",
      "Epoch 22/300\n",
      "Average training loss: 0.05069478938645787\n",
      "Average test loss: 0.0032290153803510796\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0502879032989343\n",
      "Average test loss: 0.0030974496965193088\n",
      "Epoch 24/300\n",
      "Average training loss: 0.049799098395638994\n",
      "Average test loss: 0.0030936573752098615\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04934667869740062\n",
      "Average test loss: 0.0030598342642188074\n",
      "Epoch 26/300\n",
      "Average training loss: 0.047882823245392905\n",
      "Average test loss: 0.0030096017236096993\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04754042294952605\n",
      "Average test loss: 0.002989102436436547\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04716625923249457\n",
      "Average test loss: 0.0029744236930790875\n",
      "Epoch 32/300\n",
      "Average training loss: 0.046931911528110506\n",
      "Average test loss: 0.0029720143340528013\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04666099410586887\n",
      "Average training loss: 0.04611963520447413\n",
      "Average test loss: 0.0029489828172243305\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04585068769587411\n",
      "Average test loss: 0.0029686172186500497\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04558804942501916\n",
      "Average test loss: 0.0029410493984404536\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04532709151175287\n",
      "Average test loss: 0.0029332945940809116\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04515238182412253\n",
      "Average test loss: 0.0029832979362044067\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04494334967599975\n",
      "Average test loss: 0.002912601004458136\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04481694207919969\n",
      "Average test loss: 0.00289209415929185\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04450354317161772\n",
      "Average test loss: 0.002914581550906102\n",
      "Epoch 43/300\n",
      "Average training loss: 0.044336017128494054\n",
      "Average test loss: 0.0028986019268631936\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04418064040939013\n",
      "Average test loss: 0.002894835883958472\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04400666301780277\n",
      "Average test loss: 0.002949542214266128\n",
      "Epoch 46/300\n",
      "Average training loss: 0.043808305230405596\n",
      "Average test loss: 0.0028814025471607845\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0436566590335634\n",
      "Average test loss: 0.0029004628552744788\n",
      "Epoch 48/300\n",
      "Average training loss: 0.04354751827981737\n",
      "Average test loss: 0.002890913521250089\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04329692219860024\n",
      "Average test loss: 0.002888816734775901\n",
      "Epoch 50/300\n",
      "Average training loss: 0.043213164533178014\n",
      "Average test loss: 0.0029245239581084915\n",
      "Epoch 51/300\n",
      "Average training loss: 0.043055802441305586\n",
      "Average test loss: 0.0029314761097646424\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04290957289271884\n",
      "Average test loss: 0.0028931864452444844\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04271122545003891\n",
      "Average test loss: 0.0028632140898456177\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04258220544126299\n",
      "Average test loss: 0.002870204323902726\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04244545004102919\n",
      "Average test loss: 0.002865038876318269\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04228399024738206\n",
      "Average test loss: 0.002899006777960393\n",
      "Epoch 57/300\n",
      "Average training loss: 0.04188084751367569\n",
      "Average test loss: 0.002905176792293787\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04173165046175321\n",
      "Average test loss: 0.00289658575774067\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04155383222632938\n",
      "Average test loss: 0.002868894022372034\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04147421541478899\n",
      "Average test loss: 0.0028829925616996157\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04133579404155413\n",
      "Average test loss: 0.0028748584898809592\n",
      "Epoch 64/300\n",
      "Average training loss: 0.041224440541532306\n",
      "Average test loss: 0.0029074405949148867\n",
      "Epoch 65/300\n",
      "Average training loss: 0.041066322753826774\n",
      "Average test loss: 0.0029046762126187483\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04091121391124195\n",
      "Average test loss: 0.0029057679257045188\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04078345044453939\n",
      "Average test loss: 0.002887955412475599\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04067033626635869\n",
      "Average test loss: 0.0029158639810565445\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04054760991202461\n",
      "Average test loss: 0.0029402021223472225\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0404037052459187\n",
      "Average test loss: 0.0029476939514279364\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04030227442085743\n",
      "Average test loss: 0.00291578860167\n",
      "Epoch 72/300\n",
      "Average training loss: 0.040216703686449265\n",
      "Average test loss: 0.002911402949649427\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0400758425767223\n",
      "Average test loss: 0.003017288895530833\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03995292231109407\n",
      "Average test loss: 0.002953184619752897\n",
      "Epoch 75/300\n",
      "Average training loss: 0.039827005336682\n",
      "Average test loss: 0.002926106152642104\n",
      "Epoch 76/300\n",
      "Average training loss: 0.039715553234020866\n",
      "Average test loss: 0.0029703107544531426\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03959375056458844\n",
      "Average test loss: 0.002970289039942953\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039512290308872856\n",
      "Average test loss: 0.0029375453165007962\n",
      "Epoch 79/300\n",
      "Average training loss: 0.039405839721361795\n",
      "Average test loss: 0.0029738575340145165\n",
      "Epoch 80/300\n",
      "Average training loss: 0.039256579875946045\n",
      "Average test loss: 0.0030077404940707817\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03918733816345533\n",
      "Average test loss: 0.0030239003288249173\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03906192001038127\n",
      "Average test loss: 0.0029458012502226565\n",
      "Epoch 83/300\n",
      "Average training loss: 0.039001299500465395\n",
      "Average test loss: 0.0030847535510030058\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03890681786669625\n",
      "Average test loss: 0.002983032184963425\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03883529771367709\n",
      "Average test loss: 0.0029782233921190103\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03870508403248257\n",
      "Average test loss: 0.002984950559006797\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03856588996284538\n",
      "Average test loss: 0.0029803353862629996\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03855171537068155\n",
      "Average test loss: 0.0029651484705714715\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03852116440402137\n",
      "Average test loss: 0.003087132142856717\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03834755279620489\n",
      "Average test loss: 0.002961351443082094\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03825233057969146\n",
      "Average test loss: 0.0029535500903924306\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03816576878229777\n",
      "Average test loss: 0.0029881563801318406\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03812575653526518\n",
      "Average test loss: 0.0029969994104984735\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03801749307579464\n",
      "Average test loss: 0.0030197054683748217\n",
      "Epoch 95/300\n",
      "Average training loss: 0.037948756333854465\n",
      "Average test loss: 0.003002085399193068\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03786934724450111\n",
      "Average test loss: 0.002962607798063093\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03784863036208683\n",
      "Average test loss: 0.0030265267569985653\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03773280670411057\n",
      "Average test loss: 0.003049062724535664\n",
      "Epoch 99/300\n",
      "Average training loss: 0.03767890488604705\n",
      "Average test loss: 0.003117023280925221\n",
      "Epoch 100/300\n",
      "Average training loss: 0.037649309941464\n",
      "Average test loss: 0.0030144170108768677\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03751368445820279\n",
      "Average test loss: 0.0030698210505975617\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03739368056588703\n",
      "Average test loss: 0.003016670674706499\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03721782256662846\n",
      "Average test loss: 0.0029822187936968275\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03720794998937183\n",
      "Average test loss: 0.0030725895156049066\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03706314047508769\n",
      "Average test loss: 0.0029625804730587534\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03704965128832393\n",
      "Average test loss: 0.0030394971863263185\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03702932241724597\n",
      "Average test loss: 0.003058913170463509\n",
      "Epoch 110/300\n",
      "Average training loss: 0.036911501304970847\n",
      "Average test loss: 0.0030211181212216618\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03690464904076523\n",
      "Average test loss: 0.002989294728057252\n",
      "Epoch 112/300\n",
      "Average training loss: 0.036778683208756976\n",
      "Average test loss: 0.003019100481023391\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03678840136528015\n",
      "Average test loss: 0.002975239892800649\n",
      "Epoch 114/300\n",
      "Average training loss: 0.036767493430111146\n",
      "Average test loss: 0.0030977431231488783\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0366657721069124\n",
      "Average test loss: 0.002992179837905698\n",
      "Epoch 116/300\n",
      "Average training loss: 0.036561384323570464\n",
      "Average test loss: 0.003077437790731589\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03658432814810011\n",
      "Average test loss: 0.0030115808113995524\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03650373825762007\n",
      "Average test loss: 0.003023309492609567\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03650461910830604\n",
      "Average test loss: 0.0030092544328007434\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03638223470416334\n",
      "Average test loss: 0.00315056219490038\n",
      "Epoch 121/300\n",
      "Average training loss: 0.036270106666617924\n",
      "Average test loss: 0.0031053927164110874\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03632378099030919\n",
      "Average test loss: 0.0031240273629211716\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03627731773422824\n",
      "Average test loss: 0.003040195225427548\n",
      "Epoch 124/300\n",
      "Average training loss: 0.036189832955598834\n",
      "Average test loss: 0.003094603375221292\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03615496462252405\n",
      "Average test loss: 0.0030423966511670085\n",
      "Epoch 126/300\n",
      "Average training loss: 0.036027733175290955\n",
      "Average test loss: 0.0030707859761185115\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03602807587716315\n",
      "Average test loss: 0.003081946496748262\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03589789895547761\n",
      "Average test loss: 0.0030770223778155115\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0359373392826981\n",
      "Average test loss: 0.0030080401237226194\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03586461725831032\n",
      "Average test loss: 0.003009070804135667\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03585922616057926\n",
      "Average test loss: 0.0029963190205809145\n",
      "Epoch 134/300\n",
      "Average training loss: 0.035778832071357305\n",
      "Average test loss: 0.0031335798075629606\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03572564204202758\n",
      "Average test loss: 0.003031160462854637\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03561273473501205\n",
      "Average test loss: 0.003234081222779221\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03565078933868143\n",
      "Average test loss: 0.00304668213861684\n",
      "Epoch 138/300\n",
      "Average training loss: 0.035619120236900115\n",
      "Average test loss: 0.003131140196799404\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03555283176733388\n",
      "Average test loss: 0.0031897471079395875\n",
      "Epoch 140/300\n",
      "Average training loss: 0.035610481942693395\n",
      "Average test loss: 0.0031925565492775705\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03547529645429717\n",
      "Average test loss: 0.003099596617743373\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03543446343474918\n",
      "Average test loss: 0.003041036321471135\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03543116499980291\n",
      "Average test loss: 0.003103574851113889\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03541140757501125\n",
      "Average test loss: 0.0031189157693750327\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03533677221669091\n",
      "Average test loss: 0.003069045243681305\n",
      "Epoch 146/300\n",
      "Average training loss: 0.035355499035782285\n",
      "Average test loss: 0.0030067678511970573\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03526371803383033\n",
      "Average test loss: 0.0030557003574859763\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03524699445565541\n",
      "Average test loss: 0.0031001683053457074\n",
      "Epoch 149/300\n",
      "Average training loss: 0.035212210685014726\n",
      "Average test loss: 0.0030735330275363393\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03521931298904949\n",
      "Average test loss: 0.0030886057207567823\n",
      "Epoch 151/300\n",
      "Average training loss: 0.035097821487320796\n",
      "Average test loss: 0.0030742433565772243\n",
      "Epoch 154/300\n",
      "Average training loss: 0.035063673815793464\n",
      "Average test loss: 0.0031148227498763136\n",
      "Epoch 155/300\n",
      "Average training loss: 0.034989299522505866\n",
      "Average test loss: 0.0030969883840945033\n",
      "Epoch 156/300\n",
      "Average training loss: 0.03499183013704088\n",
      "Average test loss: 0.0030986299423707855\n",
      "Epoch 157/300\n",
      "Average training loss: 0.034958612324463\n",
      "Average test loss: 0.003139319276644124\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03498080515861511\n",
      "Average test loss: 0.0031711677975124783\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03482021587093671\n",
      "Average test loss: 0.0030847911309036945\n",
      "Epoch 160/300\n",
      "Average training loss: 0.034822354778647424\n",
      "Average test loss: 0.0030520911453705696\n",
      "Epoch 161/300\n",
      "Average training loss: 0.034821885469886994\n",
      "Average test loss: 0.0030923131302826932\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03480117416050699\n",
      "Average test loss: 0.003088560755054156\n",
      "Epoch 163/300\n",
      "Average training loss: 0.034856149489680924\n",
      "Average test loss: 0.003134657302664386\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03477093855374389\n",
      "Average test loss: 0.0030812397131489383\n",
      "Epoch 165/300\n",
      "Average training loss: 0.034741848188969825\n",
      "Average test loss: 0.0031593296887973943\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03465457640422715\n",
      "Average test loss: 0.0031066093944634\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03472043760286437\n",
      "Average test loss: 0.003117625941005018\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03467640354070398\n",
      "Average test loss: 0.003252125345170498\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03462912396258778\n",
      "Average test loss: 0.0030788497047291863\n",
      "Epoch 170/300\n",
      "Average training loss: 0.034589542312754525\n",
      "Average test loss: 0.003160753289030658\n",
      "Epoch 171/300\n",
      "Average training loss: 0.034520976586474315\n",
      "Average test loss: 0.003121918415029844\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0345007676978906\n",
      "Average test loss: 0.0032188129752046533\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03439568990468979\n",
      "Average test loss: 0.0032163075171411037\n",
      "Epoch 176/300\n",
      "Average training loss: 0.034417271925343405\n",
      "Average test loss: 0.0030554551749179763\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03438660679592027\n",
      "Average test loss: 0.0030914030892567504\n",
      "Epoch 178/300\n",
      "Average training loss: 0.03436279681656096\n",
      "Average test loss: 0.0031318817442903915\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03444041663077142\n",
      "Average test loss: 0.0031778989961991706\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0343505347304874\n",
      "Average test loss: 0.003118015728270014\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03431997793416182\n",
      "Average test loss: 0.0031619447875354026\n",
      "Epoch 182/300\n",
      "Average training loss: 0.034307235211133956\n",
      "Average test loss: 0.0030865118329723674\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03428994521001975\n",
      "Average test loss: 0.003135332632395956\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03424653246998787\n",
      "Average test loss: 0.003242846579187446\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03425605119268099\n",
      "Average test loss: 0.003163809740295013\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03418261108795802\n",
      "Average test loss: 0.003131987828347418\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03416878485514058\n",
      "Average test loss: 0.003158286703750491\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0341423558195432\n",
      "Average test loss: 0.0031674922042422824\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03418180154263973\n",
      "Average test loss: 0.003126227451281415\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03407505800326665\n",
      "Average test loss: 0.0030876159067783093\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03410698557727867\n",
      "Average test loss: 0.003209024716168642\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03406204411553012\n",
      "Average test loss: 0.0031611892502341004\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03400539664758576\n",
      "Average test loss: 0.003116382133215666\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03404382764465279\n",
      "Average test loss: 0.003168147661619716\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03393629546629058\n",
      "Average test loss: 0.003129579802354177\n",
      "Epoch 196/300\n",
      "Average training loss: 0.033972080088324016\n",
      "Average test loss: 0.003175162458585368\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03394591936137941\n",
      "Average test loss: 0.0031000398478160302\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03393296153181129\n",
      "Average test loss: 0.0031563414607404007\n",
      "Epoch 199/300\n",
      "Average training loss: 0.033882535191045865\n",
      "Average test loss: 0.003182863366479675\n",
      "Epoch 200/300\n",
      "Average training loss: 0.033877930021948284\n",
      "Average test loss: 0.0030905349254608154\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0338142710874478\n",
      "Average test loss: 0.003212863711433278\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03380994775891304\n",
      "Average test loss: 0.0030447444001005753\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03390809495581521\n",
      "Average test loss: 0.0031625024285167457\n",
      "Epoch 204/300\n",
      "Average training loss: 0.033807616899410885\n",
      "Average test loss: 0.003175658255712026\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03377137848072582\n",
      "Average test loss: 0.0031129885057194365\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03371246954301993\n",
      "Average test loss: 0.003131030780987607\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03369146006637149\n",
      "Average test loss: 0.0031095247380435466\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03375440755486488\n",
      "Average test loss: 0.003167805026802752\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0336875332362122\n",
      "Average test loss: 0.0032764385951062043\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0336265148056878\n",
      "Average test loss: 0.0031133442256185743\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03361263027456072\n",
      "Average test loss: 0.003203389894631174\n",
      "Epoch 212/300\n",
      "Average training loss: 0.033578838096724614\n",
      "Average test loss: 0.003191207909542653\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03366168523497052\n",
      "Average test loss: 0.0031604688816393414\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03359314194156064\n",
      "Average test loss: 0.0031835143864154818\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03356241559320026\n",
      "Average test loss: 0.003200437255617645\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03354110187623236\n",
      "Average test loss: 0.0033409132979189357\n",
      "Epoch 217/300\n",
      "Average training loss: 0.033580925345420835\n",
      "Average test loss: 0.003180863948435419\n",
      "Epoch 218/300\n",
      "Average training loss: 0.033476418170664046\n",
      "Average test loss: 0.0032816737335589195\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03349546954698033\n",
      "Average test loss: 0.00318725707112915\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03340973010659218\n",
      "Average test loss: 0.0031103868877722157\n",
      "Epoch 223/300\n",
      "Average training loss: 0.033474476998051006\n",
      "Average test loss: 0.0031474244501441717\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03341482426226139\n",
      "Average test loss: 0.0031600750436385472\n",
      "Epoch 225/300\n",
      "Average training loss: 0.033414137416415746\n",
      "Average test loss: 0.0031420018478400175\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03338431891798973\n",
      "Average test loss: 0.0032489343761569925\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03338704768485493\n",
      "Average test loss: 0.003170662802954515\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03333233736621009\n",
      "Average test loss: 0.0033848390537831517\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03332917935649554\n",
      "Average test loss: 0.0031995403779049715\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03335224460230933\n",
      "Average test loss: 0.003215683871259292\n",
      "Epoch 231/300\n",
      "Average training loss: 0.033203352060582905\n",
      "Average test loss: 0.0032223532212277252\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03331219245990118\n",
      "Average test loss: 0.003282398926715056\n",
      "Epoch 233/300\n",
      "Average training loss: 0.033233805581927296\n",
      "Average test loss: 0.003177198881904284\n",
      "Epoch 234/300\n",
      "Average training loss: 0.033292445692751145\n",
      "Average test loss: 0.0032300809563861953\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03326973048183653\n",
      "Average test loss: 0.0033117672618892457\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03323875150084495\n",
      "Average test loss: 0.0031920499437385133\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03318369539909893\n",
      "Average test loss: 0.003165348737190167\n",
      "Epoch 238/300\n",
      "Average training loss: 0.03321451813810401\n",
      "Average test loss: 0.0031452970715860526\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03317339638206694\n",
      "Average test loss: 0.0032164251255906292\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0331562695701917\n",
      "Average test loss: 0.003236787456398209\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03312046198050181\n",
      "Average test loss: 0.0032152515914705064\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0331724654270543\n",
      "Average test loss: 0.0031040054224431516\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03310998401045799\n",
      "Average test loss: 0.0031849985635942885\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03306175787581338\n",
      "Average test loss: 0.0037343643315964274\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03315708692371845\n",
      "Average test loss: 0.003120503396209743\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03303968813021978\n",
      "Average test loss: 0.003230966868189474\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03300805682937304\n",
      "Average test loss: 0.003129825730290678\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03303318679995007\n",
      "Average test loss: 0.003278677365018262\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03300700139668253\n",
      "Average test loss: 0.003100460492902332\n",
      "Epoch 250/300\n",
      "Average training loss: 0.033031998871101274\n",
      "Average test loss: 0.0032960533361054128\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03295199136932691\n",
      "Average test loss: 0.0033489460282855564\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03303685981863075\n",
      "Average test loss: 0.0031960629460712274\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03292399240533511\n",
      "Average test loss: 0.003183417704163326\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03295875084731314\n",
      "Average test loss: 0.003221983867386977\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03294117232494884\n",
      "Average test loss: 0.0032380312403870954\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03294588715997007\n",
      "Average test loss: 0.0032674587082324757\n",
      "Epoch 257/300\n",
      "Average training loss: 0.032888460204005245\n",
      "Average test loss: 0.0032258723568585183\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03291144512428178\n",
      "Average test loss: 0.0031417795999182595\n",
      "Epoch 262/300\n",
      "Average training loss: 0.032873709105783036\n",
      "Average test loss: 0.0032788036142786343\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03278040576312277\n",
      "Average test loss: 0.0031897562390400306\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03276189095692502\n",
      "Average test loss: 0.0032291695322427486\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03280497180587716\n",
      "Average test loss: 0.003208191432058811\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03271508743365606\n",
      "Average test loss: 0.0033959489195711083\n",
      "Epoch 267/300\n",
      "Average training loss: 0.032803471442725926\n",
      "Average test loss: 0.003275195378603207\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03274374947779708\n",
      "Average test loss: 0.0032134698937750523\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03277085381746292\n",
      "Average test loss: 0.003346680298033688\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03274839583039284\n",
      "Average test loss: 0.0032917430359456273\n",
      "Epoch 271/300\n",
      "Average training loss: 0.032773612805538706\n",
      "Average test loss: 0.0031743438467383383\n",
      "Epoch 272/300\n",
      "Average training loss: 0.032678876840406\n",
      "Average test loss: 0.0031219362633095847\n",
      "Epoch 273/300\n",
      "Average training loss: 0.0326933015551832\n",
      "Average test loss: 0.003234235116591056\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0326795525153478\n",
      "Average test loss: 0.003236877150626646\n",
      "Epoch 275/300\n",
      "Average training loss: 0.032636855453252796\n",
      "Average test loss: 0.0033180686943233015\n",
      "Epoch 276/300\n",
      "Average training loss: 0.03263279902935028\n",
      "Average test loss: 0.003243400795178281\n",
      "Epoch 277/300\n",
      "Average training loss: 0.032625293796261154\n",
      "Average test loss: 0.003163214241464933\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03263244063821104\n",
      "Average test loss: 0.003188435244063536\n",
      "Epoch 279/300\n",
      "Average training loss: 0.032667082270814315\n",
      "Average test loss: 0.003235436713736918\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03261212119956811\n",
      "Average test loss: 0.0032689247706698048\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03255632704165247\n",
      "Average test loss: 0.0032185578387644554\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03270000929799345\n",
      "Average test loss: 0.003222244361208545\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03259540098408858\n",
      "Average test loss: 0.0032089874669909476\n",
      "Epoch 285/300\n",
      "Average training loss: 0.032560910562674204\n",
      "Average test loss: 0.0031624045428923437\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03250320322646035\n",
      "Average test loss: 0.0031671870357046525\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03254890474677086\n",
      "Average test loss: 0.003348922910789649\n",
      "Epoch 288/300\n",
      "Average training loss: 0.032512126112977664\n",
      "Average test loss: 0.003260775853155388\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03255439769890573\n",
      "Average test loss: 0.0032626594699298344\n",
      "Epoch 290/300\n",
      "Average training loss: 0.032496824108892014\n",
      "Average test loss: 0.003084376853166355\n",
      "Epoch 291/300\n",
      "Average training loss: 0.032443265448013944\n",
      "Average test loss: 0.0031787010247094766\n",
      "Epoch 292/300\n",
      "Average training loss: 0.032490482868419754\n",
      "Average test loss: 0.003207917407568958\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03247060393790404\n",
      "Average test loss: 0.0031928950063884257\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03238818057378133\n",
      "Average test loss: 0.00310380802427729\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03238629424240854\n",
      "Average test loss: 0.0032543350379500126\n",
      "Epoch 296/300\n",
      "Average training loss: 0.032391742698020407\n",
      "Average test loss: 0.0032331102246211637\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0323652946965562\n",
      "Average test loss: 0.0032271621686716873\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03238170324265957\n",
      "Average test loss: 0.0031921531549758382\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03240203074779775\n",
      "Average test loss: 0.0032150909196999337\n",
      "Epoch 300/300\n",
      "Average training loss: 0.2000104598402977\n",
      "Average test loss: 0.0037920302641060617\n",
      "Epoch 3/300\n",
      "Average training loss: 0.12731781618462668\n",
      "Average test loss: 0.0035261491684036124\n",
      "Epoch 4/300\n",
      "Average training loss: 0.09745310060183207\n",
      "Average test loss: 0.0033466727866066828\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0814280968705813\n",
      "Average test loss: 0.003194296285510063\n",
      "Epoch 6/300\n",
      "Average training loss: 0.07140164430273904\n",
      "Average test loss: 0.0030940696194560993\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06435023470057381\n",
      "Average test loss: 0.002996413614186976\n",
      "Epoch 8/300\n",
      "Average training loss: 0.059178994685411455\n",
      "Average test loss: 0.002921045477605528\n",
      "Epoch 9/300\n",
      "Average training loss: 0.05520742578638924\n",
      "Average test loss: 0.0029077263553109432\n",
      "Epoch 10/300\n",
      "Average training loss: 0.052212773700555166\n",
      "Average test loss: 0.0028149399041301672\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04999760346280204\n",
      "Average test loss: 0.002712914301496413\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0482673636774222\n",
      "Average test loss: 0.002789854323491454\n",
      "Epoch 13/300\n",
      "Average training loss: 0.046855973644389044\n",
      "Average test loss: 0.002589938278310001\n",
      "Epoch 14/300\n",
      "Average training loss: 0.045654149558809067\n",
      "Average test loss: 0.002658965968216459\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04454433951775233\n",
      "Average test loss: 0.002569200746508108\n",
      "Epoch 16/300\n",
      "Average training loss: 0.043706199791696335\n",
      "Average test loss: 0.002443199273198843\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04290132282508744\n",
      "Average training loss: 0.04082343085275756\n",
      "Average test loss: 0.0023408750132140185\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04027787180741628\n",
      "Average test loss: 0.0023777100718062787\n",
      "Epoch 22/300\n",
      "Average training loss: 0.039698257105218036\n",
      "Average test loss: 0.002312676376249227\n",
      "Epoch 23/300\n",
      "Average training loss: 0.039155111306243474\n",
      "Average test loss: 0.002279607176159819\n",
      "Epoch 24/300\n",
      "Average training loss: 0.03859475943446159\n",
      "Average test loss: 0.0023022403594934278\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03812975090079837\n",
      "Average test loss: 0.0023151555417312516\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03764526074462467\n",
      "Average test loss: 0.002270150629182657\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03727642869452635\n",
      "Average test loss: 0.002265335628969802\n",
      "Epoch 28/300\n",
      "Average training loss: 0.03680052859418922\n",
      "Average test loss: 0.002232015425339341\n",
      "Epoch 29/300\n",
      "Average training loss: 0.03641769808861944\n",
      "Average test loss: 0.0022132683348738483\n",
      "Epoch 30/300\n",
      "Average training loss: 0.036098679388562836\n",
      "Average test loss: 0.0022002096363446778\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03575213230815199\n",
      "Average test loss: 0.0021696133270031875\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03540063843131065\n",
      "Average test loss: 0.00216908932187491\n",
      "Epoch 33/300\n",
      "Average training loss: 0.035131643076737724\n",
      "Average test loss: 0.002174452658949627\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03485931352774302\n",
      "Average test loss: 0.0021529156364914442\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03455077545510398\n",
      "Average test loss: 0.0021583929219179685\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03435889015926255\n",
      "Average test loss: 0.002141399166650242\n",
      "Epoch 37/300\n",
      "Average training loss: 0.034083086613151764\n",
      "Average test loss: 0.002131144102766282\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03354425739745299\n",
      "Average test loss: 0.0021277867677725023\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03324946311778492\n",
      "Average test loss: 0.0020970494398433303\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0330948723902305\n",
      "Average test loss: 0.0020922275841650036\n",
      "Epoch 43/300\n",
      "Average training loss: 0.032873170980148844\n",
      "Average test loss: 0.002128618380261792\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03280331420898437\n",
      "Average test loss: 0.0020859332947681347\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03251819258266025\n",
      "Average test loss: 0.002107430543957485\n",
      "Epoch 46/300\n",
      "Average training loss: 0.032418850036131014\n",
      "Average test loss: 0.002115034421802395\n",
      "Epoch 47/300\n",
      "Average training loss: 0.03222576990061336\n",
      "Average test loss: 0.0020923075921212633\n",
      "Epoch 48/300\n",
      "Average training loss: 0.032132860423790086\n",
      "Average test loss: 0.002077338298368785\n",
      "Epoch 49/300\n",
      "Average training loss: 0.031994438514113425\n",
      "Average test loss: 0.0021300702623816\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03182152134180069\n",
      "Average test loss: 0.0020815994383560285\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03176772105859386\n",
      "Average test loss: 0.0021006780840042566\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03152464600072967\n",
      "Average test loss: 0.0021137492404215866\n",
      "Epoch 53/300\n",
      "Average training loss: 0.031408138230443\n",
      "Average test loss: 0.00207676124241617\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03127426070471605\n",
      "Average test loss: 0.002091366940074497\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03113613310125139\n",
      "Average test loss: 0.0020879397806194094\n",
      "Epoch 56/300\n",
      "Average training loss: 0.031081224009394646\n",
      "Average test loss: 0.0021371569387200807\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03090964872472816\n",
      "Average test loss: 0.002078313594063123\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030810324256618817\n",
      "Average test loss: 0.0021037373141282135\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030713111105892393\n",
      "Average test loss: 0.002070978183195823\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030526490322417683\n",
      "Average test loss: 0.0020604210868477822\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030430569344096714\n",
      "Average test loss: 0.0021116449328967266\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03008492286172178\n",
      "Average test loss: 0.0020785069453219574\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0299876080652078\n",
      "Average test loss: 0.002212827881384227\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02982563853263855\n",
      "Average test loss: 0.0022392131878683965\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02980931721131007\n",
      "Average test loss: 0.0021501913306613764\n",
      "Epoch 68/300\n",
      "Average training loss: 0.029639065321948793\n",
      "Average test loss: 0.0021488383824212684\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02957330399254958\n",
      "Average test loss: 0.002160338251437578\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02945889797144466\n",
      "Average test loss: 0.0020874910313222143\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02929250812696086\n",
      "Average test loss: 0.002147102450744973\n",
      "Epoch 72/300\n",
      "Average training loss: 0.029198714506294993\n",
      "Average test loss: 0.0021250434359535576\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02915376696983973\n",
      "Average test loss: 0.002273189653745956\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02907475975818104\n",
      "Average test loss: 0.0021252786697198945\n",
      "Epoch 75/300\n",
      "Average training loss: 0.028955159379376306\n",
      "Average test loss: 0.0021282135370290943\n",
      "Epoch 76/300\n",
      "Average training loss: 0.028877480619483524\n",
      "Average test loss: 0.0021483015827834604\n",
      "Epoch 77/300\n",
      "Average training loss: 0.02881391886704498\n",
      "Average test loss: 0.002141729607557257\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02868518848054939\n",
      "Average test loss: 0.0021153238897936213\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02859001890156004\n",
      "Average test loss: 0.002160444458325704\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02854547628429201\n",
      "Average test loss: 0.0021233075748508176\n",
      "Epoch 81/300\n",
      "Average training loss: 0.028453743654820653\n",
      "Average test loss: 0.002288790580920047\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028364596141709223\n",
      "Average test loss: 0.00213381929602474\n",
      "Epoch 83/300\n",
      "Average training loss: 0.028339314825005003\n",
      "Average test loss: 0.0021552157673156926\n",
      "Epoch 84/300\n",
      "Average training loss: 0.028192022947801484\n",
      "Average test loss: 0.0021387690792067184\n",
      "Epoch 85/300\n",
      "Average training loss: 0.028123680412769317\n",
      "Average test loss: 0.002116777079593804\n",
      "Epoch 86/300\n",
      "Average training loss: 0.027893245942890645\n",
      "Average test loss: 0.0022181681295235954\n",
      "Epoch 89/300\n",
      "Average training loss: 0.027807164553138943\n",
      "Average test loss: 0.0021396865271445776\n",
      "Epoch 90/300\n",
      "Average training loss: 0.027778511767586073\n",
      "Average test loss: 0.0022058267104956837\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02772552588582039\n",
      "Average test loss: 0.002172955834513737\n",
      "Epoch 92/300\n",
      "Average training loss: 0.027651144521103965\n",
      "Average test loss: 0.002142296548415389\n",
      "Epoch 93/300\n",
      "Average training loss: 0.027543226975533696\n",
      "Average test loss: 0.0021530366998372807\n",
      "Epoch 94/300\n",
      "Average training loss: 0.027477422619859377\n",
      "Average test loss: 0.0021431656101097664\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02740478471418222\n",
      "Average test loss: 0.0021414942116373117\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02724294806354576\n",
      "Average test loss: 0.0023857696817980874\n",
      "Epoch 99/300\n",
      "Average training loss: 0.027213060499893294\n",
      "Average test loss: 0.0021208708211779594\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02715113786690765\n",
      "Average test loss: 0.0021451971719248426\n",
      "Epoch 101/300\n",
      "Average training loss: 0.027038344568676417\n",
      "Average test loss: 0.002209761269804504\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02703972760670715\n",
      "Average test loss: 0.002236490139530765\n",
      "Epoch 103/300\n",
      "Average training loss: 0.026982012473874623\n",
      "Average test loss: 0.0021915004059879315\n",
      "Epoch 104/300\n",
      "Average training loss: 0.026934456487496695\n",
      "Average test loss: 0.002139200577357163\n",
      "Epoch 105/300\n",
      "Average training loss: 0.026818916918502914\n",
      "Average test loss: 0.002158333166916337\n",
      "Epoch 106/300\n",
      "Average training loss: 0.026796471903721492\n",
      "Average test loss: 0.002241849140367574\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0267971769389179\n",
      "Average test loss: 0.0021980707173546157\n",
      "Epoch 108/300\n",
      "Average training loss: 0.02670355788535542\n",
      "Average test loss: 0.0021976399504476125\n",
      "Epoch 109/300\n",
      "Average training loss: 0.026688904489080113\n",
      "Average test loss: 0.002212247928811444\n",
      "Epoch 110/300\n",
      "Average training loss: 0.02661208252939913\n",
      "Average test loss: 0.0022311138477590347\n",
      "Epoch 111/300\n",
      "Average training loss: 0.026604000161091487\n",
      "Average test loss: 0.0021607684896637995\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02654493576122655\n",
      "Average test loss: 0.002182687234133482\n",
      "Epoch 113/300\n",
      "Average training loss: 0.026472305845883157\n",
      "Average test loss: 0.0021793179458214178\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02645472239289019\n",
      "Average test loss: 0.0021921637303506333\n",
      "Epoch 115/300\n",
      "Average training loss: 0.026395451297362645\n",
      "Average test loss: 0.0022663964353915717\n",
      "Epoch 116/300\n",
      "Average training loss: 0.02641686591340436\n",
      "Average test loss: 0.0021491302258024613\n",
      "Epoch 117/300\n",
      "Average training loss: 0.026307753834459515\n",
      "Average test loss: 0.002161440106677926\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02621430017054081\n",
      "Average test loss: 0.002224854035096036\n",
      "Epoch 119/300\n",
      "Average training loss: 0.026141303357978662\n",
      "Average test loss: 0.0023000669605616067\n",
      "Epoch 123/300\n",
      "Average training loss: 0.026099294554856087\n",
      "Average test loss: 0.0022242874562119442\n",
      "Epoch 124/300\n",
      "Average training loss: 0.026062266363037958\n",
      "Average test loss: 0.0022044697230060896\n",
      "Epoch 125/300\n",
      "Average training loss: 0.026020595166418286\n",
      "Average test loss: 0.002216555899216069\n",
      "Epoch 126/300\n",
      "Average training loss: 0.025949332276980083\n",
      "Average test loss: 0.002242495666982399\n",
      "Epoch 127/300\n",
      "Average training loss: 0.025917129256659083\n",
      "Average test loss: 0.0022361850926859513\n",
      "Epoch 128/300\n",
      "Average training loss: 0.025908260936538377\n",
      "Average test loss: 0.0022503576448394194\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02585735141237577\n",
      "Average test loss: 0.0022098230487770506\n",
      "Epoch 130/300\n",
      "Average training loss: 0.025841030625833407\n",
      "Average test loss: 0.0022297189771715137\n",
      "Epoch 131/300\n",
      "Average training loss: 0.025845404603415067\n",
      "Average test loss: 0.002222768203355372\n",
      "Epoch 132/300\n",
      "Average training loss: 0.025799377335442436\n",
      "Average test loss: 0.002328146556806233\n",
      "Epoch 133/300\n",
      "Average training loss: 0.0257204253723224\n",
      "Average test loss: 0.0022325073308828805\n",
      "Epoch 134/300\n",
      "Average training loss: 0.025683912782205477\n",
      "Average test loss: 0.002282839730175005\n",
      "Epoch 135/300\n",
      "Average training loss: 0.025642223899563155\n",
      "Average test loss: 0.002325444226049715\n",
      "Epoch 136/300\n",
      "Average training loss: 0.025623117660482725\n",
      "Average test loss: 0.0022786140251490803\n",
      "Epoch 137/300\n",
      "Average training loss: 0.025629437314139473\n",
      "Average test loss: 0.0022714387389520803\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02561620458463828\n",
      "Average test loss: 0.002272211293379466\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02552252919640806\n",
      "Average test loss: 0.0022514389409787126\n",
      "Epoch 140/300\n",
      "Average training loss: 0.025494066160586144\n",
      "Average test loss: 0.0022983414680800506\n",
      "Epoch 141/300\n",
      "Average training loss: 0.025589413560099073\n",
      "Average test loss: 0.0022390753287408085\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02544360299077299\n",
      "Average test loss: 0.00221902156724698\n",
      "Epoch 143/300\n",
      "Average training loss: 0.025417220960060754\n",
      "Average test loss: 0.002231792276518212\n",
      "Epoch 144/300\n",
      "Average training loss: 0.025370175500710804\n",
      "Average test loss: 0.0023157643917948007\n",
      "Epoch 147/300\n",
      "Average training loss: 0.025330078817076152\n",
      "Average test loss: 0.0022984458131508696\n",
      "Epoch 148/300\n",
      "Average training loss: 0.025296722607480154\n",
      "Average test loss: 0.0022620629731358753\n",
      "Epoch 149/300\n",
      "Average training loss: 0.025315656372242503\n",
      "Average test loss: 0.002249484893762403\n",
      "Epoch 150/300\n",
      "Average training loss: 0.02523876115679741\n",
      "Average test loss: 0.0022452845777281454\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02521635843316714\n",
      "Average test loss: 0.0022415966401911442\n",
      "Epoch 152/300\n",
      "Average training loss: 0.025183593985107208\n",
      "Average test loss: 0.0022625989874617922\n",
      "Epoch 153/300\n",
      "Average training loss: 0.025186230291922886\n",
      "Average test loss: 0.002289346952819162\n",
      "Epoch 154/300\n",
      "Average training loss: 0.025122543023692236\n",
      "Average test loss: 0.002233653131251534\n",
      "Epoch 155/300\n",
      "Average training loss: 0.02513313887351089\n",
      "Average test loss: 0.0022794731745703354\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02511844091448519\n",
      "Average test loss: 0.0022752746186322638\n",
      "Epoch 157/300\n",
      "Average training loss: 0.02506625325812234\n",
      "Average test loss: 0.0022845823812401955\n",
      "Epoch 158/300\n",
      "Average training loss: 0.025036184302634663\n",
      "Average test loss: 0.0021976541293164094\n",
      "Epoch 159/300\n",
      "Average training loss: 0.025008654372559655\n",
      "Average test loss: 0.0023338075348486504\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02502511915564537\n",
      "Average test loss: 0.0022295748761130703\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02497416989836428\n",
      "Average test loss: 0.002304599419857065\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024942296367552547\n",
      "Average test loss: 0.002261610350675053\n",
      "Epoch 163/300\n",
      "Average training loss: 0.024908379246791203\n",
      "Average test loss: 0.00223317795412408\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02494152864648236\n",
      "Average test loss: 0.0023088342106590666\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024932327799499034\n",
      "Average test loss: 0.002339714891794655\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024855169367459084\n",
      "Average test loss: 0.0022175667192786933\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024862586259841918\n",
      "Average test loss: 0.0023271642698802883\n",
      "Epoch 168/300\n",
      "Average training loss: 0.024813961383369235\n",
      "Average test loss: 0.002244605256451501\n",
      "Epoch 169/300\n",
      "Average training loss: 0.02478663280275133\n",
      "Average test loss: 0.0022673602586405143\n",
      "Epoch 170/300\n",
      "Average test loss: 0.0023086401207579508\n",
      "Epoch 173/300\n",
      "Average training loss: 0.024685374284783998\n",
      "Average test loss: 0.0022661361280414795\n",
      "Epoch 174/300\n",
      "Average training loss: 0.024742831046382585\n",
      "Average test loss: 0.0022692998849476375\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024709123608138827\n",
      "Average test loss: 0.002324039219775134\n",
      "Epoch 176/300\n",
      "Average training loss: 0.02464200024969048\n",
      "Average test loss: 0.0023691541662232743\n",
      "Epoch 180/300\n",
      "Average training loss: 0.024611096476515135\n",
      "Average test loss: 0.0023830134931744802\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024590252581569882\n",
      "Average test loss: 0.002308421516273585\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02455021954410606\n",
      "Average test loss: 0.0022624157133201758\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024522585097286435\n",
      "Average test loss: 0.002246820872753031\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02455469004975425\n",
      "Average test loss: 0.0022436948381364345\n",
      "Epoch 185/300\n",
      "Average training loss: 0.024498122064603698\n",
      "Average test loss: 0.00229399723932147\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02446855365898874\n",
      "Average test loss: 0.002275965911646684\n",
      "Epoch 187/300\n",
      "Average training loss: 0.024481559877594313\n",
      "Average test loss: 0.0023209748337459234\n",
      "Epoch 188/300\n",
      "Average training loss: 0.024448711898591782\n",
      "Average test loss: 0.00229526885267761\n",
      "Epoch 189/300\n",
      "Average training loss: 0.024482724870244662\n",
      "Average test loss: 0.002397343575230075\n",
      "Epoch 190/300\n",
      "Average training loss: 0.024427278882927363\n",
      "Average test loss: 0.0023154744133353235\n",
      "Epoch 191/300\n",
      "Average training loss: 0.024356476330094867\n",
      "Average test loss: 0.0023974516835684576\n",
      "Epoch 192/300\n",
      "Average training loss: 0.024404642042186526\n",
      "Average test loss: 0.0022714196061715485\n",
      "Epoch 193/300\n",
      "Average training loss: 0.024352377009060646\n",
      "Average test loss: 0.0022764428425580264\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02437752400835355\n",
      "Average test loss: 0.002284198505803943\n",
      "Epoch 195/300\n",
      "Average training loss: 0.024341776976982754\n",
      "Average test loss: 0.0023335842452943327\n",
      "Epoch 196/300\n",
      "Average training loss: 0.0243459862023592\n",
      "Average test loss: 0.002244090093403227\n",
      "Epoch 197/300\n",
      "Average training loss: 0.024255119909842807\n",
      "Average test loss: 0.0023575600480867757\n",
      "Epoch 200/300\n",
      "Average training loss: 0.024236249374018776\n",
      "Average test loss: 0.002305088314745161\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02426411842306455\n",
      "Average test loss: 0.0023357176416450077\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02428431393371688\n",
      "Average test loss: 0.002276783074459268\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02424993241329988\n",
      "Average test loss: 0.002391929543370174\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02415485728118155\n",
      "Average test loss: 0.002318482548205389\n",
      "Epoch 205/300\n",
      "Average training loss: 0.0241941750165489\n",
      "Average test loss: 0.002303904881493913\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02422806387808588\n",
      "Average test loss: 0.0023100259700376126\n",
      "Epoch 207/300\n",
      "Average training loss: 0.024214118650390044\n",
      "Average test loss: 0.0023583970982581377\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02411034569144249\n",
      "Average test loss: 0.0022875813347183996\n",
      "Epoch 209/300\n",
      "Average training loss: 0.024121635855899916\n",
      "Average test loss: 0.0023062987364828587\n",
      "Epoch 210/300\n",
      "Average training loss: 0.024154481598072583\n",
      "Average test loss: 0.0022839175291980308\n",
      "Epoch 211/300\n",
      "Average training loss: 0.024160624931255977\n",
      "Average test loss: 0.0022749855107524327\n",
      "Epoch 212/300\n",
      "Average training loss: 0.024113896239135\n",
      "Average test loss: 0.002283672032256921\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02403824332025316\n",
      "Average test loss: 0.0023749282389051384\n",
      "Epoch 214/300\n",
      "Average training loss: 0.024041869786050586\n",
      "Average test loss: 0.0023260948763539394\n",
      "Epoch 215/300\n",
      "Average training loss: 0.023982515244020355\n",
      "Average test loss: 0.0023381262567515174\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02402087127831247\n",
      "Average test loss: 0.0023358938886473574\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02400808493627442\n",
      "Average test loss: 0.0023395508322864773\n",
      "Epoch 218/300\n",
      "Average training loss: 0.024049715815318954\n",
      "Average test loss: 0.002379822589043114\n",
      "Epoch 219/300\n",
      "Average training loss: 0.023994577061798836\n",
      "Average test loss: 0.002303319904125399\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02396080149213473\n",
      "Average test loss: 0.002327637574738926\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023963856405682035\n",
      "Average test loss: 0.0023466125254829723\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023910707821448645\n",
      "Average test loss: 0.0023033548324472375\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02388576756252183\n",
      "Average test loss: 0.002375432226806879\n",
      "Epoch 226/300\n",
      "Average training loss: 0.023921302856670486\n",
      "Average test loss: 0.0022868516147136688\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023897882213195165\n",
      "Average test loss: 0.0023435167585396104\n",
      "Epoch 228/300\n",
      "Average training loss: 0.023848965376615523\n",
      "Average test loss: 0.0023481096183467242\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023854157777296173\n",
      "Average test loss: 0.0023245765200505656\n",
      "Epoch 230/300\n",
      "Average training loss: 0.023822743445634842\n",
      "Average test loss: 0.00233029275553094\n",
      "Epoch 231/300\n",
      "Average training loss: 0.02384876823425293\n",
      "Average test loss: 0.0023315005648053356\n",
      "Epoch 232/300\n",
      "Average training loss: 0.023825351188580195\n",
      "Average test loss: 0.002376529557009538\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02383872183329529\n",
      "Average test loss: 0.002322036030391852\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023803272931112184\n",
      "Average test loss: 0.002284790240849058\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023760878880818685\n",
      "Average test loss: 0.0023467114217993286\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02379081750743919\n",
      "Average test loss: 0.0023796650022268294\n",
      "Epoch 237/300\n",
      "Average training loss: 0.023807466957304214\n",
      "Average test loss: 0.0023176318241490256\n",
      "Epoch 238/300\n",
      "Average training loss: 0.023775697716408306\n",
      "Average test loss: 0.002308625675737858\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023739845350384713\n",
      "Average test loss: 0.00232741352615671\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02377660334441397\n",
      "Average test loss: 0.0022721822328037686\n",
      "Epoch 241/300\n",
      "Average training loss: 0.023759283754560684\n",
      "Average test loss: 0.0023489083739825422\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02368994470437368\n",
      "Average test loss: 0.0023374286442995073\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023681471364365683\n",
      "Average test loss: 0.002329396202022003\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023697631478309632\n",
      "Average test loss: 0.0023418182095305785\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02366036235458321\n",
      "Average test loss: 0.0023575228574789233\n",
      "Epoch 248/300\n",
      "Average training loss: 0.02369481098320749\n",
      "Average test loss: 0.0023754429871526857\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02368856862352954\n",
      "Average test loss: 0.002350495030068689\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02362068663868639\n",
      "Average test loss: 0.0023123406935483216\n",
      "Epoch 251/300\n",
      "Average training loss: 0.023608532206879723\n",
      "Average test loss: 0.002287769460429748\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023607554437385665\n",
      "Average test loss: 0.002362529622390866\n",
      "Epoch 253/300\n",
      "Average training loss: 0.023576923424998918\n",
      "Average test loss: 0.002376168040558696\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023513712018728258\n",
      "Average test loss: 0.0023781966885758773\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023530855988462766\n",
      "Average test loss: 0.0023900748768614396\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02357492865787612\n",
      "Average test loss: 0.0023156076988412274\n",
      "Epoch 259/300\n",
      "Average training loss: 0.023518949353032642\n",
      "Average test loss: 0.0023145262348569103\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023534748532705835\n",
      "Average test loss: 0.002412636871346169\n",
      "Epoch 261/300\n",
      "Average training loss: 0.023540878070725337\n",
      "Average test loss: 0.002324301236205631\n",
      "Epoch 262/300\n",
      "Average training loss: 0.023458868195613224\n",
      "Average test loss: 0.0024357961296207374\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023467534944415093\n",
      "Average test loss: 0.002314594457960791\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02351030165453752\n",
      "Average test loss: 0.0023962658423309527\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023472084565295114\n",
      "Average test loss: 0.0022968423132681184\n",
      "Epoch 266/300\n",
      "Average training loss: 0.023480674493643973\n",
      "Average test loss: 0.002343141123652458\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023434904926353032\n",
      "Average test loss: 0.0023263911445521646\n",
      "Epoch 268/300\n",
      "Average training loss: 0.023472962649332154\n",
      "Average test loss: 0.0023651319181339607\n",
      "Epoch 269/300\n",
      "Average training loss: 0.023478636307020982\n",
      "Average test loss: 0.0023394031966519025\n",
      "Epoch 270/300\n",
      "Average training loss: 0.023420385614037514\n",
      "Average test loss: 0.0023659518525107866\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02339111889236503\n",
      "Average test loss: 0.002307384918133418\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02339328447729349\n",
      "Average test loss: 0.002361688995733857\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02337730081544982\n",
      "Average test loss: 0.0023779298228522143\n",
      "Epoch 274/300\n",
      "Average training loss: 0.023447175289193788\n",
      "Average test loss: 0.0023028333408551083\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02339634341498216\n",
      "Average test loss: 0.0024242044432709613\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02339309020174874\n",
      "Average test loss: 0.0023768944520917205\n",
      "Epoch 277/300\n",
      "Average training loss: 0.023355156910088326\n",
      "Average test loss: 0.0023769780613688957\n",
      "Epoch 278/300\n",
      "Average training loss: 0.023333795964717866\n",
      "Average test loss: 0.0022901672300779156\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023393033901850383\n",
      "Average test loss: 0.0023294308816807136\n",
      "Epoch 280/300\n",
      "Average training loss: 0.023392727750870916\n",
      "Average test loss: 0.0023673416862471236\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0232809303154548\n",
      "Average test loss: 0.00230270937561161\n",
      "Epoch 282/300\n",
      "Average training loss: 0.023316013776593737\n",
      "Average test loss: 0.0023955087763153846\n",
      "Epoch 283/300\n",
      "Average training loss: 0.023320390795667965\n",
      "Average test loss: 0.0023563510231259794\n",
      "Epoch 284/300\n",
      "Average training loss: 0.023285543272892634\n",
      "Average test loss: 0.002300851908615894\n",
      "Epoch 285/300\n",
      "Average training loss: 0.023301417339179252\n",
      "Average test loss: 0.0023976320758875872\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02326348961889744\n",
      "Average test loss: 0.002313916574542721\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02329983148475488\n",
      "Average test loss: 0.0023397461959264346\n",
      "Epoch 288/300\n",
      "Average training loss: 0.02323383192718029\n",
      "Average test loss: 0.0023653533349020615\n",
      "Epoch 289/300\n",
      "Average training loss: 0.0232338843892018\n",
      "Average test loss: 0.0023392272176634933\n",
      "Epoch 292/300\n",
      "Average training loss: 0.023219101457132234\n",
      "Average test loss: 0.0023634769391889373\n",
      "Epoch 295/300\n",
      "Average training loss: 0.023238309487700462\n",
      "Average test loss: 0.0023315655262105997\n",
      "Epoch 296/300\n",
      "Average training loss: 0.023201994318101143\n",
      "Average test loss: 0.002404286172447933\n",
      "Epoch 297/300\n",
      "Average training loss: 0.023171516954898833\n",
      "Average test loss: 0.0023947719666692945\n",
      "Epoch 298/300\n",
      "Average training loss: 0.023171472344133588\n",
      "Average test loss: 0.0023497115017639264\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.7968828027513292\n",
      "Average test loss: 0.003810748640447855\n",
      "Epoch 2/300\n",
      "Average training loss: 0.19626755266719395\n",
      "Average test loss: 0.0031982666463073756\n",
      "Epoch 3/300\n",
      "Average training loss: 0.11366982586516275\n",
      "Average test loss: 0.002886596524880992\n",
      "Epoch 4/300\n",
      "Average training loss: 0.08061151389280954\n",
      "Average test loss: 0.0027300087937878236\n",
      "Epoch 5/300\n",
      "Average training loss: 0.06544177460670471\n",
      "Average test loss: 0.0025590833909809587\n",
      "Epoch 6/300\n",
      "Average training loss: 0.057040099985069696\n",
      "Average test loss: 0.0024912629497961867\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05147097544206513\n",
      "Average test loss: 0.002447748128738668\n",
      "Epoch 8/300\n",
      "Average training loss: 0.047414012610912326\n",
      "Average test loss: 0.0022892469534029564\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04428275731537077\n",
      "Average test loss: 0.0021887343923250833\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04180105279551612\n",
      "Average test loss: 0.002328558348119259\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03976169527404838\n",
      "Average test loss: 0.0020471226705445185\n",
      "Epoch 12/300\n",
      "Average training loss: 0.038139936544828945\n",
      "Average test loss: 0.0019991896184575228\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0368760278655423\n",
      "Average test loss: 0.0019275677961607773\n",
      "Epoch 14/300\n",
      "Average training loss: 0.035783293205830784\n",
      "Average test loss: 0.0019177013395561113\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03476711356970999\n",
      "Average test loss: 0.001847629584475524\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03394968423081769\n",
      "Average test loss: 0.0018312426721677185\n",
      "Epoch 17/300\n",
      "Average training loss: 0.03314382786220974\n",
      "Average test loss: 0.0017677897629845474\n",
      "Epoch 18/300\n",
      "Average training loss: 0.03243476285040379\n",
      "Average test loss: 0.0018445279786570205\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03176092126303249\n",
      "Average test loss: 0.0017401375878188345\n",
      "Epoch 20/300\n",
      "Average training loss: 0.03110990300940143\n",
      "Average test loss: 0.0016821377278409071\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03048307122124566\n",
      "Average test loss: 0.001806727201367418\n",
      "Epoch 22/300\n",
      "Average training loss: 0.02841449692679776\n",
      "Average test loss: 0.0015949698190929162\n",
      "Epoch 26/300\n",
      "Average training loss: 0.027914942065874734\n",
      "Average test loss: 0.0016003344626062447\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02754912863009506\n",
      "Average test loss: 0.0015844842775000465\n",
      "Epoch 28/300\n",
      "Average training loss: 0.027138740352458425\n",
      "Average test loss: 0.0015749291419568988\n",
      "Epoch 29/300\n",
      "Average training loss: 0.026780149453216127\n",
      "Average test loss: 0.0015578084117215541\n",
      "Epoch 30/300\n",
      "Average training loss: 0.02640148183868991\n",
      "Average test loss: 0.0015411409895039268\n",
      "Epoch 31/300\n",
      "Average training loss: 0.026160151274667847\n",
      "Average test loss: 0.0015338395454196467\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02583288518091043\n",
      "Average test loss: 0.001533762977665497\n",
      "Epoch 33/300\n",
      "Average training loss: 0.025594735668765176\n",
      "Average test loss: 0.001518433785997331\n",
      "Epoch 34/300\n",
      "Average training loss: 0.025396329537034034\n",
      "Average test loss: 0.0015313078324413962\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02522414045366976\n",
      "Average test loss: 0.0015220156903378666\n",
      "Epoch 36/300\n",
      "Average training loss: 0.024958537455234263\n",
      "Average test loss: 0.0015038451525486178\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02475257593890031\n",
      "Average test loss: 0.0015011379135151705\n",
      "Epoch 38/300\n",
      "Average training loss: 0.024563614323735238\n",
      "Average test loss: 0.0015016603195625875\n",
      "Epoch 39/300\n",
      "Average training loss: 0.024371565292278925\n",
      "Average test loss: 0.001497771146810717\n",
      "Epoch 40/300\n",
      "Average training loss: 0.024218168972267046\n",
      "Average test loss: 0.0014773317819668187\n",
      "Epoch 41/300\n",
      "Average training loss: 0.024101102772686216\n",
      "Average test loss: 0.0014939351422298287\n",
      "Epoch 42/300\n",
      "Average training loss: 0.023953811488217777\n",
      "Average test loss: 0.001489840535653962\n",
      "Epoch 43/300\n",
      "Average training loss: 0.023842104279332692\n",
      "Average test loss: 0.001471897512363891\n",
      "Epoch 44/300\n",
      "Average training loss: 0.023625933991538155\n",
      "Average test loss: 0.0014802637564846211\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02354285239179929\n",
      "Average test loss: 0.0014678595441703995\n",
      "Epoch 46/300\n",
      "Average test loss: 0.0014836253070065544\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02308177139237523\n",
      "Average test loss: 0.0015488215784749224\n",
      "Epoch 50/300\n",
      "Average training loss: 0.022917655089663133\n",
      "Average test loss: 0.0015611085248076254\n",
      "Epoch 51/300\n",
      "Average training loss: 0.022830994609329434\n",
      "Average test loss: 0.0014699067578961451\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0229013728664981\n",
      "Average test loss: 0.0015162415864566961\n",
      "Epoch 53/300\n",
      "Average training loss: 0.022635667958193356\n",
      "Average test loss: 0.0014636759159879552\n",
      "Epoch 54/300\n",
      "Average training loss: 0.022514886238508756\n",
      "Average test loss: 0.0014658305847810374\n",
      "Epoch 55/300\n",
      "Average training loss: 0.022416047195593515\n",
      "Average test loss: 0.0014598861833413441\n",
      "Epoch 56/300\n",
      "Average training loss: 0.022355515473418765\n",
      "Average test loss: 0.0014891441006006467\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02226159947282738\n",
      "Average test loss: 0.0014859064956092172\n",
      "Epoch 58/300\n",
      "Average training loss: 0.022197756656342084\n",
      "Average test loss: 0.0014800807031699352\n",
      "Epoch 59/300\n",
      "Average training loss: 0.022056125359402764\n",
      "Average test loss: 0.0014822101367430555\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021996315489212673\n",
      "Average test loss: 0.001457998882772194\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021867304171125094\n",
      "Average test loss: 0.001488537276784579\n",
      "Epoch 62/300\n",
      "Average training loss: 0.021778170802527003\n",
      "Average test loss: 0.0014768318694291843\n",
      "Epoch 63/300\n",
      "Average training loss: 0.021679836354321903\n",
      "Average test loss: 0.0014560444498848584\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02160588779879941\n",
      "Average test loss: 0.001476838700266348\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02154606794979837\n",
      "Average test loss: 0.001472742598503828\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02150556730561786\n",
      "Average test loss: 0.0014631851981911394\n",
      "Epoch 67/300\n",
      "Average training loss: 0.021414957503477734\n",
      "Average test loss: 0.0014714119171516764\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02129033750295639\n",
      "Average test loss: 0.001465633044950664\n",
      "Epoch 69/300\n",
      "Average training loss: 0.021200407274895243\n",
      "Average test loss: 0.0015082041748488942\n",
      "Epoch 70/300\n",
      "Average training loss: 0.021118672907352448\n",
      "Average test loss: 0.0017240490487052335\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020934279536207516\n",
      "Average test loss: 0.001489296963541872\n",
      "Epoch 74/300\n",
      "Average training loss: 0.02083998207251231\n",
      "Average test loss: 0.0014809203576296567\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020767204182015525\n",
      "Average test loss: 0.0015179991157104572\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02069649581445588\n",
      "Average test loss: 0.0014780063277317417\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020645630914303992\n",
      "Average test loss: 0.0017509069146795404\n",
      "Epoch 78/300\n",
      "Average training loss: 0.020542011219594213\n",
      "Average test loss: 0.0015323513354071313\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02058683334953255\n",
      "Average test loss: 0.0014929284821781847\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02045815553598934\n",
      "Average test loss: 0.0014699954819969005\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02040798862857951\n",
      "Average test loss: 0.0014809554237872362\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020324832742412886\n",
      "Average test loss: 0.001506033990946081\n",
      "Epoch 83/300\n",
      "Average training loss: 0.020246507469978598\n",
      "Average test loss: 0.0015106573821976781\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020194750911659665\n",
      "Average test loss: 0.0015035937870335248\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020142238246070015\n",
      "Average test loss: 0.001519036033604708\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02012474729783005\n",
      "Average test loss: 0.0016191562669765618\n",
      "Epoch 87/300\n",
      "Average training loss: 0.020004206580420337\n",
      "Average test loss: 0.001514914830836157\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01997611841145489\n",
      "Average test loss: 0.0015313837927662664\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019927865457203653\n",
      "Average test loss: 0.0015219321941129036\n",
      "Epoch 90/300\n",
      "Average training loss: 0.019861197234855757\n",
      "Average test loss: 0.0015156549077687991\n",
      "Epoch 91/300\n",
      "Average training loss: 0.019812774584525163\n",
      "Average test loss: 0.001534706481732428\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019739281515280406\n",
      "Average test loss: 0.0015256885800303684\n",
      "Epoch 94/300\n",
      "Average training loss: 0.019650638214415973\n",
      "Average test loss: 0.0015560373234459095\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019513085845443937\n",
      "Average test loss: 0.0015215069250100188\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01944575888911883\n",
      "Average test loss: 0.0015068418050391806\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019434850917922127\n",
      "Average test loss: 0.0015120765967294573\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01940232985549503\n",
      "Average test loss: 0.001508743727579713\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01932508472104867\n",
      "Average test loss: 0.0014973428105521534\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019306038003000948\n",
      "Average test loss: 0.001529575781068868\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019261416620678373\n",
      "Average test loss: 0.0015119220305544634\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019233444514373938\n",
      "Average test loss: 0.001546294732329746\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019169800645775264\n",
      "Average test loss: 0.0015543116974747844\n",
      "Epoch 106/300\n",
      "Average training loss: 0.01914834221535259\n",
      "Average test loss: 0.0015632123114127253\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019108717464738422\n",
      "Average test loss: 0.0016054858818857207\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019069004769126575\n",
      "Average test loss: 0.0016520360870700744\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019047415601710478\n",
      "Average test loss: 0.0015858617204551895\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0189946827110317\n",
      "Average test loss: 0.0016060458674199052\n",
      "Epoch 111/300\n",
      "Average training loss: 0.018931034278538494\n",
      "Average test loss: 0.0015456776279542182\n",
      "Epoch 112/300\n",
      "Average training loss: 0.018912563402205707\n",
      "Average test loss: 0.0015832199584692717\n",
      "Epoch 113/300\n",
      "Average training loss: 0.018878050813244448\n",
      "Average test loss: 0.0015879981411207054\n",
      "Epoch 114/300\n",
      "Average training loss: 0.018883523224128618\n",
      "Average test loss: 0.0015417635884239441\n",
      "Epoch 115/300\n",
      "Average training loss: 0.018883043158385487\n",
      "Average test loss: 0.0015770136424754228\n",
      "Epoch 116/300\n",
      "Average training loss: 0.018811910700466897\n",
      "Average test loss: 0.001556907403593262\n",
      "Epoch 117/300\n",
      "Average training loss: 0.01881941296160221\n",
      "Average test loss: 0.0015670323591265413\n",
      "Epoch 118/300\n",
      "Average training loss: 0.018740831058886315\n",
      "Average test loss: 0.001569255838325868\n",
      "Epoch 119/300\n",
      "Average training loss: 0.018606463621060054\n",
      "Average test loss: 0.0015541954880787267\n",
      "Epoch 122/300\n",
      "Average training loss: 0.018626689126094182\n",
      "Average test loss: 0.0015954826863275634\n",
      "Epoch 123/300\n",
      "Average training loss: 0.018593298122286796\n",
      "Average test loss: 0.0015449488358572126\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0185357918722762\n",
      "Average test loss: 0.0015524209189332193\n",
      "Epoch 127/300\n",
      "Average training loss: 0.018489873612920444\n",
      "Average test loss: 0.0016303975505547392\n",
      "Epoch 128/300\n",
      "Average training loss: 0.018456920188334253\n",
      "Average test loss: 0.0015492449287946026\n",
      "Epoch 129/300\n",
      "Average training loss: 0.018430886432528496\n",
      "Average test loss: 0.0015577571315483914\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01839379894402292\n",
      "Average test loss: 0.0015710537048677603\n",
      "Epoch 131/300\n",
      "Average training loss: 0.018329877349237602\n",
      "Average test loss: 0.001572164545663529\n",
      "Epoch 132/300\n",
      "Average training loss: 0.018347878739237787\n",
      "Average test loss: 0.0015935562184701364\n",
      "Epoch 133/300\n",
      "Average training loss: 0.01832044618907902\n",
      "Average test loss: 0.0015558369989092978\n",
      "Epoch 134/300\n",
      "Average training loss: 0.01829314959132009\n",
      "Average test loss: 0.001606700097831587\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01829221532576614\n",
      "Average test loss: 0.0016184570180873077\n",
      "Epoch 136/300\n",
      "Average training loss: 0.018224357676174904\n",
      "Average test loss: 0.001572614540759888\n",
      "Epoch 137/300\n",
      "Average training loss: 0.018257780419455635\n",
      "Average test loss: 0.001566485036112782\n",
      "Epoch 138/300\n",
      "Average training loss: 0.018182624265551568\n",
      "Average test loss: 0.001621665487686793\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01816413495772415\n",
      "Average test loss: 0.0015945641003135178\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018198124041159947\n",
      "Average test loss: 0.0015671286827160253\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018187698183788195\n",
      "Average test loss: 0.0016006626876898938\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018128466283281644\n",
      "Average test loss: 0.0015672662869716683\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01805365855495135\n",
      "Average test loss: 0.001613759267143905\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018031829570730528\n",
      "Average test loss: 0.0015597141291946173\n",
      "Epoch 145/300\n",
      "Average training loss: 0.018087623432278634\n",
      "Average test loss: 0.0015908354896431167\n",
      "Epoch 146/300\n",
      "Average training loss: 0.017952950628267395\n",
      "Average test loss: 0.001584709939857324\n",
      "Epoch 149/300\n",
      "Average training loss: 0.017973187967307038\n",
      "Average test loss: 0.0016764840507465931\n",
      "Epoch 150/300\n",
      "Average training loss: 0.01794485543999407\n",
      "Average test loss: 0.0016406640323499838\n",
      "Epoch 151/300\n",
      "Average training loss: 0.017928876333766514\n",
      "Average test loss: 0.0015961932449912032\n",
      "Epoch 152/300\n",
      "Average training loss: 0.017898489742643305\n",
      "Average test loss: 0.001584991641756561\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01794513182176484\n",
      "Average test loss: 0.0015927193260027303\n",
      "Epoch 154/300\n",
      "Average training loss: 0.017880556964211994\n",
      "Average test loss: 0.0015665373711122408\n",
      "Epoch 155/300\n",
      "Average training loss: 0.017858000016874737\n",
      "Average test loss: 0.0016235390258952974\n",
      "Epoch 156/300\n",
      "Average training loss: 0.0178203920589553\n",
      "Average test loss: 0.0015886768880817624\n",
      "Epoch 157/300\n",
      "Average training loss: 0.017814176191886265\n",
      "Average test loss: 0.0016894255326026016\n",
      "Epoch 158/300\n",
      "Average training loss: 0.017847677482499016\n",
      "Average test loss: 0.0016150270553512707\n",
      "Epoch 159/300\n",
      "Average training loss: 0.017740475171142155\n",
      "Average test loss: 0.0016186434348217315\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01779810131092866\n",
      "Average test loss: 0.0016053124217109548\n",
      "Epoch 161/300\n",
      "Average training loss: 0.017698877059751088\n",
      "Average test loss: 0.0016383522500594456\n",
      "Epoch 162/300\n",
      "Average training loss: 0.017756092662612598\n",
      "Average test loss: 0.0015984432937370407\n",
      "Epoch 163/300\n",
      "Average training loss: 0.017691596378882728\n",
      "Average test loss: 0.0016069393466330237\n",
      "Epoch 164/300\n",
      "Average training loss: 0.017683628901839256\n",
      "Average test loss: 0.001601176621288889\n",
      "Epoch 165/300\n",
      "Average training loss: 0.017644687987036174\n",
      "Average test loss: 0.001592424707694186\n",
      "Epoch 166/300\n",
      "Average training loss: 0.017683767969409625\n",
      "Average test loss: 0.0016422698894101712\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01766651140815682\n",
      "Average test loss: 0.0016353094602624574\n",
      "Epoch 168/300\n",
      "Average training loss: 0.017590758989254635\n",
      "Average test loss: 0.0016199087653723028\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01764601041459375\n",
      "Average test loss: 0.0016477366153978639\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01759610575189193\n",
      "Average test loss: 0.0016444364378435743\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01754732155965434\n",
      "Average test loss: 0.0016383924105515082\n",
      "Epoch 175/300\n",
      "Average training loss: 0.017533448759052488\n",
      "Average test loss: 0.0015907388035621907\n",
      "Epoch 176/300\n",
      "Average training loss: 0.017474617896808518\n",
      "Average test loss: 0.0016362352448826036\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01749379680222935\n",
      "Average test loss: 0.0015881172731104824\n",
      "Epoch 178/300\n",
      "Average training loss: 0.017520423592792617\n",
      "Average test loss: 0.001669940300906698\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01744558907051881\n",
      "Average test loss: 0.0016019376339390873\n",
      "Epoch 180/300\n",
      "Average training loss: 0.017405737065606646\n",
      "Average test loss: 0.0016340547622077995\n",
      "Epoch 181/300\n",
      "Average training loss: 0.017490694892075328\n",
      "Average test loss: 0.0016765984973559776\n",
      "Epoch 182/300\n",
      "Average training loss: 0.017464632683330112\n",
      "Average test loss: 0.0016148592676553462\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01748129781252808\n",
      "Average test loss: 0.0016813779302562277\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01741555525941981\n",
      "Average test loss: 0.001619544536806643\n",
      "Epoch 185/300\n",
      "Average training loss: 0.017350085544917317\n",
      "Average test loss: 0.0016608581645414233\n",
      "Epoch 186/300\n",
      "Average training loss: 0.017367664834691418\n",
      "Average test loss: 0.001670755120408204\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01736806388778819\n",
      "Average test loss: 0.0016628835782822636\n",
      "Epoch 188/300\n",
      "Average training loss: 0.017350292816758157\n",
      "Average test loss: 0.0016059044782693187\n",
      "Epoch 189/300\n",
      "Average training loss: 0.017367422675093017\n",
      "Average test loss: 0.0016436062997414007\n",
      "Epoch 190/300\n",
      "Average training loss: 0.017406018063426017\n",
      "Average test loss: 0.0016426312665765485\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01729776925676399\n",
      "Average test loss: 0.0016204854773564472\n",
      "Epoch 192/300\n",
      "Average training loss: 0.017293883384929765\n",
      "Average test loss: 0.0016474930598504014\n",
      "Epoch 193/300\n",
      "Average training loss: 0.017286556523707176\n",
      "Average test loss: 0.001611776647882329\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01727512575272057\n",
      "Average test loss: 0.0018444532236705224\n",
      "Epoch 197/300\n",
      "Average training loss: 0.017253675712479485\n",
      "Average test loss: 0.0016601900077528424\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01723921032084359\n",
      "Average test loss: 0.001630897477372653\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01719158872630861\n",
      "Average test loss: 0.0016050782250240446\n",
      "Epoch 200/300\n",
      "Average training loss: 0.017220577283038035\n",
      "Average test loss: 0.001631715950038698\n",
      "Epoch 201/300\n",
      "Average training loss: 0.017194257385200924\n",
      "Average test loss: 0.0016110911690112616\n",
      "Epoch 202/300\n",
      "Average training loss: 0.017204884350299834\n",
      "Average test loss: 0.0016260697638822927\n",
      "Epoch 203/300\n",
      "Average training loss: 0.017165375587013032\n",
      "Average test loss: 0.001657007500115368\n",
      "Epoch 204/300\n",
      "Average training loss: 0.01719452955904934\n",
      "Average test loss: 0.0016490450848618316\n",
      "Epoch 205/300\n",
      "Average training loss: 0.017137976630694336\n",
      "Average test loss: 0.0016237754088102116\n",
      "Epoch 206/300\n",
      "Average training loss: 0.017128653552797104\n",
      "Average test loss: 0.0016335709090862008\n",
      "Epoch 207/300\n",
      "Average training loss: 0.017157417728669112\n",
      "Average test loss: 0.001645890458797415\n",
      "Epoch 208/300\n",
      "Average training loss: 0.017118580270144675\n",
      "Average test loss: 0.0017144938407776256\n",
      "Epoch 209/300\n",
      "Average training loss: 0.017117874420351453\n",
      "Average test loss: 0.0016717296054379808\n",
      "Epoch 210/300\n",
      "Average training loss: 0.017142607937256495\n",
      "Average test loss: 0.001641715832054615\n",
      "Epoch 211/300\n",
      "Average training loss: 0.01707783173273007\n",
      "Average test loss: 0.0016695962488237355\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01708385039948755\n",
      "Average test loss: 0.0016257025048964553\n",
      "Epoch 213/300\n",
      "Average training loss: 0.017041573041015202\n",
      "Average test loss: 0.0016445981250661943\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01708448575105932\n",
      "Average test loss: 0.0016286729574203492\n",
      "Epoch 215/300\n",
      "Average training loss: 0.017049549356102943\n",
      "Average test loss: 0.0016787028621054359\n",
      "Epoch 216/300\n",
      "Average training loss: 0.017033167673481835\n",
      "Average test loss: 0.0016521009327326384\n",
      "Epoch 217/300\n",
      "Average training loss: 0.017016154049171343\n",
      "Average test loss: 0.001711541368522578\n",
      "Epoch 220/300\n",
      "Average training loss: 0.016981558385822507\n",
      "Average test loss: 0.0016328873036222326\n",
      "Epoch 221/300\n",
      "Average training loss: 0.017040308838089307\n",
      "Average test loss: 0.0016615749563400944\n",
      "Epoch 222/300\n",
      "Average training loss: 0.016985390277372465\n",
      "Average test loss: 0.0016030688619034159\n",
      "Epoch 223/300\n",
      "Average training loss: 0.016955468733277586\n",
      "Average test loss: 0.0016729003665968776\n",
      "Epoch 224/300\n",
      "Average training loss: 0.016957437802520063\n",
      "Average test loss: 0.0016717514443314737\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0169711165924867\n",
      "Average test loss: 0.0016484285475065312\n",
      "Epoch 226/300\n",
      "Average test loss: 0.0016980504522927933\n",
      "Epoch 229/300\n",
      "Average training loss: 0.016907786652445793\n",
      "Average test loss: 0.0016590739459627205\n",
      "Epoch 230/300\n",
      "Average training loss: 0.016901950120098062\n",
      "Average test loss: 0.0016852402968539132\n",
      "Epoch 231/300\n",
      "Average training loss: 0.016900417710343996\n",
      "Average test loss: 0.0016249783867970109\n",
      "Epoch 232/300\n",
      "Average training loss: 0.016887528008057012\n",
      "Average test loss: 0.0016613169602221912\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01691037852731016\n",
      "Average test loss: 0.0017048847233462664\n",
      "Epoch 234/300\n",
      "Average training loss: 0.016875882632202573\n",
      "Average test loss: 0.0016505947068540586\n",
      "Epoch 235/300\n",
      "Average training loss: 0.016847783534063233\n",
      "Average test loss: 0.001670780236263656\n",
      "Epoch 236/300\n",
      "Average training loss: 0.016909425302512115\n",
      "Average test loss: 0.0017160898449106348\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0168424209513598\n",
      "Average test loss: 0.0016381174394239982\n",
      "Epoch 238/300\n",
      "Average training loss: 0.016816494714882637\n",
      "Average test loss: 0.0016247390794257323\n",
      "Epoch 239/300\n",
      "Average training loss: 0.016823069067464936\n",
      "Average test loss: 0.0015951923646239771\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01683142588949866\n",
      "Average test loss: 0.0016968426569882367\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01681418132947551\n",
      "Average test loss: 0.001651428443690141\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01680718149120609\n",
      "Average test loss: 0.0017157747524583506\n",
      "Epoch 243/300\n",
      "Average training loss: 0.016813238041268455\n",
      "Average test loss: 0.0016974635635399157\n",
      "Epoch 244/300\n",
      "Average training loss: 0.016782319847908286\n",
      "Average test loss: 0.001635368008683953\n",
      "Epoch 245/300\n",
      "Average training loss: 0.016767951959537137\n",
      "Average test loss: 0.001679986347961757\n",
      "Epoch 246/300\n",
      "Average training loss: 0.016782570731308726\n",
      "Average test loss: 0.0016240176790290408\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01677891659653849\n",
      "Average test loss: 0.0016784320004905263\n",
      "Epoch 248/300\n",
      "Average training loss: 0.016778761070635584\n",
      "Average test loss: 0.0016526896484817069\n",
      "Epoch 249/300\n",
      "Average training loss: 0.016751052553455034\n",
      "Average test loss: 0.0016301103650281827\n",
      "Epoch 250/300\n",
      "Average training loss: 0.016740279740757414\n",
      "Average test loss: 0.0016297596963122487\n",
      "Epoch 251/300\n",
      "Average training loss: 0.016740989667673907\n",
      "Average test loss: 0.0016740072635519835\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01675599580175347\n",
      "Average test loss: 0.0016439143221100999\n",
      "Epoch 253/300\n",
      "Average training loss: 0.016735669997003343\n",
      "Average test loss: 0.0016244964390579198\n",
      "Epoch 254/300\n",
      "Average training loss: 0.016756231107645566\n",
      "Average test loss: 0.0017269965596497059\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01669078333179156\n",
      "Average test loss: 0.0016613593610624471\n",
      "Epoch 256/300\n",
      "Average training loss: 0.016667846856017906\n",
      "Average test loss: 0.001641310577901701\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01668615787724654\n",
      "Average test loss: 0.0016853026409323017\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01669665885468324\n",
      "Average test loss: 0.0016467788925187455\n",
      "Epoch 261/300\n",
      "Average training loss: 0.016684102348155445\n",
      "Average test loss: 0.0017363843272129695\n",
      "Epoch 262/300\n",
      "Average training loss: 0.01669919344286124\n",
      "Average test loss: 0.0017051714768943687\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01663464167714119\n",
      "Average test loss: 0.0016585087529901001\n",
      "Epoch 264/300\n",
      "Average training loss: 0.01664390236304866\n",
      "Average test loss: 0.0016637553442269565\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01663633574297031\n",
      "Average test loss: 0.001694095953936792\n",
      "Epoch 266/300\n",
      "Average training loss: 0.016631058473553922\n",
      "Average test loss: 0.0016799729718930193\n",
      "Epoch 267/300\n",
      "Average training loss: 0.016612148518363633\n",
      "Average test loss: 0.0016567536319295566\n",
      "Epoch 268/300\n",
      "Average training loss: 0.016622872355083626\n",
      "Average test loss: 0.0016922895201585359\n",
      "Epoch 269/300\n",
      "Average training loss: 0.016643227491113875\n",
      "Average test loss: 0.0017121871929201816\n",
      "Epoch 270/300\n",
      "Average training loss: 0.016570305812689992\n",
      "Average test loss: 0.001665383447582523\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0166123623839683\n",
      "Average test loss: 0.0016919896441201369\n",
      "Epoch 272/300\n",
      "Average training loss: 0.016570497346421083\n",
      "Average test loss: 0.0016661680733991995\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01660061216023233\n",
      "Average test loss: 0.0017072333074692222\n",
      "Epoch 274/300\n",
      "Average training loss: 0.016601963344547485\n",
      "Average test loss: 0.0016358362522183193\n",
      "Epoch 275/300\n",
      "Average training loss: 0.016578270792133278\n",
      "Average test loss: 0.0016898856005734867\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01655093642903699\n",
      "Average test loss: 0.0017069054597781763\n",
      "Epoch 277/300\n",
      "Average training loss: 0.01656990301112334\n",
      "Average test loss: 0.0016960600666287872\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0165420830398798\n",
      "Average test loss: 0.0016745524471625686\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01655134328868654\n",
      "Average test loss: 0.0016657379166119628\n",
      "Epoch 280/300\n",
      "Average training loss: 0.016543962630960675\n",
      "Average test loss: 0.0016514669253180423\n",
      "Epoch 281/300\n",
      "Average training loss: 0.016533307804001703\n",
      "Average test loss: 0.001701873407388727\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01652097854597701\n",
      "Average test loss: 0.0017004683890069524\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01654749055372344\n",
      "Average test loss: 0.001688269270118326\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01648444358921713\n",
      "Average test loss: 0.0017133562874255908\n",
      "Epoch 285/300\n",
      "Average training loss: 0.01649266559878985\n",
      "Average test loss: 0.0016882087515874041\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01650068814555804\n",
      "Average test loss: 0.001679016092999114\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01649302742878596\n",
      "Average test loss: 0.0016232687858864666\n",
      "Epoch 288/300\n",
      "Average training loss: 0.016506627088619604\n",
      "Average test loss: 0.0016331751025799248\n",
      "Epoch 289/300\n",
      "Average training loss: 0.016468141636914676\n",
      "Average test loss: 0.0016952427625656129\n",
      "Epoch 290/300\n",
      "Average training loss: 0.016453403409984378\n",
      "Average test loss: 0.0017916790381487873\n",
      "Epoch 291/300\n",
      "Average training loss: 0.016476764781607523\n",
      "Average test loss: 0.0016490863942437701\n",
      "Epoch 292/300\n",
      "Average training loss: 0.01649814609107044\n",
      "Average test loss: 0.0017160724431483283\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01648152819606993\n",
      "Average test loss: 0.0016599273009536167\n",
      "Epoch 294/300\n",
      "Average training loss: 0.016459809278448423\n",
      "Average test loss: 0.0016588676093767087\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01642825727413098\n",
      "Average test loss: 0.0016736172305730481\n",
      "Epoch 296/300\n",
      "Average training loss: 0.016424758480654823\n",
      "Average test loss: 0.0017372897424631649\n",
      "Epoch 297/300\n",
      "Average training loss: 0.016437620368268756\n",
      "Average test loss: 0.0017137288375654153\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0164657422353824\n",
      "Average test loss: 0.0016599519161714447\n",
      "Epoch 299/300\n",
      "Average training loss: 0.016444311497939956\n",
      "Average test loss: 0.001689819606848889\n",
      "Epoch 300/300\n",
      "Average training loss: 0.016432246409356595\n",
      "Average test loss: 0.0017228033406039079\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-DCT-Additive-.01/15 Projections'\n",
    "\n",
    "DCT_10_proj15_weights, DCT_10_proj15_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj15', display=True)\n",
    "\n",
    "DCT_20_proj15_weights, DCT_20_proj15_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj15', display=True)\n",
    "\n",
    "DCT_30_proj15_weights, DCT_30_proj15_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj15', display=True)\n",
    "\n",
    "DCT_40_proj15_weights, DCT_40_proj15_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, DCT_10_proj15_weights, DCT_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, DCT_20_proj15_weights, DCT_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, DCT_30_proj15_weights, DCT_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, DCT_40_proj15_weights, DCT_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.47\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.51\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.04\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.09\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.19\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.25\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.84\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.65\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.70\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.92\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.20\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.43\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.76\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.96\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.53\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 30.17\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.56\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.72\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.99\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.09\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.27\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.40\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.44\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.65\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 31.33\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.73\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 32.07\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.37\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.44\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.54\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.57\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.77\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.82\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.78\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj15_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj15_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj15_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj15_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj15_psnr = average_PSNR(DCT_10_proj15_model, DCT_10_proj15_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj15_psnr = average_PSNR(DCT_20_proj15_model, DCT_20_proj15_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj15_psnr = average_PSNR(DCT_30_proj15_model, DCT_30_proj15_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj15_psnr = average_PSNR(DCT_40_proj15_model, DCT_40_proj15_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 11.790421631707085\n",
      "Average test loss: 0.0055234985392954615\n",
      "Epoch 2/300\n",
      "Average training loss: 4.071011177274916\n",
      "Average test loss: 0.004893697111143006\n",
      "Epoch 3/300\n",
      "Average training loss: 2.9320689849853516\n",
      "Average test loss: 0.004705563148276673\n",
      "Epoch 4/300\n",
      "Average training loss: 2.4756319065093995\n",
      "Average test loss: 0.004628172679907745\n",
      "Epoch 5/300\n",
      "Average training loss: 1.9081486092673408\n",
      "Average test loss: 0.004645625996506877\n",
      "Epoch 6/300\n",
      "Average training loss: 1.4565549502902562\n",
      "Average test loss: 0.004429575761159261\n",
      "Epoch 7/300\n",
      "Average training loss: 1.246146912044949\n",
      "Average test loss: 0.004421961520281103\n",
      "Epoch 8/300\n",
      "Average training loss: 1.04635507218043\n",
      "Average test loss: 0.004389261220064428\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8673355980449252\n",
      "Average test loss: 0.014164111652308041\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7338411982854207\n",
      "Average test loss: 0.004352202597177691\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5979125278260973\n",
      "Average test loss: 0.004283582836596502\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5010995210541619\n",
      "Average test loss: 0.025646579368246925\n",
      "Epoch 13/300\n",
      "Average training loss: 0.42382065788904827\n",
      "Average test loss: 0.004196340582023064\n",
      "Epoch 14/300\n",
      "Average training loss: 0.36438820150163437\n",
      "Average test loss: 0.004209407683047984\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3163565011024475\n",
      "Average test loss: 0.004169435170168678\n",
      "Epoch 16/300\n",
      "Average training loss: 0.278647879574034\n",
      "Average test loss: 0.004166708875033591\n",
      "Epoch 17/300\n",
      "Average training loss: 0.24709439036581252\n",
      "Average test loss: 0.004156457679553164\n",
      "Epoch 18/300\n",
      "Average training loss: 0.22148556933138105\n",
      "Average test loss: 0.0041432364191859965\n",
      "Epoch 19/300\n",
      "Average training loss: 0.20092274366484747\n",
      "Average test loss: 0.00412743192538619\n",
      "Epoch 20/300\n",
      "Average training loss: 0.18379326784610747\n",
      "Average test loss: 0.0041071796649032166\n",
      "Epoch 21/300\n",
      "Average training loss: 0.17020543338192834\n",
      "Average test loss: 0.004083736609253618\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1599113651116689\n",
      "Average test loss: 0.004076688367252549\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1525051474571228\n",
      "Average test loss: 0.004088510748205913\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1466762717432446\n",
      "Average test loss: 0.0040654126490569775\n",
      "Epoch 25/300\n",
      "Average training loss: 0.141936371061537\n",
      "Average test loss: 0.004057561537457837\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1380799826780955\n",
      "Average test loss: 0.004055752312350604\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1350669915676117\n",
      "Average test loss: 0.0040468696407559845\n",
      "Epoch 28/300\n",
      "Average training loss: 0.1325530080265469\n",
      "Average test loss: 0.0040598002676334646\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13044855335685943\n",
      "Average test loss: 0.004030860172791613\n",
      "Epoch 30/300\n",
      "Average training loss: 0.12864434758159848\n",
      "Average test loss: 0.004016429390344355\n",
      "Epoch 31/300\n",
      "Average training loss: 0.12707949395974477\n",
      "Average test loss: 0.004006907327514556\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1259685367014673\n",
      "Average test loss: 0.003995719582256344\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1251035585006078\n",
      "Average test loss: 0.003985871704502238\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12430975745121638\n",
      "Average test loss: 0.00403748772583074\n",
      "Epoch 35/300\n",
      "Average training loss: 0.1236137370467186\n",
      "Average test loss: 0.003961253305276235\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12307979264193111\n",
      "Average test loss: 0.003967535771015618\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12253692663378185\n",
      "Average test loss: 0.003955437323285474\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12209994194904963\n",
      "Average test loss: 0.003950782615691423\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12171594643592834\n",
      "Average test loss: 0.003944888583074013\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12124885034561157\n",
      "Average test loss: 0.003963564522978333\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12089898196193907\n",
      "Average test loss: 0.003957167658954859\n",
      "Epoch 42/300\n",
      "Average training loss: 0.1206114668250084\n",
      "Average test loss: 0.0039365876708179716\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12009785759449006\n",
      "Average test loss: 0.003940672809878985\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11984733004040188\n",
      "Average test loss: 0.00407680752252539\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11957599782281451\n",
      "Average test loss: 0.003944477499359184\n",
      "Epoch 46/300\n",
      "Average training loss: 0.11920614224010044\n",
      "Average test loss: 0.003936871950618095\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11897770129972034\n",
      "Average test loss: 0.003925171506901582\n",
      "Epoch 48/300\n",
      "Average training loss: 0.11871408153904808\n",
      "Average test loss: 0.003929865731961198\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11830229886372884\n",
      "Average test loss: 0.003989962354923288\n",
      "Epoch 50/300\n",
      "Average training loss: 0.11820450468195809\n",
      "Average test loss: 0.00392403598750631\n",
      "Epoch 51/300\n",
      "Average training loss: 0.11782885215348668\n",
      "Average test loss: 0.003925765452285608\n",
      "Epoch 52/300\n",
      "Average training loss: 0.11760500584046046\n",
      "Average test loss: 0.003933724334877398\n",
      "Epoch 53/300\n",
      "Average training loss: 0.11728265843126509\n",
      "Average test loss: 0.0039281777420805564\n",
      "Epoch 54/300\n",
      "Average training loss: 0.11707443393601312\n",
      "Average test loss: 0.0038950712171693645\n",
      "Epoch 55/300\n",
      "Average training loss: 0.11694096207618714\n",
      "Average test loss: 0.003916531615373161\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11656273241175545\n",
      "Average test loss: 0.003995498499936528\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11635872548818588\n",
      "Average test loss: 0.003889971833055218\n",
      "Epoch 58/300\n",
      "Average training loss: 0.1162872439622879\n",
      "Average test loss: 0.0039060467967970505\n",
      "Epoch 59/300\n",
      "Average training loss: 0.1159994476503796\n",
      "Average test loss: 0.0039057023885349434\n",
      "Epoch 60/300\n",
      "Average training loss: 0.11579077110025618\n",
      "Average test loss: 0.003904356240398354\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11547296880351172\n",
      "Average test loss: 0.00389503257162869\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11529220482375886\n",
      "Average test loss: 0.00389527114894655\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11505928224987454\n",
      "Average test loss: 0.00390796377033823\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11489002086718877\n",
      "Average test loss: 0.003903679494642549\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11482752319176992\n",
      "Average test loss: 0.0039288067749391\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1146475420196851\n",
      "Average test loss: 0.0038933595483087832\n",
      "Epoch 67/300\n",
      "Average training loss: 0.11431630586915546\n",
      "Average test loss: 0.003901159032454921\n",
      "Epoch 68/300\n",
      "Average training loss: 0.11422502378622691\n",
      "Average test loss: 0.0039165347543441585\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11388934492402607\n",
      "Average test loss: 0.003890982477201356\n",
      "Epoch 70/300\n",
      "Average training loss: 0.11372574434346623\n",
      "Average test loss: 0.003953547537119852\n",
      "Epoch 71/300\n",
      "Average training loss: 0.11357904929584928\n",
      "Average test loss: 0.00391258958602945\n",
      "Epoch 72/300\n",
      "Average training loss: 0.11340772408909268\n",
      "Average test loss: 0.0038987260576751497\n",
      "Epoch 73/300\n",
      "Average training loss: 0.11328291133377287\n",
      "Average test loss: 0.0039081705862449275\n",
      "Epoch 74/300\n",
      "Average training loss: 0.1129125298195415\n",
      "Average test loss: 0.003913612581168612\n",
      "Epoch 75/300\n",
      "Average training loss: 0.1126361745066113\n",
      "Average test loss: 0.0039086905742685\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11244254400001631\n",
      "Average test loss: 0.0039320835545659065\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11230296294556724\n",
      "Average test loss: 0.0038993322760280634\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11193935008181466\n",
      "Average test loss: 0.003917848617666297\n",
      "Epoch 80/300\n",
      "Average training loss: 0.1118291011651357\n",
      "Average test loss: 0.003916572674901949\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11139052532778845\n",
      "Average test loss: 0.003999102707124419\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11128778621223238\n",
      "Average test loss: 0.003924472602912121\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11122020138634575\n",
      "Average test loss: 0.0039543531268007226\n",
      "Epoch 85/300\n",
      "Average training loss: 0.11079171080721749\n",
      "Average test loss: 0.004022211941166057\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11059714131885105\n",
      "Average test loss: 0.003929458414514859\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11042769377099143\n",
      "Average test loss: 0.003938434855805503\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11018120724625058\n",
      "Average test loss: 0.003995292459097174\n",
      "Epoch 89/300\n",
      "Average training loss: 0.10992459517717361\n",
      "Average test loss: 0.003917745784545938\n",
      "Epoch 90/300\n",
      "Average training loss: 0.10968035421768824\n",
      "Average test loss: 0.003952653018136819\n",
      "Epoch 91/300\n",
      "Average training loss: 0.10962343433830474\n",
      "Average test loss: 0.003936023591086269\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10944157748089896\n",
      "Average test loss: 0.0039316527632375555\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1090655709107717\n",
      "Average test loss: 0.003930939060946306\n",
      "Epoch 94/300\n",
      "Average training loss: 0.10849872957335578\n",
      "Average test loss: 0.003958709658020073\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10819042435619566\n",
      "Average test loss: 0.003930674978428417\n",
      "Epoch 98/300\n",
      "Average training loss: 0.108102851178911\n",
      "Average test loss: 0.003984954968922668\n",
      "Epoch 99/300\n",
      "Average training loss: 0.1077741395632426\n",
      "Average test loss: 0.0040713638234883545\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10758528476953506\n",
      "Average test loss: 0.003942020804103878\n",
      "Epoch 101/300\n",
      "Average training loss: 0.1074403452343411\n",
      "Average test loss: 0.003944750063121318\n",
      "Epoch 102/300\n",
      "Average training loss: 0.10725625202390883\n",
      "Average test loss: 0.004003260877190364\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10683738799889883\n",
      "Average test loss: 0.004026682093325589\n",
      "Epoch 104/300\n",
      "Average training loss: 0.10689562978347142\n",
      "Average test loss: 0.004081437120007144\n",
      "Epoch 105/300\n",
      "Average training loss: 0.1065357068710857\n",
      "Average test loss: 0.003989996870979667\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10641863926914003\n",
      "Average test loss: 0.004081813583771388\n",
      "Epoch 107/300\n",
      "Average training loss: 0.10631380874580808\n",
      "Average test loss: 0.004052309438586235\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10594390775097741\n",
      "Average test loss: 0.003990006604956256\n",
      "Epoch 109/300\n",
      "Average training loss: 0.10583880251646041\n",
      "Average test loss: 0.004031013802521758\n",
      "Epoch 110/300\n",
      "Average training loss: 0.10559552006589042\n",
      "Average test loss: 0.004007133530038926\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10531959529717763\n",
      "Average test loss: 0.0040719551203979385\n",
      "Epoch 112/300\n",
      "Average training loss: 0.105233826107449\n",
      "Average test loss: 0.004004587398635017\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10485590265856849\n",
      "Average test loss: 0.004073778764241272\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10481163781881332\n",
      "Average test loss: 0.004139410608344608\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10467585617303848\n",
      "Average test loss: 0.004074490408516592\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10454466429683897\n",
      "Average test loss: 0.003994970682801472\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10435770331488715\n",
      "Average test loss: 0.004114037088221974\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10409513158268399\n",
      "Average test loss: 0.004031768830286132\n",
      "Epoch 119/300\n",
      "Average training loss: 0.10383221476607853\n",
      "Average test loss: 0.004046931684017182\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10371556667486827\n",
      "Average test loss: 0.004103777007510264\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10376836991972394\n",
      "Average test loss: 0.004019001530690325\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10332811665534973\n",
      "Average test loss: 0.004067346304655075\n",
      "Epoch 123/300\n",
      "Average training loss: 0.1031465199192365\n",
      "Average test loss: 0.004007142559935649\n",
      "Epoch 124/300\n",
      "Average training loss: 0.10311287334230211\n",
      "Average test loss: 0.004109506526341041\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10281405070755217\n",
      "Average test loss: 0.0040827263544003164\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10280747011635039\n",
      "Average test loss: 0.004081317634839151\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10264897478289074\n",
      "Average test loss: 0.004088715920845668\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10223952190081279\n",
      "Average test loss: 0.004164568537225326\n",
      "Epoch 129/300\n",
      "Average training loss: 0.1020505562822024\n",
      "Average test loss: 0.004173390216711495\n",
      "Epoch 130/300\n",
      "Average training loss: 0.10195229134956996\n",
      "Average test loss: 0.004098842657481631\n",
      "Epoch 131/300\n",
      "Average training loss: 0.10193508670727412\n",
      "Average test loss: 0.004099860738755928\n",
      "Epoch 132/300\n",
      "Average training loss: 0.1016776903072993\n",
      "Average test loss: 0.004064338440282477\n",
      "Epoch 133/300\n",
      "Average training loss: 0.10155771224366295\n",
      "Average test loss: 0.004060914081417852\n",
      "Epoch 134/300\n",
      "Average training loss: 0.10136929731236564\n",
      "Average test loss: 0.0040797722687323885\n",
      "Epoch 135/300\n",
      "Average training loss: 0.10132097691297531\n",
      "Average test loss: 0.004142509498529964\n",
      "Epoch 136/300\n",
      "Average training loss: 0.10110895125071208\n",
      "Average test loss: 0.004096362446538276\n",
      "Epoch 137/300\n",
      "Average training loss: 0.10099402831660377\n",
      "Average test loss: 0.004143389292889171\n",
      "Epoch 138/300\n",
      "Average training loss: 0.10099580572048823\n",
      "Average test loss: 0.004081324353607164\n",
      "Epoch 139/300\n",
      "Average training loss: 0.1007077978849411\n",
      "Average test loss: 0.004166518520977762\n",
      "Epoch 140/300\n",
      "Average training loss: 0.1005210572414928\n",
      "Average test loss: 0.004198286202218797\n",
      "Epoch 141/300\n",
      "Average training loss: 0.10042454881138271\n",
      "Average test loss: 0.004131336397594876\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10023505908581945\n",
      "Average test loss: 0.004145112651503749\n",
      "Epoch 143/300\n",
      "Average training loss: 0.10014716314607196\n",
      "Average test loss: 0.00408577532072862\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09996467498938243\n",
      "Average test loss: 0.0042901421835025155\n",
      "Epoch 145/300\n",
      "Average training loss: 0.10004393070273929\n",
      "Average test loss: 0.004097743817915519\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09980986293488078\n",
      "Average test loss: 0.004210491811442707\n",
      "Epoch 147/300\n",
      "Average training loss: 0.09950126539998584\n",
      "Average test loss: 0.004093079682439565\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09957641916142569\n",
      "Average test loss: 0.0041388969326184855\n",
      "Epoch 149/300\n",
      "Average training loss: 0.09926430543926026\n",
      "Average test loss: 0.0041098305227027995\n",
      "Epoch 150/300\n",
      "Average training loss: 0.09920036112599903\n",
      "Average test loss: 0.004173471062547631\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09940182287163205\n",
      "Average test loss: 0.004176969855195946\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0991984339290195\n",
      "Average test loss: 0.004152527900205718\n",
      "Epoch 153/300\n",
      "Average training loss: 0.09870710411336688\n",
      "Average test loss: 0.004208724904598461\n",
      "Epoch 154/300\n",
      "Average training loss: 0.09876107366879781\n",
      "Average test loss: 0.004165022405071391\n",
      "Epoch 155/300\n",
      "Average training loss: 0.09862065604660246\n",
      "Average test loss: 0.004328236597693629\n",
      "Epoch 156/300\n",
      "Average training loss: 0.09850443979766633\n",
      "Average test loss: 0.004115005439975196\n",
      "Epoch 157/300\n",
      "Average training loss: 0.09838953025142352\n",
      "Average test loss: 0.0041348145835929445\n",
      "Epoch 158/300\n",
      "Average training loss: 0.09842511700921588\n",
      "Average test loss: 0.004321870132038991\n",
      "Epoch 159/300\n",
      "Average training loss: 0.0980466758078999\n",
      "Average test loss: 0.004182386297732592\n",
      "Epoch 160/300\n",
      "Average training loss: 0.09804832975731956\n",
      "Average test loss: 0.004130064549028045\n",
      "Epoch 161/300\n",
      "Average training loss: 0.09791649173365699\n",
      "Average test loss: 0.0042781224296324786\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09788047195474307\n",
      "Average test loss: 0.00431649095689257\n",
      "Epoch 163/300\n",
      "Average training loss: 0.09788719783226649\n",
      "Average test loss: 0.0042372508574691084\n",
      "Epoch 164/300\n",
      "Average training loss: 0.09778457456827164\n",
      "Average test loss: 0.0041664432092673245\n",
      "Epoch 165/300\n",
      "Average training loss: 0.09751524776882596\n",
      "Average test loss: 0.004240003250539303\n",
      "Epoch 166/300\n",
      "Average training loss: 0.09733144118388494\n",
      "Average test loss: 0.004286269367155102\n",
      "Epoch 167/300\n",
      "Average training loss: 0.09732344039943483\n",
      "Average test loss: 0.004143615535149972\n",
      "Epoch 168/300\n",
      "Average training loss: 0.09728260778056251\n",
      "Average test loss: 0.0042724537203709285\n",
      "Epoch 169/300\n",
      "Average training loss: 0.09703077625566059\n",
      "Average test loss: 0.004213719677593973\n",
      "Epoch 170/300\n",
      "Average training loss: 0.09688026045428381\n",
      "Average test loss: 0.004151393084683352\n",
      "Epoch 171/300\n",
      "Average training loss: 0.09693063637945387\n",
      "Average test loss: 0.004192276015463803\n",
      "Epoch 172/300\n",
      "Average training loss: 0.09687434236870872\n",
      "Average test loss: 0.004192895154158274\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0966697555647956\n",
      "Average test loss: 0.00417694729152653\n",
      "Epoch 174/300\n",
      "Average training loss: 0.09685667338636186\n",
      "Average test loss: 0.004159596438830098\n",
      "Epoch 175/300\n",
      "Average training loss: 0.09669728322161568\n",
      "Average test loss: 0.004226784785795543\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09681155509418911\n",
      "Average test loss: 0.004166711338071359\n",
      "Epoch 177/300\n",
      "Average training loss: 0.09606838368707234\n",
      "Average test loss: 0.0042489860819445715\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09628792663084136\n",
      "Average test loss: 0.004245588885827197\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09607045094834434\n",
      "Average test loss: 0.004118820567925771\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09603769168588851\n",
      "Average test loss: 0.004352192026459509\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09590725620587667\n",
      "Average test loss: 0.004135967163162099\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0959232938223415\n",
      "Average test loss: 0.004258872450432843\n",
      "Epoch 183/300\n",
      "Average training loss: 0.09579536054531733\n",
      "Average test loss: 0.004267203828940789\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09584149545431137\n",
      "Average test loss: 0.0042460560094979075\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09567815849516127\n",
      "Average test loss: 0.0043215610571205616\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0955066844423612\n",
      "Average test loss: 0.004184322747091452\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09530677859650717\n",
      "Average test loss: 0.0042418186544544165\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09523812886079153\n",
      "Average test loss: 0.004245128974732426\n",
      "Epoch 189/300\n",
      "Average training loss: 0.0952306821876102\n",
      "Average test loss: 0.004398249045014381\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09536336651775572\n",
      "Average test loss: 0.004195752706378698\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09506707386838065\n",
      "Average test loss: 0.004137233320209715\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09495529816548029\n",
      "Average test loss: 0.004266413686590062\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09485066369838185\n",
      "Average test loss: 0.004237593336237801\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09478274451361762\n",
      "Average test loss: 0.00420934275806778\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09481159622139401\n",
      "Average test loss: 0.004193010854224364\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09471814263529248\n",
      "Average test loss: 0.004286118004264103\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09469297431574927\n",
      "Average test loss: 0.004238724511737625\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09458405339717865\n",
      "Average test loss: 0.004202337361458275\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09434958820872837\n",
      "Average test loss: 0.004238917344146304\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09442666153113047\n",
      "Average test loss: 0.004263448440159361\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09432158123122321\n",
      "Average test loss: 0.004225191050933467\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09429359349277285\n",
      "Average test loss: 0.004263080856245425\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09412468696965112\n",
      "Average test loss: 0.004266706666184796\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09412269632021586\n",
      "Average test loss: 0.004222783919009898\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09415413907501433\n",
      "Average test loss: 0.004363033540960815\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09409707210461299\n",
      "Average test loss: 0.00423369568420781\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09379812318748898\n",
      "Average test loss: 0.004595999056266414\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09374013994137446\n",
      "Average test loss: 0.004268013972789049\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0937211867902014\n",
      "Average test loss: 0.0041342612825748\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09360148413976034\n",
      "Average test loss: 0.00425723017917739\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09362706427441703\n",
      "Average test loss: 0.004306091602891683\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09375510232978397\n",
      "Average test loss: 0.004376528319385317\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09342430933647686\n",
      "Average test loss: 0.004322228268202808\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09327143265803654\n",
      "Average test loss: 0.00441381095453269\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09322438507609897\n",
      "Average test loss: 0.0042498375620279045\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09312945385111703\n",
      "Average test loss: 0.0042616101648244595\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0931717548304134\n",
      "Average test loss: 0.0043397569834358165\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09310560733742185\n",
      "Average test loss: 0.004319315631356504\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09307909772793452\n",
      "Average test loss: 0.004173921280437046\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09299739510483213\n",
      "Average test loss: 0.0042428098298195336\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09281005184517967\n",
      "Average test loss: 0.004300493761069245\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09283540177014139\n",
      "Average test loss: 0.004250164084343447\n",
      "Epoch 223/300\n",
      "Average training loss: 0.0927870689564281\n",
      "Average test loss: 0.004324662915948365\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09257410149441825\n",
      "Average test loss: 0.004309587129702171\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09266420898834864\n",
      "Average test loss: 0.004300864020569457\n",
      "Epoch 226/300\n",
      "Average training loss: 0.09251890768607457\n",
      "Average test loss: 0.004211980466834373\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09247946898804771\n",
      "Average test loss: 0.004275509463002284\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09246455045541127\n",
      "Average test loss: 0.00435556430535184\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09226826286315917\n",
      "Average test loss: 0.0042162101966225435\n",
      "Epoch 230/300\n",
      "Average training loss: 0.09231178036663268\n",
      "Average test loss: 0.00426178261016806\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09195766382084952\n",
      "Average test loss: 0.004266220816100637\n",
      "Epoch 234/300\n",
      "Average training loss: 0.09216700479719374\n",
      "Average test loss: 0.004213822045363485\n",
      "Epoch 235/300\n",
      "Average training loss: 0.09203700927893321\n",
      "Average test loss: 0.00442805344839063\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09219933337635464\n",
      "Average test loss: 0.0042796922851767805\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0918017768032021\n",
      "Average test loss: 0.00422618852266007\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09180318699942695\n",
      "Average test loss: 0.004385739376147588\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09176737716462877\n",
      "Average test loss: 0.004321237771875329\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09179077744483947\n",
      "Average test loss: 0.004161485848327478\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09157270809676912\n",
      "Average test loss: 0.0042819785479870105\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09150333539644877\n",
      "Average test loss: 0.004363654855638743\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09139069934023751\n",
      "Average test loss: 0.004333577752320303\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0912160496711731\n",
      "Average test loss: 0.004254940662326084\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09138323220279482\n",
      "Average test loss: 0.004305041069578793\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09122208755546146\n",
      "Average test loss: 0.004419024271062679\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09106995465358099\n",
      "Average test loss: 0.004349066963833239\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09101897585391998\n",
      "Average test loss: 0.004220578799231185\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09094885720147027\n",
      "Average test loss: 0.004269326016720798\n",
      "Epoch 254/300\n",
      "Average training loss: 0.09108107435041003\n",
      "Average test loss: 0.004240917068388727\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09074526531828775\n",
      "Average test loss: 0.0043128184688587985\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09085919295416937\n",
      "Average test loss: 0.004164329781507452\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09083746033906936\n",
      "Average test loss: 0.004207046308451229\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09062128108077579\n",
      "Average test loss: 0.004325439149099919\n",
      "Epoch 259/300\n",
      "Average training loss: 0.09066422591606776\n",
      "Average test loss: 0.00423977008048031\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09075163775020176\n",
      "Average test loss: 0.004313667339169317\n",
      "Epoch 261/300\n",
      "Average training loss: 0.09054264252384504\n",
      "Average test loss: 0.004293057191703055\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09044322169489331\n",
      "Average test loss: 0.004286593018720548\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09042123544216156\n",
      "Average test loss: 0.004304187507265144\n",
      "Epoch 266/300\n",
      "Average training loss: 0.0902423097954856\n",
      "Average test loss: 0.00432042951323092\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09026126656929652\n",
      "Average test loss: 0.004335997471378909\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0901529850628641\n",
      "Average test loss: 0.004211240404388971\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09011602945460213\n",
      "Average test loss: 0.004353735433063573\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09024965561760796\n",
      "Average test loss: 0.004305887422627873\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09005096644163131\n",
      "Average test loss: 0.004235326412444313\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09009427158037822\n",
      "Average test loss: 0.0042151054127348795\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08991796812083987\n",
      "Average training loss: 0.08980450463957257\n",
      "Average test loss: 0.004336911920044157\n",
      "Epoch 276/300\n",
      "Average training loss: 0.089821219669448\n",
      "Average test loss: 0.004322591438061661\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0898099530339241\n",
      "Average test loss: 0.004242866031825543\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09004801915751563\n",
      "Average test loss: 0.004265957718922032\n",
      "Epoch 279/300\n",
      "Average training loss: 0.08966429213020537\n",
      "Average test loss: 0.004368070400009552\n",
      "Epoch 280/300\n",
      "Average training loss: 0.08963145416975021\n",
      "Average test loss: 0.004502611971977684\n",
      "Epoch 281/300\n",
      "Average training loss: 0.08958516809675429\n",
      "Average test loss: 0.0042229617854787245\n",
      "Epoch 282/300\n",
      "Average training loss: 0.08970734267102347\n",
      "Average test loss: 0.004326996821082301\n",
      "Epoch 283/300\n",
      "Average training loss: 0.08943432580762439\n",
      "Average test loss: 0.004305622028600838\n",
      "Epoch 284/300\n",
      "Average training loss: 0.08951526626944542\n",
      "Average test loss: 0.004341021949218379\n",
      "Epoch 285/300\n",
      "Average training loss: 0.08932337342368232\n",
      "Average test loss: 0.004292311161756515\n",
      "Epoch 286/300\n",
      "Average training loss: 0.08930595791339874\n",
      "Average test loss: 0.004714882729781999\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0893018019133144\n",
      "Average test loss: 0.004169586949464348\n",
      "Epoch 289/300\n",
      "Average training loss: 0.08904541211658054\n",
      "Average test loss: 0.004404033174117406\n",
      "Epoch 290/300\n",
      "Average training loss: 0.0893386592467626\n",
      "Average test loss: 0.004544799861808618\n",
      "Epoch 291/300\n",
      "Average training loss: 0.08913879089223015\n",
      "Average test loss: 0.004295692682680157\n",
      "Epoch 292/300\n",
      "Average training loss: 0.08918536923329036\n",
      "Average test loss: 0.004229011370489995\n",
      "Epoch 293/300\n",
      "Average training loss: 0.08907615526517232\n",
      "Average test loss: 0.00429832025927802\n",
      "Epoch 294/300\n",
      "Average training loss: 0.08902485986550648\n",
      "Average test loss: 0.004287520479203926\n",
      "Epoch 295/300\n",
      "Average training loss: 0.08893725889921188\n",
      "Average test loss: 0.004311444438993931\n",
      "Epoch 296/300\n",
      "Average training loss: 0.08892453906933466\n",
      "Average test loss: 0.0042731265214582285\n",
      "Epoch 297/300\n",
      "Average training loss: 0.088907548232211\n",
      "Average test loss: 0.004297976485970948\n",
      "Epoch 298/300\n",
      "Average training loss: 0.08884065722756916\n",
      "Average test loss: 0.004254632795850436\n",
      "Epoch 299/300\n",
      "Average training loss: 0.08868675212065379\n",
      "Average test loss: 0.004265009403849641\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08885800827874078\n",
      "Average test loss: 0.004396601349529293\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.100170364379883\n",
      "Average test loss: 11.370128082513808\n",
      "Epoch 2/300\n",
      "Average training loss: 4.759593948364258\n",
      "Average test loss: 0.004655134525563982\n",
      "Epoch 3/300\n",
      "Average training loss: 3.3637955782148574\n",
      "Average test loss: 0.00419523812416527\n",
      "Epoch 4/300\n",
      "Average training loss: 2.6933214429219565\n",
      "Average test loss: 0.0041586912214342095\n",
      "Epoch 5/300\n",
      "Average training loss: 2.2174764133029514\n",
      "Average test loss: 0.003948505822983053\n",
      "Epoch 6/300\n",
      "Average training loss: 1.8890178076426187\n",
      "Average test loss: 0.003903061234081785\n",
      "Epoch 7/300\n",
      "Average training loss: 1.6597472488615248\n",
      "Average test loss: 0.00382958433818486\n",
      "Epoch 8/300\n",
      "Average training loss: 1.4287169201109144\n",
      "Average test loss: 0.003723094514053729\n",
      "Epoch 9/300\n",
      "Average training loss: 1.209696532037523\n",
      "Average test loss: 0.00364819151452846\n",
      "Epoch 10/300\n",
      "Average training loss: 1.0229273104137844\n",
      "Average test loss: 0.0035988670840031572\n",
      "Epoch 11/300\n",
      "Average training loss: 0.8774207612143623\n",
      "Average test loss: 0.0036540550695111354\n",
      "Epoch 12/300\n",
      "Average training loss: 0.7582565468682183\n",
      "Average test loss: 0.003496542204171419\n",
      "Epoch 13/300\n",
      "Average training loss: 0.6455225557221307\n",
      "Average test loss: 0.0034504949467049706\n",
      "Epoch 14/300\n",
      "Average training loss: 0.5452786621517606\n",
      "Average test loss: 0.003444418072907461\n",
      "Epoch 15/300\n",
      "Average training loss: 0.4605979964468214\n",
      "Average test loss: 0.0033487928633888562\n",
      "Epoch 16/300\n",
      "Average training loss: 0.39314203373591106\n",
      "Average test loss: 0.003371499174584945\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3372786467605167\n",
      "Average test loss: 0.0033219349715444776\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2923148996829987\n",
      "Average test loss: 0.0032423249837011097\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2556004157066345\n",
      "Average test loss: 0.0033212381361259354\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2260920296377606\n",
      "Average test loss: 0.0032338331567330495\n",
      "Epoch 21/300\n",
      "Average training loss: 0.20221890609794194\n",
      "Average test loss: 0.0031485747719804448\n",
      "Epoch 22/300\n",
      "Average training loss: 0.18328517515129514\n",
      "Average test loss: 0.003167214936369823\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16802770249048868\n",
      "Average test loss: 0.003134151365607977\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15587668742073907\n",
      "Average test loss: 0.0030978118046704264\n",
      "Epoch 25/300\n",
      "Average training loss: 0.14570960856808557\n",
      "Average test loss: 0.003061768827545974\n",
      "Epoch 26/300\n",
      "Average training loss: 0.1373248997595575\n",
      "Average test loss: 0.0030658246433983246\n",
      "Epoch 27/300\n",
      "Average training loss: 0.13035872500472598\n",
      "Average test loss: 0.0030556203348355163\n",
      "Epoch 28/300\n",
      "Average training loss: 0.12425716350475947\n",
      "Average test loss: 0.0030644536879327563\n",
      "Epoch 29/300\n",
      "Average training loss: 0.11887822601530287\n",
      "Average test loss: 0.002990585541145669\n",
      "Epoch 30/300\n",
      "Average training loss: 0.11481187119748858\n",
      "Average test loss: 0.0030073909376644426\n",
      "Epoch 31/300\n",
      "Average training loss: 0.1117330448296335\n",
      "Average test loss: 0.003053686400047607\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10914763468503952\n",
      "Average test loss: 0.002991364680851499\n",
      "Epoch 33/300\n",
      "Average training loss: 0.10698372023966578\n",
      "Average test loss: 0.0029698181295146546\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10503603246476916\n",
      "Average test loss: 0.002946610377687547\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10340362822347217\n",
      "Average test loss: 0.002943892160637511\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10195846241050296\n",
      "Average test loss: 0.002927921156295472\n",
      "Epoch 37/300\n",
      "Average training loss: 0.1006803854505221\n",
      "Average test loss: 0.0029390160778744352\n",
      "Epoch 38/300\n",
      "Average training loss: 0.09947890531354481\n",
      "Average test loss: 0.0029080896139558818\n",
      "Epoch 39/300\n",
      "Average training loss: 0.09840483601225747\n",
      "Average test loss: 0.002912938025262621\n",
      "Epoch 40/300\n",
      "Average training loss: 0.09751773198445637\n",
      "Average test loss: 0.0029440196183406642\n",
      "Epoch 41/300\n",
      "Average training loss: 0.09650754628578821\n",
      "Average test loss: 0.002945412036445406\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09575025089581808\n",
      "Average test loss: 0.0029482556235873036\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09494135750664605\n",
      "Average test loss: 0.0029276841471178663\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09424971119562785\n",
      "Average test loss: 0.002925000945106149\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09366926140255398\n",
      "Average test loss: 0.0028831131073335805\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09283003385861714\n",
      "Average test loss: 0.002883404380434917\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09221814774804644\n",
      "Average test loss: 0.002947719650756982\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09189328165186776\n",
      "Average test loss: 0.0028985788697997728\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09105951394306289\n",
      "Average test loss: 0.002877973407920864\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09050225276417202\n",
      "Average test loss: 0.0028851803094148637\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09006022846036488\n",
      "Average test loss: 0.0028695641046182978\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08948735668261847\n",
      "Average test loss: 0.0028692132530526984\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0889504904680782\n",
      "Average test loss: 0.002856085425656703\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08846636152267456\n",
      "Average test loss: 0.002890535885675086\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08803994660907322\n",
      "Average test loss: 0.0028739010186658964\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08751569370428722\n",
      "Average test loss: 0.0028877336389074725\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08711721600426568\n",
      "Average test loss: 0.002864514072943065\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08672297671106127\n",
      "Average test loss: 0.0028585387170314788\n",
      "Epoch 59/300\n",
      "Average training loss: 0.0862788214219941\n",
      "Average test loss: 0.00289289807755914\n",
      "Epoch 60/300\n",
      "Average training loss: 0.08578329322073194\n",
      "Average test loss: 0.0028507889347771805\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08555579450395372\n",
      "Average test loss: 0.0028461861182004214\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08498115892211597\n",
      "Average test loss: 0.0029043003318624363\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0845480472114351\n",
      "Average test loss: 0.0028461613793753916\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08411811843183306\n",
      "Average test loss: 0.0028424387325843175\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08391258494059245\n",
      "Average test loss: 0.002884704254153702\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08357931380801731\n",
      "Average test loss: 0.0032549981630096833\n",
      "Epoch 67/300\n",
      "Average training loss: 0.08304739882879787\n",
      "Average test loss: 0.002891136752027604\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08289555846320258\n",
      "Average test loss: 0.0028959417483872835\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08215183105733659\n",
      "Average test loss: 0.0028591051393498977\n",
      "Epoch 70/300\n",
      "Average training loss: 0.08192268635167016\n",
      "Average test loss: 0.0028481349073764353\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08170947622590595\n",
      "Average test loss: 0.002878204080793593\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08121017420954174\n",
      "Average test loss: 0.002877009393233392\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08085544350412156\n",
      "Average test loss: 0.002933298882510927\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08051392435365252\n",
      "Average test loss: 0.0028594284082452456\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08008940042389763\n",
      "Average test loss: 0.0029005398673729765\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07979402876562543\n",
      "Average test loss: 0.002922586542243759\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0794690517452028\n",
      "Average test loss: 0.0029064678204142385\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07906835170586904\n",
      "Average test loss: 0.002950627933566769\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07871617446343104\n",
      "Average test loss: 0.002936874819609026\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07846369949976603\n",
      "Average test loss: 0.002954275882182022\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0781918076409234\n",
      "Average test loss: 0.0029332593257228533\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07792677689260907\n",
      "Average test loss: 0.0028839502044849924\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07744570255279541\n",
      "Average test loss: 0.002980439698530568\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07722753895984756\n",
      "Average test loss: 0.002933038034165899\n",
      "Epoch 85/300\n",
      "Average training loss: 0.0768022723197937\n",
      "Average test loss: 0.002978833571076393\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07660573766628902\n",
      "Average test loss: 0.0029905848247516486\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07633915405472119\n",
      "Average test loss: 0.00292322809725172\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07617247931493654\n",
      "Average test loss: 0.0029132437999877664\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07582776951789856\n",
      "Average test loss: 0.0029533366542309524\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07539575314521789\n",
      "Average test loss: 0.0029389918719728786\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07537927924593289\n",
      "Average test loss: 0.0029855426690644686\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07490934259361691\n",
      "Average test loss: 0.003046156420889828\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07480786903036965\n",
      "Average test loss: 0.0029337785355746746\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07438669327894847\n",
      "Average test loss: 0.002924278176079194\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07425079379147953\n",
      "Average test loss: 0.0030628738962113857\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07381233304738999\n",
      "Average test loss: 0.0030510313978625667\n",
      "Epoch 97/300\n",
      "Average training loss: 0.07356464818782277\n",
      "Average test loss: 0.00296022571499149\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07353279153174824\n",
      "Average test loss: 0.002964673354393906\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07322360230485599\n",
      "Average test loss: 0.0030254155643698244\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07307677466339535\n",
      "Average test loss: 0.0029412908144295217\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07274693481789694\n",
      "Average test loss: 0.002909182883385155\n",
      "Epoch 102/300\n",
      "Average training loss: 0.072655034042067\n",
      "Average test loss: 0.0030850387004514534\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07249654811289576\n",
      "Average test loss: 0.0029725116071187786\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07214696561296781\n",
      "Average test loss: 0.0030574617619729704\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07198069064484702\n",
      "Average test loss: 0.0029957807726330226\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07174045089218352\n",
      "Average test loss: 0.0030512118082907467\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07162948423292902\n",
      "Average test loss: 0.003003904704418447\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07131252704726325\n",
      "Average test loss: 0.0030833008788112137\n",
      "Epoch 109/300\n",
      "Average training loss: 0.07141005414062077\n",
      "Average test loss: 0.0029687671160532367\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07098548473914465\n",
      "Average test loss: 0.0030503534066180387\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07087917989823553\n",
      "Average test loss: 0.0030142455773635043\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07068402379751205\n",
      "Average test loss: 0.003144247411336336\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07055874347024493\n",
      "Average test loss: 0.0030447547210173476\n",
      "Epoch 114/300\n",
      "Average training loss: 0.07042042870322864\n",
      "Average test loss: 0.003035886734724045\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07024284981356727\n",
      "Average test loss: 0.0030642422404554156\n",
      "Epoch 116/300\n",
      "Average training loss: 0.06997722860839632\n",
      "Average test loss: 0.0030070641498184864\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07011732323964437\n",
      "Average test loss: 0.003003453524990214\n",
      "Epoch 118/300\n",
      "Average training loss: 0.06960873926679294\n",
      "Average test loss: 0.0030820870453284847\n",
      "Epoch 119/300\n",
      "Average training loss: 0.06954059441222085\n",
      "Average test loss: 0.003782744348877006\n",
      "Epoch 120/300\n",
      "Average training loss: 0.06948284216721852\n",
      "Average test loss: 0.003008164311759174\n",
      "Epoch 121/300\n",
      "Average training loss: 0.06908742365903324\n",
      "Average test loss: 0.0029990281094279553\n",
      "Epoch 122/300\n",
      "Average training loss: 0.06952992367082172\n",
      "Average test loss: 0.003126038293664654\n",
      "Epoch 123/300\n",
      "Average training loss: 0.0688308490978347\n",
      "Average test loss: 0.0029941143716375033\n",
      "Epoch 124/300\n",
      "Average training loss: 0.06876725021004677\n",
      "Average test loss: 0.0029691643433438406\n",
      "Epoch 125/300\n",
      "Average training loss: 0.06863815804322561\n",
      "Average test loss: 0.003040303314829038\n",
      "Epoch 126/300\n",
      "Average training loss: 0.06855985071923998\n",
      "Average test loss: 0.003003683605334825\n",
      "Epoch 127/300\n",
      "Average training loss: 0.06833261918359333\n",
      "Average test loss: 0.0030261552602880533\n",
      "Epoch 128/300\n",
      "Average training loss: 0.06849410895506541\n",
      "Average test loss: 0.0030885960857073465\n",
      "Epoch 129/300\n",
      "Average training loss: 0.06818707590964106\n",
      "Average test loss: 0.003018112663179636\n",
      "Epoch 130/300\n",
      "Average training loss: 0.06798197821113798\n",
      "Average test loss: 0.0031585052261749904\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0685384351213773\n",
      "Average test loss: 0.0030392015001012218\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0676688453455766\n",
      "Average test loss: 0.0030559672945075565\n",
      "Epoch 133/300\n",
      "Average training loss: 0.06748216445578469\n",
      "Average test loss: 0.0030207286371539037\n",
      "Epoch 134/300\n",
      "Average training loss: 0.06752513647079468\n",
      "Average test loss: 0.003111791434801287\n",
      "Epoch 135/300\n",
      "Average training loss: 0.06745129529966248\n",
      "Average test loss: 0.0030737478385368983\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06730443228284518\n",
      "Average test loss: 0.003098336402326822\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06712780794501305\n",
      "Average test loss: 0.0030655014423860445\n",
      "Epoch 138/300\n",
      "Average training loss: 0.06710254402955373\n",
      "Average test loss: 0.003042232828421725\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0668632232911057\n",
      "Average test loss: 0.003086427669144339\n",
      "Epoch 140/300\n",
      "Average training loss: 0.06684152224328783\n",
      "Average test loss: 0.003097563340846035\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0667591724594434\n",
      "Average test loss: 0.0031271341265075737\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0665808762245708\n",
      "Average test loss: 0.0031513340810520782\n",
      "Epoch 143/300\n",
      "Average training loss: 0.06652011361055904\n",
      "Average test loss: 0.0030922540600101154\n",
      "Epoch 144/300\n",
      "Average training loss: 0.06638148347536722\n",
      "Average test loss: 0.0030451200087037353\n",
      "Epoch 145/300\n",
      "Average training loss: 0.06629179542925623\n",
      "Average test loss: 0.003111135587303175\n",
      "Epoch 146/300\n",
      "Average training loss: 0.06623254870706134\n",
      "Average test loss: 0.0030864457862658633\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06613962714539634\n",
      "Average test loss: 0.0030898908080740106\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06592967691686419\n",
      "Average test loss: 0.003181078415777948\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06588218391603894\n",
      "Average test loss: 0.003137385060182876\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06583379656407568\n",
      "Average test loss: 0.003158728381204936\n",
      "Epoch 151/300\n",
      "Average training loss: 0.06573849463131692\n",
      "Average test loss: 0.0031477232600251835\n",
      "Epoch 152/300\n",
      "Average training loss: 0.06579641335871485\n",
      "Average test loss: 0.003184186655821072\n",
      "Epoch 153/300\n",
      "Average training loss: 0.06555438515875074\n",
      "Average test loss: 0.003281448378538092\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06553364809354147\n",
      "Average test loss: 0.00305540512688458\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0653845777908961\n",
      "Average test loss: 0.0030712516465120844\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06531796291470528\n",
      "Average test loss: 0.003460401942125625\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06515236882699861\n",
      "Average test loss: 0.0032016982016050155\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06504820478293631\n",
      "Average test loss: 0.0031778890062123537\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06502122347884708\n",
      "Average test loss: 0.0030920812617987394\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06501183442605867\n",
      "Average test loss: 0.0031747409848289355\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06478729652365049\n",
      "Average test loss: 0.003184778918201725\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0647128264175521\n",
      "Average test loss: 0.003016119014368289\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06472392302751541\n",
      "Average test loss: 0.003071576705823342\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06454423885875278\n",
      "Average test loss: 0.0031546293697837327\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06455537266863717\n",
      "Average test loss: 0.0031147784719036684\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06439591018358866\n",
      "Average test loss: 0.003146533487778571\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06432388733492957\n",
      "Average test loss: 0.003145815206484662\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06435233672128783\n",
      "Average test loss: 0.003090200358277394\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0642150017188655\n",
      "Average test loss: 0.003270178374316957\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06407772758603096\n",
      "Average test loss: 0.0030721109052085213\n",
      "Epoch 171/300\n",
      "Average training loss: 0.06400352658165825\n",
      "Average test loss: 0.0034225964513089923\n",
      "Epoch 172/300\n",
      "Average training loss: 0.06387653519378768\n",
      "Average test loss: 0.0031683445413493447\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06410408378971948\n",
      "Average test loss: 0.0031267113540735508\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06378798952698708\n",
      "Average test loss: 0.003195453896497687\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06353966674539778\n",
      "Average test loss: 0.0032904950754923954\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06365955018003781\n",
      "Average test loss: 0.003045396683116754\n",
      "Epoch 177/300\n",
      "Average training loss: 0.06363829031586647\n",
      "Average test loss: 0.003114125405334764\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06363913565542963\n",
      "Average test loss: 0.0031938576199528244\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06361425420310762\n",
      "Average test loss: 0.003140480518961946\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0635890642106533\n",
      "Average test loss: 0.0031263095154944395\n",
      "Epoch 181/300\n",
      "Average training loss: 0.063342177953985\n",
      "Average test loss: 0.00317565161589947\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0632930269671811\n",
      "Average test loss: 0.003174754338752892\n",
      "Epoch 183/300\n",
      "Average training loss: 0.06319187628891733\n",
      "Average test loss: 0.0034171225726604463\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06296369691027535\n",
      "Average test loss: 0.003167821937965022\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06312871837615967\n",
      "Average test loss: 0.003112066792117225\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06315281953745418\n",
      "Average test loss: 0.003274100345042017\n",
      "Epoch 187/300\n",
      "Average training loss: 0.06289961309565438\n",
      "Average test loss: 0.0031672517688324056\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06276957016189894\n",
      "Average test loss: 0.003173798860154218\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06284722329510582\n",
      "Average test loss: 0.0030310640316456557\n",
      "Epoch 190/300\n",
      "Average training loss: 0.06272829319039981\n",
      "Average test loss: 0.0031902981402559412\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0626298045847151\n",
      "Average test loss: 0.003103170054861241\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06276638677716255\n",
      "Average test loss: 0.003121582892619901\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06250338112976817\n",
      "Average test loss: 0.003082123369185461\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06250974807143211\n",
      "Average test loss: 0.0031545161077131826\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06228514462047153\n",
      "Average test loss: 0.0031973381214257745\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06233464652299881\n",
      "Average test loss: 0.0030922130888534917\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06238984821571244\n",
      "Average test loss: 0.0031442867786520058\n",
      "Epoch 198/300\n",
      "Average training loss: 0.062142872813675136\n",
      "Average test loss: 0.0032357117138389086\n",
      "Epoch 199/300\n",
      "Average training loss: 0.062194966624180476\n",
      "Average test loss: 0.003157465371406741\n",
      "Epoch 200/300\n",
      "Average training loss: 0.062151870803700555\n",
      "Average test loss: 0.0031555465910997654\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0620254209736983\n",
      "Average test loss: 0.003221907185597552\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06239284094174703\n",
      "Average test loss: 0.003225478672629429\n",
      "Epoch 203/300\n",
      "Average training loss: 0.061988085475232865\n",
      "Average test loss: 0.0031434739087190895\n",
      "Epoch 204/300\n",
      "Average training loss: 0.061882447567250996\n",
      "Average test loss: 0.003187344398556484\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06190250265929434\n",
      "Average test loss: 0.003160772355687287\n",
      "Epoch 206/300\n",
      "Average training loss: 0.061749571684334016\n",
      "Average test loss: 0.003136085624496142\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06179344703753789\n",
      "Average test loss: 0.0031782352164801625\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06174616147081057\n",
      "Average test loss: 0.0032842126447293492\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06161019073261155\n",
      "Average test loss: 0.0031596881124294467\n",
      "Epoch 210/300\n",
      "Average training loss: 0.061540587670273254\n",
      "Average test loss: 0.0031642533246841694\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06158138058582942\n",
      "Average test loss: 0.003244290430926614\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06158852415614658\n",
      "Average test loss: 0.003148698417469859\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06149249962634511\n",
      "Average test loss: 0.003148457265665962\n",
      "Epoch 214/300\n",
      "Average training loss: 0.061347201569212806\n",
      "Average test loss: 0.0032119734017178416\n",
      "Epoch 215/300\n",
      "Average training loss: 0.061471601963043215\n",
      "Average test loss: 0.0031519145713084272\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06141781875160005\n",
      "Average test loss: 0.003132249353039596\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06127778389387661\n",
      "Average test loss: 0.0031699236558957234\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06112733300692505\n",
      "Average test loss: 0.0031715342125131023\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0612579151822461\n",
      "Average test loss: 0.003227880886031522\n",
      "Epoch 220/300\n",
      "Average training loss: 0.061038708438475926\n",
      "Average test loss: 0.00319144052411947\n",
      "Epoch 221/300\n",
      "Average training loss: 0.061029343333509234\n",
      "Average test loss: 0.0032075175835440555\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06100572990377744\n",
      "Average test loss: 0.0031777980466269787\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06103064636058277\n",
      "Average test loss: 0.003248473370861676\n",
      "Epoch 224/300\n",
      "Average training loss: 0.060982912944422825\n",
      "Average test loss: 0.003214969590409762\n",
      "Epoch 225/300\n",
      "Average training loss: 0.060837830947505105\n",
      "Average test loss: 0.0031356761200974385\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06081207341618008\n",
      "Average test loss: 0.0031668400909337733\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06073421197136243\n",
      "Average test loss: 0.0032188170241812864\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06071199576391114\n",
      "Average test loss: 0.003288883604316248\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06073286030027601\n",
      "Average test loss: 0.003138184678223398\n",
      "Epoch 230/300\n",
      "Average training loss: 0.06080586490366194\n",
      "Average test loss: 0.0032678006572855844\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06053653568029404\n",
      "Average test loss: 0.003260981903721889\n",
      "Epoch 232/300\n",
      "Average training loss: 0.06055232347713577\n",
      "Average test loss: 0.0032497217991492816\n",
      "Epoch 233/300\n",
      "Average training loss: 0.06057527712980906\n",
      "Average test loss: 0.0031938678208324645\n",
      "Epoch 234/300\n",
      "Average training loss: 0.06040348213248783\n",
      "Average test loss: 0.0031613805173999732\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06039193536837896\n",
      "Average test loss: 0.003161192029300663\n",
      "Epoch 236/300\n",
      "Average training loss: 0.060307210326194766\n",
      "Average test loss: 0.003200001411139965\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06036852308445507\n",
      "Average test loss: 0.003127133096464806\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06029248963130845\n",
      "Average test loss: 0.003170349787092871\n",
      "Epoch 239/300\n",
      "Average training loss: 0.060301803376939564\n",
      "Average test loss: 0.003159571356036597\n",
      "Epoch 240/300\n",
      "Average training loss: 0.060263562818368276\n",
      "Average test loss: 0.0030987794374426206\n",
      "Epoch 241/300\n",
      "Average training loss: 0.060228457712464864\n",
      "Average test loss: 0.0032263211713482935\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06001749864220619\n",
      "Average test loss: 0.0032159931825266946\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05993824921713935\n",
      "Average test loss: 0.0031863006787995495\n",
      "Epoch 244/300\n",
      "Average training loss: 0.060014747195773654\n",
      "Average test loss: 0.0031797806007994546\n",
      "Epoch 245/300\n",
      "Average training loss: 0.059971730490525565\n",
      "Average test loss: 0.003165717167986764\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06008654486470752\n",
      "Average test loss: 0.0033691796472089157\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05993606026305093\n",
      "Average test loss: 0.0031939131563736333\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05979906230502659\n",
      "Average test loss: 0.0031557205952703954\n",
      "Epoch 249/300\n",
      "Average training loss: 0.05994061574671004\n",
      "Average test loss: 0.003217284909139077\n",
      "Epoch 250/300\n",
      "Average training loss: 0.059768359515402054\n",
      "Average test loss: 0.003214941173377964\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05977861946490076\n",
      "Average test loss: 0.0032004507420998482\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05964162981510163\n",
      "Average test loss: 0.0031497327780558005\n",
      "Epoch 253/300\n",
      "Average training loss: 0.0596750685373942\n",
      "Average test loss: 0.0031361461666723094\n",
      "Epoch 254/300\n",
      "Average training loss: 0.059763438595665824\n",
      "Average test loss: 0.0031912611758129466\n",
      "Epoch 255/300\n",
      "Average training loss: 0.05968452457918061\n",
      "Average test loss: 0.0031590333773444095\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0594967102309068\n",
      "Average test loss: 0.0031959979590028523\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05953287656770812\n",
      "Average test loss: 0.00321953876896037\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05954654341273838\n",
      "Average test loss: 0.0031963347463558117\n",
      "Epoch 259/300\n",
      "Average training loss: 0.059560432023472255\n",
      "Average test loss: 0.0031972475838330057\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05960457870033052\n",
      "Average test loss: 0.003243442579276032\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05953017442093955\n",
      "Average test loss: 0.0035561476128382814\n",
      "Epoch 262/300\n",
      "Average training loss: 0.059409814450475905\n",
      "Average test loss: 0.003246890705285801\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05925627672341135\n",
      "Average test loss: 0.0033173239359425176\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05921629007988506\n",
      "Average test loss: 0.0032275335277534194\n",
      "Epoch 265/300\n",
      "Average training loss: 0.059263215399450724\n",
      "Average test loss: 0.0031557241617184545\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05910612622565693\n",
      "Average test loss: 0.0031759425917019446\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05913527508907848\n",
      "Average test loss: 0.003157612044364214\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05913613636295001\n",
      "Average test loss: 0.0031506300572719838\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05903047769930628\n",
      "Average test loss: 0.003261418565072947\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05903282523155212\n",
      "Average test loss: 0.003210305837914348\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05907933598094516\n",
      "Average test loss: 0.00319196709845629\n",
      "Epoch 272/300\n",
      "Average training loss: 0.058932009766499205\n",
      "Average test loss: 0.003194100364421805\n",
      "Epoch 273/300\n",
      "Average training loss: 0.058921253899733225\n",
      "Average test loss: 0.003237192039895389\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05897285471028752\n",
      "Average test loss: 0.0032530633486393427\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05886486104461882\n",
      "Average test loss: 0.0031811394658353594\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05893712465961774\n",
      "Average test loss: 0.003242846621821324\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05882146318587992\n",
      "Average test loss: 0.0032234972442189854\n",
      "Epoch 278/300\n",
      "Average training loss: 0.05903400642673175\n",
      "Average test loss: 0.0032112563765711254\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0589024079144001\n",
      "Average test loss: 0.0031632982059899306\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05862885071833929\n",
      "Average test loss: 0.0032845645021233293\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0587426712612311\n",
      "Average test loss: 0.003255403808628519\n",
      "Epoch 282/300\n",
      "Average training loss: 0.058710158477226895\n",
      "Average test loss: 0.0032693041749298573\n",
      "Epoch 283/300\n",
      "Average training loss: 0.058592974689271715\n",
      "Average test loss: 0.0031831562121709188\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05843509094251527\n",
      "Average test loss: 0.0033039602835973104\n",
      "Epoch 285/300\n",
      "Average training loss: 0.058501938071515824\n",
      "Average test loss: 0.0032630341485556627\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0584825403491656\n",
      "Average test loss: 0.0032003572383481594\n",
      "Epoch 287/300\n",
      "Average training loss: 0.05846807923913002\n",
      "Average test loss: 0.003190369541239407\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05852575668361452\n",
      "Average test loss: 0.003185499221086502\n",
      "Epoch 289/300\n",
      "Average training loss: 0.05849132814672258\n",
      "Average test loss: 0.0032516350752363602\n",
      "Epoch 290/300\n",
      "Average training loss: 0.058315419620937774\n",
      "Average test loss: 0.003184916970216566\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05840139293670654\n",
      "Average test loss: 0.003215351243606872\n",
      "Epoch 292/300\n",
      "Average training loss: 0.058401616318358315\n",
      "Average test loss: 0.0032487126023819047\n",
      "Epoch 293/300\n",
      "Average training loss: 0.058419864270422193\n",
      "Average test loss: 0.0032531879575302203\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05829904020826022\n",
      "Average test loss: 0.003140265840002232\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05825854712724686\n",
      "Average test loss: 0.0031977311331364846\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05813195938865344\n",
      "Average test loss: 0.003141682784590456\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05824285283022457\n",
      "Average test loss: 0.003197267445632153\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05814957938591639\n",
      "Average test loss: 0.0032398631144315002\n",
      "Epoch 299/300\n",
      "Average training loss: 0.058110926654603746\n",
      "Average test loss: 0.0032374849940339724\n",
      "Epoch 300/300\n",
      "Average training loss: 0.058102198640505476\n",
      "Average test loss: 0.0032292754598375823\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 8.62073334927029\n",
      "Average test loss: 0.004773232262581587\n",
      "Epoch 2/300\n",
      "Average training loss: 3.593383954577976\n",
      "Average test loss: 0.005241001316656669\n",
      "Epoch 3/300\n",
      "Average training loss: 2.5515557905832926\n",
      "Average test loss: 0.003705216710145275\n",
      "Epoch 4/300\n",
      "Average training loss: 2.0178690818150837\n",
      "Average test loss: 0.0035873793220768373\n",
      "Epoch 5/300\n",
      "Average training loss: 1.6293957410388522\n",
      "Average test loss: 0.0034985335790034796\n",
      "Epoch 6/300\n",
      "Average training loss: 1.4927600479125978\n",
      "Average test loss: 0.00334755763804747\n",
      "Epoch 7/300\n",
      "Average training loss: 1.2097928970124987\n",
      "Average test loss: 0.0032413163418985075\n",
      "Epoch 8/300\n",
      "Average training loss: 1.021149448023902\n",
      "Average test loss: 0.003119844615045521\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8520293709966872\n",
      "Average test loss: 0.0030887157093319626\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7191752330462138\n",
      "Average test loss: 0.002917071075696084\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6038189672893948\n",
      "Average test loss: 0.0028997578320817813\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5005511089695824\n",
      "Average test loss: 0.0029052633074008755\n",
      "Epoch 13/300\n",
      "Average training loss: 0.41532223494847614\n",
      "Average test loss: 0.0028107459420959155\n",
      "Epoch 14/300\n",
      "Average training loss: 0.34524262234899733\n",
      "Average test loss: 0.002689462530530161\n",
      "Epoch 15/300\n",
      "Average training loss: 0.2897967438167996\n",
      "Average test loss: 0.0026677177651888793\n",
      "Epoch 16/300\n",
      "Average training loss: 0.24730886982546912\n",
      "Average test loss: 0.002544636018367277\n",
      "Epoch 17/300\n",
      "Average training loss: 0.214256915251414\n",
      "Average test loss: 0.0026045447445164124\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1882688706583447\n",
      "Average test loss: 0.0025505830477923156\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1676201405790117\n",
      "Average test loss: 0.0024378653657105235\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1514038826227188\n",
      "Average test loss: 0.002377348897150821\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1378311287164688\n",
      "Average test loss: 0.0024010272942897345\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1268083328737153\n",
      "Average test loss: 0.002321694816566176\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11729872121082412\n",
      "Average test loss: 0.002313418533653021\n",
      "Epoch 24/300\n",
      "Average training loss: 0.10967432533370124\n",
      "Average test loss: 0.0023521339558064936\n",
      "Epoch 25/300\n",
      "Average training loss: 0.10352546511756049\n",
      "Average test loss: 0.002258684564795759\n",
      "Epoch 26/300\n",
      "Average training loss: 0.09831812200281355\n",
      "Average test loss: 0.002244100826895899\n",
      "Epoch 27/300\n",
      "Average training loss: 0.09401364838414722\n",
      "Average test loss: 0.002297881580061383\n",
      "Epoch 28/300\n",
      "Average training loss: 0.09085130555099911\n",
      "Average test loss: 0.0022189473398029802\n",
      "Epoch 29/300\n",
      "Average training loss: 0.08829164664612876\n",
      "Average test loss: 0.0022011457849293948\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0862024007472727\n",
      "Average test loss: 0.00224033533823159\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08442711809608672\n",
      "Average test loss: 0.0022266940073006683\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0828449326356252\n",
      "Average test loss: 0.0021838796253626546\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08130229389336374\n",
      "Average test loss: 0.0021872095030008093\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08006282023588816\n",
      "Average test loss: 0.0021752796754654913\n",
      "Epoch 35/300\n",
      "Average training loss: 0.07880496821800868\n",
      "Average test loss: 0.002128120493143797\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07768327046102948\n",
      "Average test loss: 0.0021417350214388634\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07652282313505808\n",
      "Average test loss: 0.002123630300888585\n",
      "Epoch 38/300\n",
      "Average training loss: 0.075594857275486\n",
      "Average test loss: 0.002127892980662485\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07457871618204646\n",
      "Average test loss: 0.0021171597412063016\n",
      "Epoch 40/300\n",
      "Average training loss: 0.07351906146936947\n",
      "Average test loss: 0.002131971954781976\n",
      "Epoch 41/300\n",
      "Average training loss: 0.07273087567753261\n",
      "Average test loss: 0.00211655559276955\n",
      "Epoch 42/300\n",
      "Average training loss: 0.0717862259613143\n",
      "Average test loss: 0.0021184912969668705\n",
      "Epoch 43/300\n",
      "Average training loss: 0.07110295201672448\n",
      "Average test loss: 0.0020866335448291567\n",
      "Epoch 44/300\n",
      "Average training loss: 0.07028198534581397\n",
      "Average test loss: 0.00222711105313566\n",
      "Epoch 45/300\n",
      "Average training loss: 0.06991607591509819\n",
      "Average test loss: 0.002115787075832486\n",
      "Epoch 46/300\n",
      "Average training loss: 0.06897711585958799\n",
      "Average test loss: 0.0020647144746035336\n",
      "Epoch 47/300\n",
      "Average training loss: 0.06840470652116669\n",
      "Average test loss: 0.002064387827904688\n",
      "Epoch 48/300\n",
      "Average training loss: 0.067906590157085\n",
      "Average test loss: 0.002101350602383415\n",
      "Epoch 49/300\n",
      "Average training loss: 0.06745325217313236\n",
      "Average test loss: 0.00205225689874755\n",
      "Epoch 50/300\n",
      "Average training loss: 0.06660483904017342\n",
      "Average test loss: 0.0021187551048480802\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06625491674741109\n",
      "Average test loss: 0.002081591495830152\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06562269147237142\n",
      "Average test loss: 0.002055400807203518\n",
      "Epoch 53/300\n",
      "Average training loss: 0.06542853160036935\n",
      "Average test loss: 0.002072479157656845\n",
      "Epoch 54/300\n",
      "Average training loss: 0.0647211901711093\n",
      "Average test loss: 0.002078844620742732\n",
      "Epoch 55/300\n",
      "Average training loss: 0.064333019706938\n",
      "Average test loss: 0.002058243824686441\n",
      "Epoch 56/300\n",
      "Average training loss: 0.06394233074122005\n",
      "Average test loss: 0.0020830121903369823\n",
      "Epoch 57/300\n",
      "Average training loss: 0.06347056629260381\n",
      "Average test loss: 0.0020738134856025377\n",
      "Epoch 58/300\n",
      "Average training loss: 0.06304779425594542\n",
      "Average test loss: 0.0020461608842015267\n",
      "Epoch 59/300\n",
      "Average training loss: 0.06267402426070637\n",
      "Average test loss: 0.002049018559563491\n",
      "Epoch 60/300\n",
      "Average training loss: 0.06216843229863379\n",
      "Average test loss: 0.0020473780523364744\n",
      "Epoch 61/300\n",
      "Average training loss: 0.06189166343874401\n",
      "Average test loss: 0.0020618639375186627\n",
      "Epoch 62/300\n",
      "Average training loss: 0.06153080259429084\n",
      "Average test loss: 0.0020531984596616694\n",
      "Epoch 63/300\n",
      "Average training loss: 0.061079566405879124\n",
      "Average test loss: 0.002058468505119284\n",
      "Epoch 64/300\n",
      "Average training loss: 0.06068271855513255\n",
      "Average test loss: 0.002087213240356909\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06039612662461069\n",
      "Average test loss: 0.002055720335079564\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06003646321760284\n",
      "Average test loss: 0.0020819924490319357\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0597272859149509\n",
      "Average test loss: 0.0020505075905885963\n",
      "Epoch 68/300\n",
      "Average training loss: 0.05926785439915127\n",
      "Average test loss: 0.0020719535450140635\n",
      "Epoch 69/300\n",
      "Average training loss: 0.05893407565024164\n",
      "Average test loss: 0.0020644618701189756\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05863387882709503\n",
      "Average test loss: 0.002241724882688787\n",
      "Epoch 71/300\n",
      "Average training loss: 0.058309163904852336\n",
      "Average test loss: 0.0020885090492665766\n",
      "Epoch 72/300\n",
      "Average training loss: 0.057988559610313836\n",
      "Average test loss: 0.0021143107171066934\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05773187303543091\n",
      "Average test loss: 0.0020561193813466365\n",
      "Epoch 74/300\n",
      "Average training loss: 0.057480809032917024\n",
      "Average test loss: 0.002112109046222435\n",
      "Epoch 75/300\n",
      "Average training loss: 0.05646916046407487\n",
      "Average test loss: 0.0021189654796487756\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05635161948866314\n",
      "Average test loss: 0.002111598363249666\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05591520014074114\n",
      "Average test loss: 0.002067160136687259\n",
      "Epoch 80/300\n",
      "Average training loss: 0.055104973554611206\n",
      "Average test loss: 0.0021203064249000615\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05482805239160855\n",
      "Average test loss: 0.00209034634505709\n",
      "Epoch 84/300\n",
      "Average training loss: 0.05477846768829558\n",
      "Average test loss: 0.0021257176263671782\n",
      "Epoch 85/300\n",
      "Average training loss: 0.054270863274733225\n",
      "Average test loss: 0.0021421430092304944\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05404777085781098\n",
      "Average test loss: 0.0022059491735158697\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0539204310046302\n",
      "Average test loss: 0.0021119870542445117\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05360200862089793\n",
      "Average test loss: 0.0021211225342833333\n",
      "Epoch 89/300\n",
      "Average training loss: 0.05336136840449439\n",
      "Average test loss: 0.0021462538751463095\n",
      "Epoch 90/300\n",
      "Average training loss: 0.05342470822400517\n",
      "Average test loss: 0.0021267049977969792\n",
      "Epoch 91/300\n",
      "Average training loss: 0.05293500559528669\n",
      "Average test loss: 0.0021588439769628974\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05286364399062263\n",
      "Average test loss: 0.0021190887965882818\n",
      "Epoch 93/300\n",
      "Average training loss: 0.052577002114719815\n",
      "Average test loss: 0.0021741949854832557\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05236775278713968\n",
      "Average test loss: 0.002156099173343844\n",
      "Epoch 96/300\n",
      "Average training loss: 0.05203422762950261\n",
      "Average test loss: 0.0021417991262343197\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05176865845587519\n",
      "Average test loss: 0.0021552378185507324\n",
      "Epoch 98/300\n",
      "Average training loss: 0.051588238027360706\n",
      "Average test loss: 0.0021779953483492135\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05132722729113367\n",
      "Average test loss: 0.002156789635411567\n",
      "Epoch 100/300\n",
      "Average training loss: 0.051212371574507816\n",
      "Average test loss: 0.0021822825885481306\n",
      "Epoch 101/300\n",
      "Average training loss: 0.05108455561267005\n",
      "Average test loss: 0.0021447393976979784\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05091311953134007\n",
      "Average test loss: 0.0021742680817842482\n",
      "Epoch 103/300\n",
      "Average training loss: 0.050683467318614325\n",
      "Average test loss: 0.0021986611931481296\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05062742551167806\n",
      "Average test loss: 0.002143648352680935\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05038864478137758\n",
      "Average test loss: 0.0021552062527173095\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05016181715660625\n",
      "Average test loss: 0.0022453751351891294\n",
      "Epoch 107/300\n",
      "Average training loss: 0.050137804571125245\n",
      "Average test loss: 0.002157883108076122\n",
      "Epoch 108/300\n",
      "Average training loss: 0.049864523043235146\n",
      "Average test loss: 0.0021581945741135215\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04982961941096518\n",
      "Average test loss: 0.0021986837786518863\n",
      "Epoch 110/300\n",
      "Average training loss: 0.049647899819744955\n",
      "Average test loss: 0.002150551294700967\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04958878766165839\n",
      "Average test loss: 0.002203627127740118\n",
      "Epoch 112/300\n",
      "Average training loss: 0.049249384565485846\n",
      "Average test loss: 0.0021974466058115164\n",
      "Epoch 113/300\n",
      "Average training loss: 0.04929631212022569\n",
      "Average test loss: 0.0021501730386581687\n",
      "Epoch 114/300\n",
      "Average training loss: 0.049110293007559244\n",
      "Average test loss: 0.0021837609665882256\n",
      "Epoch 115/300\n",
      "Average training loss: 0.0489461285604371\n",
      "Average test loss: 0.0021855733038650617\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04893946342998081\n",
      "Average test loss: 0.0022013894950764046\n",
      "Epoch 117/300\n",
      "Average training loss: 0.048663710256417594\n",
      "Average test loss: 0.0021771449285248917\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04851075904402468\n",
      "Average test loss: 0.002203299646990167\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04842955266767078\n",
      "Average test loss: 0.0021773543767631054\n",
      "Epoch 120/300\n",
      "Average training loss: 0.04827912048498789\n",
      "Average test loss: 0.0022576193445258672\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04830514290597704\n",
      "Average test loss: 0.0022587905284017326\n",
      "Epoch 122/300\n",
      "Average training loss: 0.048168619983726076\n",
      "Average test loss: 0.0022322807216809855\n",
      "Epoch 123/300\n",
      "Average training loss: 0.048006623165475\n",
      "Average test loss: 0.0023098244855387345\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04788921404381593\n",
      "Average test loss: 0.002388669206864304\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04775522081388368\n",
      "Average test loss: 0.0022043141329454053\n",
      "Epoch 126/300\n",
      "Average training loss: 0.047720353454351425\n",
      "Average test loss: 0.002226909829924504\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04762396117217011\n",
      "Average test loss: 0.002287302601047688\n",
      "Epoch 128/300\n",
      "Average training loss: 0.047474185827705594\n",
      "Average test loss: 0.002208709587653478\n",
      "Epoch 129/300\n",
      "Average training loss: 0.047365768995549944\n",
      "Average test loss: 0.002240337734421094\n",
      "Epoch 130/300\n",
      "Average training loss: 0.04740864715642399\n",
      "Average test loss: 0.002301389966884421\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04714397627777524\n",
      "Average test loss: 0.002273672061661879\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04699884677264426\n",
      "Average test loss: 0.0022315612545029985\n",
      "Epoch 133/300\n",
      "Average training loss: 0.04706033422218429\n",
      "Average test loss: 0.002250142436578042\n",
      "Epoch 134/300\n",
      "Average training loss: 0.046905149870448644\n",
      "Average test loss: 0.0022282033057676423\n",
      "Epoch 135/300\n",
      "Average training loss: 0.046950661258565056\n",
      "Average test loss: 0.0022622238289978767\n",
      "Epoch 136/300\n",
      "Average test loss: 0.0022911367991732224\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04653107880221473\n",
      "Average test loss: 0.002352906566320194\n",
      "Epoch 139/300\n",
      "Average training loss: 0.046523124711381064\n",
      "Average test loss: 0.002281322830667098\n",
      "Epoch 140/300\n",
      "Average training loss: 0.0464946997265021\n",
      "Average test loss: 0.002281638290526138\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0463494938313961\n",
      "Average test loss: 0.0022496469165715904\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04626698048578368\n",
      "Average test loss: 0.002234627567852537\n",
      "Epoch 143/300\n",
      "Average training loss: 0.046177921735578116\n",
      "Average test loss: 0.0022218234480565623\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04603695339295599\n",
      "Average test loss: 0.0022617933996435667\n",
      "Epoch 145/300\n",
      "Average training loss: 0.046056044750743444\n",
      "Average test loss: 0.0022497319055514203\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04593330344226625\n",
      "Average test loss: 0.0022912804134604\n",
      "Epoch 147/300\n",
      "Average training loss: 0.0457303126818604\n",
      "Average test loss: 0.002271613167391883\n",
      "Epoch 148/300\n",
      "Average training loss: 0.045872904555665124\n",
      "Average test loss: 0.002218779338730706\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04576216875844532\n",
      "Average test loss: 0.0022296214596264894\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04567990133166313\n",
      "Average test loss: 0.0022361609566335875\n",
      "Epoch 151/300\n",
      "Average training loss: 0.045752396209372416\n",
      "Average test loss: 0.0022276141858763167\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04551975865827666\n",
      "Average test loss: 0.0022236779522564675\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04546057012346056\n",
      "Average test loss: 0.002243504668482476\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04525068825152185\n",
      "Average test loss: 0.0022872351478371356\n",
      "Epoch 156/300\n",
      "Average training loss: 0.04518891184859806\n",
      "Average test loss: 0.0022682612537302904\n",
      "Epoch 157/300\n",
      "Average training loss: 0.045202284687095216\n",
      "Average test loss: 0.0023434192336474857\n",
      "Epoch 158/300\n",
      "Average training loss: 0.045085078597068784\n",
      "Average test loss: 0.0022703558752934136\n",
      "Epoch 159/300\n",
      "Average training loss: 0.04508269825246599\n",
      "Average test loss: 0.002249530046971308\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04495411279466417\n",
      "Average test loss: 0.002263514051420821\n",
      "Epoch 161/300\n",
      "Average training loss: 0.044772688671946524\n",
      "Average test loss: 0.0023272878035075135\n",
      "Epoch 162/300\n",
      "Average training loss: 0.044933156861199273\n",
      "Average test loss: 0.0023119393575729594\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04475825364059872\n",
      "Average test loss: 0.002263471472180552\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04487385716537635\n",
      "Average test loss: 0.002245071722711954\n",
      "Epoch 165/300\n",
      "Average training loss: 0.044768197183807694\n",
      "Average test loss: 0.0023479634543052976\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04457727642854055\n",
      "Average test loss: 0.0022675166368070574\n",
      "Epoch 168/300\n",
      "Average training loss: 0.044483840194013385\n",
      "Average test loss: 0.0023005697367091973\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0443928884698285\n",
      "Average test loss: 0.0023065858564029137\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04432333193222682\n",
      "Average test loss: 0.002466318749822676\n",
      "Epoch 171/300\n",
      "Average training loss: 0.044297068781322904\n",
      "Average test loss: 0.002238399029709399\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04426471183366246\n",
      "Average test loss: 0.002295172081225448\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04420870768361621\n",
      "Average test loss: 0.0022579385586496857\n",
      "Epoch 175/300\n",
      "Average training loss: 0.044107263260417515\n",
      "Average test loss: 0.002438961083172924\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04411499983403418\n",
      "Average test loss: 0.0022071642260998487\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04398894179695182\n",
      "Average test loss: 0.0023342445378916133\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04388514467411571\n",
      "Average test loss: 0.002290911955551969\n",
      "Epoch 180/300\n",
      "Average training loss: 0.04386487613121669\n",
      "Average test loss: 0.0023309412127774623\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04383785852127605\n",
      "Average test loss: 0.002282453486075004\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0437947311103344\n",
      "Average test loss: 0.0023494196901511816\n",
      "Epoch 183/300\n",
      "Average training loss: 0.043806188735697006\n",
      "Average test loss: 0.0022635796099073355\n",
      "Epoch 184/300\n",
      "Average training loss: 0.043707590523693295\n",
      "Average test loss: 0.0022958488322587477\n",
      "Epoch 185/300\n",
      "Average training loss: 0.04363566660549906\n",
      "Average test loss: 0.002233371007980572\n",
      "Epoch 186/300\n",
      "Average training loss: 0.04365316997965177\n",
      "Average test loss: 0.0022918601457236542\n",
      "Epoch 187/300\n",
      "Average training loss: 0.043559906913174525\n",
      "Average test loss: 0.0032871166820534403\n",
      "Epoch 188/300\n",
      "Average training loss: 0.043496329817507\n",
      "Average test loss: 0.002320627729098002\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04347775428493818\n",
      "Average test loss: 0.0022958181145497493\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04349281956421004\n",
      "Average test loss: 0.0022455889886866015\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04331511475311385\n",
      "Average test loss: 0.002299006750807166\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04330230059391922\n",
      "Average test loss: 0.0023629903354578548\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04318019194735421\n",
      "Average test loss: 0.0023393913203229506\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04318112067050404\n",
      "Average test loss: 0.0023356195877616602\n",
      "Epoch 196/300\n",
      "Average training loss: 0.04317394730448723\n",
      "Average test loss: 0.002278842934821215\n",
      "Epoch 197/300\n",
      "Average training loss: 0.043092425952355067\n",
      "Average test loss: 0.002355981323661076\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04309777268767357\n",
      "Average test loss: 0.0023883998638225927\n",
      "Epoch 199/300\n",
      "Average training loss: 0.04301615823308627\n",
      "Average test loss: 0.0022884632328318225\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04305983607967694\n",
      "Average test loss: 0.0022977359712951714\n",
      "Epoch 201/300\n",
      "Average training loss: 0.04300008241004414\n",
      "Average test loss: 0.0023454553075134754\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0428776568406158\n",
      "Average test loss: 0.002297896863478753\n",
      "Epoch 203/300\n",
      "Average training loss: 0.042801733896136285\n",
      "Average test loss: 0.002304620315010349\n",
      "Epoch 204/300\n",
      "Average training loss: 0.042822840170727836\n",
      "Average test loss: 0.0022986383022119603\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04273919694622358\n",
      "Average test loss: 0.0023347293591747683\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04275204163955318\n",
      "Average test loss: 0.0023094820698930157\n",
      "Epoch 207/300\n",
      "Average training loss: 0.042635940770308176\n",
      "Average test loss: 0.002368952786963847\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04265677756071091\n",
      "Average test loss: 0.0022589302845299245\n",
      "Epoch 210/300\n",
      "Average training loss: 0.042650232795212004\n",
      "Average test loss: 0.0023251193271329007\n",
      "Epoch 211/300\n",
      "Average training loss: 0.04253940403130319\n",
      "Average test loss: 0.002295379287149343\n",
      "Epoch 212/300\n",
      "Average training loss: 0.042653787987099755\n",
      "Average test loss: 0.002355250385072496\n",
      "Epoch 213/300\n",
      "Average training loss: 0.042498941221170955\n",
      "Average test loss: 0.0025000196179995934\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04239109147919549\n",
      "Average test loss: 0.002330992981377575\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04247354367044237\n",
      "Average test loss: 0.0023147170816858608\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04243353433410327\n",
      "Average test loss: 0.005780917926794952\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04234591629770067\n",
      "Average test loss: 0.0022728863451629876\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04231477544705073\n",
      "Average test loss: 0.0022811409529919424\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04230637734134992\n",
      "Average test loss: 0.0023094355859276322\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04226046110524072\n",
      "Average test loss: 0.0022852826420631674\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04219333338406351\n",
      "Average test loss: 0.002312174853765302\n",
      "Epoch 222/300\n",
      "Average training loss: 0.042155581848488916\n",
      "Average test loss: 0.0023228023716559015\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04208452698588371\n",
      "Average test loss: 0.0023134052014599244\n",
      "Epoch 226/300\n",
      "Average training loss: 0.042006209194660186\n",
      "Average test loss: 0.0024614135849600036\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04204539280467563\n",
      "Average test loss: 0.0023313117782688803\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0419636614256435\n",
      "Average test loss: 0.0023285092003643513\n",
      "Epoch 229/300\n",
      "Average training loss: 0.04201209254728423\n",
      "Average test loss: 0.002305525657720864\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04187446003158887\n",
      "Average test loss: 0.00231202132275535\n",
      "Epoch 231/300\n",
      "Average training loss: 0.041911585201819736\n",
      "Average test loss: 0.0023327494093941317\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04176028721200095\n",
      "Average test loss: 0.0023802649999658267\n",
      "Epoch 233/300\n",
      "Average training loss: 0.041797363215022615\n",
      "Average test loss: 0.002352580630323953\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04182412435611089\n",
      "Average test loss: 0.002360133094299171\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04173417174153858\n",
      "Average test loss: 0.002299949884000752\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04171159323056539\n",
      "Average test loss: 0.002330940377381113\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04178887140585317\n",
      "Average test loss: 0.0022818556018173695\n",
      "Epoch 238/300\n",
      "Average test loss: 0.0022968046946658027\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04162217131257057\n",
      "Average test loss: 0.002309806422433919\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04150956350730525\n",
      "Average test loss: 0.0023019520464456745\n",
      "Epoch 242/300\n",
      "Average training loss: 0.041570757493376735\n",
      "Average test loss: 0.0023382403968522946\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04151976171135902\n",
      "Average test loss: 0.002306580792698595\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04153204233116574\n",
      "Average test loss: 0.0022752152281916805\n",
      "Epoch 245/300\n",
      "Average training loss: 0.041505452980597816\n",
      "Average test loss: 0.002406045053568151\n",
      "Epoch 246/300\n",
      "Average training loss: 0.04135983542270131\n",
      "Average test loss: 0.0023117379817283817\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04137714491950141\n",
      "Average test loss: 0.002331239849328995\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04150710552930832\n",
      "Average test loss: 0.002336452472334107\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04140263039204809\n",
      "Average test loss: 0.00233258056650973\n",
      "Epoch 250/300\n",
      "Average training loss: 0.041293183588319356\n",
      "Average test loss: 0.0023197355618079503\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04129265057709482\n",
      "Average test loss: 0.0023648707865633898\n",
      "Epoch 253/300\n",
      "Average training loss: 0.04127048514452245\n",
      "Average test loss: 0.002315696120676067\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04122538167569372\n",
      "Average test loss: 0.0023917121171123453\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04116890209582117\n",
      "Average test loss: 0.0023502452557699546\n",
      "Epoch 256/300\n",
      "Average training loss: 0.04114415484004551\n",
      "Average test loss: 0.0023689555246382953\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04110117761956321\n",
      "Average test loss: 0.0023504451637466747\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04110657953222593\n",
      "Average test loss: 0.0023491106182336806\n",
      "Epoch 259/300\n",
      "Average training loss: 0.04106303546163771\n",
      "Average test loss: 0.0023620077398502164\n",
      "Epoch 260/300\n",
      "Average training loss: 0.0410647760513756\n",
      "Average test loss: 0.002637596697650022\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04107360996802648\n",
      "Average test loss: 0.0024275195059470004\n",
      "Epoch 262/300\n",
      "Average training loss: 0.04100684166782432\n",
      "Average test loss: 0.002333361381250951\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04088219095932113\n",
      "Average test loss: 0.002305964910942647\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04096314742498928\n",
      "Average test loss: 0.002315820533989204\n",
      "Epoch 265/300\n",
      "Average training loss: 0.04095422187116411\n",
      "Average test loss: 0.0024250527783814405\n",
      "Epoch 266/300\n",
      "Average training loss: 0.040867055184311336\n",
      "Average test loss: 0.0023097122019777694\n",
      "Epoch 267/300\n",
      "Average training loss: 0.040843983077340655\n",
      "Average test loss: 0.002339556309704979\n",
      "Epoch 268/300\n",
      "Average training loss: 0.040910232368442746\n",
      "Average test loss: 0.0023278205800387596\n",
      "Epoch 269/300\n",
      "Average training loss: 0.040794803783297535\n",
      "Average test loss: 0.0024504454844734734\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0406993730796708\n",
      "Average test loss: 0.002360048798016376\n",
      "Epoch 272/300\n",
      "Average training loss: 0.040700222743882075\n",
      "Average test loss: 0.0023503161081009443\n",
      "Epoch 273/300\n",
      "Average training loss: 0.04072093131144842\n",
      "Average test loss: 0.0023558897638900414\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04079735214180417\n",
      "Average test loss: 0.0023190045022509166\n",
      "Epoch 275/300\n",
      "Average training loss: 0.040688837130864464\n",
      "Average test loss: 0.002331853777791063\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04080027121967739\n",
      "Average test loss: 0.002328012030468219\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0406461495757103\n",
      "Average test loss: 0.0023478276312558187\n",
      "Epoch 278/300\n",
      "Average training loss: 0.04062912838492128\n",
      "Average test loss: 0.0024010617377029525\n",
      "Epoch 279/300\n",
      "Average training loss: 0.040673736174901325\n",
      "Average test loss: 0.0023193968459963798\n",
      "Epoch 280/300\n",
      "Average training loss: 0.04056119731068611\n",
      "Average test loss: 0.0022982474987188147\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04055204445785946\n",
      "Average test loss: 0.0023594083605955043\n",
      "Epoch 282/300\n",
      "Average training loss: 0.040612347536616855\n",
      "Average test loss: 0.0023831130031289325\n",
      "Epoch 283/300\n",
      "Average training loss: 0.040482388294405404\n",
      "Average test loss: 0.00232652738586896\n",
      "Epoch 286/300\n",
      "Average training loss: 0.04049853763480981\n",
      "Average test loss: 0.002445102091671692\n",
      "Epoch 287/300\n",
      "Average training loss: 0.04047437772485945\n",
      "Average test loss: 0.0023984816399299437\n",
      "Epoch 288/300\n",
      "Average training loss: 0.040343373444345264\n",
      "Average test loss: 0.002375365666217274\n",
      "Epoch 289/300\n",
      "Average training loss: 0.040421014391713674\n",
      "Average test loss: 0.0023431630839283266\n",
      "Epoch 290/300\n",
      "Average training loss: 0.04042933969034089\n",
      "Average test loss: 0.0023692402746528386\n",
      "Epoch 291/300\n",
      "Average training loss: 0.04034970899919669\n",
      "Average test loss: 0.00253389985466169\n",
      "Epoch 292/300\n",
      "Average training loss: 0.04022901847296291\n",
      "Average test loss: 0.00236145673899187\n",
      "Epoch 293/300\n",
      "Average training loss: 0.04029783769779735\n",
      "Average test loss: 0.002487188825176822\n",
      "Epoch 294/300\n",
      "Average training loss: 0.04024527530868848\n",
      "Average test loss: 0.002320165751087997\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0402074730694294\n",
      "Average test loss: 0.002348174067421092\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04017820729811986\n",
      "Average test loss: 0.002331030157291227\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04017258147398631\n",
      "Average test loss: 0.002388152548836337\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04014952545199129\n",
      "Average test loss: 0.002342653484808074\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 9.038266793145073\n",
      "Average test loss: 0.01797960331580705\n",
      "Epoch 2/300\n",
      "Average training loss: 3.3856761714087593\n",
      "Average test loss: 0.003317201437842515\n",
      "Epoch 3/300\n",
      "Average training loss: 2.301637224621243\n",
      "Average test loss: 0.003015699116720094\n",
      "Epoch 4/300\n",
      "Average training loss: 1.753035041279263\n",
      "Average test loss: 0.002850955885524551\n",
      "Epoch 5/300\n",
      "Average training loss: 1.4505771169662476\n",
      "Average test loss: 0.00285217590464486\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1730079044765895\n",
      "Average test loss: 0.0030483388289188344\n",
      "Epoch 7/300\n",
      "Average training loss: 0.965168652481503\n",
      "Average test loss: 0.0024925188983066214\n",
      "Epoch 8/300\n",
      "Average training loss: 0.7904142331547207\n",
      "Average test loss: 0.00242791388121744\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6529721940888299\n",
      "Average test loss: 0.0023001598719921378\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5400858679347568\n",
      "Average test loss: 0.0022198393549770117\n",
      "Epoch 11/300\n",
      "Average training loss: 0.44624014512697857\n",
      "Average test loss: 0.002186539321516951\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3692142313586341\n",
      "Average test loss: 0.0020525010579990014\n",
      "Epoch 13/300\n",
      "Average training loss: 0.3100156269603305\n",
      "Average test loss: 0.002018144621203343\n",
      "Epoch 14/300\n",
      "Average training loss: 0.26285886624124316\n",
      "Average test loss: 0.0020529401656240224\n",
      "Epoch 15/300\n",
      "Average training loss: 0.2242573718494839\n",
      "Average test loss: 0.001900242208917108\n",
      "Epoch 16/300\n",
      "Average training loss: 0.19422497403621675\n",
      "Average test loss: 0.0019737715316522453\n",
      "Epoch 17/300\n",
      "Average training loss: 0.16953838582833608\n",
      "Average test loss: 0.0017916508961675895\n",
      "Epoch 18/300\n",
      "Average training loss: 0.12019181815783182\n",
      "Average test loss: 0.0017221913281828165\n",
      "Epoch 21/300\n",
      "Average training loss: 0.10929518218835195\n",
      "Average test loss: 0.0016995852417829964\n",
      "Epoch 22/300\n",
      "Average training loss: 0.09986092055506177\n",
      "Average test loss: 0.0016886332506934802\n",
      "Epoch 23/300\n",
      "Average training loss: 0.09168992585606045\n",
      "Average test loss: 0.0016916196203480163\n",
      "Epoch 24/300\n",
      "Average training loss: 0.08492476321591272\n",
      "Average test loss: 0.0016474765601257482\n",
      "Epoch 25/300\n",
      "Average training loss: 0.08000335260894563\n",
      "Average test loss: 0.0016358770003749264\n",
      "Epoch 26/300\n",
      "Average training loss: 0.07590588188833661\n",
      "Average test loss: 0.0016321626207273867\n",
      "Epoch 27/300\n",
      "Average training loss: 0.07260367872979906\n",
      "Average test loss: 0.0016096658334136009\n",
      "Epoch 28/300\n",
      "Average training loss: 0.0698818221224679\n",
      "Average test loss: 0.001573489167003168\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06751333896981346\n",
      "Average test loss: 0.0015909635697801908\n",
      "Epoch 30/300\n",
      "Average training loss: 0.06543172242906359\n",
      "Average test loss: 0.0015637027830299403\n",
      "Epoch 31/300\n",
      "Average test loss: 0.0015635372047416037\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06081612254844772\n",
      "Average test loss: 0.0015360799503202239\n",
      "Epoch 34/300\n",
      "Average training loss: 0.05954141906897227\n",
      "Average test loss: 0.001514504581793315\n",
      "Epoch 35/300\n",
      "Average training loss: 0.058450589855511985\n",
      "Average test loss: 0.0015334356497559282\n",
      "Epoch 36/300\n",
      "Average training loss: 0.057249973247448606\n",
      "Average test loss: 0.0015033034947183397\n",
      "Epoch 37/300\n",
      "Average training loss: 0.056261506471369\n",
      "Average test loss: 0.001499114737742477\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05545817603667577\n",
      "Average test loss: 0.0015158669720921252\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05435666048195627\n",
      "Average test loss: 0.0014914256550578608\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05354104995727539\n",
      "Average test loss: 0.0015320917916380697\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05278107423914803\n",
      "Average test loss: 0.001483634058593048\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05201151571340031\n",
      "Average test loss: 0.0015438799307578139\n",
      "Epoch 43/300\n",
      "Average training loss: 0.05137386363082462\n",
      "Average test loss: 0.0014934877783267034\n",
      "Epoch 44/300\n",
      "Average training loss: 0.050735602074199256\n",
      "Average test loss: 0.001464773060546981\n",
      "Epoch 45/300\n",
      "Average training loss: 0.050115959193971424\n",
      "Average test loss: 0.0014620437339569132\n",
      "Epoch 46/300\n",
      "Average training loss: 0.049695615659157436\n",
      "Average test loss: 0.0014940230240527955\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0490663214524587\n",
      "Average test loss: 0.0014582306181805\n",
      "Epoch 48/300\n",
      "Average training loss: 0.048490631921423805\n",
      "Average test loss: 0.0014419644464635187\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04792851026521789\n",
      "Average test loss: 0.0014479903593245479\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04765178798635801\n",
      "Average test loss: 0.0014850532427016232\n",
      "Epoch 51/300\n",
      "Average training loss: 0.047153223961591724\n",
      "Average test loss: 0.0014370779818337824\n",
      "Epoch 52/300\n",
      "Average training loss: 0.04694578727748659\n",
      "Average test loss: 0.001480225425420536\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04641194591919581\n",
      "Average test loss: 0.001465426917705271\n",
      "Epoch 54/300\n",
      "Average training loss: 0.04605380421214633\n",
      "Average test loss: 0.0014492833382553525\n",
      "Epoch 55/300\n",
      "Average training loss: 0.045627362786067854\n",
      "Average test loss: 0.0014653628436952\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04526513272689448\n",
      "Average test loss: 0.0014442608456851707\n",
      "Epoch 57/300\n",
      "Average training loss: 0.044982200609313114\n",
      "Average test loss: 0.0015032609741513928\n",
      "Epoch 58/300\n",
      "Average training loss: 0.044697795808315274\n",
      "Average test loss: 0.0014600672306906847\n",
      "Epoch 59/300\n",
      "Average training loss: 0.044321113371186785\n",
      "Average test loss: 0.0014388967897329065\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04405659182866414\n",
      "Average test loss: 0.0014221814594541987\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04389317147930463\n",
      "Average test loss: 0.0014386079455208447\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04343276447719998\n",
      "Average test loss: 0.001435434585540659\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04301509003506766\n",
      "Average test loss: 0.001440416631495787\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04314425864153438\n",
      "Average test loss: 0.0014322564975462026\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04266147165828281\n",
      "Average test loss: 0.0014741323356413179\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04221260457237561\n",
      "Average test loss: 0.0014513752113820778\n",
      "Epoch 67/300\n",
      "Average training loss: 0.041986339804199004\n",
      "Average test loss: 0.0014335356707581215\n",
      "Epoch 68/300\n",
      "Average training loss: 0.04195649237765206\n",
      "Average test loss: 0.0014699347818063365\n",
      "Epoch 69/300\n",
      "Average training loss: 0.041519930140839684\n",
      "Average test loss: 0.001448920509674483\n",
      "Epoch 70/300\n",
      "Average training loss: 0.0411503308945232\n",
      "Average test loss: 0.0014439045286530421\n",
      "Epoch 71/300\n",
      "Average training loss: 0.040942769878440435\n",
      "Average test loss: 0.001464814426894817\n",
      "Epoch 72/300\n",
      "Average training loss: 0.040847404797871904\n",
      "Average test loss: 0.0014785407233155435\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04046023892694049\n",
      "Average test loss: 0.001455389284218351\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04032985160251459\n",
      "Average test loss: 0.00148223505821079\n",
      "Epoch 75/300\n",
      "Average training loss: 0.03995157224271033\n",
      "Average test loss: 0.0014616120149277979\n",
      "Epoch 76/300\n",
      "Average training loss: 0.039743803400132394\n",
      "Average test loss: 0.0014971174048259855\n",
      "Epoch 77/300\n",
      "Average training loss: 0.039622817479901845\n",
      "Average test loss: 0.0014552827259111736\n",
      "Epoch 78/300\n",
      "Average training loss: 0.039182890362209746\n",
      "Average test loss: 0.0014944518714522321\n",
      "Epoch 79/300\n",
      "Average training loss: 0.03907941365904278\n",
      "Average test loss: 0.001479134967447155\n",
      "Epoch 80/300\n",
      "Average training loss: 0.038913844302296635\n",
      "Average test loss: 0.0015231629125773906\n",
      "Epoch 81/300\n",
      "Average training loss: 0.0386556250916587\n",
      "Average test loss: 0.0014590303011031613\n",
      "Epoch 82/300\n",
      "Average training loss: 0.038661565121677184\n",
      "Average test loss: 0.001483645242949327\n",
      "Epoch 83/300\n",
      "Average training loss: 0.038526492314206226\n",
      "Average test loss: 0.0014970209480573734\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03812191832727856\n",
      "Average test loss: 0.0014951777148784864\n",
      "Epoch 85/300\n",
      "Average training loss: 0.037832745833529365\n",
      "Average test loss: 0.0014637331914984517\n",
      "Epoch 86/300\n",
      "Average training loss: 0.03779554713931348\n",
      "Average test loss: 0.001489478031380309\n",
      "Epoch 87/300\n",
      "Average training loss: 0.037425659080346424\n",
      "Average test loss: 0.0014808344108362993\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03730084543757969\n",
      "Average test loss: 0.0015434568257381518\n",
      "Epoch 89/300\n",
      "Average training loss: 0.037142274614837434\n",
      "Average test loss: 0.0015070133306499985\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03692008853289816\n",
      "Average test loss: 0.0018327058764795463\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03676754102110863\n",
      "Average test loss: 0.001639716318446315\n",
      "Epoch 92/300\n",
      "Average training loss: 0.036624685131841236\n",
      "Average test loss: 0.0015208378391236895\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03656589240663581\n",
      "Average test loss: 0.001506271356716752\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03633475893404749\n",
      "Average test loss: 0.022763746772375373\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03621456279688411\n",
      "Average test loss: 0.0014857842127482097\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03599255112144682\n",
      "Average test loss: 0.0016303771117495167\n",
      "Epoch 97/300\n",
      "Average training loss: 0.035828322655624814\n",
      "Average test loss: 0.0015116226635873318\n",
      "Epoch 98/300\n",
      "Average training loss: 0.035732463334997494\n",
      "Average test loss: 0.001547465558577743\n",
      "Epoch 99/300\n",
      "Average training loss: 0.035591729283332825\n",
      "Average test loss: 0.0015221908117334048\n",
      "Epoch 100/300\n",
      "Average training loss: 0.035542049985792905\n",
      "Average test loss: 0.0015035719625237916\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03533144049180879\n",
      "Average test loss: 0.0015563888795052966\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03517128070526653\n",
      "Average test loss: 0.0015466766960711943\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03505506189333068\n",
      "Average test loss: 0.0015326144518330693\n",
      "Epoch 104/300\n",
      "Average training loss: 0.034964817947811554\n",
      "Average test loss: 0.0015419028505372504\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03477299029131731\n",
      "Average test loss: 0.0016073940213148792\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0347287813756201\n",
      "Average test loss: 0.0015084780688501068\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034623788562085894\n",
      "Average test loss: 0.0015289817032300764\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03464987968570656\n",
      "Average test loss: 0.0015594241851940751\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03439073918097549\n",
      "Average test loss: 0.0015782256932515236\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03425325166847971\n",
      "Average test loss: 0.0015534623180412583\n",
      "Epoch 111/300\n",
      "Average training loss: 0.03416961784329679\n",
      "Average test loss: 0.0015899885029842457\n",
      "Epoch 112/300\n",
      "Average training loss: 0.03417885917425156\n",
      "Average test loss: 0.001549016435423659\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03398741180698077\n",
      "Average test loss: 0.0015980762693410119\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03379957840840022\n",
      "Average test loss: 0.0015208611738764578\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03373462524016698\n",
      "Average test loss: 0.0015244880493523346\n",
      "Epoch 116/300\n",
      "Average training loss: 0.033696919550498326\n",
      "Average test loss: 0.001545205076949464\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03351693256696065\n",
      "Average test loss: 0.001577269344073203\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03350425234105852\n",
      "Average test loss: 0.00378193689427442\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03351469740933842\n",
      "Average test loss: 0.0015961833115046224\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03329367506007353\n",
      "Average test loss: 0.001557739534104864\n",
      "Epoch 121/300\n",
      "Average training loss: 0.033202187365955774\n",
      "Average test loss: 0.0016174928304842776\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03311183737052811\n",
      "Average test loss: 0.0016317001481850942\n",
      "Epoch 123/300\n",
      "Average training loss: 0.033106135487556455\n",
      "Average test loss: 0.0019135377686470747\n",
      "Epoch 124/300\n",
      "Average training loss: 0.033015431940555576\n",
      "Average test loss: 0.001581696257305642\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03282818527685272\n",
      "Average test loss: 0.0015551600633189082\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032759067313538656\n",
      "Average test loss: 0.0015695810398707788\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03277286259002156\n",
      "Average test loss: 0.0015527787246844835\n",
      "Epoch 128/300\n",
      "Average training loss: 0.032680258888337345\n",
      "Average test loss: 0.001593310279978646\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03257864690158102\n",
      "Average test loss: 0.0015947989624821477\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0324739036841525\n",
      "Average test loss: 0.0018652850633694067\n",
      "Epoch 131/300\n",
      "Average training loss: 0.032664973674549\n",
      "Average test loss: 0.0015865558593844374\n",
      "Epoch 132/300\n",
      "Average training loss: 0.032404041606518955\n",
      "Average test loss: 0.0015567328348341915\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03227049266298612\n",
      "Average test loss: 0.0020313994526449178\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03220299063126246\n",
      "Average test loss: 0.0015955423729287254\n",
      "Epoch 135/300\n",
      "Average training loss: 0.03219337983263863\n",
      "Average test loss: 0.0016053130568729507\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03212603675160143\n",
      "Average test loss: 0.0016550164357241658\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0320157354440954\n",
      "Average test loss: 0.0017800150051092108\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03200344605081611\n",
      "Average test loss: 0.0016154404917938842\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03198248379429181\n",
      "Average test loss: 0.0015787696395483282\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03184843836890327\n",
      "Average test loss: 0.00162172307446599\n",
      "Epoch 141/300\n",
      "Average training loss: 0.031792535642782845\n",
      "Average test loss: 0.0015755503847160273\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03168185454275873\n",
      "Average test loss: 0.0016446038453529278\n",
      "Epoch 143/300\n",
      "Average training loss: 0.031655460374222864\n",
      "Average test loss: 0.0016207704714809854\n",
      "Epoch 144/300\n",
      "Average training loss: 0.031663785088393424\n",
      "Average test loss: 0.001569779620402389\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03164324727985594\n",
      "Average test loss: 0.0015764905406783025\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03159351192083624\n",
      "Average test loss: 0.0015670371109412775\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03144701145092646\n",
      "Average test loss: 0.0015773214124557044\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03149994032084942\n",
      "Average test loss: 0.0016053267936739658\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03125832641786999\n",
      "Average test loss: 0.0017008163579222229\n",
      "Epoch 150/300\n",
      "Average training loss: 0.031300854921340944\n",
      "Average test loss: 0.00156258156347192\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03127263639701737\n",
      "Average test loss: 0.0016208083364698622\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03119491612083382\n",
      "Average test loss: 0.0015647780100504557\n",
      "Epoch 153/300\n",
      "Average training loss: 0.031163762585984335\n",
      "Average test loss: 0.001581065818770892\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03125154057807392\n",
      "Average test loss: 0.0016179136961905493\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03105496876935164\n",
      "Average test loss: 0.0016368310182458825\n",
      "Epoch 156/300\n",
      "Average training loss: 0.031009701737099223\n",
      "Average test loss: 0.0016077923940287695\n",
      "Epoch 157/300\n",
      "Average training loss: 0.030963417612844043\n",
      "Average test loss: 0.0017326909514764944\n",
      "Epoch 158/300\n",
      "Average training loss: 0.031084646431108316\n",
      "Average test loss: 0.0019675762675081692\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03088827020757728\n",
      "Average test loss: 0.0016746165722401605\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03090635404487451\n",
      "Average test loss: 0.002948065913282335\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03075754779411687\n",
      "Average test loss: 0.0016143826649834712\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03068763668338458\n",
      "Average test loss: 0.0016090662745862371\n",
      "Epoch 163/300\n",
      "Average training loss: 0.030695053665174377\n",
      "Average test loss: 0.0015893146664732033\n",
      "Epoch 164/300\n",
      "Average training loss: 0.030624540557463963\n",
      "Average test loss: 0.0028866750945647558\n",
      "Epoch 165/300\n",
      "Average training loss: 0.030641739850242933\n",
      "Average test loss: 0.002135682269309958\n",
      "Epoch 166/300\n",
      "Average training loss: 0.030530565510193508\n",
      "Average test loss: 0.0016579954065382482\n",
      "Epoch 167/300\n",
      "Average training loss: 0.030523540183901788\n",
      "Average test loss: 0.0016910576661013895\n",
      "Epoch 168/300\n",
      "Average training loss: 0.030491837691929605\n",
      "Average test loss: 0.0016057271556928754\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03049343128171232\n",
      "Average test loss: 0.0016731031336304214\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03047180658413304\n",
      "Average test loss: 0.001609750795385076\n",
      "Epoch 171/300\n",
      "Average training loss: 0.030324654130472077\n",
      "Average test loss: 0.00207224875378112\n",
      "Epoch 172/300\n",
      "Average training loss: 0.030267968298660384\n",
      "Average test loss: 0.0016093523477514586\n",
      "Epoch 173/300\n",
      "Average training loss: 0.030309654745790695\n",
      "Average test loss: 0.0019811443611979485\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03028868396911356\n",
      "Average test loss: 0.0016957068981395828\n",
      "Epoch 175/300\n",
      "Average training loss: 0.030192481882042356\n",
      "Average test loss: 0.0016193102850682207\n",
      "Epoch 176/300\n",
      "Average training loss: 0.030395891401502822\n",
      "Average test loss: 0.0018474115011178785\n",
      "Epoch 177/300\n",
      "Average training loss: 0.03026877059870296\n",
      "Average test loss: 0.001978967968788412\n",
      "Epoch 178/300\n",
      "Average training loss: 0.030165880873799324\n",
      "Average test loss: 0.001829578497343593\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03003942686981625\n",
      "Average test loss: 0.0019025451244993343\n",
      "Epoch 180/300\n",
      "Average training loss: 0.030003931076990235\n",
      "Average test loss: 0.001596285512153473\n",
      "Epoch 181/300\n",
      "Average training loss: 0.029998631830016772\n",
      "Average test loss: 0.0015914415023807022\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02992155585851934\n",
      "Average test loss: 0.001610104658951362\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029993159014317726\n",
      "Average test loss: 0.0020988423335883354\n",
      "Epoch 184/300\n",
      "Average training loss: 0.029860068884160783\n",
      "Average test loss: 0.001597092076101237\n",
      "Epoch 185/300\n",
      "Average training loss: 0.029870982337329124\n",
      "Average test loss: 0.0016625773654215866\n",
      "Epoch 186/300\n",
      "Average training loss: 0.0299052971088224\n",
      "Average test loss: 0.0016953019987170895\n",
      "Epoch 187/300\n",
      "Average training loss: 0.029795917212963104\n",
      "Average test loss: 0.0016938220169395207\n",
      "Epoch 188/300\n",
      "Average training loss: 0.029722359344363212\n",
      "Average test loss: 0.0016560485147767597\n",
      "Epoch 189/300\n",
      "Average training loss: 0.029696661655273703\n",
      "Average test loss: 0.001945800013633238\n",
      "Epoch 190/300\n",
      "Average training loss: 0.029657750505540106\n",
      "Average test loss: 0.001669321696480943\n",
      "Epoch 191/300\n",
      "Average training loss: 0.029633079376485613\n",
      "Average test loss: 0.0019967229788502056\n",
      "Epoch 192/300\n",
      "Average training loss: 0.029645627942350174\n",
      "Average test loss: 0.0017122680273734862\n",
      "Epoch 193/300\n",
      "Average training loss: 0.029686793158451717\n",
      "Average test loss: 0.0016484745641549428\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02954020733303494\n",
      "Average test loss: 0.001892744734365907\n",
      "Epoch 195/300\n",
      "Average training loss: 0.029478750556707383\n",
      "Average test loss: 0.0016487941205915477\n",
      "Epoch 196/300\n",
      "Average training loss: 0.029480031131042375\n",
      "Average test loss: 0.0016658191724887325\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02947758070793417\n",
      "Average test loss: 0.0016165392060453693\n",
      "Epoch 198/300\n",
      "Average training loss: 0.029443566037548913\n",
      "Average test loss: 0.0016659809535162317\n",
      "Epoch 199/300\n",
      "Average training loss: 0.029472763738698428\n",
      "Average test loss: 0.0017372587652256092\n",
      "Epoch 200/300\n",
      "Average training loss: 0.029456165229280788\n",
      "Average test loss: 0.0016398929667969546\n",
      "Epoch 201/300\n",
      "Average training loss: 0.029339582302504115\n",
      "Average test loss: 0.0017473399178642365\n",
      "Epoch 202/300\n",
      "Average training loss: 0.02929580151206917\n",
      "Average test loss: 0.0016632017783510188\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02926899155974388\n",
      "Average test loss: 0.0016312066233820385\n",
      "Epoch 205/300\n",
      "Average training loss: 0.029316093302435346\n",
      "Average test loss: 0.0016341871962779098\n",
      "Epoch 206/300\n",
      "Average training loss: 0.029271056632200875\n",
      "Average test loss: 0.0016830232529900968\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02916749994787905\n",
      "Average test loss: 0.0016476979225149586\n",
      "Epoch 208/300\n",
      "Average training loss: 0.029184439831309847\n",
      "Average test loss: 0.0016077812575838633\n",
      "Epoch 209/300\n",
      "Average training loss: 0.029180244434210988\n",
      "Average test loss: 0.0016195950581588678\n",
      "Epoch 210/300\n",
      "Average training loss: 0.029055558058122793\n",
      "Average test loss: 0.0019051561783999206\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02906628065307935\n",
      "Average test loss: 0.001641453060735431\n",
      "Epoch 212/300\n",
      "Average training loss: 0.029065295073721145\n",
      "Average test loss: 0.0016844156633855569\n",
      "Epoch 213/300\n",
      "Average training loss: 0.029041308455997044\n",
      "Average test loss: 0.033005621157586575\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02901950694951746\n",
      "Average test loss: 0.0016615406033686465\n",
      "Epoch 215/300\n",
      "Average training loss: 0.028950493219825957\n",
      "Average test loss: 0.0017054245326047143\n",
      "Epoch 216/300\n",
      "Average training loss: 0.02892994909485181\n",
      "Average test loss: 0.001630547231580648\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028995162402590115\n",
      "Average test loss: 0.0016331269261427223\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028933604694075056\n",
      "Average test loss: 0.001633849980102645\n",
      "Epoch 219/300\n",
      "Average training loss: 0.028853904381394388\n",
      "Average test loss: 0.0016472793167663946\n",
      "Epoch 220/300\n",
      "Average training loss: 0.02884823464188311\n",
      "Average test loss: 0.0017454797993931505\n",
      "Epoch 221/300\n",
      "Average training loss: 0.028836029155386817\n",
      "Average test loss: 0.0016718451593381663\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028817572193013296\n",
      "Average test loss: 0.0016500530601996515\n",
      "Epoch 223/300\n",
      "Average training loss: 0.028769017179807028\n",
      "Average test loss: 0.001683996080317431\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028821030151512887\n",
      "Average test loss: 0.0016472386117610666\n",
      "Epoch 225/300\n",
      "Average training loss: 0.02870033779574765\n",
      "Average test loss: 0.001823993759850661\n",
      "Epoch 226/300\n",
      "Average training loss: 0.028728301947315533\n",
      "Average test loss: 0.0016835649995547203\n",
      "Epoch 227/300\n",
      "Average training loss: 0.028769828430480426\n",
      "Average test loss: 0.0016497090824155343\n",
      "Epoch 228/300\n",
      "Average training loss: 0.02868148429526223\n",
      "Average test loss: 0.0016338332163997823\n",
      "Epoch 229/300\n",
      "Average training loss: 0.028689578609334097\n",
      "Average test loss: 0.0016108014732599259\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02862228229476346\n",
      "Average test loss: 0.0016481543471001917\n",
      "Epoch 231/300\n",
      "Average training loss: 0.028638094383809303\n",
      "Average test loss: 0.0017052556531917719\n",
      "Epoch 232/300\n",
      "Average training loss: 0.028600090879533024\n",
      "Average test loss: 0.0016553040710795257\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02854650236169497\n",
      "Average test loss: 0.0016421198981503645\n",
      "Epoch 234/300\n",
      "Average training loss: 0.028526994029680886\n",
      "Average test loss: 0.0016625869943656855\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02858908450934622\n",
      "Average test loss: 0.0017121989406231376\n",
      "Epoch 236/300\n",
      "Average training loss: 0.02857118766506513\n",
      "Average test loss: 0.0016625483979781468\n",
      "Epoch 237/300\n",
      "Average training loss: 0.028420902123053867\n",
      "Average test loss: 0.0017506239046860073\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02847650652130445\n",
      "Average test loss: 0.001689478424170779\n",
      "Epoch 239/300\n",
      "Average training loss: 0.028467677174343002\n",
      "Average test loss: 0.003355150397039122\n",
      "Epoch 240/300\n",
      "Average training loss: 0.028353718077143035\n",
      "Average test loss: 0.0016365227523363298\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02835794062415759\n",
      "Average test loss: 0.001768944115481443\n",
      "Epoch 242/300\n",
      "Average training loss: 0.028338275574975542\n",
      "Average test loss: 0.001762569980488883\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02840063586168819\n",
      "Average test loss: 0.0016771714898447196\n",
      "Epoch 244/300\n",
      "Average training loss: 0.028279813890655835\n",
      "Average test loss: 0.0016836841364080708\n",
      "Epoch 245/300\n",
      "Average training loss: 0.028272132303979663\n",
      "Average test loss: 0.0016559394737705587\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0282952512105306\n",
      "Average test loss: 0.0016740539367828104\n",
      "Epoch 247/300\n",
      "Average training loss: 0.02833320058716668\n",
      "Average test loss: 0.0016761501230713393\n",
      "Epoch 248/300\n",
      "Average training loss: 0.028257165125674673\n",
      "Average test loss: 0.0016786215297049946\n",
      "Epoch 249/300\n",
      "Average training loss: 0.028188109742270574\n",
      "Average test loss: 0.0017171877413574193\n",
      "Epoch 250/300\n",
      "Average training loss: 0.02821797037952476\n",
      "Average test loss: 0.001717219395821707\n",
      "Epoch 251/300\n",
      "Average training loss: 0.028141852726538977\n",
      "Average test loss: 0.0017156768790963623\n",
      "Epoch 252/300\n",
      "Average training loss: 0.0281391535649697\n",
      "Average test loss: 0.0016842825763548413\n",
      "Epoch 253/300\n",
      "Average training loss: 0.028154651353756586\n",
      "Average test loss: 0.001687162355416351\n",
      "Epoch 254/300\n",
      "Average training loss: 0.028142429480950037\n",
      "Average test loss: 0.0016828224507884847\n",
      "Epoch 255/300\n",
      "Average training loss: 0.028110348284244538\n",
      "Average test loss: 0.0016606385101460748\n",
      "Epoch 256/300\n",
      "Average training loss: 0.028132556218239995\n",
      "Average test loss: 0.0017681363032509884\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02810841345290343\n",
      "Average test loss: 0.0016750707546662952\n",
      "Epoch 258/300\n",
      "Average training loss: 0.028119028465615377\n",
      "Average test loss: 0.001668058877500395\n",
      "Epoch 259/300\n",
      "Average training loss: 0.028024744326869647\n",
      "Average test loss: 0.0017349664997309447\n",
      "Epoch 260/300\n",
      "Average training loss: 0.028077906388375495\n",
      "Average test loss: 0.0017049599737963743\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027925436245070562\n",
      "Average test loss: 0.0016534475901474556\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02805928351647324\n",
      "Average test loss: 0.0017079877520187034\n",
      "Epoch 263/300\n",
      "Average training loss: 0.028033780593011114\n",
      "Average test loss: 0.002612025561225083\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02793842244313823\n",
      "Average test loss: 0.0016146440729498863\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027903848811984062\n",
      "Average test loss: 0.0017239256778524982\n",
      "Epoch 266/300\n",
      "Average training loss: 0.027942877943317096\n",
      "Average test loss: 0.0016974457442863948\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02793960930241479\n",
      "Average test loss: 0.001648006988896264\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02785078738629818\n",
      "Average test loss: 0.0016484857653785083\n",
      "Epoch 269/300\n",
      "Average training loss: 0.027847440138459207\n",
      "Average test loss: 0.0017076727317439184\n",
      "Epoch 270/300\n",
      "Average training loss: 0.02796721218360795\n",
      "Average test loss: 0.0016766120205736821\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02779969993399249\n",
      "Average test loss: 0.0016623180918395519\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027764489738477602\n",
      "Average test loss: 0.0016628226899645395\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02784191416700681\n",
      "Average test loss: 0.0017581603768178158\n",
      "Epoch 274/300\n",
      "Average training loss: 0.027834117636084555\n",
      "Average test loss: 0.0017228956350849734\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02776049393746588\n",
      "Average test loss: 0.0018043587980791926\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02767552412218518\n",
      "Average test loss: 0.001683766381504635\n",
      "Epoch 277/300\n",
      "Average training loss: 0.027770790353417396\n",
      "Average test loss: 0.0016353070382028817\n",
      "Epoch 278/300\n",
      "Average training loss: 0.02777562750213676\n",
      "Average test loss: 0.0016405693885559838\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027725001931190492\n",
      "Average test loss: 0.0016212710260620547\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02773511818051338\n",
      "Average test loss: 0.0017099063655154573\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027649396321839756\n",
      "Average test loss: 0.0017405114993453027\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027600230965349408\n",
      "Average test loss: 0.003945731409307983\n",
      "Epoch 283/300\n",
      "Average training loss: 0.027672135457396507\n",
      "Average test loss: 0.0016593214995745155\n",
      "Epoch 284/300\n",
      "Average training loss: 0.027602709258596102\n",
      "Average test loss: 0.0016527307039747635\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027662018972966405\n",
      "Average test loss: 0.0016915973288317522\n",
      "Epoch 286/300\n",
      "Average training loss: 0.027639048220382795\n",
      "Average test loss: 0.0016815249745009673\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02755963472194142\n",
      "Average test loss: 0.001646855374591218\n",
      "Epoch 288/300\n",
      "Average training loss: 0.027562611050075955\n",
      "Average test loss: 0.001677601207461622\n",
      "Epoch 289/300\n",
      "Average training loss: 0.027516371531618965\n",
      "Average test loss: 0.0016821172912087705\n",
      "Epoch 290/300\n",
      "Average training loss: 0.027510085513194404\n",
      "Average test loss: 0.0016994549773840441\n",
      "Epoch 291/300\n",
      "Average training loss: 0.02760064085490174\n",
      "Average test loss: 0.0016483621610742477\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02748653323782815\n",
      "Average test loss: 0.001646724774605698\n",
      "Epoch 293/300\n",
      "Average training loss: 0.02745742660595311\n",
      "Average test loss: 0.0016878009314338367\n",
      "Epoch 294/300\n",
      "Average training loss: 0.027511974102920955\n",
      "Average test loss: 0.0017465823907405137\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02749057645102342\n",
      "Average test loss: 0.0017134902379475534\n",
      "Epoch 296/300\n",
      "Average training loss: 0.027476946961548593\n",
      "Average test loss: 0.0016258158791913754\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02736977322234048\n",
      "Average test loss: 0.001701388045731518\n",
      "Epoch 298/300\n",
      "Average training loss: 0.027410562324855062\n",
      "Average test loss: 0.0017069080624108513\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02741382402843899\n",
      "Average test loss: 0.0019276572859121693\n",
      "Epoch 300/300\n",
      "Average training loss: 0.027412628619207277\n",
      "Average test loss: 0.001629138283121089\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-DCT-Additive-.01/30 Projections'\n",
    "\n",
    "DCT_10_proj30_weights, DCT_10_proj30_hist = train_main(DCT_10_normalized, [1] * num_projections, DCT_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_10_proj30', display=True)\n",
    "\n",
    "DCT_20_proj30_weights, DCT_20_proj30_hist = train_main(DCT_20_normalized, [1] * num_projections, DCT_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_20_proj30', display=True)\n",
    "\n",
    "DCT_30_proj30_weights, DCT_30_proj30_hist = train_main(DCT_30_normalized, [1] * num_projections, DCT_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_30_proj30', display=True)\n",
    "\n",
    "DCT_40_proj30_weights, DCT_40_proj30_hist = train_main(DCT_40_normalized, [1] * num_projections, DCT_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/DCT_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, DCT_10_proj30_weights, DCT_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, DCT_20_proj30_weights, DCT_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, DCT_30_proj30_weights, DCT_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, DCT_40_proj30_weights, DCT_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.81\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.13\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.12\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 28.08\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 28.11\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.03\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.15\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 28.28\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.33\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.87\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.77\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.25\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.30\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.49\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.55\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.56\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.72\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.69\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.88\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.91\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.05\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 29.02\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 30.26\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.20\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.76\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.80\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.88\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.91\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.96\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 31.03\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 31.06\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 31.12\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 31.10\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 31.29\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 31.36\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 31.47\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 31.46\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 31.43\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 29.10\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 30.82\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.17\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.54\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.45\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 31.75\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 31.93\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.20\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.09\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.23\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.30\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 32.31\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 32.34\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 32.36\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.40\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.46\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.34\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.62\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.72\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.79\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.90\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.80\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.79\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 32.90\n"
     ]
    }
   ],
   "source": [
    "DCT_10_proj30_model = MemoryNetwork(DCT_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_20_proj30_model = MemoryNetwork(DCT_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_30_proj30_model = MemoryNetwork(DCT_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "DCT_40_proj30_model = MemoryNetwork(DCT_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "DCT_10_proj30_psnr = average_PSNR(DCT_10_proj30_model, DCT_10_proj30_weights, num_projections, DCT_10_test, print_psnr=True)\n",
    "DCT_20_proj30_psnr = average_PSNR(DCT_20_proj30_model, DCT_20_proj30_weights, num_projections, DCT_20_test, print_psnr=True)\n",
    "DCT_30_proj30_psnr = average_PSNR(DCT_30_proj30_model, DCT_30_proj30_weights, num_projections, DCT_30_test, print_psnr=True)\n",
    "DCT_40_proj30_psnr = average_PSNR(DCT_40_proj30_model, DCT_40_proj30_weights, num_projections, DCT_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(DCT_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
