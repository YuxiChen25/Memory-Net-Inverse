{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.025)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.025)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.025)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.025)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 3)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2439235275189082\n",
      "Average test loss: 0.011546283601058855\n",
      "Epoch 2/300\n",
      "Average training loss: 0.06649092972278595\n",
      "Average test loss: 0.009961043787499268\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06002429973416858\n",
      "Average test loss: 0.00929556762923797\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05647814848356777\n",
      "Average test loss: 0.009294438863380088\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05484282209806972\n",
      "Average test loss: 0.010332172716657321\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05308656574289004\n",
      "Average test loss: 0.008898439862661891\n",
      "Epoch 7/300\n",
      "Average training loss: 0.05134414080447621\n",
      "Average test loss: 0.009015811949140496\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05024015131261614\n",
      "Average test loss: 0.008792651386724578\n",
      "Epoch 9/300\n",
      "Average training loss: 0.049249530573685966\n",
      "Average test loss: 0.008333049951328171\n",
      "Epoch 10/300\n",
      "Average training loss: 0.04852541668547524\n",
      "Average test loss: 0.008392016620271735\n",
      "Epoch 11/300\n",
      "Average training loss: 0.04783210657702552\n",
      "Average test loss: 0.008219080102940401\n",
      "Epoch 12/300\n",
      "Average training loss: 0.04724489284886254\n",
      "Average test loss: 0.008905390640099844\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04652993071079254\n",
      "Average test loss: 0.00789877704034249\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04607633599307802\n",
      "Average test loss: 0.007976713105208344\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04544396223293411\n",
      "Average test loss: 0.007932504347629018\n",
      "Epoch 16/300\n",
      "Average training loss: 0.044951134863826965\n",
      "Average test loss: 0.007738611436966393\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04445790405737029\n",
      "Average test loss: 0.007563908418847455\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04402594989207056\n",
      "Average test loss: 0.0076683933105733655\n",
      "Epoch 19/300\n",
      "Average training loss: 0.04373807019988696\n",
      "Average test loss: 0.007348742899795373\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04331586016549004\n",
      "Average test loss: 0.007435106752233373\n",
      "Epoch 21/300\n",
      "Average training loss: 0.04297930771774716\n",
      "Average test loss: 0.007278904555158483\n",
      "Epoch 22/300\n",
      "Average training loss: 0.04265356061524815\n",
      "Average test loss: 0.007240039115564691\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04228910136885113\n",
      "Average test loss: 0.007149255223986175\n",
      "Epoch 24/300\n",
      "Average training loss: 0.04209481928414768\n",
      "Average test loss: 0.007496123891737726\n",
      "Epoch 25/300\n",
      "Average training loss: 0.04171383812526862\n",
      "Average test loss: 0.007140993153883351\n",
      "Epoch 26/300\n",
      "Average training loss: 0.04156065449449751\n",
      "Average test loss: 0.007054696652624342\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04127826603253682\n",
      "Average test loss: 0.007222809995214145\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04102397342191802\n",
      "Average test loss: 0.006932172168460158\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04080293251077334\n",
      "Average test loss: 0.006940094633234872\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04061615465746986\n",
      "Average test loss: 0.007090771237181293\n",
      "Epoch 31/300\n",
      "Average training loss: 0.040437466551860174\n",
      "Average test loss: 0.006867182905475298\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04020704389280743\n",
      "Average test loss: 0.006801720116701391\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04014650614394082\n",
      "Average test loss: 0.006804430883791712\n",
      "Epoch 34/300\n",
      "Average training loss: 0.039847467886077036\n",
      "Average test loss: 0.006775697759870026\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03971419799658987\n",
      "Average test loss: 0.00703699133462376\n",
      "Epoch 36/300\n",
      "Average training loss: 0.03958939254283905\n",
      "Average test loss: 0.006714122845894761\n",
      "Epoch 37/300\n",
      "Average training loss: 0.039395851757791304\n",
      "Average test loss: 0.0067823069550924835\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03926872005727556\n",
      "Average test loss: 0.007099665326790677\n",
      "Epoch 39/300\n",
      "Average training loss: 0.039226562284761006\n",
      "Average test loss: 0.0066374732595351005\n",
      "Epoch 40/300\n",
      "Average training loss: 0.0390399967763159\n",
      "Average test loss: 0.0065816789865493774\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03902261230680678\n",
      "Average test loss: 0.006602889982362589\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03877261201540629\n",
      "Average test loss: 0.006624750672943062\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03867506573928727\n",
      "Average test loss: 0.006524593635979626\n",
      "Epoch 44/300\n",
      "Average training loss: 0.03859509892265002\n",
      "Average test loss: 0.00648495391342375\n",
      "Epoch 45/300\n",
      "Average training loss: 0.038550532794660995\n",
      "Average test loss: 0.00676732549071312\n",
      "Epoch 46/300\n",
      "Average training loss: 0.038329151037666535\n",
      "Average test loss: 0.006502813772194915\n",
      "Epoch 47/300\n",
      "Average training loss: 0.038411870847145715\n",
      "Average test loss: 0.0064485240471031935\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03819394898911317\n",
      "Average test loss: 0.006505790198014842\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03811505974994765\n",
      "Average test loss: 0.006416530339668195\n",
      "Epoch 50/300\n",
      "Average training loss: 0.037982081949710846\n",
      "Average test loss: 0.006450612420423163\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03791171935697397\n",
      "Average test loss: 0.006461539371560018\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03792190400759379\n",
      "Average test loss: 0.006479992869827482\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03779207490881284\n",
      "Average test loss: 0.0063959914329979155\n",
      "Epoch 54/300\n",
      "Average training loss: 0.037697284867366156\n",
      "Average test loss: 0.006335429694089624\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03759009824527634\n",
      "Average test loss: 0.00644486973186334\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03758201558391253\n",
      "Average test loss: 0.006346862741642528\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03750085185964902\n",
      "Average test loss: 0.006339260242051548\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0374349645210637\n",
      "Average test loss: 0.0063377904887828565\n",
      "Epoch 59/300\n",
      "Average training loss: 0.03738641001118554\n",
      "Average test loss: 0.006356160699907277\n",
      "Epoch 60/300\n",
      "Average training loss: 0.03727875912851757\n",
      "Average test loss: 0.006385526856200563\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03724011723531617\n",
      "Average test loss: 0.006389623449494441\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03719137493107054\n",
      "Average test loss: 0.00633350144740608\n",
      "Epoch 63/300\n",
      "Average training loss: 0.03715763952334722\n",
      "Average test loss: 0.006333860115044647\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0370676660405265\n",
      "Average test loss: 0.006410491035216384\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0370224474867185\n",
      "Average test loss: 0.006349816881948047\n",
      "Epoch 66/300\n",
      "Average training loss: 0.03701875685652097\n",
      "Average test loss: 0.006445345662534237\n",
      "Epoch 67/300\n",
      "Average training loss: 0.036955672310458286\n",
      "Average test loss: 0.006299281472133266\n",
      "Epoch 68/300\n",
      "Average training loss: 0.03691035467386246\n",
      "Average test loss: 0.006390896176712381\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03712990176677704\n",
      "Average test loss: 0.006303798987633652\n",
      "Epoch 70/300\n",
      "Average training loss: 0.036787048218978775\n",
      "Average test loss: 0.006275169871747493\n",
      "Epoch 71/300\n",
      "Average training loss: 0.0367229978028271\n",
      "Average test loss: 0.006577684782445431\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03670405245323976\n",
      "Average test loss: 0.006413579819930924\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03672531474961175\n",
      "Average test loss: 0.006377756806711356\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03667878930767377\n",
      "Average test loss: 0.006493703597535689\n",
      "Epoch 75/300\n",
      "Average training loss: 0.036711058093441856\n",
      "Average test loss: 0.006307126887970501\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0365916268610292\n",
      "Average test loss: 0.006248431373801496\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03654859736230638\n",
      "Average test loss: 0.00638345360962881\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03656012373831537\n",
      "Average test loss: 0.006215409060319265\n",
      "Epoch 79/300\n",
      "Average training loss: 0.036464479045735465\n",
      "Average test loss: 0.006264051253596942\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03644421163201332\n",
      "Average test loss: 0.0066194908474054605\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03647024798724387\n",
      "Average test loss: 0.0062709736699859304\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03639985282553567\n",
      "Average test loss: 0.006317772171149651\n",
      "Epoch 83/300\n",
      "Average training loss: 0.036347770038578246\n",
      "Average test loss: 0.006306495108124283\n",
      "Epoch 84/300\n",
      "Average training loss: 0.036358087986707685\n",
      "Average test loss: 0.006621927266733514\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03636424864663018\n",
      "Average test loss: 0.0061863492992189194\n",
      "Epoch 86/300\n",
      "Average training loss: 0.036296393622954684\n",
      "Average test loss: 0.006257841301461061\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03625581172770924\n",
      "Average test loss: 0.006281527312265502\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03620969499150912\n",
      "Average test loss: 0.006417573798861769\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03621010756161478\n",
      "Average test loss: 0.0062477017655554745\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03618652753200796\n",
      "Average test loss: 0.006214420218020678\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03611282676127222\n",
      "Average test loss: 0.006301285938256317\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03609142943885591\n",
      "Average test loss: 0.006159947785238425\n",
      "Epoch 93/300\n",
      "Average training loss: 0.036150227155950336\n",
      "Average test loss: 0.006238984422551261\n",
      "Epoch 94/300\n",
      "Average training loss: 0.036115813086430235\n",
      "Average test loss: 0.00658630417763359\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03603097278541989\n",
      "Average test loss: 0.006233730617496702\n",
      "Epoch 96/300\n",
      "Average training loss: 0.036015620019700795\n",
      "Average test loss: 0.00617152869866954\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03597964524560505\n",
      "Average test loss: 0.006361260354932811\n",
      "Epoch 98/300\n",
      "Average training loss: 0.03603585884306166\n",
      "Average test loss: 0.006671822607103322\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0360460987455315\n",
      "Average test loss: 0.006180669674442874\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0359164018895891\n",
      "Average test loss: 0.0063153026757968795\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0358902294172181\n",
      "Average test loss: 0.0063803875189688474\n",
      "Epoch 102/300\n",
      "Average training loss: 0.035901586098803416\n",
      "Average test loss: 0.006355519180910455\n",
      "Epoch 103/300\n",
      "Average training loss: 0.035887041343583004\n",
      "Average test loss: 0.006762191295209858\n",
      "Epoch 104/300\n",
      "Average training loss: 0.035866027431355585\n",
      "Average test loss: 0.006214308022210995\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03583835048476855\n",
      "Average test loss: 0.006292937065164248\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03579696104592747\n",
      "Average test loss: 0.0062423822354111405\n",
      "Epoch 107/300\n",
      "Average training loss: 0.035800854855113556\n",
      "Average test loss: 0.0064050414864387775\n",
      "Epoch 108/300\n",
      "Average training loss: 0.035857052147388456\n",
      "Average test loss: 0.0062476254167656104\n",
      "Epoch 109/300\n",
      "Average training loss: 0.035752534002065656\n",
      "Average test loss: 0.006304885806929734\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03576201313071781\n",
      "Average test loss: 0.0061516225698093575\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0357637791203128\n",
      "Average test loss: 0.0069833587921328015\n",
      "Epoch 112/300\n",
      "Average training loss: 0.035667712665266464\n",
      "Average test loss: 0.006251242231163714\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03571145578556591\n",
      "Average test loss: 0.006196959207041396\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03565756922298007\n",
      "Average test loss: 0.006913720429771476\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03563638527194659\n",
      "Average test loss: 0.006225046597007248\n",
      "Epoch 116/300\n",
      "Average training loss: 0.035647456121113565\n",
      "Average test loss: 0.006243098799553182\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03559124760495292\n",
      "Average test loss: 0.006271639671176672\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03564837378263473\n",
      "Average test loss: 0.006773681077692243\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03557956407467524\n",
      "Average test loss: 0.006258021092249287\n",
      "Epoch 120/300\n",
      "Average training loss: 0.03551139823595683\n",
      "Average test loss: 0.00618105818827947\n",
      "Epoch 121/300\n",
      "Average training loss: 0.0355552993979719\n",
      "Average test loss: 0.006268697454283635\n",
      "Epoch 122/300\n",
      "Average training loss: 0.0355334622628159\n",
      "Average test loss: 0.006210200158879161\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03550268871254391\n",
      "Average test loss: 0.00635798988574081\n",
      "Epoch 124/300\n",
      "Average training loss: 0.035484253582027225\n",
      "Average test loss: 0.006370017840630478\n",
      "Epoch 125/300\n",
      "Average training loss: 0.0354899539815055\n",
      "Average test loss: 0.0062840841644340095\n",
      "Epoch 126/300\n",
      "Average training loss: 0.035619627363151975\n",
      "Average test loss: 0.00632449852488935\n",
      "Epoch 127/300\n",
      "Average training loss: 0.035440465132395425\n",
      "Average test loss: 0.006314608650902907\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03542107516858313\n",
      "Average test loss: 0.006178980233354701\n",
      "Epoch 129/300\n",
      "Average training loss: 0.035458054679963324\n",
      "Average test loss: 0.006215268739395672\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03538526812526915\n",
      "Average test loss: 0.006135925622449981\n",
      "Epoch 131/300\n",
      "Average training loss: 0.03539441863364644\n",
      "Average test loss: 0.006252177029434178\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03537007504701614\n",
      "Average test loss: 0.006144320218099488\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03536561059620645\n",
      "Average test loss: 0.006137532723860608\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03538015056318707\n",
      "Average test loss: 0.006213564921584394\n",
      "Epoch 135/300\n",
      "Average training loss: 0.035382103342148995\n",
      "Average test loss: 0.006462482700745265\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03530620178249147\n",
      "Average test loss: 0.006260052494704724\n",
      "Epoch 137/300\n",
      "Average training loss: 0.03539994561341074\n",
      "Average test loss: 0.006148546806640095\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03524221239818467\n",
      "Average test loss: 0.006140706845041778\n",
      "Epoch 139/300\n",
      "Average training loss: 0.035282350771956975\n",
      "Average test loss: 0.0062353335362341665\n",
      "Epoch 140/300\n",
      "Average training loss: 0.03527172359824181\n",
      "Average test loss: 0.006902131753249301\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0353069084154235\n",
      "Average test loss: 0.0064063273726238145\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03519457784957356\n",
      "Average test loss: 0.006141066034220987\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03524797760446866\n",
      "Average test loss: 0.006246545857439438\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03523079654905531\n",
      "Average test loss: 0.006131383344531059\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03516113504105144\n",
      "Average test loss: 0.006118893718553914\n",
      "Epoch 146/300\n",
      "Average training loss: 0.035186974667840536\n",
      "Average test loss: 0.00622495400491688\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03521997987230619\n",
      "Average test loss: 0.006238735237055355\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03519815873106321\n",
      "Average test loss: 0.006223439824250009\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03512990926206112\n",
      "Average test loss: 0.006253612296034892\n",
      "Epoch 150/300\n",
      "Average training loss: 0.03510597741272714\n",
      "Average test loss: 0.006213427335437801\n",
      "Epoch 151/300\n",
      "Average training loss: 0.035142548309432135\n",
      "Average test loss: 0.006273752734065056\n",
      "Epoch 152/300\n",
      "Average training loss: 0.035109740502304504\n",
      "Average test loss: 0.0061185652783347505\n",
      "Epoch 153/300\n",
      "Average training loss: 0.03507625009616216\n",
      "Average test loss: 0.006200534536192815\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03510090611378352\n",
      "Average test loss: 0.006100939941489034\n",
      "Epoch 155/300\n",
      "Average training loss: 0.035085233214828704\n",
      "Average test loss: 0.006223048614545001\n",
      "Epoch 156/300\n",
      "Average training loss: 0.035080616146326064\n",
      "Average test loss: 0.006210010242544943\n",
      "Epoch 157/300\n",
      "Average training loss: 0.035075650526417625\n",
      "Average test loss: 0.00660890005239182\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03504675397939152\n",
      "Average test loss: 0.006118216587437524\n",
      "Epoch 159/300\n",
      "Average training loss: 0.035078806115521326\n",
      "Average test loss: 0.006127705965191126\n",
      "Epoch 160/300\n",
      "Average training loss: 0.03501192281974687\n",
      "Average test loss: 0.0061571338147752815\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03502531229787403\n",
      "Average test loss: 0.006435406440248092\n",
      "Epoch 162/300\n",
      "Average training loss: 0.03500667239228884\n",
      "Average test loss: 0.0062760743047628135\n",
      "Epoch 163/300\n",
      "Average training loss: 0.035003444734546876\n",
      "Average test loss: 0.006270872880601221\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03498137617442343\n",
      "Average test loss: 0.006170641471114424\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03495907960004277\n",
      "Average test loss: 0.2688255090183682\n",
      "Epoch 166/300\n",
      "Average training loss: 0.035253738476170436\n",
      "Average test loss: 0.006132027609894673\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03493484795921379\n",
      "Average test loss: 0.006126528831819693\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03490955544511477\n",
      "Average test loss: 0.006098239727732208\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03488115373253822\n",
      "Average test loss: 0.006138354008810388\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03489396138654815\n",
      "Average test loss: 0.006140286964674791\n",
      "Epoch 171/300\n",
      "Average training loss: 0.034868985518813136\n",
      "Average test loss: 0.006423063061717484\n",
      "Epoch 172/300\n",
      "Average training loss: 0.034886190318399006\n",
      "Average test loss: 0.006164816331946188\n",
      "Epoch 173/300\n",
      "Average training loss: 0.034886020259724725\n",
      "Average test loss: 0.006554826263752249\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03483598673012522\n",
      "Average test loss: 0.00624150199132661\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03488522867361705\n",
      "Average test loss: 0.00612892635166645\n",
      "Epoch 176/300\n",
      "Average training loss: 0.03487392822239134\n",
      "Average test loss: 0.006333020991749234\n",
      "Epoch 177/300\n",
      "Average training loss: 0.034702265467908645\n",
      "Average test loss: 0.006178376930455367\n",
      "Epoch 189/300\n",
      "Average training loss: 0.034720675925413766\n",
      "Average test loss: 0.0061122665057579675\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03471070424053404\n",
      "Average test loss: 0.006166229968269666\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03470789603392283\n",
      "Average test loss: 0.006141366105940607\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03467940394414796\n",
      "Average test loss: 0.0061250958881444405\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03471225724286503\n",
      "Average test loss: 0.006323173965017001\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03465172267953555\n",
      "Average test loss: 0.006182355975525247\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03466913500759337\n",
      "Average test loss: 0.00624336346031891\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03467061775922775\n",
      "Average test loss: 0.007826573927783303\n",
      "Epoch 197/300\n",
      "Average training loss: 0.03465688814057244\n",
      "Average test loss: 0.006173626080569294\n",
      "Epoch 198/300\n",
      "Average training loss: 0.034661206583182017\n",
      "Average test loss: 0.006206767364508576\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03458601315485107\n",
      "Average test loss: 0.0062150553456611106\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03465871330764558\n",
      "Average test loss: 0.006175612428122096\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03460730588601695\n",
      "Average test loss: 0.006163293087648021\n",
      "Epoch 202/300\n",
      "Average training loss: 0.03468878398835659\n",
      "Average test loss: 0.006155921707550685\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03458170881205135\n",
      "Average test loss: 0.0061708440002467895\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03461332111888462\n",
      "Average test loss: 0.0061818691740433375\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03457103967666626\n",
      "Average test loss: 0.0061335409089095065\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03455048032270538\n",
      "Average test loss: 0.006530430679519971\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0345911853959163\n",
      "Average test loss: 0.006143148699568377\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03455383641521136\n",
      "Average test loss: 0.006208716168171829\n",
      "Epoch 209/300\n",
      "Average training loss: 0.034565625470545557\n",
      "Average test loss: 0.0064058234451545605\n",
      "Epoch 210/300\n",
      "Average training loss: 0.0346040910548634\n",
      "Average test loss: 0.0061255255482263035\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03450774024261369\n",
      "Average test loss: 0.006584884458945857\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03453723646203677\n",
      "Average test loss: 0.006167348162995444\n",
      "Epoch 213/300\n",
      "Average training loss: 0.034499429732561114\n",
      "Average test loss: 0.006159538782305188\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03449874867664443\n",
      "Average test loss: 0.006236121152010229\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03450798507531484\n",
      "Average test loss: 0.006185044195916918\n",
      "Epoch 216/300\n",
      "Average training loss: 0.03449861306945483\n",
      "Average test loss: 0.006167501563413276\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03452379322052002\n",
      "Average test loss: 0.006291690710104174\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03443585893346204\n",
      "Average test loss: 0.0061385761991971065\n",
      "Epoch 219/300\n",
      "Average training loss: 0.034493438863092\n",
      "Average test loss: 0.006137342058950001\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03441175737314754\n",
      "Average test loss: 0.006760076446665658\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03442155805892415\n",
      "Average test loss: 0.006353177478743924\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03443076864878337\n",
      "Average test loss: 0.006246167251633274\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03442775592704614\n",
      "Average test loss: 0.00620098213561707\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03436741936206818\n",
      "Average test loss: 0.006192501004785299\n",
      "Epoch 236/300\n",
      "Average training loss: 0.03432722099290954\n",
      "Average test loss: 0.00773794941438569\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03432126062280602\n",
      "Average test loss: 0.006142442545543115\n",
      "Epoch 238/300\n",
      "Average training loss: 0.034320509665542176\n",
      "Average test loss: 0.0061436711856060556\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03429559325509601\n",
      "Average test loss: 0.006201574992181527\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0343146070142587\n",
      "Average test loss: 0.00622155714697308\n",
      "Epoch 241/300\n",
      "Average training loss: 0.034305191460582946\n",
      "Average test loss: 0.006167604425715075\n",
      "Epoch 242/300\n",
      "Average training loss: 0.03427883678012424\n",
      "Average test loss: 0.006211879335343838\n",
      "Epoch 243/300\n",
      "Average training loss: 0.034336677845981385\n",
      "Average test loss: 0.006251862784640657\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03433207294676039\n",
      "Average test loss: 0.006142728225224548\n",
      "Epoch 245/300\n",
      "Average training loss: 0.034236196680201426\n",
      "Average test loss: 0.0062344582490622994\n",
      "Epoch 246/300\n",
      "Average training loss: 0.034272361980544196\n",
      "Average test loss: 0.0061173722872303595\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03429003362688753\n",
      "Average test loss: 0.006128689436448945\n",
      "Epoch 248/300\n",
      "Average training loss: 0.034240658038192325\n",
      "Average test loss: 0.006130177344712946\n",
      "Epoch 249/300\n",
      "Average training loss: 0.03424122835861312\n",
      "Average test loss: 0.006273631285462115\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03425049743056297\n",
      "Average test loss: 0.006151452978038125\n",
      "Epoch 251/300\n",
      "Average training loss: 0.034245931194888224\n",
      "Average test loss: 0.006179908931255341\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03420773695574866\n",
      "Average test loss: 0.00614590172138479\n",
      "Epoch 253/300\n",
      "Average training loss: 0.034202711294094724\n",
      "Average test loss: 0.006140064794570208\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03423084419137902\n",
      "Average test loss: 0.006198467293961181\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0341968503329489\n",
      "Average test loss: 0.006146033314781057\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03415740144252777\n",
      "Average test loss: 0.006171725217666891\n",
      "Epoch 257/300\n",
      "Average training loss: 0.034391751817531055\n",
      "Average test loss: 0.006195691669980685\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03423828849858708\n",
      "Average test loss: 0.006180448333008422\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03419318946202596\n",
      "Average test loss: 0.006152388573106792\n",
      "Epoch 260/300\n",
      "Average training loss: 0.034156564043627845\n",
      "Average test loss: 0.006654747074056003\n",
      "Epoch 261/300\n",
      "Average training loss: 0.034154436882999205\n",
      "Average test loss: 0.006271902188658715\n",
      "Epoch 262/300\n",
      "Average training loss: 0.034132947587304646\n",
      "Average test loss: 0.00624340282049444\n",
      "Epoch 263/300\n",
      "Average training loss: 0.034163445939620334\n",
      "Average test loss: 0.006335375692695379\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03413532226284345\n",
      "Average test loss: 0.006168311155504651\n",
      "Epoch 265/300\n",
      "Average training loss: 0.034121723373730976\n",
      "Average test loss: 0.006141123727791839\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03412607818841934\n",
      "Average test loss: 0.008929723735484812\n",
      "Epoch 267/300\n",
      "Average training loss: 0.034125532878769765\n",
      "Average test loss: 0.006201210951639546\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03414255200160874\n",
      "Average test loss: 0.00620409335113234\n",
      "Epoch 269/300\n",
      "Average training loss: 0.0340735693540838\n",
      "Average test loss: 0.006179044898599387\n",
      "Epoch 270/300\n",
      "Average training loss: 0.034126045708854996\n",
      "Average test loss: 0.00621396364354425\n",
      "Epoch 271/300\n",
      "Average training loss: 0.034082070794370437\n",
      "Average test loss: 0.006195418735345204\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03407327401969168\n",
      "Average test loss: 0.006169211360315481\n",
      "Epoch 273/300\n",
      "Average training loss: 0.034101316064596175\n",
      "Average test loss: 0.006239007634835111\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03409957069158554\n",
      "Average test loss: 0.006220849049174124\n",
      "Epoch 275/300\n",
      "Average training loss: 0.0340940654228131\n",
      "Average test loss: 0.006391763841112455\n",
      "Epoch 276/300\n",
      "Average training loss: 0.0341108816034264\n",
      "Average test loss: 0.00621944963435332\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03405175214343601\n",
      "Average test loss: 0.006201249767715732\n",
      "Epoch 278/300\n",
      "Average training loss: 0.03405823872321182\n",
      "Average test loss: 0.0063520195475882954\n",
      "Epoch 279/300\n",
      "Average training loss: 0.03405272562967406\n",
      "Average test loss: 0.006184379414137867\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03406635253462527\n",
      "Average test loss: 0.006192592721432447\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03404804038504759\n",
      "Average test loss: 0.0069510211509962875\n",
      "Epoch 282/300\n",
      "Average training loss: 0.034040532380342485\n",
      "Average test loss: 0.006302733062869973\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03400593041711383\n",
      "Average test loss: 0.006266960091474983\n",
      "Epoch 284/300\n",
      "Average training loss: 0.034017850743399726\n",
      "Average test loss: 0.006235594908810324\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03398732790350914\n",
      "Average test loss: 0.006133880114389791\n",
      "Epoch 286/300\n",
      "Average training loss: 0.034011979083220165\n",
      "Average test loss: 0.00623127345699403\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03397910897930463\n",
      "Average test loss: 0.006263175445298354\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03400780133075184\n",
      "Average test loss: 0.006192276328802109\n",
      "Epoch 289/300\n",
      "Average training loss: 0.034018091345826784\n",
      "Average test loss: 0.006284687490099006\n",
      "Epoch 290/300\n",
      "Average training loss: 0.033998797406752906\n",
      "Average test loss: 0.006238425835139221\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0339836844884687\n",
      "Average test loss: 0.006235969511171182\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03398977088928223\n",
      "Average test loss: 0.006338981809301509\n",
      "Epoch 293/300\n",
      "Average training loss: 0.03398124677936236\n",
      "Average test loss: 0.006270931016239855\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03393019979861048\n",
      "Average test loss: 0.006179051424066225\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03396585019098388\n",
      "Average test loss: 0.006266674424211184\n",
      "Epoch 296/300\n",
      "Average training loss: 0.03396744504570961\n",
      "Average test loss: 0.006294425762361951\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03393928653995196\n",
      "Average test loss: 0.006424485971530279\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03395123639040523\n",
      "Average test loss: 0.00626061641673247\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03394118738671144\n",
      "Average test loss: 0.006242090138296286\n",
      "Epoch 300/300\n",
      "Average training loss: 0.0339185408519374\n",
      "Average test loss: 0.006189200143019358\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.20956461997164622\n",
      "Average test loss: 0.008082653861078952\n",
      "Epoch 2/300\n",
      "Average training loss: 0.054076988137430615\n",
      "Average test loss: 0.007270460120505757\n",
      "Epoch 3/300\n",
      "Average training loss: 0.04744052418404155\n",
      "Average test loss: 0.006527393540574445\n",
      "Epoch 4/300\n",
      "Average training loss: 0.04437973732087347\n",
      "Average test loss: 0.006908884371734328\n",
      "Epoch 5/300\n",
      "Average training loss: 0.041282821976476244\n",
      "Average test loss: 0.006172679031888644\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03993592929177814\n",
      "Average test loss: 0.005842091247439385\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03816303754846255\n",
      "Average test loss: 0.005965066626667976\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03695789148079025\n",
      "Average test loss: 0.005681555579106013\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03594028689463933\n",
      "Average test loss: 0.0058180603786475125\n",
      "Epoch 10/300\n",
      "Average training loss: 0.035021899736589854\n",
      "Average test loss: 0.005452258177101612\n",
      "Epoch 11/300\n",
      "Average training loss: 0.034211080729961395\n",
      "Average test loss: 0.005439470012154844\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03360801972945531\n",
      "Average test loss: 0.005463796527021461\n",
      "Epoch 13/300\n",
      "Average training loss: 0.033054445362753336\n",
      "Average test loss: 0.005264767665829923\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03239781231019232\n",
      "Average test loss: 0.0049456540925635235\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03187003543641832\n",
      "Average test loss: 0.00497628225841456\n",
      "Epoch 16/300\n",
      "Average training loss: 0.03144126547376315\n",
      "Average test loss: 0.004898247153808673\n",
      "Epoch 17/300\n",
      "Average training loss: 0.031017002125581106\n",
      "Average test loss: 0.004755837517480055\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030635568908519217\n",
      "Average test loss: 0.004858490044044123\n",
      "Epoch 19/300\n",
      "Average training loss: 0.03035885820786158\n",
      "Average test loss: 0.00484880966362026\n",
      "Epoch 20/300\n",
      "Average training loss: 0.029953274418910346\n",
      "Average test loss: 0.004635818047862914\n",
      "Epoch 21/300\n",
      "Average training loss: 0.029627806478076512\n",
      "Average test loss: 0.004518749161106017\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027364027894205518\n",
      "Average test loss: 0.004128407373196549\n",
      "Epoch 34/300\n",
      "Average training loss: 0.0272231565978792\n",
      "Average test loss: 0.004041959183911483\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02709019678003258\n",
      "Average test loss: 0.0041642820307364065\n",
      "Epoch 36/300\n",
      "Average training loss: 0.027000125227702988\n",
      "Average test loss: 0.00406283319575919\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02686668146815565\n",
      "Average test loss: 0.00406331687565479\n",
      "Epoch 38/300\n",
      "Average training loss: 0.0267967109117243\n",
      "Average test loss: 0.003972187948930595\n",
      "Epoch 39/300\n",
      "Average training loss: 0.02671697235107422\n",
      "Average test loss: 0.004004348700245222\n",
      "Epoch 40/300\n",
      "Average training loss: 0.026614508054322666\n",
      "Average test loss: 0.0040258921455178\n",
      "Epoch 41/300\n",
      "Average training loss: 0.02658197034067578\n",
      "Average test loss: 0.004183623231533501\n",
      "Epoch 42/300\n",
      "Average training loss: 0.026472083892259333\n",
      "Average test loss: 0.003923620975679822\n",
      "Epoch 43/300\n",
      "Average training loss: 0.026420801874664095\n",
      "Average test loss: 0.003943338021429049\n",
      "Epoch 44/300\n",
      "Average training loss: 0.02636055938899517\n",
      "Average test loss: 0.003987157268242704\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02625774909887049\n",
      "Average test loss: 0.0038981197107997205\n",
      "Epoch 46/300\n",
      "Average training loss: 0.02626230080756876\n",
      "Average test loss: 0.004012684725638893\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02620913731224007\n",
      "Average test loss: 0.0038979996898108058\n",
      "Epoch 48/300\n",
      "Average training loss: 0.026170162104898028\n",
      "Average test loss: 0.003924302717463838\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02605948255956173\n",
      "Average test loss: 0.003974679393072923\n",
      "Epoch 50/300\n",
      "Average training loss: 0.026078240156173705\n",
      "Average test loss: 0.003853264233718316\n",
      "Epoch 51/300\n",
      "Average training loss: 0.025964816174573368\n",
      "Average test loss: 0.003980459283209509\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02595933112833235\n",
      "Average test loss: 0.003972728478411834\n",
      "Epoch 53/300\n",
      "Average training loss: 0.025917689056860076\n",
      "Average test loss: 0.0038793110717087985\n",
      "Epoch 54/300\n",
      "Average training loss: 0.025859455580512683\n",
      "Average test loss: 0.0038435118556436566\n",
      "Epoch 55/300\n",
      "Average training loss: 0.025822948810127047\n",
      "Average test loss: 0.003870104114835461\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02577331633865833\n",
      "Average test loss: 0.003861182728989257\n",
      "Epoch 57/300\n",
      "Average training loss: 0.025774240252044466\n",
      "Average test loss: 0.004219540062877867\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02572575941681862\n",
      "Average test loss: 0.003905527599569824\n",
      "Epoch 59/300\n",
      "Average training loss: 0.025715347934100363\n",
      "Average test loss: 0.0038192557158569494\n",
      "Epoch 60/300\n",
      "Average training loss: 0.025660444827543365\n",
      "Average test loss: 0.0039885037421352335\n",
      "Epoch 61/300\n",
      "Average training loss: 0.02564096546007527\n",
      "Average test loss: 0.003918997674352593\n",
      "Epoch 62/300\n",
      "Average training loss: 0.025589939802885056\n",
      "Average test loss: 0.0038629482748607793\n",
      "Epoch 63/300\n",
      "Average training loss: 0.02556428401172161\n",
      "Average test loss: 0.0038330871760845183\n",
      "Epoch 64/300\n",
      "Average training loss: 0.025561516472034983\n",
      "Average test loss: 0.0037997695534593527\n",
      "Epoch 65/300\n",
      "Average training loss: 0.0254943328499794\n",
      "Average test loss: 0.003794397639731566\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02554069786436028\n",
      "Average test loss: 0.07642577962080638\n",
      "Epoch 67/300\n",
      "Average training loss: 0.025641135017077127\n",
      "Average test loss: 0.0038512784731056953\n",
      "Epoch 68/300\n",
      "Average training loss: 0.025461939513683318\n",
      "Average test loss: 0.0037777201862384878\n",
      "Epoch 69/300\n",
      "Average training loss: 0.025379574495885106\n",
      "Average test loss: 0.0038101603699227174\n",
      "Epoch 70/300\n",
      "Average training loss: 0.025383054908778933\n",
      "Average test loss: 0.0038046286948439146\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02534618616104126\n",
      "Average test loss: 0.0037923242155876425\n",
      "Epoch 72/300\n",
      "Average training loss: 0.025324684974220062\n",
      "Average test loss: 0.003845932091689772\n",
      "Epoch 73/300\n",
      "Average training loss: 0.02530241947538323\n",
      "Average test loss: 0.003767898831516504\n",
      "Epoch 74/300\n",
      "Average training loss: 0.025305223868952856\n",
      "Average test loss: 0.003783686744670073\n",
      "Epoch 75/300\n",
      "Average training loss: 0.025281540365682708\n",
      "Average test loss: 0.003844683906684319\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02523908667266369\n",
      "Average test loss: 0.0038014038072692025\n",
      "Epoch 77/300\n",
      "Average training loss: 0.025227615697516335\n",
      "Average test loss: 0.0037994519552836817\n",
      "Epoch 78/300\n",
      "Average training loss: 0.025192876049213938\n",
      "Average test loss: 0.0038022901138497723\n",
      "Epoch 79/300\n",
      "Average training loss: 0.02520387063258224\n",
      "Average test loss: 0.00378889022477799\n",
      "Epoch 80/300\n",
      "Average training loss: 0.02514610070652432\n",
      "Average test loss: 0.003789267974595229\n",
      "Epoch 81/300\n",
      "Average training loss: 0.025148398372862073\n",
      "Average test loss: 0.0038509398558073575\n",
      "Epoch 82/300\n",
      "Average training loss: 0.025137252734767065\n",
      "Average test loss: 0.003864761486235592\n",
      "Epoch 83/300\n",
      "Average training loss: 0.025115503019756742\n",
      "Average test loss: 0.003785975147452619\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02508253912627697\n",
      "Average test loss: 0.0037452519883712132\n",
      "Epoch 85/300\n",
      "Average training loss: 0.025071941100888784\n",
      "Average test loss: 0.00377245713935958\n",
      "Epoch 86/300\n",
      "Average training loss: 0.02504302090903123\n",
      "Average test loss: 0.003811656807652778\n",
      "Epoch 87/300\n",
      "Average training loss: 0.026269280544585653\n",
      "Average test loss: 0.004039815732174449\n",
      "Epoch 88/300\n",
      "Average training loss: 0.02520358118414879\n",
      "Average test loss: 0.0038455551084544924\n",
      "Epoch 89/300\n",
      "Average training loss: 0.024942840413914785\n",
      "Average test loss: 0.003767951406952408\n",
      "Epoch 90/300\n",
      "Average training loss: 0.024926616706781916\n",
      "Average test loss: 0.0037792071998119354\n",
      "Epoch 91/300\n",
      "Average training loss: 0.02495542282362779\n",
      "Average test loss: 0.0037380195404920313\n",
      "Epoch 92/300\n",
      "Average training loss: 0.024912017695605755\n",
      "Average test loss: 0.0037961311067144076\n",
      "Epoch 93/300\n",
      "Average training loss: 0.024951205030083655\n",
      "Average test loss: 0.0038650525125364463\n",
      "Epoch 94/300\n",
      "Average training loss: 0.024912200124727356\n",
      "Average test loss: 0.0037950515823645726\n",
      "Epoch 95/300\n",
      "Average training loss: 0.024919756538338132\n",
      "Average test loss: 0.003922511194936103\n",
      "Epoch 96/300\n",
      "Average training loss: 0.02489704128768709\n",
      "Average test loss: 0.003765855007287529\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02490713597006268\n",
      "Average test loss: 0.0038557379432022573\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024867884510093266\n",
      "Average test loss: 0.0038356044408347873\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0248582853741116\n",
      "Average test loss: 0.0041800624835822316\n",
      "Epoch 100/300\n",
      "Average training loss: 0.02485261876715554\n",
      "Average test loss: 0.0037642784507738217\n",
      "Epoch 101/300\n",
      "Average training loss: 0.02484582719620731\n",
      "Average test loss: 0.003925560477707121\n",
      "Epoch 102/300\n",
      "Average training loss: 0.02481208299431536\n",
      "Average test loss: 0.0037409267634567287\n",
      "Epoch 103/300\n",
      "Average training loss: 0.024823321473267344\n",
      "Average test loss: 0.003766398252091474\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024799863522251445\n",
      "Average test loss: 0.006186282170729505\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024857114124629234\n",
      "Average test loss: 0.0038051077495846485\n",
      "Epoch 106/300\n",
      "Average training loss: 0.024772958487272262\n",
      "Average test loss: 0.0037822851784941225\n",
      "Epoch 107/300\n",
      "Average training loss: 0.02475639289120833\n",
      "Average test loss: 0.003736003944443332\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024741298963626226\n",
      "Average test loss: 0.003769551037500302\n",
      "Epoch 109/300\n",
      "Average training loss: 0.024745843278037177\n",
      "Average test loss: 0.003751970653732618\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024720315809051197\n",
      "Average test loss: 0.00378348971489403\n",
      "Epoch 111/300\n",
      "Average training loss: 0.024702852292193305\n",
      "Average test loss: 0.003737716712885433\n",
      "Epoch 112/300\n",
      "Average training loss: 0.024709187590413625\n",
      "Average test loss: 0.0037650053795013163\n",
      "Epoch 113/300\n",
      "Average training loss: 0.0246854547990693\n",
      "Average test loss: 0.0037675956851906246\n",
      "Epoch 114/300\n",
      "Average training loss: 0.02467300624317593\n",
      "Average test loss: 0.0037502136973457203\n",
      "Epoch 115/300\n",
      "Average training loss: 0.024655671291881138\n",
      "Average test loss: 0.003762696867187818\n",
      "Epoch 116/300\n",
      "Average training loss: 0.024660271399550966\n",
      "Average test loss: 0.003754657469689846\n",
      "Epoch 117/300\n",
      "Average training loss: 0.024653788569900723\n",
      "Average test loss: 0.0037630215837723678\n",
      "Epoch 118/300\n",
      "Average training loss: 0.02461444015800953\n",
      "Average test loss: 0.0037892870302829477\n",
      "Epoch 119/300\n",
      "Average training loss: 0.02464002962741587\n",
      "Average test loss: 0.0037554859086457227\n",
      "Epoch 120/300\n",
      "Average training loss: 0.024588011332684092\n",
      "Average test loss: 0.0038334866190950076\n",
      "Epoch 121/300\n",
      "Average training loss: 0.024594871373640165\n",
      "Average test loss: 0.0037328201530294285\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02457980646689733\n",
      "Average test loss: 0.003765845817824205\n",
      "Epoch 123/300\n",
      "Average training loss: 0.024569090854790476\n",
      "Average test loss: 0.003719937158127626\n",
      "Epoch 124/300\n",
      "Average training loss: 0.024553561002016066\n",
      "Average test loss: 0.003748910307677256\n",
      "Epoch 125/300\n",
      "Average training loss: 0.024552691145075693\n",
      "Average test loss: 0.003749801475347744\n",
      "Epoch 126/300\n",
      "Average training loss: 0.02455486535363727\n",
      "Average test loss: 0.003731686953144769\n",
      "Epoch 127/300\n",
      "Average training loss: 0.024536619340380032\n",
      "Average test loss: 0.003861607408978873\n",
      "Epoch 128/300\n",
      "Average training loss: 0.027159802017940417\n",
      "Average test loss: 0.003767673783418205\n",
      "Epoch 129/300\n",
      "Average training loss: 0.02459184200399452\n",
      "Average test loss: 0.003723341915342543\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02445766812066237\n",
      "Average test loss: 0.0037400566296031078\n",
      "Epoch 131/300\n",
      "Average training loss: 0.024410593057672184\n",
      "Average test loss: 0.003742296907222933\n",
      "Epoch 132/300\n",
      "Average training loss: 0.024430688710676298\n",
      "Average test loss: 0.0038065708577632905\n",
      "Epoch 133/300\n",
      "Average training loss: 0.02444342638221052\n",
      "Average test loss: 0.0037390093455712\n",
      "Epoch 134/300\n",
      "Average training loss: 0.02445589775674873\n",
      "Average test loss: 0.0037643484038611254\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02449857885307736\n",
      "Average test loss: 0.003778920287473334\n",
      "Epoch 136/300\n",
      "Average training loss: 0.02447385859903362\n",
      "Average test loss: 0.0037911698428086107\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02448433944914076\n",
      "Average test loss: 0.003721814158269101\n",
      "Epoch 138/300\n",
      "Average training loss: 0.02445198001464208\n",
      "Average test loss: 0.0037674485894127023\n",
      "Epoch 139/300\n",
      "Average training loss: 0.024453744875060187\n",
      "Average test loss: 0.003915306854993105\n",
      "Epoch 140/300\n",
      "Average training loss: 0.024451941733558972\n",
      "Average test loss: 0.003864806833780474\n",
      "Epoch 141/300\n",
      "Average training loss: 0.02442821583317386\n",
      "Average test loss: 0.0037461778935458925\n",
      "Epoch 142/300\n",
      "Average training loss: 0.024415769620074166\n",
      "Average test loss: 0.003719558562255568\n",
      "Epoch 143/300\n",
      "Average training loss: 0.024412933356232114\n",
      "Average test loss: 0.0037551110693150097\n",
      "Epoch 144/300\n",
      "Average training loss: 0.024418584992488224\n",
      "Average test loss: 0.003780786621073882\n",
      "Epoch 145/300\n",
      "Average training loss: 0.0243798671066761\n",
      "Average test loss: 0.003818989234459069\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02440245837138759\n",
      "Average test loss: 0.003731615048108829\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024379188852177727\n",
      "Average test loss: 0.0037504491056833\n",
      "Epoch 148/300\n",
      "Average training loss: 0.024366446180476082\n",
      "Average test loss: 0.004141044858429167\n",
      "Epoch 149/300\n",
      "Average training loss: 0.024373318249980607\n",
      "Average test loss: 0.0038073337976303368\n",
      "Epoch 150/300\n",
      "Average training loss: 0.024356984794139863\n",
      "Average test loss: 0.0037963445871654483\n",
      "Epoch 151/300\n",
      "Average training loss: 0.024341663519541423\n",
      "Average test loss: 0.003813868213444948\n",
      "Epoch 152/300\n",
      "Average training loss: 0.024343722679548794\n",
      "Average test loss: 0.003725583259016275\n",
      "Epoch 153/300\n",
      "Average training loss: 0.024329079278641277\n",
      "Average test loss: 0.0037506176622377502\n",
      "Epoch 154/300\n",
      "Average training loss: 0.024319095290369457\n",
      "Average test loss: 0.0038444447240067853\n",
      "Epoch 155/300\n",
      "Average training loss: 0.024332538584868113\n",
      "Average test loss: 0.0037189804369376763\n",
      "Epoch 156/300\n",
      "Average training loss: 0.02431287196940846\n",
      "Average test loss: 0.003706383042037487\n",
      "Epoch 157/300\n",
      "Average training loss: 0.024324206944968965\n",
      "Average test loss: 0.0038946652716646594\n",
      "Epoch 158/300\n",
      "Average training loss: 0.024288826271891596\n",
      "Average test loss: 0.0037282493648429713\n",
      "Epoch 159/300\n",
      "Average training loss: 0.024280695665213795\n",
      "Average test loss: 0.0039445000479204784\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02426350863940186\n",
      "Average test loss: 0.0037351367212831972\n",
      "Epoch 161/300\n",
      "Average training loss: 0.02426422220799658\n",
      "Average test loss: 0.003800742727186945\n",
      "Epoch 162/300\n",
      "Average training loss: 0.024262123371164002\n",
      "Average test loss: 0.0038037701890700393\n",
      "Epoch 163/300\n",
      "Average training loss: 0.024272261816594337\n",
      "Average test loss: 0.0037166428917811975\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02424363371067577\n",
      "Average test loss: 0.0037353061305152044\n",
      "Epoch 165/300\n",
      "Average training loss: 0.024234383306569525\n",
      "Average test loss: 0.0037162674494708578\n",
      "Epoch 166/300\n",
      "Average training loss: 0.024226348573962848\n",
      "Average test loss: 0.0037882517892867326\n",
      "Epoch 167/300\n",
      "Average training loss: 0.024213365827997525\n",
      "Average test loss: 0.003768636647197935\n",
      "Epoch 168/300\n",
      "Average training loss: 0.0242341691735718\n",
      "Average test loss: 0.003804057884961367\n",
      "Epoch 169/300\n",
      "Average training loss: 0.024217527334888776\n",
      "Average test loss: 0.0037566214131398335\n",
      "Epoch 170/300\n",
      "Average training loss: 0.024209956775108975\n",
      "Average test loss: 0.003723274428397417\n",
      "Epoch 171/300\n",
      "Average training loss: 0.02419125387734837\n",
      "Average test loss: 0.003740269388589594\n",
      "Epoch 172/300\n",
      "Average training loss: 0.024200841777854497\n",
      "Average test loss: 0.04744190220038096\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02434014097187254\n",
      "Average test loss: 0.003726640482743581\n",
      "Epoch 174/300\n",
      "Average training loss: 0.024154404883583388\n",
      "Average test loss: 0.0037445479252686104\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024163590719302495\n",
      "Average test loss: 0.003739652050245139\n",
      "Epoch 176/300\n",
      "Average training loss: 0.024157066312101152\n",
      "Average test loss: 0.0038371213968429302\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02414083414607578\n",
      "Average test loss: 0.0037472226011256377\n",
      "Epoch 178/300\n",
      "Average training loss: 0.02416143702301714\n",
      "Average test loss: 0.0038428414977259105\n",
      "Epoch 179/300\n",
      "Average training loss: 0.024127548669775328\n",
      "Average test loss: 0.003761963210793005\n",
      "Epoch 180/300\n",
      "Average training loss: 0.024128394790821604\n",
      "Average test loss: 0.0038002071641385555\n",
      "Epoch 181/300\n",
      "Average training loss: 0.024147377842002445\n",
      "Average test loss: 0.0037680991979108917\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02413165435857243\n",
      "Average test loss: 0.00372685329698854\n",
      "Epoch 183/300\n",
      "Average training loss: 0.024107795180545913\n",
      "Average test loss: 0.003765996764310532\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02410440687669648\n",
      "Average test loss: 0.003703684563645058\n",
      "Epoch 185/300\n",
      "Average training loss: 0.024130103992091284\n",
      "Average test loss: 0.0037411287472479873\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02411723388565911\n",
      "Average test loss: 0.0037425231414122714\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02408899209068881\n",
      "Average test loss: 0.0037501467876136303\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02407116752366225\n",
      "Average test loss: 0.0038174556866288185\n",
      "Epoch 189/300\n",
      "Average training loss: 0.024094973395268122\n",
      "Average test loss: 0.0037472307671689324\n",
      "Epoch 190/300\n",
      "Average training loss: 0.024105418741703032\n",
      "Average test loss: 0.0037315769967519574\n",
      "Epoch 191/300\n",
      "Average training loss: 0.024071710854768752\n",
      "Average test loss: 0.0037567960160473984\n",
      "Epoch 192/300\n",
      "Average training loss: 0.024057623359892102\n",
      "Average test loss: 0.0037969065974983902\n",
      "Epoch 193/300\n",
      "Average training loss: 0.02405998383462429\n",
      "Average test loss: 0.0037188231535255907\n",
      "Epoch 194/300\n",
      "Average training loss: 0.024065160772866672\n",
      "Average test loss: 0.0038827115231090123\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02408240433368418\n",
      "Average test loss: 0.003733656680625346\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02403607909215821\n",
      "Average test loss: 0.0037481919415295125\n",
      "Epoch 197/300\n",
      "Average training loss: 0.024029386639595033\n",
      "Average test loss: 0.0037641060683462355\n",
      "Epoch 198/300\n",
      "Average training loss: 0.024046951906548607\n",
      "Average test loss: 0.0037933014072477817\n",
      "Epoch 199/300\n",
      "Average training loss: 0.024029454158412084\n",
      "Average test loss: 0.003743911629749669\n",
      "Epoch 200/300\n",
      "Average training loss: 0.023999526952703795\n",
      "Average test loss: 0.003727227289022671\n",
      "Epoch 201/300\n",
      "Average training loss: 0.023995326297150718\n",
      "Average test loss: 0.0037260563468767536\n",
      "Epoch 202/300\n",
      "Average training loss: 0.024027029626899295\n",
      "Average test loss: 0.003831095108141502\n",
      "Epoch 203/300\n",
      "Average training loss: 0.024005239221784803\n",
      "Average test loss: 0.003734582699007458\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02399405637052324\n",
      "Average test loss: 0.003766552339825365\n",
      "Epoch 205/300\n",
      "Average training loss: 0.024008812175856695\n",
      "Average test loss: 0.0037585258800536393\n",
      "Epoch 206/300\n",
      "Average training loss: 0.023993779083093008\n",
      "Average test loss: 0.0037399998915692173\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0239989738514026\n",
      "Average test loss: 0.0039480317483345665\n",
      "Epoch 208/300\n",
      "Average training loss: 0.023989083770248627\n",
      "Average test loss: 0.0037838935893442895\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02396299125585291\n",
      "Average test loss: 0.003732526475770606\n",
      "Epoch 210/300\n",
      "Average training loss: 0.023971683625545765\n",
      "Average test loss: 0.003750313400394387\n",
      "Epoch 211/300\n",
      "Average training loss: 0.023947631317708228\n",
      "Average test loss: 0.003748915159660909\n",
      "Epoch 212/300\n",
      "Average training loss: 0.023968930646777152\n",
      "Average test loss: 0.0037421929974936776\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02393687200711833\n",
      "Average test loss: 0.003805426962673664\n",
      "Epoch 214/300\n",
      "Average training loss: 0.023949906072682804\n",
      "Average test loss: 0.00374950207852655\n",
      "Epoch 215/300\n",
      "Average training loss: 0.02394303642047776\n",
      "Average test loss: 0.003794775526970625\n",
      "Epoch 216/300\n",
      "Average training loss: 0.023928018394443723\n",
      "Average test loss: 0.0037162989139970805\n",
      "Epoch 217/300\n",
      "Average training loss: 0.023933272235923342\n",
      "Average test loss: 0.003745564978155825\n",
      "Epoch 218/300\n",
      "Average training loss: 0.0239248392979304\n",
      "Average test loss: 0.003789263915994929\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02395209604832861\n",
      "Average test loss: 0.003968501135706902\n",
      "Epoch 220/300\n",
      "Average training loss: 0.023913402610354954\n",
      "Average test loss: 0.0037436660499208505\n",
      "Epoch 221/300\n",
      "Average training loss: 0.023888513122995694\n",
      "Average test loss: 0.0037548039719048475\n",
      "Epoch 222/300\n",
      "Average training loss: 0.023912785195642047\n",
      "Average test loss: 0.0037614671256807116\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02390935058395068\n",
      "Average test loss: 0.0037620194939275583\n",
      "Epoch 224/300\n",
      "Average training loss: 0.023915664439400037\n",
      "Average test loss: 0.003922551310103801\n",
      "Epoch 225/300\n",
      "Average training loss: 0.023887944178448785\n",
      "Average test loss: 0.003765347989069091\n",
      "Epoch 226/300\n",
      "Average training loss: 0.02390518413649665\n",
      "Average test loss: 0.003815853368697895\n",
      "Epoch 227/300\n",
      "Average training loss: 0.023861735050876937\n",
      "Average test loss: 0.0037355125008357894\n",
      "Epoch 228/300\n",
      "Average training loss: 0.0238730311907\n",
      "Average test loss: 0.0038047238911191623\n",
      "Epoch 229/300\n",
      "Average training loss: 0.023894520160224704\n",
      "Average test loss: 0.0037789461182223425\n",
      "Epoch 230/300\n",
      "Average training loss: 0.024449598174956108\n",
      "Average test loss: 0.0038074102157519924\n",
      "Epoch 231/300\n",
      "Average training loss: 0.023846894467870396\n",
      "Average test loss: 0.0037308261522816287\n",
      "Epoch 232/300\n",
      "Average training loss: 0.023834738281038072\n",
      "Average test loss: 0.0037433902790976895\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02385566063059701\n",
      "Average test loss: 0.003822909950796101\n",
      "Epoch 234/300\n",
      "Average training loss: 0.023851737134986454\n",
      "Average test loss: 0.0037660843262241946\n",
      "Epoch 235/300\n",
      "Average training loss: 0.023831109881401063\n",
      "Average test loss: 0.003810923575113217\n",
      "Epoch 236/300\n",
      "Average training loss: 0.023854946076869964\n",
      "Average test loss: 0.0037477728629277812\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02387313170896636\n",
      "Average test loss: 0.0037867784806423716\n",
      "Epoch 238/300\n",
      "Average training loss: 0.02382412374847465\n",
      "Average test loss: 0.003767259468221002\n",
      "Epoch 239/300\n",
      "Average training loss: 0.023823721870779992\n",
      "Average test loss: 0.0037424299613469177\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02382224728498194\n",
      "Average test loss: 0.0038429091142283545\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02383875432279375\n",
      "Average test loss: 0.0037642789836972953\n",
      "Epoch 242/300\n",
      "Average training loss: 0.02384610969821612\n",
      "Average test loss: 0.0037113338340487743\n",
      "Epoch 243/300\n",
      "Average training loss: 0.023825612754457527\n",
      "Average test loss: 0.0038424743943744235\n",
      "Epoch 244/300\n",
      "Average training loss: 0.023826609146263864\n",
      "Average test loss: 0.003731350712892082\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02379332955347167\n",
      "Average test loss: 0.0037148636794752544\n",
      "Epoch 246/300\n",
      "Average training loss: 0.023803934708237648\n",
      "Average test loss: 0.0037988038081675766\n",
      "Epoch 247/300\n",
      "Average training loss: 0.023784016336003938\n",
      "Average test loss: 0.0037656162474304436\n",
      "Epoch 248/300\n",
      "Average training loss: 0.023780061884058845\n",
      "Average test loss: 0.0038043090607970953\n",
      "Epoch 249/300\n",
      "Average training loss: 0.023827945190999244\n",
      "Average test loss: 0.003744252408751183\n",
      "Epoch 250/300\n",
      "Average training loss: 0.023780089399880834\n",
      "Average test loss: 0.0037248428190747897\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02379868531641033\n",
      "Average test loss: 0.003823975869557924\n",
      "Epoch 252/300\n",
      "Average training loss: 0.023784300398495464\n",
      "Average test loss: 0.003735929084941745\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02376690746843815\n",
      "Average test loss: 0.003808200579757492\n",
      "Epoch 254/300\n",
      "Average training loss: 0.023772338749633894\n",
      "Average test loss: 0.0038552017485102017\n",
      "Epoch 255/300\n",
      "Average training loss: 0.023796229168772696\n",
      "Average test loss: 0.0390951326870256\n",
      "Epoch 256/300\n",
      "Average training loss: 0.023801009404990407\n",
      "Average test loss: 0.0037473764758971\n",
      "Epoch 257/300\n",
      "Average training loss: 0.023758173652821117\n",
      "Average test loss: 0.003766600072797802\n",
      "Epoch 258/300\n",
      "Average training loss: 0.023745887683497537\n",
      "Average test loss: 0.004461407872537772\n",
      "Epoch 259/300\n",
      "Average training loss: 0.023731885885198912\n",
      "Average test loss: 0.003731292266398668\n",
      "Epoch 260/300\n",
      "Average training loss: 0.023738747702704534\n",
      "Average test loss: 0.0037447549644857646\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02375784425271882\n",
      "Average test loss: 0.0037571989438600012\n",
      "Epoch 262/300\n",
      "Average training loss: 0.023725399773981835\n",
      "Average test loss: 0.004365594305511978\n",
      "Epoch 263/300\n",
      "Average training loss: 0.023739514018098512\n",
      "Average test loss: 0.003741783672115869\n",
      "Epoch 264/300\n",
      "Average training loss: 0.023740242042475275\n",
      "Average test loss: 0.003735985152423382\n",
      "Epoch 265/300\n",
      "Average training loss: 0.023733592097957928\n",
      "Average test loss: 0.00372458357632988\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02372064088781675\n",
      "Average test loss: 0.003769625451415777\n",
      "Epoch 267/300\n",
      "Average training loss: 0.023716830033395024\n",
      "Average test loss: 0.0038062348781774443\n",
      "Epoch 268/300\n",
      "Average training loss: 0.02373012495537599\n",
      "Average test loss: 0.012864064295258788\n",
      "Epoch 269/300\n",
      "Average training loss: 0.02371527912053797\n",
      "Average test loss: 0.00379641131663488\n",
      "Epoch 270/300\n",
      "Average training loss: 0.023722908463742996\n",
      "Average test loss: 0.0037893819510936735\n",
      "Epoch 271/300\n",
      "Average training loss: 0.02369129561053382\n",
      "Average test loss: 0.003846201889630821\n",
      "Epoch 272/300\n",
      "Average training loss: 0.023749063493476975\n",
      "Average test loss: 0.0037667115504542987\n",
      "Epoch 273/300\n",
      "Average training loss: 0.023689116494523155\n",
      "Average test loss: 0.003751280552190211\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02368693946633074\n",
      "Average test loss: 0.0037427984207040734\n",
      "Epoch 275/300\n",
      "Average training loss: 0.023672613297899565\n",
      "Average test loss: 0.0038253987996528546\n",
      "Epoch 276/300\n",
      "Average training loss: 0.023692485282818476\n",
      "Average test loss: 0.003752877486248811\n",
      "Epoch 277/300\n",
      "Average training loss: 0.02368718953099516\n",
      "Average test loss: 0.003746740316351255\n",
      "Epoch 278/300\n",
      "Average training loss: 0.023705900963809756\n",
      "Average test loss: 0.0037529198964022926\n",
      "Epoch 279/300\n",
      "Average training loss: 0.023656645177139175\n",
      "Average test loss: 0.003759897131472826\n",
      "Epoch 280/300\n",
      "Average training loss: 0.02366729458173116\n",
      "Average test loss: 0.003814614282300075\n",
      "Epoch 281/300\n",
      "Average training loss: 0.023656367063522338\n",
      "Average test loss: 0.003866431255100502\n",
      "Epoch 282/300\n",
      "Average training loss: 0.02366839724447992\n",
      "Average test loss: 0.0037411255327363807\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02367695906923877\n",
      "Average test loss: 0.003800207499000761\n",
      "Epoch 284/300\n",
      "Average training loss: 0.023661638293001386\n",
      "Average test loss: 0.003736318795217408\n",
      "Epoch 285/300\n",
      "Average training loss: 0.023673733961251046\n",
      "Average test loss: 0.003785398732870817\n",
      "Epoch 286/300\n",
      "Average training loss: 0.023637245765990682\n",
      "Average test loss: 0.0037334276959300043\n",
      "Epoch 287/300\n",
      "Average training loss: 0.023672193595104748\n",
      "Average test loss: 0.004006628539827135\n",
      "Epoch 288/300\n",
      "Average training loss: 0.023640581982003316\n",
      "Average test loss: 0.0037287749697764716\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02365173115664058\n",
      "Average test loss: 0.003751770958304405\n",
      "Epoch 290/300\n",
      "Average training loss: 0.023643429110447565\n",
      "Average test loss: 0.003737077953086959\n",
      "Epoch 291/300\n",
      "Average training loss: 0.023619805615809227\n",
      "Average test loss: 0.0038373236523734197\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02364699439207713\n",
      "Average test loss: 0.0037673140693869856\n",
      "Epoch 293/300\n",
      "Average training loss: 0.023634592190384864\n",
      "Average test loss: 0.0038411935031827955\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02361191337472863\n",
      "Average test loss: 0.003789237285239829\n",
      "Epoch 295/300\n",
      "Average training loss: 0.023637601231535275\n",
      "Average test loss: 0.003749856804187099\n",
      "Epoch 296/300\n",
      "Average training loss: 0.02361217891673247\n",
      "Average test loss: 0.0037399097621026965\n",
      "Epoch 297/300\n",
      "Average training loss: 0.023634027805593278\n",
      "Average test loss: 0.0037749743318806093\n",
      "Epoch 298/300\n",
      "Average training loss: 0.023594515247477424\n",
      "Average test loss: 0.003757417033943865\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02359999827378326\n",
      "Average test loss: 0.0038010859640522135\n",
      "Epoch 300/300\n",
      "Average training loss: 0.023641364612513118\n",
      "Average test loss: 0.0037545998895333874\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1903574288553662\n",
      "Average test loss: 0.00606253849921955\n",
      "Epoch 2/300\n",
      "Average training loss: 0.04515533306201299\n",
      "Average test loss: 0.005566688329809242\n",
      "Epoch 3/300\n",
      "Average training loss: 0.03970560106303957\n",
      "Average test loss: 0.005171845783376031\n",
      "Epoch 4/300\n",
      "Average training loss: 0.03713981607556343\n",
      "Average test loss: 0.004945171607244346\n",
      "Epoch 5/300\n",
      "Average training loss: 0.03483335530426767\n",
      "Average test loss: 0.004969128345449766\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03319703463382191\n",
      "Average test loss: 0.00541580104869273\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03189951696660784\n",
      "Average test loss: 0.004762639176100493\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03067273023724556\n",
      "Average test loss: 0.0043703636638820174\n",
      "Epoch 9/300\n",
      "Average training loss: 0.029746429906951057\n",
      "Average test loss: 0.0040712586697191\n",
      "Epoch 10/300\n",
      "Average training loss: 0.028794686550895374\n",
      "Average test loss: 0.00395880026742816\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027939339879486297\n",
      "Average test loss: 0.00399488819266359\n",
      "Epoch 12/300\n",
      "Average training loss: 0.027241601804892223\n",
      "Average test loss: 0.003986566969503959\n",
      "Epoch 13/300\n",
      "Average training loss: 0.026611215487122537\n",
      "Average test loss: 0.003715563272850381\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02609678766131401\n",
      "Average test loss: 0.0037454189066257744\n",
      "Epoch 15/300\n",
      "Average training loss: 0.025542414607273207\n",
      "Average test loss: 0.0036003086281319457\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02513321802020073\n",
      "Average test loss: 0.003581637304276228\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02478422735631466\n",
      "Average test loss: 0.003806946612894535\n",
      "Epoch 18/300\n",
      "Average training loss: 0.024419758290052412\n",
      "Average test loss: 0.003322694290429354\n",
      "Epoch 19/300\n",
      "Average training loss: 0.024042291661103568\n",
      "Average test loss: 0.003294196957929267\n",
      "Epoch 20/300\n",
      "Average training loss: 0.02372091366847356\n",
      "Average test loss: 0.0032869029459026123\n",
      "Epoch 21/300\n",
      "Average training loss: 0.023483225261171657\n",
      "Average test loss: 0.0033959695272561576\n",
      "Epoch 22/300\n",
      "Average training loss: 0.023290713550315964\n",
      "Average test loss: 0.003125152703374624\n",
      "Epoch 23/300\n",
      "Average training loss: 0.023018463571866352\n",
      "Average test loss: 0.0033326657158840034\n",
      "Epoch 24/300\n",
      "Average training loss: 0.022830698485175768\n",
      "Average test loss: 0.003079235860457023\n",
      "Epoch 25/300\n",
      "Average training loss: 0.022663430739608074\n",
      "Average test loss: 0.0030906839944008324\n",
      "Epoch 26/300\n",
      "Average training loss: 0.022516640020741358\n",
      "Average test loss: 0.002993823844525549\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02235315749877029\n",
      "Average test loss: 0.0030097558461129664\n",
      "Epoch 28/300\n",
      "Average training loss: 0.02218497374488248\n",
      "Average test loss: 0.0029562560632410977\n",
      "Epoch 29/300\n",
      "Average training loss: 0.02206071331269211\n",
      "Average test loss: 0.002964971740419666\n",
      "Epoch 30/300\n",
      "Average training loss: 0.021977904794944658\n",
      "Average test loss: 0.0029140965851644674\n",
      "Epoch 31/300\n",
      "Average training loss: 0.021816432643267842\n",
      "Average test loss: 0.0029262348121653\n",
      "Epoch 32/300\n",
      "Average training loss: 0.021729697149660852\n",
      "Average test loss: 0.002937165069083373\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02166776719192664\n",
      "Average test loss: 0.0028662673665417566\n",
      "Epoch 34/300\n",
      "Average training loss: 0.021558838551243145\n",
      "Average test loss: 0.002836971256674992\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02150987741516696\n",
      "Average test loss: 0.0029741021316084595\n",
      "Epoch 36/300\n",
      "Average training loss: 0.021371653182639016\n",
      "Average test loss: 0.0028650995068666006\n",
      "Epoch 37/300\n",
      "Average training loss: 0.02134040418929524\n",
      "Average test loss: 0.0030785587908079225\n",
      "Epoch 38/300\n",
      "Average training loss: 0.021252725293238958\n",
      "Average test loss: 0.0028453782335337665\n",
      "Epoch 39/300\n",
      "Average training loss: 0.021197400425871215\n",
      "Average test loss: 0.002817307270856367\n",
      "Epoch 40/300\n",
      "Average training loss: 0.02115022481812371\n",
      "Average test loss: 0.0028550487115151352\n",
      "Epoch 41/300\n",
      "Average training loss: 0.021109388868014018\n",
      "Average test loss: 0.002802414539373583\n",
      "Epoch 42/300\n",
      "Average training loss: 0.021015421537889375\n",
      "Average test loss: 0.002794925377704203\n",
      "Epoch 43/300\n",
      "Average training loss: 0.021011686265468597\n",
      "Average test loss: 0.0027402323494768806\n",
      "Epoch 44/300\n",
      "Average training loss: 0.0209448512610462\n",
      "Average test loss: 0.0027861157442546553\n",
      "Epoch 45/300\n",
      "Average training loss: 0.020891545771724647\n",
      "Average test loss: 0.0027727508681515854\n",
      "Epoch 46/300\n",
      "Average training loss: 0.020879073758920032\n",
      "Average test loss: 0.002747797876596451\n",
      "Epoch 47/300\n",
      "Average training loss: 0.02081182686322265\n",
      "Average test loss: 0.002816738030148877\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02077293333576785\n",
      "Average test loss: 0.002729038928118017\n",
      "Epoch 49/300\n",
      "Average training loss: 0.02072894130150477\n",
      "Average test loss: 0.0028534115567389463\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02068324533601602\n",
      "Average test loss: 0.0027912533984829984\n",
      "Epoch 51/300\n",
      "Average training loss: 0.020653869375586508\n",
      "Average test loss: 0.002699704601118962\n",
      "Epoch 52/300\n",
      "Average training loss: 0.020636315221587816\n",
      "Average test loss: 0.0027148187706867856\n",
      "Epoch 53/300\n",
      "Average training loss: 0.02061690196229352\n",
      "Average test loss: 0.0027516942107015186\n",
      "Epoch 54/300\n",
      "Average training loss: 0.02054639164937867\n",
      "Average test loss: 0.002727847802866664\n",
      "Epoch 55/300\n",
      "Average training loss: 0.02053864549762673\n",
      "Average test loss: 0.002696526311751869\n",
      "Epoch 56/300\n",
      "Average training loss: 0.02047972227467431\n",
      "Average test loss: 0.0027062133157418834\n",
      "Epoch 57/300\n",
      "Average training loss: 0.02048736869626575\n",
      "Average test loss: 0.0026884183458363016\n",
      "Epoch 58/300\n",
      "Average training loss: 0.020427800913651786\n",
      "Average test loss: 0.002707447512075305\n",
      "Epoch 59/300\n",
      "Average training loss: 0.020404966173900498\n",
      "Average test loss: 0.0026983272191137076\n",
      "Epoch 60/300\n",
      "Average training loss: 0.020405466092957392\n",
      "Average test loss: 0.0026584333827098213\n",
      "Epoch 61/300\n",
      "Average training loss: 0.020414307321111363\n",
      "Average test loss: 0.002712148418856992\n",
      "Epoch 62/300\n",
      "Average training loss: 0.020338750104109446\n",
      "Average test loss: 0.0027211303777164883\n",
      "Epoch 63/300\n",
      "Average training loss: 0.020316061991784307\n",
      "Average test loss: 0.0028167582241197427\n",
      "Epoch 64/300\n",
      "Average training loss: 0.02031631117562453\n",
      "Average test loss: 0.002665412700838513\n",
      "Epoch 65/300\n",
      "Average training loss: 0.020257402332292663\n",
      "Average test loss: 0.002774708674185806\n",
      "Epoch 66/300\n",
      "Average training loss: 0.020252303207914036\n",
      "Average test loss: 0.0026990006694363224\n",
      "Epoch 67/300\n",
      "Average training loss: 0.02022596529291736\n",
      "Average test loss: 0.0026669078663819367\n",
      "Epoch 68/300\n",
      "Average training loss: 0.020207570167051422\n",
      "Average test loss: 0.0027111103134229778\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020173650859130754\n",
      "Average test loss: 0.002704796782384316\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020167659274405902\n",
      "Average test loss: 0.0027143810997820564\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02020607964363363\n",
      "Average test loss: 0.0028116264761322072\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02010750716096825\n",
      "Average test loss: 0.002658911074925628\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020116934090024897\n",
      "Average test loss: 0.0026970406342297793\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020086581096053123\n",
      "Average test loss: 0.0026579299722280765\n",
      "Epoch 75/300\n",
      "Average training loss: 0.020060106289055614\n",
      "Average test loss: 0.002644521467594637\n",
      "Epoch 76/300\n",
      "Average training loss: 0.02003819804886977\n",
      "Average test loss: 0.0026645417254832055\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020028528559539054\n",
      "Average test loss: 0.0026894718000872266\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02002671408653259\n",
      "Average test loss: 0.0026753428272075124\n",
      "Epoch 79/300\n",
      "Average training loss: 0.019997774980134435\n",
      "Average test loss: 0.00265721533074975\n",
      "Epoch 80/300\n",
      "Average training loss: 0.019995129724343617\n",
      "Average test loss: 0.0026855185700373515\n",
      "Epoch 81/300\n",
      "Average training loss: 0.019972493261098862\n",
      "Average test loss: 0.002890441780702935\n",
      "Epoch 82/300\n",
      "Average training loss: 0.019954136792984273\n",
      "Average test loss: 0.002652520501365264\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01993609313832389\n",
      "Average test loss: 0.003437529097414679\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01993636022342576\n",
      "Average test loss: 0.0027307278795374763\n",
      "Epoch 85/300\n",
      "Average training loss: 0.019898106914427546\n",
      "Average test loss: 0.002680726735956139\n",
      "Epoch 86/300\n",
      "Average training loss: 0.01987781217528714\n",
      "Average test loss: 0.002639043734512395\n",
      "Epoch 87/300\n",
      "Average training loss: 0.020024387482967643\n",
      "Average test loss: 0.0026703816757847865\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01983773605359925\n",
      "Average test loss: 0.002636380144705375\n",
      "Epoch 89/300\n",
      "Average training loss: 0.019836354325215022\n",
      "Average test loss: 0.002631297491490841\n",
      "Epoch 90/300\n",
      "Average training loss: 0.01981838518381119\n",
      "Average test loss: 0.0026232992843207388\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01980946097936895\n",
      "Average test loss: 0.0026252949126064776\n",
      "Epoch 92/300\n",
      "Average training loss: 0.019810911128090487\n",
      "Average test loss: 0.0026400682551579345\n",
      "Epoch 93/300\n",
      "Average training loss: 0.01979080812798606\n",
      "Average test loss: 0.002694649179569549\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01979558125552204\n",
      "Average test loss: 0.0026528378122796614\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019767426782184176\n",
      "Average test loss: 0.0026416777471701303\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019768793609407212\n",
      "Average test loss: 0.002682448954424924\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019734181973669263\n",
      "Average test loss: 0.002631451967068844\n",
      "Epoch 98/300\n",
      "Average training loss: 0.019725081869297556\n",
      "Average test loss: 0.0026177371798290147\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01972449677520328\n",
      "Average test loss: 0.002676219884927074\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019715876206755637\n",
      "Average test loss: 0.0026299445012377366\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01971311469707224\n",
      "Average test loss: 0.0026238700716445843\n",
      "Epoch 102/300\n",
      "Average training loss: 0.019669010841184192\n",
      "Average test loss: 0.002665865068634351\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019653403150538602\n",
      "Average test loss: 0.0026388439492632944\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019647882948319116\n",
      "Average test loss: 0.0026735465669383606\n",
      "Epoch 105/300\n",
      "Average training loss: 0.01967276499999894\n",
      "Average test loss: 0.0026248045220143264\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019641307392054132\n",
      "Average test loss: 0.002665655211855968\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019636428877711297\n",
      "Average test loss: 0.0026177434685329595\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019617643199033207\n",
      "Average test loss: 0.002636519426272975\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019598159939050674\n",
      "Average test loss: 0.0026642196813805234\n",
      "Epoch 110/300\n",
      "Average training loss: 0.01960355262292756\n",
      "Average test loss: 0.0026624098018639616\n",
      "Epoch 111/300\n",
      "Average training loss: 0.019583188188572725\n",
      "Average test loss: 0.0026642168348448144\n",
      "Epoch 112/300\n",
      "Average training loss: 0.019562694152196247\n",
      "Average test loss: 0.0026190387732866736\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01957498153878583\n",
      "Average test loss: 0.010748995423316955\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019570769627888996\n",
      "Average test loss: 0.002663731246151858\n",
      "Epoch 115/300\n",
      "Average training loss: 0.01954966281188859\n",
      "Average test loss: 0.0026100771385762426\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01955348673959573\n",
      "Average test loss: 0.002631857567984197\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019529136170943578\n",
      "Average test loss: 0.002632135912982954\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01950400516225232\n",
      "Average test loss: 0.002622805745444364\n",
      "Epoch 119/300\n",
      "Average training loss: 0.019507317539718416\n",
      "Average test loss: 0.002624949987563822\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0194877901987897\n",
      "Average test loss: 0.0026443573063653376\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01951387541823917\n",
      "Average test loss: 0.0026777533156176407\n",
      "Epoch 122/300\n",
      "Average training loss: 0.019498246906532183\n",
      "Average test loss: 0.002715280962694022\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019516906044549413\n",
      "Average test loss: 0.0026371462605893612\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01947286915944682\n",
      "Average test loss: 0.0026171808052394127\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01944827762246132\n",
      "Average test loss: 0.002607324740332034\n",
      "Epoch 126/300\n",
      "Average training loss: 0.019445923515492014\n",
      "Average test loss: 0.00260971791897383\n",
      "Epoch 127/300\n",
      "Average training loss: 0.01943233002639479\n",
      "Average test loss: 0.0026181305433726973\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019441374182701112\n",
      "Average test loss: 0.0026237122126751477\n",
      "Epoch 129/300\n",
      "Average training loss: 0.01942722164756722\n",
      "Average test loss: 0.0026137886240871415\n",
      "Epoch 130/300\n",
      "Average training loss: 0.01941481205324332\n",
      "Average test loss: 0.002606479998057087\n",
      "Epoch 131/300\n",
      "Average training loss: 0.01940571523209413\n",
      "Average test loss: 0.0026327881086617706\n",
      "Epoch 132/300\n",
      "Average training loss: 0.019404298125041856\n",
      "Average test loss: 0.0026832291591498587\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019393879984815916\n",
      "Average test loss: 0.002634065854466624\n",
      "Epoch 134/300\n",
      "Average training loss: 0.019387879558735423\n",
      "Average test loss: 0.0026294135546518697\n",
      "Epoch 135/300\n",
      "Average training loss: 0.01937486837638749\n",
      "Average test loss: 0.0026219277365340126\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019353744404183495\n",
      "Average test loss: 0.0025953235389250848\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01935170986917284\n",
      "Average test loss: 0.0025986182406130764\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0193580046478245\n",
      "Average test loss: 0.002662028177123931\n",
      "Epoch 139/300\n",
      "Average training loss: 0.019334830400016573\n",
      "Average test loss: 0.0026523485167158976\n",
      "Epoch 140/300\n",
      "Average training loss: 0.01933730764190356\n",
      "Average test loss: 0.002673575811708967\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01932065943711334\n",
      "Average test loss: 0.0026338999650130667\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01932637089325322\n",
      "Average test loss: 0.0026157614319688746\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01930998281472259\n",
      "Average test loss: 0.002603758361811439\n",
      "Epoch 144/300\n",
      "Average training loss: 0.019297020018100738\n",
      "Average test loss: 0.0026266624180393086\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01929901459813118\n",
      "Average test loss: 0.0026179951638397245\n",
      "Epoch 146/300\n",
      "Average training loss: 0.019289490807387562\n",
      "Average test loss: 0.0025937755308631395\n",
      "Epoch 147/300\n",
      "Average training loss: 0.019291009532080756\n",
      "Average test loss: 0.002610761051894062\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01928231470617983\n",
      "Average test loss: 0.002592701577477985\n",
      "Epoch 149/300\n",
      "Average training loss: 0.019290822313891516\n",
      "Average test loss: 0.0026076964071641367\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0192608639680677\n",
      "Average test loss: 0.0026091988562709756\n",
      "Epoch 151/300\n",
      "Average training loss: 0.019249705960353216\n",
      "Average test loss: 0.002731143485237327\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01925466409822305\n",
      "Average test loss: 0.002622316168828143\n",
      "Epoch 153/300\n",
      "Average training loss: 0.019250516206026077\n",
      "Average test loss: 0.0028173778226806057\n",
      "Epoch 154/300\n",
      "Average training loss: 0.019236689440078204\n",
      "Average test loss: 0.0025949325155880717\n",
      "Epoch 155/300\n",
      "Average training loss: 0.019235564965340825\n",
      "Average test loss: 0.0026202606838196516\n",
      "Epoch 156/300\n",
      "Average training loss: 0.019197224216328728\n",
      "Average test loss: 0.0026194974958068796\n",
      "Epoch 157/300\n",
      "Average training loss: 0.019239253155887127\n",
      "Average test loss: 0.002618552296525902\n",
      "Epoch 158/300\n",
      "Average training loss: 0.019206722289323807\n",
      "Average test loss: 0.0026022510504763985\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01918257175385952\n",
      "Average test loss: 0.002616075236764219\n",
      "Epoch 160/300\n",
      "Average training loss: 0.019192783165309164\n",
      "Average test loss: 0.0026620799282358752\n",
      "Epoch 161/300\n",
      "Average training loss: 0.01919590657287174\n",
      "Average test loss: 0.0025937877119415337\n",
      "Epoch 162/300\n",
      "Average training loss: 0.019185036136044397\n",
      "Average test loss: 0.002626179010089901\n",
      "Epoch 163/300\n",
      "Average training loss: 0.019178750794794825\n",
      "Average test loss: 0.002605868733798464\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01918659880757332\n",
      "Average test loss: 0.0026439540369643105\n",
      "Epoch 165/300\n",
      "Average training loss: 0.019173789284295507\n",
      "Average test loss: 0.002646717809761564\n",
      "Epoch 166/300\n",
      "Average training loss: 0.019147997917400465\n",
      "Average test loss: 0.0026332686127473913\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01915188062356578\n",
      "Average test loss: 0.0027118375301361086\n",
      "Epoch 168/300\n",
      "Average training loss: 0.019152640884121257\n",
      "Average test loss: 0.002612727544580897\n",
      "Epoch 169/300\n",
      "Average training loss: 0.01916244869182507\n",
      "Average test loss: 0.0026332329662723673\n",
      "Epoch 170/300\n",
      "Average training loss: 0.019131071206596163\n",
      "Average test loss: 0.0026036968682375218\n",
      "Epoch 171/300\n",
      "Average training loss: 0.019132503022750217\n",
      "Average test loss: 0.002613020379717151\n",
      "Epoch 172/300\n",
      "Average training loss: 0.019127929518620173\n",
      "Average test loss: 0.0029220956394241915\n",
      "Epoch 173/300\n",
      "Average training loss: 0.01911735248234537\n",
      "Average test loss: 0.002593947001422445\n",
      "Epoch 174/300\n",
      "Average training loss: 0.019112210778726473\n",
      "Average test loss: 0.002652350147979127\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01912166039314535\n",
      "Average test loss: 0.002650346683131324\n",
      "Epoch 176/300\n",
      "Average training loss: 0.019114834989938472\n",
      "Average test loss: 0.0026307082650148207\n",
      "Epoch 177/300\n",
      "Average training loss: 0.019101754331754314\n",
      "Average test loss: 0.0026041242008407912\n",
      "Epoch 178/300\n",
      "Average training loss: 0.019103662605086964\n",
      "Average test loss: 0.002619779668437938\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01908662372165256\n",
      "Average test loss: 0.006440383567578263\n",
      "Epoch 180/300\n",
      "Average training loss: 0.019093568121393522\n",
      "Average test loss: 0.002617216401216057\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01909251964920097\n",
      "Average test loss: 0.0027390370501412286\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019075674088464844\n",
      "Average test loss: 0.0026179370275802084\n",
      "Epoch 183/300\n",
      "Average training loss: 0.019069428318076665\n",
      "Average test loss: 0.002594915951912602\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01906790294746558\n",
      "Average test loss: 0.0026152967451554205\n",
      "Epoch 185/300\n",
      "Average training loss: 0.019057764728864033\n",
      "Average test loss: 0.0026037719189706777\n",
      "Epoch 186/300\n",
      "Average training loss: 0.019053917706012728\n",
      "Average test loss: 0.002590167934178478\n",
      "Epoch 187/300\n",
      "Average training loss: 0.019086348432633612\n",
      "Average test loss: 0.0025981637458834385\n",
      "Epoch 188/300\n",
      "Average training loss: 0.01905377559363842\n",
      "Average test loss: 0.002626562696364191\n",
      "Epoch 189/300\n",
      "Average training loss: 0.019041021321382787\n",
      "Average test loss: 0.00269948638147778\n",
      "Epoch 190/300\n",
      "Average training loss: 0.019042415857315063\n",
      "Average test loss: 0.0026119138995806374\n",
      "Epoch 191/300\n",
      "Average training loss: 0.019021608009106582\n",
      "Average test loss: 0.002703449798333976\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0190240016364389\n",
      "Average test loss: 0.0026169969965186383\n",
      "Epoch 193/300\n",
      "Average training loss: 0.019028119434085156\n",
      "Average test loss: 0.0026340559764454763\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01903023622598913\n",
      "Average test loss: 0.0026127454966513647\n",
      "Epoch 195/300\n",
      "Average training loss: 0.019012907521592248\n",
      "Average test loss: 0.0026188316374189326\n",
      "Epoch 196/300\n",
      "Average training loss: 0.019009531578256023\n",
      "Average test loss: 0.0026721349517918294\n",
      "Epoch 197/300\n",
      "Average training loss: 0.019000155477060212\n",
      "Average test loss: 0.002613926440684332\n",
      "Epoch 198/300\n",
      "Average training loss: 0.019010671261284088\n",
      "Average test loss: 0.002650041113504105\n",
      "Epoch 199/300\n",
      "Average training loss: 0.018990998190310266\n",
      "Average test loss: 0.0026135488268401886\n",
      "Epoch 200/300\n",
      "Average training loss: 0.01898949221769969\n",
      "Average test loss: 0.002608669739630487\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01899303204483456\n",
      "Average test loss: 0.0025959491328232816\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01898627366622289\n",
      "Average test loss: 0.0026222785795107484\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0189731678482559\n",
      "Average test loss: 0.0026137097356840966\n",
      "Epoch 204/300\n",
      "Average training loss: 0.018963254135515955\n",
      "Average test loss: 0.0025967559408810402\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018956609138184123\n",
      "Average test loss: 0.002639893837273121\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018963053125474187\n",
      "Average test loss: 0.002610780693590641\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018957588918507098\n",
      "Average test loss: 0.0027011195035237406\n",
      "Epoch 208/300\n",
      "Average training loss: 0.01895874674618244\n",
      "Average test loss: 0.00261356804602676\n",
      "Epoch 209/300\n",
      "Average training loss: 0.01895459852615992\n",
      "Average test loss: 0.0026150210921963056\n",
      "Epoch 210/300\n",
      "Average training loss: 0.018948079311185414\n",
      "Average test loss: 0.002618450644115607\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018932380065321924\n",
      "Average test loss: 0.002616650980379846\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018944797987739244\n",
      "Average test loss: 0.0026263726465404033\n",
      "Epoch 213/300\n",
      "Average training loss: 0.01892588755985101\n",
      "Average test loss: 0.0026256495045704972\n",
      "Epoch 214/300\n",
      "Average training loss: 0.018934386778208946\n",
      "Average test loss: 0.0026283887461241748\n",
      "Epoch 215/300\n",
      "Average training loss: 0.018934227757983736\n",
      "Average test loss: 0.0026354196313768626\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018920826454129482\n",
      "Average test loss: 0.002658306196745899\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0189433246999979\n",
      "Average test loss: 0.002624287954842051\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018914836921625666\n",
      "Average test loss: 0.0026532305468701654\n",
      "Epoch 219/300\n",
      "Average training loss: 0.0189014030645291\n",
      "Average test loss: 0.0026384670870999495\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018892013458742037\n",
      "Average test loss: 0.0027579385480946965\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018904084927505917\n",
      "Average test loss: 0.0029615730381467274\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018897664725780486\n",
      "Average test loss: 0.002695323559145133\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018913990972770585\n",
      "Average test loss: 0.0026941834332214463\n",
      "Epoch 224/300\n",
      "Average training loss: 0.01888431982199351\n",
      "Average test loss: 0.002655628223799997\n",
      "Epoch 225/300\n",
      "Average training loss: 0.018882295360167823\n",
      "Average test loss: 0.0026006936848991446\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018893657255503866\n",
      "Average test loss: 0.0026226794881125293\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018868517703480192\n",
      "Average test loss: 0.002605425589821405\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018872055206033918\n",
      "Average test loss: 0.002616549048365818\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01886516110599041\n",
      "Average test loss: 0.0026270853265499077\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018856256292925942\n",
      "Average test loss: 0.002880216606168283\n",
      "Epoch 231/300\n",
      "Average training loss: 0.018856889370414947\n",
      "Average test loss: 0.002591712392038769\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018851801969938808\n",
      "Average test loss: 0.003124819496853484\n",
      "Epoch 233/300\n",
      "Average training loss: 0.018866918144954576\n",
      "Average test loss: 0.0025946750084145202\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01886578545636601\n",
      "Average test loss: 0.0027321423679176303\n",
      "Epoch 235/300\n",
      "Average training loss: 0.01884541322953171\n",
      "Average test loss: 0.002687412463128567\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018849663929806814\n",
      "Average test loss: 0.01704588158428669\n",
      "Epoch 237/300\n",
      "Average training loss: 0.01896225888033708\n",
      "Average test loss: 0.002654672605709897\n",
      "Epoch 238/300\n",
      "Average training loss: 0.018822672809163728\n",
      "Average test loss: 0.0026298360801819297\n",
      "Epoch 239/300\n",
      "Average training loss: 0.018823778695530362\n",
      "Average test loss: 0.0026504706078105504\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01884602504471938\n",
      "Average test loss: 0.0026426495824837023\n",
      "Epoch 241/300\n",
      "Average training loss: 0.018808856922719212\n",
      "Average test loss: 0.0026156094254304965\n",
      "Epoch 242/300\n",
      "Average training loss: 0.018828183218836784\n",
      "Average test loss: 0.002620992603401343\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01881939767797788\n",
      "Average test loss: 0.0026072447827706732\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01882532062050369\n",
      "Average test loss: 0.002617189928371873\n",
      "Epoch 245/300\n",
      "Average training loss: 0.018812060281634332\n",
      "Average test loss: 0.0026093627150273985\n",
      "Epoch 246/300\n",
      "Average training loss: 0.018813647747039794\n",
      "Average test loss: 0.002630783961775402\n",
      "Epoch 247/300\n",
      "Average training loss: 0.018813015033801398\n",
      "Average test loss: 0.0026267434762169916\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01879930636783441\n",
      "Average test loss: 0.002618370519330104\n",
      "Epoch 249/300\n",
      "Average training loss: 0.018797438240713542\n",
      "Average test loss: 0.002799091938883066\n",
      "Epoch 250/300\n",
      "Average training loss: 0.018799128269983662\n",
      "Average test loss: 0.002751693603893121\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01880733753078514\n",
      "Average test loss: 0.0026363841796086893\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01880576685567697\n",
      "Average test loss: 0.0026488128571460646\n",
      "Epoch 253/300\n",
      "Average training loss: 0.018793047996030914\n",
      "Average test loss: 0.0026086617386382486\n",
      "Epoch 254/300\n",
      "Average training loss: 0.01878490602970123\n",
      "Average test loss: 0.0026316216507305703\n",
      "Epoch 255/300\n",
      "Average training loss: 0.01877807458407349\n",
      "Average test loss: 0.002616271060374048\n",
      "Epoch 256/300\n",
      "Average training loss: 0.01879017364813222\n",
      "Average test loss: 0.0026326398137542935\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01878397316733996\n",
      "Average test loss: 0.002623514126158423\n",
      "Epoch 258/300\n",
      "Average training loss: 0.018767777474390134\n",
      "Average test loss: 0.002626195687386725\n",
      "Epoch 259/300\n",
      "Average training loss: 0.01876582860781087\n",
      "Average test loss: 0.002619794781423277\n",
      "Epoch 260/300\n",
      "Average training loss: 0.018774794058667287\n",
      "Average test loss: 0.0026116825620540315\n",
      "Epoch 261/300\n",
      "Average training loss: 0.018764814678165646\n",
      "Average test loss: 0.002639635107169549\n",
      "Epoch 262/300\n",
      "Average training loss: 0.018752588333355056\n",
      "Average test loss: 0.002646800104321705\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01878382982313633\n",
      "Average test loss: 0.002658929909269015\n",
      "Epoch 264/300\n",
      "Average training loss: 0.018738018517692884\n",
      "Average test loss: 0.0025972466071446736\n",
      "Epoch 265/300\n",
      "Average training loss: 0.018740892893738217\n",
      "Average test loss: 0.002627063393799795\n",
      "Epoch 266/300\n",
      "Average training loss: 0.018862433368961016\n",
      "Average test loss: 0.002610615548367302\n",
      "Epoch 267/300\n",
      "Average training loss: 0.018742904581957393\n",
      "Average test loss: 0.0026156216205822096\n",
      "Epoch 268/300\n",
      "Average training loss: 0.018734290955795183\n",
      "Average test loss: 0.002602174040964908\n",
      "Epoch 269/300\n",
      "Average training loss: 0.018735724368857013\n",
      "Average test loss: 0.0027925684839073153\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01873704664905866\n",
      "Average test loss: 0.0026867789831012488\n",
      "Epoch 271/300\n",
      "Average training loss: 0.018737430940071742\n",
      "Average test loss: 0.0025987237265540494\n",
      "Epoch 272/300\n",
      "Average training loss: 0.018728073288997015\n",
      "Average test loss: 0.0026159057182570298\n",
      "Epoch 273/300\n",
      "Average training loss: 0.018716271219982043\n",
      "Average test loss: 0.0027005928616142937\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0187339249253273\n",
      "Average test loss: 0.0026265292630220454\n",
      "Epoch 275/300\n",
      "Average training loss: 0.018720914349787766\n",
      "Average test loss: 0.0026349170056896077\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01874021033032073\n",
      "Average test loss: 0.0026714306504776082\n",
      "Epoch 277/300\n",
      "Average training loss: 0.018714267301890585\n",
      "Average test loss: 0.002671623558633857\n",
      "Epoch 278/300\n",
      "Average training loss: 0.018708335475789175\n",
      "Average test loss: 0.0026355923468040095\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01870585590766536\n",
      "Average test loss: 0.0026240193392667505\n",
      "Epoch 280/300\n",
      "Average training loss: 0.018702841973967023\n",
      "Average test loss: 0.0026847590006267032\n",
      "Epoch 281/300\n",
      "Average training loss: 0.018715056048499212\n",
      "Average test loss: 0.002616847120846311\n",
      "Epoch 282/300\n",
      "Average training loss: 0.018693195224636132\n",
      "Average test loss: 0.0026805842065562805\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01869864088131322\n",
      "Average test loss: 0.002645256608310673\n",
      "Epoch 284/300\n",
      "Average training loss: 0.018703841952813997\n",
      "Average test loss: 0.0026116864728844827\n",
      "Epoch 285/300\n",
      "Average training loss: 0.018689554082022775\n",
      "Average test loss: 0.002607929868416654\n",
      "Epoch 286/300\n",
      "Average training loss: 0.018683530734644994\n",
      "Average test loss: 0.0026417862886769904\n",
      "Epoch 287/300\n",
      "Average training loss: 0.01868342287838459\n",
      "Average test loss: 0.0026657235773487223\n",
      "Epoch 288/300\n",
      "Average training loss: 0.018678558187352288\n",
      "Average test loss: 0.002654386755492952\n",
      "Epoch 289/300\n",
      "Average training loss: 0.018692040340767966\n",
      "Average test loss: 0.002632541182761391\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01867201271487607\n",
      "Average test loss: 0.002627777650538418\n",
      "Epoch 291/300\n",
      "Average training loss: 0.018659564932187397\n",
      "Average test loss: 0.0026238211074637043\n",
      "Epoch 292/300\n",
      "Average training loss: 0.0186814488776856\n",
      "Average test loss: 0.0026418848989738356\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01867891635828548\n",
      "Average test loss: 0.0026832555702163115\n",
      "Epoch 294/300\n",
      "Average training loss: 0.018664366907543608\n",
      "Average test loss: 0.002626720520357291\n",
      "Epoch 295/300\n",
      "Average training loss: 0.01865923105345832\n",
      "Average test loss: 0.0026174993926866186\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01865803396039539\n",
      "Average test loss: 0.00259681361541152\n",
      "Epoch 297/300\n",
      "Average training loss: 0.01866211798787117\n",
      "Average test loss: 0.002612378523995479\n",
      "Epoch 298/300\n",
      "Average training loss: 0.018660757664177154\n",
      "Average test loss: 0.002695438457859887\n",
      "Epoch 299/300\n",
      "Average training loss: 0.018642881540788546\n",
      "Average test loss: 0.002693384597905808\n",
      "Epoch 300/300\n",
      "Average training loss: 0.01865223869184653\n",
      "Average test loss: 0.0026187997879460454\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.1670395461320877\n",
      "Average test loss: 0.0051948059052228926\n",
      "Epoch 2/300\n",
      "Average training loss: 0.03893389566408263\n",
      "Average test loss: 0.005016216717660427\n",
      "Epoch 3/300\n",
      "Average training loss: 0.034534031215641234\n",
      "Average test loss: 0.004668634755329954\n",
      "Epoch 4/300\n",
      "Average training loss: 0.031580243753062356\n",
      "Average test loss: 0.004155167423602607\n",
      "Epoch 5/300\n",
      "Average training loss: 0.02984873690870073\n",
      "Average test loss: 0.0037315128369049895\n",
      "Epoch 6/300\n",
      "Average training loss: 0.02797973506483767\n",
      "Average test loss: 0.00354663217253983\n",
      "Epoch 7/300\n",
      "Average training loss: 0.026979041601220766\n",
      "Average test loss: 0.0035949761234223845\n",
      "Epoch 8/300\n",
      "Average training loss: 0.02581609044637945\n",
      "Average test loss: 0.0032600188710623316\n",
      "Epoch 9/300\n",
      "Average training loss: 0.025030651602480147\n",
      "Average test loss: 0.0031341949488139817\n",
      "Epoch 10/300\n",
      "Average training loss: 0.024175993649495974\n",
      "Average test loss: 0.003686409001135164\n",
      "Epoch 11/300\n",
      "Average training loss: 0.023390279816256628\n",
      "Average test loss: 0.0030498275755801133\n",
      "Epoch 12/300\n",
      "Average training loss: 0.0227799067861504\n",
      "Average test loss: 0.002854380675488048\n",
      "Epoch 13/300\n",
      "Average training loss: 0.022187395501467918\n",
      "Average test loss: 0.0028329193035347595\n",
      "Epoch 14/300\n",
      "Average training loss: 0.02181687628891733\n",
      "Average test loss: 0.0027593496452189155\n",
      "Epoch 15/300\n",
      "Average training loss: 0.021307159756620724\n",
      "Average test loss: 0.002741806420394116\n",
      "Epoch 16/300\n",
      "Average training loss: 0.020921606492665078\n",
      "Average test loss: 0.0027254144706659847\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02059509809480773\n",
      "Average test loss: 0.0027983891376190716\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020330791655513974\n",
      "Average test loss: 0.0025022810970743497\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02003692436218262\n",
      "Average test loss: 0.002461506118170089\n",
      "Epoch 20/300\n",
      "Average training loss: 0.019767156571149826\n",
      "Average test loss: 0.0024999214253491825\n",
      "Epoch 21/300\n",
      "Average training loss: 0.019559052204092344\n",
      "Average test loss: 0.0024383789192264278\n",
      "Epoch 22/300\n",
      "Average training loss: 0.019386855686704316\n",
      "Average test loss: 0.002386294764983985\n",
      "Epoch 23/300\n",
      "Average training loss: 0.019165948684016863\n",
      "Average test loss: 0.0023247470532854397\n",
      "Epoch 24/300\n",
      "Average training loss: 0.019047614263163672\n",
      "Average test loss: 0.0023819355716307956\n",
      "Epoch 25/300\n",
      "Average training loss: 0.01890184961590502\n",
      "Average test loss: 0.0022662056411306064\n",
      "Epoch 26/300\n",
      "Average training loss: 0.018747587056623567\n",
      "Average test loss: 0.002263493403999342\n",
      "Epoch 27/300\n",
      "Average training loss: 0.01866190998007854\n",
      "Average test loss: 0.0022805225991954408\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01852791891992092\n",
      "Average test loss: 0.002208566409225265\n",
      "Epoch 29/300\n",
      "Average training loss: 0.018447162775529755\n",
      "Average test loss: 0.002269363525013129\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01830297291278839\n",
      "Average test loss: 0.002214769461709592\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01825839975476265\n",
      "Average test loss: 0.0021968405740335584\n",
      "Epoch 32/300\n",
      "Average training loss: 0.018174363705019157\n",
      "Average test loss: 0.0022077753742535907\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018059511785705883\n",
      "Average test loss: 0.002149071289640334\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01801214759879642\n",
      "Average test loss: 0.0021552469285411966\n",
      "Epoch 35/300\n",
      "Average training loss: 0.017950129513939223\n",
      "Average test loss: 0.0021331970586131017\n",
      "Epoch 36/300\n",
      "Average training loss: 0.017881032018197907\n",
      "Average test loss: 0.0021308453058203063\n",
      "Epoch 37/300\n",
      "Average training loss: 0.01782758090065585\n",
      "Average test loss: 0.0022313354570004675\n",
      "Epoch 38/300\n",
      "Average training loss: 0.01774517315046655\n",
      "Average test loss: 0.00214954612383412\n",
      "Epoch 39/300\n",
      "Average training loss: 0.017731238113509285\n",
      "Average test loss: 0.0020854667195429407\n",
      "Epoch 40/300\n",
      "Average training loss: 0.017644031685259607\n",
      "Average test loss: 0.0020759308387835822\n",
      "Epoch 41/300\n",
      "Average training loss: 0.017607827267713018\n",
      "Average test loss: 0.00206605071015656\n",
      "Epoch 42/300\n",
      "Average training loss: 0.017577352116505306\n",
      "Average test loss: 0.002093193349531955\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017548485255075824\n",
      "Average test loss: 0.0031968139368626805\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017486894971794554\n",
      "Average test loss: 0.0020758772577262586\n",
      "Epoch 45/300\n",
      "Average training loss: 0.017452934821446737\n",
      "Average test loss: 0.002063797252666619\n",
      "Epoch 46/300\n",
      "Average training loss: 0.01739159577422672\n",
      "Average test loss: 0.0020544107810904584\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017363593263758553\n",
      "Average test loss: 0.0020572101639376745\n",
      "Epoch 48/300\n",
      "Average training loss: 0.017333640110161568\n",
      "Average test loss: 0.0020427049015545183\n",
      "Epoch 49/300\n",
      "Average training loss: 0.017299879090653527\n",
      "Average test loss: 0.0020909768847955598\n",
      "Epoch 50/300\n",
      "Average training loss: 0.017264349236256547\n",
      "Average test loss: 0.0020810739659807747\n",
      "Epoch 51/300\n",
      "Average training loss: 0.017259563249018457\n",
      "Average test loss: 0.0020537608108586736\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01721558951586485\n",
      "Average test loss: 0.002024565249164071\n",
      "Epoch 53/300\n",
      "Average training loss: 0.01717458993030919\n",
      "Average test loss: 0.0020578721899849675\n",
      "Epoch 54/300\n",
      "Average training loss: 0.01713729398449262\n",
      "Average test loss: 0.002017053528999289\n",
      "Epoch 55/300\n",
      "Average training loss: 0.01713076709707578\n",
      "Average test loss: 0.002192456436964373\n",
      "Epoch 56/300\n",
      "Average training loss: 0.017085854286120996\n",
      "Average test loss: 0.0020457236420156227\n",
      "Epoch 57/300\n",
      "Average training loss: 0.017071882769465446\n",
      "Average test loss: 0.002047790962995754\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01705706948538621\n",
      "Average test loss: 0.0020284511766706904\n",
      "Epoch 59/300\n",
      "Average training loss: 0.017028198740548558\n",
      "Average test loss: 0.0020126193751477534\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016993499517440797\n",
      "Average test loss: 0.002044400019157264\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016983044432269202\n",
      "Average test loss: 0.0020023474345604577\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016954325755437216\n",
      "Average test loss: 0.00206824778051426\n",
      "Epoch 63/300\n",
      "Average training loss: 0.016961221996280883\n",
      "Average test loss: 0.002013490262441337\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016912433923946486\n",
      "Average test loss: 0.002048961189058092\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016895097124907707\n",
      "Average test loss: 0.002017927674162719\n",
      "Epoch 66/300\n",
      "Average training loss: 0.016870447681181962\n",
      "Average test loss: 0.001992798560816381\n",
      "Epoch 67/300\n",
      "Average training loss: 0.016869748681783675\n",
      "Average test loss: 0.0019991237663974366\n",
      "Epoch 68/300\n",
      "Average training loss: 0.01685893655816714\n",
      "Average test loss: 0.0020168476534179517\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016816928553912376\n",
      "Average test loss: 0.0019940326692950393\n",
      "Epoch 70/300\n",
      "Average training loss: 0.01677490156557825\n",
      "Average test loss: 0.0019908324651834036\n",
      "Epoch 71/300\n",
      "Average training loss: 0.016786530336572063\n",
      "Average test loss: 0.0020075180609193115\n",
      "Epoch 72/300\n",
      "Average training loss: 0.016772037322322526\n",
      "Average test loss: 0.0020430527984475096\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016758708466258314\n",
      "Average test loss: 0.0020174643757442635\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01673482483708196\n",
      "Average test loss: 0.0019822865147143603\n",
      "Epoch 75/300\n",
      "Average training loss: 0.016716868991653124\n",
      "Average test loss: 0.001982114049916466\n",
      "Epoch 76/300\n",
      "Average training loss: 0.01670485134671132\n",
      "Average test loss: 0.0020042645380728774\n",
      "Epoch 77/300\n",
      "Average training loss: 0.016682903379201888\n",
      "Average test loss: 0.001996680650756591\n",
      "Epoch 78/300\n",
      "Average training loss: 0.016674978009528583\n",
      "Average test loss: 0.0033487162084008255\n",
      "Epoch 79/300\n",
      "Average training loss: 0.016667684896124735\n",
      "Average test loss: 0.0019795550323194926\n",
      "Epoch 80/300\n",
      "Average training loss: 0.016627620046337447\n",
      "Average test loss: 0.0019900199597080548\n",
      "Epoch 81/300\n",
      "Average training loss: 0.016625305824809603\n",
      "Average test loss: 0.0020213476239393153\n",
      "Epoch 82/300\n",
      "Average training loss: 0.016620315190818576\n",
      "Average test loss: 0.0020321175780975156\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01659425546725591\n",
      "Average test loss: 0.0019886613100146256\n",
      "Epoch 84/300\n",
      "Average training loss: 0.016586706830395592\n",
      "Average test loss: 0.001978580721964439\n",
      "Epoch 85/300\n",
      "Average training loss: 0.01657865789035956\n",
      "Average test loss: 0.0020055483107765514\n",
      "Epoch 86/300\n",
      "Average training loss: 0.016549162119627\n",
      "Average test loss: 0.0019735596869140865\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01654971343610022\n",
      "Average test loss: 0.0019878780162996715\n",
      "Epoch 88/300\n",
      "Average training loss: 0.01652977682981226\n",
      "Average test loss: 0.002006252304547363\n",
      "Epoch 89/300\n",
      "Average training loss: 0.016532191938824124\n",
      "Average test loss: 0.0020109315859153866\n",
      "Epoch 90/300\n",
      "Average training loss: 0.016496847331523894\n",
      "Average test loss: 0.0020621447240312895\n",
      "Epoch 91/300\n",
      "Average training loss: 0.016494939850436315\n",
      "Average test loss: 0.001990882307911913\n",
      "Epoch 92/300\n",
      "Average training loss: 0.016491726805766424\n",
      "Average test loss: 0.001973508022622102\n",
      "Epoch 93/300\n",
      "Average training loss: 0.016473895634214083\n",
      "Average test loss: 0.0019712504748668935\n",
      "Epoch 94/300\n",
      "Average training loss: 0.016458982171283828\n",
      "Average test loss: 0.0019792335100678933\n",
      "Epoch 95/300\n",
      "Average training loss: 0.01645294955538379\n",
      "Average test loss: 0.0019919230754797657\n",
      "Epoch 96/300\n",
      "Average training loss: 0.016437748476035064\n",
      "Average test loss: 0.001998244175480472\n",
      "Epoch 97/300\n",
      "Average training loss: 0.016438966754410003\n",
      "Average test loss: 0.00198848629453116\n",
      "Epoch 98/300\n",
      "Average training loss: 0.016423717099759313\n",
      "Average test loss: 0.0019732256420991485\n",
      "Epoch 99/300\n",
      "Average training loss: 0.016400519683129257\n",
      "Average test loss: 0.0020109211961842244\n",
      "Epoch 100/300\n",
      "Average training loss: 0.01640100830793381\n",
      "Average test loss: 0.002021766097077893\n",
      "Epoch 101/300\n",
      "Average training loss: 0.01638184762166606\n",
      "Average test loss: 0.0019743648703313536\n",
      "Epoch 102/300\n",
      "Average training loss: 0.016388912812703185\n",
      "Average test loss: 0.0019910785504099397\n",
      "Epoch 103/300\n",
      "Average training loss: 0.016383540295892292\n",
      "Average test loss: 0.0019891107209130295\n",
      "Epoch 104/300\n",
      "Average training loss: 0.01636724039995008\n",
      "Average test loss: 0.001991359847390817\n",
      "Epoch 105/300\n",
      "Average training loss: 0.016358243425687154\n",
      "Average test loss: 0.0038802957433379357\n",
      "Epoch 106/300\n",
      "Average training loss: 0.016355909388926296\n",
      "Average test loss: 0.0019834215169151626\n",
      "Epoch 107/300\n",
      "Average training loss: 0.016350848140815895\n",
      "Average test loss: 0.0019678351892572312\n",
      "Epoch 108/300\n",
      "Average training loss: 0.016316823975907432\n",
      "Average test loss: 0.0019634279017336667\n",
      "Epoch 109/300\n",
      "Average training loss: 0.01631063600215647\n",
      "Average test loss: 0.0019610996198736958\n",
      "Epoch 110/300\n",
      "Average training loss: 0.016308470929662387\n",
      "Average test loss: 0.0020000875775391855\n",
      "Epoch 111/300\n",
      "Average training loss: 0.016297267663809987\n",
      "Average test loss: 0.001972259648040765\n",
      "Epoch 112/300\n",
      "Average training loss: 0.016285302869147724\n",
      "Average test loss: 0.0019942170990010105\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01628403526544571\n",
      "Average test loss: 0.001989633397820095\n",
      "Epoch 114/300\n",
      "Average training loss: 0.016281110412544673\n",
      "Average test loss: 0.0019787487705341645\n",
      "Epoch 115/300\n",
      "Average training loss: 0.016263977247807714\n",
      "Average test loss: 0.0019636778438256847\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01626375948720508\n",
      "Average test loss: 0.0020012566147165167\n",
      "Epoch 117/300\n",
      "Average training loss: 0.016256847018169034\n",
      "Average test loss: 0.0019885309611757597\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01623459230363369\n",
      "Average test loss: 0.0019987835246655676\n",
      "Epoch 119/300\n",
      "Average training loss: 0.016237345930602814\n",
      "Average test loss: 0.001978641998436716\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01621757119231754\n",
      "Average test loss: 0.0019581378974641364\n",
      "Epoch 121/300\n",
      "Average training loss: 0.016211598525444668\n",
      "Average test loss: 0.001997938992145161\n",
      "Epoch 122/300\n",
      "Average training loss: 0.016202510130074288\n",
      "Average test loss: 0.0020443487671307392\n",
      "Epoch 123/300\n",
      "Average training loss: 0.016194135831462012\n",
      "Average test loss: 0.001960515771061182\n",
      "Epoch 124/300\n",
      "Average training loss: 0.01619743099477556\n",
      "Average test loss: 0.0020785259483382104\n",
      "Epoch 125/300\n",
      "Average training loss: 0.01618290744556321\n",
      "Average test loss: 0.009302002117865615\n",
      "Epoch 126/300\n",
      "Average training loss: 0.016269924955235587\n",
      "Average test loss: 0.0019713851109974916\n",
      "Epoch 127/300\n",
      "Average training loss: 0.016160548905531567\n",
      "Average test loss: 0.001983431942553984\n",
      "Epoch 128/300\n",
      "Average training loss: 0.016159601509571075\n",
      "Average test loss: 0.001975157861183915\n",
      "Epoch 129/300\n",
      "Average training loss: 0.016170446395874023\n",
      "Average test loss: 0.0019627964836027886\n",
      "Epoch 130/300\n",
      "Average training loss: 0.016145666087667147\n",
      "Average test loss: 0.001950113760928313\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0161279859079255\n",
      "Average test loss: 0.001984566101183494\n",
      "Epoch 132/300\n",
      "Average training loss: 0.01615069226258331\n",
      "Average test loss: 0.0019548425753083496\n",
      "Epoch 133/300\n",
      "Average training loss: 0.016119668386876583\n",
      "Average test loss: 0.0020177910804955496\n",
      "Epoch 134/300\n",
      "Average training loss: 0.016131356509195435\n",
      "Average test loss: 0.0019634097481353416\n",
      "Epoch 135/300\n",
      "Average training loss: 0.016105798910061517\n",
      "Average test loss: 0.0019826688666103615\n",
      "Epoch 136/300\n",
      "Average training loss: 0.016095344639486735\n",
      "Average test loss: 0.001963717904666232\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01609921212991079\n",
      "Average test loss: 0.001966394412434763\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01610290880004565\n",
      "Average test loss: 0.0019867514436029726\n",
      "Epoch 139/300\n",
      "Average training loss: 0.016093837204906675\n",
      "Average test loss: 0.0019602963402867318\n",
      "Epoch 140/300\n",
      "Average training loss: 0.016079627282089656\n",
      "Average test loss: 0.001991310959060987\n",
      "Epoch 141/300\n",
      "Average training loss: 0.016080215038524734\n",
      "Average test loss: 0.002171938474600514\n",
      "Epoch 142/300\n",
      "Average training loss: 0.01605817113816738\n",
      "Average test loss: 0.002023332796783911\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01607687404503425\n",
      "Average test loss: 0.0020150762719826566\n",
      "Epoch 144/300\n",
      "Average training loss: 0.016050314029885663\n",
      "Average test loss: 0.0019896000437438486\n",
      "Epoch 145/300\n",
      "Average training loss: 0.016050635861025918\n",
      "Average test loss: 0.0020282867944075\n",
      "Epoch 146/300\n",
      "Average training loss: 0.016046796023845674\n",
      "Average test loss: 0.0019889939553621745\n",
      "Epoch 147/300\n",
      "Average training loss: 0.016051815590924688\n",
      "Average test loss: 0.0019548017097016177\n",
      "Epoch 148/300\n",
      "Average training loss: 0.016019809090428883\n",
      "Average test loss: 0.0022658982759134636\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01604012748847405\n",
      "Average test loss: 0.001982707405773302\n",
      "Epoch 150/300\n",
      "Average training loss: 0.016055058288905355\n",
      "Average test loss: 0.001962850763876405\n",
      "Epoch 151/300\n",
      "Average training loss: 0.01601510974019766\n",
      "Average test loss: 0.002023040340385503\n",
      "Epoch 152/300\n",
      "Average training loss: 0.016014467494355308\n",
      "Average test loss: 0.0019937662164577177\n",
      "Epoch 153/300\n",
      "Average training loss: 0.016005808538860744\n",
      "Average test loss: 0.001963943678471777\n",
      "Epoch 154/300\n",
      "Average training loss: 0.016000468340184953\n",
      "Average test loss: 0.0019496817261808448\n",
      "Epoch 155/300\n",
      "Average training loss: 0.015986576426360342\n",
      "Average test loss: 0.001962953224364254\n",
      "Epoch 156/300\n",
      "Average training loss: 0.015986191726393168\n",
      "Average test loss: 0.001979457368970745\n",
      "Epoch 157/300\n",
      "Average training loss: 0.015992522693342632\n",
      "Average test loss: 0.001969850535194079\n",
      "Epoch 158/300\n",
      "Average training loss: 0.015980352575580278\n",
      "Average test loss: 0.001981773354113102\n",
      "Epoch 159/300\n",
      "Average training loss: 0.015984273933702046\n",
      "Average test loss: 0.0019697258416563274\n",
      "Epoch 160/300\n",
      "Average training loss: 0.015975485367079576\n",
      "Average test loss: 0.001992610634201103\n",
      "Epoch 161/300\n",
      "Average training loss: 0.015963368536697494\n",
      "Average test loss: 0.0019587198339609635\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01594975408911705\n",
      "Average test loss: 0.0019526167758223084\n",
      "Epoch 163/300\n",
      "Average training loss: 0.015955334381924736\n",
      "Average test loss: 0.0019703245582059024\n",
      "Epoch 164/300\n",
      "Average training loss: 0.01594361171623071\n",
      "Average test loss: 0.0019469346932859885\n",
      "Epoch 165/300\n",
      "Average training loss: 0.015936822916070622\n",
      "Average test loss: 0.001947901692862312\n",
      "Epoch 166/300\n",
      "Average training loss: 0.015949566968613202\n",
      "Average test loss: 0.001959552444414132\n",
      "Epoch 167/300\n",
      "Average training loss: 0.015918907736738524\n",
      "Average test loss: 0.0019578600513438385\n",
      "Epoch 168/300\n",
      "Average training loss: 0.015941582747631602\n",
      "Average test loss: 0.001987650193687942\n",
      "Epoch 169/300\n",
      "Average training loss: 0.015922678928408356\n",
      "Average test loss: 0.0019615911428506176\n",
      "Epoch 170/300\n",
      "Average training loss: 0.01593159414662255\n",
      "Average test loss: 0.00196678523450262\n",
      "Epoch 171/300\n",
      "Average training loss: 0.015890081671377024\n",
      "Average test loss: 0.0022975077107548715\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01591742092370987\n",
      "Average test loss: 0.001964953969543179\n",
      "Epoch 173/300\n",
      "Average training loss: 0.015900481809344558\n",
      "Average test loss: 0.0019954590920565857\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01589707784106334\n",
      "Average test loss: 0.0019541182816028594\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01590702579004897\n",
      "Average test loss: 0.001964532060962584\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015890936044355233\n",
      "Average test loss: 0.001971161818338765\n",
      "Epoch 177/300\n",
      "Average training loss: 0.01588884870873557\n",
      "Average test loss: 0.0019563466124236585\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015871182542708185\n",
      "Average test loss: 0.0019440230060782699\n",
      "Epoch 179/300\n",
      "Average training loss: 0.0158536013563474\n",
      "Average test loss: 0.0019547427772647805\n",
      "Epoch 180/300\n",
      "Average training loss: 0.015880873647001054\n",
      "Average test loss: 0.001977657960727811\n",
      "Epoch 181/300\n",
      "Average training loss: 0.015869872214065658\n",
      "Average test loss: 0.0019792828071448537\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01587981343269348\n",
      "Average test loss: 0.0019812164233169623\n",
      "Epoch 183/300\n",
      "Average training loss: 0.01584984981848134\n",
      "Average test loss: 0.0019612845230019757\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0158690120652318\n",
      "Average test loss: 0.0019566899442838297\n",
      "Epoch 185/300\n",
      "Average training loss: 0.015844221686323483\n",
      "Average test loss: 0.0019634588998225\n",
      "Epoch 186/300\n",
      "Average training loss: 0.015860480160348946\n",
      "Average test loss: 0.001959086671264635\n",
      "Epoch 187/300\n",
      "Average training loss: 0.015858485826187662\n",
      "Average test loss: 0.0019947901270869705\n",
      "Epoch 188/300\n",
      "Average training loss: 0.015835499093764357\n",
      "Average test loss: 0.002223300157942706\n",
      "Epoch 189/300\n",
      "Average training loss: 0.015835128613644178\n",
      "Average test loss: 0.0025681143800417583\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01583488485051526\n",
      "Average test loss: 0.0019640092400627003\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01583297754658593\n",
      "Average test loss: 0.0019874504497274757\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01582723230537441\n",
      "Average test loss: 0.002023526285464565\n",
      "Epoch 193/300\n",
      "Average training loss: 0.015809228239787948\n",
      "Average test loss: 0.001952527070004079\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01581167606761058\n",
      "Average test loss: 0.001968922305852175\n",
      "Epoch 195/300\n",
      "Average training loss: 0.01587402535478274\n",
      "Average test loss: 0.0019701748504820796\n",
      "Epoch 196/300\n",
      "Average training loss: 0.015796120402713616\n",
      "Average test loss: 0.0019835387180662818\n",
      "Epoch 197/300\n",
      "Average training loss: 0.015820389083690115\n",
      "Average test loss: 0.0019858132512826057\n",
      "Epoch 198/300\n",
      "Average training loss: 0.015792614261309307\n",
      "Average test loss: 0.001961445765569806\n",
      "Epoch 199/300\n",
      "Average training loss: 0.015786400409208402\n",
      "Average test loss: 0.001961860110569331\n",
      "Epoch 200/300\n",
      "Average training loss: 0.015798292715516356\n",
      "Average test loss: 0.0019936660359510115\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01578802670703994\n",
      "Average test loss: 0.0019476075526326895\n",
      "Epoch 202/300\n",
      "Average training loss: 0.01578753703004784\n",
      "Average test loss: 0.0019516725707799196\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0157785835299227\n",
      "Average test loss: 0.0019685926219034527\n",
      "Epoch 204/300\n",
      "Average training loss: 0.015774098717504077\n",
      "Average test loss: 0.0019772505211747356\n",
      "Epoch 205/300\n",
      "Average training loss: 0.015784130765332118\n",
      "Average test loss: 0.001960578029975295\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0157607755313317\n",
      "Average test loss: 0.0019789466556782522\n",
      "Epoch 207/300\n",
      "Average training loss: 0.015764237269759177\n",
      "Average test loss: 0.0019602697022880118\n",
      "Epoch 208/300\n",
      "Average training loss: 0.015755646627810267\n",
      "Average test loss: 0.001958106997422874\n",
      "Epoch 209/300\n",
      "Average training loss: 0.015766846793393293\n",
      "Average test loss: 0.003507271127982272\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01574996883339352\n",
      "Average test loss: 0.0019595581317941348\n",
      "Epoch 211/300\n",
      "Average training loss: 0.015757047053012584\n",
      "Average test loss: 0.001986809778958559\n",
      "Epoch 212/300\n",
      "Average training loss: 0.01574208609925376\n",
      "Average test loss: 0.0020071214313308397\n",
      "Epoch 213/300\n",
      "Average training loss: 0.015737906825211312\n",
      "Average test loss: 0.001997373775475555\n",
      "Epoch 214/300\n",
      "Average training loss: 0.015739542711112234\n",
      "Average test loss: 0.0020431874967697596\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01573168120616012\n",
      "Average test loss: 0.0019626909939365255\n",
      "Epoch 216/300\n",
      "Average training loss: 0.015740187358525063\n",
      "Average test loss: 0.006236542738146252\n",
      "Epoch 217/300\n",
      "Average training loss: 0.015741061146060626\n",
      "Average test loss: 0.0019513740727884902\n",
      "Epoch 218/300\n",
      "Average training loss: 0.015729349349108007\n",
      "Average test loss: 0.0019984768095115822\n",
      "Epoch 219/300\n",
      "Average training loss: 0.015726800660292306\n",
      "Average test loss: 0.0019691264075744484\n",
      "Epoch 220/300\n",
      "Average training loss: 0.015714014609654745\n",
      "Average test loss: 0.0021135441551191937\n",
      "Epoch 221/300\n",
      "Average training loss: 0.01572242345743709\n",
      "Average test loss: 0.0019596468264030087\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01570759076376756\n",
      "Average test loss: 0.0019895380958914756\n",
      "Epoch 223/300\n",
      "Average training loss: 0.015699188345008427\n",
      "Average test loss: 0.001953893288763033\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0157173604319493\n",
      "Average test loss: 0.0019856232816560402\n",
      "Epoch 225/300\n",
      "Average training loss: 0.015694869242608548\n",
      "Average test loss: 0.0019830159931961033\n",
      "Epoch 226/300\n",
      "Average training loss: 0.015696341016226346\n",
      "Average test loss: 0.0019506841426094373\n",
      "Epoch 227/300\n",
      "Average training loss: 0.015701127864420412\n",
      "Average test loss: 0.0020059432859222096\n",
      "Epoch 228/300\n",
      "Average training loss: 0.015692227178977596\n",
      "Average test loss: 0.001962455765240722\n",
      "Epoch 229/300\n",
      "Average training loss: 0.015692325855294863\n",
      "Average test loss: 0.001978175594471395\n",
      "Epoch 230/300\n",
      "Average training loss: 0.01569935165511237\n",
      "Average test loss: 0.001971363293007016\n",
      "Epoch 231/300\n",
      "Average training loss: 0.015674188156922658\n",
      "Average test loss: 0.001964371353387833\n",
      "Epoch 232/300\n",
      "Average training loss: 0.015686178396145504\n",
      "Average test loss: 0.0021158477881302435\n",
      "Epoch 233/300\n",
      "Average training loss: 0.015680709200600784\n",
      "Average test loss: 0.0019732125938559573\n",
      "Epoch 234/300\n",
      "Average training loss: 0.01566551315287749\n",
      "Average test loss: 0.0019562949753469893\n",
      "Epoch 235/300\n",
      "Average training loss: 0.015667472613354525\n",
      "Average test loss: 0.0019556406380401717\n",
      "Epoch 236/300\n",
      "Average training loss: 0.01568001889520221\n",
      "Average test loss: 0.0019795571466286977\n",
      "Epoch 237/300\n",
      "Average training loss: 0.015660857459737196\n",
      "Average test loss: 0.0020051755130084024\n",
      "Epoch 238/300\n",
      "Average training loss: 0.01566277648508549\n",
      "Average test loss: 0.00195288560166955\n",
      "Epoch 239/300\n",
      "Average training loss: 0.015662070375349786\n",
      "Average test loss: 0.001968879108627637\n",
      "Epoch 240/300\n",
      "Average training loss: 0.015665864560339186\n",
      "Average test loss: 0.001948775270125932\n",
      "Epoch 241/300\n",
      "Average training loss: 0.015655671059257456\n",
      "Average test loss: 0.002056513546759056\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01565319641431173\n",
      "Average test loss: 0.00200206204627951\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01564720379975107\n",
      "Average test loss: 0.0019555652946647672\n",
      "Epoch 244/300\n",
      "Average training loss: 0.015648449023564655\n",
      "Average test loss: 0.001948269221207334\n",
      "Epoch 245/300\n",
      "Average training loss: 0.015624543502098983\n",
      "Average test loss: 0.0019623118485841487\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01564523475865523\n",
      "Average test loss: 0.0019780790126985976\n",
      "Epoch 247/300\n",
      "Average training loss: 0.015643153607845305\n",
      "Average test loss: 0.0019898627352797323\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01563911139137215\n",
      "Average test loss: 0.0020537461342497003\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01562557691335678\n",
      "Average test loss: 0.001972501204866502\n",
      "Epoch 250/300\n",
      "Average training loss: 0.015640675474372177\n",
      "Average test loss: 0.001966613877357708\n",
      "Epoch 251/300\n",
      "Average training loss: 0.015625696336229643\n",
      "Average test loss: 0.0019475317479421695\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01562363650235865\n",
      "Average test loss: 0.0019666683254763485\n",
      "Epoch 253/300\n",
      "Average training loss: 0.015625816111763318\n",
      "Average test loss: 0.0020889236024684375\n",
      "Epoch 254/300\n",
      "Average training loss: 0.015613579933842024\n",
      "Average test loss: 0.0020087363730288215\n",
      "Epoch 255/300\n",
      "Average training loss: 0.015615395443307029\n",
      "Average test loss: 0.0019802997774548002\n",
      "Epoch 256/300\n",
      "Average training loss: 0.015619679763085313\n",
      "Average test loss: 0.001974014566383428\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01561404704136981\n",
      "Average test loss: 0.001950595850124955\n",
      "Epoch 258/300\n",
      "Average training loss: 0.015596115224891238\n",
      "Average test loss: 0.0019689240006523\n",
      "Epoch 259/300\n",
      "Average training loss: 0.015605707777871026\n",
      "Average test loss: 0.001977297437791195\n",
      "Epoch 260/300\n",
      "Average training loss: 0.015605224484370815\n",
      "Average test loss: 0.0020035795354180867\n",
      "Epoch 261/300\n",
      "Average training loss: 0.015608661188019646\n",
      "Average test loss: 0.001949676527020832\n",
      "Epoch 262/300\n",
      "Average training loss: 0.015584064837959077\n",
      "Average test loss: 0.0019658468740267887\n",
      "Epoch 263/300\n",
      "Average training loss: 0.015591312249501546\n",
      "Average test loss: 0.001948634264887207\n",
      "Epoch 264/300\n",
      "Average training loss: 0.015618114959862497\n",
      "Average test loss: 0.0019521011741211018\n",
      "Epoch 265/300\n",
      "Average training loss: 0.015594059553411272\n",
      "Average test loss: 0.0019827696358164153\n",
      "Epoch 266/300\n",
      "Average training loss: 0.015572687945432134\n",
      "Average test loss: 0.0019462854175104035\n",
      "Epoch 267/300\n",
      "Average training loss: 0.015591957435011863\n",
      "Average test loss: 0.0019601599112566977\n",
      "Epoch 268/300\n",
      "Average training loss: 0.015580115650263097\n",
      "Average test loss: 0.002072710653560029\n",
      "Epoch 269/300\n",
      "Average training loss: 0.015575623745719592\n",
      "Average test loss: 0.0019730749840123785\n",
      "Epoch 270/300\n",
      "Average training loss: 0.015571626075439982\n",
      "Average test loss: 0.0019574544270419413\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01556948835319943\n",
      "Average test loss: 0.001977347967421843\n",
      "Epoch 272/300\n",
      "Average training loss: 0.015565492598546875\n",
      "Average test loss: 0.0019460564356090294\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01558653987861342\n",
      "Average test loss: 0.0019811461557530693\n",
      "Epoch 274/300\n",
      "Average training loss: 0.015562735832399793\n",
      "Average test loss: 0.0019562296687314906\n",
      "Epoch 275/300\n",
      "Average training loss: 0.015557556845247745\n",
      "Average test loss: 0.001960889314611753\n",
      "Epoch 276/300\n",
      "Average training loss: 0.015552748609748151\n",
      "Average test loss: 0.001959490372488896\n",
      "Epoch 277/300\n",
      "Average training loss: 0.015552783921360969\n",
      "Average test loss: 0.001964815895590517\n",
      "Epoch 278/300\n",
      "Average training loss: 0.01555383179254002\n",
      "Average test loss: 0.0019646882526576518\n",
      "Epoch 279/300\n",
      "Average training loss: 0.015559445993767844\n",
      "Average test loss: 0.0021861741944319674\n",
      "Epoch 280/300\n",
      "Average training loss: 0.015554354845649666\n",
      "Average test loss: 0.001953079070068068\n",
      "Epoch 281/300\n",
      "Average training loss: 0.015534454726510577\n",
      "Average test loss: 0.0019702360370299882\n",
      "Epoch 282/300\n",
      "Average training loss: 0.015548428382310603\n",
      "Average test loss: 0.002017877449488474\n",
      "Epoch 283/300\n",
      "Average training loss: 0.015557506425513162\n",
      "Average test loss: 0.0019631861868417925\n",
      "Epoch 284/300\n",
      "Average training loss: 0.015546530920598243\n",
      "Average test loss: 0.001974591736578279\n",
      "Epoch 285/300\n",
      "Average training loss: 0.015533765312698152\n",
      "Average test loss: 0.0019663153553588523\n",
      "Epoch 286/300\n",
      "Average training loss: 0.015539418627818425\n",
      "Average test loss: 0.0019465267717217406\n",
      "Epoch 287/300\n",
      "Average training loss: 0.015545705565975771\n",
      "Average test loss: 0.00197808883463343\n",
      "Epoch 288/300\n",
      "Average training loss: 0.015532225556671619\n",
      "Average test loss: 0.001980587106301553\n",
      "Epoch 289/300\n",
      "Average training loss: 0.015523079852263132\n",
      "Average test loss: 0.00198240533305539\n",
      "Epoch 290/300\n",
      "Average training loss: 0.015529563229117128\n",
      "Average test loss: 0.001977277670469549\n",
      "Epoch 291/300\n",
      "Average training loss: 0.01552640382366048\n",
      "Average test loss: 0.0019838191281176276\n",
      "Epoch 292/300\n",
      "Average training loss: 0.015524719867441389\n",
      "Average test loss: 0.001995428333369394\n",
      "Epoch 293/300\n",
      "Average training loss: 0.01552497737771935\n",
      "Average test loss: 0.0019922898449003697\n",
      "Epoch 294/300\n",
      "Average training loss: 0.015517853153248627\n",
      "Average test loss: 0.002044809890910983\n",
      "Epoch 295/300\n",
      "Average training loss: 0.015514954581856727\n",
      "Average test loss: 0.0019632139859928027\n",
      "Epoch 296/300\n",
      "Average training loss: 0.015518202937311596\n",
      "Average test loss: 0.0019646485418909126\n",
      "Epoch 297/300\n",
      "Average training loss: 0.015515447821882036\n",
      "Average test loss: 0.0019595399239203995\n",
      "Epoch 298/300\n",
      "Average training loss: 0.015508223856488864\n",
      "Average test loss: 0.0019623984705863726\n",
      "Epoch 299/300\n",
      "Average training loss: 0.015505714185535908\n",
      "Average test loss: 0.002003639776673582\n",
      "Epoch 300/300\n",
      "Average training loss: 0.015507568746805192\n",
      "Average test loss: 0.001964492604860829\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth3-.025/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.99\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.72\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.19\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 25.44\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.31\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.69\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.79\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 27.35\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.66\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.49\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.48\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.95\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.14\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.34\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.59\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.24\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2.3669319887426163\n",
      "Average test loss: 0.01271583796126975\n",
      "Epoch 2/300\n",
      "Average training loss: 0.317440862748358\n",
      "Average test loss: 0.009962028327915404\n",
      "Epoch 3/300\n",
      "Average training loss: 0.23561574668354457\n",
      "Average test loss: 0.010617465525865556\n",
      "Epoch 4/300\n",
      "Average training loss: 0.20240697773297628\n",
      "Average test loss: 0.008880946823292308\n",
      "Epoch 5/300\n",
      "Average training loss: 0.18375196958912743\n",
      "Average test loss: 0.008417618570228418\n",
      "Epoch 6/300\n",
      "Average training loss: 0.17120032694604662\n",
      "Average test loss: 0.008922439727518293\n",
      "Epoch 7/300\n",
      "Average training loss: 0.16213086846139696\n",
      "Average test loss: 0.008786427955660555\n",
      "Epoch 8/300\n",
      "Average training loss: 0.1555699796213044\n",
      "Average test loss: 0.009084398147960504\n",
      "Epoch 9/300\n",
      "Average training loss: 0.1491713978184594\n",
      "Average test loss: 0.007926046117312378\n",
      "Epoch 10/300\n",
      "Average training loss: 0.14411517930030823\n",
      "Average test loss: 0.007566487178620365\n",
      "Epoch 11/300\n",
      "Average training loss: 0.1403030964003669\n",
      "Average test loss: 0.007316635853714413\n",
      "Epoch 12/300\n",
      "Average training loss: 0.13656599982579548\n",
      "Average test loss: 0.0072219382756286195\n",
      "Epoch 13/300\n",
      "Average training loss: 0.13333638121022118\n",
      "Average test loss: 0.007237204548385408\n",
      "Epoch 14/300\n",
      "Average training loss: 0.12995447817775937\n",
      "Average test loss: 0.007415456705623203\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1274045706457562\n",
      "Average test loss: 0.007790898567272557\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1253641192118327\n",
      "Average test loss: 0.0068620031339426835\n",
      "Epoch 17/300\n",
      "Average training loss: 0.12282210328181585\n",
      "Average test loss: 0.0068264343386722935\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1207239679892858\n",
      "Average test loss: 0.007086613765193356\n",
      "Epoch 19/300\n",
      "Average training loss: 0.11885551757282681\n",
      "Average test loss: 0.006705888340870539\n",
      "Epoch 20/300\n",
      "Average training loss: 0.11727593942483266\n",
      "Average test loss: 0.00669182523422771\n",
      "Epoch 21/300\n",
      "Average training loss: 0.11565691892968284\n",
      "Average test loss: 0.006531786654558447\n",
      "Epoch 22/300\n",
      "Average training loss: 0.11433312458462186\n",
      "Average test loss: 0.006412722681131628\n",
      "Epoch 23/300\n",
      "Average training loss: 0.11292706149154239\n",
      "Average test loss: 0.006413242389758428\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11168130342827903\n",
      "Average test loss: 0.006555273676498069\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11083892564641105\n",
      "Average test loss: 0.006516880920363797\n",
      "Epoch 26/300\n",
      "Average training loss: 0.10952554129891925\n",
      "Average test loss: 0.00617528425819344\n",
      "Epoch 27/300\n",
      "Average training loss: 0.1087748604880439\n",
      "Average test loss: 0.00617494660947058\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10796898550457425\n",
      "Average test loss: 0.006197528292321497\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10672224985890919\n",
      "Average test loss: 0.0064657165449526575\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10611669439077377\n",
      "Average test loss: 0.006106249396999677\n",
      "Epoch 31/300\n",
      "Average training loss: 0.10524121032158534\n",
      "Average test loss: 0.006215720812893576\n",
      "Epoch 32/300\n",
      "Average training loss: 0.10461158045795228\n",
      "Average test loss: 0.0059968204241659905\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1042696285645167\n",
      "Average test loss: 0.005990536808553669\n",
      "Epoch 34/300\n",
      "Average training loss: 0.10332911648352941\n",
      "Average test loss: 0.055611106316248576\n",
      "Epoch 35/300\n",
      "Average training loss: 0.10280128362443712\n",
      "Average test loss: 0.005946155125896136\n",
      "Epoch 36/300\n",
      "Average training loss: 0.10246453046136432\n",
      "Average test loss: 0.00604719396142496\n",
      "Epoch 37/300\n",
      "Average training loss: 0.10175970939132903\n",
      "Average test loss: 0.006207144346088171\n",
      "Epoch 38/300\n",
      "Average training loss: 0.10127854352527195\n",
      "Average test loss: 0.00593278161312143\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1007784915500217\n",
      "Average test loss: 0.005863723083916638\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10079960727029376\n",
      "Average test loss: 0.005875007716317972\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10015565486086739\n",
      "Average test loss: 0.006050574441336923\n",
      "Epoch 42/300\n",
      "Average training loss: 0.09973659452464845\n",
      "Average test loss: 0.0063045688014891415\n",
      "Epoch 43/300\n",
      "Average training loss: 0.09939486717515522\n",
      "Average test loss: 0.008130845040082931\n",
      "Epoch 44/300\n",
      "Average training loss: 0.09788030445575714\n",
      "Average test loss: 0.005880946500019895\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09769734161429935\n",
      "Average test loss: 0.00583603669454654\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09729869016011557\n",
      "Average test loss: 0.005778774564050966\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09698590592543284\n",
      "Average test loss: 0.007318380907177925\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09692516466312938\n",
      "Average test loss: 0.005937450145681699\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09699941218561596\n",
      "Average test loss: 0.005908352143648598\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09609903623660405\n",
      "Average test loss: 0.0057572711581985155\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09704880598518584\n",
      "Average test loss: 0.0060591683983802795\n",
      "Epoch 55/300\n",
      "Average training loss: 0.09581985750463273\n",
      "Average test loss: 0.005710298714952336\n",
      "Epoch 56/300\n",
      "Average training loss: 0.09566681077745226\n",
      "Average test loss: 0.005796067376931509\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0955456376671791\n",
      "Average test loss: 0.0057408421060277355\n",
      "Epoch 58/300\n",
      "Average training loss: 0.09533830751313103\n",
      "Average test loss: 0.005741140940537055\n",
      "Epoch 59/300\n",
      "Average training loss: 0.09492586540513569\n",
      "Average test loss: 0.005995669327262375\n",
      "Epoch 60/300\n",
      "Average training loss: 0.09551536291175418\n",
      "Average test loss: 0.005958046097722319\n",
      "Epoch 61/300\n",
      "Average training loss: 0.09450011340777079\n",
      "Average test loss: 0.005706464221080144\n",
      "Epoch 62/300\n",
      "Average training loss: 0.09429304295778275\n",
      "Average test loss: 0.005722403961751196\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09467680920494927\n",
      "Average test loss: 0.006351542350318697\n",
      "Epoch 64/300\n",
      "Average training loss: 0.09412107881572511\n",
      "Average test loss: 0.00573437103918857\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09400236396988233\n",
      "Average test loss: 0.1458226639131705\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09488347961505254\n",
      "Average test loss: 0.005993735848201646\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09371730746825536\n",
      "Average test loss: 0.005643715787265036\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09337431222862667\n",
      "Average test loss: 0.005848416371064054\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09314762804905574\n",
      "Average test loss: 0.005701773217154873\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09330147451824612\n",
      "Average test loss: 0.005785605295664734\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09282064372963375\n",
      "Average test loss: 0.005772209232466088\n",
      "Epoch 72/300\n",
      "Average training loss: 0.09331290595399008\n",
      "Average test loss: 0.006039906126757462\n",
      "Epoch 73/300\n",
      "Average training loss: 0.09219588264491824\n",
      "Average test loss: 0.016585129621956084\n",
      "Epoch 78/300\n",
      "Average training loss: 0.09211942574712965\n",
      "Average test loss: 0.005874151287393437\n",
      "Epoch 79/300\n",
      "Average training loss: 5342.781022494859\n",
      "Average test loss: 0.09835699364874098\n",
      "Epoch 80/300\n",
      "Average training loss: 15.931147338867188\n",
      "Average test loss: 0.1035664310918914\n",
      "Epoch 81/300\n",
      "Average training loss: 14.681979132758247\n",
      "Average test loss: 0.06231829716099633\n",
      "Epoch 82/300\n",
      "Average training loss: 13.779294503953722\n",
      "Average test loss: 0.062365749796231586\n",
      "Epoch 83/300\n",
      "Average training loss: 12.974273700290256\n",
      "Average test loss: 0.029505051508545876\n",
      "Epoch 84/300\n",
      "Average training loss: 12.168570103963216\n",
      "Average test loss: 0.049094910181230966\n",
      "Epoch 85/300\n",
      "Average training loss: 11.41236841583252\n",
      "Average test loss: 0.04211753354138798\n",
      "Epoch 86/300\n",
      "Average training loss: 10.742529165479873\n",
      "Average test loss: 0.019681867942214014\n",
      "Epoch 87/300\n",
      "Average training loss: 10.022264982435438\n",
      "Average test loss: 0.019814465680056147\n",
      "Epoch 88/300\n",
      "Average training loss: 9.29541210301717\n",
      "Average test loss: 0.019027696660823292\n",
      "Epoch 89/300\n",
      "Average training loss: 8.721026362948948\n",
      "Average test loss: 0.016610034816794926\n",
      "Epoch 90/300\n",
      "Average training loss: 8.264116364796957\n",
      "Average test loss: 0.014649828372730149\n",
      "Epoch 91/300\n",
      "Average training loss: 7.8395180397033695\n",
      "Average test loss: 0.013463169005182055\n",
      "Epoch 92/300\n",
      "Average training loss: 7.427435148027208\n",
      "Average test loss: 0.013433042840825186\n",
      "Epoch 93/300\n",
      "Average training loss: 7.011958441416422\n",
      "Average test loss: 0.011486405344472992\n",
      "Epoch 94/300\n",
      "Average training loss: 6.5593210881551105\n",
      "Average test loss: 0.012997220348152849\n",
      "Epoch 95/300\n",
      "Average training loss: 6.153753925747342\n",
      "Average test loss: 0.011669048240615262\n",
      "Epoch 96/300\n",
      "Average training loss: 5.697211209615071\n",
      "Average test loss: 0.009680272534075711\n",
      "Epoch 97/300\n",
      "Average training loss: 5.21402670372857\n",
      "Average test loss: 0.009728211514651775\n",
      "Epoch 98/300\n",
      "Average training loss: 4.766946755727132\n",
      "Average test loss: 0.01726630297427376\n",
      "Epoch 99/300\n",
      "Average training loss: 4.309009534200032\n",
      "Average test loss: 0.008360892055763139\n",
      "Epoch 100/300\n",
      "Average training loss: 3.88584614944458\n",
      "Average test loss: 0.008178244672715664\n",
      "Epoch 101/300\n",
      "Average training loss: 3.5041881347232393\n",
      "Average test loss: 0.008077664666705661\n",
      "Epoch 102/300\n",
      "Average training loss: 3.139963079240587\n",
      "Average test loss: 0.00783424298879173\n",
      "Epoch 103/300\n",
      "Average training loss: 2.778975544611613\n",
      "Average test loss: 0.007693981513381004\n",
      "Epoch 104/300\n",
      "Average training loss: 2.4141628290812176\n",
      "Average test loss: 0.008257981746560998\n",
      "Epoch 105/300\n",
      "Average training loss: 2.040657141049703\n",
      "Average test loss: 0.007541403460419841\n",
      "Epoch 106/300\n",
      "Average training loss: 1.659261307398478\n",
      "Average test loss: 0.007372472978300518\n",
      "Epoch 107/300\n",
      "Average training loss: 1.2889551270802815\n",
      "Average test loss: 0.007200750552117824\n",
      "Epoch 108/300\n",
      "Average training loss: 0.9309483054478963\n",
      "Average test loss: 0.006775720941523711\n",
      "Epoch 109/300\n",
      "Average training loss: 0.6414361669222514\n",
      "Average test loss: 0.006648008175608185\n",
      "Epoch 110/300\n",
      "Average training loss: 0.46453806445333695\n",
      "Average test loss: 0.006501434755408102\n",
      "Epoch 111/300\n",
      "Average training loss: 0.3441934843328264\n",
      "Average test loss: 0.006510267599589295\n",
      "Epoch 112/300\n",
      "Average training loss: 0.2608485178814994\n",
      "Average test loss: 0.006237065215905507\n",
      "Epoch 113/300\n",
      "Average training loss: 0.21023621870411766\n",
      "Average test loss: 0.006163999391926659\n",
      "Epoch 114/300\n",
      "Average training loss: 0.18119659950998093\n",
      "Average test loss: 0.006205038141045305\n",
      "Epoch 115/300\n",
      "Average training loss: 0.16348938768439822\n",
      "Average test loss: 0.0060239101284080086\n",
      "Epoch 116/300\n",
      "Average training loss: 0.15158692297670576\n",
      "Average test loss: 0.006173513004763259\n",
      "Epoch 117/300\n",
      "Average training loss: 0.14172322283188502\n",
      "Average test loss: 0.005971855392886533\n",
      "Epoch 118/300\n",
      "Average training loss: 0.13402535760402678\n",
      "Average test loss: 0.0059586956832144\n",
      "Epoch 119/300\n",
      "Average training loss: 0.12822995524273978\n",
      "Average test loss: 0.006237115137279033\n",
      "Epoch 120/300\n",
      "Average training loss: 0.1234209447171953\n",
      "Average test loss: 0.005906790326452918\n",
      "Epoch 121/300\n",
      "Average training loss: 0.11933304553561741\n",
      "Average test loss: 0.0058459608840445675\n",
      "Epoch 122/300\n",
      "Average training loss: 0.11554664038287268\n",
      "Average test loss: 0.005925171736213896\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10515196644597584\n",
      "Average test loss: 0.005783942854238881\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10295014247629378\n",
      "Average test loss: 0.005717632931139734\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10113500545422237\n",
      "Average test loss: 0.005738722912553284\n",
      "Epoch 129/300\n",
      "Average training loss: 0.0999087052543958\n",
      "Average test loss: 0.0065669175270530915\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09824094147152371\n",
      "Average test loss: 0.005978372573024697\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09728155824873183\n",
      "Average test loss: 0.005740944231549899\n",
      "Epoch 132/300\n",
      "Average training loss: 0.09621585469113456\n",
      "Average test loss: 0.005917984887543652\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09582460173633363\n",
      "Average test loss: 0.00570442977692518\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09444355471266641\n",
      "Average test loss: 0.005668979639394416\n",
      "Epoch 135/300\n",
      "Average training loss: 0.09385346284839842\n",
      "Average test loss: 0.005662905053959952\n",
      "Epoch 136/300\n",
      "Average training loss: 0.09367131920655568\n",
      "Average test loss: 0.005649385095470481\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09300786959462695\n",
      "Average test loss: 0.005773972723633051\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09358748294247521\n",
      "Average test loss: 0.005696658456077178\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09225479323334164\n",
      "Average test loss: 0.005644060100946161\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09221430257956187\n",
      "Average test loss: 0.0057162608992722295\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09217303988668654\n",
      "Average test loss: 0.005751290399995115\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09200508754120933\n",
      "Average test loss: 0.005741347014904022\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09190876589881049\n",
      "Average test loss: 0.005882565757466687\n",
      "Epoch 144/300\n",
      "Average training loss: 0.0918710979355706\n",
      "Average test loss: 0.0057126142788264484\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09169163032372793\n",
      "Average test loss: 0.005657760334097677\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09149836560752657\n",
      "Average test loss: 0.005826975413080719\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10126387496789296\n",
      "Average test loss: 0.00565224184965094\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09095225491788653\n",
      "Average test loss: 0.005576065515892373\n",
      "Epoch 151/300\n",
      "Average training loss: 0.09084543356630537\n",
      "Average test loss: 0.005652071824090349\n",
      "Epoch 152/300\n",
      "Average training loss: 28.203917658050855\n",
      "Average test loss: 0.015960963310466873\n",
      "Epoch 153/300\n",
      "Average training loss: 4.724363620758057\n",
      "Average test loss: 0.015289594060844845\n",
      "Epoch 154/300\n",
      "Average training loss: 3.8722564720577664\n",
      "Average test loss: 0.010553009081217977\n",
      "Epoch 155/300\n",
      "Average training loss: 3.2821516386667886\n",
      "Average test loss: 0.009858026945756541\n",
      "Epoch 156/300\n",
      "Average training loss: 2.875190043343438\n",
      "Average test loss: 0.010623472198016114\n",
      "Epoch 157/300\n",
      "Average training loss: 2.52748051071167\n",
      "Average test loss: 0.008838085667126709\n",
      "Epoch 158/300\n",
      "Average training loss: 2.228013546731737\n",
      "Average test loss: 0.008651765036914083\n",
      "Epoch 159/300\n",
      "Average training loss: 1.959097178141276\n",
      "Average test loss: 0.00794747076349126\n",
      "Epoch 160/300\n",
      "Average training loss: 1.6895073738098145\n",
      "Average test loss: 0.007813245835817523\n",
      "Epoch 161/300\n",
      "Average training loss: 1.4179334965811836\n",
      "Average test loss: 0.007444353811442852\n",
      "Epoch 162/300\n",
      "Average training loss: 1.1805409659279718\n",
      "Average test loss: 0.00731434962608748\n",
      "Epoch 163/300\n",
      "Average training loss: 0.9907917338477241\n",
      "Average test loss: 0.007104247735606299\n",
      "Epoch 164/300\n",
      "Average training loss: 0.830045610692766\n",
      "Average test loss: 0.007164374272028605\n",
      "Epoch 165/300\n",
      "Average training loss: 0.6925624198383755\n",
      "Average test loss: 0.006919122457918194\n",
      "Epoch 166/300\n",
      "Average training loss: 0.5754914280573526\n",
      "Average test loss: 0.006696921399070157\n",
      "Epoch 167/300\n",
      "Average training loss: 0.4743303056822883\n",
      "Average test loss: 0.0072785688005387785\n",
      "Epoch 168/300\n",
      "Average training loss: 0.38888848500781587\n",
      "Average test loss: 0.008009552663399113\n",
      "Epoch 169/300\n",
      "Average training loss: 0.31868663011656867\n",
      "Average test loss: 0.006421485176516904\n",
      "Epoch 170/300\n",
      "Average training loss: 0.26236716124746534\n",
      "Average test loss: 0.006296792418385546\n",
      "Epoch 171/300\n",
      "Average training loss: 0.2177229728433821\n",
      "Average test loss: 0.006437989802410205\n",
      "Epoch 172/300\n",
      "Average training loss: 0.18380031478404998\n",
      "Average test loss: 0.00604557453137305\n",
      "Epoch 173/300\n",
      "Average training loss: 0.15964038529660968\n",
      "Average test loss: 0.006082531748960416\n",
      "Epoch 174/300\n",
      "Average training loss: 0.14277673518657685\n",
      "Average test loss: 0.006097286664777332\n",
      "Epoch 175/300\n",
      "Average training loss: 0.13250056521760092\n",
      "Average test loss: 0.005860323643104897\n",
      "Epoch 176/300\n",
      "Average training loss: 0.12578848008977042\n",
      "Average test loss: 0.005965915937390593\n",
      "Epoch 177/300\n",
      "Average training loss: 0.12031921810574002\n",
      "Average test loss: 0.005786378764030006\n",
      "Epoch 178/300\n",
      "Average training loss: 0.11625089844067892\n",
      "Average test loss: 0.005741060179968675\n",
      "Epoch 179/300\n",
      "Average training loss: 0.11263335288895501\n",
      "Average test loss: 0.005880590736865998\n",
      "Epoch 180/300\n",
      "Average training loss: 0.1030509979724884\n",
      "Average test loss: 0.005656272203971942\n",
      "Epoch 184/300\n",
      "Average training loss: 0.10124271170960532\n",
      "Average test loss: 0.005724287243766917\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09985932928323746\n",
      "Average test loss: 0.005837229118165043\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09827976275814904\n",
      "Average test loss: 0.0056404408837358155\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09713987776968214\n",
      "Average test loss: 0.005722004555579689\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09605109700229433\n",
      "Average test loss: 0.005643976425545083\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09496906026866701\n",
      "Average test loss: 0.005673783871034781\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09396065847078959\n",
      "Average test loss: 0.005618726349539227\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09365497111611897\n",
      "Average test loss: 0.0057498478413456015\n",
      "Epoch 192/300\n",
      "Average training loss: 0.09264950237671535\n",
      "Average test loss: 0.005652839734115535\n",
      "Epoch 193/300\n",
      "Average training loss: 0.09250268340110779\n",
      "Average test loss: 0.00593806278001931\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09186848965618345\n",
      "Average test loss: 0.006686376322474745\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09269518483347362\n",
      "Average test loss: 0.005627083080096377\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09226626758111847\n",
      "Average test loss: 0.005618190757102437\n",
      "Epoch 197/300\n",
      "Average training loss: 0.0912889628012975\n",
      "Average test loss: 0.005709789483911461\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09118500838014815\n",
      "Average test loss: 0.005910873930487368\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09128750152720345\n",
      "Average test loss: 0.005641072491804759\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09058524964915382\n",
      "Average test loss: 0.005663216655039125\n",
      "Epoch 201/300\n",
      "Average training loss: 0.0907374010350969\n",
      "Average test loss: 0.005762740636865298\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09059606399801043\n",
      "Average test loss: 0.0056413758438494475\n",
      "Epoch 203/300\n",
      "Average training loss: 0.09057636561658647\n",
      "Average test loss: 0.005652911394420597\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09029671090841293\n",
      "Average test loss: 0.006353308942996794\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09115307403273053\n",
      "Average test loss: 0.005633060655246178\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09013590859042274\n",
      "Average test loss: 0.0058330732795099415\n",
      "Epoch 207/300\n",
      "Average training loss: 0.09007239490085178\n",
      "Average test loss: 0.005716420304858022\n",
      "Epoch 208/300\n",
      "Average training loss: 0.08999622775448693\n",
      "Average test loss: 0.005622735731717613\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09048566833469603\n",
      "Average test loss: 0.0056773212837676204\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08970160971747504\n",
      "Average test loss: 0.00561806995049119\n",
      "Epoch 211/300\n",
      "Average training loss: 0.08947350647714403\n",
      "Average test loss: 0.0056615578985462585\n",
      "Epoch 212/300\n",
      "Average training loss: 0.08991287691725625\n",
      "Average test loss: 0.005709892078406281\n",
      "Epoch 213/300\n",
      "Average training loss: 0.08933317690425449\n",
      "Average test loss: 0.005634921617392037\n",
      "Epoch 214/300\n",
      "Average training loss: 0.08946219521760941\n",
      "Average test loss: 0.005621458332571719\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09443410233656566\n",
      "Average test loss: 0.0056949124597012995\n",
      "Epoch 216/300\n",
      "Average training loss: 9466.81114376566\n",
      "Average test loss: 0.2333222597440084\n",
      "Epoch 217/300\n",
      "Average training loss: 12.208102312723796\n",
      "Average test loss: 0.1252148269679811\n",
      "Epoch 218/300\n",
      "Average training loss: 10.611471372816299\n",
      "Average test loss: 0.07130688335498174\n",
      "Epoch 219/300\n",
      "Average training loss: 9.47910974714491\n",
      "Average test loss: 0.06784574449724622\n",
      "Epoch 220/300\n",
      "Average training loss: 8.558829178704157\n",
      "Average test loss: 0.03080008869038688\n",
      "Epoch 221/300\n",
      "Average training loss: 7.719583067576091\n",
      "Average test loss: 0.02566862688627508\n",
      "Epoch 222/300\n",
      "Average training loss: 6.9696460778978135\n",
      "Average test loss: 0.02003181437154611\n",
      "Epoch 223/300\n",
      "Average training loss: 6.302421064588759\n",
      "Average test loss: 0.01744158298108313\n",
      "Epoch 224/300\n",
      "Average training loss: 5.783939112345378\n",
      "Average test loss: 0.013650713216099473\n",
      "Epoch 225/300\n",
      "Average training loss: 5.373369294060601\n",
      "Average test loss: 0.01203183628949854\n",
      "Epoch 226/300\n",
      "Average training loss: 5.0519035152859155\n",
      "Average test loss: 0.011460535013841259\n",
      "Epoch 227/300\n",
      "Average training loss: 4.766882953643798\n",
      "Average test loss: 0.011487656207548248\n",
      "Epoch 228/300\n",
      "Average training loss: 4.509600975884331\n",
      "Average test loss: 0.012187021224035158\n",
      "Epoch 229/300\n",
      "Average training loss: 4.266425008985731\n",
      "Average test loss: 0.010047038518720203\n",
      "Epoch 230/300\n",
      "Average training loss: 4.022414586173164\n",
      "Average test loss: 0.009365062002506521\n",
      "Epoch 231/300\n",
      "Average training loss: 3.7599671713511147\n",
      "Average test loss: 0.008849284108314249\n",
      "Epoch 232/300\n",
      "Average training loss: 3.5071553800370956\n",
      "Average test loss: 0.008787982436103953\n",
      "Epoch 233/300\n",
      "Average training loss: 3.254142592112223\n",
      "Average test loss: 0.00809685895136661\n",
      "Epoch 234/300\n",
      "Average training loss: 2.9928311032189265\n",
      "Average test loss: 0.00799914759149154\n",
      "Epoch 235/300\n",
      "Average training loss: 2.7189778440263535\n",
      "Average test loss: 0.007546706482768058\n",
      "Epoch 236/300\n",
      "Average training loss: 2.418834568447537\n",
      "Average test loss: 0.007628309022635222\n",
      "Epoch 237/300\n",
      "Average training loss: 2.110371738857693\n",
      "Average test loss: 0.007454184214274089\n",
      "Epoch 238/300\n",
      "Average training loss: 1.8455431759092542\n",
      "Average test loss: 0.0072446344643831255\n",
      "Epoch 239/300\n",
      "Average training loss: 1.6294881121317546\n",
      "Average test loss: 0.00698339366250568\n",
      "Epoch 240/300\n",
      "Average training loss: 1.4337177622053359\n",
      "Average test loss: 0.0068310392726626664\n",
      "Epoch 241/300\n",
      "Average training loss: 1.2450181474685669\n",
      "Average test loss: 0.007040909831308656\n",
      "Epoch 242/300\n",
      "Average training loss: 1.0689693019655016\n",
      "Average test loss: 0.006707576521154907\n",
      "Epoch 243/300\n",
      "Average training loss: 0.9078133664131165\n",
      "Average test loss: 0.006719569262117147\n",
      "Epoch 244/300\n",
      "Average training loss: 0.7499422912067837\n",
      "Average test loss: 0.006472348741359181\n",
      "Epoch 245/300\n",
      "Average training loss: 0.597485452969869\n",
      "Average test loss: 0.006376803079413043\n",
      "Epoch 246/300\n",
      "Average training loss: 0.4693158045079973\n",
      "Average test loss: 0.006409911397844553\n",
      "Epoch 247/300\n",
      "Average training loss: 0.372177907731798\n",
      "Average test loss: 0.0062471424457099705\n",
      "Epoch 248/300\n",
      "Average training loss: 0.30221060628361174\n",
      "Average test loss: 0.00608775765572985\n",
      "Epoch 249/300\n",
      "Average training loss: 0.2567790994246801\n",
      "Average test loss: 0.006199083840267526\n",
      "Epoch 250/300\n",
      "Average training loss: 0.2219768491850959\n",
      "Average test loss: 0.006001241830488046\n",
      "Epoch 251/300\n",
      "Average training loss: 0.19613114122549694\n",
      "Average test loss: 0.0060564397039512795\n",
      "Epoch 252/300\n",
      "Average training loss: 0.1753412712679969\n",
      "Average test loss: 0.005979981428219212\n",
      "Epoch 253/300\n",
      "Average training loss: 0.1593772906594806\n",
      "Average test loss: 0.005887219402939081\n",
      "Epoch 254/300\n",
      "Average training loss: 0.1460866365035375\n",
      "Average test loss: 0.008420940933956041\n",
      "Epoch 255/300\n",
      "Average training loss: 0.13562323525216843\n",
      "Average test loss: 0.0058624068602091736\n",
      "Epoch 256/300\n",
      "Average training loss: 0.12774307447009617\n",
      "Average test loss: 0.005718659162935283\n",
      "Epoch 257/300\n",
      "Average training loss: 0.12197463455465105\n",
      "Average test loss: 0.005703838936156697\n",
      "Epoch 258/300\n",
      "Average training loss: 0.11805065576235453\n",
      "Average test loss: 0.005783919073227379\n",
      "Epoch 259/300\n",
      "Average training loss: 0.11409585284524494\n",
      "Average test loss: 0.007206210180289215\n",
      "Epoch 260/300\n",
      "Average training loss: 0.11149956304497188\n",
      "Average test loss: 0.00636481539077229\n",
      "Epoch 261/300\n",
      "Average training loss: 0.10883071780867047\n",
      "Average test loss: 0.0056906030966589844\n",
      "Epoch 262/300\n",
      "Average training loss: 0.1064329434633255\n",
      "Average test loss: 0.005698840082105663\n",
      "Epoch 263/300\n",
      "Average training loss: 0.10452839579847124\n",
      "Average test loss: 0.005888261576493581\n",
      "Epoch 264/300\n",
      "Average training loss: 0.10276751885149214\n",
      "Average test loss: 0.005746396873974138\n",
      "Epoch 265/300\n",
      "Average training loss: 0.10108719403213925\n",
      "Average test loss: 0.005689739126298163\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09942782977554533\n",
      "Average test loss: 0.005656830013626151\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09833918931749132\n",
      "Average test loss: 0.005914504890640577\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09715501777993309\n",
      "Average test loss: 0.006415669531871875\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09741622088352839\n",
      "Average test loss: 0.006647048753168848\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09491105247868432\n",
      "Average test loss: 0.005709897307472097\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09376137261920506\n",
      "Average test loss: 0.0055804459340870385\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09302925004561742\n",
      "Average test loss: 0.005843100231554773\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09224459536208046\n",
      "Average test loss: 0.008795308941768276\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09200200053056082\n",
      "Average test loss: 0.006784019901934597\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09239856308698655\n",
      "Average test loss: 0.005851059007561869\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09072874055968391\n",
      "Average test loss: 0.00567922522375981\n",
      "Epoch 277/300\n",
      "Average training loss: 0.09031070638365216\n",
      "Average test loss: 0.006153938087738222\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09018126500977411\n",
      "Average test loss: 0.00561505797257026\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09025214093923568\n",
      "Average test loss: 0.005791819791413016\n",
      "Epoch 280/300\n",
      "Average training loss: 846.6891117950082\n",
      "Average test loss: 0.0831668247712983\n",
      "Epoch 281/300\n",
      "Average training loss: 5.45383984375\n",
      "Average test loss: 1.7406741844415665\n",
      "Epoch 282/300\n",
      "Average training loss: 4.618735295189752\n",
      "Average test loss: 0.017366740433706177\n",
      "Epoch 283/300\n",
      "Average training loss: 4.201479639689127\n",
      "Average test loss: 0.015884617653157976\n",
      "Epoch 284/300\n",
      "Average training loss: 3.9039167300330266\n",
      "Average test loss: 0.0917884221441216\n",
      "Epoch 285/300\n",
      "Average training loss: 3.648206250084771\n",
      "Average test loss: 0.012764371807376544\n",
      "Epoch 286/300\n",
      "Average training loss: 3.4087031762864854\n",
      "Average test loss: 0.03320104570190112\n",
      "Epoch 287/300\n",
      "Average training loss: 3.1667818075815837\n",
      "Average test loss: 0.022390123748944866\n",
      "Epoch 288/300\n",
      "Average training loss: 2.918811513688829\n",
      "Average test loss: 2.357600720151431\n",
      "Epoch 289/300\n",
      "Average training loss: 2.670622770945231\n",
      "Average test loss: 0.3506613896720939\n",
      "Epoch 290/300\n",
      "Average training loss: 2.430395318137275\n",
      "Average test loss: 0.009894503270586332\n",
      "Epoch 291/300\n",
      "Average training loss: 2.201267737494575\n",
      "Average test loss: 0.015317139339943726\n",
      "Epoch 292/300\n",
      "Average training loss: 1.9810570759243435\n",
      "Average test loss: 0.007771238171392017\n",
      "Epoch 293/300\n",
      "Average training loss: 1.7672035273445976\n",
      "Average test loss: 0.007364350971662336\n",
      "Epoch 294/300\n",
      "Average training loss: 1.5543371614880033\n",
      "Average test loss: 0.008441439233720302\n",
      "Epoch 295/300\n",
      "Average training loss: 1.384873429722256\n",
      "Average test loss: 0.006910911915202936\n",
      "Epoch 296/300\n",
      "Average training loss: 1.236072793536716\n",
      "Average test loss: 0.006759328736613194\n",
      "Epoch 297/300\n",
      "Average training loss: 1.09861191950904\n",
      "Average test loss: 0.006490710345614287\n",
      "Epoch 298/300\n",
      "Average training loss: 0.969316044277615\n",
      "Average test loss: 0.006535549423760838\n",
      "Epoch 299/300\n",
      "Average training loss: 0.8476306833691067\n",
      "Average test loss: 0.006404544445375602\n",
      "Epoch 300/300\n",
      "Average training loss: 0.7495891494221157\n",
      "Average test loss: 0.006508449751883745\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.8572307420571645\n",
      "Average test loss: 0.008478624684115251\n",
      "Epoch 2/300\n",
      "Average training loss: 0.2181179464260737\n",
      "Average test loss: 0.006539337321701977\n",
      "Epoch 3/300\n",
      "Average training loss: 0.16274390380912357\n",
      "Average test loss: 0.006125779785215855\n",
      "Epoch 4/300\n",
      "Average training loss: 0.1406211722162035\n",
      "Average test loss: 0.005852550158070193\n",
      "Epoch 5/300\n",
      "Average training loss: 0.12759849488735198\n",
      "Average test loss: 0.005549363520410326\n",
      "Epoch 6/300\n",
      "Average training loss: 0.11932513415813446\n",
      "Average test loss: 0.005528819807701641\n",
      "Epoch 7/300\n",
      "Average training loss: 0.11313175430562761\n",
      "Average test loss: 0.005304241255339649\n",
      "Epoch 8/300\n",
      "Average training loss: 0.10869230898221334\n",
      "Average test loss: 0.005106565490778949\n",
      "Epoch 9/300\n",
      "Average training loss: 0.10485258349445131\n",
      "Average test loss: 0.005124663655956587\n",
      "Epoch 10/300\n",
      "Average training loss: 0.10099210016594992\n",
      "Average test loss: 0.006734399537659354\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0971719903283649\n",
      "Average test loss: 0.004874635057316886\n",
      "Epoch 12/300\n",
      "Average training loss: 0.09380674737029605\n",
      "Average test loss: 0.005062165441405442\n",
      "Epoch 13/300\n",
      "Average training loss: 0.0909521533648173\n",
      "Average test loss: 0.006127173461019993\n",
      "Epoch 14/300\n",
      "Average training loss: 0.08773784135778745\n",
      "Average test loss: 0.004444215703755617\n",
      "Epoch 15/300\n",
      "Average training loss: 0.08573457084761725\n",
      "Average test loss: 0.0047078066079152955\n",
      "Epoch 16/300\n",
      "Average training loss: 0.08318951474957996\n",
      "Average test loss: 0.004338996149806513\n",
      "Epoch 17/300\n",
      "Average training loss: 0.08094496931301223\n",
      "Average test loss: 0.004245208565352691\n",
      "Epoch 18/300\n",
      "Average training loss: 0.07933690039316812\n",
      "Average test loss: 0.004335328824818134\n",
      "Epoch 19/300\n",
      "Average training loss: 0.07745837112930086\n",
      "Average test loss: 0.006291962640980879\n",
      "Epoch 20/300\n",
      "Average training loss: 0.07599796133240064\n",
      "Average test loss: 0.004024483972746465\n",
      "Epoch 21/300\n",
      "Average training loss: 0.07485658093955781\n",
      "Average test loss: 0.004148119108544456\n",
      "Epoch 22/300\n",
      "Average training loss: 0.07365127484003703\n",
      "Average test loss: 0.004362404538111554\n",
      "Epoch 23/300\n",
      "Average training loss: 0.0726120478577084\n",
      "Average test loss: 0.003916773087862465\n",
      "Epoch 24/300\n",
      "Average training loss: 0.07147950610849592\n",
      "Average test loss: 0.00395169713265366\n",
      "Epoch 25/300\n",
      "Average training loss: 0.07069879316952493\n",
      "Average test loss: 0.003977893416873283\n",
      "Epoch 26/300\n",
      "Average training loss: 0.0700594975815879\n",
      "Average test loss: 0.003933766783939467\n",
      "Epoch 27/300\n",
      "Average training loss: 0.06904603001144197\n",
      "Average test loss: 0.0037144561579657925\n",
      "Epoch 28/300\n",
      "Average training loss: 0.06855675057570139\n",
      "Average test loss: 0.0037699059307989146\n",
      "Epoch 29/300\n",
      "Average training loss: 0.06789372376600901\n",
      "Average test loss: 0.0037586270690792138\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0673252888917923\n",
      "Average test loss: 0.003676884244092637\n",
      "Epoch 31/300\n",
      "Average training loss: 0.06672851289643182\n",
      "Average test loss: 0.003983773379276196\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06622900685667991\n",
      "Average test loss: 0.0036589323046306767\n",
      "Epoch 33/300\n",
      "Average training loss: 0.06572812823123401\n",
      "Average test loss: 0.0036427531134751107\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06552631942762269\n",
      "Average test loss: 0.003686220689987143\n",
      "Epoch 35/300\n",
      "Average training loss: 0.06504743975400924\n",
      "Average test loss: 0.003577951779589057\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06450444042682647\n",
      "Average test loss: 0.003596425003061692\n",
      "Epoch 37/300\n",
      "Average training loss: 0.06416556054353714\n",
      "Average test loss: 0.0035757853090763094\n",
      "Epoch 38/300\n",
      "Average training loss: 0.06387986230187946\n",
      "Average test loss: 0.0035983510160197815\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0637962007423242\n",
      "Average test loss: 0.003759585669057237\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06334711013899909\n",
      "Average test loss: 0.0036233085869914958\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06308373911513222\n",
      "Average test loss: 0.0036920958277251986\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06317055439617898\n",
      "Average test loss: 0.0035639445090459455\n",
      "Epoch 43/300\n",
      "Average training loss: 1.535951011478901\n",
      "Average test loss: 0.013139058233963118\n",
      "Epoch 44/300\n",
      "Average training loss: 2.4544251312679717\n",
      "Average test loss: 0.008995550186269814\n",
      "Epoch 45/300\n",
      "Average training loss: 1.2777820046742756\n",
      "Average test loss: 0.006114355891942978\n",
      "Epoch 46/300\n",
      "Average training loss: 0.8251418891482883\n",
      "Average test loss: 0.006315690648224619\n",
      "Epoch 47/300\n",
      "Average training loss: 0.5871347321404351\n",
      "Average test loss: 0.004911322835004992\n",
      "Epoch 48/300\n",
      "Average training loss: 0.44209050395753646\n",
      "Average test loss: 0.013935834644569291\n",
      "Epoch 49/300\n",
      "Average training loss: 0.3476073279115889\n",
      "Average test loss: 0.005637840599028601\n",
      "Epoch 50/300\n",
      "Average training loss: 0.27913268200556435\n",
      "Average test loss: 0.00450582896421353\n",
      "Epoch 51/300\n",
      "Average training loss: 0.22682251000404358\n",
      "Average test loss: 0.004678628692196475\n",
      "Epoch 52/300\n",
      "Average training loss: 0.18829786496692233\n",
      "Average test loss: 0.004598566935294204\n",
      "Epoch 53/300\n",
      "Average training loss: 0.16047436039977603\n",
      "Average test loss: 0.004239614034899407\n",
      "Epoch 54/300\n",
      "Average training loss: 0.14106603834364148\n",
      "Average test loss: 0.004604752561491397\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12394430954588784\n",
      "Average test loss: 0.004127631701322065\n",
      "Epoch 56/300\n",
      "Average training loss: 0.11108668077654309\n",
      "Average test loss: 0.003939975408837199\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09745244167579545\n",
      "Average test loss: 0.00392199450565709\n",
      "Epoch 58/300\n",
      "Average training loss: 0.087757047014104\n",
      "Average test loss: 0.0038542678794927067\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08333321497837702\n",
      "Average test loss: 0.0037912350280417334\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07983994519710541\n",
      "Average test loss: 0.0038841440236816803\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07719075261884266\n",
      "Average test loss: 0.0037158063732915456\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07498201645082897\n",
      "Average test loss: 0.003749437454260058\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07316389530897141\n",
      "Average test loss: 0.00376732693405615\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07162674652867847\n",
      "Average test loss: 0.0039715644787583085\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07013501332865821\n",
      "Average test loss: 0.0036191863032678765\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06892841938469145\n",
      "Average test loss: 0.0035797310434281825\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06802018814616734\n",
      "Average test loss: 0.003629412490874529\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06705145654082298\n",
      "Average test loss: 0.003559660648306211\n",
      "Epoch 69/300\n",
      "Average training loss: 0.06634106257226732\n",
      "Average test loss: 0.003627384173994263\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06563076011008687\n",
      "Average test loss: 0.003583372235298157\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06503077191776699\n",
      "Average test loss: 0.0036711041140887473\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06446799297465218\n",
      "Average test loss: 0.0036516137472871277\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0640543923775355\n",
      "Average test loss: 0.0036138572183748085\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06380193232165443\n",
      "Average test loss: 0.003549909015496572\n",
      "Epoch 75/300\n",
      "Average training loss: 0.06362331065204409\n",
      "Average test loss: 0.003641799160382814\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06386522314283583\n",
      "Average test loss: 0.0036489643489734996\n",
      "Epoch 77/300\n",
      "Average training loss: 0.0627052352031072\n",
      "Average test loss: 0.0034865834195580746\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06243192912803756\n",
      "Average test loss: 0.003517598615752326\n",
      "Epoch 79/300\n",
      "Average training loss: 0.062337520331144335\n",
      "Average test loss: 0.003557220576331019\n",
      "Epoch 80/300\n",
      "Average training loss: 0.062197875357336466\n",
      "Average test loss: 0.0035206562920163074\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06199766768349541\n",
      "Average test loss: 0.003491018662850062\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0618680341872904\n",
      "Average test loss: 0.003486684537389212\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06162243408958117\n",
      "Average test loss: 0.0034539789323591525\n",
      "Epoch 84/300\n",
      "Average training loss: 0.061496794521808625\n",
      "Average test loss: 0.0034438321143388747\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06138781379328834\n",
      "Average test loss: 0.003523029460882147\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06115175790588061\n",
      "Average test loss: 0.0036313887323356337\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0611848949889342\n",
      "Average test loss: 0.003432937672568692\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06082797284258736\n",
      "Average test loss: 0.0034817605250411564\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06081021522151099\n",
      "Average test loss: 0.003815819986992412\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06055916584200329\n",
      "Average test loss: 0.003443938000127673\n",
      "Epoch 91/300\n",
      "Average training loss: 0.061168209771315256\n",
      "Average test loss: 0.0034420304290122454\n",
      "Epoch 92/300\n",
      "Average training loss: 0.10127057963609695\n",
      "Average test loss: 0.008804578743047185\n",
      "Epoch 93/300\n",
      "Average training loss: 0.09181591278976864\n",
      "Average test loss: 0.003579532285117441\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06975880753331715\n",
      "Average test loss: 0.25909487917688157\n",
      "Epoch 95/300\n",
      "Average training loss: 0.0665925540526708\n",
      "Average test loss: 0.003465279697544045\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06471590289142397\n",
      "Average test loss: 0.0034765715977797904\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0635543112622367\n",
      "Average test loss: 0.0035223120593776305\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06272410791450077\n",
      "Average test loss: 0.003396649107750919\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06221996474928326\n",
      "Average test loss: 0.0034341041669249533\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06166266750627094\n",
      "Average test loss: 0.003457489827440845\n",
      "Epoch 101/300\n",
      "Average training loss: 0.0614276332921452\n",
      "Average test loss: 0.003477998525939054\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0610740002128813\n",
      "Average test loss: 0.0034056178279634978\n",
      "Epoch 103/300\n",
      "Average training loss: 0.06074569271670448\n",
      "Average test loss: 0.0034696390500499144\n",
      "Epoch 104/300\n",
      "Average training loss: 0.06061251713501083\n",
      "Average test loss: 0.00343998567511638\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06046287567747964\n",
      "Average test loss: 0.003531978632633885\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06031380114952723\n",
      "Average test loss: 0.00352710787496633\n",
      "Epoch 107/300\n",
      "Average training loss: 0.060232323911454944\n",
      "Average test loss: 0.003457235967119535\n",
      "Epoch 108/300\n",
      "Average training loss: 0.060128248949845635\n",
      "Average test loss: 0.0034560221833073433\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05989501517017683\n",
      "Average test loss: 0.003447624783962965\n",
      "Epoch 110/300\n",
      "Average training loss: 0.05985792312026024\n",
      "Average test loss: 0.003439607260334823\n",
      "Epoch 111/300\n",
      "Average training loss: 0.059751507600148517\n",
      "Average test loss: 0.005651940546929836\n",
      "Epoch 112/300\n",
      "Average training loss: 0.059874725225898956\n",
      "Average test loss: 0.0036145169714258776\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05972990983393457\n",
      "Average test loss: 0.0034380454681813715\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05952833705147107\n",
      "Average test loss: 0.003512984603229496\n",
      "Epoch 115/300\n",
      "Average training loss: 0.059374199092388155\n",
      "Average test loss: 0.003567865270914303\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05920474977625741\n",
      "Average test loss: 0.00828570821053452\n",
      "Epoch 117/300\n",
      "Average training loss: 0.059146207587586506\n",
      "Average test loss: 0.0034755897840691935\n",
      "Epoch 118/300\n",
      "Average training loss: 0.059142519374688465\n",
      "Average test loss: 0.003374285522020525\n",
      "Epoch 119/300\n",
      "Average training loss: 0.058986173066827984\n",
      "Average test loss: 0.0034157410328172974\n",
      "Epoch 120/300\n",
      "Average training loss: 0.05892254903250271\n",
      "Average test loss: 0.003532389819001158\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05881897348827786\n",
      "Average test loss: 0.003432867212841908\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05898123579223951\n",
      "Average test loss: 0.00624973804772728\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05962267013390859\n",
      "Average test loss: 0.0034494972945087487\n",
      "Epoch 124/300\n",
      "Average training loss: 0.0585342460208469\n",
      "Average test loss: 0.00339288425931914\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05838271501660347\n",
      "Average test loss: 0.0033994093731873564\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05833347616261906\n",
      "Average test loss: 0.003395806933856673\n",
      "Epoch 127/300\n",
      "Average training loss: 0.058273551632960635\n",
      "Average test loss: 0.0033810327943000528\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05821811150179969\n",
      "Average test loss: 0.003388243676887618\n",
      "Epoch 129/300\n",
      "Average training loss: 0.058525023910734386\n",
      "Average test loss: 0.0034746211119410066\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05801057670844926\n",
      "Average test loss: 0.003401606023104654\n",
      "Epoch 131/300\n",
      "Average training loss: 0.058001601705948515\n",
      "Average test loss: 0.003439589543061124\n",
      "Epoch 132/300\n",
      "Average training loss: 0.057980843669838376\n",
      "Average test loss: 0.0033786952644586565\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05789723651607831\n",
      "Average test loss: 0.0033967695141004194\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05782010452946027\n",
      "Average test loss: 0.003344381212981211\n",
      "Epoch 135/300\n",
      "Average training loss: 0.0577664555311203\n",
      "Average test loss: 0.004745318971574306\n",
      "Epoch 136/300\n",
      "Average training loss: 0.05766623381111357\n",
      "Average test loss: 0.0034419036507606507\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05793684315019184\n",
      "Average test loss: 0.0036493460891975295\n",
      "Epoch 138/300\n",
      "Average training loss: 0.057394904103544024\n",
      "Average test loss: 0.003409865105524659\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0574387023780081\n",
      "Average test loss: 0.0034179775503774485\n",
      "Epoch 140/300\n",
      "Average training loss: 0.057433795107735525\n",
      "Average test loss: 0.005178803092903561\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05889677059650421\n",
      "Average test loss: 0.0034460266155915127\n",
      "Epoch 142/300\n",
      "Average training loss: 0.059761986212597956\n",
      "Average test loss: 0.003380039845282833\n",
      "Epoch 143/300\n",
      "Average training loss: 0.057203526119391127\n",
      "Average test loss: 0.0034310226125849616\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05712204825547006\n",
      "Average test loss: 0.003510228687690364\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05718877778119511\n",
      "Average test loss: 0.0033576188969115417\n",
      "Epoch 146/300\n",
      "Average training loss: 0.05691239995095465\n",
      "Average test loss: 0.0033849570680823593\n",
      "Epoch 147/300\n",
      "Average training loss: 0.057557536933157176\n",
      "Average test loss: 0.003439314314060741\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05709127067857318\n",
      "Average test loss: 0.003416643858369854\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05695773465765847\n",
      "Average test loss: 0.0033789566622840035\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05685225611593988\n",
      "Average test loss: 0.003410777793162399\n",
      "Epoch 151/300\n",
      "Average training loss: 0.057269999573628105\n",
      "Average test loss: 0.00375800932964517\n",
      "Epoch 152/300\n",
      "Average training loss: 0.057506748504108855\n",
      "Average test loss: 0.0033474317654553385\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05684521101580726\n",
      "Average test loss: 0.003390208201068971\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05687833441297213\n",
      "Average test loss: 0.003511319561344054\n",
      "Epoch 155/300\n",
      "Average training loss: 0.0566230683028698\n",
      "Average test loss: 0.003469309840972225\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05663497241338094\n",
      "Average test loss: 0.003454488340765238\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05663595480720202\n",
      "Average test loss: 0.0034241969395014974\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05650437198082606\n",
      "Average test loss: 0.003993236339961489\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05684325087732739\n",
      "Average test loss: 0.003636647837857405\n",
      "Epoch 160/300\n",
      "Average training loss: 0.056705717189444435\n",
      "Average test loss: 0.0034292787491447397\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05657021048996184\n",
      "Average test loss: 0.0033542111693984932\n",
      "Epoch 162/300\n",
      "Average training loss: 0.056377440714173846\n",
      "Average test loss: 0.0033955275867548255\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05622806197735998\n",
      "Average test loss: 0.0034019935398052134\n",
      "Epoch 164/300\n",
      "Average training loss: 0.056367529617415535\n",
      "Average test loss: 0.003413596650171611\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0563234591682752\n",
      "Average test loss: 0.0034471119004819127\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05626572481460042\n",
      "Average test loss: 0.0033952352147963312\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05614718825949563\n",
      "Average test loss: 0.003514190905623966\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05621764008866416\n",
      "Average test loss: 0.0034017591985563435\n",
      "Epoch 169/300\n",
      "Average training loss: 0.0560697530011336\n",
      "Average test loss: 0.0074892893069320255\n",
      "Epoch 170/300\n",
      "Average training loss: 0.056026081598467295\n",
      "Average test loss: 0.003652330521080229\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05609878758589427\n",
      "Average test loss: 0.003393782389246755\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05619052330321736\n",
      "Average test loss: 0.003456745882415109\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05676537768046061\n",
      "Average test loss: 0.0035085943266749383\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05597954052686691\n",
      "Average test loss: 0.004274523315330347\n",
      "Epoch 175/300\n",
      "Average training loss: 0.055807292805777654\n",
      "Average test loss: 0.0033930588830262424\n",
      "Epoch 176/300\n",
      "Average training loss: 0.05579158575667275\n",
      "Average test loss: 0.0035341210406687525\n",
      "Epoch 177/300\n",
      "Average training loss: 0.055770022090938356\n",
      "Average test loss: 0.0033877100518180264\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0557824382185936\n",
      "Average test loss: 0.0033722969165278804\n",
      "Epoch 179/300\n",
      "Average training loss: 0.055630782703558604\n",
      "Average test loss: 0.0035901368844012418\n",
      "Epoch 180/300\n",
      "Average training loss: 0.05572202140755123\n",
      "Average test loss: 0.003448482609043519\n",
      "Epoch 181/300\n",
      "Average training loss: 0.055668744143512514\n",
      "Average test loss: 0.0034350025467574596\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05548900414506595\n",
      "Average test loss: 0.0033641109050561984\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05556514535347621\n",
      "Average test loss: 0.0034059624411165714\n",
      "Epoch 184/300\n",
      "Average training loss: 0.055528392844729955\n",
      "Average test loss: 0.0034423683861063586\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05544020845492681\n",
      "Average test loss: 0.003401520830268661\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05586555497845014\n",
      "Average test loss: 0.003415006735051672\n",
      "Epoch 187/300\n",
      "Average training loss: 0.05572980748282538\n",
      "Average test loss: 0.0034101203535166053\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05558866933981577\n",
      "Average test loss: 0.0033882929355733924\n",
      "Epoch 189/300\n",
      "Average training loss: 0.05524083710710208\n",
      "Average test loss: 0.0036209753068784873\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05550966126388974\n",
      "Average test loss: 0.003407617915007803\n",
      "Epoch 191/300\n",
      "Average training loss: 0.0552470087243451\n",
      "Average test loss: 0.0033947193190041517\n",
      "Epoch 192/300\n",
      "Average training loss: 0.055284826931026244\n",
      "Average test loss: 0.004412926521566179\n",
      "Epoch 193/300\n",
      "Average training loss: 0.055307547377215494\n",
      "Average test loss: 0.0034486140112082162\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05509429316057099\n",
      "Average test loss: 0.005018134739249945\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05522679164674547\n",
      "Average test loss: 0.0034746401009874213\n",
      "Epoch 196/300\n",
      "Average training loss: 0.05516019419497914\n",
      "Average test loss: 0.0035381728828781183\n",
      "Epoch 197/300\n",
      "Average training loss: 0.055316992974943587\n",
      "Average test loss: 0.0034238186875979104\n",
      "Epoch 198/300\n",
      "Average training loss: 0.054985079689158334\n",
      "Average test loss: 0.0035375420906477504\n",
      "Epoch 199/300\n",
      "Average training loss: 0.054951990948783024\n",
      "Average test loss: 0.0034097736057721907\n",
      "Epoch 200/300\n",
      "Average training loss: 0.054990443564123574\n",
      "Average test loss: 0.0033902410440560843\n",
      "Epoch 201/300\n",
      "Average training loss: 0.05532555592722363\n",
      "Average test loss: 0.0035058749549918703\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05493220300806893\n",
      "Average test loss: 0.0034448599728445213\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05506410820285479\n",
      "Average test loss: 0.003420074326296647\n",
      "Epoch 204/300\n",
      "Average training loss: 0.05472795361942715\n",
      "Average test loss: 0.003494044928914971\n",
      "Epoch 205/300\n",
      "Average training loss: 0.055021050539281635\n",
      "Average test loss: 0.003404938119360142\n",
      "Epoch 206/300\n",
      "Average training loss: 0.05477843216061592\n",
      "Average test loss: 0.004164462023311191\n",
      "Epoch 207/300\n",
      "Average training loss: 0.05488237065076828\n",
      "Average test loss: 0.003373817959593402\n",
      "Epoch 208/300\n",
      "Average training loss: 0.05458122102419535\n",
      "Average test loss: 0.0034634011638247303\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05475452068779204\n",
      "Average test loss: 0.005270430114741127\n",
      "Epoch 210/300\n",
      "Average training loss: 0.05467378061347537\n",
      "Average test loss: 0.0034467412125733163\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05457523665825526\n",
      "Average test loss: 0.003459882955998182\n",
      "Epoch 212/300\n",
      "Average training loss: 0.05473144240511788\n",
      "Average test loss: 0.0033976930050800243\n",
      "Epoch 213/300\n",
      "Average training loss: 0.05579037614663442\n",
      "Average test loss: 0.0033748149561385315\n",
      "Epoch 214/300\n",
      "Average training loss: 0.054558984455135136\n",
      "Average test loss: 0.003430481473811799\n",
      "Epoch 215/300\n",
      "Average training loss: 0.05461867109272215\n",
      "Average test loss: 0.00343473669865893\n",
      "Epoch 216/300\n",
      "Average training loss: 0.0544907518559032\n",
      "Average test loss: 0.0034827475907901924\n",
      "Epoch 217/300\n",
      "Average training loss: 0.054632687581910026\n",
      "Average test loss: 0.0034147693080206713\n",
      "Epoch 218/300\n",
      "Average training loss: 0.05449443938334783\n",
      "Average test loss: 0.003488947448010246\n",
      "Epoch 219/300\n",
      "Average training loss: 0.05429243787460857\n",
      "Average test loss: 0.0034140062715030377\n",
      "Epoch 220/300\n",
      "Average training loss: 0.05443306126197179\n",
      "Average test loss: 0.0033733649378021556\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05429351144532363\n",
      "Average test loss: 0.0035008773766458034\n",
      "Epoch 222/300\n",
      "Average training loss: 0.05436175197031763\n",
      "Average test loss: 0.004527940715145734\n",
      "Epoch 223/300\n",
      "Average training loss: 0.054379486229684616\n",
      "Average test loss: 0.015683186375018622\n",
      "Epoch 224/300\n",
      "Average training loss: 0.054678851743539175\n",
      "Average test loss: 0.026401702764961454\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0544235025578075\n",
      "Average test loss: 0.00346899140232967\n",
      "Epoch 226/300\n",
      "Average training loss: 0.054270185788472494\n",
      "Average test loss: 0.0034801179294784864\n",
      "Epoch 227/300\n",
      "Average training loss: 0.05423481841219796\n",
      "Average test loss: 0.0034631225156287354\n",
      "Epoch 228/300\n",
      "Average training loss: 0.054244247645139694\n",
      "Average test loss: 0.003559805353068643\n",
      "Epoch 229/300\n",
      "Average training loss: 0.054031555732091265\n",
      "Average test loss: 0.0034868522114637826\n",
      "Epoch 230/300\n",
      "Average training loss: 0.054272686160273025\n",
      "Average test loss: 0.0034428204086919627\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05408115134967698\n",
      "Average test loss: 0.0034631080598466926\n",
      "Epoch 232/300\n",
      "Average training loss: 0.05422563996911049\n",
      "Average test loss: 0.0034626100909792715\n",
      "Epoch 233/300\n",
      "Average training loss: 0.05412819019291136\n",
      "Average test loss: 0.00346721753395266\n",
      "Epoch 234/300\n",
      "Average training loss: 0.054053426580296625\n",
      "Average test loss: 0.0046680323378079465\n",
      "Epoch 235/300\n",
      "Average training loss: 0.054378268169032205\n",
      "Average test loss: 0.0035316645097401406\n",
      "Epoch 236/300\n",
      "Average training loss: 0.053907219780815974\n",
      "Average test loss: 0.0034639030090636677\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05394533112976286\n",
      "Average test loss: 0.003434015467349026\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0688868983189265\n",
      "Average test loss: 0.009709770442710982\n",
      "Epoch 239/300\n",
      "Average training loss: 0.11395567633046044\n",
      "Average test loss: 0.0034548160684191517\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06686902678012847\n",
      "Average test loss: 0.0034214340048945614\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06335328296158049\n",
      "Average test loss: 0.003455795896343059\n",
      "Epoch 242/300\n",
      "Average training loss: 0.061424058907561835\n",
      "Average test loss: 0.0034085462614893915\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05985789122846392\n",
      "Average test loss: 0.003414727330414785\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05861192244291306\n",
      "Average test loss: 0.003417962051100201\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0577290677593814\n",
      "Average test loss: 0.00342779465061095\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05694922234283553\n",
      "Average test loss: 0.005588550745199124\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05687973556915919\n",
      "Average test loss: 0.007704178821502461\n",
      "Epoch 248/300\n",
      "Average training loss: 0.05591491742597686\n",
      "Average test loss: 0.003373107074863381\n",
      "Epoch 249/300\n",
      "Average training loss: 0.055218261112769444\n",
      "Average test loss: 0.0035534407214985954\n",
      "Epoch 250/300\n",
      "Average training loss: 0.055022426860200035\n",
      "Average test loss: 0.0035936291567567323\n",
      "Epoch 251/300\n",
      "Average training loss: 0.05460554668638441\n",
      "Average test loss: 0.003422947605864869\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05440425921810998\n",
      "Average test loss: 0.0034014455609851415\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05425739654567507\n",
      "Average test loss: 0.003454843529810508\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05421233590443929\n",
      "Average test loss: 0.0034532654370284742\n",
      "Epoch 255/300\n",
      "Average training loss: 0.054008311473660996\n",
      "Average test loss: 0.003431548453660475\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05394308163722356\n",
      "Average test loss: 0.003455868338131242\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05402175881796413\n",
      "Average test loss: 0.003424502556108766\n",
      "Epoch 258/300\n",
      "Average training loss: 0.05404012742307451\n",
      "Average test loss: 0.003421794819128182\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05390931024816301\n",
      "Average test loss: 0.00351739588048723\n",
      "Epoch 260/300\n",
      "Average training loss: 0.05395981788304117\n",
      "Average test loss: 0.0037377217368533213\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05432471375332938\n",
      "Average test loss: 0.0035494962342911295\n",
      "Epoch 262/300\n",
      "Average training loss: 0.05405982831120491\n",
      "Average test loss: 0.003410783665668633\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05379549167222447\n",
      "Average test loss: 0.00341320879281395\n",
      "Epoch 264/300\n",
      "Average training loss: 0.053609011381864545\n",
      "Average test loss: 0.0034786977294004626\n",
      "Epoch 265/300\n",
      "Average training loss: 0.05382321498129103\n",
      "Average test loss: 0.0036068613053196006\n",
      "Epoch 266/300\n",
      "Average training loss: 0.053788889795541764\n",
      "Average test loss: 0.0055101352354718575\n",
      "Epoch 267/300\n",
      "Average training loss: 0.053743304948012036\n",
      "Average test loss: 0.004452465752139687\n",
      "Epoch 268/300\n",
      "Average training loss: 0.05368375729189979\n",
      "Average test loss: 0.003599189469176862\n",
      "Epoch 269/300\n",
      "Average training loss: 0.053598829958173964\n",
      "Average test loss: 0.0034285023816757734\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05399000252948867\n",
      "Average test loss: 0.0034315034355968236\n",
      "Epoch 271/300\n",
      "Average training loss: 0.053424877143568465\n",
      "Average test loss: 0.00346062734681699\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05352558018432723\n",
      "Average test loss: 0.0034901186260912153\n",
      "Epoch 273/300\n",
      "Average training loss: 0.05355469536450174\n",
      "Average test loss: 0.0034394298500070968\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05362515480981933\n",
      "Average test loss: 0.028567204119430648\n",
      "Epoch 275/300\n",
      "Average training loss: 0.05341958784725931\n",
      "Average test loss: 0.003481415907541911\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05348829801215066\n",
      "Average test loss: 0.0035115944540335072\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05350306565562884\n",
      "Average test loss: 0.0034502581495584714\n",
      "Epoch 278/300\n",
      "Average training loss: 0.053427531517214244\n",
      "Average test loss: 0.0036066161890824637\n",
      "Epoch 279/300\n",
      "Average training loss: 0.05346736664904488\n",
      "Average test loss: 0.00406126820254657\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05328862318396568\n",
      "Average test loss: 0.0035083944540884758\n",
      "Epoch 281/300\n",
      "Average training loss: 0.053275985644923314\n",
      "Average test loss: 0.003990627117455006\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05349647511376275\n",
      "Average test loss: 0.0034657864169114166\n",
      "Epoch 283/300\n",
      "Average training loss: 0.05340348348683781\n",
      "Average test loss: 0.0035554000242716734\n",
      "Epoch 284/300\n",
      "Average training loss: 0.053350322988298206\n",
      "Average test loss: 0.003435589638228218\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0531814954413308\n",
      "Average test loss: 0.003507678095665243\n",
      "Epoch 286/300\n",
      "Average training loss: 0.05325094977352354\n",
      "Average test loss: 0.28209318677584333\n",
      "Epoch 287/300\n",
      "Average training loss: 0.053303554783264796\n",
      "Average test loss: 0.0037299536065095\n",
      "Epoch 288/300\n",
      "Average training loss: 0.053236429658201\n",
      "Average test loss: 0.0034837720567981404\n",
      "Epoch 289/300\n",
      "Average training loss: 0.053061076472202935\n",
      "Average test loss: 301.2767140875922\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05326037771171994\n",
      "Average test loss: 0.003480815008489622\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05306314414739609\n",
      "Average test loss: 0.0035285344415654737\n",
      "Epoch 292/300\n",
      "Average training loss: 0.053027401006884044\n",
      "Average test loss: 0.003511679539250003\n",
      "Epoch 293/300\n",
      "Average training loss: 0.05310070021284951\n",
      "Average test loss: 0.003502195874642995\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05322265425655577\n",
      "Average test loss: 0.0034432226315968565\n",
      "Epoch 295/300\n",
      "Average training loss: 0.053005404707458285\n",
      "Average test loss: 0.003470390778655807\n",
      "Epoch 296/300\n",
      "Average training loss: 0.053343058274851904\n",
      "Average test loss: 0.0035795817428992856\n",
      "Epoch 297/300\n",
      "Average training loss: 0.05295896836453014\n",
      "Average test loss: 0.004314480527407593\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05292382863495085\n",
      "Average test loss: 0.003466751428734925\n",
      "Epoch 299/300\n",
      "Average training loss: 0.05297080487675137\n",
      "Average test loss: 0.003516838210117486\n",
      "Epoch 300/300\n",
      "Average training loss: 0.05297415059804916\n",
      "Average test loss: 0.003471738475685318\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5952126659949621\n",
      "Average test loss: 0.006690434046089649\n",
      "Epoch 2/300\n",
      "Average training loss: 0.17255428704950546\n",
      "Average test loss: 0.005305507929374774\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1288492333624098\n",
      "Average test loss: 0.005198923315852881\n",
      "Epoch 4/300\n",
      "Average training loss: 0.11133810736735662\n",
      "Average test loss: 0.004499948223431905\n",
      "Epoch 5/300\n",
      "Average training loss: 0.10109638441271251\n",
      "Average test loss: 0.004629662804719475\n",
      "Epoch 6/300\n",
      "Average training loss: 0.09438850022024578\n",
      "Average test loss: 0.004290426833348142\n",
      "Epoch 7/300\n",
      "Average training loss: 0.08830950924423006\n",
      "Average test loss: 0.004130675913352105\n",
      "Epoch 8/300\n",
      "Average training loss: 0.08444581522544226\n",
      "Average test loss: 0.00402410410799914\n",
      "Epoch 9/300\n",
      "Average training loss: 0.08076110422611237\n",
      "Average test loss: 0.003916104947113329\n",
      "Epoch 10/300\n",
      "Average training loss: 0.07736671762665112\n",
      "Average test loss: 0.0037259193737473753\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0742623284359773\n",
      "Average test loss: 0.0034757370120949214\n",
      "Epoch 12/300\n",
      "Average training loss: 0.07180167845884959\n",
      "Average test loss: 0.005038333096851905\n",
      "Epoch 13/300\n",
      "Average training loss: 0.06946019519699945\n",
      "Average test loss: 0.0034762436097694766\n",
      "Epoch 14/300\n",
      "Average training loss: 0.06692018784748183\n",
      "Average test loss: 0.0032348648587034808\n",
      "Epoch 15/300\n",
      "Average training loss: 0.06468503757648998\n",
      "Average test loss: 0.0032612280682143236\n",
      "Epoch 16/300\n",
      "Average training loss: 0.06285538478692372\n",
      "Average test loss: 0.003115179615509179\n",
      "Epoch 17/300\n",
      "Average training loss: 0.06106833065218396\n",
      "Average test loss: 0.0030812143019090096\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05939352327916357\n",
      "Average test loss: 0.003120651931398445\n",
      "Epoch 19/300\n",
      "Average training loss: 0.058014546831448875\n",
      "Average test loss: 0.002954116374047266\n",
      "Epoch 20/300\n",
      "Average training loss: 0.056823919743299485\n",
      "Average test loss: 0.002800093104235\n",
      "Epoch 21/300\n",
      "Average training loss: 0.055602590223153434\n",
      "Average test loss: 0.0028892456942962276\n",
      "Epoch 22/300\n",
      "Average training loss: 0.054524980316559474\n",
      "Average test loss: 0.0027753168476952446\n",
      "Epoch 23/300\n",
      "Average training loss: 0.05361313898364703\n",
      "Average test loss: 0.0027576328461161917\n",
      "Epoch 24/300\n",
      "Average training loss: 0.0529512980679671\n",
      "Average test loss: 0.0027048401135123436\n",
      "Epoch 25/300\n",
      "Average training loss: 0.05218763398461872\n",
      "Average test loss: 0.0027166523670570718\n",
      "Epoch 26/300\n",
      "Average training loss: 0.05167532691690657\n",
      "Average test loss: 0.0026433844396637545\n",
      "Epoch 27/300\n",
      "Average training loss: 0.05102970651454396\n",
      "Average test loss: 0.002635938030771083\n",
      "Epoch 28/300\n",
      "Average training loss: 0.050650320771667694\n",
      "Average test loss: 0.002635672186811765\n",
      "Epoch 29/300\n",
      "Average training loss: 0.04993052161733309\n",
      "Average test loss: 0.0029228414949029682\n",
      "Epoch 30/300\n",
      "Average training loss: 0.049563914633459515\n",
      "Average test loss: 0.0026380234998133445\n",
      "Epoch 31/300\n",
      "Average training loss: 0.049082939816845784\n",
      "Average test loss: 0.0034920835776461497\n",
      "Epoch 32/300\n",
      "Average training loss: 0.0488593399822712\n",
      "Average test loss: 0.002530751726900538\n",
      "Epoch 33/300\n",
      "Average training loss: 0.04839497115545802\n",
      "Average test loss: 0.002560488337961336\n",
      "Epoch 34/300\n",
      "Average training loss: 0.04810839145051109\n",
      "Average test loss: 0.0033206092562112544\n",
      "Epoch 35/300\n",
      "Average training loss: 0.047793635103437634\n",
      "Average test loss: 0.002530457186202208\n",
      "Epoch 36/300\n",
      "Average training loss: 0.04759213141931428\n",
      "Average test loss: 0.002524654101373421\n",
      "Epoch 37/300\n",
      "Average training loss: 0.04732846689224243\n",
      "Average test loss: 0.0024891512170434\n",
      "Epoch 38/300\n",
      "Average training loss: 0.04709041342139244\n",
      "Average test loss: 0.002549345758640104\n",
      "Epoch 39/300\n",
      "Average training loss: 0.04685239398479462\n",
      "Average test loss: 0.002496981481090188\n",
      "Epoch 40/300\n",
      "Average training loss: 0.04673753533098433\n",
      "Average test loss: 0.0024998913490109974\n",
      "Epoch 41/300\n",
      "Average training loss: 0.04652087692088551\n",
      "Average test loss: 0.002464287551223404\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04622914495401912\n",
      "Average test loss: 0.002436368520682057\n",
      "Epoch 43/300\n",
      "Average training loss: 0.046883977404899066\n",
      "Average test loss: 0.002599188665859401\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04647401208347744\n",
      "Average test loss: 0.0033422110786454546\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04585483832822906\n",
      "Average test loss: 0.002441745472761492\n",
      "Epoch 46/300\n",
      "Average training loss: 0.045707630342907375\n",
      "Average test loss: 0.0024255018677148553\n",
      "Epoch 47/300\n",
      "Average training loss: 0.04557096749875281\n",
      "Average test loss: 0.002418315191546248\n",
      "Epoch 48/300\n",
      "Average training loss: 0.045517435050672954\n",
      "Average test loss: 0.00258225404119326\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04528111485309071\n",
      "Average test loss: 0.0024803743538343243\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04528745025727484\n",
      "Average test loss: 0.0024102996647771863\n",
      "Epoch 51/300\n",
      "Average training loss: 0.04512390785747104\n",
      "Average test loss: 0.0024129171296954155\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0450541229777866\n",
      "Average test loss: 0.0027632258575823574\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04495802157786157\n",
      "Average test loss: 0.0024202739297308857\n",
      "Epoch 54/300\n",
      "Average training loss: 0.044712101297246086\n",
      "Average test loss: 0.002381328055324654\n",
      "Epoch 55/300\n",
      "Average training loss: 0.044668859256638424\n",
      "Average test loss: 0.002393271507281396\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04466707159744369\n",
      "Average test loss: 0.002438505236680309\n",
      "Epoch 57/300\n",
      "Average training loss: 0.044523204581605066\n",
      "Average test loss: 0.0027313815847867066\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04446304767661625\n",
      "Average test loss: 0.0023809386398643256\n",
      "Epoch 59/300\n",
      "Average training loss: 0.044296299298604326\n",
      "Average test loss: 0.0025486394226964977\n",
      "Epoch 60/300\n",
      "Average training loss: 0.04424566198388735\n",
      "Average test loss: 0.0026271683745500113\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04423744682802094\n",
      "Average test loss: 0.00239556590312471\n",
      "Epoch 62/300\n",
      "Average training loss: 0.04405224333206813\n",
      "Average test loss: 0.0023672591449899806\n",
      "Epoch 63/300\n",
      "Average training loss: 0.044669010718663534\n",
      "Average test loss: 0.002373948894234167\n",
      "Epoch 64/300\n",
      "Average training loss: 0.04430698681208822\n",
      "Average test loss: 0.002392547951183385\n",
      "Epoch 65/300\n",
      "Average training loss: 0.04382254072692659\n",
      "Average test loss: 0.002426151803591185\n",
      "Epoch 66/300\n",
      "Average training loss: 0.04382954284383191\n",
      "Average test loss: 0.002354412982033359\n",
      "Epoch 67/300\n",
      "Average training loss: 0.04370381310582161\n",
      "Average test loss: 0.0024965495254016586\n",
      "Epoch 68/300\n",
      "Average training loss: 0.043758229659663306\n",
      "Average test loss: 0.002411983968690038\n",
      "Epoch 69/300\n",
      "Average training loss: 0.0436283957792653\n",
      "Average test loss: 0.0024063508985564114\n",
      "Epoch 70/300\n",
      "Average training loss: 0.043518940154049134\n",
      "Average test loss: 0.002499977949178881\n",
      "Epoch 71/300\n",
      "Average training loss: 0.04348816874292162\n",
      "Average test loss: 0.002352231741986341\n",
      "Epoch 72/300\n",
      "Average training loss: 0.043356714795033134\n",
      "Average test loss: 0.0023589301349388227\n",
      "Epoch 73/300\n",
      "Average training loss: 0.04346328695615133\n",
      "Average test loss: 0.0024395982035332257\n",
      "Epoch 74/300\n",
      "Average training loss: 0.04329821607967218\n",
      "Average test loss: 0.0024802399170067574\n",
      "Epoch 75/300\n",
      "Average training loss: 0.043317581872145336\n",
      "Average test loss: 0.002341498195090228\n",
      "Epoch 76/300\n",
      "Average training loss: 0.04319710977541076\n",
      "Average test loss: 0.0024196077812876967\n",
      "Epoch 77/300\n",
      "Average training loss: 0.04311483503712548\n",
      "Average test loss: 0.0026548558432194923\n",
      "Epoch 78/300\n",
      "Average training loss: 0.0430539392332236\n",
      "Average test loss: 0.002395841187487046\n",
      "Epoch 79/300\n",
      "Average training loss: 0.04301926516327593\n",
      "Average test loss: 0.0027034442325433094\n",
      "Epoch 80/300\n",
      "Average training loss: 0.04306015298101637\n",
      "Average test loss: 0.002384026429926356\n",
      "Epoch 81/300\n",
      "Average training loss: 0.04283514098988639\n",
      "Average test loss: 0.0023458305179244942\n",
      "Epoch 82/300\n",
      "Average training loss: 0.042920892597900495\n",
      "Average test loss: 0.002456385025133689\n",
      "Epoch 83/300\n",
      "Average training loss: 0.042837529400984446\n",
      "Average test loss: 0.002390383513437377\n",
      "Epoch 84/300\n",
      "Average training loss: 0.04277323379119237\n",
      "Average test loss: 0.0023392416298803355\n",
      "Epoch 85/300\n",
      "Average training loss: 0.04274994107749727\n",
      "Average test loss: 0.0028897703842570384\n",
      "Epoch 86/300\n",
      "Average training loss: 0.043031424538956746\n",
      "Average test loss: 0.00253459837867154\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0428932645254665\n",
      "Average test loss: 0.0023464206092887454\n",
      "Epoch 88/300\n",
      "Average training loss: 0.04250858893328243\n",
      "Average test loss: 0.0023405020982027055\n",
      "Epoch 89/300\n",
      "Average training loss: 0.04239526182744238\n",
      "Average test loss: 0.0026836765089796647\n",
      "Epoch 90/300\n",
      "Average training loss: 0.04249421996374925\n",
      "Average test loss: 0.0023470854287346203\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04249739349881808\n",
      "Average test loss: 0.002330145654785964\n",
      "Epoch 92/300\n",
      "Average training loss: 0.042366910603311325\n",
      "Average test loss: 0.0023713292645083533\n",
      "Epoch 93/300\n",
      "Average training loss: 0.04239301233159171\n",
      "Average test loss: 0.002784474232337541\n",
      "Epoch 94/300\n",
      "Average training loss: 0.042267641421821385\n",
      "Average test loss: 0.0023879914064374235\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04232624863915973\n",
      "Average test loss: 0.002351104576451083\n",
      "Epoch 96/300\n",
      "Average training loss: 0.04225530109140608\n",
      "Average test loss: 0.0023311801217496394\n",
      "Epoch 97/300\n",
      "Average training loss: 0.04227808349000083\n",
      "Average test loss: 0.0023273822892871166\n",
      "Epoch 98/300\n",
      "Average training loss: 0.042196052104234695\n",
      "Average test loss: 0.0030175352580845355\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04209675770998001\n",
      "Average test loss: 0.00233101962486075\n",
      "Epoch 100/300\n",
      "Average training loss: 0.042116640985012055\n",
      "Average test loss: 0.0023624605072869193\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04206409208642112\n",
      "Average test loss: 0.0024456236012693907\n",
      "Epoch 102/300\n",
      "Average training loss: 0.04196056415968471\n",
      "Average test loss: 0.002856540159839723\n",
      "Epoch 103/300\n",
      "Average training loss: 0.042090419885185026\n",
      "Average test loss: 0.0027157994127935832\n",
      "Epoch 104/300\n",
      "Average training loss: 0.041897491508060034\n",
      "Average test loss: 0.002358366099703643\n",
      "Epoch 105/300\n",
      "Average training loss: 0.042147072702646254\n",
      "Average test loss: 0.002368068875848419\n",
      "Epoch 106/300\n",
      "Average training loss: 0.041897889610793854\n",
      "Average test loss: 0.002549766352607144\n",
      "Epoch 107/300\n",
      "Average training loss: 0.041955639590819674\n",
      "Average test loss: 0.0023765842457198435\n",
      "Epoch 108/300\n",
      "Average training loss: 0.04203265424569448\n",
      "Average test loss: 0.004388952329547869\n",
      "Epoch 109/300\n",
      "Average training loss: 0.041742711471186744\n",
      "Average test loss: 0.002325071380784114\n",
      "Epoch 110/300\n",
      "Average training loss: 0.04166019221478039\n",
      "Average test loss: 0.002362047746156653\n",
      "Epoch 111/300\n",
      "Average training loss: 0.041731141825517015\n",
      "Average test loss: 0.0023881292854332262\n",
      "Epoch 112/300\n",
      "Average training loss: 0.04178780859377649\n",
      "Average test loss: 0.0023339725544469225\n",
      "Epoch 113/300\n",
      "Average training loss: 0.041636888960997265\n",
      "Average test loss: 0.002363249332126644\n",
      "Epoch 114/300\n",
      "Average training loss: 0.04164178707864549\n",
      "Average test loss: 0.002324723568434517\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04152060489190949\n",
      "Average test loss: 0.002542453345325258\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04155852098928557\n",
      "Average test loss: 0.0023861129445334277\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04145149169365565\n",
      "Average test loss: 0.00240352874290612\n",
      "Epoch 118/300\n",
      "Average training loss: 0.04153543899125523\n",
      "Average test loss: 0.002334195888083842\n",
      "Epoch 119/300\n",
      "Average training loss: 0.04180536539355914\n",
      "Average test loss: 0.002659818103950885\n",
      "Epoch 120/300\n",
      "Average training loss: 0.0417563193904029\n",
      "Average test loss: 0.0024050923047794237\n",
      "Epoch 121/300\n",
      "Average training loss: 0.04135381109515826\n",
      "Average test loss: 0.0025457645321471825\n",
      "Epoch 122/300\n",
      "Average training loss: 0.04126887487371762\n",
      "Average test loss: 0.0025373897224457726\n",
      "Epoch 123/300\n",
      "Average training loss: 0.04142895845572154\n",
      "Average test loss: 0.0025686443907519184\n",
      "Epoch 124/300\n",
      "Average training loss: 0.041247103916274175\n",
      "Average test loss: 4.327572242101033\n",
      "Epoch 125/300\n",
      "Average training loss: 0.04128452136119207\n",
      "Average test loss: 0.002441672735537092\n",
      "Epoch 126/300\n",
      "Average training loss: 0.041182712846332126\n",
      "Average test loss: 0.002392364547898372\n",
      "Epoch 127/300\n",
      "Average training loss: 0.04123941518531905\n",
      "Average test loss: 0.002346922201725344\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0411515072716607\n",
      "Average test loss: 0.002378367721620533\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04113673390944799\n",
      "Average test loss: 0.00434727232116792\n",
      "Epoch 130/300\n",
      "Average training loss: 0.041160799235105516\n",
      "Average test loss: 0.006894033737480641\n",
      "Epoch 131/300\n",
      "Average training loss: 0.0413036200205485\n",
      "Average test loss: 0.002440195987621943\n",
      "Epoch 132/300\n",
      "Average training loss: 0.04127350078854296\n",
      "Average test loss: 0.003147923112743431\n",
      "Epoch 133/300\n",
      "Average training loss: 0.041242131127251516\n",
      "Average test loss: 0.002431690229102969\n",
      "Epoch 134/300\n",
      "Average training loss: 0.04107122520605723\n",
      "Average test loss: 0.002400660356403225\n",
      "Epoch 135/300\n",
      "Average training loss: 0.04095666931072871\n",
      "Average test loss: 0.06408107282055749\n",
      "Epoch 136/300\n",
      "Average training loss: 0.041015609257751044\n",
      "Average test loss: 0.0023876569730540117\n",
      "Epoch 137/300\n",
      "Average training loss: 0.04095285418960783\n",
      "Average test loss: 0.00256151556265023\n",
      "Epoch 138/300\n",
      "Average training loss: 0.04086381267839008\n",
      "Average test loss: 0.002339143082085583\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04087362499700652\n",
      "Average test loss: 0.002342032609714402\n",
      "Epoch 140/300\n",
      "Average training loss: 0.04087900571028392\n",
      "Average test loss: 0.002410633984953165\n",
      "Epoch 141/300\n",
      "Average training loss: 0.0411180554330349\n",
      "Average test loss: 0.002444504479360249\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04082965867718061\n",
      "Average test loss: 0.0023531450756515064\n",
      "Epoch 143/300\n",
      "Average training loss: 0.04098801471789678\n",
      "Average test loss: 0.0024648379102970163\n",
      "Epoch 144/300\n",
      "Average training loss: 0.040747724102603064\n",
      "Average test loss: 0.002350496796063251\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04077523347735405\n",
      "Average test loss: 0.002565249244785971\n",
      "Epoch 146/300\n",
      "Average training loss: 0.0408719450864527\n",
      "Average test loss: 0.002384715444718798\n",
      "Epoch 147/300\n",
      "Average training loss: 0.04062857068247265\n",
      "Average test loss: 0.0023801564750158123\n",
      "Epoch 148/300\n",
      "Average training loss: 0.040775707953506046\n",
      "Average test loss: 0.0023991714616616567\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04062320487035646\n",
      "Average test loss: 0.0023527627914316125\n",
      "Epoch 150/300\n",
      "Average training loss: 0.040683618783950805\n",
      "Average test loss: 0.0024280235959837832\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04075922421614329\n",
      "Average test loss: 0.002425023911210398\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04055569051206112\n",
      "Average test loss: 0.0023908928260207175\n",
      "Epoch 153/300\n",
      "Average training loss: 0.040579724490642545\n",
      "Average test loss: 0.00451917796002494\n",
      "Epoch 154/300\n",
      "Average training loss: 0.04050003529257244\n",
      "Average test loss: 0.0024605769465367\n",
      "Epoch 155/300\n",
      "Average training loss: 0.04052878832817078\n",
      "Average test loss: 0.0023885228337926995\n",
      "Epoch 156/300\n",
      "Average training loss: 0.040558357649379304\n",
      "Average test loss: 0.0024123607704208956\n",
      "Epoch 157/300\n",
      "Average training loss: 0.04056903560625182\n",
      "Average test loss: 0.0023441143958932826\n",
      "Epoch 158/300\n",
      "Average training loss: 0.04042733242445522\n",
      "Average test loss: 0.0023787043208463323\n",
      "Epoch 159/300\n",
      "Average training loss: 0.040438990144266024\n",
      "Average test loss: 0.0024290662358204523\n",
      "Epoch 160/300\n",
      "Average training loss: 0.04041724234488275\n",
      "Average test loss: 0.0023831362517343626\n",
      "Epoch 161/300\n",
      "Average training loss: 0.040420271848638854\n",
      "Average test loss: 0.0023773421537544993\n",
      "Epoch 162/300\n",
      "Average training loss: 0.040595301611555945\n",
      "Average test loss: 0.002674108948144648\n",
      "Epoch 163/300\n",
      "Average training loss: 0.04051200508740213\n",
      "Average test loss: 0.0024568612116078535\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04043116733762953\n",
      "Average test loss: 0.002386958522722125\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04038584699895647\n",
      "Average test loss: 0.0025826791620088946\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04045437138941553\n",
      "Average test loss: 0.0027201133173786933\n",
      "Epoch 167/300\n",
      "Average training loss: 0.040338620669311945\n",
      "Average test loss: 0.00238040167817639\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04021178108122614\n",
      "Average test loss: 0.0023409997747383185\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04022932697667016\n",
      "Average test loss: 0.0023857125762022204\n",
      "Epoch 170/300\n",
      "Average training loss: 0.04028095343377855\n",
      "Average test loss: 0.0025081235240730975\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04038739588028855\n",
      "Average test loss: 0.0024629221744835376\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04023823298348321\n",
      "Average test loss: 0.0025819146477927763\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04017307858996921\n",
      "Average test loss: 0.0024075884457884565\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04011062646574444\n",
      "Average test loss: 0.002380459577466051\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04012854815191693\n",
      "Average test loss: 0.0023608060795813797\n",
      "Epoch 176/300\n",
      "Average training loss: 0.04015975789891349\n",
      "Average test loss: 0.0023748084381222724\n",
      "Epoch 177/300\n",
      "Average training loss: 0.040193465845452414\n",
      "Average test loss: 0.002372849521330661\n",
      "Epoch 178/300\n",
      "Average training loss: 0.040078475967049595\n",
      "Average test loss: 0.00291917359477116\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04010845181842645\n",
      "Average test loss: 0.0026079352313859595\n",
      "Epoch 180/300\n",
      "Average training loss: 0.040017352471748986\n",
      "Average test loss: 0.0028268932286236023\n",
      "Epoch 181/300\n",
      "Average training loss: 0.04000511663489872\n",
      "Average test loss: 0.0025356961954385044\n",
      "Epoch 182/300\n",
      "Average training loss: 0.04006365442607138\n",
      "Average test loss: 0.0025633816396196684\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04035726970434189\n",
      "Average test loss: 0.00283129535191175\n",
      "Epoch 184/300\n",
      "Average training loss: 0.0400270034107897\n",
      "Average test loss: 0.0023547306451946496\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03993544462323189\n",
      "Average test loss: 0.002470758450114065\n",
      "Epoch 186/300\n",
      "Average training loss: 0.039894346882899605\n",
      "Average test loss: 0.0023986064354992575\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03988724182546139\n",
      "Average test loss: 0.003412993411637015\n",
      "Epoch 188/300\n",
      "Average training loss: 0.039973984933561746\n",
      "Average test loss: 0.002358268638762335\n",
      "Epoch 189/300\n",
      "Average training loss: 0.03995532210336791\n",
      "Average test loss: 0.004157602138403389\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03995122927427292\n",
      "Average test loss: 0.0024098770626717145\n",
      "Epoch 191/300\n",
      "Average training loss: 0.04170467733177874\n",
      "Average test loss: 0.0025210235917733775\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03991099012725883\n",
      "Average test loss: 0.002391158985801869\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04007487498720487\n",
      "Average test loss: 0.0023865572890887656\n",
      "Epoch 194/300\n",
      "Average training loss: 0.03980314218666818\n",
      "Average test loss: 0.002372254241257906\n",
      "Epoch 195/300\n",
      "Average training loss: 0.03969404697418213\n",
      "Average test loss: 0.002363733651737372\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03973577474885517\n",
      "Average test loss: 0.0028184724087930387\n",
      "Epoch 197/300\n",
      "Average training loss: 0.039769718190034234\n",
      "Average test loss: 0.002383555274249779\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0397998842563894\n",
      "Average test loss: 0.0024028458123405774\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03990805736846394\n",
      "Average test loss: 0.0024567600237205625\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03969882233606444\n",
      "Average test loss: 0.002372193204239011\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03971276900834508\n",
      "Average test loss: 0.0023960435434968934\n",
      "Epoch 202/300\n",
      "Average training loss: 0.040155570824941\n",
      "Average test loss: 0.0023971949451499514\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03985713387694624\n",
      "Average test loss: 0.002532673612443937\n",
      "Epoch 204/300\n",
      "Average training loss: 0.039598672952916886\n",
      "Average test loss: 0.0024627942106583054\n",
      "Epoch 205/300\n",
      "Average training loss: 0.03971574439936214\n",
      "Average test loss: 0.002886739241373208\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03959039336277379\n",
      "Average test loss: 0.0024265666782028144\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03965336555904812\n",
      "Average test loss: 0.002436763021474083\n",
      "Epoch 208/300\n",
      "Average training loss: 0.03964457209904989\n",
      "Average test loss: 0.0024561130000899235\n",
      "Epoch 209/300\n",
      "Average training loss: 0.03961856653624111\n",
      "Average test loss: 0.002391838599410322\n",
      "Epoch 210/300\n",
      "Average training loss: 0.039638384938240054\n",
      "Average test loss: 0.002445511623389191\n",
      "Epoch 211/300\n",
      "Average training loss: 0.03957495155764951\n",
      "Average test loss: 0.002387110425469776\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03954151198267937\n",
      "Average test loss: 0.0023734832039723794\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0406198070579105\n",
      "Average test loss: 0.002466841540195876\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03952981602814463\n",
      "Average test loss: 0.002419949281960726\n",
      "Epoch 215/300\n",
      "Average training loss: 0.03937352988123894\n",
      "Average test loss: 0.0027626256493644584\n",
      "Epoch 216/300\n",
      "Average training loss: 0.039479446075028846\n",
      "Average test loss: 0.0026049430867036184\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03944523377550973\n",
      "Average test loss: 0.0026927080270316866\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03948573464817471\n",
      "Average test loss: 0.0025890586574872335\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03938566173447503\n",
      "Average test loss: 0.002376532868585653\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03953807057440281\n",
      "Average test loss: 0.0025315977943440276\n",
      "Epoch 221/300\n",
      "Average training loss: 0.039494816535049014\n",
      "Average test loss: 0.002467480089722408\n",
      "Epoch 222/300\n",
      "Average training loss: 0.039909490128358205\n",
      "Average test loss: 0.0024067643257892793\n",
      "Epoch 223/300\n",
      "Average training loss: 0.03936168794168366\n",
      "Average test loss: 0.002466992962691519\n",
      "Epoch 224/300\n",
      "Average training loss: 0.039398123645120194\n",
      "Average test loss: 0.0024488877885871464\n",
      "Epoch 225/300\n",
      "Average training loss: 0.039306510696808496\n",
      "Average test loss: 5601.410266031901\n",
      "Epoch 226/300\n",
      "Average training loss: 0.03937509474489424\n",
      "Average test loss: 0.0024250807931853664\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03943014930023087\n",
      "Average test loss: 0.0025737436035027108\n",
      "Epoch 228/300\n",
      "Average training loss: 0.03935261106491089\n",
      "Average test loss: 0.002506372516974807\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03941739324728648\n",
      "Average test loss: 0.0036570779089298514\n",
      "Epoch 230/300\n",
      "Average training loss: 0.03928600168890423\n",
      "Average test loss: 0.0025188999929361875\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03934038430452347\n",
      "Average test loss: 0.002417186565283272\n",
      "Epoch 232/300\n",
      "Average training loss: 0.03960030417309867\n",
      "Average test loss: 0.0024180807767228947\n",
      "Epoch 233/300\n",
      "Average training loss: 0.039172669135861926\n",
      "Average test loss: 0.0024212723508891133\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03939059642619557\n",
      "Average test loss: 0.0024089095385538208\n",
      "Epoch 235/300\n",
      "Average training loss: 0.03930571467512184\n",
      "Average test loss: 0.0024205872358547317\n",
      "Epoch 236/300\n",
      "Average training loss: 0.039356576705972354\n",
      "Average test loss: 0.002498630312995778\n",
      "Epoch 237/300\n",
      "Average training loss: 0.03924856762422456\n",
      "Average test loss: 0.0024517901393895348\n",
      "Epoch 238/300\n",
      "Average training loss: 0.039252410949932204\n",
      "Average test loss: 0.0025653204414993525\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03920387047694789\n",
      "Average test loss: 0.0025011445097625254\n",
      "Epoch 240/300\n",
      "Average training loss: 0.039419984448287225\n",
      "Average test loss: 0.0024504454146242805\n",
      "Epoch 241/300\n",
      "Average training loss: 0.03914490649435255\n",
      "Average test loss: 0.003491819438834985\n",
      "Epoch 242/300\n",
      "Average training loss: 0.039189697639809715\n",
      "Average test loss: 0.00241679585704373\n",
      "Epoch 243/300\n",
      "Average training loss: 0.0391219668785731\n",
      "Average test loss: 0.002391866642774807\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03931724503967497\n",
      "Average test loss: 0.002387757335892982\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0391385255323516\n",
      "Average test loss: 0.0024847173732188014\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03918028486106131\n",
      "Average test loss: 0.0024239912811252805\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03912490850687027\n",
      "Average test loss: 0.002618840342801478\n",
      "Epoch 248/300\n",
      "Average training loss: 0.039120516515440414\n",
      "Average test loss: 0.0023956662985599704\n",
      "Epoch 249/300\n",
      "Average training loss: 0.039181015531222024\n",
      "Average test loss: 0.00244007244023184\n",
      "Epoch 250/300\n",
      "Average training loss: 0.039037863807545765\n",
      "Average test loss: 0.0023880189328143995\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03924993767672115\n",
      "Average test loss: 0.002399170428928402\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03904565855364005\n",
      "Average test loss: 0.0024857085425820614\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03903491868575414\n",
      "Average test loss: 0.0024191980550272597\n",
      "Epoch 254/300\n",
      "Average training loss: 0.039402971780962415\n",
      "Average test loss: 0.002434921541147762\n",
      "Epoch 255/300\n",
      "Average training loss: 0.0389841036564774\n",
      "Average test loss: 0.0027675642768541973\n",
      "Epoch 256/300\n",
      "Average training loss: 0.039112507618135874\n",
      "Average test loss: 0.0026380855052007567\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03903323202000724\n",
      "Average test loss: 0.0025070813278564147\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03895056066910426\n",
      "Average test loss: 0.0023948160712089803\n",
      "Epoch 259/300\n",
      "Average training loss: 0.039096743166446686\n",
      "Average test loss: 0.0024272904260497956\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03891889094312986\n",
      "Average test loss: 0.0025495018877295986\n",
      "Epoch 261/300\n",
      "Average training loss: 0.038986736314164265\n",
      "Average test loss: 0.00243625706827475\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03911717246472836\n",
      "Average test loss: 0.0024092090183662042\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03897981192668279\n",
      "Average test loss: 0.0024101186831378273\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03910134099258317\n",
      "Average test loss: 0.002534870250150561\n",
      "Epoch 265/300\n",
      "Average training loss: 0.03896323483188947\n",
      "Average test loss: 0.002677153038378391\n",
      "Epoch 266/300\n",
      "Average training loss: 0.039005049145883985\n",
      "Average test loss: 0.0023930464742912185\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03890234224332703\n",
      "Average test loss: 0.0026246098273744185\n",
      "Epoch 268/300\n",
      "Average training loss: 0.038935394641425876\n",
      "Average test loss: 0.002410237318318751\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03916097015142441\n",
      "Average test loss: 0.0024840579668266908\n",
      "Epoch 270/300\n",
      "Average training loss: 0.03901939130822817\n",
      "Average test loss: 0.0023950056166698536\n",
      "Epoch 271/300\n",
      "Average training loss: 0.038897985776265465\n",
      "Average test loss: 0.002901407066732645\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03887311757935418\n",
      "Average test loss: 0.002410346069683631\n",
      "Epoch 273/300\n",
      "Average training loss: 0.038798243019315934\n",
      "Average test loss: 0.002420471145461003\n",
      "Epoch 274/300\n",
      "Average training loss: 0.03891605545083682\n",
      "Average test loss: 0.0024203829794294304\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03896870090232955\n",
      "Average test loss: 0.0024095515956481296\n",
      "Epoch 276/300\n",
      "Average training loss: 0.038800965908500885\n",
      "Average test loss: 0.029770349049423304\n",
      "Epoch 277/300\n",
      "Average training loss: 0.038853527598910864\n",
      "Average test loss: 0.0024183510813034245\n",
      "Epoch 278/300\n",
      "Average training loss: 0.038832944623298116\n",
      "Average test loss: 0.004021726312322749\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0389213358014822\n",
      "Average test loss: 0.002477072222572234\n",
      "Epoch 280/300\n",
      "Average training loss: 0.038827396866348056\n",
      "Average test loss: 0.0025347614371114308\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03880371293094423\n",
      "Average test loss: 0.002430890747035543\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03882426373826133\n",
      "Average test loss: 0.002432678942879041\n",
      "Epoch 283/300\n",
      "Average training loss: 0.038759289234876634\n",
      "Average test loss: 0.0024670902509242297\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03884800860948033\n",
      "Average test loss: 0.0024935653996136452\n",
      "Epoch 285/300\n",
      "Average training loss: 0.038789917280276615\n",
      "Average test loss: 0.0024417263739224936\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03881224393182331\n",
      "Average test loss: 0.004098233816420866\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03881240809294913\n",
      "Average test loss: 0.002436203435270323\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03877947833471828\n",
      "Average test loss: 0.0024152966783278517\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03874975291887919\n",
      "Average test loss: 0.002601200620747275\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03876933631632063\n",
      "Average test loss: 0.002451603578403592\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03863504649533166\n",
      "Average test loss: 0.0025612353016104964\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03885000785854128\n",
      "Average test loss: 0.0024289926603022548\n",
      "Epoch 293/300\n",
      "Average training loss: 0.038705769663055736\n",
      "Average test loss: 0.002468438701497184\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03873687982890341\n",
      "Average test loss: 0.00311765635986295\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03875963414708773\n",
      "Average test loss: 0.0024339001996235717\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04043758486708005\n",
      "Average test loss: 0.00243217648131152\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03852108766304122\n",
      "Average test loss: 0.0024756369485209387\n",
      "Epoch 298/300\n",
      "Average training loss: 0.038573468191756145\n",
      "Average test loss: 0.00239970951858494\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03856814176175329\n",
      "Average test loss: 0.0024442681113464966\n",
      "Epoch 300/300\n",
      "Average training loss: 0.03848864424228668\n",
      "Average test loss: 0.002541817353417476\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1.5326695342461267\n",
      "Average test loss: 0.005866226889193058\n",
      "Epoch 2/300\n",
      "Average training loss: 0.16859210108386147\n",
      "Average test loss: 0.004877244363642401\n",
      "Epoch 3/300\n",
      "Average training loss: 0.1204514071808921\n",
      "Average test loss: 0.004029503482911321\n",
      "Epoch 4/300\n",
      "Average training loss: 0.10159232377343708\n",
      "Average test loss: 0.004483098446495003\n",
      "Epoch 5/300\n",
      "Average training loss: 0.09074581414461136\n",
      "Average test loss: 0.00372355768415663\n",
      "Epoch 6/300\n",
      "Average training loss: 0.08359235922164387\n",
      "Average test loss: 0.0035283863616900313\n",
      "Epoch 7/300\n",
      "Average training loss: 0.07840754917595122\n",
      "Average test loss: 0.003501536142287983\n",
      "Epoch 8/300\n",
      "Average training loss: 0.07441811052958171\n",
      "Average test loss: 0.0031984914150089026\n",
      "Epoch 9/300\n",
      "Average training loss: 0.07126774193843206\n",
      "Average test loss: 0.003364484975942307\n",
      "Epoch 10/300\n",
      "Average training loss: 0.0680237158868048\n",
      "Average test loss: 0.002920665907363097\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0653675993680954\n",
      "Average test loss: 0.003176146273811658\n",
      "Epoch 12/300\n",
      "Average training loss: 0.06386826441023084\n",
      "Average test loss: 0.0030538660461703936\n",
      "Epoch 13/300\n",
      "Average training loss: 0.061517964214086535\n",
      "Average test loss: 0.002773644112671415\n",
      "Epoch 14/300\n",
      "Average training loss: 0.05936780143777529\n",
      "Average test loss: 0.0031353404557125436\n",
      "Epoch 15/300\n",
      "Average training loss: 0.05738774387372864\n",
      "Average test loss: 0.003004073394048545\n",
      "Epoch 16/300\n",
      "Average training loss: 0.05571556382377942\n",
      "Average test loss: 0.003108600398939517\n",
      "Epoch 17/300\n",
      "Average training loss: 0.054420504848162334\n",
      "Average test loss: 0.003159043688740995\n",
      "Epoch 18/300\n",
      "Average training loss: 0.05289897805452347\n",
      "Average test loss: 0.002590096469347676\n",
      "Epoch 19/300\n",
      "Average training loss: 0.05074851435754034\n",
      "Average test loss: 0.002442845890401966\n",
      "Epoch 20/300\n",
      "Average training loss: 0.04988054777847396\n",
      "Average test loss: 0.0023776057807521687\n",
      "Epoch 21/300\n",
      "Average training loss: 0.048460545380910235\n",
      "Average test loss: 0.0027995943187011612\n",
      "Epoch 22/300\n",
      "Average training loss: 0.0475578767226802\n",
      "Average test loss: 0.0022740970690631203\n",
      "Epoch 23/300\n",
      "Average training loss: 0.04664912072817484\n",
      "Average test loss: 0.0024837022671062086\n",
      "Epoch 24/300\n",
      "Average training loss: 0.045517703182167475\n",
      "Average test loss: 0.002278347287534012\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0445990796453423\n",
      "Average test loss: 0.0021498395998237863\n",
      "Epoch 26/300\n",
      "Average training loss: 0.043822637157307734\n",
      "Average test loss: 0.002082927871081564\n",
      "Epoch 27/300\n",
      "Average training loss: 0.04312588195999464\n",
      "Average test loss: 0.002167142178449366\n",
      "Epoch 28/300\n",
      "Average training loss: 0.04256090648306741\n",
      "Average test loss: 0.002126604702530636\n",
      "Epoch 29/300\n",
      "Average training loss: 0.041893084834019344\n",
      "Average test loss: 0.002139664743613038\n",
      "Epoch 30/300\n",
      "Average training loss: 0.04127033864127265\n",
      "Average test loss: 0.002060572561290529\n",
      "Epoch 31/300\n",
      "Average training loss: 0.04095013855894407\n",
      "Average test loss: 0.0020140816864247122\n",
      "Epoch 32/300\n",
      "Average training loss: 0.04036511859297753\n",
      "Average test loss: 0.0019471767658574713\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03983758092588849\n",
      "Average test loss: 0.0020181217963496844\n",
      "Epoch 34/300\n",
      "Average training loss: 0.039456366462839974\n",
      "Average test loss: 0.0019449663889697856\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03910883846216732\n",
      "Average test loss: 0.001931728713007437\n",
      "Epoch 36/300\n",
      "Average training loss: 0.0388073104818662\n",
      "Average test loss: 0.001999674309665958\n",
      "Epoch 37/300\n",
      "Average training loss: 0.03841062314477232\n",
      "Average test loss: 0.0018866127493480842\n",
      "Epoch 38/300\n",
      "Average training loss: 0.038214966148138046\n",
      "Average test loss: 0.001884969883908828\n",
      "Epoch 39/300\n",
      "Average training loss: 0.03799698434273402\n",
      "Average test loss: 0.001909582196838326\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03767746299505234\n",
      "Average test loss: 0.0018880543511153923\n",
      "Epoch 41/300\n",
      "Average training loss: 0.03746939166718059\n",
      "Average test loss: 0.0018604544630895059\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03728074285222424\n",
      "Average test loss: 0.0018281603522805705\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0370897727906704\n",
      "Average test loss: 0.002084030745861431\n",
      "Epoch 44/300\n",
      "Average training loss: 0.036823449343442914\n",
      "Average test loss: 0.0018679866475156612\n",
      "Epoch 45/300\n",
      "Average training loss: 0.03694203206234508\n",
      "Average test loss: 0.0018225755840539932\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03650924982296096\n",
      "Average test loss: 0.0018441760385822918\n",
      "Epoch 47/300\n",
      "Average training loss: 0.036315461301141314\n",
      "Average test loss: 0.005322870632840528\n",
      "Epoch 48/300\n",
      "Average training loss: 0.036357753492063945\n",
      "Average test loss: 0.0018372339668373267\n",
      "Epoch 49/300\n",
      "Average training loss: 0.036535054286321006\n",
      "Average test loss: 0.0018326721895072196\n",
      "Epoch 50/300\n",
      "Average training loss: 0.03598092732495732\n",
      "Average test loss: 0.0018156363951663177\n",
      "Epoch 51/300\n",
      "Average training loss: 0.03591553220318423\n",
      "Average test loss: 0.0018715515157414807\n",
      "Epoch 52/300\n",
      "Average training loss: 0.035809050172567365\n",
      "Average test loss: 0.004854786613335212\n",
      "Epoch 53/300\n",
      "Average training loss: 0.03578096032804913\n",
      "Average test loss: 0.0018071878471722206\n",
      "Epoch 54/300\n",
      "Average training loss: 0.03560787662863731\n",
      "Average test loss: 0.001782472740444872\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03547372252742449\n",
      "Average test loss: 0.0018141521726631456\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03540257538027233\n",
      "Average test loss: 0.00177012601453397\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03531059101555083\n",
      "Average test loss: 0.0017884077500138018\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03526050849424468\n",
      "Average test loss: 0.0018038923707273272\n",
      "Epoch 59/300\n",
      "Average training loss: 0.035074816869364846\n",
      "Average test loss: 0.0023012595882432327\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0351044450600942\n",
      "Average test loss: 0.0020567572882605922\n",
      "Epoch 61/300\n",
      "Average training loss: 0.03501068651345041\n",
      "Average test loss: 0.0017824567973406778\n",
      "Epoch 62/300\n",
      "Average training loss: 0.034930362366967734\n",
      "Average test loss: 0.001797505139062802\n",
      "Epoch 63/300\n",
      "Average training loss: 0.035156310071547826\n",
      "Average test loss: 0.0017573025290750794\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03482673160235087\n",
      "Average test loss: 0.0018034557425934408\n",
      "Epoch 65/300\n",
      "Average training loss: 0.034689037223656974\n",
      "Average test loss: 0.001763001676959296\n",
      "Epoch 66/300\n",
      "Average training loss: 0.034649111085467865\n",
      "Average test loss: 0.0018371645603328943\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03459096634056833\n",
      "Average test loss: 0.00174524014039586\n",
      "Epoch 68/300\n",
      "Average training loss: 0.034533160338799156\n",
      "Average test loss: 0.00175393095633222\n",
      "Epoch 69/300\n",
      "Average training loss: 0.03476350103815397\n",
      "Average test loss: 0.0018362468303077752\n",
      "Epoch 70/300\n",
      "Average training loss: 0.034388642349176934\n",
      "Average test loss: 0.001947011163354748\n",
      "Epoch 71/300\n",
      "Average training loss: 0.03437799057033327\n",
      "Average test loss: 0.0017813176314553452\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03424111995763249\n",
      "Average test loss: 0.0017692681146371696\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03430857254399194\n",
      "Average test loss: 0.0017680336009297105\n",
      "Epoch 74/300\n",
      "Average training loss: 0.03420898018280665\n",
      "Average test loss: 0.001800204460405641\n",
      "Epoch 75/300\n",
      "Average training loss: 0.034166784154044255\n",
      "Average test loss: 0.0017496877091212404\n",
      "Epoch 76/300\n",
      "Average training loss: 0.034103135554326904\n",
      "Average test loss: 0.001754461932513449\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03468834452165498\n",
      "Average test loss: 0.001734939509899252\n",
      "Epoch 78/300\n",
      "Average training loss: 0.033938138822714486\n",
      "Average test loss: 0.001737091689887974\n",
      "Epoch 79/300\n",
      "Average training loss: 0.0339044466846519\n",
      "Average test loss: 0.001800509372105201\n",
      "Epoch 80/300\n",
      "Average training loss: 0.03403445663385921\n",
      "Average test loss: 0.0018036396105049386\n",
      "Epoch 81/300\n",
      "Average training loss: 0.03387520749370257\n",
      "Average test loss: 0.0017548465005432566\n",
      "Epoch 82/300\n",
      "Average training loss: 0.033847064369254644\n",
      "Average test loss: 0.0017707267589867114\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03381821094784472\n",
      "Average test loss: 0.0018871843253986703\n",
      "Epoch 84/300\n",
      "Average training loss: 0.03395826713906394\n",
      "Average test loss: 0.0017368045294036468\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03377262805402279\n",
      "Average test loss: 0.0017512279928972325\n",
      "Epoch 86/300\n",
      "Average training loss: 0.033721491081847084\n",
      "Average test loss: 0.0017565215472131967\n",
      "Epoch 87/300\n",
      "Average training loss: 0.033651941335863536\n",
      "Average test loss: 0.0017335159961755077\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03360474654701021\n",
      "Average test loss: 0.0018351712741164698\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03356874668267038\n",
      "Average test loss: 0.001913485508826044\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03360332761539353\n",
      "Average test loss: 0.0017395397652354505\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03351342561509874\n",
      "Average test loss: 0.0017647254038602113\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03378509496483538\n",
      "Average test loss: 0.0017329980365725028\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03336946584781011\n",
      "Average test loss: 0.001732040492300358\n",
      "Epoch 94/300\n",
      "Average training loss: 0.03339544894960191\n",
      "Average test loss: 0.001788882342270679\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03338798773123158\n",
      "Average test loss: 0.0017640837828318278\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03334397840168741\n",
      "Average test loss: 0.00172812189348042\n",
      "Epoch 97/300\n",
      "Average training loss: 0.033405087585250536\n",
      "Average test loss: 0.0017469530796839132\n",
      "Epoch 98/300\n",
      "Average training loss: 0.033301575822962656\n",
      "Average test loss: 0.0017380884983059432\n",
      "Epoch 99/300\n",
      "Average training loss: 0.0332656278444661\n",
      "Average test loss: 0.001754423082081808\n",
      "Epoch 100/300\n",
      "Average training loss: 0.033238580607705646\n",
      "Average test loss: 0.0017273536811893185\n",
      "Epoch 101/300\n",
      "Average training loss: 0.03312319778402646\n",
      "Average test loss: 0.0017612876585788198\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0332473161816597\n",
      "Average test loss: 0.001764728076962961\n",
      "Epoch 103/300\n",
      "Average training loss: 0.03311482691764832\n",
      "Average test loss: 0.0017316820141341951\n",
      "Epoch 104/300\n",
      "Average training loss: 0.033075380070341956\n",
      "Average test loss: 0.001948303619089226\n",
      "Epoch 105/300\n",
      "Average training loss: 0.033078736441002954\n",
      "Average test loss: 0.0017454058111955723\n",
      "Epoch 106/300\n",
      "Average training loss: 0.0331097652796242\n",
      "Average test loss: 0.0019736019174257913\n",
      "Epoch 107/300\n",
      "Average training loss: 0.03308973506258594\n",
      "Average test loss: 0.0018837016868508524\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03305452200770378\n",
      "Average test loss: 0.005359172005827228\n",
      "Epoch 109/300\n",
      "Average training loss: 0.03306856131884787\n",
      "Average test loss: 0.0018211169000715017\n",
      "Epoch 110/300\n",
      "Average training loss: 0.03293275592393345\n",
      "Average test loss: 0.0017398766360970006\n",
      "Epoch 111/300\n",
      "Average training loss: 0.0330210046817859\n",
      "Average test loss: 0.001936400294303894\n",
      "Epoch 112/300\n",
      "Average training loss: 0.0329409706360764\n",
      "Average test loss: 0.0018919864913655652\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03288047817349434\n",
      "Average test loss: 0.0018992232342975007\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03286131684647666\n",
      "Average test loss: 0.08270514081418515\n",
      "Epoch 115/300\n",
      "Average training loss: 0.032807980805635455\n",
      "Average test loss: 0.001737030369747016\n",
      "Epoch 116/300\n",
      "Average training loss: 0.03314710558123059\n",
      "Average test loss: 0.0017815977927918235\n",
      "Epoch 117/300\n",
      "Average training loss: 0.03275317776203156\n",
      "Average test loss: 0.0017734599721928438\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03276543144053883\n",
      "Average test loss: 0.0017327863142515222\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03273959942658742\n",
      "Average test loss: 0.0017720404234197404\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032753464998470413\n",
      "Average test loss: 0.002285582142985529\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03273635712265968\n",
      "Average test loss: 0.0017872066684067249\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03266022183828884\n",
      "Average test loss: 0.012850791101654371\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03268237423731221\n",
      "Average test loss: 0.0017408114944895108\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03268414699700144\n",
      "Average test loss: 0.0018620843685542544\n",
      "Epoch 125/300\n",
      "Average training loss: 0.032706112066904706\n",
      "Average test loss: 0.0017292375990913974\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032569065789381665\n",
      "Average test loss: 0.0017403333776940901\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03252065787381596\n",
      "Average test loss: 0.0017627538582310081\n",
      "Epoch 128/300\n",
      "Average training loss: 0.033254260496960744\n",
      "Average test loss: 0.001802781969308853\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03255001945296923\n",
      "Average test loss: 0.0017430951063417727\n",
      "Epoch 130/300\n",
      "Average training loss: 0.032474065616726874\n",
      "Average test loss: 0.0017782620566172732\n",
      "Epoch 131/300\n",
      "Average training loss: 0.032497085548109476\n",
      "Average test loss: 0.0017401023634398976\n",
      "Epoch 132/300\n",
      "Average training loss: 0.032454632961087755\n",
      "Average test loss: 0.0017409711812312405\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03250452968643771\n",
      "Average test loss: 0.001880915491634773\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03240382414725092\n",
      "Average test loss: 0.001750373635544545\n",
      "Epoch 135/300\n",
      "Average training loss: 0.032485966131091115\n",
      "Average test loss: 0.0017704542395141389\n",
      "Epoch 136/300\n",
      "Average training loss: 0.03250153933631049\n",
      "Average test loss: 0.001748552454635501\n",
      "Epoch 137/300\n",
      "Average training loss: 0.0323703517417113\n",
      "Average test loss: 0.001755183345741696\n",
      "Epoch 138/300\n",
      "Average training loss: 0.032529638793733384\n",
      "Average test loss: 0.17663627799683146\n",
      "Epoch 139/300\n",
      "Average training loss: 0.03228627465830909\n",
      "Average test loss: 0.0017341635646815929\n",
      "Epoch 140/300\n",
      "Average training loss: 0.032359195210867456\n",
      "Average test loss: 0.0017629077008201016\n",
      "Epoch 141/300\n",
      "Average training loss: 0.03222733206384712\n",
      "Average test loss: 0.0017469796039577988\n",
      "Epoch 142/300\n",
      "Average training loss: 0.03233062477244271\n",
      "Average test loss: 0.0017533326076550617\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03229759716325336\n",
      "Average test loss: 0.0018449121760204434\n",
      "Epoch 144/300\n",
      "Average training loss: 0.032274406112730504\n",
      "Average test loss: 0.0018166138684997956\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03224336436390877\n",
      "Average test loss: 0.0017766476842678255\n",
      "Epoch 146/300\n",
      "Average training loss: 0.032175123333930966\n",
      "Average test loss: 0.0017635368306396736\n",
      "Epoch 147/300\n",
      "Average training loss: 0.03220060008101993\n",
      "Average test loss: 0.0017351669970278938\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03228792432116138\n",
      "Average test loss: 0.0018046121073679792\n",
      "Epoch 149/300\n",
      "Average training loss: 0.03214513904849688\n",
      "Average test loss: 0.0017388883963641193\n",
      "Epoch 150/300\n",
      "Average training loss: 0.032191199680169424\n",
      "Average test loss: 0.26738651504781513\n",
      "Epoch 151/300\n",
      "Average training loss: 0.032167468590868845\n",
      "Average test loss: 0.002001607889206045\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03211074547304048\n",
      "Average test loss: 0.0017515054119543896\n",
      "Epoch 153/300\n",
      "Average training loss: 0.032111986872222685\n",
      "Average test loss: 0.001842319653266006\n",
      "Epoch 154/300\n",
      "Average training loss: 0.03211322097977003\n",
      "Average test loss: 0.0017551025946935018\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03198208743168248\n",
      "Average test loss: 0.0020053747813734745\n",
      "Epoch 156/300\n",
      "Average training loss: 0.032153719196716946\n",
      "Average test loss: 0.0018310152704103125\n",
      "Epoch 157/300\n",
      "Average training loss: 0.032001724107397926\n",
      "Average test loss: 0.002093625909752316\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03203671284682221\n",
      "Average test loss: 0.0017634399530167381\n",
      "Epoch 159/300\n",
      "Average training loss: 0.032004138343864015\n",
      "Average test loss: 0.00768516846290893\n",
      "Epoch 160/300\n",
      "Average training loss: 0.0319351977871524\n",
      "Average test loss: 0.0017955062054097653\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03202873796555731\n",
      "Average test loss: 0.0017434557624575165\n",
      "Epoch 162/300\n",
      "Average training loss: 0.031992319234543376\n",
      "Average test loss: 0.0017856296085649067\n",
      "Epoch 163/300\n",
      "Average training loss: 0.032034832401408087\n",
      "Average test loss: 0.004956729417045911\n",
      "Epoch 164/300\n",
      "Average training loss: 0.03194561338590251\n",
      "Average test loss: 0.0017505135273353921\n",
      "Epoch 165/300\n",
      "Average training loss: 0.03181723439362314\n",
      "Average test loss: 0.0017805712317220039\n",
      "Epoch 166/300\n",
      "Average training loss: 0.03189069926738739\n",
      "Average test loss: 0.001759806321726905\n",
      "Epoch 167/300\n",
      "Average training loss: 0.03189641864928934\n",
      "Average test loss: 0.0017565086727134055\n",
      "Epoch 168/300\n",
      "Average training loss: 0.03185200189550718\n",
      "Average test loss: 0.0017574071154619257\n",
      "Epoch 169/300\n",
      "Average training loss: 0.03196989895403385\n",
      "Average test loss: 0.015532593904270066\n",
      "Epoch 170/300\n",
      "Average training loss: 0.03186461737917529\n",
      "Average test loss: 0.0017745405848448475\n",
      "Epoch 171/300\n",
      "Average training loss: 0.03175996796952354\n",
      "Average test loss: 0.0017447570212599304\n",
      "Epoch 172/300\n",
      "Average training loss: 0.031764171060588625\n",
      "Average test loss: 0.001748896555043757\n",
      "Epoch 173/300\n",
      "Average training loss: 0.03182241412334972\n",
      "Average test loss: 0.0022281834695281255\n",
      "Epoch 174/300\n",
      "Average training loss: 0.03171233416597048\n",
      "Average test loss: 0.0019625143901341492\n",
      "Epoch 175/300\n",
      "Average training loss: 0.03180718513660961\n",
      "Average test loss: 0.0017518211949823631\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0317418176283439\n",
      "Average test loss: 0.0017753792984618081\n",
      "Epoch 177/300\n",
      "Average training loss: 0.031715038486652906\n",
      "Average test loss: 0.0017570097934868601\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0317992182970047\n",
      "Average test loss: 0.0019374842712034782\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03170759223567115\n",
      "Average test loss: 0.001748947351757023\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03182163995173242\n",
      "Average test loss: 0.0024201856845368942\n",
      "Epoch 181/300\n",
      "Average training loss: 0.03204849244819747\n",
      "Average test loss: 0.0019227821748289797\n",
      "Epoch 182/300\n",
      "Average training loss: 0.0315887050959799\n",
      "Average test loss: 0.04669448451863395\n",
      "Epoch 183/300\n",
      "Average training loss: 0.03169027312265502\n",
      "Average test loss: 0.003468562062829733\n",
      "Epoch 184/300\n",
      "Average training loss: 0.03165868927869532\n",
      "Average test loss: 0.0018105605112181769\n",
      "Epoch 185/300\n",
      "Average training loss: 0.03156146696044339\n",
      "Average test loss: 0.0020043088032139673\n",
      "Epoch 186/300\n",
      "Average training loss: 0.03163046529226833\n",
      "Average test loss: 0.0018157348181638453\n",
      "Epoch 187/300\n",
      "Average training loss: 0.03161692119306988\n",
      "Average test loss: 0.0017855903574575981\n",
      "Epoch 188/300\n",
      "Average training loss: 0.031545705599917304\n",
      "Average test loss: 0.002173900070703692\n",
      "Epoch 189/300\n",
      "Average training loss: 0.031661304922567475\n",
      "Average test loss: 0.0018216156324164735\n",
      "Epoch 190/300\n",
      "Average training loss: 0.03160269725322724\n",
      "Average test loss: 0.001787026137734453\n",
      "Epoch 191/300\n",
      "Average training loss: 0.031508052464988494\n",
      "Average test loss: 0.0017794393098188772\n",
      "Epoch 192/300\n",
      "Average training loss: 0.03156305727362633\n",
      "Average test loss: 0.0018962289726154671\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03150308310654428\n",
      "Average test loss: 0.0017804891293247541\n",
      "Epoch 194/300\n",
      "Average training loss: 0.0315258907576402\n",
      "Average test loss: 0.0018457420800502102\n",
      "Epoch 195/300\n",
      "Average training loss: 0.031619272841347586\n",
      "Average test loss: 0.37084374872843423\n",
      "Epoch 196/300\n",
      "Average training loss: 0.03471493680940734\n",
      "Average test loss: 0.002409676287840638\n",
      "Epoch 197/300\n",
      "Average training loss: 0.031521371705664526\n",
      "Average test loss: 0.0017551630022418169\n",
      "Epoch 198/300\n",
      "Average training loss: 0.03139293709894021\n",
      "Average test loss: 0.0017892753439438013\n",
      "Epoch 199/300\n",
      "Average training loss: 0.03139294255607658\n",
      "Average test loss: 0.001982883875982629\n",
      "Epoch 200/300\n",
      "Average training loss: 0.03137996508015527\n",
      "Average test loss: 0.001773840556645559\n",
      "Epoch 201/300\n",
      "Average training loss: 0.03138641856776343\n",
      "Average test loss: 0.0036616747375163767\n",
      "Epoch 202/300\n",
      "Average training loss: 0.031363999678028956\n",
      "Average test loss: 0.00180876157950196\n",
      "Epoch 203/300\n",
      "Average training loss: 0.03146826308965683\n",
      "Average test loss: 0.0022388864958451855\n",
      "Epoch 204/300\n",
      "Average training loss: 0.03139534640974469\n",
      "Average test loss: 0.00185071308289965\n",
      "Epoch 205/300\n",
      "Average training loss: 0.031451473486092354\n",
      "Average test loss: 0.0017772851201395194\n",
      "Epoch 206/300\n",
      "Average training loss: 0.03133982394470109\n",
      "Average test loss: 0.001798885038536456\n",
      "Epoch 207/300\n",
      "Average training loss: 0.03130114424228668\n",
      "Average test loss: 0.0018250205360559953\n",
      "Epoch 208/300\n",
      "Average training loss: 0.031380750917726095\n",
      "Average test loss: 0.0018498566325546968\n",
      "Epoch 209/300\n",
      "Average training loss: 0.031319110641876854\n",
      "Average test loss: 0.0020473132400463025\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03143829639256\n",
      "Average test loss: 0.001773591913593312\n",
      "Epoch 211/300\n",
      "Average training loss: 0.031290035580595336\n",
      "Average test loss: 0.0017899885678456888\n",
      "Epoch 212/300\n",
      "Average training loss: 0.03131544300748242\n",
      "Average test loss: 3.4072210602230495\n",
      "Epoch 213/300\n",
      "Average training loss: 0.031412954166531566\n",
      "Average test loss: 0.0018397841076884004\n",
      "Epoch 214/300\n",
      "Average training loss: 0.031289078010453115\n",
      "Average test loss: 0.0023551517588396866\n",
      "Epoch 215/300\n",
      "Average training loss: 0.031495441301001445\n",
      "Average test loss: 0.0017779705439590746\n",
      "Epoch 216/300\n",
      "Average training loss: 0.031247845583491855\n",
      "Average test loss: 0.0017853885836278398\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0314191878967815\n",
      "Average test loss: 0.0017847326437218322\n",
      "Epoch 218/300\n",
      "Average training loss: 0.031221733326713245\n",
      "Average test loss: 0.0018226297799911763\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03125279440979163\n",
      "Average test loss: 0.0017902056536533767\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03119872202972571\n",
      "Average test loss: 0.0018529522876358695\n",
      "Epoch 221/300\n",
      "Average training loss: 0.03134400742087099\n",
      "Average test loss: 0.0018060313768477904\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03115382778644562\n",
      "Average test loss: 0.0019373502826525106\n",
      "Epoch 223/300\n",
      "Average test loss: 0.0017734894606595238\n",
      "Epoch 224/300\n",
      "Average training loss: 0.031285548663801614\n",
      "Average test loss: 0.0017957444832039375\n",
      "Epoch 225/300\n",
      "Average training loss: 0.031177055637041727\n",
      "Average test loss: 0.0017591408590475718\n",
      "Epoch 226/300\n",
      "Average training loss: 0.031195254120561813\n",
      "Average test loss: 0.0018578989495419793\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03116282057431009\n",
      "Average test loss: 0.0018275207572927077\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0314437648554643\n",
      "Average test loss: 0.002000891058912708\n",
      "Epoch 233/300\n",
      "Average training loss: 0.031054456018739275\n",
      "Average test loss: 0.0017846737162520488\n",
      "Epoch 234/300\n",
      "Average training loss: 0.031095336438881026\n",
      "Average test loss: 0.001798972761258483\n",
      "Epoch 235/300\n",
      "Average training loss: 0.031098487340741686\n",
      "Average test loss: 0.0017935137166641653\n",
      "Epoch 236/300\n",
      "Average training loss: 0.031031549021601677\n",
      "Average test loss: 0.004389873617225223\n",
      "Epoch 237/300\n",
      "Average training loss: 0.031209985421763526\n",
      "Average test loss: 0.001797840463411477\n",
      "Epoch 238/300\n",
      "Average training loss: 0.031085008968909582\n",
      "Average test loss: 0.001787071633670065\n",
      "Epoch 239/300\n",
      "Average training loss: 0.03113995259669092\n",
      "Average test loss: 0.00251371165468461\n",
      "Epoch 240/300\n",
      "Average training loss: 0.03111023036473327\n",
      "Average test loss: 0.0018027586285024881\n",
      "Epoch 244/300\n",
      "Average training loss: 0.031026811652713353\n",
      "Average test loss: 0.001775220794706709\n",
      "Epoch 245/300\n",
      "Average training loss: 0.030977853134274484\n",
      "Average test loss: 0.0018430748312837548\n",
      "Epoch 246/300\n",
      "Average training loss: 0.030941196473108397\n",
      "Average test loss: 0.001792036557984021\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03104822073214584\n",
      "Average test loss: 0.0017932260615958107\n",
      "Epoch 248/300\n",
      "Average training loss: 0.03098948462969727\n",
      "Average test loss: 0.001896372287741138\n",
      "Epoch 249/300\n",
      "Average training loss: 0.030931683878103893\n",
      "Average test loss: 0.001850220774196916\n",
      "Epoch 250/300\n",
      "Average training loss: 0.031136523402399488\n",
      "Average test loss: 0.0018300913588868246\n",
      "Epoch 251/300\n",
      "Average training loss: 0.030954132061865595\n",
      "Average test loss: 0.0019841006607231168\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03089590150780148\n",
      "Average test loss: 0.0022778045089087554\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03091710551414225\n",
      "Average test loss: 0.002094239838214384\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03101032460067007\n",
      "Average test loss: 0.0018036858294055694\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03142527491516537\n",
      "Average test loss: 0.0018799182182798783\n",
      "Epoch 256/300\n",
      "Average training loss: 0.030865052269564736\n",
      "Average test loss: 0.0019524838640871975\n",
      "Epoch 257/300\n",
      "Average training loss: 0.030958071837822596\n",
      "Average test loss: 0.001936976293205387\n",
      "Epoch 258/300\n",
      "Average training loss: 0.030871592794855435\n",
      "Average test loss: 0.0017715590592059824\n",
      "Epoch 259/300\n",
      "Average training loss: 0.030874208620852894\n",
      "Average test loss: 0.0018303153769423563\n",
      "Epoch 260/300\n",
      "Average training loss: 0.03086789964967304\n",
      "Average test loss: 0.0019308471623808146\n",
      "Epoch 261/300\n",
      "Average training loss: 0.030900017941991487\n",
      "Average test loss: 0.0017673775922093127\n",
      "Epoch 262/300\n",
      "Average training loss: 0.030942721194691127\n",
      "Average test loss: 0.0027051278062992625\n",
      "Epoch 263/300\n",
      "Average training loss: 0.03084638379679786\n",
      "Average test loss: 0.0018528021583333611\n",
      "Epoch 264/300\n",
      "Average training loss: 0.030899831977155474\n",
      "Average test loss: 0.0017899651417715681\n",
      "Epoch 265/300\n",
      "Average training loss: 0.030836158090167574\n",
      "Average test loss: 0.003170137052527732\n",
      "Epoch 266/300\n",
      "Average training loss: 0.030801327884197236\n",
      "Average test loss: 0.002475747196521196\n",
      "Epoch 267/300\n",
      "Average training loss: 0.030812475108438066\n",
      "Average test loss: 0.0022389808715217643\n",
      "Epoch 268/300\n",
      "Average training loss: 0.030879010485278236\n",
      "Average test loss: 0.001859776724750797\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03082815068297916\n",
      "Average test loss: 0.0019133313988438912\n",
      "Epoch 270/300\n",
      "Average training loss: 0.0307812987698449\n",
      "Average test loss: 0.0018537738557077117\n",
      "Epoch 271/300\n",
      "Average training loss: 0.030849733024835586\n",
      "Average test loss: 0.0017950240164581273\n",
      "Epoch 272/300\n",
      "Average training loss: 0.030794528889987204\n",
      "Average test loss: 0.0019256445744799244\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03077236058314641\n",
      "Average test loss: 0.0018660391230757038\n",
      "Epoch 277/300\n",
      "Average training loss: 0.03071642989748054\n",
      "Average test loss: 0.001988954647340708\n",
      "Epoch 278/300\n",
      "Average training loss: 0.030726530134677885\n",
      "Average test loss: 0.0019123810595936246\n",
      "Epoch 279/300\n",
      "Average training loss: 0.030825265698962743\n",
      "Average test loss: 0.0017885646654499901\n",
      "Epoch 280/300\n",
      "Average training loss: 0.030745716787046858\n",
      "Average test loss: 0.0017890312645791305\n",
      "Epoch 281/300\n",
      "Average training loss: 0.030749693698353238\n",
      "Average test loss: 0.0018418014019520746\n",
      "Epoch 282/300\n",
      "Average training loss: 0.030783477299743227\n",
      "Average test loss: 11.434810119628906\n",
      "Epoch 283/300\n",
      "Average training loss: 0.030818722393777635\n",
      "Average test loss: 0.0018674036655575038\n",
      "Epoch 284/300\n",
      "Average training loss: 0.030716271728277206\n",
      "Average test loss: 0.0017957760876872472\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03075435551835431\n",
      "Average test loss: 0.0018610371067706083\n",
      "Epoch 286/300\n",
      "Average training loss: 0.030778000611397956\n",
      "Average test loss: 0.0018059362332440085\n",
      "Epoch 287/300\n",
      "Average training loss: 0.030650925416085457\n",
      "Average test loss: 0.0018492983606540495\n",
      "Epoch 288/300\n",
      "Average training loss: 0.030727852669027115\n",
      "Average test loss: 0.0017912499317899347\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03063997337884373\n",
      "Average test loss: 0.001814588346829017\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030895634704165988\n",
      "Average test loss: 0.0022188092639876737\n",
      "Epoch 291/300\n",
      "Average training loss: 0.030585994929075242\n",
      "Average test loss: 0.0020074600194477372\n",
      "Epoch 292/300\n",
      "Average training loss: 0.030646652988261646\n",
      "Average test loss: 0.001844244914750258\n",
      "Epoch 293/300\n",
      "Average training loss: 0.030682894357376627\n",
      "Average test loss: 0.0018155144219183259\n",
      "Epoch 294/300\n",
      "Average training loss: 0.030670556942621868\n",
      "Average test loss: 0.001801103790394134\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03090780005355676\n",
      "Average test loss: 0.0017963907272658414\n",
      "Epoch 296/300\n",
      "Average training loss: 0.030633132262362375\n",
      "Average test loss: 0.0018023491131348743\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03058735836380058\n",
      "Average test loss: 0.002518300717075666\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03057537938323286\n",
      "Average test loss: 0.0017948735540525781\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03060703147451083\n",
      "Average test loss: 0.0019230633940961626\n",
      "Epoch 300/300\n",
      "Average training loss: 0.030655198161800703\n",
      "Average test loss: 0.0018584081906204422\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth3-.025/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 21.71\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.52\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 24.20\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 24.71\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.89\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 25.16\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.26\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.37\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.54\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.62\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.72\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.67\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.86\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.91\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 25.97\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.91\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.24\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.37\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.83\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.22\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.39\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.52\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.63\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.83\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.96\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 28.16\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 28.22\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.53\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.49\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.34\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.69\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.86\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.02\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.42\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.59\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.63\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.95\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.17\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.71\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.34\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.81\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.01\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.10\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.29\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.41\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.49\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.57\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.63\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 16.292712008794147\n",
      "Average test loss: 0.013777002493540446\n",
      "Epoch 2/300\n",
      "Average training loss: 2.892537515004476\n",
      "Average test loss: 0.010987621621125274\n",
      "Epoch 3/300\n",
      "Average training loss: 1.8477383125093247\n",
      "Average test loss: 0.009881972153981526\n",
      "Epoch 4/300\n",
      "Average training loss: 1.2223681674003601\n",
      "Average test loss: 0.01068829691161712\n",
      "Epoch 5/300\n",
      "Average training loss: 0.8572062628534105\n",
      "Average test loss: 0.009046468903621038\n",
      "Epoch 6/300\n",
      "Average training loss: 0.6610418448448181\n",
      "Average test loss: 0.008692497688035171\n",
      "Epoch 7/300\n",
      "Average training loss: 0.5367785685327318\n",
      "Average test loss: 0.008178243149485852\n",
      "Epoch 8/300\n",
      "Average training loss: 0.46275006294250487\n",
      "Average test loss: 0.007774744020568\n",
      "Epoch 9/300\n",
      "Average training loss: 0.41469513845443723\n",
      "Average test loss: 0.00785827324539423\n",
      "Epoch 10/300\n",
      "Average training loss: 0.38207147187656826\n",
      "Average test loss: 0.008772583153512743\n",
      "Epoch 11/300\n",
      "Average training loss: 0.3566027466721005\n",
      "Average test loss: 0.009645883295271131\n",
      "Epoch 12/300\n",
      "Average training loss: 0.339453913503223\n",
      "Average test loss: 0.008223388524519072\n",
      "Epoch 13/300\n",
      "Average training loss: 0.32359974686304727\n",
      "Average test loss: 0.008124744878874884\n",
      "Epoch 14/300\n",
      "Average training loss: 0.31329763234986197\n",
      "Average test loss: 0.007289035687016116\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30264279227786595\n",
      "Average test loss: 0.008898692972958087\n",
      "Epoch 16/300\n",
      "Average training loss: 0.29656959766811797\n",
      "Average test loss: 0.007046032720969783\n",
      "Epoch 17/300\n",
      "Average training loss: 0.2859617333544625\n",
      "Average test loss: 0.007521688165764014\n",
      "Epoch 18/300\n",
      "Average training loss: 0.2779877711137136\n",
      "Average test loss: 0.0073911245448721775\n",
      "Epoch 19/300\n",
      "Average training loss: 0.26903930770026313\n",
      "Average test loss: 0.007194642016871108\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2628571508460575\n",
      "Average test loss: 0.00955951955748929\n",
      "Epoch 21/300\n",
      "Average training loss: 0.2548251157336765\n",
      "Average test loss: 0.006934948449333509\n",
      "Epoch 22/300\n",
      "Average training loss: 0.24799837778674233\n",
      "Average test loss: 0.006300962965935469\n",
      "Epoch 23/300\n",
      "Average training loss: 0.2429726033873028\n",
      "Average test loss: 0.006741465364479357\n",
      "Epoch 24/300\n",
      "Average training loss: 0.23859236211246915\n",
      "Average test loss: 0.006535990812712246\n",
      "Epoch 25/300\n",
      "Average training loss: 0.23483519236246744\n",
      "Average test loss: 0.006216312291721503\n",
      "Epoch 26/300\n",
      "Average training loss: 0.2304847958220376\n",
      "Average test loss: 0.006770783395816883\n",
      "Epoch 27/300\n",
      "Average training loss: 0.2253351979388131\n",
      "Average test loss: 0.006176699529919359\n",
      "Epoch 28/300\n",
      "Average training loss: 0.22259008819527096\n",
      "Average test loss: 0.006182334741784467\n",
      "Epoch 29/300\n",
      "Average training loss: 0.2199060445494122\n",
      "Average test loss: 0.005967539887875318\n",
      "Epoch 30/300\n",
      "Average training loss: 0.21621478311220804\n",
      "Average test loss: 0.006074019671935174\n",
      "Epoch 31/300\n",
      "Average training loss: 0.21290019497606488\n",
      "Average test loss: 0.007099888535423411\n",
      "Epoch 32/300\n",
      "Average training loss: 0.2120219889746772\n",
      "Average test loss: 0.008136768134103882\n",
      "Epoch 33/300\n",
      "Average training loss: 0.20889686701695123\n",
      "Average test loss: 0.006083205427974463\n",
      "Epoch 34/300\n",
      "Average training loss: 0.20642563417222765\n",
      "Average test loss: 0.005879003192815516\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2044153107272254\n",
      "Average test loss: 0.005837244581845071\n",
      "Epoch 36/300\n",
      "Average training loss: 0.20117752583821616\n",
      "Average test loss: 0.005987587157222959\n",
      "Epoch 39/300\n",
      "Average training loss: 0.1985429428021113\n",
      "Average test loss: 0.007348083607852459\n",
      "Epoch 40/300\n",
      "Average training loss: 0.19736532841788398\n",
      "Average test loss: 0.006202653235031498\n",
      "Epoch 41/300\n",
      "Average training loss: 0.1961440877252155\n",
      "Average test loss: 0.005915661504285203\n",
      "Epoch 42/300\n",
      "Average training loss: 0.19492775016360814\n",
      "Average test loss: 0.005778458381278648\n",
      "Epoch 43/300\n",
      "Average training loss: 0.19424067302544912\n",
      "Average test loss: 0.005958749894052744\n",
      "Epoch 44/300\n",
      "Average training loss: 0.19331184387207032\n",
      "Average test loss: 0.008138517251859108\n",
      "Epoch 45/300\n",
      "Average training loss: 0.19199410853121016\n",
      "Average test loss: 0.005921015511370368\n",
      "Epoch 46/300\n",
      "Average training loss: 0.19060106021828122\n",
      "Average test loss: 0.005925919817553626\n",
      "Epoch 47/300\n",
      "Average training loss: 0.18949406123161316\n",
      "Average test loss: 0.005738512460556295\n",
      "Epoch 48/300\n",
      "Average training loss: 0.18906053335136838\n",
      "Average test loss: 0.01753362477901909\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1892112929622332\n",
      "Average test loss: 0.00573363470327523\n",
      "Epoch 50/300\n",
      "Average training loss: 0.18755795033772787\n",
      "Average test loss: 0.005914464860740635\n",
      "Epoch 51/300\n",
      "Average training loss: 0.1866396009127299\n",
      "Average test loss: 0.005930348253498475\n",
      "Epoch 52/300\n",
      "Average training loss: 0.18654460910956064\n",
      "Average test loss: 0.006041138263833192\n",
      "Epoch 53/300\n",
      "Average training loss: 0.18550420433945125\n",
      "Average test loss: 0.005847413858605756\n",
      "Epoch 54/300\n",
      "Average training loss: 0.18463018918037413\n",
      "Average test loss: 0.005736828419897292\n",
      "Epoch 55/300\n",
      "Average training loss: 0.18697523656156328\n",
      "Average test loss: 0.012099461895724138\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1878749071624544\n",
      "Average test loss: 125.57660410054525\n",
      "Epoch 57/300\n",
      "Average training loss: 0.18225312275356717\n",
      "Average test loss: 9.698063040627373\n",
      "Epoch 59/300\n",
      "Average training loss: 0.18173380512661405\n",
      "Average test loss: 0.005556242709358533\n",
      "Epoch 60/300\n",
      "Average training loss: 0.18189759720696344\n",
      "Average test loss: 0.00561704373318288\n",
      "Epoch 61/300\n",
      "Average training loss: 0.18089926269319323\n",
      "Average test loss: 0.005678199275707205\n",
      "Epoch 62/300\n",
      "Average training loss: 0.18061338692241244\n",
      "Average test loss: 0.005611209050648742\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1804670595990287\n",
      "Average test loss: 0.005589527996877829\n",
      "Epoch 64/300\n",
      "Average training loss: 0.18002362549304962\n",
      "Average test loss: 0.00577913204414977\n",
      "Epoch 65/300\n",
      "Average training loss: 0.17915969438023038\n",
      "Average test loss: 0.005571578023748265\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1777506321006351\n",
      "Average test loss: 0.005723250781496366\n",
      "Epoch 67/300\n",
      "Average training loss: 0.17912091504202948\n",
      "Average test loss: 0.005704613722860813\n",
      "Epoch 68/300\n",
      "Average training loss: 0.1781613104475869\n",
      "Average test loss: 0.005537652783923679\n",
      "Epoch 69/300\n",
      "Average training loss: 0.17712426455815633\n",
      "Average test loss: 0.722562844687038\n",
      "Epoch 70/300\n",
      "Average training loss: 0.17759776127338409\n",
      "Average test loss: 0.008073278981778356\n",
      "Epoch 71/300\n",
      "Average training loss: 0.17677200623353323\n",
      "Average test loss: 0.0055202303330103555\n",
      "Epoch 72/300\n",
      "Average training loss: 0.17609045867125192\n",
      "Average test loss: 0.005761462133377791\n",
      "Epoch 73/300\n",
      "Average training loss: 0.1754789375530349\n",
      "Average test loss: 0.0057359011405044135\n",
      "Epoch 75/300\n",
      "Average training loss: 0.17530349134074316\n",
      "Average test loss: 0.005664829705738359\n",
      "Epoch 76/300\n",
      "Average training loss: 0.17447613663143582\n",
      "Average test loss: 0.020797906558132833\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1741665249996715\n",
      "Average test loss: 0.005780198430021604\n",
      "Epoch 78/300\n",
      "Average training loss: 0.17558963106738196\n",
      "Average test loss: 0.005801281343317694\n",
      "Epoch 79/300\n",
      "Average training loss: 0.17387878574265375\n",
      "Average test loss: 0.005746583321028286\n",
      "Epoch 80/300\n",
      "Average training loss: 0.1730813515583674\n",
      "Average test loss: 0.005528129308174054\n",
      "Epoch 82/300\n",
      "Average training loss: 0.17428606912824843\n",
      "Average test loss: 0.005640864090994\n",
      "Epoch 83/300\n",
      "Average training loss: 0.1758982243405448\n",
      "Average test loss: 0.005568527367172969\n",
      "Epoch 84/300\n",
      "Average training loss: 0.17167372614807552\n",
      "Average test loss: 0.005503798291914993\n",
      "Epoch 85/300\n",
      "Average training loss: 0.17123562869760725\n",
      "Average test loss: 0.005564212477869457\n",
      "Epoch 87/300\n",
      "Average training loss: 0.17088162222173478\n",
      "Average test loss: 0.005695290948781702\n",
      "Epoch 88/300\n",
      "Average training loss: 0.17155889923042722\n",
      "Average test loss: 0.005636373160199986\n",
      "Epoch 89/300\n",
      "Average training loss: 0.17095496347215441\n",
      "Average test loss: 0.00549932686239481\n",
      "Epoch 90/300\n",
      "Average training loss: 0.17125445146693125\n",
      "Average test loss: 0.006226589142448372\n",
      "Epoch 91/300\n",
      "Average training loss: 0.17149718855486976\n",
      "Average test loss: 0.005547322898689243\n",
      "Epoch 92/300\n",
      "Average training loss: 0.1698147753543324\n",
      "Average test loss: 0.005599228334509664\n",
      "Epoch 93/300\n",
      "Average training loss: 0.17070039738549125\n",
      "Average test loss: 0.005529101307193438\n",
      "Epoch 94/300\n",
      "Average training loss: 0.16917624274889628\n",
      "Average test loss: 0.0057339318576786254\n",
      "Epoch 95/300\n",
      "Average training loss: 0.16866871750354767\n",
      "Average test loss: 0.009568996030423377\n",
      "Epoch 96/300\n",
      "Average training loss: 0.16925686184565225\n",
      "Average test loss: 0.006088644183758232\n",
      "Epoch 97/300\n",
      "Average training loss: 3057091.086985281\n",
      "Average test loss: 2.758467509335942\n",
      "Epoch 98/300\n",
      "Average training loss: 11.384290517171223\n",
      "Average test loss: 0.8386397559510337\n",
      "Epoch 99/300\n",
      "Average training loss: 10.47851663631863\n",
      "Average test loss: 55.849147957854804\n",
      "Epoch 100/300\n",
      "Average training loss: 9.870888120015461\n",
      "Average test loss: 34.02877488313781\n",
      "Epoch 101/300\n",
      "Average training loss: 9.429074461195205\n",
      "Average test loss: 6.616011933033665\n",
      "Epoch 102/300\n",
      "Average training loss: 9.079234638637967\n",
      "Average test loss: 240.8652715543111\n",
      "Epoch 103/300\n",
      "Average training loss: 8.789998491923015\n",
      "Average test loss: 24.70389639280902\n",
      "Epoch 104/300\n",
      "Average training loss: 8.00482318327162\n",
      "Average test loss: 1390.5600839661558\n",
      "Epoch 107/300\n",
      "Average training loss: 7.730743813832601\n",
      "Average test loss: 132.51045146866971\n",
      "Epoch 108/300\n",
      "Average training loss: 7.493485870785183\n",
      "Average test loss: 17060.118555784855\n",
      "Epoch 109/300\n",
      "Average training loss: 7.238188264634874\n",
      "Average test loss: 10327.70665961941\n",
      "Epoch 110/300\n",
      "Average training loss: 7.028944135453966\n",
      "Average test loss: 7163.3273664525495\n",
      "Epoch 111/300\n",
      "Average training loss: 6.801461714002821\n",
      "Average test loss: 454.69327778917426\n",
      "Epoch 112/300\n",
      "Average training loss: 6.581471669514974\n",
      "Average test loss: 269420.21285452467\n",
      "Epoch 113/300\n",
      "Average training loss: 6.337232264624702\n",
      "Average test loss: 154072.30991729672\n",
      "Epoch 114/300\n",
      "Average training loss: 6.091560102674697\n",
      "Average test loss: 1348104.5329455486\n",
      "Epoch 115/300\n",
      "Average training loss: 5.840992332458496\n",
      "Average test loss: 20491.58171856273\n",
      "Epoch 116/300\n",
      "Average training loss: 5.601729162851969\n",
      "Average test loss: 3423302.805611111\n",
      "Epoch 117/300\n",
      "Average training loss: 5.354223437839084\n",
      "Average test loss: 41783.26778530408\n",
      "Epoch 118/300\n",
      "Average training loss: 5.1206874563429094\n",
      "Average test loss: 11.197312342700858\n",
      "Epoch 119/300\n",
      "Average training loss: 4.848987866719564\n",
      "Average test loss: 244261.46110301765\n",
      "Epoch 120/300\n",
      "Average training loss: 4.573134052276611\n",
      "Average test loss: 98.23966611738834\n",
      "Epoch 121/300\n",
      "Average training loss: 4.289504595862494\n",
      "Average test loss: 0.7758031937463415\n",
      "Epoch 122/300\n",
      "Average training loss: 3.9963013665941025\n",
      "Average test loss: 0.40754143191128966\n",
      "Epoch 123/300\n",
      "Average training loss: 3.709494685067071\n",
      "Average test loss: 0.02328323957986302\n",
      "Epoch 124/300\n",
      "Average training loss: 2.971015808529324\n",
      "Average test loss: 753.6956876186795\n",
      "Epoch 127/300\n",
      "Average training loss: 2.7714968367682564\n",
      "Average test loss: 0.008239198596113257\n",
      "Epoch 128/300\n",
      "Average training loss: 2.5827963496314155\n",
      "Average test loss: 0.006413670394983557\n",
      "Epoch 129/300\n",
      "Average training loss: 2.4061208765241835\n",
      "Average test loss: 5245.552067199707\n",
      "Epoch 130/300\n",
      "Average training loss: 2.23683393351237\n",
      "Average test loss: 0.006541952150563399\n",
      "Epoch 131/300\n",
      "Average training loss: 2.0735699628194175\n",
      "Average test loss: 0.13086390649941232\n",
      "Epoch 132/300\n",
      "Average training loss: 1.933786840862698\n",
      "Average test loss: 0.008495950139438112\n",
      "Epoch 133/300\n",
      "Average training loss: 1.8075985794067382\n",
      "Average test loss: 0.006268487707194355\n",
      "Epoch 134/300\n",
      "Average training loss: 1.6871815653906928\n",
      "Average test loss: 0.005951629589208298\n",
      "Epoch 135/300\n",
      "Average training loss: 1.5768195809258354\n",
      "Average test loss: 0.010393330227997568\n",
      "Epoch 136/300\n",
      "Average training loss: 1.4730514901479086\n",
      "Average test loss: 0.006136912927031517\n",
      "Epoch 137/300\n",
      "Average training loss: 1.3761104757520888\n",
      "Average test loss: 0.006714219018403027\n",
      "Epoch 138/300\n",
      "Average training loss: 1.2916700418260363\n",
      "Average test loss: 0.007342122500969304\n",
      "Epoch 139/300\n",
      "Average training loss: 1.2122720952563815\n",
      "Average test loss: 0.006201073088993629\n",
      "Epoch 140/300\n",
      "Average training loss: 1.1366346512900458\n",
      "Average test loss: 0.005818306184477276\n",
      "Epoch 141/300\n",
      "Average training loss: 1.0659507503509522\n",
      "Average test loss: 0.0057776834708121085\n",
      "Epoch 142/300\n",
      "Average training loss: 0.9978290221956041\n",
      "Average test loss: 0.0058199281324115065\n",
      "Epoch 143/300\n",
      "Average training loss: 0.9257220904032389\n",
      "Average test loss: 0.005950016302367051\n",
      "Epoch 144/300\n",
      "Average training loss: 0.8551262531280518\n",
      "Average test loss: 0.0058115082606673245\n",
      "Epoch 145/300\n",
      "Average training loss: 0.6405402879714965\n",
      "Average test loss: 0.00566772983264592\n",
      "Epoch 148/300\n",
      "Average training loss: 0.5715998295148214\n",
      "Average test loss: 0.00570741575004326\n",
      "Epoch 149/300\n",
      "Average training loss: 0.5073801913526323\n",
      "Average test loss: 0.005708899032324553\n",
      "Epoch 150/300\n",
      "Average training loss: 0.44797140383720396\n",
      "Average test loss: 0.006091215634511577\n",
      "Epoch 151/300\n",
      "Average training loss: 0.3997410465346442\n",
      "Average test loss: 0.005615286972373724\n",
      "Epoch 152/300\n",
      "Average training loss: 0.35916777404149375\n",
      "Average test loss: 0.006152200001809332\n",
      "Epoch 153/300\n",
      "Average training loss: 0.32215061608950296\n",
      "Average test loss: 0.005586294900212023\n",
      "Epoch 154/300\n",
      "Average training loss: 0.3004477991527981\n",
      "Average test loss: 0.005833876959979534\n",
      "Epoch 155/300\n",
      "Average training loss: 0.2750499937004513\n",
      "Average test loss: 0.008042870010766718\n",
      "Epoch 156/300\n",
      "Average training loss: 0.2589633330769009\n",
      "Average test loss: 0.0062149576532344025\n",
      "Epoch 157/300\n",
      "Average training loss: 0.24631637043423124\n",
      "Average test loss: 0.005754236203100946\n",
      "Epoch 158/300\n",
      "Average training loss: 0.23582568375269572\n",
      "Average test loss: 0.00586496205917663\n",
      "Epoch 159/300\n",
      "Average training loss: 0.22733486194080776\n",
      "Average test loss: 0.0055457360636856825\n",
      "Epoch 160/300\n",
      "Average training loss: 0.22725501806206172\n",
      "Average test loss: 0.005983031223217646\n",
      "Epoch 161/300\n",
      "Average training loss: 0.21275617464383442\n",
      "Average test loss: 0.0055284455509649385\n",
      "Epoch 162/300\n",
      "Average test loss: 0.005582334688554207\n",
      "Epoch 165/300\n",
      "Average training loss: 0.1929009078608619\n",
      "Average test loss: 0.00550745395074288\n",
      "Epoch 166/300\n",
      "Average training loss: 0.19221037381225162\n",
      "Average test loss: 0.005491232571916448\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1883344586690267\n",
      "Average test loss: 0.005540844964484374\n",
      "Epoch 168/300\n",
      "Average training loss: 0.18715334803528255\n",
      "Average test loss: 0.005624563223578864\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1844658776919047\n",
      "Average test loss: 0.005477419645422035\n",
      "Epoch 170/300\n",
      "Average training loss: 0.18371653203169505\n",
      "Average test loss: 0.005463562076704369\n",
      "Epoch 171/300\n",
      "Average training loss: 0.1821499316162533\n",
      "Average test loss: 0.005441314353297154\n",
      "Epoch 172/300\n",
      "Average training loss: 0.185603313975864\n",
      "Average test loss: 0.00551773532686962\n",
      "Epoch 173/300\n",
      "Average training loss: 0.17927831656403012\n",
      "Average test loss: 0.005461756152825223\n",
      "Epoch 174/300\n",
      "Average training loss: 0.17829567351606157\n",
      "Average test loss: 0.005455436828235785\n",
      "Epoch 175/300\n",
      "Average training loss: 0.17733753301037683\n",
      "Average test loss: 0.0055126386458675064\n",
      "Epoch 176/300\n",
      "Average training loss: 0.1763785855770111\n",
      "Average test loss: 0.006370375825920039\n",
      "Epoch 177/300\n",
      "Average training loss: 0.1753366717232598\n",
      "Average test loss: 0.005546704835775826\n",
      "Epoch 178/300\n",
      "Average training loss: 0.1745941283305486\n",
      "Average test loss: 0.15539839636617236\n",
      "Epoch 179/300\n",
      "Average training loss: 0.17259639475080701\n",
      "Average test loss: 0.005547269121226338\n",
      "Epoch 182/300\n",
      "Average training loss: 0.17115868577692245\n",
      "Average test loss: 0.005452265336902605\n",
      "Epoch 183/300\n",
      "Average training loss: 0.17104762139585283\n",
      "Average test loss: 0.005562499341865381\n",
      "Epoch 184/300\n",
      "Average training loss: 0.16995128707091015\n",
      "Average test loss: 0.0054809414661592904\n",
      "Epoch 185/300\n",
      "Average training loss: 0.17558906984329223\n",
      "Average test loss: 0.005772052987168232\n",
      "Epoch 186/300\n",
      "Average training loss: 0.16848153336842855\n",
      "Average test loss: 0.005525565584914552\n",
      "Epoch 187/300\n",
      "Average training loss: 0.169344894932376\n",
      "Average test loss: 0.005633591635773579\n",
      "Epoch 188/300\n",
      "Average training loss: 0.16887558161550098\n",
      "Average test loss: 0.005587350805186563\n",
      "Epoch 189/300\n",
      "Average training loss: 0.16899359770615896\n",
      "Average test loss: 0.005538458160228199\n",
      "Epoch 190/300\n",
      "Average training loss: 0.1681048859755198\n",
      "Average test loss: 0.005563180696633127\n",
      "Epoch 191/300\n",
      "Average training loss: 0.16706268311871422\n",
      "Average test loss: 0.005531238995699419\n",
      "Epoch 192/300\n",
      "Average training loss: 0.17061374358336132\n",
      "Average test loss: 258.3120790184869\n",
      "Epoch 193/300\n",
      "Average training loss: 0.1684202407201131\n",
      "Average test loss: 0.005553984280261728\n",
      "Epoch 194/300\n",
      "Average training loss: 0.16603985565900803\n",
      "Average test loss: 0.005521672888762421\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1658114585214191\n",
      "Average test loss: 0.005511518629060851\n",
      "Epoch 196/300\n",
      "Average training loss: 0.16554391962952084\n",
      "Average test loss: 0.00568433433253732\n",
      "Epoch 197/300\n",
      "Average training loss: 2846134.3952007694\n",
      "Average test loss: 87870.85273319758\n",
      "Epoch 198/300\n",
      "Average training loss: 16.818860895792643\n",
      "Average test loss: 603.9594154935943\n",
      "Epoch 199/300\n",
      "Average training loss: 14.007538564893935\n",
      "Average test loss: 682.7103086374866\n",
      "Epoch 200/300\n",
      "Average training loss: 12.361085469563802\n",
      "Average test loss: 1406.0534582450953\n",
      "Epoch 201/300\n",
      "Average training loss: 11.349301778157551\n",
      "Average test loss: 8.92053879551424\n",
      "Epoch 202/300\n",
      "Average training loss: 9.74229567972819\n",
      "Average test loss: 1064.5078106403002\n",
      "Epoch 205/300\n",
      "Average training loss: 9.321807163662381\n",
      "Average test loss: 0.10026195094982783\n",
      "Epoch 206/300\n",
      "Average training loss: 8.930727851019965\n",
      "Average test loss: 24.167024763325852\n",
      "Epoch 207/300\n",
      "Average training loss: 8.495622706943088\n",
      "Average test loss: 2489.0000617230735\n",
      "Epoch 208/300\n",
      "Average training loss: 8.091416492886014\n",
      "Average test loss: 0.013426076413856613\n",
      "Epoch 209/300\n",
      "Average training loss: 7.666726359049479\n",
      "Average test loss: 3.0448048824403022\n",
      "Epoch 210/300\n",
      "Average training loss: 7.302555203331841\n",
      "Average test loss: 8.021418800057637\n",
      "Epoch 211/300\n",
      "Average training loss: 6.956804085625542\n",
      "Average test loss: 0.0430114828215705\n",
      "Epoch 212/300\n",
      "Average training loss: 6.644372497134738\n",
      "Average test loss: 107.22057913459672\n",
      "Epoch 213/300\n",
      "Average training loss: 6.364027147081163\n",
      "Average test loss: 13434.24922866461\n",
      "Epoch 214/300\n",
      "Average training loss: 6.055613880581326\n",
      "Average test loss: 26606.18391538142\n",
      "Epoch 215/300\n",
      "Average training loss: 5.785155196295844\n",
      "Average test loss: 40.87869180966831\n",
      "Epoch 216/300\n",
      "Average training loss: 5.568141699896918\n",
      "Average test loss: 1325.5384779886272\n",
      "Epoch 217/300\n",
      "Average training loss: 5.322605291578505\n",
      "Average test loss: 81356.1773372347\n",
      "Epoch 218/300\n",
      "Average training loss: 5.099312906053331\n",
      "Average test loss: 58547.38758176486\n",
      "Epoch 219/300\n",
      "Average training loss: 4.6270814069112145\n",
      "Average test loss: 12409.701214932995\n",
      "Epoch 221/300\n",
      "Average training loss: 4.370864971584744\n",
      "Average test loss: 145939.40239079075\n",
      "Epoch 222/300\n",
      "Average training loss: 4.108654212527805\n",
      "Average test loss: 209.0386914037938\n",
      "Epoch 223/300\n",
      "Average training loss: 3.852754095289442\n",
      "Average test loss: 55855758.31686767\n",
      "Epoch 224/300\n",
      "Average training loss: 3.6240310944451224\n",
      "Average test loss: 840137980.5991111\n",
      "Epoch 225/300\n",
      "Average training loss: 3.3972910266452367\n",
      "Average test loss: 173515.67469810726\n",
      "Epoch 226/300\n",
      "Average training loss: 3.235289724773831\n",
      "Average test loss: 7465120.808993507\n",
      "Epoch 227/300\n",
      "Average training loss: 3.072508076561822\n",
      "Average test loss: 59150.39870699894\n",
      "Epoch 228/300\n",
      "Average training loss: 2.914961060841878\n",
      "Average test loss: 14686298.784490936\n",
      "Epoch 229/300\n",
      "Average training loss: 2.7752690461476646\n",
      "Average test loss: 5339182.8725010855\n",
      "Epoch 230/300\n",
      "Average training loss: 2.655742299185859\n",
      "Average test loss: 6787679.381347732\n",
      "Epoch 231/300\n",
      "Average training loss: 2.5325530067019995\n",
      "Average test loss: 433854533.1179056\n",
      "Epoch 232/300\n",
      "Average training loss: 2.4113309184180367\n",
      "Average test loss: 0.42068876444941594\n",
      "Epoch 233/300\n",
      "Average training loss: 2.2656941534678143\n",
      "Average test loss: 320.6579868597587\n",
      "Epoch 234/300\n",
      "Average training loss: 2.1244203595055473\n",
      "Average test loss: 1.1990958888240986\n",
      "Epoch 235/300\n",
      "Average training loss: 1.9646042400995891\n",
      "Average test loss: 26.420410993927057\n",
      "Epoch 236/300\n",
      "Average training loss: 1.819059467845493\n",
      "Average test loss: 0.49154385498828357\n",
      "Epoch 237/300\n",
      "Average training loss: 1.6658756219016182\n",
      "Average test loss: 1.0040896879997518\n",
      "Epoch 238/300\n",
      "Average training loss: 1.2286086466047499\n",
      "Average test loss: 11.192945316387547\n",
      "Epoch 241/300\n",
      "Average training loss: 1.1246003906461928\n",
      "Average test loss: 0.019541224661800596\n",
      "Epoch 242/300\n",
      "Average training loss: 1.025518585840861\n",
      "Average test loss: 0.06804444552461306\n",
      "Epoch 243/300\n",
      "Average training loss: 0.9398018922805786\n",
      "Average test loss: 0.008343286099119319\n",
      "Epoch 244/300\n",
      "Average training loss: 0.8594330695470174\n",
      "Average test loss: 0.010918846370445358\n",
      "Epoch 245/300\n",
      "Average training loss: 0.788499170144399\n",
      "Average test loss: 0.006138059371875392\n",
      "Epoch 246/300\n",
      "Average training loss: 0.7198964257240296\n",
      "Average test loss: 0.005818074445757601\n",
      "Epoch 247/300\n",
      "Average training loss: 0.651674520916409\n",
      "Average test loss: 0.0123719445814689\n",
      "Epoch 248/300\n",
      "Average training loss: 0.5864858267042372\n",
      "Average test loss: 0.005749369246264299\n",
      "Epoch 249/300\n",
      "Average training loss: 0.5166703144974178\n",
      "Average test loss: 0.005683313062414527\n",
      "Epoch 250/300\n",
      "Average training loss: 0.4488965298864577\n",
      "Average test loss: 0.005567096116228236\n",
      "Epoch 251/300\n",
      "Average training loss: 0.3928560900953081\n",
      "Average test loss: 0.005481615968462494\n",
      "Epoch 252/300\n",
      "Average training loss: 0.34690600384606257\n",
      "Average test loss: 0.005511026457779938\n",
      "Epoch 253/300\n",
      "Average training loss: 0.3111841266155243\n",
      "Average test loss: 0.0055305156641536285\n",
      "Epoch 254/300\n",
      "Average training loss: 0.2833636951711443\n",
      "Average test loss: 0.005505726621382766\n",
      "Epoch 255/300\n",
      "Average training loss: 0.2623213735686408\n",
      "Average test loss: 0.005647801549070412\n",
      "Epoch 256/300\n",
      "Average training loss: 0.24693525399102106\n",
      "Average test loss: 0.005443293266826206\n",
      "Epoch 257/300\n",
      "Average training loss: 0.23280337405204773\n",
      "Average test loss: 834874010.4248888\n",
      "Epoch 258/300\n",
      "Average training loss: 0.22637959725326962\n",
      "Average test loss: 0.005774872082802984\n",
      "Epoch 259/300\n",
      "Average training loss: 0.21303858795430924\n",
      "Average test loss: 0.018944676064782673\n",
      "Epoch 260/300\n",
      "Average training loss: 0.20627533057000902\n",
      "Average test loss: 0.02311470708085431\n",
      "Epoch 261/300\n",
      "Average training loss: 0.20179082335366144\n",
      "Average test loss: 0.005545840908255842\n",
      "Epoch 262/300\n",
      "Average training loss: 0.19647428090042537\n",
      "Average test loss: 0.0054326309201618034\n",
      "Epoch 265/300\n",
      "Average training loss: 0.18781683516502382\n",
      "Average test loss: 0.0054681756529543135\n",
      "Epoch 266/300\n",
      "Average training loss: 0.18610489095581903\n",
      "Average test loss: 0.005493828546048867\n",
      "Epoch 267/300\n",
      "Average training loss: 0.1837009718683031\n",
      "Average test loss: 0.005896325134154823\n",
      "Epoch 268/300\n",
      "Average training loss: 0.18230966933568318\n",
      "Average test loss: 0.005949298141317235\n",
      "Epoch 269/300\n",
      "Average training loss: 0.18741573475466833\n",
      "Average test loss: 0.0054564120699134135\n",
      "Epoch 270/300\n",
      "Average training loss: 0.17908484812577566\n",
      "Average test loss: 0.005585572219557232\n",
      "Epoch 271/300\n",
      "Average training loss: 0.17843618031342826\n",
      "Average test loss: 0.005548112579931815\n",
      "Epoch 272/300\n",
      "Average training loss: 0.17587602969672944\n",
      "Average test loss: 0.005619823979420794\n",
      "Epoch 273/300\n",
      "Average training loss: 0.17501730852656894\n",
      "Average test loss: 0.005785015165805816\n",
      "Epoch 274/300\n",
      "Average training loss: 382718.8820932354\n",
      "Average test loss: 257.6662415151596\n",
      "Epoch 275/300\n",
      "Average training loss: 9.933457814534504\n",
      "Average test loss: 3251.9942611994215\n",
      "Epoch 276/300\n",
      "Average training loss: 9.10819098409017\n",
      "Average test loss: 6163.318608497212\n",
      "Epoch 277/300\n",
      "Average training loss: 8.398193775600857\n",
      "Average test loss: 1084.6563225402451\n",
      "Epoch 278/300\n",
      "Average training loss: 7.869233342488607\n",
      "Average test loss: 3.7107524222963386\n",
      "Epoch 279/300\n",
      "Average training loss: 6.961902151743571\n",
      "Average test loss: 31.546280030369758\n",
      "Epoch 282/300\n",
      "Average training loss: 6.709726353963216\n",
      "Average test loss: 89.24580953497357\n",
      "Epoch 283/300\n",
      "Average training loss: 6.462529034932454\n",
      "Average test loss: 2.2660714163821605\n",
      "Epoch 284/300\n",
      "Average training loss: 6.2211954778035485\n",
      "Average test loss: 53.905168077866236\n",
      "Epoch 285/300\n",
      "Average training loss: 5.977992990705702\n",
      "Average test loss: 200.23696064080133\n",
      "Epoch 286/300\n",
      "Average training loss: 5.757485696580675\n",
      "Average test loss: 4.406191952319609\n",
      "Epoch 287/300\n",
      "Average training loss: 5.519968664805094\n",
      "Average test loss: 56292.84067926272\n",
      "Epoch 288/300\n",
      "Average training loss: 5.2724051988389755\n",
      "Average test loss: 14253.662464946667\n",
      "Epoch 289/300\n",
      "Average training loss: 5.013826209598117\n",
      "Average test loss: 19234.286212441373\n",
      "Epoch 290/300\n",
      "Average training loss: 4.755818614535862\n",
      "Average test loss: 44234.978916112785\n",
      "Epoch 291/300\n",
      "Average training loss: 4.50242959552341\n",
      "Average test loss: 313885.0949646354\n",
      "Epoch 292/300\n",
      "Average training loss: 4.280501274956597\n",
      "Average test loss: 4.586730017424458\n",
      "Epoch 293/300\n",
      "Average training loss: 4.059766603681776\n",
      "Average test loss: 0.047795311586724384\n",
      "Epoch 294/300\n",
      "Average training loss: 3.8607460233900284\n",
      "Average test loss: 0.007173859764718347\n",
      "Epoch 295/300\n",
      "Average training loss: 3.6694715501997206\n",
      "Average test loss: 0.007102233466588789\n",
      "Epoch 296/300\n",
      "Average training loss: 3.4749977118174233\n",
      "Average test loss: 0.006929287462598748\n",
      "Epoch 297/300\n",
      "Average training loss: 3.2925552101135254\n",
      "Average test loss: 0.015247081521070665\n",
      "Epoch 298/300\n",
      "Average training loss: 3.1153018273247612\n",
      "Average test loss: 0.006720694645411438\n",
      "Epoch 299/300\n",
      "Average training loss: 2.953853457768758\n",
      "Average test loss: 0.13296861909164323\n",
      "Epoch 300/300\n",
      "Average training loss: 2.801707209269206\n",
      "Average test loss: 0.006544505902876457\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 12.546793257819282\n",
      "Average test loss: 0.009779195834365156\n",
      "Epoch 2/300\n",
      "Average training loss: 2.0486035055584377\n",
      "Average test loss: 0.0072793331576718225\n",
      "Epoch 3/300\n",
      "Average training loss: 1.1311721370485095\n",
      "Average test loss: 0.006619724810537365\n",
      "Epoch 4/300\n",
      "Average training loss: 0.7352628154754639\n",
      "Average test loss: 0.0063552427991396855\n",
      "Epoch 5/300\n",
      "Average training loss: 0.5461105446550581\n",
      "Average test loss: 0.005983259886503219\n",
      "Epoch 6/300\n",
      "Average training loss: 0.44062376740243697\n",
      "Average test loss: 0.006725739426083035\n",
      "Epoch 7/300\n",
      "Average training loss: 0.37339871970812477\n",
      "Average test loss: 0.005930308925608794\n",
      "Epoch 8/300\n",
      "Average training loss: 0.3269503918223911\n",
      "Average test loss: 0.00585383235787352\n",
      "Epoch 9/300\n",
      "Average training loss: 0.2935801248550415\n",
      "Average test loss: 0.005589223149749968\n",
      "Epoch 10/300\n",
      "Average training loss: 0.2674630347357856\n",
      "Average test loss: 0.005255557955967056\n",
      "Epoch 11/300\n",
      "Average training loss: 0.24851378111044567\n",
      "Average test loss: 0.005331152016917864\n",
      "Epoch 12/300\n",
      "Average training loss: 0.23321968738238016\n",
      "Average test loss: 0.005196747892846664\n",
      "Epoch 13/300\n",
      "Average training loss: 0.22147243475914002\n",
      "Average test loss: 0.0048804456558492445\n",
      "Epoch 14/300\n",
      "Average training loss: 0.21113342519601186\n",
      "Average test loss: 0.005347269799974229\n",
      "Epoch 15/300\n",
      "Average training loss: 0.20286348397201961\n",
      "Average test loss: 0.0045892316189905\n",
      "Epoch 16/300\n",
      "Average training loss: 0.1961255513297187\n",
      "Average test loss: 0.00512143939257496\n",
      "Epoch 17/300\n",
      "Average training loss: 0.19015351654423607\n",
      "Average test loss: 0.004851819863335954\n",
      "Epoch 18/300\n",
      "Average training loss: 0.1840339452955458\n",
      "Average test loss: 0.005411265989558565\n",
      "Epoch 19/300\n",
      "Average training loss: 0.18023674652311536\n",
      "Average test loss: 0.005796746661679612\n",
      "Epoch 20/300\n",
      "Average training loss: 0.17358253622055053\n",
      "Average test loss: 0.004028452454341782\n",
      "Epoch 21/300\n",
      "Average training loss: 0.16771998206774394\n",
      "Average test loss: 0.004226086873975065\n",
      "Epoch 22/300\n",
      "Average training loss: 0.16475141802099016\n",
      "Average test loss: 0.004349682476785448\n",
      "Epoch 23/300\n",
      "Average training loss: 0.16003771290514204\n",
      "Average test loss: 0.003956797399454647\n",
      "Epoch 24/300\n",
      "Average training loss: 0.15587889203760358\n",
      "Average test loss: 0.004685547016561031\n",
      "Epoch 25/300\n",
      "Average training loss: 0.15218211077319252\n",
      "Average test loss: 0.003919081772367159\n",
      "Epoch 26/300\n",
      "Average training loss: 0.14774581253528596\n",
      "Average test loss: 0.004264342911541462\n",
      "Epoch 27/300\n",
      "Average training loss: 0.14516645226213667\n",
      "Average test loss: 0.004160917970455355\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14165522441599104\n",
      "Average test loss: 0.004230604674253199\n",
      "Epoch 29/300\n",
      "Average training loss: 0.13876663549741108\n",
      "Average test loss: 0.0038715544591347377\n",
      "Epoch 30/300\n",
      "Average training loss: 0.1366993025276396\n",
      "Average test loss: 0.003657019527422057\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13390251889493732\n",
      "Average test loss: 0.0038327021189033983\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1323214550945494\n",
      "Average test loss: 0.003871778537829717\n",
      "Epoch 33/300\n",
      "Average training loss: 0.13047674565845066\n",
      "Average test loss: 0.0036000561155378817\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12932708978652954\n",
      "Average test loss: 0.00359410781144268\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12745326687230005\n",
      "Average test loss: 0.003562436207301087\n",
      "Epoch 36/300\n",
      "Average training loss: 0.12586089752780066\n",
      "Average test loss: 0.003513675880514913\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12481880001227061\n",
      "Average test loss: 0.0035092313070264128\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12441444722149107\n",
      "Average test loss: 0.00417709121439192\n",
      "Epoch 39/300\n",
      "Average training loss: 0.12270162631405725\n",
      "Average test loss: 0.0035623705380906662\n",
      "Epoch 40/300\n",
      "Average training loss: 0.12341282674339082\n",
      "Average test loss: 0.0035552016602208216\n",
      "Epoch 41/300\n",
      "Average training loss: 0.12071212416225009\n",
      "Average test loss: 0.005300061347170009\n",
      "Epoch 42/300\n",
      "Average training loss: 0.12025111063321431\n",
      "Average test loss: 0.006310254128028949\n",
      "Epoch 43/300\n",
      "Average training loss: 0.12007197173436483\n",
      "Average test loss: 0.0034834647534622088\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11806872414880329\n",
      "Average test loss: 0.0037250404585566785\n",
      "Epoch 45/300\n",
      "Average training loss: 0.11879172938399844\n",
      "Average test loss: 0.003870675578713417\n",
      "Epoch 46/300\n",
      "Average training loss: 0.1177557526561949\n",
      "Average test loss: 0.054388021055195065\n",
      "Epoch 47/300\n",
      "Average training loss: 0.22251155635383393\n",
      "Average test loss: 0.007022625601126088\n",
      "Epoch 48/300\n",
      "Average training loss: 0.22429190956221687\n",
      "Average test loss: 0.003947613229768144\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1440215980940395\n",
      "Average test loss: 0.0036565754152834413\n",
      "Epoch 50/300\n",
      "Average training loss: 0.13486241087648604\n",
      "Average test loss: 0.003628063309109873\n",
      "Epoch 51/300\n",
      "Average training loss: 0.13021594600545036\n",
      "Average test loss: 0.0035697132396615215\n",
      "Epoch 52/300\n",
      "Average training loss: 0.1269253206120597\n",
      "Average test loss: 0.0035239560438527\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12467874171998766\n",
      "Average test loss: 0.003494869508056177\n",
      "Epoch 54/300\n",
      "Average training loss: 0.12527094068129857\n",
      "Average test loss: 0.0035171833121114306\n",
      "Epoch 55/300\n",
      "Average training loss: 0.12184352198574278\n",
      "Average test loss: 0.0034623667798522446\n",
      "Epoch 56/300\n",
      "Average training loss: 0.1205960361758868\n",
      "Average test loss: 0.0035116994296097094\n",
      "Epoch 57/300\n",
      "Average training loss: 0.11939955453077952\n",
      "Average test loss: 0.003616304663527343\n",
      "Epoch 58/300\n",
      "Average training loss: 0.12013024473852582\n",
      "Average test loss: 0.0035708216385294993\n",
      "Epoch 59/300\n",
      "Average training loss: 0.8520619622733858\n",
      "Average test loss: 0.004040370661765337\n",
      "Epoch 60/300\n",
      "Average training loss: 0.299113475256496\n",
      "Average test loss: 0.003774570687363545\n",
      "Epoch 61/300\n",
      "Average training loss: 0.20477013201183744\n",
      "Average test loss: 0.0036153240303198496\n",
      "Epoch 62/300\n",
      "Average training loss: 0.171488768113984\n",
      "Average test loss: 0.0035680889466570484\n",
      "Epoch 63/300\n",
      "Average training loss: 0.15161722056070964\n",
      "Average test loss: 0.003562427475013667\n",
      "Epoch 64/300\n",
      "Average training loss: 0.14257904997136858\n",
      "Average test loss: 0.003494808813350068\n",
      "Epoch 65/300\n",
      "Average training loss: 0.13714389476511213\n",
      "Average test loss: 0.0035577878554662067\n",
      "Epoch 66/300\n",
      "Average training loss: 0.13317685997486114\n",
      "Average test loss: 0.003633977558877733\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1302851440641615\n",
      "Average test loss: 0.003438226703968313\n",
      "Epoch 68/300\n",
      "Average training loss: 0.12772641701168483\n",
      "Average test loss: 0.003581587753776047\n",
      "Epoch 69/300\n",
      "Average training loss: 0.12573736854394277\n",
      "Average test loss: 0.0035627559528996545\n",
      "Epoch 70/300\n",
      "Average training loss: 0.12426063137584262\n",
      "Average test loss: 0.0035938787261645\n",
      "Epoch 71/300\n",
      "Average training loss: 0.12283696822325389\n",
      "Average test loss: 0.0035659585458536944\n",
      "Epoch 72/300\n",
      "Average training loss: 0.12168942821025848\n",
      "Average test loss: 0.0034241079483181237\n",
      "Epoch 73/300\n",
      "Average training loss: 0.12067561050256094\n",
      "Average test loss: 0.0041984926658786\n",
      "Epoch 74/300\n",
      "Average training loss: 0.11987095136112637\n",
      "Average test loss: 0.0035770313296880985\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11876124230358336\n",
      "Average test loss: 0.004058066536982854\n",
      "Epoch 76/300\n",
      "Average training loss: 0.1182158546646436\n",
      "Average test loss: 0.0034066263308955565\n",
      "Epoch 77/300\n",
      "Average training loss: 0.11782259744405746\n",
      "Average test loss: 0.0033972630413870017\n",
      "Epoch 78/300\n",
      "Average training loss: 0.11705782243940566\n",
      "Average test loss: 0.0034408225955234635\n",
      "Epoch 79/300\n",
      "Average training loss: 0.11664833600653543\n",
      "Average test loss: 0.003433595730819636\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11605708081854714\n",
      "Average test loss: 0.003376078963693645\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11612870314386156\n",
      "Average test loss: 0.0034514137882118425\n",
      "Epoch 82/300\n",
      "Average training loss: 0.11624516034788555\n",
      "Average test loss: 0.0034205775819718836\n",
      "Epoch 83/300\n",
      "Average training loss: 0.11436031725009282\n",
      "Average test loss: 0.00588123574314846\n",
      "Epoch 84/300\n",
      "Average training loss: 0.11417139116260741\n",
      "Average test loss: 0.003637192396654023\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1140841523276435\n",
      "Average test loss: 0.0033800885193049908\n",
      "Epoch 86/300\n",
      "Average training loss: 0.11348634460568428\n",
      "Average test loss: 0.0034332856459336147\n",
      "Epoch 87/300\n",
      "Average training loss: 0.11306088583336936\n",
      "Average test loss: 0.0033557247579511667\n",
      "Epoch 88/300\n",
      "Average training loss: 0.11277519450585047\n",
      "Average test loss: 0.005090196810662746\n",
      "Epoch 89/300\n",
      "Average training loss: 0.11238539453347524\n",
      "Average test loss: 0.004595217218415605\n",
      "Epoch 90/300\n",
      "Average training loss: 0.11207058821121851\n",
      "Average test loss: 0.00708582540270355\n",
      "Epoch 91/300\n",
      "Average training loss: 0.11211152172750898\n",
      "Average test loss: 0.0033644902526090543\n",
      "Epoch 92/300\n",
      "Average training loss: 0.11152015942335129\n",
      "Average test loss: 0.0033269263698409\n",
      "Epoch 93/300\n",
      "Average training loss: 0.1109298466708925\n",
      "Average test loss: 0.0036973613616493014\n",
      "Epoch 94/300\n",
      "Average training loss: 0.11124819095929464\n",
      "Average test loss: 0.003979189560645156\n",
      "Epoch 95/300\n",
      "Average training loss: 0.11692151330577003\n",
      "Average test loss: 0.0033240087860160403\n",
      "Epoch 96/300\n",
      "Average training loss: 0.1313317050602701\n",
      "Average test loss: 0.0034150383530391587\n",
      "Epoch 97/300\n",
      "Average training loss: 0.11367765569024615\n",
      "Average test loss: 0.0033792614887158076\n",
      "Epoch 98/300\n",
      "Average training loss: 0.11183153996202681\n",
      "Average test loss: 0.003321524030218522\n",
      "Epoch 99/300\n",
      "Average training loss: 0.11084774836566713\n",
      "Average test loss: 0.003321232545086079\n",
      "Epoch 100/300\n",
      "Average training loss: 0.11041654620567957\n",
      "Average test loss: 0.003542309342159165\n",
      "Epoch 101/300\n",
      "Average training loss: 0.11068012146817313\n",
      "Average test loss: 0.003315413041247262\n",
      "Epoch 102/300\n",
      "Average training loss: 0.1097041172252761\n",
      "Average test loss: 0.0033928172522121004\n",
      "Epoch 103/300\n",
      "Average training loss: 0.10972524917125702\n",
      "Average test loss: 0.003392172008131941\n",
      "Epoch 104/300\n",
      "Average training loss: 0.11179624762800004\n",
      "Average test loss: 0.003314861320787006\n",
      "Epoch 105/300\n",
      "Average training loss: 0.10962807965278626\n",
      "Average test loss: 0.003745477505028248\n",
      "Epoch 106/300\n",
      "Average training loss: 0.10940992005003823\n",
      "Average test loss: 0.0032838871818449762\n",
      "Epoch 107/300\n",
      "Average training loss: 0.1088651068939103\n",
      "Average test loss: 0.0033087520197861726\n",
      "Epoch 108/300\n",
      "Average training loss: 0.10834247861968146\n",
      "Average test loss: 0.020593749869200918\n",
      "Epoch 109/300\n",
      "Average training loss: 0.1086498029894299\n",
      "Average test loss: 0.003311913479740421\n",
      "Epoch 110/300\n",
      "Average training loss: 0.1078707599176301\n",
      "Average test loss: 0.0037517989739361737\n",
      "Epoch 111/300\n",
      "Average training loss: 0.10807164979643293\n",
      "Average test loss: 0.0033158601592812274\n",
      "Epoch 112/300\n",
      "Average training loss: 0.10781148482031293\n",
      "Average test loss: 0.003349689232599404\n",
      "Epoch 113/300\n",
      "Average training loss: 0.10735968936151928\n",
      "Average test loss: 0.030838549586633842\n",
      "Epoch 114/300\n",
      "Average training loss: 0.10759757569763395\n",
      "Average test loss: 0.003678213311566247\n",
      "Epoch 115/300\n",
      "Average training loss: 0.10663128728999031\n",
      "Average test loss: 0.0033479146481388143\n",
      "Epoch 116/300\n",
      "Average training loss: 0.10694767296976514\n",
      "Average test loss: 0.005713761299020714\n",
      "Epoch 117/300\n",
      "Average training loss: 0.10665617034170363\n",
      "Average test loss: 0.003337811082808508\n",
      "Epoch 118/300\n",
      "Average training loss: 0.10630708784527249\n",
      "Average test loss: 0.0032885118259323967\n",
      "Epoch 119/300\n",
      "Average training loss: 0.1058439597553677\n",
      "Average test loss: 0.0034616857500125962\n",
      "Epoch 120/300\n",
      "Average training loss: 0.10620203218195173\n",
      "Average test loss: 0.003403643577463097\n",
      "Epoch 121/300\n",
      "Average training loss: 0.10550983299811681\n",
      "Average test loss: 0.004643288858234882\n",
      "Epoch 122/300\n",
      "Average training loss: 0.10549369339148204\n",
      "Average test loss: 0.003458407748490572\n",
      "Epoch 123/300\n",
      "Average training loss: 0.10557399364974764\n",
      "Average test loss: 0.004189309669451581\n",
      "Epoch 124/300\n",
      "Average training loss: 0.1054531030787362\n",
      "Average test loss: 0.003959402743106087\n",
      "Epoch 125/300\n",
      "Average training loss: 0.1049263887140486\n",
      "Average test loss: 0.003326341941435304\n",
      "Epoch 126/300\n",
      "Average training loss: 0.10466900498337216\n",
      "Average test loss: 0.0034152492394463885\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10459621028767692\n",
      "Average test loss: 0.0036911021607617537\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10536825688680013\n",
      "Average test loss: 0.003394234255163206\n",
      "Epoch 129/300\n",
      "Average training loss: 0.1042072484029664\n",
      "Average test loss: 0.011516894800795449\n",
      "Epoch 130/300\n",
      "Average training loss: 0.11063409126467175\n",
      "Average test loss: 0.003486535519775417\n",
      "Epoch 131/300\n",
      "Average training loss: 0.46769364435805216\n",
      "Average test loss: 0.003685466223706802\n",
      "Epoch 132/300\n",
      "Average training loss: 0.2386280040078693\n",
      "Average test loss: 0.0036545253563672305\n",
      "Epoch 133/300\n",
      "Average training loss: 0.159052741739485\n",
      "Average test loss: 0.004352751748015483\n",
      "Epoch 134/300\n",
      "Average training loss: 0.1360263037217988\n",
      "Average test loss: 0.003434212655863828\n",
      "Epoch 135/300\n",
      "Average training loss: 0.12556753565867743\n",
      "Average test loss: 0.0033002092881749072\n",
      "Epoch 136/300\n",
      "Average training loss: 0.12133051708009508\n",
      "Average test loss: 0.0033087402565611735\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11683457914325926\n",
      "Average test loss: 0.0034119944303399987\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11475217594703038\n",
      "Average test loss: 0.0033110072799026964\n",
      "Epoch 139/300\n",
      "Average training loss: 0.11289572391907374\n",
      "Average test loss: 0.003629055668496423\n",
      "Epoch 140/300\n",
      "Average training loss: 0.11131629096799427\n",
      "Average test loss: 0.007988958965573046\n",
      "Epoch 141/300\n",
      "Average training loss: 0.1101284878651301\n",
      "Average test loss: 0.003618748575035069\n",
      "Epoch 142/300\n",
      "Average training loss: 0.10905323153071933\n",
      "Average test loss: 0.0033223288648037445\n",
      "Epoch 143/300\n",
      "Average training loss: 0.1081843928164906\n",
      "Average test loss: 0.0033904141053143475\n",
      "Epoch 144/300\n",
      "Average training loss: 0.10744718304607603\n",
      "Average test loss: 0.003317781542117397\n",
      "Epoch 145/300\n",
      "Average training loss: 0.106990141901705\n",
      "Average test loss: 0.0033012844795982043\n",
      "Epoch 146/300\n",
      "Average training loss: 0.10615260615613725\n",
      "Average test loss: 0.0032839453164488076\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10619942161771986\n",
      "Average test loss: 0.0033099182357804644\n",
      "Epoch 148/300\n",
      "Average training loss: 0.10488484178980191\n",
      "Average test loss: 0.0037335618254211215\n",
      "Epoch 149/300\n",
      "Average training loss: 0.1051646588312255\n",
      "Average test loss: 0.0034313080608844755\n",
      "Epoch 150/300\n",
      "Average training loss: 0.10444260827038024\n",
      "Average test loss: 0.003470546208322048\n",
      "Epoch 151/300\n",
      "Average training loss: 0.10429900547530915\n",
      "Average test loss: 0.0032947061873144575\n",
      "Epoch 152/300\n",
      "Average training loss: 0.10404472823937734\n",
      "Average test loss: 0.0033653250903719002\n",
      "Epoch 153/300\n",
      "Average training loss: 0.10402057868904538\n",
      "Average test loss: 0.003440409438891543\n",
      "Epoch 154/300\n",
      "Average training loss: 0.10353533055384954\n",
      "Average test loss: 0.003480845163886746\n",
      "Epoch 155/300\n",
      "Average training loss: 0.10336383293072382\n",
      "Average test loss: 0.0033874474805262356\n",
      "Epoch 156/300\n",
      "Average training loss: 0.1032589744064543\n",
      "Average test loss: 0.003465957522806194\n",
      "Epoch 157/300\n",
      "Average training loss: 0.10303604220019447\n",
      "Average test loss: 0.0033419720480839413\n",
      "Epoch 158/300\n",
      "Average training loss: 0.10336124231417974\n",
      "Average test loss: 0.0033880937362296714\n",
      "Epoch 159/300\n",
      "Average training loss: 0.10241806385914484\n",
      "Average test loss: 0.0032876362525340584\n",
      "Epoch 160/300\n",
      "Average training loss: 0.10248043249050776\n",
      "Average test loss: 0.0032717101449767747\n",
      "Epoch 161/300\n",
      "Average training loss: 0.10298288447327084\n",
      "Average test loss: 0.003345217659034663\n",
      "Epoch 162/300\n",
      "Average training loss: 0.10238041676415338\n",
      "Average test loss: 0.0032959676020675233\n",
      "Epoch 163/300\n",
      "Average training loss: 0.10205854165554047\n",
      "Average test loss: 0.003339277023035619\n",
      "Epoch 164/300\n",
      "Average training loss: 0.10165734838777118\n",
      "Average test loss: 0.0033688375250332885\n",
      "Epoch 165/300\n",
      "Average training loss: 0.10148845681879255\n",
      "Average test loss: 0.00329380800243881\n",
      "Epoch 166/300\n",
      "Average training loss: 0.1025213152302636\n",
      "Average test loss: 0.003775491292277972\n",
      "Epoch 167/300\n",
      "Average training loss: 0.10149050062232548\n",
      "Average test loss: 0.003338371378266149\n",
      "Epoch 168/300\n",
      "Average training loss: 0.1015427535838551\n",
      "Average test loss: 0.0035665972812308207\n",
      "Epoch 169/300\n",
      "Average training loss: 0.10099319899744458\n",
      "Average test loss: 0.003342053552468618\n",
      "Epoch 170/300\n",
      "Average training loss: 0.10120081405507193\n",
      "Average test loss: 0.003345350726611084\n",
      "Epoch 171/300\n",
      "Average training loss: 0.10092192908128103\n",
      "Average test loss: 0.003294167019219862\n",
      "Epoch 172/300\n",
      "Average training loss: 0.100703885032071\n",
      "Average test loss: 0.038837963672147854\n",
      "Epoch 173/300\n",
      "Average training loss: 0.10062432369258668\n",
      "Average test loss: 0.006760136991325352\n",
      "Epoch 174/300\n",
      "Average training loss: 0.10038067521651586\n",
      "Average test loss: 0.0039872912147806755\n",
      "Epoch 175/300\n",
      "Average training loss: 0.10083981662988663\n",
      "Average test loss: 0.003388062511260311\n",
      "Epoch 176/300\n",
      "Average training loss: 0.09974242181910409\n",
      "Average test loss: 0.003332351026021772\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10025584896405539\n",
      "Average test loss: 0.004215182606958681\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09967688518762588\n",
      "Average test loss: 0.003374978347371022\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09985729649994109\n",
      "Average test loss: 0.0033963827970955106\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09951374993721644\n",
      "Average test loss: 0.003458340896293521\n",
      "Epoch 181/300\n",
      "Average training loss: 0.09948982943428887\n",
      "Average test loss: 0.003375595687164201\n",
      "Epoch 182/300\n",
      "Average training loss: 0.10148688032229741\n",
      "Average test loss: 0.0033715010815196567\n",
      "Epoch 183/300\n",
      "Average training loss: 0.10007148279746374\n",
      "Average test loss: 0.003454628771584895\n",
      "Epoch 184/300\n",
      "Average training loss: 0.09886425462034014\n",
      "Average test loss: 0.003330082786579927\n",
      "Epoch 185/300\n",
      "Average training loss: 0.09956999422444238\n",
      "Average test loss: 0.003307059097207255\n",
      "Epoch 186/300\n",
      "Average training loss: 0.09941336415873633\n",
      "Average test loss: 0.003353239216324356\n",
      "Epoch 187/300\n",
      "Average training loss: 0.09861368401183022\n",
      "Average test loss: 0.003296202454302046\n",
      "Epoch 188/300\n",
      "Average training loss: 0.09865528027216594\n",
      "Average test loss: 0.004348036734180318\n",
      "Epoch 189/300\n",
      "Average training loss: 0.09967327044407527\n",
      "Average test loss: 0.003504840664772524\n",
      "Epoch 190/300\n",
      "Average training loss: 0.09837587840027279\n",
      "Average test loss: 0.0034172809428224963\n",
      "Epoch 191/300\n",
      "Average training loss: 0.09856623978084988\n",
      "Average test loss: 0.003347801039202346\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0983425953520669\n",
      "Average test loss: 0.003535876768330733\n",
      "Epoch 194/300\n",
      "Average training loss: 0.09785068405336804\n",
      "Average test loss: 0.0034992646140356858\n",
      "Epoch 195/300\n",
      "Average training loss: 0.09822157600853178\n",
      "Average test loss: 0.0033491393582274517\n",
      "Epoch 196/300\n",
      "Average training loss: 0.09782140890094969\n",
      "Average test loss: 0.008271147806611326\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09812850561406877\n",
      "Average test loss: 0.0035787857700553204\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09792839337057538\n",
      "Average test loss: 0.003349038017499778\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09754645707872178\n",
      "Average test loss: 0.003392144152894616\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09776141320996815\n",
      "Average test loss: 0.0034715669037153323\n",
      "Epoch 201/300\n",
      "Average training loss: 0.09818256510628594\n",
      "Average test loss: 0.0035020233601745633\n",
      "Epoch 202/300\n",
      "Average training loss: 0.09730969908502367\n",
      "Average test loss: 0.0034577328426142534\n",
      "Epoch 203/300\n",
      "Average training loss: 0.10004077100753785\n",
      "Average test loss: 0.0033974785717825096\n",
      "Epoch 204/300\n",
      "Average training loss: 0.09710456803772184\n",
      "Average test loss: 0.0033842906749082936\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09698303385575613\n",
      "Average test loss: 0.00470436242595315\n",
      "Epoch 206/300\n",
      "Average training loss: 0.09668366728226344\n",
      "Average test loss: 0.00442646022596293\n",
      "Epoch 207/300\n",
      "Average training loss: 0.0969400789671474\n",
      "Average test loss: 0.003505554330431753\n",
      "Epoch 208/300\n",
      "Average training loss: 0.09687648651334975\n",
      "Average test loss: 0.0033584296421872244\n",
      "Epoch 209/300\n",
      "Average training loss: 0.09661021279626422\n",
      "Average test loss: 0.0035115640606317253\n",
      "Epoch 210/300\n",
      "Average training loss: 0.09655387059847514\n",
      "Average test loss: 0.0033465888765123156\n",
      "Epoch 211/300\n",
      "Average training loss: 0.09621736864248912\n",
      "Average test loss: 0.0036107413549390103\n",
      "Epoch 212/300\n",
      "Average training loss: 0.09656460414330165\n",
      "Average test loss: 0.0034782898281183507\n",
      "Epoch 213/300\n",
      "Average training loss: 0.09680548787779278\n",
      "Average test loss: 0.0034497400650547612\n",
      "Epoch 214/300\n",
      "Average training loss: 0.09642078896363576\n",
      "Average test loss: 0.003441468819354971\n",
      "Epoch 215/300\n",
      "Average training loss: 0.09578644492228826\n",
      "Average test loss: 0.0038400508608255122\n",
      "Epoch 216/300\n",
      "Average training loss: 0.09758294943306181\n",
      "Average test loss: 0.003374351435651382\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0959722505344285\n",
      "Average test loss: 0.0036700797147221037\n",
      "Epoch 218/300\n",
      "Average training loss: 0.09559460084968142\n",
      "Average test loss: 0.003459035287093785\n",
      "Epoch 219/300\n",
      "Average training loss: 0.09579566685358683\n",
      "Average test loss: 0.0034380662623378967\n",
      "Epoch 220/300\n",
      "Average training loss: 0.09584398612711165\n",
      "Average test loss: 0.004397806387808588\n",
      "Epoch 221/300\n",
      "Average training loss: 0.09553144241041607\n",
      "Average test loss: 0.0034074507392942905\n",
      "Epoch 222/300\n",
      "Average training loss: 0.09570482165945901\n",
      "Average test loss: 0.0034773024399247436\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09528708961937163\n",
      "Average test loss: 0.0034155991029822166\n",
      "Epoch 224/300\n",
      "Average training loss: 0.09581784554984835\n",
      "Average test loss: 0.004023593608496918\n",
      "Epoch 225/300\n",
      "Average training loss: 0.09539708335532082\n",
      "Average test loss: 0.004061329388370116\n",
      "Epoch 226/300\n",
      "Average training loss: 0.0951317698624399\n",
      "Average test loss: 0.0034657963009344208\n",
      "Epoch 227/300\n",
      "Average training loss: 0.09573035813040204\n",
      "Average test loss: 0.003556080101057887\n",
      "Epoch 228/300\n",
      "Average training loss: 0.09513065522909164\n",
      "Average test loss: 0.003440694155999356\n",
      "Epoch 229/300\n",
      "Average training loss: 0.09495640460650126\n",
      "Average test loss: 0.003556985372470485\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0950235608153873\n",
      "Average test loss: 0.0044451248236000536\n",
      "Epoch 231/300\n",
      "Average training loss: 0.09664637600051032\n",
      "Average test loss: 0.00430297043443554\n",
      "Epoch 232/300\n",
      "Average training loss: 0.09539659461710188\n",
      "Average test loss: 0.020553797892398303\n",
      "Epoch 233/300\n",
      "Average training loss: 0.09498341178894043\n",
      "Average test loss: 0.003388921903239356\n",
      "Epoch 234/300\n",
      "Average training loss: 0.0948713261352645\n",
      "Average test loss: 0.003599537825004922\n",
      "Epoch 235/300\n",
      "Average training loss: 0.0946338765223821\n",
      "Average test loss: 0.0033646048814472227\n",
      "Epoch 236/300\n",
      "Average training loss: 0.09420491146379047\n",
      "Average test loss: 0.0035329440950105586\n",
      "Epoch 237/300\n",
      "Average training loss: 0.0943164353635576\n",
      "Average test loss: 0.004162863759116994\n",
      "Epoch 238/300\n",
      "Average training loss: 0.09495754728714625\n",
      "Average test loss: 0.003671028831973672\n",
      "Epoch 239/300\n",
      "Average training loss: 0.09473472998539607\n",
      "Average test loss: 0.0034753926526755094\n",
      "Epoch 240/300\n",
      "Average training loss: 0.09393828934431075\n",
      "Average test loss: 0.0048669519068466295\n",
      "Epoch 241/300\n",
      "Average training loss: 0.09440398007631302\n",
      "Average test loss: 0.003409146696743038\n",
      "Epoch 242/300\n",
      "Average training loss: 0.09424406097332637\n",
      "Average test loss: 0.003543919030576944\n",
      "Epoch 243/300\n",
      "Average training loss: 0.09401592959298027\n",
      "Average test loss: 0.00355579125136137\n",
      "Epoch 244/300\n",
      "Average training loss: 0.09401345619890425\n",
      "Average test loss: 0.0034793058389590847\n",
      "Epoch 245/300\n",
      "Average training loss: 0.09516073693831761\n",
      "Average test loss: 0.0033989497779144183\n",
      "Epoch 246/300\n",
      "Average training loss: 0.09349482268095016\n",
      "Average test loss: 0.004223847254282899\n",
      "Epoch 247/300\n",
      "Average training loss: 0.0939895648625162\n",
      "Average test loss: 0.003524329716546668\n",
      "Epoch 248/300\n",
      "Average training loss: 0.09414107331964705\n",
      "Average test loss: 0.003420154126567973\n",
      "Epoch 249/300\n",
      "Average training loss: 0.09331032664246029\n",
      "Average test loss: 0.003685231056685249\n",
      "Epoch 250/300\n",
      "Average training loss: 0.09384940891133414\n",
      "Average test loss: 0.003450025338679552\n",
      "Epoch 251/300\n",
      "Average training loss: 0.09445844621128506\n",
      "Average test loss: 0.003487674040098985\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09391586001714071\n",
      "Average test loss: 0.0035637628398835657\n",
      "Epoch 253/300\n",
      "Average training loss: 0.09391704561312994\n",
      "Average test loss: 0.003381816003471613\n",
      "Epoch 254/300\n",
      "Average training loss: 0.093408578256766\n",
      "Average test loss: 0.0035118043188833528\n",
      "Epoch 255/300\n",
      "Average training loss: 0.09354224204354816\n",
      "Average test loss: 0.0035305651240050795\n",
      "Epoch 256/300\n",
      "Average training loss: 0.09341429866684808\n",
      "Average test loss: 0.005407157553566827\n",
      "Epoch 257/300\n",
      "Average training loss: 0.09485578265455034\n",
      "Average test loss: 0.0034648087142656247\n",
      "Epoch 258/300\n",
      "Average training loss: 0.09256139045953751\n",
      "Average test loss: 0.004452662668294377\n",
      "Epoch 259/300\n",
      "Average training loss: 0.0928627053697904\n",
      "Average test loss: 0.003516568488544888\n",
      "Epoch 260/300\n",
      "Average training loss: 0.09366763983170191\n",
      "Average test loss: 0.010770553780926598\n",
      "Epoch 261/300\n",
      "Average training loss: 0.093642337097062\n",
      "Average test loss: 0.0035226250824828944\n",
      "Epoch 262/300\n",
      "Average training loss: 0.09463641341527303\n",
      "Average test loss: 0.003500251934553186\n",
      "Epoch 263/300\n",
      "Average training loss: 0.09321808411015405\n",
      "Average test loss: 0.003963277725295888\n",
      "Epoch 264/300\n",
      "Average training loss: 0.09276097687747743\n",
      "Average test loss: 0.0034753244012180303\n",
      "Epoch 265/300\n",
      "Average training loss: 0.09255259603261948\n",
      "Average test loss: 0.00351768848962254\n",
      "Epoch 266/300\n",
      "Average training loss: 0.09281382687224282\n",
      "Average test loss: 0.003487893329312404\n",
      "Epoch 267/300\n",
      "Average training loss: 0.09345275342464447\n",
      "Average test loss: 0.0034309592470526695\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09240467899375492\n",
      "Average test loss: 0.004555379276888238\n",
      "Epoch 269/300\n",
      "Average training loss: 0.09279412189457152\n",
      "Average test loss: 0.003500211284806331\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09318697992298339\n",
      "Average test loss: 0.0034724384310344854\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09260467890898387\n",
      "Average test loss: 0.0033969843205478457\n",
      "Epoch 272/300\n",
      "Average training loss: 0.09246769570642048\n",
      "Average test loss: 0.00346378512846099\n",
      "Epoch 273/300\n",
      "Average training loss: 0.09225563377804226\n",
      "Average test loss: 0.0034293963503506447\n",
      "Epoch 274/300\n",
      "Average training loss: 0.09308159219556385\n",
      "Average test loss: 0.0034696566015481947\n",
      "Epoch 275/300\n",
      "Average training loss: 0.09360913413100773\n",
      "Average test loss: 0.006569937810301781\n",
      "Epoch 276/300\n",
      "Average training loss: 0.09251193968454997\n",
      "Average test loss: 0.003499047173394097\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0922872687180837\n",
      "Average test loss: 0.0035774269064681396\n",
      "Epoch 278/300\n",
      "Average training loss: 0.09207384542624156\n",
      "Average test loss: 0.0035368149121188454\n",
      "Epoch 279/300\n",
      "Average training loss: 0.09273352508412468\n",
      "Average test loss: 0.0034336738396022054\n",
      "Epoch 280/300\n",
      "Average training loss: 0.09203147207366096\n",
      "Average test loss: 0.003495180544339948\n",
      "Epoch 281/300\n",
      "Average training loss: 0.09245612338516447\n",
      "Average test loss: 0.0062870204928848475\n",
      "Epoch 282/300\n",
      "Average training loss: 0.09223858840598001\n",
      "Average test loss: 0.003634029169463449\n",
      "Epoch 283/300\n",
      "Average training loss: 0.09179807937145233\n",
      "Average test loss: 0.0387631920211845\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09223281411992179\n",
      "Average test loss: 0.0034745802370210487\n",
      "Epoch 285/300\n",
      "Average training loss: 0.09205040581358803\n",
      "Average test loss: 0.0037488849949505595\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0921457979215516\n",
      "Average test loss: 0.0035717315276463826\n",
      "Epoch 287/300\n",
      "Average training loss: 0.09165754709641138\n",
      "Average test loss: 0.003985388619618283\n",
      "Epoch 288/300\n",
      "Average training loss: 0.09233885837925805\n",
      "Average test loss: 0.003524657237033049\n",
      "Epoch 289/300\n",
      "Average training loss: 0.09181836503081851\n",
      "Average test loss: 0.0038437627632584835\n",
      "Epoch 290/300\n",
      "Average training loss: 0.09289661680327521\n",
      "Average test loss: 0.0034298670494721994\n",
      "Epoch 291/300\n",
      "Average training loss: 0.09143982956144545\n",
      "Average test loss: 0.0058584044790930215\n",
      "Epoch 292/300\n",
      "Average training loss: 0.09146802661154005\n",
      "Average test loss: 0.00346739471786552\n",
      "Epoch 293/300\n",
      "Average training loss: 0.091337980694241\n",
      "Average test loss: 0.003550884943248497\n",
      "Epoch 294/300\n",
      "Average training loss: 0.09203433252043194\n",
      "Average test loss: 0.0034666416260103385\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0914604880809784\n",
      "Average test loss: 0.0035997471776273516\n",
      "Epoch 296/300\n",
      "Average training loss: 0.09145872165759404\n",
      "Average test loss: 0.003551533069875505\n",
      "Epoch 297/300\n",
      "Average training loss: 0.0915493695669704\n",
      "Average test loss: 0.003422933049706949\n",
      "Epoch 298/300\n",
      "Average training loss: 0.0912201889620887\n",
      "Average test loss: 0.0034235691407488453\n",
      "Epoch 299/300\n",
      "Average training loss: 0.09158112410704294\n",
      "Average test loss: 0.003624748443563779\n",
      "Epoch 300/300\n",
      "Average training loss: 0.09112231455908881\n",
      "Average test loss: 0.003488006506115198\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 10.577627712037827\n",
      "Average test loss: 0.008069192764659722\n",
      "Epoch 2/300\n",
      "Average training loss: 1.5818008577558729\n",
      "Average test loss: 0.006447024425698651\n",
      "Epoch 3/300\n",
      "Average training loss: 0.9283765807151795\n",
      "Average test loss: 0.005738803020781941\n",
      "Epoch 4/300\n",
      "Average training loss: 0.6371276846991645\n",
      "Average test loss: 0.004951099854790502\n",
      "Epoch 5/300\n",
      "Average training loss: 0.4790087007416619\n",
      "Average test loss: 0.004527506455779076\n",
      "Epoch 6/300\n",
      "Average training loss: 0.3858333540492588\n",
      "Average test loss: 0.004327108369519313\n",
      "Epoch 7/300\n",
      "Average training loss: 0.3238511924743652\n",
      "Average test loss: 0.00537643934165438\n",
      "Epoch 8/300\n",
      "Average training loss: 0.2803238652282291\n",
      "Average test loss: 0.004054572934077846\n",
      "Epoch 9/300\n",
      "Average training loss: 0.24737858200073243\n",
      "Average test loss: 0.004063098742109206\n",
      "Epoch 10/300\n",
      "Average training loss: 0.22256493344571857\n",
      "Average test loss: 0.004088259435569247\n",
      "Epoch 11/300\n",
      "Average training loss: 0.20333041580518088\n",
      "Average test loss: 0.003815906646144059\n",
      "Epoch 12/300\n",
      "Average training loss: 0.18940122837490506\n",
      "Average test loss: 0.004226725624667273\n",
      "Epoch 13/300\n",
      "Average training loss: 0.1775371660788854\n",
      "Average test loss: 0.005435881004151371\n",
      "Epoch 14/300\n",
      "Average training loss: 0.1687431314190229\n",
      "Average test loss: 0.004217159317185482\n",
      "Epoch 15/300\n",
      "Average training loss: 0.1597427186038759\n",
      "Average test loss: 0.006316823379033142\n",
      "Epoch 16/300\n",
      "Average training loss: 0.15305684067143335\n",
      "Average test loss: 0.0033921004869043826\n",
      "Epoch 17/300\n",
      "Average training loss: 0.14834042757087285\n",
      "Average test loss: 0.00367027810940312\n",
      "Epoch 18/300\n",
      "Average training loss: 0.14295471107959748\n",
      "Average test loss: 0.0033280498017039563\n",
      "Epoch 19/300\n",
      "Average training loss: 0.1374385945002238\n",
      "Average test loss: 0.0034474866847611137\n",
      "Epoch 20/300\n",
      "Average training loss: 0.1338853822482957\n",
      "Average test loss: 0.003218535670182771\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1287425239549743\n",
      "Average test loss: 0.0030077132388121553\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1247115618189176\n",
      "Average test loss: 0.003014873241384824\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12129579162597656\n",
      "Average test loss: 0.0028963150270283223\n",
      "Epoch 24/300\n",
      "Average training loss: 0.11810952143536674\n",
      "Average test loss: 0.0031076441879073777\n",
      "Epoch 25/300\n",
      "Average training loss: 0.11424706055720647\n",
      "Average test loss: 0.0031567020882955856\n",
      "Epoch 26/300\n",
      "Average training loss: 0.11106678426265716\n",
      "Average test loss: 0.0030809961224181784\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10886026408937242\n",
      "Average test loss: 0.0026695740463005173\n",
      "Epoch 28/300\n",
      "Average training loss: 0.10573709494537778\n",
      "Average test loss: 0.0026806801492348315\n",
      "Epoch 29/300\n",
      "Average training loss: 0.10373649864726596\n",
      "Average test loss: 0.0034925577056904634\n",
      "Epoch 30/300\n",
      "Average training loss: 0.10087507940000957\n",
      "Average test loss: 0.0027220993052340216\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09860480468802982\n",
      "Average test loss: 0.0026845036202834713\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09690314075681898\n",
      "Average test loss: 0.003051154568584429\n",
      "Epoch 33/300\n",
      "Average training loss: 0.09530322408676148\n",
      "Average test loss: 0.0025749219577345583\n",
      "Epoch 34/300\n",
      "Average training loss: 0.09433841570218404\n",
      "Average test loss: 0.0025769043063951862\n",
      "Epoch 35/300\n",
      "Average training loss: 0.09276516777276993\n",
      "Average test loss: 0.00327993224043813\n",
      "Epoch 36/300\n",
      "Average training loss: 0.09185111104779774\n",
      "Average test loss: 0.0026341174487024546\n",
      "Epoch 37/300\n",
      "Average training loss: 0.09029306319024828\n",
      "Average test loss: 0.003092285070982244\n",
      "Epoch 38/300\n",
      "Average training loss: 0.08990435973803203\n",
      "Average test loss: 0.002743066030450993\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0922156552473704\n",
      "Average test loss: 0.0025309745373411313\n",
      "Epoch 40/300\n",
      "Average training loss: 0.08778072170416513\n",
      "Average test loss: 0.00273314832461377\n",
      "Epoch 41/300\n",
      "Average training loss: 0.08742352696259817\n",
      "Average test loss: 0.0025324165282977953\n",
      "Epoch 42/300\n",
      "Average training loss: 0.08662026532491048\n",
      "Average test loss: 0.002482328400636713\n",
      "Epoch 43/300\n",
      "Average training loss: 0.08645392931501071\n",
      "Average test loss: 0.0025496097906596133\n",
      "Epoch 44/300\n",
      "Average training loss: 0.08543735196855333\n",
      "Average test loss: 0.0024649598143166966\n",
      "Epoch 45/300\n",
      "Average training loss: 0.08517196887731553\n",
      "Average test loss: 0.002727334081505736\n",
      "Epoch 46/300\n",
      "Average training loss: 0.08448048423396216\n",
      "Average test loss: 0.0033157860584970976\n",
      "Epoch 47/300\n",
      "Average training loss: 0.08406836850775613\n",
      "Average test loss: 0.0025317236019505394\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08998940903610653\n",
      "Average test loss: 0.002370240479086836\n",
      "Epoch 49/300\n",
      "Average training loss: 0.12102303341362211\n",
      "Average test loss: 0.0026310280211683775\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09789211786124441\n",
      "Average test loss: 0.002502423090653287\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09080380614598592\n",
      "Average test loss: 0.00253124654893246\n",
      "Epoch 52/300\n",
      "Average training loss: 0.0880366790758239\n",
      "Average test loss: 0.0023976062989483275\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08673413633637958\n",
      "Average test loss: 0.0024036391232576636\n",
      "Epoch 54/300\n",
      "Average training loss: 0.08560562570227517\n",
      "Average test loss: 0.0026229590363800524\n",
      "Epoch 55/300\n",
      "Average training loss: 0.08486282932096058\n",
      "Average test loss: 0.0024321765013866955\n",
      "Epoch 56/300\n",
      "Average training loss: 0.08398060811559359\n",
      "Average test loss: 0.002352288233012789\n",
      "Epoch 57/300\n",
      "Average training loss: 0.08371242689424091\n",
      "Average test loss: 0.002493940544211202\n",
      "Epoch 58/300\n",
      "Average training loss: 0.08366450072659387\n",
      "Average test loss: 0.00242399212386873\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08288927000098759\n",
      "Average test loss: 0.0039818983533316185\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0824670762485928\n",
      "Average test loss: 0.0024947914212114283\n",
      "Epoch 61/300\n",
      "Average training loss: 0.08332212222947015\n",
      "Average test loss: 0.0024178331773728134\n",
      "Epoch 62/300\n",
      "Average training loss: 0.08163733281360733\n",
      "Average test loss: 0.002450909027312365\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08145531075861719\n",
      "Average test loss: 0.0024042059518396856\n",
      "Epoch 64/300\n",
      "Average training loss: 0.08159438505437638\n",
      "Average test loss: 0.002419777371403244\n",
      "Epoch 65/300\n",
      "Average training loss: 0.08100610522760285\n",
      "Average test loss: 0.002344328997035821\n",
      "Epoch 66/300\n",
      "Average training loss: 0.08071987056401041\n",
      "Average test loss: 0.002346665664989915\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0806713228225708\n",
      "Average test loss: 0.002364863515107168\n",
      "Epoch 68/300\n",
      "Average training loss: 0.08290285814470714\n",
      "Average test loss: 0.0024987503737211227\n",
      "Epoch 69/300\n",
      "Average training loss: 0.08012679033146965\n",
      "Average test loss: 0.002348676452206241\n",
      "Epoch 70/300\n",
      "Average training loss: 0.07981141022841136\n",
      "Average test loss: 0.002994100947346952\n",
      "Epoch 71/300\n",
      "Average training loss: 0.07940120307935608\n",
      "Average test loss: 0.002341937897519933\n",
      "Epoch 72/300\n",
      "Average training loss: 0.07921897522277302\n",
      "Average test loss: 0.002393919654500981\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07899822440743447\n",
      "Average test loss: 0.0023770320523116323\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07925756555133395\n",
      "Average test loss: 0.0023069790932867262\n",
      "Epoch 75/300\n",
      "Average training loss: 0.0787159426079856\n",
      "Average test loss: 0.0023411162396272025\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07845702718363867\n",
      "Average test loss: 0.002394484533617894\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07851971408393647\n",
      "Average test loss: 0.0023628307082172897\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07804579910304811\n",
      "Average test loss: 0.0023962642272106474\n",
      "Epoch 79/300\n",
      "Average training loss: 0.07806223401096132\n",
      "Average test loss: 0.0024678507002277505\n",
      "Epoch 80/300\n",
      "Average training loss: 0.07752512363592784\n",
      "Average test loss: 0.0024015290327370166\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07746494312418832\n",
      "Average test loss: 0.21932129754622778\n",
      "Epoch 82/300\n",
      "Average training loss: 0.07746552863385943\n",
      "Average test loss: 0.0023238834985014464\n",
      "Epoch 83/300\n",
      "Average training loss: 0.07709230168660482\n",
      "Average test loss: 0.0023333782479166986\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07722360514932208\n",
      "Average test loss: 0.002313545598855449\n",
      "Epoch 85/300\n",
      "Average training loss: 0.07674289266268412\n",
      "Average test loss: 0.0023050544023927714\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07697612004147636\n",
      "Average test loss: 0.0024330113728841148\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07640452256798744\n",
      "Average test loss: 0.0023468573374880687\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07643472401301066\n",
      "Average test loss: 0.00227168619291236\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07604731920692656\n",
      "Average test loss: 0.002285688153778513\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07623354925380812\n",
      "Average test loss: 0.0023051705525981053\n",
      "Epoch 91/300\n",
      "Average training loss: 0.0765179487599267\n",
      "Average test loss: 0.004957563006836507\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07565391249789132\n",
      "Average test loss: 0.0024349849563505914\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07565956097178989\n",
      "Average test loss: 0.002340522131572167\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07531864818599489\n",
      "Average test loss: 0.0026037063224034177\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07565680471724934\n",
      "Average test loss: 0.00266547126798994\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07504573829306496\n",
      "Average test loss: 0.002533514657161302\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0760997292134497\n",
      "Average test loss: 0.0023477310463786124\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07500816212760078\n",
      "Average test loss: 0.002418137353948421\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07446169539954928\n",
      "Average test loss: 0.0025384549111541776\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07474514659908083\n",
      "Average test loss: 0.0022878107387158605\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07464491299125883\n",
      "Average test loss: 0.002297344082345565\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07784769830107689\n",
      "Average test loss: 0.0023128690734091732\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07425925608476003\n",
      "Average test loss: 0.0024117189208045604\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07403738951683045\n",
      "Average test loss: 0.0035621129170888\n",
      "Epoch 105/300\n",
      "Average training loss: 0.07390669559107886\n",
      "Average test loss: 0.002321741766192847\n",
      "Epoch 106/300\n",
      "Average training loss: 0.07411987449063195\n",
      "Average test loss: 0.002531619581911299\n",
      "Epoch 107/300\n",
      "Average training loss: 0.07394384715954462\n",
      "Average test loss: 0.0028745750474433104\n",
      "Epoch 108/300\n",
      "Average training loss: 0.07392337463299434\n",
      "Average test loss: 0.002298700394626293\n",
      "Epoch 109/300\n",
      "Average training loss: 0.073597606824504\n",
      "Average test loss: 0.0025315004483693177\n",
      "Epoch 110/300\n",
      "Average training loss: 0.07349629225995806\n",
      "Average test loss: 0.002272394159446574\n",
      "Epoch 111/300\n",
      "Average training loss: 0.07401891124248505\n",
      "Average test loss: 0.0022864912820772993\n",
      "Epoch 112/300\n",
      "Average training loss: 0.07301868987083435\n",
      "Average test loss: 0.0022997953177740175\n",
      "Epoch 113/300\n",
      "Average training loss: 0.07368241231309043\n",
      "Average test loss: 0.0022760114283818336\n",
      "Epoch 114/300\n",
      "Average training loss: 0.0729328260521094\n",
      "Average test loss: 0.0023998648491170673\n",
      "Epoch 115/300\n",
      "Average training loss: 0.07347185288535224\n",
      "Average test loss: 0.002332602840123905\n",
      "Epoch 116/300\n",
      "Average training loss: 0.07376976926459207\n",
      "Average test loss: 0.0030370348662965827\n",
      "Epoch 117/300\n",
      "Average training loss: 0.07286945669518577\n",
      "Average test loss: 0.0023348904440386427\n",
      "Epoch 118/300\n",
      "Average training loss: 0.07242183914449479\n",
      "Average test loss: 0.002380151009497543\n",
      "Epoch 119/300\n",
      "Average training loss: 0.07282600289583206\n",
      "Average test loss: 0.002443361105811265\n",
      "Epoch 120/300\n",
      "Average training loss: 0.07240438548061583\n",
      "Average test loss: 0.002388009368028078\n",
      "Epoch 121/300\n",
      "Average training loss: 0.07261720248063405\n",
      "Average test loss: 0.003263604668072528\n",
      "Epoch 122/300\n",
      "Average training loss: 0.07210657648245493\n",
      "Average test loss: 0.0023266431652009486\n",
      "Epoch 123/300\n",
      "Average training loss: 0.07294220073355569\n",
      "Average test loss: 0.002365495776136716\n",
      "Epoch 124/300\n",
      "Average training loss: 0.07215313637256622\n",
      "Average test loss: 0.005447560807896985\n",
      "Epoch 125/300\n",
      "Average training loss: 0.07167571757237116\n",
      "Average test loss: 0.002365490381088522\n",
      "Epoch 126/300\n",
      "Average training loss: 0.07201925755871667\n",
      "Average test loss: 0.0023768649999466205\n",
      "Epoch 127/300\n",
      "Average training loss: 0.07192089401019944\n",
      "Average test loss: 0.002344786649155948\n",
      "Epoch 128/300\n",
      "Average training loss: 0.07532967710494995\n",
      "Average test loss: 0.0023781967235522138\n",
      "Epoch 129/300\n",
      "Average training loss: 0.07164572528998057\n",
      "Average test loss: 0.0024357001611755953\n",
      "Epoch 130/300\n",
      "Average training loss: 0.07143002912402154\n",
      "Average test loss: 0.0023178589112228818\n",
      "Epoch 131/300\n",
      "Average training loss: 0.07153506875038147\n",
      "Average test loss: 0.012500933139481479\n",
      "Epoch 132/300\n",
      "Average training loss: 0.07136350895298851\n",
      "Average test loss: 0.002316814001235697\n",
      "Epoch 133/300\n",
      "Average training loss: 0.07147923847701815\n",
      "Average test loss: 0.0025494782312048807\n",
      "Epoch 134/300\n",
      "Average training loss: 0.07109821680519315\n",
      "Average test loss: 0.002370342108110587\n",
      "Epoch 135/300\n",
      "Average training loss: 0.07396601040495766\n",
      "Average test loss: 0.0023053451630597314\n",
      "Epoch 136/300\n",
      "Average training loss: 0.07109449639585283\n",
      "Average test loss: 0.0023251034781957666\n",
      "Epoch 137/300\n",
      "Average training loss: 0.07077981941567527\n",
      "Average test loss: 0.002325124561579691\n",
      "Epoch 138/300\n",
      "Average training loss: 0.07076480196581947\n",
      "Average test loss: 0.002362130782256524\n",
      "Epoch 139/300\n",
      "Average training loss: 0.07148561334609985\n",
      "Average test loss: 0.0023898280806218582\n",
      "Epoch 140/300\n",
      "Average training loss: 0.07078123099936379\n",
      "Average test loss: 0.0023044661233822504\n",
      "Epoch 141/300\n",
      "Average training loss: 0.07133644466267691\n",
      "Average test loss: 0.0024370353079090517\n",
      "Epoch 142/300\n",
      "Average training loss: 0.07059439043866264\n",
      "Average test loss: 0.002391659172769222\n",
      "Epoch 143/300\n",
      "Average training loss: 0.07047322109010484\n",
      "Average test loss: 0.031534158570898904\n",
      "Epoch 144/300\n",
      "Average training loss: 0.07163808649778367\n",
      "Average test loss: 0.002319675691736241\n",
      "Epoch 145/300\n",
      "Average training loss: 0.07040565538406372\n",
      "Average test loss: 0.002373483884665701\n",
      "Epoch 146/300\n",
      "Average training loss: 0.07246583578983942\n",
      "Average test loss: 0.002314151134548916\n",
      "Epoch 147/300\n",
      "Average training loss: 0.06990767235888375\n",
      "Average test loss: 0.0023681290585340726\n",
      "Epoch 148/300\n",
      "Average training loss: 0.0702398734556304\n",
      "Average test loss: 0.0023865025484313567\n",
      "Epoch 149/300\n",
      "Average training loss: 0.07050581585698658\n",
      "Average test loss: 0.002309547007497814\n",
      "Epoch 150/300\n",
      "Average training loss: 0.06998150566882558\n",
      "Average test loss: 0.002345994795569115\n",
      "Epoch 151/300\n",
      "Average training loss: 0.07021525087952614\n",
      "Average test loss: 0.002345351674904426\n",
      "Epoch 152/300\n",
      "Average training loss: 0.07094633249441783\n",
      "Average test loss: 0.00405901414487097\n",
      "Epoch 153/300\n",
      "Average training loss: 0.07020768425861994\n",
      "Average test loss: 0.0023289470747113227\n",
      "Epoch 154/300\n",
      "Average training loss: 0.06973324049181408\n",
      "Average test loss: 0.006963804839799801\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07144002403484451\n",
      "Average test loss: 0.002313342630656229\n",
      "Epoch 156/300\n",
      "Average training loss: 0.06942535589138667\n",
      "Average test loss: 0.00292712215334177\n",
      "Epoch 157/300\n",
      "Average training loss: 0.06949469294812945\n",
      "Average test loss: 0.0027321482197278078\n",
      "Epoch 158/300\n",
      "Average training loss: 0.06953568425774574\n",
      "Average test loss: 0.01622448995295498\n",
      "Epoch 159/300\n",
      "Average training loss: 0.06961222532722686\n",
      "Average test loss: 0.0023448254068692527\n",
      "Epoch 160/300\n",
      "Average training loss: 0.06944165038731363\n",
      "Average test loss: 0.003040348134520981\n",
      "Epoch 161/300\n",
      "Average training loss: 0.06941030424171024\n",
      "Average test loss: 0.002613482356071472\n",
      "Epoch 162/300\n",
      "Average training loss: 0.0697801730533441\n",
      "Average test loss: 0.0026112729030557803\n",
      "Epoch 163/300\n",
      "Average training loss: 0.06907734760310914\n",
      "Average test loss: 0.002322028856827981\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06937352817919519\n",
      "Average test loss: 0.002400332963301076\n",
      "Epoch 165/300\n",
      "Average training loss: 0.06923386673794853\n",
      "Average test loss: 0.00235927731419603\n",
      "Epoch 166/300\n",
      "Average training loss: 0.06918819507625368\n",
      "Average test loss: 0.002536542937366499\n",
      "Epoch 167/300\n",
      "Average training loss: 0.06938435798883438\n",
      "Average test loss: 0.002321101725515392\n",
      "Epoch 168/300\n",
      "Average training loss: 0.06918038487434387\n",
      "Average test loss: 0.0026064670872357155\n",
      "Epoch 169/300\n",
      "Average training loss: 0.06884940126538276\n",
      "Average test loss: 0.0024217429341127474\n",
      "Epoch 170/300\n",
      "Average training loss: 0.06908835250139236\n",
      "Average test loss: 0.002382950973076125\n",
      "Epoch 171/300\n",
      "Average training loss: 0.0695396027300093\n",
      "Average test loss: 0.0026966245021257137\n",
      "Epoch 172/300\n",
      "Average training loss: 0.0691742443508572\n",
      "Average test loss: 0.002352485383550326\n",
      "Epoch 173/300\n",
      "Average training loss: 0.06851353170805508\n",
      "Average test loss: 0.002357201649289992\n",
      "Epoch 174/300\n",
      "Average training loss: 0.06894412563906775\n",
      "Average test loss: 0.0024492379441443416\n",
      "Epoch 175/300\n",
      "Average training loss: 0.06858235691653358\n",
      "Average test loss: 0.002520753118313021\n",
      "Epoch 176/300\n",
      "Average training loss: 0.06913949325349596\n",
      "Average test loss: 0.002330297784051961\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0684071994887458\n",
      "Average test loss: 0.002353537485624353\n",
      "Epoch 178/300\n",
      "Average training loss: 0.06879607299301359\n",
      "Average test loss: 0.0023431796161457897\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06873126768403583\n",
      "Average test loss: 0.0023780255867168307\n",
      "Epoch 180/300\n",
      "Average training loss: 0.06977189705106947\n",
      "Average test loss: 0.002402780328359869\n",
      "Epoch 181/300\n",
      "Average training loss: 0.06825471056832208\n",
      "Average test loss: 0.002539432662849625\n",
      "Epoch 182/300\n",
      "Average training loss: 0.06809721822208828\n",
      "Average test loss: 0.002612361553021603\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0683679773343934\n",
      "Average test loss: 0.004209085627976391\n",
      "Epoch 184/300\n",
      "Average training loss: 0.06837849489185545\n",
      "Average test loss: 0.002675032284317745\n",
      "Epoch 185/300\n",
      "Average training loss: 0.06844340179363886\n",
      "Average test loss: 0.0023808652819651698\n",
      "Epoch 186/300\n",
      "Average training loss: 0.06852560332748625\n",
      "Average test loss: 0.0023737268635175292\n",
      "Epoch 187/300\n",
      "Average training loss: 0.0684300637907452\n",
      "Average test loss: 0.002396712404779262\n",
      "Epoch 188/300\n",
      "Average training loss: 0.06802102255821228\n",
      "Average test loss: 0.0028599870248387257\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06800224945942561\n",
      "Average test loss: 2197.954613905165\n",
      "Epoch 190/300\n",
      "Average training loss: 0.068088898340861\n",
      "Average test loss: 0.00239033776625163\n",
      "Epoch 191/300\n",
      "Average training loss: 0.06792650384373135\n",
      "Average test loss: 2738.129511501736\n",
      "Epoch 192/300\n",
      "Average training loss: 0.06836272997988595\n",
      "Average test loss: 0.008233043030111326\n",
      "Epoch 193/300\n",
      "Average training loss: 0.06782892568906149\n",
      "Average test loss: 0.002436148473785983\n",
      "Epoch 194/300\n",
      "Average training loss: 0.06788483037551245\n",
      "Average test loss: 0.002387280663785835\n",
      "Epoch 195/300\n",
      "Average training loss: 0.06792614130179088\n",
      "Average test loss: 0.010113750002450413\n",
      "Epoch 196/300\n",
      "Average training loss: 0.06741053023603227\n",
      "Average test loss: 0.002419305988173518\n",
      "Epoch 197/300\n",
      "Average training loss: 0.06793012764718798\n",
      "Average test loss: 0.002451153443919288\n",
      "Epoch 198/300\n",
      "Average training loss: 0.06773929133680132\n",
      "Average test loss: 0.0023919286692721978\n",
      "Epoch 199/300\n",
      "Average training loss: 0.06763456241952048\n",
      "Average test loss: 0.0023848858096947273\n",
      "Epoch 200/300\n",
      "Average training loss: 0.06784315602978071\n",
      "Average test loss: 0.00240464680434929\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06756460296445423\n",
      "Average test loss: 0.002519634917171465\n",
      "Epoch 202/300\n",
      "Average training loss: 0.06788939120372137\n",
      "Average test loss: 0.002440379083260066\n",
      "Epoch 203/300\n",
      "Average training loss: 0.06773223894172245\n",
      "Average test loss: 0.0023522471058078937\n",
      "Epoch 204/300\n",
      "Average training loss: 0.06703748565249973\n",
      "Average test loss: 0.002501871127014359\n",
      "Epoch 205/300\n",
      "Average training loss: 0.06751583932505714\n",
      "Average test loss: 0.0025779252040924296\n",
      "Epoch 206/300\n",
      "Average training loss: 0.0677790172762341\n",
      "Average test loss: 0.0036653965935111044\n",
      "Epoch 207/300\n",
      "Average training loss: 0.06719678650299708\n",
      "Average test loss: 0.002884785452236732\n",
      "Epoch 208/300\n",
      "Average training loss: 0.06747613096899456\n",
      "Average test loss: 0.002375079007819295\n",
      "Epoch 209/300\n",
      "Average training loss: 0.06785390414132013\n",
      "Average test loss: 0.0025879432362400823\n",
      "Epoch 210/300\n",
      "Average training loss: 0.06709675479597516\n",
      "Average test loss: 0.0023970793762968646\n",
      "Epoch 211/300\n",
      "Average training loss: 0.06682797791560491\n",
      "Average test loss: 0.0024186246780057746\n",
      "Epoch 212/300\n",
      "Average training loss: 0.06812604446543588\n",
      "Average test loss: 0.002398300940170884\n",
      "Epoch 213/300\n",
      "Average training loss: 0.06757806689209409\n",
      "Average test loss: 0.0024119374890708262\n",
      "Epoch 214/300\n",
      "Average training loss: 0.0668908129731814\n",
      "Average test loss: 0.0026956761978152726\n",
      "Epoch 215/300\n",
      "Average training loss: 0.06715877536932627\n",
      "Average test loss: 0.002486422380949888\n",
      "Epoch 216/300\n",
      "Average training loss: 0.06715957460138533\n",
      "Average test loss: 0.00239375954762929\n",
      "Epoch 217/300\n",
      "Average training loss: 0.06797541248798371\n",
      "Average test loss: 0.0024042534345967903\n",
      "Epoch 218/300\n",
      "Average training loss: 0.06678848575883442\n",
      "Average test loss: 0.008726282828590936\n",
      "Epoch 219/300\n",
      "Average training loss: 0.06685256325536304\n",
      "Average test loss: 0.002648147326687144\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06671555889315076\n",
      "Average test loss: 0.002426343561874496\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06683367425865597\n",
      "Average test loss: 0.013963676512655284\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06696265063683192\n",
      "Average test loss: 0.002465193044808176\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06665736690825887\n",
      "Average test loss: 0.0024254803551981845\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0667567090259658\n",
      "Average test loss: 0.0025984771149232984\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0665214726196395\n",
      "Average test loss: 0.002437119167712\n",
      "Epoch 226/300\n",
      "Average training loss: 0.06692892237504323\n",
      "Average test loss: 0.01749714234802458\n",
      "Epoch 227/300\n",
      "Average training loss: 0.06717565440469318\n",
      "Average test loss: 0.002672565810278886\n",
      "Epoch 228/300\n",
      "Average training loss: 0.06646270334058338\n",
      "Average test loss: 0.0028304915885544488\n",
      "Epoch 229/300\n",
      "Average training loss: 0.06641724167267482\n",
      "Average test loss: 0.002439618378670679\n",
      "Epoch 230/300\n",
      "Average training loss: 0.0669047572347853\n",
      "Average test loss: 0.0023858387565447226\n",
      "Epoch 231/300\n",
      "Average training loss: 0.06678180670738221\n",
      "Average test loss: 0.0028827541052467293\n",
      "Epoch 232/300\n",
      "Average training loss: 0.0661916268269221\n",
      "Average test loss: 0.002430952933306495\n",
      "Epoch 233/300\n",
      "Average training loss: 0.0693952384756671\n",
      "Average test loss: 0.002370898489943809\n",
      "Epoch 234/300\n",
      "Average training loss: 0.066220212717851\n",
      "Average test loss: 34741947.12404735\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07006604708565606\n",
      "Average test loss: 0.004774670707682768\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0659578221042951\n",
      "Average test loss: 0.00243550230593731\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06595965668559074\n",
      "Average test loss: 0.0025716896098521022\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06609638465775383\n",
      "Average test loss: 0.002843380613459481\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06632449083858066\n",
      "Average test loss: 0.07276527435249752\n",
      "Epoch 240/300\n",
      "Average training loss: 0.0665639571679963\n",
      "Average test loss: 0.02073034749428431\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06600286600324842\n",
      "Average test loss: 0.003114974116699563\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06594112454520332\n",
      "Average test loss: 18.4955919869211\n",
      "Epoch 243/300\n",
      "Average training loss: 0.06788195639186435\n",
      "Average test loss: 0.002471902301741971\n",
      "Epoch 244/300\n",
      "Average training loss: 0.06603709129492442\n",
      "Average test loss: 0.0024171537028418646\n",
      "Epoch 245/300\n",
      "Average training loss: 0.06574862688117557\n",
      "Average test loss: 0.003278447979854213\n",
      "Epoch 246/300\n",
      "Average training loss: 0.0660704718861315\n",
      "Average test loss: 0.0025442410802675617\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06608177405926917\n",
      "Average test loss: 0.0024813639250480467\n",
      "Epoch 248/300\n",
      "Average training loss: 0.06612429162197643\n",
      "Average test loss: 0.0023944929161419473\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06672606499327553\n",
      "Average test loss: 0.0023755828872736957\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06602140949169795\n",
      "Average test loss: 0.0024221994119385877\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06597434153821734\n",
      "Average test loss: 0.00246135101115538\n",
      "Epoch 252/300\n",
      "Average training loss: 0.06564312697119183\n",
      "Average test loss: 0.002558539933214585\n",
      "Epoch 253/300\n",
      "Average training loss: 0.06608302338255777\n",
      "Average test loss: 0.002520735562675529\n",
      "Epoch 254/300\n",
      "Average training loss: 0.06566446703672409\n",
      "Average test loss: 0.012535732955568367\n",
      "Epoch 255/300\n",
      "Average training loss: 0.06607879690991508\n",
      "Average test loss: 0.0025251026838603945\n",
      "Epoch 256/300\n",
      "Average training loss: 0.06591896147860421\n",
      "Average test loss: 0.005182326112563411\n",
      "Epoch 257/300\n",
      "Average training loss: 0.06632727225952678\n",
      "Average test loss: 0.002430488344695833\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06558452943298551\n",
      "Average test loss: 0.007496857913004027\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06562653165724543\n",
      "Average test loss: 0.002426325528985924\n",
      "Epoch 260/300\n",
      "Average training loss: 0.065736751695474\n",
      "Average test loss: 0.002461251695950826\n",
      "Epoch 261/300\n",
      "Average training loss: 0.06579303691784541\n",
      "Average test loss: 0.0024473426855272716\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06567096302244399\n",
      "Average test loss: 0.002909942219654719\n",
      "Epoch 263/300\n",
      "Average training loss: 0.06560047704643673\n",
      "Average test loss: 0.0032976254106809695\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06550261873006821\n",
      "Average test loss: 0.0024289620758758653\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06562171496947607\n",
      "Average test loss: 2.4830919264554976\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06546495252847671\n",
      "Average test loss: 0.0024511215846157734\n",
      "Epoch 267/300\n",
      "Average training loss: 0.06555240375465816\n",
      "Average test loss: 0.002577408238624533\n",
      "Epoch 268/300\n",
      "Average training loss: 0.06534549827045864\n",
      "Average test loss: 0.002449124747369852\n",
      "Epoch 269/300\n",
      "Average training loss: 0.06535448479321268\n",
      "Average test loss: 0.002473982626572251\n",
      "Epoch 270/300\n",
      "Average training loss: 0.06533017718460825\n",
      "Average test loss: 0.002964528175484803\n",
      "Epoch 271/300\n",
      "Average training loss: 0.06547864221202003\n",
      "Average test loss: 0.002433723362783591\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06523281879557503\n",
      "Average test loss: 0.0024441504675067134\n",
      "Epoch 273/300\n",
      "Average training loss: 0.06575363498263889\n",
      "Average test loss: 2.963039511332495\n",
      "Epoch 274/300\n",
      "Average training loss: 0.06507441826661428\n",
      "Average test loss: 0.0024590190125422344\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06579696561892827\n",
      "Average test loss: 0.0024674445222028427\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06530705291032791\n",
      "Average test loss: 0.002508611677007543\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06534988166888554\n",
      "Average test loss: 0.0030615414190623496\n",
      "Epoch 278/300\n",
      "Average training loss: 0.06511375614669587\n",
      "Average test loss: 0.002430771514359448\n",
      "Epoch 279/300\n",
      "Average training loss: 0.06557904346783956\n",
      "Average test loss: 0.0024814850739720796\n",
      "Epoch 280/300\n",
      "Average training loss: 0.06533810669183732\n",
      "Average test loss: 0.002701391239547067\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0648495822681321\n",
      "Average test loss: 0.0028133623856637213\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06538385128974915\n",
      "Average test loss: 0.002607128991641932\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06519372532102796\n",
      "Average test loss: 0.0024866958960062927\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06513051281372706\n",
      "Average test loss: 0.002468549129449659\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06491379406054815\n",
      "Average test loss: 0.00242693858479874\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06547565381394492\n",
      "Average test loss: 0.003320668239974313\n",
      "Epoch 287/300\n",
      "Average training loss: 0.06537445167038176\n",
      "Average test loss: 0.00249029373894963\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06479994325505363\n",
      "Average test loss: 0.002588590444376071\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06627150319019953\n",
      "Average test loss: 0.002446097359682123\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06504763601223627\n",
      "Average test loss: 0.002379548420301742\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06475536715984344\n",
      "Average test loss: 0.0023998482996183965\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06493113620413674\n",
      "Average test loss: 0.002628234624241789\n",
      "Epoch 293/300\n",
      "Average training loss: 0.064953379339642\n",
      "Average test loss: 0.0025112022219432725\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06472520303395059\n",
      "Average test loss: 0.050528297383959093\n",
      "Epoch 295/300\n",
      "Average training loss: 0.0654185938305325\n",
      "Average test loss: 0.002460033489494688\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06493707319100697\n",
      "Average test loss: 0.0024138076303319798\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06481643972131941\n",
      "Average test loss: 0.005150083120705353\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06486755961842007\n",
      "Average test loss: 0.002426330733837353\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06475637490881814\n",
      "Average test loss: 0.004369679413735866\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07296322462293837\n",
      "Average test loss: 0.003304688179658519\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 1293.806177295261\n",
      "Average test loss: 0.11979488128092554\n",
      "Epoch 2/300\n",
      "Average training loss: 15.26753711191813\n",
      "Average test loss: 0.022452557669745552\n",
      "Epoch 3/300\n",
      "Average training loss: 11.043403348286947\n",
      "Average test loss: 0.021202209705279935\n",
      "Epoch 4/300\n",
      "Average training loss: 8.518637218899197\n",
      "Average test loss: 0.0105085116132266\n",
      "Epoch 5/300\n",
      "Average training loss: 6.863747969309489\n",
      "Average test loss: 0.008859855378253593\n",
      "Epoch 6/300\n",
      "Average training loss: 5.601619683159722\n",
      "Average test loss: 0.00820570065619217\n",
      "Epoch 7/300\n",
      "Average training loss: 4.676092224968804\n",
      "Average test loss: 0.024280503280460834\n",
      "Epoch 8/300\n",
      "Average training loss: 3.9592676828172473\n",
      "Average test loss: 0.00607657878432009\n",
      "Epoch 9/300\n",
      "Average training loss: 3.3811859027014837\n",
      "Average test loss: 0.005661997038043208\n",
      "Epoch 10/300\n",
      "Average training loss: 2.930452892091539\n",
      "Average test loss: 0.06057705131255918\n",
      "Epoch 11/300\n",
      "Average training loss: 2.5277759535047744\n",
      "Average test loss: 0.011458956311146419\n",
      "Epoch 12/300\n",
      "Average training loss: 2.180690196355184\n",
      "Average test loss: 0.004712253227829933\n",
      "Epoch 13/300\n",
      "Average training loss: 1.902740329000685\n",
      "Average test loss: 0.004449350696471003\n",
      "Epoch 14/300\n",
      "Average training loss: 1.6410010845396255\n",
      "Average test loss: 0.004279303935459919\n",
      "Epoch 15/300\n",
      "Average training loss: 1.40885142771403\n",
      "Average test loss: 0.004095372975700432\n",
      "Epoch 16/300\n",
      "Average training loss: 1.2129547330008612\n",
      "Average test loss: 0.003932547944287459\n",
      "Epoch 17/300\n",
      "Average training loss: 1.0479908720652262\n",
      "Average test loss: 0.003916486541016235\n",
      "Epoch 18/300\n",
      "Average training loss: 0.9048156393898859\n",
      "Average test loss: 0.0036502300745083227\n",
      "Epoch 19/300\n",
      "Average training loss: 0.7788228646914164\n",
      "Average test loss: 0.0038094428301685386\n",
      "Epoch 20/300\n",
      "Average training loss: 0.6716438970035977\n",
      "Average test loss: 0.003605427327255408\n",
      "Epoch 21/300\n",
      "Average training loss: 0.579458975897895\n",
      "Average test loss: 0.003521543865195579\n",
      "Epoch 22/300\n",
      "Average training loss: 0.500739151318868\n",
      "Average test loss: 0.0033465092182159424\n",
      "Epoch 23/300\n",
      "Average training loss: 0.4333981836107042\n",
      "Average test loss: 0.003224127216057645\n",
      "Epoch 24/300\n",
      "Average training loss: 0.3783569821516673\n",
      "Average test loss: 0.003190418082392878\n",
      "Epoch 25/300\n",
      "Average training loss: 0.3332837297916412\n",
      "Average test loss: 0.003284843346931868\n",
      "Epoch 26/300\n",
      "Average training loss: 0.29557803361945684\n",
      "Average test loss: 0.002944184218015936\n",
      "Epoch 27/300\n",
      "Average training loss: 0.264451602379481\n",
      "Average test loss: 0.0028266560066905286\n",
      "Epoch 28/300\n",
      "Average training loss: 0.23685660882790882\n",
      "Average test loss: 0.002839543469250202\n",
      "Epoch 29/300\n",
      "Average training loss: 0.21406341103712717\n",
      "Average test loss: 0.0026892513173321884\n",
      "Epoch 30/300\n",
      "Average training loss: 0.19569549940692055\n",
      "Average test loss: 0.00250727863320046\n",
      "Epoch 31/300\n",
      "Average training loss: 0.18013810114065806\n",
      "Average test loss: 0.002644448720332649\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1658616041474872\n",
      "Average training loss: 0.15484204929404788\n",
      "Average test loss: 0.0028798721494774026\n",
      "Epoch 34/300\n",
      "Average training loss: 0.14606546869542864\n",
      "Average test loss: 0.002440327629240023\n",
      "Epoch 35/300\n",
      "Average training loss: 0.13188522599803076\n",
      "Average test loss: 0.0023613854729466967\n",
      "Epoch 37/300\n",
      "Average training loss: 0.12528767373826769\n",
      "Average test loss: 0.0022480568312522437\n",
      "Epoch 38/300\n",
      "Average training loss: 0.12032625973886914\n",
      "Average test loss: 0.0022048090301040146\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11596781810124715\n",
      "Average test loss: 0.002153494851870669\n",
      "Epoch 40/300\n",
      "Average training loss: 0.1119421444270346\n",
      "Average test loss: 0.0021853231098502876\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10786895312203301\n",
      "Average test loss: 0.002149432988112999\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10502486193842359\n",
      "Average test loss: 0.002607148739405804\n",
      "Epoch 43/300\n",
      "Average training loss: 0.15562817666927972\n",
      "Average test loss: 0.0024421138713757195\n",
      "Epoch 44/300\n",
      "Average training loss: 0.11075370144181781\n",
      "Average test loss: 0.0023705235249880286\n",
      "Epoch 45/300\n",
      "Average training loss: 0.10283795493841172\n",
      "Average test loss: 0.002160351743714677\n",
      "Epoch 46/300\n",
      "Average training loss: 0.09897396423419316\n",
      "Average test loss: 0.0021960737289239965\n",
      "Epoch 47/300\n",
      "Average training loss: 0.0951458548174964\n",
      "Average test loss: 0.002044331407070988\n",
      "Epoch 48/300\n",
      "Average training loss: 0.0923216542866495\n",
      "Average test loss: 0.002007031165063381\n",
      "Epoch 49/300\n",
      "Average training loss: 0.08932442369394832\n",
      "Average test loss: 0.002002257750266128\n",
      "Epoch 50/300\n",
      "Average training loss: 0.08697874531481001\n",
      "Average test loss: 0.002015504094047679\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0864128875931104\n",
      "Average test loss: 0.002055431990780764\n",
      "Epoch 52/300\n",
      "Average training loss: 0.08120632954438528\n",
      "Average test loss: 0.0019423566077732377\n",
      "Epoch 54/300\n",
      "Average training loss: 0.07989469432168536\n",
      "Average test loss: 0.0020774254805097977\n",
      "Epoch 55/300\n",
      "Average training loss: 0.07861479129393896\n",
      "Average test loss: 0.0019595153394879566\n",
      "Epoch 56/300\n",
      "Average training loss: 0.07752012854152256\n",
      "Average test loss: 0.0018962387174574866\n",
      "Epoch 57/300\n",
      "Average training loss: 0.07558695689837137\n",
      "Average test loss: 0.0018845399030380779\n",
      "Epoch 58/300\n",
      "Average training loss: 0.07460782821973165\n",
      "Average test loss: 0.001925148983589477\n",
      "Epoch 59/300\n",
      "Average training loss: 0.07339793699317508\n",
      "Average test loss: 0.0018550823303974337\n",
      "Epoch 60/300\n",
      "Average training loss: 0.07321149424049589\n",
      "Average test loss: 0.0032171307692511215\n",
      "Epoch 61/300\n",
      "Average training loss: 0.07238275097476111\n",
      "Average test loss: 0.0018557104949528972\n",
      "Epoch 62/300\n",
      "Average training loss: 0.07119028248389563\n",
      "Average test loss: 0.0019402301798885067\n",
      "Epoch 63/300\n",
      "Average training loss: 0.07019579477442635\n",
      "Average test loss: 0.0018158932670743928\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0692381016280916\n",
      "Average test loss: 0.0018785571976461344\n",
      "Epoch 65/300\n",
      "Average training loss: 0.06853495104445352\n",
      "Average test loss: 0.0019160941584656637\n",
      "Epoch 66/300\n",
      "Average training loss: 0.06859169791142146\n",
      "Average test loss: 0.0018869647323671314\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06756982119878133\n",
      "Average test loss: 0.0017947753129733933\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06644556981987423\n",
      "Average test loss: 0.015787459334565535\n",
      "Epoch 70/300\n",
      "Average training loss: 0.06793463671207428\n",
      "Average test loss: 0.001870021706343525\n",
      "Epoch 71/300\n",
      "Average training loss: 0.06640013458331426\n",
      "Average test loss: 0.0018167884579549233\n",
      "Epoch 72/300\n",
      "Average training loss: 0.06534909607304468\n",
      "Average test loss: 0.0017899910061516696\n",
      "Epoch 73/300\n",
      "Average training loss: 0.06539852911896175\n",
      "Average test loss: 0.0025291358046233654\n",
      "Epoch 74/300\n",
      "Average training loss: 0.06495563778612348\n",
      "Average test loss: 0.0018366576699126098\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07492804263366594\n",
      "Average test loss: 0.001856052394128508\n",
      "Epoch 76/300\n",
      "Average training loss: 0.06601333413852585\n",
      "Average test loss: 0.00176924142976188\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06482455444998211\n",
      "Average test loss: 0.0018546538972813223\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06428063835700353\n",
      "Average test loss: 0.0017465837576116125\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06410760208964347\n",
      "Average test loss: 0.0021525904246502453\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0635113991300265\n",
      "Average test loss: 0.002281223695518242\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06302822362383206\n",
      "Average test loss: 0.0017489905218697256\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06558408364984725\n",
      "Average test loss: 0.0017273390725668933\n",
      "Epoch 84/300\n",
      "Average training loss: 0.06244765923751725\n",
      "Average test loss: 0.00172018432430923\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06230774834089809\n",
      "Average test loss: 0.0018161101438519027\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06218719555271996\n",
      "Average test loss: 0.001711344412424498\n",
      "Epoch 87/300\n",
      "Average training loss: 0.062116053819656375\n",
      "Average test loss: 0.0017437117199103037\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06151580731736289\n",
      "Average test loss: 0.0017378243796734346\n",
      "Epoch 89/300\n",
      "Average training loss: 0.06159752554694811\n",
      "Average test loss: 0.0017473134665956928\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06148387126790153\n",
      "Average test loss: 0.0018438434403182731\n",
      "Epoch 91/300\n",
      "Average training loss: 0.060984432346291015\n",
      "Average test loss: 0.0017094415365200904\n",
      "Epoch 92/300\n",
      "Average training loss: 0.0614241460164388\n",
      "Average test loss: 0.0017080203062958188\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06082053283188078\n",
      "Average test loss: 0.001753291569960614\n",
      "Epoch 94/300\n",
      "Average training loss: 0.06038830401831203\n",
      "Average test loss: 0.0017302812641072605\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06278484826617771\n",
      "Average test loss: 0.001716416174131963\n",
      "Epoch 96/300\n",
      "Average training loss: 0.06011842441227701\n",
      "Average test loss: 0.0017301923303554454\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05998421119981342\n",
      "Average test loss: 0.0027215547767571275\n",
      "Epoch 98/300\n",
      "Average training loss: 0.06019849791791704\n",
      "Average test loss: 0.0017857535183429718\n",
      "Epoch 99/300\n",
      "Average training loss: 0.05979328864812851\n",
      "Average test loss: 0.0017032547568281492\n",
      "Epoch 100/300\n",
      "Average training loss: 0.0593090776403745\n",
      "Average test loss: 0.0019322794114963876\n",
      "Epoch 103/300\n",
      "Average training loss: 0.059390107032325536\n",
      "Average test loss: 0.0019869875402914153\n",
      "Epoch 104/300\n",
      "Average training loss: 0.05971342778868145\n",
      "Average test loss: 0.0017110403548512194\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05905362054374483\n",
      "Average test loss: 0.00174487797729671\n",
      "Epoch 106/300\n",
      "Average training loss: 0.05904075519243876\n",
      "Average test loss: 0.0017761143243147266\n",
      "Epoch 107/300\n",
      "Average training loss: 0.058704684949583474\n",
      "Average test loss: 0.0019203367361591921\n",
      "Epoch 108/300\n",
      "Average training loss: 0.05867341031630834\n",
      "Average test loss: 0.0018164003207865689\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05876690442032284\n",
      "Average test loss: 0.0017976426978905996\n",
      "Epoch 110/300\n",
      "Average training loss: 0.058643300162421336\n",
      "Average test loss: 0.001703598012940751\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05835454487469461\n",
      "Average test loss: 0.08754144374943441\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05830574568774965\n",
      "Average test loss: 0.0016843469492677184\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05842839140693347\n",
      "Average test loss: 0.0017091240407899023\n",
      "Epoch 114/300\n",
      "Average training loss: 0.05884545158346494\n",
      "Average test loss: 0.0017545902530352274\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05798662862181664\n",
      "Average test loss: 0.0024511285602218575\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05779240191313956\n",
      "Average test loss: 0.0018015301867077749\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05779082436362902\n",
      "Average test loss: 0.001701573677878413\n",
      "Epoch 118/300\n",
      "Average training loss: 0.057791555146376294\n",
      "Average test loss: 0.0017094117538589571\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05727941667371326\n",
      "Average test loss: 0.001880487412110799\n",
      "Epoch 122/300\n",
      "Average training loss: 0.057335494673914376\n",
      "Average test loss: 0.0016957638759372963\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05769272139999602\n",
      "Average test loss: 0.0017477334733638499\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05710636312762896\n",
      "Average test loss: 0.0017132346025771565\n",
      "Epoch 125/300\n",
      "Average training loss: 0.057564602265755334\n",
      "Average test loss: 0.001690577721533676\n",
      "Epoch 126/300\n",
      "Average training loss: 0.05683116377724542\n",
      "Average test loss: 0.0017777290095885596\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05693805232975218\n",
      "Average test loss: 0.0017742954983065527\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05707224176327388\n",
      "Average test loss: 0.0017463239970513516\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05778554005755319\n",
      "Average test loss: 0.004128450824361709\n",
      "Epoch 130/300\n",
      "Average training loss: 0.05667305902640025\n",
      "Average test loss: 0.0018939506027640567\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05680974523557557\n",
      "Average test loss: 0.0031277222004201678\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05661670494410727\n",
      "Average test loss: 0.0017241487432685164\n",
      "Epoch 133/300\n",
      "Average training loss: 0.05626384355624517\n",
      "Average test loss: 0.00188665311721464\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05650467571285036\n",
      "Average test loss: 0.0017238808272199498\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05711347054110633\n",
      "Average test loss: 0.0019139990490964717\n",
      "Epoch 138/300\n",
      "Average training loss: 0.056270397931337354\n",
      "Average test loss: 0.0020886859090791807\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05603944740692775\n",
      "Average test loss: 0.006763536022769081\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05587485901514689\n",
      "Average test loss: 0.0017385015193786886\n",
      "Epoch 141/300\n",
      "Average training loss: 0.05593227271901237\n",
      "Average test loss: 0.0017155943802661367\n",
      "Epoch 142/300\n",
      "Average training loss: 0.055783301721016566\n",
      "Average test loss: 0.00172601099508918\n",
      "Epoch 143/300\n",
      "Average training loss: 0.055583996007839837\n",
      "Average test loss: 0.0018222395746658245\n",
      "Epoch 144/300\n",
      "Average training loss: 0.056619490891695025\n",
      "Average test loss: 0.0017650549152038164\n",
      "Epoch 145/300\n",
      "Average training loss: 0.05559596570663982\n",
      "Average test loss: 0.001754759644365145\n",
      "Epoch 146/300\n",
      "Average training loss: 0.055392804622650144\n",
      "Average test loss: 0.002310399984733926\n",
      "Epoch 147/300\n",
      "Average training loss: 0.055590171032481726\n",
      "Average test loss: 0.0017585253468197252\n",
      "Epoch 148/300\n",
      "Average training loss: 0.05590973471601804\n",
      "Average test loss: 0.0034092057016160754\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05559330807460679\n",
      "Average test loss: 0.001704566913863851\n",
      "Epoch 150/300\n",
      "Average training loss: 0.05550074478321605\n",
      "Average test loss: 0.0020252559763482875\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05506645452313953\n",
      "Average test loss: 0.0017848168642570575\n",
      "Epoch 154/300\n",
      "Average training loss: 0.055317475931511984\n",
      "Average test loss: 0.0019273648827026289\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05495784202549193\n",
      "Average test loss: 0.001719667355943885\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05509562877813975\n",
      "Average test loss: 0.0017526832510613732\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05562848213977284\n",
      "Average test loss: 0.0017188320741471318\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05477909369601144\n",
      "Average test loss: 0.001905550941514472\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05468739868866073\n",
      "Average test loss: 0.0017617865257586042\n",
      "Epoch 160/300\n",
      "Average training loss: 0.054702823274665406\n",
      "Average test loss: 0.0017423487367729347\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05449096583988931\n",
      "Average test loss: 0.002214073487867912\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05509053217702442\n",
      "Average test loss: 0.0025964499447080824\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05510046134061283\n",
      "Average test loss: 0.00172253420141836\n",
      "Epoch 164/300\n",
      "Average training loss: 0.05455460160308414\n",
      "Average test loss: 0.0017838605247024032\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05445272022485733\n",
      "Average test loss: 0.001712107749345402\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05454619800051053\n",
      "Average test loss: 0.0027072025258094074\n",
      "Epoch 167/300\n",
      "Average training loss: 0.05418559493290054\n",
      "Average test loss: 0.0020536244054221446\n",
      "Epoch 168/300\n",
      "Average training loss: 0.054674923148420124\n",
      "Average test loss: 0.001750714332693153\n",
      "Epoch 169/300\n",
      "Average training loss: 0.054364252381854584\n",
      "Average test loss: 0.0018209721931359835\n",
      "Epoch 170/300\n",
      "Average training loss: 0.05432624658279949\n",
      "Average test loss: 0.0020288733998313547\n",
      "Epoch 171/300\n",
      "Average training loss: 0.05410222622421053\n",
      "Average test loss: 0.0018590011410415173\n",
      "Epoch 172/300\n",
      "Average training loss: 0.05392873927288585\n",
      "Average test loss: 0.0017221815563324425\n",
      "Epoch 173/300\n",
      "Average training loss: 0.05433900795711411\n",
      "Average test loss: 0.001877211856138375\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05495183031757673\n",
      "Average test loss: 0.0017636256543919445\n",
      "Epoch 175/300\n",
      "Average training loss: 0.0537110408378972\n",
      "Average test loss: 0.0018524840256820122\n",
      "Epoch 176/300\n",
      "Average training loss: 0.053776869154638716\n",
      "Average test loss: 0.0017424555183905694\n",
      "Epoch 177/300\n",
      "Average training loss: 0.054005673037634955\n",
      "Average test loss: 0.001751302076658855\n",
      "Epoch 178/300\n",
      "Average training loss: 0.05353960757785373\n",
      "Average test loss: 0.0017534596228765116\n",
      "Epoch 179/300\n",
      "Average training loss: 0.05385808491044574\n",
      "Average test loss: 0.0017617550810488563\n",
      "Epoch 182/300\n",
      "Average training loss: 0.053505816320578256\n",
      "Average test loss: 0.0017541346871811482\n",
      "Epoch 183/300\n",
      "Average training loss: 0.05403875838054551\n",
      "Average test loss: 0.0017680035650523172\n",
      "Epoch 184/300\n",
      "Average training loss: 0.05330083324180709\n",
      "Average test loss: 0.0018463268648419115\n",
      "Epoch 185/300\n",
      "Average training loss: 0.05360915950934092\n",
      "Average test loss: 0.00812960360944271\n",
      "Epoch 186/300\n",
      "Average training loss: 0.053323599623309244\n",
      "Average test loss: 0.0017300531544412177\n",
      "Epoch 187/300\n",
      "Average training loss: 0.053966575649049546\n",
      "Average test loss: 0.0017463697842839692\n",
      "Epoch 188/300\n",
      "Average training loss: 0.05342990999089347\n",
      "Average test loss: 0.0025004367724888853\n",
      "Epoch 189/300\n",
      "Average training loss: 0.054932593001259696\n",
      "Average test loss: 0.002571704246517685\n",
      "Epoch 190/300\n",
      "Average training loss: 0.05311460934744941\n",
      "Average test loss: 0.00177181077727841\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05301272827717993\n",
      "Average test loss: 0.0018466562084439727\n",
      "Epoch 192/300\n",
      "Average training loss: 0.053051170147127576\n",
      "Average test loss: 0.0019364801121668682\n",
      "Epoch 193/300\n",
      "Average training loss: 0.053257452921734914\n",
      "Average test loss: 0.024643256617916956\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05356939261489444\n",
      "Average test loss: 0.0017609916054126288\n",
      "Epoch 195/300\n",
      "Average training loss: 0.052942839963568585\n",
      "Average test loss: 0.029789428624841904\n",
      "Epoch 196/300\n",
      "Average training loss: 0.053321943455272254\n",
      "Average test loss: 0.00208088275231421\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05294901961750454\n",
      "Average test loss: 0.0017332548333538903\n",
      "Epoch 200/300\n",
      "Average training loss: 0.05278380697965622\n",
      "Average test loss: 0.0018025618410772747\n",
      "Epoch 201/300\n",
      "Average training loss: 0.052883212705453236\n",
      "Average test loss: 0.002121047037757105\n",
      "Epoch 202/300\n",
      "Average training loss: 0.052878880507416195\n",
      "Average test loss: 0.0017551227001887228\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05326543883151478\n",
      "Average test loss: 0.00350445478513009\n",
      "Epoch 204/300\n",
      "Average training loss: 0.052762620401051306\n",
      "Average test loss: 0.001780370141276055\n",
      "Epoch 205/300\n",
      "Average training loss: 0.053140213446484674\n",
      "Average test loss: 0.001848813937873476\n",
      "Epoch 206/300\n",
      "Average training loss: 0.052806129740344154\n",
      "Average test loss: 0.0018091732282191516\n",
      "Epoch 207/300\n",
      "Average training loss: 0.052834503329462476\n",
      "Average test loss: 0.0017849783335501948\n",
      "Epoch 208/300\n",
      "Average training loss: 0.052390282932255006\n",
      "Average test loss: 0.0017988624147449932\n",
      "Epoch 209/300\n",
      "Average training loss: 0.05356252443790436\n",
      "Average test loss: 0.0017487677686537305\n",
      "Epoch 210/300\n",
      "Average training loss: 0.052648315969440675\n",
      "Average test loss: 0.0021289200300557746\n",
      "Epoch 211/300\n",
      "Average training loss: 0.052387629161278405\n",
      "Average test loss: 0.0017939488867090808\n",
      "Epoch 212/300\n",
      "Average training loss: 0.052712231907579635\n",
      "Average test loss: 0.0021953535576661426\n",
      "Epoch 215/300\n",
      "Average training loss: 0.052606442471345265\n",
      "Average test loss: 0.0018694350611832408\n",
      "Epoch 216/300\n",
      "Average training loss: 0.2059623143672943\n",
      "Average test loss: 3.381203659640418\n",
      "Epoch 217/300\n",
      "Average training loss: 0.12868599569797515\n",
      "Average test loss: 0.0018531096678020226\n",
      "Epoch 218/300\n",
      "Average training loss: 0.08173761653237872\n",
      "Average test loss: 0.001910519989931749\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07316201487514708\n",
      "Average test loss: 0.0017825209525310331\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06861339816119935\n",
      "Average test loss: 0.011458095915615559\n",
      "Epoch 221/300\n",
      "Average training loss: 0.06534313286675347\n",
      "Average test loss: 0.0017362514121664896\n",
      "Epoch 222/300\n",
      "Average training loss: 0.06303075930807325\n",
      "Average test loss: 0.0017993185218009684\n",
      "Epoch 223/300\n",
      "Average training loss: 0.06110845009154744\n",
      "Average test loss: 0.0020086320959445503\n",
      "Epoch 224/300\n",
      "Average training loss: 0.059572101323140995\n",
      "Average test loss: 0.0017258156386928427\n",
      "Epoch 225/300\n",
      "Average training loss: 0.05823896392848756\n",
      "Average test loss: 0.00185927368597024\n",
      "Epoch 226/300\n",
      "Average training loss: 0.057086087190442614\n",
      "Average test loss: 0.002430810666539603\n",
      "Epoch 227/300\n",
      "Average training loss: 0.056158150103357105\n",
      "Average test loss: 0.002094769443695744\n",
      "Epoch 228/300\n",
      "Average training loss: 0.055346893890036476\n",
      "Average test loss: 0.0017699170220229362\n",
      "Epoch 229/300\n",
      "Average training loss: 0.05453981476691034\n",
      "Average test loss: 0.001940580641436908\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05319726590977775\n",
      "Average test loss: 0.001754897101678782\n",
      "Epoch 233/300\n",
      "Average training loss: 0.052960252748595345\n",
      "Average test loss: 0.0017801327914413478\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07202614629268646\n",
      "Average test loss: 0.0018053350524149007\n",
      "Epoch 235/300\n",
      "Average training loss: 0.05468400999903679\n",
      "Average test loss: 0.0017592304644899236\n",
      "Epoch 236/300\n",
      "Average training loss: 0.05272968343231413\n",
      "Average test loss: 0.0017569732045133908\n",
      "Epoch 237/300\n",
      "Average training loss: 0.05220618659920163\n",
      "Average test loss: 0.06352064927915732\n",
      "Epoch 238/300\n",
      "Average training loss: 0.05208841927846273\n",
      "Average test loss: 0.0018112149472451872\n",
      "Epoch 239/300\n",
      "Average training loss: 0.05215505572822359\n",
      "Average test loss: 0.0018034858228121366\n",
      "Epoch 240/300\n",
      "Average training loss: 0.052627621581157046\n",
      "Average test loss: 0.001954660430136654\n",
      "Epoch 241/300\n",
      "Average training loss: 0.052203427132633\n",
      "Average test loss: 0.00581983082741499\n",
      "Epoch 242/300\n",
      "Average training loss: 0.05236938604712486\n",
      "Average test loss: 0.0020317056936522324\n",
      "Epoch 243/300\n",
      "Average training loss: 0.05262874084711075\n",
      "Average test loss: 0.005202754449513223\n",
      "Epoch 244/300\n",
      "Average training loss: 0.05220860640207926\n",
      "Average test loss: 0.0017848067169802056\n",
      "Epoch 245/300\n",
      "Average training loss: 0.05241874004734887\n",
      "Average test loss: 0.0018071324710423747\n",
      "Epoch 246/300\n",
      "Average training loss: 0.05210103616118431\n",
      "Average test loss: 0.0018070863192487093\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05231811263495022\n",
      "Average test loss: 0.0021137244183984066\n",
      "Epoch 248/300\n",
      "Average training loss: 0.052344085044331023\n",
      "Average test loss: 0.001820348158478737\n",
      "Epoch 249/300\n",
      "Average training loss: 0.052177496105432514\n",
      "Average test loss: 0.04496516203714741\n",
      "Epoch 252/300\n",
      "Average training loss: 0.05192317965957854\n",
      "Average test loss: 0.002066724464711216\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05222450237141715\n",
      "Average test loss: 0.0017549150164963472\n",
      "Epoch 254/300\n",
      "Average training loss: 0.05238501849108272\n",
      "Average test loss: 0.001810584061882562\n",
      "Epoch 255/300\n",
      "Average training loss: 0.051997716635465624\n",
      "Average test loss: 0.001799799801988734\n",
      "Epoch 256/300\n",
      "Average training loss: 0.05165925125446585\n",
      "Average test loss: 0.0018652574792504312\n",
      "Epoch 257/300\n",
      "Average training loss: 0.05198918651209937\n",
      "Average test loss: 0.001821645398520761\n",
      "Epoch 258/300\n",
      "Average training loss: 0.06079990261462\n",
      "Average test loss: 0.0017324114385992289\n",
      "Epoch 259/300\n",
      "Average training loss: 0.05182148844997088\n",
      "Average test loss: 0.0017914266845004427\n",
      "Epoch 260/300\n",
      "Average training loss: 0.051324664351012975\n",
      "Average test loss: 0.0017425106869389613\n",
      "Epoch 261/300\n",
      "Average training loss: 0.05128396719694137\n",
      "Average test loss: 0.0018654101946287685\n",
      "Epoch 262/300\n",
      "Average training loss: 0.051704667813248104\n",
      "Average test loss: 0.0017557947809497515\n",
      "Epoch 263/300\n",
      "Average training loss: 0.05140285340613789\n",
      "Average test loss: 0.0017598601058125495\n",
      "Epoch 264/300\n",
      "Average training loss: 0.05166579226652781\n",
      "Average test loss: 0.00237364111861421\n",
      "Epoch 265/300\n",
      "Average training loss: 0.051956394520070816\n",
      "Average test loss: 0.0017940963599830865\n",
      "Epoch 266/300\n",
      "Average training loss: 0.05181268668837018\n",
      "Average test loss: 0.001847661914407379\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05151412696639697\n",
      "Average test loss: 0.0018222504413376252\n",
      "Epoch 268/300\n",
      "Average training loss: 0.051711215400033525\n",
      "Average test loss: 0.0017985077178519634\n",
      "Epoch 269/300\n",
      "Average training loss: 0.05255970546603203\n",
      "Average test loss: 0.001774748740821249\n",
      "Epoch 270/300\n",
      "Average training loss: 0.05149380091163847\n",
      "Average test loss: 0.0017633518388287888\n",
      "Epoch 271/300\n",
      "Average training loss: 0.05142936101555824\n",
      "Average test loss: 0.0017915488031382363\n",
      "Epoch 272/300\n",
      "Average training loss: 0.05129183030459616\n",
      "Average test loss: 0.001788243767287996\n",
      "Epoch 273/300\n",
      "Average training loss: 0.051626093576351804\n",
      "Average test loss: 0.0018030866645276547\n",
      "Epoch 274/300\n",
      "Average training loss: 0.05127216929197311\n",
      "Average test loss: 0.0017921004456778367\n",
      "Epoch 275/300\n",
      "Average training loss: 0.051339358826478325\n",
      "Average test loss: 0.001766093790013757\n",
      "Epoch 276/300\n",
      "Average training loss: 0.05158752600351969\n",
      "Average test loss: 0.0018891683739299576\n",
      "Epoch 277/300\n",
      "Average training loss: 0.05150098687741492\n",
      "Average test loss: 0.002136623264807794\n",
      "Epoch 278/300\n",
      "Average training loss: 0.051148019797272154\n",
      "Average test loss: 0.0018062520321044657\n",
      "Epoch 279/300\n",
      "Average training loss: 0.051842889206277\n",
      "Average test loss: 0.002418871646022631\n",
      "Epoch 280/300\n",
      "Average training loss: 0.05141001408961084\n",
      "Average test loss: 0.0018055544330014123\n",
      "Epoch 281/300\n",
      "Average training loss: 0.05106993528869417\n",
      "Average test loss: 0.0033333199659569394\n",
      "Epoch 282/300\n",
      "Average training loss: 0.05214406568805377\n",
      "Average test loss: 0.001825631133487655\n",
      "Epoch 283/300\n",
      "Average training loss: 0.051022905247079\n",
      "Average test loss: 0.0017933387809122602\n",
      "Epoch 284/300\n",
      "Average training loss: 0.05089642975727717\n",
      "Average test loss: 0.0019376305720458429\n",
      "Epoch 285/300\n",
      "Average training loss: 0.05121183142728276\n",
      "Average test loss: 0.0018647820339020755\n",
      "Epoch 286/300\n",
      "Average training loss: 0.051031113965643776\n",
      "Average test loss: 0.0031226911513755717\n",
      "Epoch 287/300\n",
      "Average training loss: 0.051583217779795326\n",
      "Average test loss: 0.001795184366715451\n",
      "Epoch 288/300\n",
      "Average training loss: 0.05090795529882113\n",
      "Average test loss: 0.058864357738859126\n",
      "Epoch 289/300\n",
      "Average training loss: 0.051147529949744544\n",
      "Average test loss: 0.0018314326649738683\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05080463275975651\n",
      "Average test loss: 0.0018233991937918795\n",
      "Epoch 291/300\n",
      "Average training loss: 0.051154034396012626\n",
      "Average test loss: 0.0018237248009277715\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05117858513196309\n",
      "Average test loss: 0.0023047468676749205\n",
      "Epoch 293/300\n",
      "Average training loss: 0.0515192301372687\n",
      "Average test loss: 0.002338289899337623\n",
      "Epoch 294/300\n",
      "Average training loss: 0.05067596432566643\n",
      "Average test loss: 0.0018046975618021355\n",
      "Epoch 295/300\n",
      "Average training loss: 0.05072456752260526\n",
      "Average test loss: 0.0018365917950868608\n",
      "Epoch 296/300\n",
      "Average training loss: 0.05084340368045701\n",
      "Average test loss: 0.0018834168215592702\n",
      "Epoch 297/300\n",
      "Average training loss: 0.051479211327102446\n",
      "Average test loss: 0.0018085684722496404\n",
      "Epoch 298/300\n",
      "Average training loss: 0.05108717496858703\n",
      "Average test loss: 0.006451620363112953\n",
      "Epoch 299/300\n",
      "Average training loss: 0.050882641050550674\n",
      "Average test loss: 0.0018040119724141226\n",
      "Epoch 300/300\n",
      "Average training loss: 0.050927607162131204\n",
      "Average test loss: 0.001852287977726923\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth3-.025/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 19.20\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.52\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.17\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.10\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.64\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.01\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.31\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 24.33\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 24.48\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 24.73\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 24.78\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 24.86\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 24.85\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 24.83\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 25.19\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 25.36\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 25.35\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 25.46\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 25.68\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 25.76\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 25.60\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 25.66\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 25.84\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 25.71\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 25.83\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 25.91\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 25.95\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.02\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.00\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 26.03\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.72\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.09\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.08\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.12\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.21\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 27.40\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 27.41\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 27.38\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 27.58\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 27.67\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 27.78\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 27.69\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 27.85\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 27.92\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 28.00\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 27.86\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 27.98\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 28.10\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 28.02\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 28.14\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 27.99\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 28.26\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 28.18\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 28.29\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 28.36\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 28.27\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 28.31\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.43\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.05\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.41\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.67\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.61\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.90\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 29.05\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.09\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.16\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.12\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.29\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.36\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.46\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.45\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.40\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.60\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.47\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.52\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 29.61\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.76\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.55\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.30\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.01\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.35\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.67\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.83\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.93\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.02\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.14\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.11\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.13\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 30.37\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 30.42\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 30.44\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 30.43\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 30.38\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 30.47\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 30.53\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 30.52\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 30.60\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 30.54\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 30.58\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 30.64\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.70\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
