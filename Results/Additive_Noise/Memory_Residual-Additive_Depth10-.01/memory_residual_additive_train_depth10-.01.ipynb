{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d51225-6fda-4ea9-af57-26b2b60569cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from Auxiliary_Functions.generate_observation import generate_observations, split_dataset\n",
    "from Auxiliary_Functions.save_progress import save_progress\n",
    "from Auxiliary_Functions.matrix_normalization import normalize_max_row_norm\n",
    "\n",
    "from Denoising_Algorithms.Projectors.DnCNN import DnCNN\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR\n",
    "from Denoising_Algorithms.DL_Training.loss_functions import IntermediateScaledLoss\n",
    "from Denoising_Algorithms.DL_Training.training import train_main\n",
    "from Denoising_Algorithms.Memory_Network.Memory_Net import MemoryNetwork\n",
    "from Denoising_Algorithms.DL_Training.evaluation import average_PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b70c7c6-61c6-4b32-981a-f98b05104232",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trial = 0\n",
    "\n",
    "torch.manual_seed(current_trial)\n",
    "numpy.random.seed(current_trial)\n",
    "random.seed(current_trial)\n",
    "torch.cuda.manual_seed(current_trial)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a15bd73-7bf2-470b-bfa3-6f2fd5c323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_10.pkl', 'rb') as f:\n",
    "    gaussian_10 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_20.pkl', 'rb') as f:\n",
    "    gaussian_20 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_30.pkl', 'rb') as f:\n",
    "    gaussian_30 = pickle.load(f)\n",
    "with open('Auxiliary_Functions/sensing_matrices/50 x 50 images/Gaussian_40.pkl', 'rb') as f:\n",
    "    gaussian_40 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dbb662-5673-4553-95c9-f1bfb8805e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ILSVRC2012_50x50_small.pkl', 'rb') as f:\n",
    "    ILSVRC2012 = pickle.load(f)\n",
    "\n",
    "# We only work with the first 25000 images\n",
    "img_dataset = ILSVRC2012[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7e1e-ab10-40ee-ad32-56b7e4e86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_10_normalized = normalize_max_row_norm(gaussian_10)\n",
    "gaussian_20_normalized = normalize_max_row_norm(gaussian_20)\n",
    "gaussian_30_normalized = normalize_max_row_norm(gaussian_30)\n",
    "gaussian_40_normalized = normalize_max_row_norm(gaussian_40)\n",
    "\n",
    "gaussian_10_observations = generate_observations(img_dataset, gaussian_10_normalized, 0.01)\n",
    "gaussian_20_observations = generate_observations(img_dataset, gaussian_20_normalized, 0.01)\n",
    "gaussian_30_observations = generate_observations(img_dataset, gaussian_30_normalized, 0.01)\n",
    "gaussian_40_observations = generate_observations(img_dataset, gaussian_40_normalized, 0.01)\n",
    "\n",
    "gaussian_10_train, gaussian_10_test = split_dataset(gaussian_10_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_20_train, gaussian_20_test = split_dataset(gaussian_20_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_30_train, gaussian_30_test = split_dataset(gaussian_30_observations, train_ratio = 0.9, seed = 0)\n",
    "gaussian_40_train, gaussian_40_test = split_dataset(gaussian_40_observations, train_ratio = 0.9, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ed207f-8049-45f2-a7e5-4a250bb7af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = DnCNN(depth = 10)\n",
    "\n",
    "# Equal Weighting\n",
    "loss_function = IntermediateScaledLoss(omega = 1.0)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9771be38-7c00-4dad-8871-d9c1993f61bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.28116231168640987\n",
      "Average test loss: 0.013998244442045688\n",
      "Epoch 2/300\n",
      "Average training loss: 0.10253174093696805\n",
      "Average test loss: 0.011639097789095507\n",
      "Epoch 3/300\n",
      "Average training loss: 0.08349368072880639\n",
      "Average test loss: 0.014221046048733922\n",
      "Epoch 4/300\n",
      "Average training loss: 0.0753149357702997\n",
      "Average test loss: 0.009103507315119108\n",
      "Epoch 5/300\n",
      "Average training loss: 0.0691623524957233\n",
      "Average test loss: 0.015326453828977214\n",
      "Epoch 6/300\n",
      "Average training loss: 0.06374343168735504\n",
      "Average test loss: 0.009467638175520631\n",
      "Epoch 7/300\n",
      "Average training loss: 0.06198756770292918\n",
      "Average test loss: 0.008526010759174824\n",
      "Epoch 8/300\n",
      "Average training loss: 0.05813114857673645\n",
      "Average test loss: 0.009317908874402444\n",
      "Epoch 9/300\n",
      "Average training loss: 0.056453960537910464\n",
      "Average test loss: 0.009949641190469266\n",
      "Epoch 10/300\n",
      "Average training loss: 0.053942013674312166\n",
      "Average test loss: 0.008711476021342807\n",
      "Epoch 11/300\n",
      "Average training loss: 0.0518493669629097\n",
      "Average test loss: 0.0343242803596788\n",
      "Epoch 12/300\n",
      "Average training loss: 0.05011900562710232\n",
      "Average test loss: 0.008184275969035096\n",
      "Epoch 13/300\n",
      "Average training loss: 0.04847447370489438\n",
      "Average test loss: 0.00747342507292827\n",
      "Epoch 14/300\n",
      "Average training loss: 0.04698493138949077\n",
      "Average test loss: 0.008476401991314358\n",
      "Epoch 15/300\n",
      "Average training loss: 0.04563938567373488\n",
      "Average test loss: 0.01174297516958581\n",
      "Epoch 16/300\n",
      "Average training loss: 0.04414745171202553\n",
      "Average test loss: 0.007431550739539994\n",
      "Epoch 17/300\n",
      "Average training loss: 0.04291799944308069\n",
      "Average test loss: 0.007265480280336406\n",
      "Epoch 18/300\n",
      "Average training loss: 0.04204148022002644\n",
      "Average test loss: 0.006727530981103579\n",
      "Epoch 19/300\n",
      "Average training loss: 0.041778600815269684\n",
      "Average test loss: 0.007803244881331921\n",
      "Epoch 20/300\n",
      "Average training loss: 0.0405224036110772\n",
      "Average test loss: 0.007809675114850203\n",
      "Epoch 21/300\n",
      "Average training loss: 0.03969179293513298\n",
      "Average test loss: 0.006597916584875849\n",
      "Epoch 22/300\n",
      "Average training loss: 0.039090623014503056\n",
      "Average test loss: 0.0072268260820872255\n",
      "Epoch 23/300\n",
      "Average training loss: 0.038502431521813076\n",
      "Average test loss: 0.006504047222435474\n",
      "Epoch 24/300\n",
      "Average training loss: 0.038001967671844696\n",
      "Average test loss: 0.006402032074001101\n",
      "Epoch 25/300\n",
      "Average training loss: 0.03729627470175425\n",
      "Average test loss: 0.006504712250911527\n",
      "Epoch 26/300\n",
      "Average training loss: 0.03717036393450366\n",
      "Average test loss: 0.006354653530443708\n",
      "Epoch 27/300\n",
      "Average training loss: 0.03664410545759731\n",
      "Average test loss: 0.008566337706728114\n",
      "Epoch 28/300\n",
      "Average training loss: 0.036549615263938906\n",
      "Average test loss: 0.00660958741646674\n",
      "Epoch 29/300\n",
      "Average training loss: 0.035922260291046566\n",
      "Average test loss: 0.009538764889041583\n",
      "Epoch 30/300\n",
      "Average training loss: 0.03549959801303016\n",
      "Average test loss: 0.008426329706278112\n",
      "Epoch 31/300\n",
      "Average training loss: 0.03510135680768225\n",
      "Average test loss: 0.006174187584055794\n",
      "Epoch 32/300\n",
      "Average training loss: 0.03483685918980175\n",
      "Average test loss: 0.006357195330990686\n",
      "Epoch 33/300\n",
      "Average training loss: 0.03483197262055344\n",
      "Average test loss: 0.006289529870367712\n",
      "Epoch 34/300\n",
      "Average training loss: 0.03427463899552822\n",
      "Average test loss: 0.006187520261440012\n",
      "Epoch 35/300\n",
      "Average training loss: 0.03399218934112125\n",
      "Average test loss: 0.0061291476951705086\n",
      "Epoch 36/300\n",
      "Average training loss: 0.033827904633349844\n",
      "Average test loss: 0.006188392700834407\n",
      "Epoch 37/300\n",
      "Average training loss: 0.033668595363696416\n",
      "Average test loss: 0.006480478795866171\n",
      "Epoch 38/300\n",
      "Average training loss: 0.03359191112385856\n",
      "Average test loss: 0.0065724941343069075\n",
      "Epoch 39/300\n",
      "Average training loss: 0.0331540591650539\n",
      "Average test loss: 0.00602186668291688\n",
      "Epoch 40/300\n",
      "Average training loss: 0.03296173011925485\n",
      "Average test loss: 0.006277889229771164\n",
      "Epoch 41/300\n",
      "Average training loss: 0.032733163237571715\n",
      "Average test loss: 0.006898121265073618\n",
      "Epoch 42/300\n",
      "Average training loss: 0.03257312776810593\n",
      "Average test loss: 0.008024117631216843\n",
      "Epoch 43/300\n",
      "Average training loss: 0.03241223069528738\n",
      "Average test loss: 0.006042187439070808\n",
      "Epoch 44/300\n",
      "Average training loss: 0.032278279102510875\n",
      "Average test loss: 0.008138008938895332\n",
      "Epoch 45/300\n",
      "Average training loss: 0.032049850394328436\n",
      "Average test loss: 0.006268439609143469\n",
      "Epoch 46/300\n",
      "Average training loss: 0.031895553843842614\n",
      "Average test loss: 0.007601552421020137\n",
      "Epoch 47/300\n",
      "Average training loss: 0.031809381605850326\n",
      "Average test loss: 0.006167600340313381\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03174786818855339\n",
      "Average test loss: 0.006384086264918248\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03162414160205258\n",
      "Average test loss: 0.007054522223770618\n",
      "Epoch 50/300\n",
      "Average training loss: 0.031317584478192856\n",
      "Average test loss: 0.00657684322570761\n",
      "Epoch 51/300\n",
      "Average training loss: 0.0312741913927926\n",
      "Average test loss: 0.007029081576814254\n",
      "Epoch 52/300\n",
      "Average training loss: 0.031231431123283176\n",
      "Average test loss: 0.005900414271901051\n",
      "Epoch 53/300\n",
      "Average training loss: 0.030986002173688677\n",
      "Average test loss: 0.0068667363127072655\n",
      "Epoch 54/300\n",
      "Average training loss: 0.030976837323771583\n",
      "Average test loss: 0.01830202827685409\n",
      "Epoch 55/300\n",
      "Average training loss: 0.030791028492980533\n",
      "Average test loss: 0.006283064312818978\n",
      "Epoch 56/300\n",
      "Average training loss: 0.03064381182193756\n",
      "Average test loss: 0.0060383949097659855\n",
      "Epoch 57/300\n",
      "Average training loss: 0.030609361552529864\n",
      "Average test loss: 0.005976641075478659\n",
      "Epoch 58/300\n",
      "Average training loss: 0.030510319858789443\n",
      "Average test loss: 0.006127601739433076\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030387801854146853\n",
      "Average test loss: 0.006128420771823989\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030340492596228917\n",
      "Average test loss: 0.00742998537255658\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030368998368581136\n",
      "Average test loss: 0.006311235944016112\n",
      "Epoch 62/300\n",
      "Average training loss: 0.03009939534465472\n",
      "Average test loss: 0.00928211138356063\n",
      "Epoch 63/300\n",
      "Average training loss: 0.030043802787860233\n",
      "Average test loss: 0.006333189312368631\n",
      "Epoch 64/300\n",
      "Average training loss: 0.029978947858015696\n",
      "Average test loss: 0.006625516219271554\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02989428122507201\n",
      "Average test loss: 0.00635984458691544\n",
      "Epoch 66/300\n",
      "Average training loss: 0.02988842977086703\n",
      "Average test loss: 0.006090728986594412\n",
      "Epoch 67/300\n",
      "Average training loss: 0.029719659686088563\n",
      "Average test loss: 0.00658066299971607\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02963586414853732\n",
      "Average test loss: 0.006082286721716325\n",
      "Epoch 69/300\n",
      "Average training loss: 0.02958643048670557\n",
      "Average test loss: 0.007675677810692125\n",
      "Epoch 70/300\n",
      "Average training loss: 0.02945511620574527\n",
      "Average test loss: 0.006690509712530507\n",
      "Epoch 71/300\n",
      "Average training loss: 0.02949798651205169\n",
      "Average test loss: 0.00654574784802066\n",
      "Epoch 72/300\n",
      "Average training loss: 0.02937618103954527\n",
      "Average test loss: 0.006623196061286661\n",
      "Epoch 73/300\n",
      "Average training loss: 0.029260400940974554\n",
      "Average test loss: 0.021383099815911716\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0293027004516787\n",
      "Average test loss: 0.009001979755030738\n",
      "Epoch 75/300\n",
      "Average training loss: 0.029248969591326182\n",
      "Average test loss: 0.005907571348879073\n",
      "Epoch 76/300\n",
      "Average training loss: 0.029116518871651755\n",
      "Average test loss: 0.0062088585160672664\n",
      "Epoch 77/300\n",
      "Average training loss: 0.029033664335807164\n",
      "Average test loss: 0.006155181616544723\n",
      "Epoch 78/300\n",
      "Average training loss: 0.028970049460728963\n",
      "Average test loss: 0.009725145457519426\n",
      "Epoch 79/300\n",
      "Average training loss: 0.028875174219409626\n",
      "Average test loss: 0.0061532823075022966\n",
      "Epoch 80/300\n",
      "Average training loss: 0.028889924021230803\n",
      "Average test loss: 0.006152337382237117\n",
      "Epoch 81/300\n",
      "Average training loss: 0.028836781746811337\n",
      "Average test loss: 0.006167409469683965\n",
      "Epoch 82/300\n",
      "Average training loss: 0.028805896361668904\n",
      "Average test loss: 0.006375256866216659\n",
      "Epoch 83/300\n",
      "Average training loss: 0.028711860800782838\n",
      "Average test loss: 0.006125776094694932\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02866343928873539\n",
      "Average test loss: 0.0062510413370198675\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02869189167684979\n",
      "Average test loss: 0.007432567165129715\n",
      "Epoch 86/300\n",
      "Average training loss: 0.028593581659926308\n",
      "Average test loss: 0.006014988423635562\n",
      "Epoch 87/300\n",
      "Average training loss: 0.028494506544537015\n",
      "Average test loss: 0.006970500645538171\n",
      "Epoch 88/300\n",
      "Average training loss: 0.028427806516488394\n",
      "Average test loss: 0.006251030137141546\n",
      "Epoch 89/300\n",
      "Average training loss: 0.028432720783683987\n",
      "Average test loss: 0.006064449630263779\n",
      "Epoch 90/300\n",
      "Average training loss: 0.028320891817410786\n",
      "Average test loss: 0.006337128254688448\n",
      "Epoch 91/300\n",
      "Average training loss: 0.028370892418755424\n",
      "Average test loss: 0.006103245948337846\n",
      "Epoch 92/300\n",
      "Average training loss: 0.02827322288023101\n",
      "Average test loss: 0.0069769473150372505\n",
      "Epoch 93/300\n",
      "Average training loss: 0.028200306695368556\n",
      "Average test loss: 0.006028958001898395\n",
      "Epoch 94/300\n",
      "Average training loss: 0.028004466235637664\n",
      "Average test loss: 0.13614360161622366\n",
      "Epoch 99/300\n",
      "Average training loss: 0.028043373627795114\n",
      "Average test loss: 0.006165978826996353\n",
      "Epoch 100/300\n",
      "Average training loss: 0.027891127568152214\n",
      "Average test loss: 0.0069383459935585656\n",
      "Epoch 101/300\n",
      "Average training loss: 0.027902077603671287\n",
      "Average test loss: 0.00597181609686878\n",
      "Epoch 102/300\n",
      "Average training loss: 0.027851972603135638\n",
      "Average test loss: 0.006197727782858742\n",
      "Epoch 103/300\n",
      "Average training loss: 0.027838538656632106\n",
      "Average test loss: 0.006011083482868142\n",
      "Epoch 104/300\n",
      "Average training loss: 0.02778400415016545\n",
      "Average test loss: 0.006717086243546671\n",
      "Epoch 105/300\n",
      "Average training loss: 0.02771631738046805\n",
      "Average test loss: 0.006018290577249395\n",
      "Epoch 106/300\n",
      "Average training loss: 0.02772216393550237\n",
      "Average test loss: 0.007789966089857949\n",
      "Epoch 107/300\n",
      "Average training loss: 0.027668890721268123\n",
      "Average test loss: 0.006861943839324845\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02753629421028826\n",
      "Average test loss: 0.007044215749949217\n",
      "Epoch 110/300\n",
      "Average training loss: 0.027560071354111034\n",
      "Average test loss: 0.0072128229633801515\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02755799885922008\n",
      "Average test loss: 0.006652311437659793\n",
      "Epoch 112/300\n",
      "Average training loss: 0.02749729135301378\n",
      "Average test loss: 0.00610712763791283\n",
      "Epoch 113/300\n",
      "Average training loss: 0.027397045956717596\n",
      "Average test loss: 0.0061087209064927366\n",
      "Epoch 118/300\n",
      "Average training loss: 0.027343514290120868\n",
      "Average test loss: 0.006488300239874257\n",
      "Epoch 119/300\n",
      "Average training loss: 0.027278856597012943\n",
      "Average test loss: 0.007315384871429867\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02725318958527512\n",
      "Average test loss: 0.006301436896953318\n",
      "Epoch 121/300\n",
      "Average training loss: 0.02720685417122311\n",
      "Average test loss: 0.006416683089401987\n",
      "Epoch 122/300\n",
      "Average training loss: 0.027129286552468934\n",
      "Average test loss: 0.0068276812202400635\n",
      "Epoch 123/300\n",
      "Average training loss: 0.027233903497457503\n",
      "Average test loss: 0.006300681995848815\n",
      "Epoch 124/300\n",
      "Average training loss: 0.027150889403290218\n",
      "Average test loss: 0.00615467249477903\n",
      "Epoch 125/300\n",
      "Average training loss: 0.027091695643133587\n",
      "Average test loss: 0.006445130791101191\n",
      "Epoch 126/300\n",
      "Average training loss: 0.027095972705218525\n",
      "Average test loss: 0.006535194740113285\n",
      "Epoch 127/300\n",
      "Average training loss: 0.02701790530814065\n",
      "Average test loss: 0.006064150195154879\n",
      "Epoch 128/300\n",
      "Average training loss: 0.02703728841410743\n",
      "Average test loss: 0.006520397540181876\n",
      "Epoch 129/300\n",
      "Average training loss: 0.027007453721430565\n",
      "Average test loss: 0.008703176091942523\n",
      "Epoch 130/300\n",
      "Average training loss: 0.02699734981689188\n",
      "Average test loss: 0.006200914661089579\n",
      "Epoch 131/300\n",
      "Average training loss: 0.026931476395991114\n",
      "Average test loss: 0.006892787083155579\n",
      "Epoch 132/300\n",
      "Average training loss: 0.026907811593678263\n",
      "Average test loss: 0.006078793827030394\n",
      "Epoch 133/300\n",
      "Average training loss: 0.026897031757566664\n",
      "Average test loss: 0.006106171517736382\n",
      "Epoch 134/300\n",
      "Average training loss: 0.026991753520237076\n",
      "Average test loss: 0.006552801101158062\n",
      "Epoch 135/300\n",
      "Average training loss: 0.026838180649611684\n",
      "Average test loss: 0.006368400604360633\n",
      "Epoch 136/300\n",
      "Average training loss: 0.026771001777715152\n",
      "Average test loss: 0.0062590086836781765\n",
      "Epoch 137/300\n",
      "Average training loss: 0.02691114759941896\n",
      "Average test loss: 0.006377946236895191\n",
      "Epoch 138/300\n",
      "Average training loss: 0.026768264494008487\n",
      "Average test loss: 0.006904578070673678\n",
      "Epoch 139/300\n",
      "Average training loss: 0.026705237438281376\n",
      "Average test loss: 0.00673133918394645\n",
      "Epoch 140/300\n",
      "Average training loss: 0.026783987863196267\n",
      "Average test loss: 0.006158555719587538\n",
      "Epoch 141/300\n",
      "Average training loss: 0.026727846266494857\n",
      "Average test loss: 0.00720378380558557\n",
      "Epoch 142/300\n",
      "Average training loss: 0.02664683391650518\n",
      "Average test loss: 0.006245479368087318\n",
      "Epoch 143/300\n",
      "Average training loss: 0.026592839201291404\n",
      "Average test loss: 0.006089150094323688\n",
      "Epoch 144/300\n",
      "Average training loss: 0.026638765244020358\n",
      "Average test loss: 0.011149809670945008\n",
      "Epoch 145/300\n",
      "Average training loss: 0.02658128899666998\n",
      "Average test loss: 0.006180311294479502\n",
      "Epoch 146/300\n",
      "Average training loss: 0.02657740154531267\n",
      "Average test loss: 0.006573622805376848\n",
      "Epoch 147/300\n",
      "Average training loss: 0.026603990602824423\n",
      "Average test loss: 0.006195707645474209\n",
      "Epoch 148/300\n",
      "Average training loss: 0.026583783941136467\n",
      "Average test loss: 0.0060783466754688155\n",
      "Epoch 149/300\n",
      "Average training loss: 0.026502287147773636\n",
      "Average test loss: 0.006959211922354169\n",
      "Epoch 150/300\n",
      "Average training loss: 0.026484945742620364\n",
      "Average test loss: 0.006837080787453387\n",
      "Epoch 151/300\n",
      "Average training loss: 0.02646235605743196\n",
      "Average test loss: 0.006511172393543853\n",
      "Epoch 152/300\n",
      "Average training loss: 0.026400144944588342\n",
      "Average test loss: 0.007299823001441028\n",
      "Epoch 157/300\n",
      "Average training loss: 0.026394249737262725\n",
      "Average test loss: 0.006894487815598647\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02631925600104862\n",
      "Average test loss: 0.006853654083278444\n",
      "Epoch 159/300\n",
      "Average training loss: 0.026397120242317516\n",
      "Average test loss: 0.006756479582438866\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02634467775788572\n",
      "Average test loss: 0.006263774168988069\n",
      "Epoch 161/300\n",
      "Average training loss: 0.026293757480051784\n",
      "Average test loss: 0.006290932533848617\n",
      "Epoch 162/300\n",
      "Average training loss: 0.026283363075719938\n",
      "Average test loss: 0.006489702705293894\n",
      "Epoch 163/300\n",
      "Average training loss: 0.026252374202013016\n",
      "Average test loss: 0.006255676050980886\n",
      "Epoch 164/300\n",
      "Average training loss: 0.026212322481804422\n",
      "Average test loss: 0.00626920698914263\n",
      "Epoch 165/300\n",
      "Average training loss: 0.02624260594447454\n",
      "Average test loss: 0.006341811680545409\n",
      "Epoch 166/300\n",
      "Average training loss: 0.026199040411247147\n",
      "Average test loss: 0.006371118258270953\n",
      "Epoch 167/300\n",
      "Average training loss: 0.02619223816692829\n",
      "Average test loss: 0.006479633375174469\n",
      "Epoch 168/300\n",
      "Average training loss: 0.026199673684106934\n",
      "Average test loss: 0.006553900382171075\n",
      "Epoch 169/300\n",
      "Average training loss: 0.026130346051520772\n",
      "Average test loss: 0.006267794836312532\n",
      "Epoch 170/300\n",
      "Average training loss: 0.02618007046646542\n",
      "Average test loss: 0.006702152530766196\n",
      "Epoch 171/300\n",
      "Average training loss: 0.026137020193868214\n",
      "Average test loss: 0.006362525617496835\n",
      "Epoch 172/300\n",
      "Average training loss: 0.026039009261462423\n",
      "Average test loss: 0.006463037935396035\n",
      "Epoch 173/300\n",
      "Average training loss: 0.026120000862412982\n",
      "Average test loss: 0.006851494180245532\n",
      "Epoch 174/300\n",
      "Average training loss: 0.026108867365452978\n",
      "Average test loss: 0.006218961349791951\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02604077595141199\n",
      "Average test loss: 0.006241671584546566\n",
      "Epoch 176/300\n",
      "Average training loss: 0.026063503603968356\n",
      "Average test loss: 0.006442249629232619\n",
      "Epoch 177/300\n",
      "Average training loss: 0.02599225822918945\n",
      "Average test loss: 0.006491943911131885\n",
      "Epoch 178/300\n",
      "Average training loss: 0.0260504569494062\n",
      "Average test loss: 0.006318595607454578\n",
      "Epoch 179/300\n",
      "Average training loss: 0.02602230010761155\n",
      "Average test loss: 0.006349638699243466\n",
      "Epoch 180/300\n",
      "Average training loss: 0.026016812599367566\n",
      "Average test loss: 0.006411298810193936\n",
      "Epoch 181/300\n",
      "Average training loss: 0.025937004644009803\n",
      "Average test loss: 0.006309579867455694\n",
      "Epoch 182/300\n",
      "Average training loss: 0.025952263969514104\n",
      "Average test loss: 0.006316703315410349\n",
      "Epoch 183/300\n",
      "Average training loss: 0.025919618560208215\n",
      "Average test loss: 0.006834410207966963\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02602435545457734\n",
      "Average test loss: 0.0065373885370790955\n",
      "Epoch 185/300\n",
      "Average training loss: 0.025909454830818707\n",
      "Average test loss: 0.006629462974973851\n",
      "Epoch 186/300\n",
      "Average training loss: 0.02594441824985875\n",
      "Average test loss: 0.006628374968965848\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02584242223203182\n",
      "Average test loss: 0.006222041947146257\n",
      "Epoch 188/300\n",
      "Average training loss: 0.025970140563117132\n",
      "Average test loss: 0.006822731769747205\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02588510445257028\n",
      "Average test loss: 0.0062484397602578\n",
      "Epoch 190/300\n",
      "Average training loss: 0.025819662218292554\n",
      "Average test loss: 0.006402508379684554\n",
      "Epoch 191/300\n",
      "Average training loss: 0.025745143545998467\n",
      "Average test loss: 0.0063310173783037394\n",
      "Epoch 192/300\n",
      "Average training loss: 0.025848849925729965\n",
      "Average test loss: 0.006244743605454762\n",
      "Epoch 193/300\n",
      "Average training loss: 0.025785733140177198\n",
      "Average test loss: 0.00657220033597615\n",
      "Epoch 194/300\n",
      "Average training loss: 0.02575663287937641\n",
      "Average test loss: 0.0062799149114224646\n",
      "Epoch 195/300\n",
      "Average training loss: 0.02578975517551104\n",
      "Average test loss: 0.006446950848110848\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02573869406680266\n",
      "Average test loss: 0.00622927417481939\n",
      "Epoch 197/300\n",
      "Average training loss: 0.025741557538509367\n",
      "Average test loss: 0.006235216890358263\n",
      "Epoch 198/300\n",
      "Average training loss: 0.02581932807299826\n",
      "Average test loss: 0.0069633662479205265\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0256958952728245\n",
      "Average test loss: 0.0063218947868380285\n",
      "Epoch 200/300\n",
      "Average training loss: 0.025736558818154866\n",
      "Average test loss: 0.006282031024495761\n",
      "Epoch 201/300\n",
      "Average training loss: 0.02569969460864862\n",
      "Average test loss: 0.006342508491128683\n",
      "Epoch 202/300\n",
      "Average training loss: 0.025686260215110248\n",
      "Average test loss: 0.006461202533708678\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02565794241759512\n",
      "Average test loss: 0.006710376616153452\n",
      "Epoch 204/300\n",
      "Average training loss: 0.02574263517227438\n",
      "Average test loss: 0.006669277941601144\n",
      "Epoch 205/300\n",
      "Average training loss: 0.025614986896514893\n",
      "Average test loss: 0.006389151723434528\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02566727604965369\n",
      "Average test loss: 0.007941359077062872\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02562769291136\n",
      "Average test loss: 0.007325036182999611\n",
      "Epoch 208/300\n",
      "Average training loss: 0.02557166027526061\n",
      "Average test loss: 0.00631222546307577\n",
      "Epoch 209/300\n",
      "Average training loss: 0.0255847011340989\n",
      "Average test loss: 0.006449947108411127\n",
      "Epoch 210/300\n",
      "Average training loss: 0.025513607806629604\n",
      "Average test loss: 0.006394241582188341\n",
      "Epoch 211/300\n",
      "Average training loss: 0.025608241902457342\n",
      "Average test loss: 0.006377611204153962\n",
      "Epoch 212/300\n",
      "Average training loss: 0.025532851308584213\n",
      "Average test loss: 0.006568089683022764\n",
      "Epoch 213/300\n",
      "Average training loss: 0.025508341569039555\n",
      "Average test loss: 0.006573327716026041\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02554137227932612\n",
      "Average test loss: 0.006560512327071694\n",
      "Epoch 215/300\n",
      "Average training loss: 0.025507740284005802\n",
      "Average test loss: 0.0067045961316261025\n",
      "Epoch 216/300\n",
      "Average training loss: 0.025539223672615158\n",
      "Average test loss: 0.006297086073292627\n",
      "Epoch 217/300\n",
      "Average training loss: 0.025470241321457757\n",
      "Average test loss: 0.006387867040104336\n",
      "Epoch 218/300\n",
      "Average training loss: 0.025479483533236716\n",
      "Average test loss: 0.006973925052417649\n",
      "Epoch 219/300\n",
      "Average training loss: 0.02543948143058353\n",
      "Average test loss: 0.006608829905589421\n",
      "Epoch 220/300\n",
      "Average training loss: 0.025441936077343092\n",
      "Average test loss: 0.007036593865603209\n",
      "Epoch 221/300\n",
      "Average training loss: 0.025466997027397156\n",
      "Average test loss: 0.006682404252803988\n",
      "Epoch 222/300\n",
      "Average training loss: 0.025475003325276906\n",
      "Average test loss: 0.006740050933013359\n",
      "Epoch 223/300\n",
      "Average training loss: 0.025420590470234553\n",
      "Average test loss: 0.006286487970501184\n",
      "Epoch 224/300\n",
      "Average training loss: 0.02538906286822425\n",
      "Average test loss: 0.0065845924516518915\n",
      "Epoch 225/300\n",
      "Average training loss: 0.025384440296226076\n",
      "Average test loss: 0.006644752779768573\n",
      "Epoch 226/300\n",
      "Average training loss: 0.025386131655838754\n",
      "Average test loss: 0.006610098811073436\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02536622081034713\n",
      "Average test loss: 0.006431879159890943\n",
      "Epoch 228/300\n",
      "Average training loss: 0.025343739334079955\n",
      "Average test loss: 0.006286816152433554\n",
      "Epoch 229/300\n",
      "Average training loss: 0.025331999538673294\n",
      "Average test loss: 0.006298933530847231\n",
      "Epoch 230/300\n",
      "Average training loss: 0.025349138698644108\n",
      "Average test loss: 0.00642881841212511\n",
      "Epoch 231/300\n",
      "Average training loss: 0.025361393234795995\n",
      "Average test loss: 0.006553787740154399\n",
      "Epoch 232/300\n",
      "Average training loss: 0.02541134453068177\n",
      "Average test loss: 0.006719686872429318\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02529217199815644\n",
      "Average test loss: 0.00644522347384029\n",
      "Epoch 234/300\n",
      "Average training loss: 0.025294018290109105\n",
      "Average test loss: 0.006497691827929682\n",
      "Epoch 235/300\n",
      "Average training loss: 0.025267080490787824\n",
      "Average test loss: 0.006234423643185032\n",
      "Epoch 236/300\n",
      "Average training loss: 0.025286712899804116\n",
      "Average test loss: 0.006991446997142501\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02527991959452629\n",
      "Average test loss: 0.007580953953166803\n",
      "Epoch 238/300\n",
      "Average training loss: 0.025286157437496716\n",
      "Average test loss: 0.0063005982297990055\n",
      "Epoch 239/300\n",
      "Average training loss: 0.025231464881036017\n",
      "Average test loss: 0.006330128675740626\n",
      "Epoch 240/300\n",
      "Average training loss: 0.02522754434082243\n",
      "Average test loss: 0.007946616751452288\n",
      "Epoch 241/300\n",
      "Average training loss: 0.025243889869915114\n",
      "Average test loss: 0.006529564104560349\n",
      "Epoch 242/300\n",
      "Average training loss: 0.025194521349337366\n",
      "Average test loss: 0.006603518369297187\n",
      "Epoch 243/300\n",
      "Average training loss: 0.02519575215710534\n",
      "Average test loss: 0.006446342594093746\n",
      "Epoch 244/300\n",
      "Average training loss: 0.025150146684712833\n",
      "Average test loss: 0.0063719194812907115\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0251807179533773\n",
      "Average test loss: 0.00616016625199053\n",
      "Epoch 246/300\n",
      "Average training loss: 0.02525436182320118\n",
      "Average test loss: 0.00632060586164395\n",
      "Epoch 247/300\n",
      "Average training loss: 0.025259171212712924\n",
      "Average test loss: 0.0072817398483554525\n",
      "Epoch 248/300\n",
      "Average training loss: 0.025122759921683207\n",
      "Average test loss: 0.006522393997137745\n",
      "Epoch 249/300\n",
      "Average training loss: 0.02519589405424065\n",
      "Average test loss: 0.006282009133448204\n",
      "Epoch 250/300\n",
      "Average training loss: 0.025096773374411796\n",
      "Average test loss: 0.007105245225959354\n",
      "Epoch 251/300\n",
      "Average training loss: 0.025105552322334714\n",
      "Average test loss: 0.007382728560931153\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02515284996231397\n",
      "Average test loss: 0.006320814920382368\n",
      "Epoch 253/300\n",
      "Average training loss: 0.025134634292787977\n",
      "Average test loss: 0.0062719100494351655\n",
      "Epoch 254/300\n",
      "Average training loss: 0.025101760571201642\n",
      "Average test loss: 0.006344154928293493\n",
      "Epoch 255/300\n",
      "Average training loss: 0.025136647716164588\n",
      "Average test loss: 0.0063920395248052144\n",
      "Epoch 256/300\n",
      "Average training loss: 0.025044206508331827\n",
      "Average test loss: 0.006768121978474988\n",
      "Epoch 257/300\n",
      "Average training loss: 0.025050690475437377\n",
      "Average test loss: 0.00631009437640508\n",
      "Epoch 258/300\n",
      "Average training loss: 0.02503804281519519\n",
      "Average test loss: 0.0067390458426541755\n",
      "Epoch 259/300\n",
      "Average training loss: 0.025062469944357874\n",
      "Average test loss: 0.00675946458718843\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02503222237361802\n",
      "Average test loss: 0.006670978425691525\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02506854021218088\n",
      "Average test loss: 0.006648677630970876\n",
      "Epoch 262/300\n",
      "Average training loss: 0.02499503918323252\n",
      "Average test loss: 0.006549924167493979\n",
      "Epoch 263/300\n",
      "Average training loss: 0.02509656330777539\n",
      "Average test loss: 0.00646725171038674\n",
      "Epoch 264/300\n",
      "Average training loss: 0.02503450713058313\n",
      "Average test loss: 0.006453281093802717\n",
      "Epoch 265/300\n",
      "Average training loss: 0.025025271678136455\n",
      "Average test loss: 0.006835271335310407\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02499447195397483\n",
      "Average test loss: 0.0063578984288291795\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02499482267598311\n",
      "Average test loss: 0.007151879110270076\n",
      "Epoch 268/300\n",
      "Average training loss: 0.024982793763279915\n",
      "Average test loss: 0.00636252873390913\n",
      "Epoch 269/300\n",
      "Average training loss: 0.025078195702698494\n",
      "Average test loss: 0.006290417041629553\n",
      "Epoch 270/300\n",
      "Average training loss: 0.024948093079858357\n",
      "Average test loss: 0.006339880725575818\n",
      "Epoch 271/300\n",
      "Average training loss: 0.0248818990819984\n",
      "Average test loss: 0.006318172730505466\n",
      "Epoch 272/300\n",
      "Average training loss: 0.024937119013733335\n",
      "Average test loss: 0.006656216179745065\n",
      "Epoch 273/300\n",
      "Average training loss: 0.02490790186656846\n",
      "Average test loss: 0.00637231259006593\n",
      "Epoch 274/300\n",
      "Average training loss: 0.024943345506985983\n",
      "Average test loss: 0.006668748127089607\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02494528222249614\n",
      "Average test loss: 0.006570939925809701\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02487167012029224\n",
      "Average test loss: 0.006666878792974684\n",
      "Epoch 277/300\n",
      "Average training loss: 0.024922160304254957\n",
      "Average test loss: 0.0062713284335202645\n",
      "Epoch 278/300\n",
      "Average training loss: 0.024881119829085138\n",
      "Average test loss: 0.006424462731513712\n",
      "Epoch 279/300\n",
      "Average training loss: 0.024874316321478948\n",
      "Average test loss: 0.006477497146775325\n",
      "Epoch 280/300\n",
      "Average training loss: 0.024871452424261305\n",
      "Average test loss: 0.006546404528948996\n",
      "Epoch 281/300\n",
      "Average training loss: 0.024901368164353902\n",
      "Average test loss: 0.006571284678661161\n",
      "Epoch 282/300\n",
      "Average training loss: 0.024834009134107167\n",
      "Average test loss: 0.00631724457277192\n",
      "Epoch 283/300\n",
      "Average training loss: 0.024866430916719966\n",
      "Average test loss: 0.0064175243750214574\n",
      "Epoch 284/300\n",
      "Average training loss: 0.024851420059800147\n",
      "Average test loss: 0.006777748776806726\n",
      "Epoch 285/300\n",
      "Average training loss: 0.024903941470715736\n",
      "Average test loss: 0.006424229338351223\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02477377714051141\n",
      "Average test loss: 0.006558785758084721\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02481047140724129\n",
      "Average test loss: 0.006487444281578064\n",
      "Epoch 288/300\n",
      "Average training loss: 0.024725038894348673\n",
      "Average test loss: 0.006423939085668988\n",
      "Epoch 289/300\n",
      "Average training loss: 0.024766222804784774\n",
      "Average test loss: 0.0064025839840372405\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02484888355765078\n",
      "Average test loss: 0.006449743734879626\n",
      "Epoch 291/300\n",
      "Average training loss: 0.024737830037871996\n",
      "Average test loss: 0.006702834669500589\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02478023930887381\n",
      "Average test loss: 0.006549379859119654\n",
      "Epoch 293/300\n",
      "Average training loss: 0.024756523453527027\n",
      "Average test loss: 0.00796800178123845\n",
      "Epoch 294/300\n",
      "Average training loss: 0.024716933824949796\n",
      "Average test loss: 0.006505786841114362\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02474825126098262\n",
      "Average test loss: 0.007454353235661983\n",
      "Epoch 296/300\n",
      "Average training loss: 0.024726957408918275\n",
      "Average test loss: 0.006547337626417478\n",
      "Epoch 297/300\n",
      "Average training loss: 0.024784141214357482\n",
      "Average test loss: 0.006519864473905828\n",
      "Epoch 298/300\n",
      "Average training loss: 0.024749513634377055\n",
      "Average test loss: 0.006983887299481365\n",
      "Epoch 299/300\n",
      "Average training loss: 0.024743661425179905\n",
      "Average test loss: 0.006627439611487918\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02473029993640052\n",
      "Average test loss: 0.006400756555712885\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.26292904354466334\n",
      "Average test loss: 0.0189733443086346\n",
      "Epoch 2/300\n",
      "Average training loss: 0.09409406727552413\n",
      "Average test loss: 0.009161481915248765\n",
      "Epoch 3/300\n",
      "Average training loss: 0.07290673439370261\n",
      "Average test loss: 0.009127249759932359\n",
      "Epoch 4/300\n",
      "Average training loss: 0.06284139958355162\n",
      "Average test loss: 0.007054492726094193\n",
      "Epoch 5/300\n",
      "Average training loss: 0.05671692097849316\n",
      "Average test loss: 0.007407162152230739\n",
      "Epoch 6/300\n",
      "Average training loss: 0.05183566073245472\n",
      "Average test loss: 0.005619799013766977\n",
      "Epoch 7/300\n",
      "Average training loss: 0.048623628278573354\n",
      "Average test loss: 0.005494828750689825\n",
      "Epoch 8/300\n",
      "Average training loss: 0.04620657723148664\n",
      "Average test loss: 0.005703340056041876\n",
      "Epoch 9/300\n",
      "Average training loss: 0.04324720899926292\n",
      "Average test loss: 0.03646841596729226\n",
      "Epoch 10/300\n",
      "Average training loss: 0.041226202074024415\n",
      "Average test loss: 0.005007622345454163\n",
      "Epoch 11/300\n",
      "Average training loss: 0.03898370362983809\n",
      "Average test loss: 0.005276102749009927\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03773548124565018\n",
      "Average test loss: 0.004903487674478028\n",
      "Epoch 13/300\n",
      "Average training loss: 0.03608536405364672\n",
      "Average test loss: 0.004624978022856845\n",
      "Epoch 14/300\n",
      "Average training loss: 0.03481779111756219\n",
      "Average test loss: 0.004529432184166378\n",
      "Epoch 15/300\n",
      "Average training loss: 0.03347457830442323\n",
      "Average test loss: 0.005062682687408394\n",
      "Epoch 16/300\n",
      "Average training loss: 0.0322191676431232\n",
      "Average test loss: 0.004317807559337881\n",
      "Epoch 17/300\n",
      "Average training loss: 0.030940884550412495\n",
      "Average test loss: 0.00505877935513854\n",
      "Epoch 18/300\n",
      "Average training loss: 0.030246330930127036\n",
      "Average test loss: 0.004324251804500818\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02938584535817305\n",
      "Average test loss: 0.005555140683220492\n",
      "Epoch 20/300\n",
      "Average training loss: 0.028669387140207822\n",
      "Average test loss: 0.004245071758619613\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02804182548324267\n",
      "Average test loss: 0.004031753190275695\n",
      "Epoch 22/300\n",
      "Average training loss: 0.027613828322953646\n",
      "Average test loss: 0.003962490826845169\n",
      "Epoch 23/300\n",
      "Average training loss: 0.027099747704135046\n",
      "Average test loss: 0.0038572444452179802\n",
      "Epoch 24/300\n",
      "Average training loss: 0.026858446419239044\n",
      "Average test loss: 0.004023623637441132\n",
      "Epoch 25/300\n",
      "Average training loss: 0.026254916588465373\n",
      "Average test loss: 0.003958863481879234\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02597567090557681\n",
      "Average test loss: 0.003841109028293027\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02572149569292863\n",
      "Average test loss: 0.0036873225296537083\n",
      "Epoch 28/300\n",
      "Average training loss: 0.025304512277245523\n",
      "Average test loss: 0.00375283807143569\n",
      "Epoch 29/300\n",
      "Average training loss: 0.025111365204056105\n",
      "Average test loss: 0.0037147059918691714\n",
      "Epoch 30/300\n",
      "Average training loss: 0.024876524027850892\n",
      "Average test loss: 0.0036420674816601806\n",
      "Epoch 31/300\n",
      "Average training loss: 0.024469578014479743\n",
      "Average test loss: 0.003607686419246925\n",
      "Epoch 32/300\n",
      "Average training loss: 0.02441578729616271\n",
      "Average test loss: 0.003650270415676965\n",
      "Epoch 33/300\n",
      "Average training loss: 0.02420738898548815\n",
      "Average test loss: 0.0038486492530339293\n",
      "Epoch 34/300\n",
      "Average training loss: 0.023927852059404054\n",
      "Average test loss: 0.0036154413550264306\n",
      "Epoch 35/300\n",
      "Average training loss: 0.02385643783873982\n",
      "Average test loss: 0.003756439212916626\n",
      "Epoch 36/300\n",
      "Average training loss: 0.023605193283822802\n",
      "Average test loss: 0.003666681543820434\n",
      "Epoch 37/300\n",
      "Average training loss: 0.023529363319277765\n",
      "Average test loss: 0.0042577821264664336\n",
      "Epoch 38/300\n",
      "Average training loss: 0.023322164446115495\n",
      "Average test loss: 0.003755983900485767\n",
      "Epoch 39/300\n",
      "Average training loss: 0.023176124377383128\n",
      "Average test loss: 0.004179489983452691\n",
      "Epoch 40/300\n",
      "Average training loss: 0.023005596535073385\n",
      "Average test loss: 0.0040892248182661005\n",
      "Epoch 41/300\n",
      "Average training loss: 0.022969868504338794\n",
      "Average test loss: 0.003568776852140824\n",
      "Epoch 42/300\n",
      "Average training loss: 0.022882550242874358\n",
      "Average test loss: 0.0034716477890809378\n",
      "Epoch 43/300\n",
      "Average training loss: 0.022678916859957907\n",
      "Average test loss: 0.0036983626967089043\n",
      "Epoch 44/300\n",
      "Average training loss: 0.022609377336170938\n",
      "Average test loss: 0.0036251129247248173\n",
      "Epoch 45/300\n",
      "Average training loss: 0.02250209130015638\n",
      "Average test loss: 0.004100741164965762\n",
      "Epoch 46/300\n",
      "Average training loss: 0.022412169080641534\n",
      "Average test loss: 0.003915842756008109\n",
      "Epoch 47/300\n",
      "Average training loss: 0.022376076479752857\n",
      "Average test loss: 0.003631620112599598\n",
      "Epoch 48/300\n",
      "Average training loss: 0.02221247003144688\n",
      "Average test loss: 0.003672175286958615\n",
      "Epoch 49/300\n",
      "Average training loss: 0.022119975143836606\n",
      "Average test loss: 0.0035405630899800196\n",
      "Epoch 50/300\n",
      "Average training loss: 0.02205481603079372\n",
      "Average test loss: 0.0036562001742422583\n",
      "Epoch 51/300\n",
      "Average training loss: 0.022026798137360148\n",
      "Average test loss: 0.0034551953807887103\n",
      "Epoch 52/300\n",
      "Average training loss: 0.02187278772228294\n",
      "Average test loss: 0.0037378724606500734\n",
      "Epoch 53/300\n",
      "Average training loss: 0.021854260344472198\n",
      "Average test loss: 0.005157119192596939\n",
      "Epoch 54/300\n",
      "Average training loss: 0.021760094958874914\n",
      "Average test loss: 0.003490984525117609\n",
      "Epoch 55/300\n",
      "Average training loss: 0.021663388949301508\n",
      "Average test loss: 0.003621035174363189\n",
      "Epoch 56/300\n",
      "Average training loss: 0.021661511719226836\n",
      "Average test loss: 0.0037033436561210286\n",
      "Epoch 57/300\n",
      "Average training loss: 0.021529149050513903\n",
      "Average test loss: 0.003470279208280974\n",
      "Epoch 58/300\n",
      "Average training loss: 0.02143514528373877\n",
      "Average test loss: 0.0035557762202289367\n",
      "Epoch 59/300\n",
      "Average training loss: 0.02154732534620497\n",
      "Average test loss: 0.0036537330756998723\n",
      "Epoch 60/300\n",
      "Average training loss: 0.021336081190241708\n",
      "Average test loss: 0.003463057386999329\n",
      "Epoch 61/300\n",
      "Average training loss: 0.021325981923275523\n",
      "Average test loss: 0.003481285540593995\n",
      "Epoch 62/300\n",
      "Average training loss: 0.02125531755056646\n",
      "Average test loss: 0.003648309430728356\n",
      "Epoch 63/300\n",
      "Average training loss: 0.021167744886544014\n",
      "Average test loss: 0.003527570022063123\n",
      "Epoch 64/300\n",
      "Average training loss: 0.021132791350285212\n",
      "Average test loss: 0.0037165740985009407\n",
      "Epoch 65/300\n",
      "Average training loss: 0.02106372561885251\n",
      "Average test loss: 0.0034256290015247134\n",
      "Epoch 66/300\n",
      "Average training loss: 0.021033126768138673\n",
      "Average test loss: 0.0035107431028866105\n",
      "Epoch 67/300\n",
      "Average training loss: 0.0209277632849084\n",
      "Average test loss: 0.0034757735712660684\n",
      "Epoch 68/300\n",
      "Average training loss: 0.02092320766713884\n",
      "Average test loss: 0.003556099442144235\n",
      "Epoch 69/300\n",
      "Average training loss: 0.020855482146143915\n",
      "Average test loss: 0.003508726917621162\n",
      "Epoch 70/300\n",
      "Average training loss: 0.020843544168604743\n",
      "Average test loss: 0.0035062855440709324\n",
      "Epoch 71/300\n",
      "Average training loss: 0.020809077246321573\n",
      "Average test loss: 0.0036081572466840346\n",
      "Epoch 72/300\n",
      "Average training loss: 0.020723722886708047\n",
      "Average test loss: 0.0035861136809819272\n",
      "Epoch 73/300\n",
      "Average training loss: 0.020731126523680157\n",
      "Average test loss: 0.0034311376131243177\n",
      "Epoch 74/300\n",
      "Average training loss: 0.020675181387199294\n",
      "Average test loss: 0.0034737637392762634\n",
      "Epoch 75/300\n",
      "Average training loss: 0.02063022243231535\n",
      "Average test loss: 0.004016563731142216\n",
      "Epoch 76/300\n",
      "Average training loss: 0.020533783750401602\n",
      "Average test loss: 0.003648739353650146\n",
      "Epoch 77/300\n",
      "Average training loss: 0.020546046584844588\n",
      "Average test loss: 0.004128618836402893\n",
      "Epoch 78/300\n",
      "Average training loss: 0.02048119835389985\n",
      "Average test loss: 0.0038977454168101153\n",
      "Epoch 79/300\n",
      "Average training loss: 0.020468385277522934\n",
      "Average test loss: 0.003473403344137801\n",
      "Epoch 80/300\n",
      "Average training loss: 0.020412127463354003\n",
      "Average test loss: 0.003387186157827576\n",
      "Epoch 81/300\n",
      "Average training loss: 0.020363730660743182\n",
      "Average test loss: 0.0034568827119138505\n",
      "Epoch 82/300\n",
      "Average training loss: 0.020332708857125705\n",
      "Average test loss: 0.003512973224123319\n",
      "Epoch 83/300\n",
      "Average training loss: 0.0203245793465111\n",
      "Average test loss: 0.0034868975261019334\n",
      "Epoch 84/300\n",
      "Average training loss: 0.020274754081335333\n",
      "Average test loss: 0.0034895067032840515\n",
      "Epoch 85/300\n",
      "Average training loss: 0.020242526683542462\n",
      "Average test loss: 0.003642990900410546\n",
      "Epoch 86/300\n",
      "Average training loss: 0.020221467946966488\n",
      "Average test loss: 0.0033707106759150824\n",
      "Epoch 87/300\n",
      "Average training loss: 0.02023558862424559\n",
      "Average test loss: 0.003821413197865089\n",
      "Epoch 88/300\n",
      "Average training loss: 0.020102559965517786\n",
      "Average test loss: 0.0035587653145194055\n",
      "Epoch 89/300\n",
      "Average training loss: 0.0200834082149797\n",
      "Average test loss: 0.0035380782222168314\n",
      "Epoch 90/300\n",
      "Average training loss: 0.020073030883239376\n",
      "Average test loss: 0.0038771393655074966\n",
      "Epoch 91/300\n",
      "Average training loss: 0.020046665026081933\n",
      "Average test loss: 0.00347960850265291\n",
      "Epoch 92/300\n",
      "Average training loss: 0.020057859748601914\n",
      "Average test loss: 0.003509280393520991\n",
      "Epoch 93/300\n",
      "Average training loss: 0.020093312410844697\n",
      "Average test loss: 0.0037128304379681746\n",
      "Epoch 94/300\n",
      "Average training loss: 0.01990272339847353\n",
      "Average test loss: 0.0035308703891932963\n",
      "Epoch 95/300\n",
      "Average training loss: 0.019884378600451683\n",
      "Average test loss: 0.0035052444351216156\n",
      "Epoch 96/300\n",
      "Average training loss: 0.019917218428519037\n",
      "Average test loss: 0.0036237402653528585\n",
      "Epoch 97/300\n",
      "Average training loss: 0.019876876572767894\n",
      "Average test loss: 0.0035106425243947243\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01982395787537098\n",
      "Average test loss: 0.0034048362248059777\n",
      "Epoch 99/300\n",
      "Average training loss: 0.019843036436372332\n",
      "Average test loss: 0.0034510086443689135\n",
      "Epoch 100/300\n",
      "Average training loss: 0.019751029933492342\n",
      "Average test loss: 0.0036657272175782255\n",
      "Epoch 101/300\n",
      "Average training loss: 0.019763847801420424\n",
      "Average test loss: 0.0035078536375529237\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01971836330327723\n",
      "Average test loss: 0.0034583102944824432\n",
      "Epoch 103/300\n",
      "Average training loss: 0.019693266602026093\n",
      "Average test loss: 0.003616883399585883\n",
      "Epoch 104/300\n",
      "Average training loss: 0.019691292946537337\n",
      "Average test loss: 0.003963442986624108\n",
      "Epoch 105/300\n",
      "Average training loss: 0.019645119378964106\n",
      "Average test loss: 0.003701394172178374\n",
      "Epoch 106/300\n",
      "Average training loss: 0.019692759757240615\n",
      "Average test loss: 0.004289031989044613\n",
      "Epoch 107/300\n",
      "Average training loss: 0.019612299409177567\n",
      "Average test loss: 0.003865861150332623\n",
      "Epoch 108/300\n",
      "Average training loss: 0.019599221269289653\n",
      "Average test loss: 0.0034948391895741225\n",
      "Epoch 109/300\n",
      "Average training loss: 0.019576201549834676\n",
      "Average test loss: 0.0035262432226704226\n",
      "Epoch 110/300\n",
      "Average training loss: 0.019513349485066202\n",
      "Average test loss: 0.0037412315437363253\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01958282913433181\n",
      "Average test loss: 0.0034809575871460967\n",
      "Epoch 112/300\n",
      "Average training loss: 0.019476680760582288\n",
      "Average test loss: 0.003447551644510693\n",
      "Epoch 113/300\n",
      "Average training loss: 0.019466617956757547\n",
      "Average test loss: 0.0034185896544820732\n",
      "Epoch 114/300\n",
      "Average training loss: 0.019472067919042376\n",
      "Average test loss: 0.003519406272512343\n",
      "Epoch 115/300\n",
      "Average training loss: 0.019456787083711888\n",
      "Average test loss: 0.0038670627743833596\n",
      "Epoch 116/300\n",
      "Average training loss: 0.01939817579752869\n",
      "Average test loss: 0.0034344713466448917\n",
      "Epoch 117/300\n",
      "Average training loss: 0.019425018161535263\n",
      "Average test loss: 0.0035416649141245416\n",
      "Epoch 118/300\n",
      "Average training loss: 0.019378691967990663\n",
      "Average test loss: 0.003837474972009659\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01937454230752256\n",
      "Average test loss: 0.0036841403130027984\n",
      "Epoch 120/300\n",
      "Average training loss: 0.01929471242758963\n",
      "Average test loss: 0.003483549843645758\n",
      "Epoch 121/300\n",
      "Average training loss: 0.019294866098297968\n",
      "Average test loss: 0.005659508474585083\n",
      "Epoch 122/300\n",
      "Average training loss: 0.019311390373441907\n",
      "Average test loss: 0.0035955713585846955\n",
      "Epoch 123/300\n",
      "Average training loss: 0.019232575269209014\n",
      "Average test loss: 0.003703673610670699\n",
      "Epoch 124/300\n",
      "Average training loss: 0.019280314543180996\n",
      "Average test loss: 0.0034781937667479118\n",
      "Epoch 125/300\n",
      "Average training loss: 0.019216263583964773\n",
      "Average test loss: 0.004094800926744938\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01920779330200619\n",
      "Average test loss: 0.00347575525359975\n",
      "Epoch 127/300\n",
      "Average training loss: 0.019221134710643028\n",
      "Average test loss: 0.0034756699576973917\n",
      "Epoch 128/300\n",
      "Average training loss: 0.019166473767823645\n",
      "Average test loss: 0.003707651072906123\n",
      "Epoch 129/300\n",
      "Average training loss: 0.019108432436982792\n",
      "Average test loss: 0.003529663781531983\n",
      "Epoch 130/300\n",
      "Average training loss: 0.019095048434204526\n",
      "Average test loss: 0.0035384249012503357\n",
      "Epoch 131/300\n",
      "Average training loss: 0.019133658473690352\n",
      "Average test loss: 0.003499093368028601\n",
      "Epoch 132/300\n",
      "Average training loss: 0.019058353283339078\n",
      "Average test loss: 0.00359788304546641\n",
      "Epoch 133/300\n",
      "Average training loss: 0.019099731711877718\n",
      "Average test loss: 0.003532309734986888\n",
      "Epoch 134/300\n",
      "Average training loss: 0.019048461005091666\n",
      "Average test loss: 0.003487001934192247\n",
      "Epoch 135/300\n",
      "Average training loss: 0.019061504416995577\n",
      "Average test loss: 0.004003401295178466\n",
      "Epoch 136/300\n",
      "Average training loss: 0.019047068160441185\n",
      "Average test loss: 0.003555588833987713\n",
      "Epoch 137/300\n",
      "Average training loss: 0.01905852885875437\n",
      "Average test loss: 0.0034149140910142\n",
      "Epoch 138/300\n",
      "Average training loss: 0.019020785237352053\n",
      "Average test loss: 0.0034528522404531636\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01894462539917893\n",
      "Average test loss: 0.00372409029946559\n",
      "Epoch 140/300\n",
      "Average training loss: 0.018936225831508637\n",
      "Average test loss: 0.004001935506653454\n",
      "Epoch 141/300\n",
      "Average training loss: 0.018943912343018586\n",
      "Average test loss: 0.003580445936570565\n",
      "Epoch 142/300\n",
      "Average training loss: 0.018921553595198527\n",
      "Average test loss: 0.0034443800004406106\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01890557598653767\n",
      "Average test loss: 0.003574565239664581\n",
      "Epoch 144/300\n",
      "Average training loss: 0.018873663538032107\n",
      "Average test loss: 0.003477689510004388\n",
      "Epoch 145/300\n",
      "Average training loss: 0.01886888430515925\n",
      "Average test loss: 0.0036470160484313964\n",
      "Epoch 146/300\n",
      "Average training loss: 0.01884112931953536\n",
      "Average test loss: 0.003499916632142332\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01903121012283696\n",
      "Average test loss: 0.0037075952492240404\n",
      "Epoch 148/300\n",
      "Average training loss: 0.01892264846795135\n",
      "Average test loss: 0.003556259787744946\n",
      "Epoch 149/300\n",
      "Average training loss: 0.018803043637838628\n",
      "Average test loss: 0.003504967992918359\n",
      "Epoch 150/300\n",
      "Average training loss: 0.018792312358816465\n",
      "Average test loss: 0.0034817798365321426\n",
      "Epoch 151/300\n",
      "Average training loss: 0.018813758527239165\n",
      "Average test loss: 0.0037428416630460155\n",
      "Epoch 152/300\n",
      "Average training loss: 0.01878195486134953\n",
      "Average test loss: 0.0035742540278782447\n",
      "Epoch 153/300\n",
      "Average training loss: 0.01871722883482774\n",
      "Average test loss: 0.003542242286933793\n",
      "Epoch 154/300\n",
      "Average training loss: 0.018721027485198445\n",
      "Average test loss: 0.0036515141882830196\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01870703509615527\n",
      "Average test loss: 0.003579476939307319\n",
      "Epoch 156/300\n",
      "Average training loss: 0.018787304703560142\n",
      "Average test loss: 0.0035885841192470655\n",
      "Epoch 157/300\n",
      "Average training loss: 0.01870639100836383\n",
      "Average test loss: 0.003449278086837795\n",
      "Epoch 158/300\n",
      "Average training loss: 0.018698713090684677\n",
      "Average test loss: 0.006718097197512786\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01863908774488502\n",
      "Average test loss: 0.003702964176941249\n",
      "Epoch 160/300\n",
      "Average training loss: 0.018722967073321342\n",
      "Average test loss: 0.00360034222735299\n",
      "Epoch 161/300\n",
      "Average training loss: 0.018596603728002974\n",
      "Average test loss: 0.003501541753609975\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01862753541767597\n",
      "Average test loss: 0.0036374409542315536\n",
      "Epoch 163/300\n",
      "Average training loss: 0.018628950375649662\n",
      "Average test loss: 0.0036273612185484835\n",
      "Epoch 164/300\n",
      "Average training loss: 0.018604543781942793\n",
      "Average test loss: 0.003640764990200599\n",
      "Epoch 165/300\n",
      "Average training loss: 0.018619868472218512\n",
      "Average test loss: 0.0036052608080208303\n",
      "Epoch 166/300\n",
      "Average training loss: 0.01855174279709657\n",
      "Average test loss: 0.003474064552742574\n",
      "Epoch 167/300\n",
      "Average training loss: 0.01855377099580235\n",
      "Average test loss: 0.004243289226045211\n",
      "Epoch 168/300\n",
      "Average training loss: 0.01861941261175606\n",
      "Average test loss: 0.004386105498092042\n",
      "Epoch 169/300\n",
      "Average training loss: 0.018561251872115665\n",
      "Average test loss: 0.0927324334250556\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07014414094885191\n",
      "Average test loss: 0.004687877996721201\n",
      "Epoch 171/300\n",
      "Average training loss: 0.032235213216808105\n",
      "Average test loss: 0.0040254922095272275\n",
      "Epoch 172/300\n",
      "Average training loss: 0.029274422963460285\n",
      "Average test loss: 0.004189087592686216\n",
      "Epoch 173/300\n",
      "Average training loss: 0.02719279165731536\n",
      "Average test loss: 0.004027188507633077\n",
      "Epoch 174/300\n",
      "Average training loss: 0.025632643174793985\n",
      "Average test loss: 0.0036812969086070855\n",
      "Epoch 175/300\n",
      "Average training loss: 0.024356558407346407\n",
      "Average test loss: 0.0035547867473214863\n",
      "Epoch 176/300\n",
      "Average training loss: 0.023334108193715414\n",
      "Average test loss: 0.0036386464105712044\n",
      "Epoch 177/300\n",
      "Average training loss: 0.022436295901735622\n",
      "Average test loss: 0.0034824915371007388\n",
      "Epoch 178/300\n",
      "Average training loss: 0.021620006144046782\n",
      "Average test loss: 0.003522348426385886\n",
      "Epoch 179/300\n",
      "Average training loss: 0.020892682455480097\n",
      "Average test loss: 0.003544916097902589\n",
      "Epoch 180/300\n",
      "Average training loss: 0.020215915378597047\n",
      "Average test loss: 0.0034621341209858654\n",
      "Epoch 181/300\n",
      "Average training loss: 0.01966201073096858\n",
      "Average test loss: 0.0034520501415762636\n",
      "Epoch 182/300\n",
      "Average training loss: 0.019209333015812766\n",
      "Average test loss: 0.0035629835919373564\n",
      "Epoch 183/300\n",
      "Average training loss: 0.018888686837421523\n",
      "Average test loss: 0.003612091287970543\n",
      "Epoch 184/300\n",
      "Average training loss: 0.018692863936225573\n",
      "Average test loss: 0.003688757052852048\n",
      "Epoch 185/300\n",
      "Average training loss: 0.018614655017852783\n",
      "Average test loss: 0.003576090945965714\n",
      "Epoch 186/300\n",
      "Average training loss: 0.01860783399807082\n",
      "Average test loss: 0.0035644354377355842\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01853186755461825\n",
      "Average test loss: 0.003524300488125947\n",
      "Epoch 188/300\n",
      "Average training loss: 0.018465870504577955\n",
      "Average test loss: 0.003635157797899511\n",
      "Epoch 189/300\n",
      "Average training loss: 0.018480091891354983\n",
      "Average test loss: 0.0034607301025340953\n",
      "Epoch 190/300\n",
      "Average training loss: 0.018497897987564405\n",
      "Average test loss: 0.003969069898749391\n",
      "Epoch 191/300\n",
      "Average training loss: 0.018486646235817007\n",
      "Average test loss: 0.0036166949334243935\n",
      "Epoch 192/300\n",
      "Average training loss: 0.018426926078067885\n",
      "Average test loss: 0.0037050417953481277\n",
      "Epoch 193/300\n",
      "Average training loss: 0.018435384079813957\n",
      "Average test loss: 0.0036052345627297956\n",
      "Epoch 194/300\n",
      "Average training loss: 0.01843223612341616\n",
      "Average test loss: 0.003658129959884617\n",
      "Epoch 195/300\n",
      "Average training loss: 0.018433186161849233\n",
      "Average test loss: 0.00356986822001636\n",
      "Epoch 196/300\n",
      "Average training loss: 0.018444461130433614\n",
      "Average test loss: 0.0036060515737368\n",
      "Epoch 197/300\n",
      "Average training loss: 0.018414307864175902\n",
      "Average test loss: 0.0036282659077809917\n",
      "Epoch 198/300\n",
      "Average training loss: 0.01840884035660161\n",
      "Average test loss: 0.0036244789662046566\n",
      "Epoch 199/300\n",
      "Average training loss: 0.01842498562236627\n",
      "Average test loss: 0.00355811572385331\n",
      "Epoch 200/300\n",
      "Average training loss: 0.018379367901219263\n",
      "Average test loss: 0.0037964015133264994\n",
      "Epoch 201/300\n",
      "Average training loss: 0.01836467202173339\n",
      "Average test loss: 0.0035853591459906763\n",
      "Epoch 202/300\n",
      "Average training loss: 0.018368538432651096\n",
      "Average test loss: 0.003555852483958006\n",
      "Epoch 203/300\n",
      "Average training loss: 0.01831199491189586\n",
      "Average test loss: 0.0035751696471124886\n",
      "Epoch 204/300\n",
      "Average training loss: 0.0182900669955545\n",
      "Average test loss: 0.0036078051246909632\n",
      "Epoch 205/300\n",
      "Average training loss: 0.018367704840170012\n",
      "Average test loss: 0.0035075748854627212\n",
      "Epoch 206/300\n",
      "Average training loss: 0.018283632944855424\n",
      "Average test loss: 0.0037811077046725483\n",
      "Epoch 207/300\n",
      "Average training loss: 0.018317780004607306\n",
      "Average test loss: 0.003808403371108903\n",
      "Epoch 208/300\n",
      "Average training loss: 0.018284197345376015\n",
      "Average test loss: 0.004077432207763195\n",
      "Epoch 209/300\n",
      "Average training loss: 0.018231442636085882\n",
      "Average test loss: 0.0037000843514170914\n",
      "Epoch 210/300\n",
      "Average training loss: 0.01824573858247863\n",
      "Average test loss: 0.003712069318526321\n",
      "Epoch 211/300\n",
      "Average training loss: 0.018182474676933555\n",
      "Average test loss: 0.003606964934617281\n",
      "Epoch 212/300\n",
      "Average training loss: 0.018227784363759887\n",
      "Average test loss: 0.003597444735053513\n",
      "Epoch 213/300\n",
      "Average training loss: 0.018189818581773175\n",
      "Average test loss: 0.0038901465800073413\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01820317185504569\n",
      "Average test loss: 0.003689217057906919\n",
      "Epoch 215/300\n",
      "Average training loss: 0.01821434537238545\n",
      "Average test loss: 0.003626749537885189\n",
      "Epoch 216/300\n",
      "Average training loss: 0.018163556029399237\n",
      "Average test loss: 0.003585688340374165\n",
      "Epoch 217/300\n",
      "Average training loss: 0.0181664233547118\n",
      "Average test loss: 0.0037326896101650265\n",
      "Epoch 218/300\n",
      "Average training loss: 0.018173820795284377\n",
      "Average test loss: 0.003963072898073329\n",
      "Epoch 219/300\n",
      "Average training loss: 0.018189617279503082\n",
      "Average test loss: 0.0036551307338393395\n",
      "Epoch 220/300\n",
      "Average training loss: 0.018129589819245867\n",
      "Average test loss: 0.0035607449128809902\n",
      "Epoch 221/300\n",
      "Average training loss: 0.018136986010604435\n",
      "Average test loss: 0.0036477127551204627\n",
      "Epoch 222/300\n",
      "Average training loss: 0.018127154500120216\n",
      "Average test loss: 0.0035852498149292335\n",
      "Epoch 223/300\n",
      "Average training loss: 0.018159307814306684\n",
      "Average test loss: 0.00362523434849249\n",
      "Epoch 224/300\n",
      "Average training loss: 0.018093585247794787\n",
      "Average test loss: 0.004154817622568872\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01808428054385715\n",
      "Average test loss: 0.0035650844109348127\n",
      "Epoch 226/300\n",
      "Average training loss: 0.018067638816104997\n",
      "Average test loss: 0.003816818754292197\n",
      "Epoch 227/300\n",
      "Average training loss: 0.018129654614461792\n",
      "Average test loss: 0.0035835562280068793\n",
      "Epoch 228/300\n",
      "Average training loss: 0.018103030909266737\n",
      "Average test loss: 0.0037742817296336096\n",
      "Epoch 229/300\n",
      "Average training loss: 0.01808264254199134\n",
      "Average test loss: 0.003585024615542756\n",
      "Epoch 230/300\n",
      "Average training loss: 0.018040483356349998\n",
      "Average test loss: 0.003617072291051348\n",
      "Epoch 231/300\n",
      "Average training loss: 0.01806355049378342\n",
      "Average test loss: 0.003793349856096837\n",
      "Epoch 232/300\n",
      "Average training loss: 0.018045194764931997\n",
      "Average test loss: 0.0037794967158180145\n",
      "Epoch 233/300\n",
      "Average training loss: 0.017999052852392197\n",
      "Average test loss: 0.0037968581198818156\n",
      "Epoch 234/300\n",
      "Average training loss: 0.018025938007566664\n",
      "Average test loss: 0.003551316994552811\n",
      "Epoch 235/300\n",
      "Average training loss: 0.018012550466590456\n",
      "Average test loss: 0.003686667581813203\n",
      "Epoch 236/300\n",
      "Average training loss: 0.018036018462644684\n",
      "Average test loss: 0.003879796713590622\n",
      "Epoch 237/300\n",
      "Average training loss: 0.017993557011087735\n",
      "Average test loss: 0.003597631240884463\n",
      "Epoch 238/300\n",
      "Average training loss: 0.017997045268615088\n",
      "Average test loss: 0.0037519560754299165\n",
      "Epoch 239/300\n",
      "Average training loss: 0.017960554665989347\n",
      "Average test loss: 0.0035816083546313976\n",
      "Epoch 240/300\n",
      "Average training loss: 0.017974308499031595\n",
      "Average test loss: 0.0036606200954152476\n",
      "Epoch 241/300\n",
      "Average training loss: 0.017958539581961103\n",
      "Average test loss: 0.003831851775654488\n",
      "Epoch 242/300\n",
      "Average training loss: 0.017999679452015294\n",
      "Average test loss: 0.0036534297118584317\n",
      "Epoch 243/300\n",
      "Average training loss: 0.01795554769039154\n",
      "Average test loss: 0.0036336760657529037\n",
      "Epoch 244/300\n",
      "Average training loss: 0.01797623930623134\n",
      "Average test loss: 0.0036948747026423613\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0179478496213754\n",
      "Average test loss: 0.0036121761115888757\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01793257602221436\n",
      "Average test loss: 0.0036126174935036236\n",
      "Epoch 247/300\n",
      "Average training loss: 0.017928396215041477\n",
      "Average test loss: 0.0035563162565231325\n",
      "Epoch 248/300\n",
      "Average training loss: 0.017951849463913174\n",
      "Average test loss: 0.0035946255535301234\n",
      "Epoch 249/300\n",
      "Average training loss: 0.01792223626540767\n",
      "Average test loss: 0.0038331986462904347\n",
      "Epoch 250/300\n",
      "Average training loss: 0.01792482312520345\n",
      "Average test loss: 0.003766293245057265\n",
      "Epoch 251/300\n",
      "Average training loss: 0.0179049813962645\n",
      "Average test loss: 0.003559827809739444\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01783306020332707\n",
      "Average test loss: 0.0036054157134559417\n",
      "Epoch 253/300\n",
      "Average training loss: 0.017875946432352068\n",
      "Average test loss: 0.0037192632984369995\n",
      "Epoch 254/300\n",
      "Average training loss: 0.017908231490188175\n",
      "Average test loss: 0.004213051411012809\n",
      "Epoch 255/300\n",
      "Average training loss: 0.017872694820165635\n",
      "Average test loss: 0.004245724073300759\n",
      "Epoch 256/300\n",
      "Average training loss: 0.017803659280141194\n",
      "Average test loss: 0.0037698663458642034\n",
      "Epoch 261/300\n",
      "Average training loss: 0.017796180628240107\n",
      "Average test loss: 0.0037752305381000043\n",
      "Epoch 262/300\n",
      "Average training loss: 0.017857754603028297\n",
      "Average test loss: 0.003851748204065694\n",
      "Epoch 263/300\n",
      "Average training loss: 0.0178102635294199\n",
      "Average test loss: 0.003709233513308896\n",
      "Epoch 264/300\n",
      "Average training loss: 0.017846276617712445\n",
      "Average test loss: 0.0036618477298567694\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01777572266674704\n",
      "Average test loss: 0.003735940164782935\n",
      "Epoch 266/300\n",
      "Average training loss: 0.017809538319706917\n",
      "Average test loss: 0.003771226496228741\n",
      "Epoch 267/300\n",
      "Average training loss: 0.01778219101495213\n",
      "Average test loss: 0.003911335311830044\n",
      "Epoch 268/300\n",
      "Average training loss: 0.017780100069112247\n",
      "Average test loss: 0.003570895108497805\n",
      "Epoch 269/300\n",
      "Average training loss: 0.017780530903074476\n",
      "Average test loss: 0.003693395840624968\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01774392356723547\n",
      "Average test loss: 0.003996636437873046\n",
      "Epoch 271/300\n",
      "Average training loss: 0.017727753199636938\n",
      "Average test loss: 0.0038004313910173044\n",
      "Epoch 272/300\n",
      "Average training loss: 0.01776013387988011\n",
      "Average test loss: 0.0037084255795925857\n",
      "Epoch 273/300\n",
      "Average training loss: 0.01772443139470286\n",
      "Average test loss: 0.0035658279317948554\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01770557185345226\n",
      "Average test loss: 0.0036614315828515424\n",
      "Epoch 275/300\n",
      "Average training loss: 0.017689050634702046\n",
      "Average test loss: 0.0037444118360678356\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01773240537279182\n",
      "Average test loss: 0.0037506654204593763\n",
      "Epoch 277/300\n",
      "Average training loss: 0.0176894836988714\n",
      "Average test loss: 0.004413927747143639\n",
      "Epoch 278/300\n",
      "Average training loss: 0.017791204283634822\n",
      "Average test loss: 0.003748576913856798\n",
      "Epoch 279/300\n",
      "Average training loss: 0.01769608089658949\n",
      "Average test loss: 0.0036879009598245224\n",
      "Epoch 280/300\n",
      "Average training loss: 0.017644982559813394\n",
      "Average test loss: 0.003584785443213251\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01770146674828397\n",
      "Average test loss: 0.003660600205262502\n",
      "Epoch 282/300\n",
      "Average training loss: 0.017668659866684013\n",
      "Average test loss: 0.0036664065430975622\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01771728288465076\n",
      "Average test loss: 0.00374784489783148\n",
      "Epoch 284/300\n",
      "Average training loss: 0.017728369924757216\n",
      "Average test loss: 0.0037406097360783153\n",
      "Epoch 285/300\n",
      "Average training loss: 0.017718973277343644\n",
      "Average test loss: 0.0038260132956008117\n",
      "Epoch 286/300\n",
      "Average training loss: 0.01764934114449554\n",
      "Average test loss: 0.0036030938091377415\n",
      "Epoch 287/300\n",
      "Average training loss: 0.017621495589613914\n",
      "Average test loss: 0.003820759943169024\n",
      "Epoch 288/300\n",
      "Average training loss: 0.017671355317864155\n",
      "Average test loss: 0.0036733891579012077\n",
      "Epoch 289/300\n",
      "Average training loss: 0.01762592780010568\n",
      "Average test loss: 0.003705876102257106\n",
      "Epoch 290/300\n",
      "Average training loss: 0.01762275526093112\n",
      "Average test loss: 0.003642723374482658\n",
      "Epoch 295/300\n",
      "Average training loss: 0.017638344845838018\n",
      "Average test loss: 0.0036410848072005644\n",
      "Epoch 296/300\n",
      "Average training loss: 0.017614751923415398\n",
      "Average test loss: 0.0038253675430185265\n",
      "Epoch 297/300\n",
      "Average training loss: 0.017590002318223316\n",
      "Average test loss: 0.0037840492270059057\n",
      "Epoch 298/300\n",
      "Average training loss: 0.017549481310778194\n",
      "Average test loss: 0.00375377795431349\n",
      "Epoch 299/300\n",
      "Average training loss: 0.017598855685856608\n",
      "Average test loss: 0.003774623371246788\n",
      "Epoch 300/300\n",
      "Average training loss: 0.017621771607134077\n",
      "Average test loss: 0.003698081537667248\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2481741140352355\n",
      "Average test loss: 0.0074486357648339534\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08685218640168509\n",
      "Average test loss: 0.005769147670931286\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06601764707432853\n",
      "Average test loss: 0.006680693693045113\n",
      "Epoch 4/300\n",
      "Average training loss: 0.055315599862072205\n",
      "Average test loss: 0.004775090116800534\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04849250260326597\n",
      "Average test loss: 0.0044965858937551575\n",
      "Epoch 6/300\n",
      "Average training loss: 0.044319831884569595\n",
      "Average test loss: 0.007847412284049723\n",
      "Epoch 7/300\n",
      "Average training loss: 0.04050464342865679\n",
      "Average test loss: 0.003984643005869455\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03824266036020385\n",
      "Average test loss: 0.004032302026947339\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03604843069944117\n",
      "Average test loss: 0.0037712726748238006\n",
      "Epoch 10/300\n",
      "Average training loss: 0.03415635577175352\n",
      "Average test loss: 0.004101520435470673\n",
      "Epoch 11/300\n",
      "Average training loss: 0.032007031470537184\n",
      "Average test loss: 0.0034389600404020814\n",
      "Epoch 12/300\n",
      "Average training loss: 0.03080250640379058\n",
      "Average test loss: 0.006438359979126188\n",
      "Epoch 13/300\n",
      "Average training loss: 0.02918251723713345\n",
      "Average test loss: 0.0034943326531598966\n",
      "Epoch 14/300\n",
      "Average training loss: 0.028052403797705967\n",
      "Average test loss: 0.0031794955047468342\n",
      "Epoch 15/300\n",
      "Average training loss: 0.026657939791679384\n",
      "Average test loss: 0.0034087369160519707\n",
      "Epoch 16/300\n",
      "Average training loss: 0.02578632868329684\n",
      "Average test loss: 0.0030178863153689436\n",
      "Epoch 17/300\n",
      "Average training loss: 0.024777927094035677\n",
      "Average test loss: 0.0029390587146497435\n",
      "Epoch 18/300\n",
      "Average training loss: 0.023913608257969222\n",
      "Average test loss: 0.002780257573351264\n",
      "Epoch 19/300\n",
      "Average training loss: 0.02339881830745273\n",
      "Average test loss: 0.002829225404188037\n",
      "Epoch 20/300\n",
      "Average training loss: 0.022572237744927406\n",
      "Average test loss: 0.0029673312575452856\n",
      "Epoch 21/300\n",
      "Average training loss: 0.02217825693057643\n",
      "Average test loss: 0.00275129440261258\n",
      "Epoch 22/300\n",
      "Average training loss: 0.021773404682676\n",
      "Average test loss: 0.002945040164204935\n",
      "Epoch 23/300\n",
      "Average training loss: 0.021304892660843\n",
      "Average test loss: 0.00539420709758997\n",
      "Epoch 24/300\n",
      "Average training loss: 0.021127771619293426\n",
      "Average test loss: 0.00323066204910477\n",
      "Epoch 25/300\n",
      "Average training loss: 0.020729213727845086\n",
      "Average test loss: 0.0025386679294622607\n",
      "Epoch 26/300\n",
      "Average training loss: 0.02038954659137461\n",
      "Average test loss: 0.0025889825434941382\n",
      "Epoch 27/300\n",
      "Average training loss: 0.02015636349717776\n",
      "Average test loss: 0.002520328715754052\n",
      "Epoch 28/300\n",
      "Average training loss: 0.01986697982913918\n",
      "Average test loss: 0.0025192334471891326\n",
      "Epoch 29/300\n",
      "Average training loss: 0.019697400930855008\n",
      "Average test loss: 0.002444124151642124\n",
      "Epoch 30/300\n",
      "Average training loss: 0.019420080400175517\n",
      "Average test loss: 0.0024136543911364343\n",
      "Epoch 31/300\n",
      "Average training loss: 0.019320909048120182\n",
      "Average test loss: 0.002700942574482825\n",
      "Epoch 32/300\n",
      "Average training loss: 0.01911543826510509\n",
      "Average test loss: 0.0028603702783584596\n",
      "Epoch 33/300\n",
      "Average training loss: 0.018898268987735112\n",
      "Average test loss: 0.002781193573648731\n",
      "Epoch 34/300\n",
      "Average training loss: 0.018730341619915433\n",
      "Average test loss: 0.00254879434282581\n",
      "Epoch 35/300\n",
      "Average training loss: 0.018626587892572086\n",
      "Average test loss: 0.0025277960871656737\n",
      "Epoch 36/300\n",
      "Average training loss: 0.01850409976972474\n",
      "Average test loss: 0.002691489551630285\n",
      "Epoch 37/300\n",
      "Average training loss: 0.0184557753139072\n",
      "Average test loss: 0.0024553649133692186\n",
      "Epoch 38/300\n",
      "Average training loss: 0.018289184479249848\n",
      "Average test loss: 0.0023626238612665072\n",
      "Epoch 39/300\n",
      "Average training loss: 0.01816184130642149\n",
      "Average test loss: 0.0024090111328081954\n",
      "Epoch 40/300\n",
      "Average training loss: 0.01816777770386802\n",
      "Average test loss: 0.0026065198396229084\n",
      "Epoch 41/300\n",
      "Average training loss: 0.01804154364599122\n",
      "Average test loss: 0.002400826319741706\n",
      "Epoch 42/300\n",
      "Average training loss: 0.01793713891506195\n",
      "Average test loss: 0.0030943565480411055\n",
      "Epoch 43/300\n",
      "Average training loss: 0.017785044588976437\n",
      "Average test loss: 0.0024530724934819672\n",
      "Epoch 44/300\n",
      "Average training loss: 0.017735499847266408\n",
      "Average test loss: 0.0024133337239424387\n",
      "Epoch 45/300\n",
      "Average training loss: 0.017634390720062787\n",
      "Average test loss: 0.002413454068617688\n",
      "Epoch 46/300\n",
      "Average training loss: 0.017618091742197674\n",
      "Average test loss: 0.002417517673431171\n",
      "Epoch 47/300\n",
      "Average training loss: 0.017244472831487657\n",
      "Average test loss: 0.002374070777247349\n",
      "Epoch 52/300\n",
      "Average training loss: 0.01719509865012434\n",
      "Average test loss: 0.0023506439646912946\n",
      "Epoch 53/300\n",
      "Average training loss: 0.017126951941185527\n",
      "Average test loss: 0.0023195381527766584\n",
      "Epoch 54/300\n",
      "Average training loss: 0.017010379905502002\n",
      "Average test loss: 0.0024298339103245073\n",
      "Epoch 55/300\n",
      "Average training loss: 0.017022735823359756\n",
      "Average test loss: 0.004155778856534097\n",
      "Epoch 56/300\n",
      "Average training loss: 0.016977288455598885\n",
      "Average test loss: 0.0023781477641314267\n",
      "Epoch 57/300\n",
      "Average training loss: 0.016892812409334712\n",
      "Average test loss: 0.002495855149088634\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01688152078125212\n",
      "Average test loss: 0.003657609551317162\n",
      "Epoch 59/300\n",
      "Average training loss: 0.016739271965291767\n",
      "Average test loss: 0.002284336345580717\n",
      "Epoch 60/300\n",
      "Average training loss: 0.016746918220486906\n",
      "Average test loss: 0.0024092119733492533\n",
      "Epoch 61/300\n",
      "Average training loss: 0.016779930006298755\n",
      "Average test loss: 0.002674961825625764\n",
      "Epoch 62/300\n",
      "Average training loss: 0.016732953656050892\n",
      "Average test loss: 0.002276866463530395\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01658701321979364\n",
      "Average test loss: 0.002301044932877024\n",
      "Epoch 64/300\n",
      "Average training loss: 0.016567576532562574\n",
      "Average test loss: 0.002372450969906317\n",
      "Epoch 65/300\n",
      "Average training loss: 0.016540958508849145\n",
      "Average test loss: 0.0022801198398487437\n",
      "Epoch 66/300\n",
      "Average training loss: 0.01651591952310668\n",
      "Average test loss: 0.0029005353140334287\n",
      "Epoch 67/300\n",
      "Average training loss: 0.01646557456586096\n",
      "Average test loss: 0.002269411463704374\n",
      "Epoch 68/300\n",
      "Average training loss: 0.016422605926791827\n",
      "Average test loss: 0.002732795178062386\n",
      "Epoch 69/300\n",
      "Average training loss: 0.016385575579272376\n",
      "Average test loss: 0.0023348747483558125\n",
      "Epoch 70/300\n",
      "Average training loss: 0.016279627662565972\n",
      "Average test loss: 0.00249194837299486\n",
      "Epoch 71/300\n",
      "Average training loss: 0.016300394040842853\n",
      "Average test loss: 0.0022900939368539386\n",
      "Epoch 72/300\n",
      "Average training loss: 0.016229613786770236\n",
      "Average test loss: 0.002297711886258589\n",
      "Epoch 73/300\n",
      "Average training loss: 0.016213728258179294\n",
      "Average test loss: 0.0023619232531636954\n",
      "Epoch 74/300\n",
      "Average training loss: 0.016198318493035106\n",
      "Average test loss: 0.0024085509938498338\n",
      "Epoch 75/300\n",
      "Average training loss: 0.016079137755764854\n",
      "Average test loss: 0.002311440910316176\n",
      "Epoch 79/300\n",
      "Average training loss: 0.016011085285080804\n",
      "Average test loss: 0.0023621292466090783\n",
      "Epoch 80/300\n",
      "Average training loss: 0.015972938168379996\n",
      "Average test loss: 0.00233165818432139\n",
      "Epoch 81/300\n",
      "Average training loss: 0.015971290008889305\n",
      "Average test loss: 0.0023605155524694258\n",
      "Epoch 82/300\n",
      "Average training loss: 0.01592512535552184\n",
      "Average test loss: 0.002365560096171167\n",
      "Epoch 83/300\n",
      "Average training loss: 0.01588725869026449\n",
      "Average test loss: 0.0023104541721857257\n",
      "Epoch 84/300\n",
      "Average training loss: 0.01583502480553256\n",
      "Average test loss: 0.0022771018410308493\n",
      "Epoch 85/300\n",
      "Average training loss: 0.015831454674402872\n",
      "Average test loss: 0.0022545143438296184\n",
      "Epoch 86/300\n",
      "Average training loss: 0.015844096855984795\n",
      "Average test loss: 0.0023485519145098\n",
      "Epoch 87/300\n",
      "Average training loss: 0.01581413547363546\n",
      "Average test loss: 0.0023045687396079303\n",
      "Epoch 88/300\n",
      "Average training loss: 0.015773440428906018\n",
      "Average test loss: 0.002283932821204265\n",
      "Epoch 89/300\n",
      "Average training loss: 0.01575994265245067\n",
      "Average test loss: 0.0022672989012466538\n",
      "Epoch 90/300\n",
      "Average training loss: 0.015692038928469022\n",
      "Average test loss: 0.0022991040465939376\n",
      "Epoch 91/300\n",
      "Average training loss: 0.01566846803161833\n",
      "Average test loss: 0.002288192505844765\n",
      "Epoch 92/300\n",
      "Average training loss: 0.01566205827229553\n",
      "Average test loss: 0.0024576833210885525\n",
      "Epoch 93/300\n",
      "Average training loss: 0.015646300916042594\n",
      "Average test loss: 0.002383282056492236\n",
      "Epoch 94/300\n",
      "Average training loss: 0.015622336483663983\n",
      "Average test loss: 0.002404896759117643\n",
      "Epoch 95/300\n",
      "Average training loss: 0.015530856466127766\n",
      "Average test loss: 0.002296620478750103\n",
      "Epoch 96/300\n",
      "Average training loss: 0.015592330409420862\n",
      "Average test loss: 0.0022926012859162356\n",
      "Epoch 97/300\n",
      "Average training loss: 0.01553571541276243\n",
      "Average test loss: 0.0022820116558836564\n",
      "Epoch 98/300\n",
      "Average training loss: 0.015542480390932825\n",
      "Average test loss: 0.002491819782389535\n",
      "Epoch 99/300\n",
      "Average training loss: 0.015497480149898265\n",
      "Average test loss: 0.002329784906986687\n",
      "Epoch 100/300\n",
      "Average training loss: 0.015431899793446065\n",
      "Average test loss: 0.0024832154698669912\n",
      "Epoch 101/300\n",
      "Average training loss: 0.015445470436579652\n",
      "Average test loss: 0.00228255007457402\n",
      "Epoch 102/300\n",
      "Average training loss: 0.015415542668766446\n",
      "Average test loss: 0.002332700936951571\n",
      "Epoch 103/300\n",
      "Average training loss: 0.015415057206319438\n",
      "Average test loss: 0.0023067587128736905\n",
      "Epoch 104/300\n",
      "Average training loss: 0.015338242216242684\n",
      "Average test loss: 0.0022690366583151948\n",
      "Epoch 105/300\n",
      "Average training loss: 0.015339058700535032\n",
      "Average test loss: 0.002267714685656958\n",
      "Epoch 106/300\n",
      "Average training loss: 0.015364394191238615\n",
      "Average test loss: 0.0022933420093937053\n",
      "Epoch 107/300\n",
      "Average training loss: 0.015328652409215769\n",
      "Average test loss: 0.002351037545854019\n",
      "Epoch 108/300\n",
      "Average training loss: 0.015291798307663865\n",
      "Average test loss: 0.0023036648378603987\n",
      "Epoch 109/300\n",
      "Average training loss: 0.015294275831845072\n",
      "Average test loss: 0.002286014093707005\n",
      "Epoch 110/300\n",
      "Average training loss: 0.015228412152992355\n",
      "Average test loss: 0.002340846372043921\n",
      "Epoch 111/300\n",
      "Average training loss: 0.015223623265822729\n",
      "Average test loss: 0.002458946852013469\n",
      "Epoch 112/300\n",
      "Average training loss: 0.015106970239844588\n",
      "Average test loss: 0.0022667033568852476\n",
      "Epoch 117/300\n",
      "Average training loss: 0.015124644639591376\n",
      "Average test loss: 0.0023347315788269044\n",
      "Epoch 118/300\n",
      "Average training loss: 0.015097138113445705\n",
      "Average test loss: 0.0023052006142420902\n",
      "Epoch 119/300\n",
      "Average training loss: 0.015085690440403091\n",
      "Average test loss: 0.0024364722652567756\n",
      "Epoch 120/300\n",
      "Average training loss: 0.015080804374482896\n",
      "Average test loss: 0.0023100521653476687\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01505429910040564\n",
      "Average test loss: 0.0023890805625253254\n",
      "Epoch 122/300\n",
      "Average training loss: 0.015016239313615693\n",
      "Average test loss: 0.0023428941758142576\n",
      "Epoch 123/300\n",
      "Average training loss: 0.015037116672429773\n",
      "Average test loss: 0.002737335919092099\n",
      "Epoch 124/300\n",
      "Average training loss: 0.015019428619907961\n",
      "Average test loss: 0.0023706299497021567\n",
      "Epoch 125/300\n",
      "Average training loss: 0.014964036806590027\n",
      "Average test loss: 0.0023977315144406426\n",
      "Epoch 126/300\n",
      "Average training loss: 0.01495515339407656\n",
      "Average test loss: 0.002400870425419675\n",
      "Epoch 127/300\n",
      "Average training loss: 0.014943116499318017\n",
      "Average test loss: 0.0022936529241916206\n",
      "Epoch 128/300\n",
      "Average training loss: 0.014951854506300556\n",
      "Average test loss: 0.0023559666222168337\n",
      "Epoch 129/300\n",
      "Average training loss: 0.014928988056050406\n",
      "Average test loss: 0.002509939844202664\n",
      "Epoch 130/300\n",
      "Average training loss: 0.014913451780047682\n",
      "Average test loss: 0.002369971937706901\n",
      "Epoch 131/300\n",
      "Average training loss: 0.014874483979410595\n",
      "Average test loss: 0.0023184955978973044\n",
      "Epoch 132/300\n",
      "Average training loss: 0.014847041952941153\n",
      "Average test loss: 0.0024198705442249773\n",
      "Epoch 133/300\n",
      "Average training loss: 0.014848043374717236\n",
      "Average test loss: 0.0023659348545802964\n",
      "Epoch 134/300\n",
      "Average training loss: 0.014862506128847599\n",
      "Average test loss: 0.0024396057141323883\n",
      "Epoch 135/300\n",
      "Average training loss: 0.014847389351990488\n",
      "Average test loss: 0.0024009521756735114\n",
      "Epoch 136/300\n",
      "Average training loss: 0.014806676755348842\n",
      "Average test loss: 0.0024025032801760567\n",
      "Epoch 137/300\n",
      "Average training loss: 0.014785952742728922\n",
      "Average test loss: 0.002382054441815449\n",
      "Epoch 138/300\n",
      "Average training loss: 0.01479553162927429\n",
      "Average test loss: 0.002544649138632748\n",
      "Epoch 139/300\n",
      "Average training loss: 0.014764495004382398\n",
      "Average test loss: 0.002441261411541038\n",
      "Epoch 140/300\n",
      "Average training loss: 0.014775606186853515\n",
      "Average test loss: 0.0026090942095551226\n",
      "Epoch 141/300\n",
      "Average training loss: 0.014751871291134092\n",
      "Average test loss: 0.0024002855180038345\n",
      "Epoch 142/300\n",
      "Average training loss: 0.014733015776508384\n",
      "Average test loss: 0.0023646179816375176\n",
      "Epoch 143/300\n",
      "Average training loss: 0.014750833725763692\n",
      "Average test loss: 0.002284848239686754\n",
      "Epoch 144/300\n",
      "Average training loss: 0.014691649301184549\n",
      "Average test loss: 0.0024169064689841535\n",
      "Epoch 145/300\n",
      "Average training loss: 0.014690266085995569\n",
      "Average test loss: 0.0024090734728508525\n",
      "Epoch 146/300\n",
      "Average training loss: 0.014690311785373424\n",
      "Average test loss: 0.0022903511116488112\n",
      "Epoch 147/300\n",
      "Average training loss: 0.01467546367065774\n",
      "Average test loss: 0.002523831175433265\n",
      "Epoch 148/300\n",
      "Average training loss: 0.014645818945434359\n",
      "Average test loss: 0.0024590731607750057\n",
      "Epoch 149/300\n",
      "Average training loss: 0.014673643425107002\n",
      "Average test loss: 0.0024250544001244837\n",
      "Epoch 150/300\n",
      "Average training loss: 0.014624423899584345\n",
      "Average test loss: 0.0023499411611507337\n",
      "Epoch 151/300\n",
      "Average training loss: 0.014614548459649086\n",
      "Average test loss: 0.0025081707400580245\n",
      "Epoch 152/300\n",
      "Average training loss: 0.014614747568137116\n",
      "Average test loss: 0.0029053359727064768\n",
      "Epoch 153/300\n",
      "Average training loss: 0.014623128990332286\n",
      "Average test loss: 0.00250290546586944\n",
      "Epoch 154/300\n",
      "Average training loss: 0.01459289235373338\n",
      "Average test loss: 0.002652211030531261\n",
      "Epoch 155/300\n",
      "Average training loss: 0.01458761664148834\n",
      "Average test loss: 0.0023772111980037555\n",
      "Epoch 156/300\n",
      "Average training loss: 0.014543579122258557\n",
      "Average test loss: 0.0023458818923681973\n",
      "Epoch 157/300\n",
      "Average training loss: 0.014509909505645435\n",
      "Average test loss: 0.002485909014940262\n",
      "Epoch 158/300\n",
      "Average training loss: 0.01455823934243785\n",
      "Average test loss: 0.0024125815253290867\n",
      "Epoch 159/300\n",
      "Average training loss: 0.014521777302854592\n",
      "Average test loss: 0.0023783871786048016\n",
      "Epoch 160/300\n",
      "Average training loss: 0.01451877353257603\n",
      "Average test loss: 0.002521444962670406\n",
      "Epoch 161/300\n",
      "Average training loss: 0.014502925791260269\n",
      "Average test loss: 0.002648083934974339\n",
      "Epoch 162/300\n",
      "Average training loss: 0.01453875916534\n",
      "Average test loss: 0.00238753717020154\n",
      "Epoch 163/300\n",
      "Average training loss: 0.014492477209203772\n",
      "Average test loss: 0.002426145778141088\n",
      "Epoch 164/300\n",
      "Average training loss: 0.014453885274628799\n",
      "Average test loss: 0.0027438031670947867\n",
      "Epoch 165/300\n",
      "Average training loss: 0.014437723535630439\n",
      "Average test loss: 0.0023588523144523303\n",
      "Epoch 166/300\n",
      "Average training loss: 0.014470712716380755\n",
      "Average test loss: 0.0023475224326054253\n",
      "Epoch 167/300\n",
      "Average training loss: 0.014445174587269623\n",
      "Average test loss: 0.0024524489715695382\n",
      "Epoch 168/300\n",
      "Average training loss: 0.014469897937443521\n",
      "Average test loss: 0.0034100577156576843\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04631008287767569\n",
      "Average test loss: 0.003264453958099087\n",
      "Epoch 170/300\n",
      "Average training loss: 0.0223959378980928\n",
      "Average test loss: 0.0026255639154050083\n",
      "Epoch 171/300\n",
      "Average training loss: 0.01994483911825551\n",
      "Average test loss: 0.0024952811528411176\n",
      "Epoch 172/300\n",
      "Average training loss: 0.01858672544029024\n",
      "Average test loss: 0.0024253790945642523\n",
      "Epoch 173/300\n",
      "Average training loss: 0.017635359679659207\n",
      "Average test loss: 0.002364660182967782\n",
      "Epoch 174/300\n",
      "Average training loss: 0.01693096669514974\n",
      "Average test loss: 0.00244129482180708\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01633285424941116\n",
      "Average test loss: 0.002329715141819583\n",
      "Epoch 176/300\n",
      "Average training loss: 0.015848263533578977\n",
      "Average test loss: 0.002434488614089787\n",
      "Epoch 177/300\n",
      "Average training loss: 0.015431112814280721\n",
      "Average test loss: 0.002301212459699147\n",
      "Epoch 178/300\n",
      "Average training loss: 0.015140141536378198\n",
      "Average test loss: 0.0024251460559252236\n",
      "Epoch 179/300\n",
      "Average training loss: 0.01491199748383628\n",
      "Average test loss: 0.00234509816331168\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0147438029108776\n",
      "Average test loss: 0.002465530330625673\n",
      "Epoch 181/300\n",
      "Average training loss: 0.014619008681840366\n",
      "Average test loss: 0.0024483543508168723\n",
      "Epoch 182/300\n",
      "Average training loss: 0.01459287251614862\n",
      "Average test loss: 0.002423582951641745\n",
      "Epoch 183/300\n",
      "Average training loss: 0.014516968733734554\n",
      "Average test loss: 0.002496069183573127\n",
      "Epoch 184/300\n",
      "Average training loss: 0.01446814954198069\n",
      "Average test loss: 0.002420446684377061\n",
      "Epoch 185/300\n",
      "Average training loss: 0.01444087385882934\n",
      "Average test loss: 0.0024635494524199103\n",
      "Epoch 186/300\n",
      "Average training loss: 0.014416626494791772\n",
      "Average test loss: 0.00239340997269998\n",
      "Epoch 187/300\n",
      "Average training loss: 0.014406495803760157\n",
      "Average test loss: 0.0026325266588893203\n",
      "Epoch 188/300\n",
      "Average training loss: 0.014386982236471441\n",
      "Average test loss: 0.00244390388040079\n",
      "Epoch 189/300\n",
      "Average training loss: 0.014354302207628886\n",
      "Average test loss: 0.0024982729856338768\n",
      "Epoch 190/300\n",
      "Average training loss: 0.01437827372054259\n",
      "Average test loss: 0.0023713625898170803\n",
      "Epoch 191/300\n",
      "Average training loss: 0.01434054296132591\n",
      "Average test loss: 0.002553031111860441\n",
      "Epoch 192/300\n",
      "Average training loss: 0.01436095243278477\n",
      "Average test loss: 0.002415542152296338\n",
      "Epoch 193/300\n",
      "Average training loss: 0.014335279499491056\n",
      "Average test loss: 0.0024859678126457666\n",
      "Epoch 194/300\n",
      "Average training loss: 0.014343850194580026\n",
      "Average test loss: 0.0023625772235294183\n",
      "Epoch 195/300\n",
      "Average training loss: 0.014339929496248563\n",
      "Average test loss: 0.002458459352246589\n",
      "Epoch 196/300\n",
      "Average training loss: 0.014304375613729159\n",
      "Average test loss: 0.002494506792165339\n",
      "Epoch 197/300\n",
      "Average training loss: 0.014282967045903205\n",
      "Average test loss: 0.0025024109842876592\n",
      "Epoch 198/300\n",
      "Average training loss: 0.014283787050180964\n",
      "Average test loss: 0.002457090060744021\n",
      "Epoch 199/300\n",
      "Average training loss: 0.014276874113414023\n",
      "Average test loss: 0.0023928965541223685\n",
      "Epoch 200/300\n",
      "Average training loss: 0.014266873669293191\n",
      "Average test loss: 0.0023986545914991032\n",
      "Epoch 201/300\n",
      "Average training loss: 0.014256508829693\n",
      "Average test loss: 0.002430119808556305\n",
      "Epoch 202/300\n",
      "Average training loss: 0.014237590632504887\n",
      "Average test loss: 0.0023932227581325506\n",
      "Epoch 203/300\n",
      "Average training loss: 0.014242906981044346\n",
      "Average test loss: 0.0024694991677792536\n",
      "Epoch 204/300\n",
      "Average training loss: 0.014244584458569685\n",
      "Average test loss: 0.002431954595570763\n",
      "Epoch 205/300\n",
      "Average training loss: 0.014236946287254493\n",
      "Average test loss: 0.002524604742311769\n",
      "Epoch 206/300\n",
      "Average training loss: 0.014267214687334167\n",
      "Average test loss: 0.00238177187099225\n",
      "Epoch 207/300\n",
      "Average training loss: 0.014220777109265328\n",
      "Average test loss: 0.002647757242537207\n",
      "Epoch 208/300\n",
      "Average training loss: 0.014197907184561093\n",
      "Average test loss: 0.00239720974303782\n",
      "Epoch 209/300\n",
      "Average training loss: 0.014187528673973349\n",
      "Average test loss: 0.0024178804635173745\n",
      "Epoch 210/300\n",
      "Average training loss: 0.014131436213850976\n",
      "Average test loss: 0.0023811302373392715\n",
      "Epoch 211/300\n",
      "Average training loss: 0.014180242667595546\n",
      "Average test loss: 0.0024090832441838253\n",
      "Epoch 212/300\n",
      "Average training loss: 0.014178669926193025\n",
      "Average test loss: 0.0023470508669399554\n",
      "Epoch 213/300\n",
      "Average training loss: 0.014173699157105552\n",
      "Average test loss: 0.0023692244979449445\n",
      "Epoch 214/300\n",
      "Average training loss: 0.01413907216489315\n",
      "Average test loss: 0.0026840848941355943\n",
      "Epoch 215/300\n",
      "Average training loss: 0.014160782364507516\n",
      "Average test loss: 0.0025135490134772327\n",
      "Epoch 216/300\n",
      "Average training loss: 0.014127935389677684\n",
      "Average test loss: 0.0025646604895591734\n",
      "Epoch 217/300\n",
      "Average training loss: 0.014118551378448804\n",
      "Average test loss: 0.002406716671565341\n",
      "Epoch 218/300\n",
      "Average training loss: 0.014121663183801704\n",
      "Average test loss: 0.002406701569755872\n",
      "Epoch 219/300\n",
      "Average training loss: 0.014113724877436956\n",
      "Average test loss: 0.0037398984644355046\n",
      "Epoch 220/300\n",
      "Average training loss: 0.014104849639866086\n",
      "Average test loss: 0.0026187610290944576\n",
      "Epoch 221/300\n",
      "Average training loss: 0.014073687603904142\n",
      "Average test loss: 0.0024164241602023444\n",
      "Epoch 222/300\n",
      "Average training loss: 0.01407604558351967\n",
      "Average test loss: 0.0024820142914023665\n",
      "Epoch 223/300\n",
      "Average training loss: 0.014051914496554269\n",
      "Average test loss: 0.002582053005695343\n",
      "Epoch 224/300\n",
      "Average training loss: 0.014088048093020916\n",
      "Average test loss: 0.00249887541288303\n",
      "Epoch 225/300\n",
      "Average training loss: 0.014035151296191746\n",
      "Average test loss: 0.0024823033711355593\n",
      "Epoch 226/300\n",
      "Average training loss: 0.014043971309231388\n",
      "Average test loss: 0.002451818651830157\n",
      "Epoch 227/300\n",
      "Average training loss: 0.014124906194706757\n",
      "Average test loss: 0.002501639525923464\n",
      "Epoch 228/300\n",
      "Average training loss: 0.014050664908356136\n",
      "Average test loss: 0.0025328699346217846\n",
      "Epoch 229/300\n",
      "Average training loss: 0.014019002502163251\n",
      "Average test loss: 0.0026939155283487505\n",
      "Epoch 230/300\n",
      "Average training loss: 0.014061295430693361\n",
      "Average test loss: 0.0024757844840900766\n",
      "Epoch 231/300\n",
      "Average training loss: 0.014011211342281765\n",
      "Average test loss: 0.0023999526297880545\n",
      "Epoch 232/300\n",
      "Average training loss: 0.014017144897745715\n",
      "Average test loss: 0.0023480448067809143\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01402579878601763\n",
      "Average test loss: 0.0026031298894021245\n",
      "Epoch 234/300\n",
      "Average training loss: 0.013959887243807315\n",
      "Average test loss: 0.0034375571449183754\n",
      "Epoch 235/300\n",
      "Average training loss: 0.013994654948512713\n",
      "Average test loss: 0.0024163482293693556\n",
      "Epoch 236/300\n",
      "Average training loss: 0.014045731316010158\n",
      "Average test loss: 0.0023856824131475553\n",
      "Epoch 237/300\n",
      "Average training loss: 0.013964868365062607\n",
      "Average test loss: 0.0024896239797688193\n",
      "Epoch 238/300\n",
      "Average training loss: 0.013978980060252878\n",
      "Average test loss: 0.0025287735496337216\n",
      "Epoch 239/300\n",
      "Average training loss: 0.013965090443690618\n",
      "Average test loss: 0.0025120596289634704\n",
      "Epoch 240/300\n",
      "Average training loss: 0.01397931983653042\n",
      "Average test loss: 0.002480857068258855\n",
      "Epoch 241/300\n",
      "Average training loss: 0.01398589104331202\n",
      "Average test loss: 0.002622215623449948\n",
      "Epoch 242/300\n",
      "Average training loss: 0.01394004310750299\n",
      "Average test loss: 0.002413524568080902\n",
      "Epoch 243/300\n",
      "Average training loss: 0.013953264045218627\n",
      "Average test loss: 0.0024396602703879275\n",
      "Epoch 244/300\n",
      "Average training loss: 0.013910036947992113\n",
      "Average test loss: 0.002428788523707125\n",
      "Epoch 245/300\n",
      "Average training loss: 0.013917852985362212\n",
      "Average test loss: 0.00294885062550505\n",
      "Epoch 246/300\n",
      "Average training loss: 0.01394114838871691\n",
      "Average test loss: 0.002592464390107327\n",
      "Epoch 247/300\n",
      "Average training loss: 0.013907355561852455\n",
      "Average test loss: 0.0023898978028446434\n",
      "Epoch 248/300\n",
      "Average training loss: 0.01394525018086036\n",
      "Average test loss: 0.002442374379374087\n",
      "Epoch 249/300\n",
      "Average training loss: 0.013892056906388865\n",
      "Average test loss: 0.0024133482946910793\n",
      "Epoch 250/300\n",
      "Average training loss: 0.013889099312325319\n",
      "Average test loss: 0.0030979683697223664\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01391575964126322\n",
      "Average test loss: 0.0024721164949652224\n",
      "Epoch 252/300\n",
      "Average training loss: 0.013881090067327023\n",
      "Average test loss: 0.002521474267045657\n",
      "Epoch 253/300\n",
      "Average training loss: 0.013896252151992586\n",
      "Average test loss: 0.0024512740491578976\n",
      "Epoch 254/300\n",
      "Average training loss: 0.013868157635960315\n",
      "Average test loss: 0.0024998782738629315\n",
      "Epoch 255/300\n",
      "Average training loss: 0.013870739719933934\n",
      "Average test loss: 0.0028247557437668246\n",
      "Epoch 256/300\n",
      "Average training loss: 0.013856106508937148\n",
      "Average test loss: 0.0024471139188648925\n",
      "Epoch 257/300\n",
      "Average training loss: 0.01385938861552212\n",
      "Average test loss: 0.0024110096954844066\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01386250504768557\n",
      "Average test loss: 0.0025670710257771944\n",
      "Epoch 259/300\n",
      "Average training loss: 0.013829269810683198\n",
      "Average test loss: 0.00257096405648109\n",
      "Epoch 260/300\n",
      "Average training loss: 0.01387204090009133\n",
      "Average test loss: 0.0025851927178187502\n",
      "Epoch 261/300\n",
      "Average training loss: 0.013855320952832699\n",
      "Average test loss: 0.0025310998434821767\n",
      "Epoch 262/300\n",
      "Average training loss: 0.013834613288442293\n",
      "Average test loss: 0.0024753853937404023\n",
      "Epoch 263/300\n",
      "Average training loss: 0.013840637740161683\n",
      "Average test loss: 0.0024618685651156638\n",
      "Epoch 264/300\n",
      "Average training loss: 0.013795225346253978\n",
      "Average test loss: 0.002475187482312322\n",
      "Epoch 265/300\n",
      "Average training loss: 0.013807448625564576\n",
      "Average test loss: 0.002483117219681541\n",
      "Epoch 266/300\n",
      "Average training loss: 0.013840522811644607\n",
      "Average test loss: 0.002656774105918076\n",
      "Epoch 267/300\n",
      "Average training loss: 0.013786805619796116\n",
      "Average test loss: 0.0025450471163623862\n",
      "Epoch 268/300\n",
      "Average training loss: 0.013800933295653926\n",
      "Average test loss: 0.0025425666710361837\n",
      "Epoch 269/300\n",
      "Average training loss: 0.013781896472805076\n",
      "Average test loss: 0.0025188910253345965\n",
      "Epoch 270/300\n",
      "Average training loss: 0.01383170759677887\n",
      "Average test loss: 0.0024283643164154556\n",
      "Epoch 271/300\n",
      "Average training loss: 0.01382198578533199\n",
      "Average test loss: 0.0024728692254672446\n",
      "Epoch 272/300\n",
      "Average training loss: 0.013766160532004303\n",
      "Average test loss: 0.0024665242445965608\n",
      "Epoch 273/300\n",
      "Average training loss: 0.013776098258793354\n",
      "Average test loss: 0.0023892426358328926\n",
      "Epoch 274/300\n",
      "Average training loss: 0.01378003546843926\n",
      "Average test loss: 0.002562081240945392\n",
      "Epoch 275/300\n",
      "Average training loss: 0.01378953627910879\n",
      "Average test loss: 0.002529936057411962\n",
      "Epoch 276/300\n",
      "Average training loss: 0.01373725561135345\n",
      "Average test loss: 0.0024619330916967655\n",
      "Epoch 277/300\n",
      "Average training loss: 0.013736457761791018\n",
      "Average test loss: 0.0024560331462158097\n",
      "Epoch 278/300\n",
      "Average training loss: 0.013795353495412402\n",
      "Average test loss: 0.0025368178443362314\n",
      "Epoch 279/300\n",
      "Average training loss: 0.013750135897762245\n",
      "Average test loss: 0.002675640500978463\n",
      "Epoch 280/300\n",
      "Average training loss: 0.013727297296126683\n",
      "Average test loss: 0.0024706048770911163\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01374633390787575\n",
      "Average test loss: 0.0026154080387204887\n",
      "Epoch 282/300\n",
      "Average training loss: 0.01373390270355675\n",
      "Average test loss: 0.0025533331542586285\n",
      "Epoch 283/300\n",
      "Average training loss: 0.013755476228892803\n",
      "Average test loss: 0.00241832743698938\n",
      "Epoch 284/300\n",
      "Average training loss: 0.013705678006841077\n",
      "Average test loss: 0.003048653098030223\n",
      "Epoch 285/300\n",
      "Average training loss: 0.013737062888840834\n",
      "Average test loss: 0.002526214020119773\n",
      "Epoch 286/300\n",
      "Average training loss: 0.013709690757095813\n",
      "Average test loss: 0.0026616283980094723\n",
      "Epoch 287/300\n",
      "Average training loss: 0.013720046014421516\n",
      "Average test loss: 0.002528162297068371\n",
      "Epoch 288/300\n",
      "Average training loss: 0.013667027688688702\n",
      "Average test loss: 0.0025027363000230656\n",
      "Epoch 289/300\n",
      "Average training loss: 0.013715779713458486\n",
      "Average test loss: 0.002444119686881701\n",
      "Epoch 290/300\n",
      "Average training loss: 0.013763574624227153\n",
      "Average test loss: 0.002452785046564208\n",
      "Epoch 291/300\n",
      "Average training loss: 0.013707463122904301\n",
      "Average test loss: 0.0025064313459313577\n",
      "Epoch 292/300\n",
      "Average training loss: 0.013671311629315216\n",
      "Average test loss: 0.0024796833510821064\n",
      "Epoch 293/300\n",
      "Average training loss: 0.013679327564107047\n",
      "Average test loss: 0.0024651047074132496\n",
      "Epoch 294/300\n",
      "Average training loss: 0.013667159563965267\n",
      "Average test loss: 0.0024977650373346275\n",
      "Epoch 295/300\n",
      "Average training loss: 0.013701521077917682\n",
      "Average test loss: 0.002457827217049069\n",
      "Epoch 296/300\n",
      "Average training loss: 0.013672285695870718\n",
      "Average test loss: 0.0024940176808999643\n",
      "Epoch 297/300\n",
      "Average training loss: 0.013665893830358981\n",
      "Average test loss: 0.002471760416403413\n",
      "Epoch 298/300\n",
      "Average training loss: 0.01367238910496235\n",
      "Average test loss: 0.0025680783059861927\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01369954002979729\n",
      "Average test loss: 0.0025767647272182835\n",
      "Epoch 300/300\n",
      "Average training loss: 0.013656768379112085\n",
      "Average test loss: 0.002588766605489784\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 0.2461125700738695\n",
      "Average test loss: 0.009223829100529353\n",
      "Epoch 2/300\n",
      "Average training loss: 0.08459233750237359\n",
      "Average test loss: 0.007934830283953084\n",
      "Epoch 3/300\n",
      "Average training loss: 0.06181312812699212\n",
      "Average test loss: 0.003962270039651129\n",
      "Epoch 4/300\n",
      "Average training loss: 0.05088869507776366\n",
      "Average test loss: 0.0035527934307853382\n",
      "Epoch 5/300\n",
      "Average training loss: 0.04476425162288878\n",
      "Average test loss: 0.0037058330174121593\n",
      "Epoch 6/300\n",
      "Average training loss: 0.03974508846799533\n",
      "Average test loss: 0.00426927852547831\n",
      "Epoch 7/300\n",
      "Average training loss: 0.03683277611931165\n",
      "Average test loss: 0.003111198819345898\n",
      "Epoch 8/300\n",
      "Average training loss: 0.03367392824259069\n",
      "Average test loss: 0.004221655056708389\n",
      "Epoch 9/300\n",
      "Average training loss: 0.03126347334517373\n",
      "Average test loss: 0.0028130804107834896\n",
      "Epoch 10/300\n",
      "Average training loss: 0.02992957675953706\n",
      "Average test loss: 0.003096283449480931\n",
      "Epoch 11/300\n",
      "Average training loss: 0.027967700423465833\n",
      "Average test loss: 0.002773258349340823\n",
      "Epoch 12/300\n",
      "Average training loss: 0.02667393521550629\n",
      "Average test loss: 0.002660931914113462\n",
      "Epoch 13/300\n",
      "Average training loss: 0.025447006872958606\n",
      "Average test loss: 0.02506021872493956\n",
      "Epoch 14/300\n",
      "Average training loss: 0.023989535465836525\n",
      "Average test loss: 0.0025284686705304518\n",
      "Epoch 15/300\n",
      "Average training loss: 0.023096076269944508\n",
      "Average test loss: 0.002332952704280615\n",
      "Epoch 16/300\n",
      "Average training loss: 0.022004852005177072\n",
      "Average test loss: 0.0024523392468690872\n",
      "Epoch 17/300\n",
      "Average training loss: 0.02134309546980593\n",
      "Average test loss: 0.004784628470738729\n",
      "Epoch 18/300\n",
      "Average training loss: 0.020353579945862295\n",
      "Average test loss: 0.0022113228974243006\n",
      "Epoch 19/300\n",
      "Average training loss: 0.01973112177848816\n",
      "Average test loss: 0.0020666500127149955\n",
      "Epoch 20/300\n",
      "Average training loss: 0.01931816937857204\n",
      "Average test loss: 0.002083647539011306\n",
      "Epoch 21/300\n",
      "Average training loss: 0.018695420323974556\n",
      "Average test loss: 0.002234399488816659\n",
      "Epoch 22/300\n",
      "Average training loss: 0.01840368349519041\n",
      "Average test loss: 0.002209154346750842\n",
      "Epoch 23/300\n",
      "Average training loss: 0.017958874163528284\n",
      "Average test loss: 0.00305745812267479\n",
      "Epoch 24/300\n",
      "Average training loss: 0.017505661480956607\n",
      "Average test loss: 0.002042119073164132\n",
      "Epoch 25/300\n",
      "Average training loss: 0.0173498330331511\n",
      "Average test loss: 0.0018762767206256588\n",
      "Epoch 26/300\n",
      "Average training loss: 0.017087243623203702\n",
      "Average test loss: 0.0020168010618330703\n",
      "Epoch 27/300\n",
      "Average training loss: 0.016837206146783298\n",
      "Average test loss: 0.0018480552963705526\n",
      "Epoch 28/300\n",
      "Average training loss: 0.016571079972717496\n",
      "Average test loss: 0.0019787616218543717\n",
      "Epoch 29/300\n",
      "Average training loss: 0.016422500780887073\n",
      "Average test loss: 0.001953736323863268\n",
      "Epoch 30/300\n",
      "Average training loss: 0.01633603580792745\n",
      "Average test loss: 0.0018224091645744113\n",
      "Epoch 31/300\n",
      "Average training loss: 0.01600445224675867\n",
      "Average test loss: 0.0020550337113026114\n",
      "Epoch 32/300\n",
      "Average training loss: 0.015933225931392774\n",
      "Average test loss: 0.0017981879275499118\n",
      "Epoch 33/300\n",
      "Average training loss: 0.015716485497024323\n",
      "Average test loss: 0.0020360797710923684\n",
      "Epoch 34/300\n",
      "Average training loss: 0.01566978107806709\n",
      "Average test loss: 0.002356630929849214\n",
      "Epoch 35/300\n",
      "Average training loss: 0.01552269428057803\n",
      "Average test loss: 0.001738929026035799\n",
      "Epoch 36/300\n",
      "Average training loss: 0.015378263976838854\n",
      "Average test loss: 0.0017213412509817217\n",
      "Epoch 37/300\n",
      "Average training loss: 0.015332776554756695\n",
      "Average test loss: 0.0017901450014776654\n",
      "Epoch 38/300\n",
      "Average training loss: 0.015187725719478394\n",
      "Average test loss: 0.0018269422059464786\n",
      "Epoch 39/300\n",
      "Average training loss: 0.015064601161413723\n",
      "Average test loss: 0.001807588167488575\n",
      "Epoch 40/300\n",
      "Average training loss: 0.015010338983601995\n",
      "Average test loss: 0.0017430455730193192\n",
      "Epoch 41/300\n",
      "Average training loss: 0.014943027834097544\n",
      "Average test loss: 0.00493922447744343\n",
      "Epoch 42/300\n",
      "Average training loss: 0.014858361382451323\n",
      "Average test loss: 0.001700989067968395\n",
      "Epoch 43/300\n",
      "Average training loss: 0.0147651400681999\n",
      "Average test loss: 0.002221783252329462\n",
      "Epoch 44/300\n",
      "Average training loss: 0.014706086691882876\n",
      "Average test loss: 0.0016957343560126093\n",
      "Epoch 45/300\n",
      "Average training loss: 0.014623328984611564\n",
      "Average test loss: 0.001807090040192836\n",
      "Epoch 46/300\n",
      "Average training loss: 0.014554393612676197\n",
      "Average test loss: 0.001675347656218542\n",
      "Epoch 47/300\n",
      "Average training loss: 0.014491266674465604\n",
      "Average test loss: 0.0017145063331764606\n",
      "Epoch 48/300\n",
      "Average training loss: 0.014430848605930805\n",
      "Average test loss: 0.0017389367759848633\n",
      "Epoch 49/300\n",
      "Average training loss: 0.014340109678606192\n",
      "Average test loss: 0.0016464034873578284\n",
      "Epoch 50/300\n",
      "Average training loss: 0.014286422011752923\n",
      "Average test loss: 0.0016589540121042066\n",
      "Epoch 51/300\n",
      "Average training loss: 0.014235070336196158\n",
      "Average test loss: 0.0019094100003648135\n",
      "Epoch 52/300\n",
      "Average training loss: 0.014199379652738571\n",
      "Average test loss: 0.0017366210700323185\n",
      "Epoch 53/300\n",
      "Average training loss: 0.014162119785116778\n",
      "Average test loss: 0.002803669997801383\n",
      "Epoch 54/300\n",
      "Average training loss: 0.014129194945924812\n",
      "Average test loss: 0.0016788594513717624\n",
      "Epoch 55/300\n",
      "Average training loss: 0.0140170324800743\n",
      "Average test loss: 0.001657579316240218\n",
      "Epoch 56/300\n",
      "Average training loss: 0.013956722530225913\n",
      "Average test loss: 0.0017027350129145715\n",
      "Epoch 57/300\n",
      "Average training loss: 0.013996781579322284\n",
      "Average test loss: 0.00165052437564979\n",
      "Epoch 58/300\n",
      "Average training loss: 0.01388964973969592\n",
      "Average test loss: 0.0016963302577949232\n",
      "Epoch 59/300\n",
      "Average training loss: 0.013878498171766599\n",
      "Average test loss: 0.0020772418787495957\n",
      "Epoch 60/300\n",
      "Average training loss: 0.013818822785384125\n",
      "Average test loss: 0.00167183516241817\n",
      "Epoch 61/300\n",
      "Average training loss: 0.013807105102472836\n",
      "Average test loss: 0.011960985170470343\n",
      "Epoch 62/300\n",
      "Average training loss: 0.01377559314088689\n",
      "Average test loss: 0.0016984515049391323\n",
      "Epoch 63/300\n",
      "Average training loss: 0.01370499384154876\n",
      "Average test loss: 0.0016446701247348555\n",
      "Epoch 64/300\n",
      "Average training loss: 0.013624868764645524\n",
      "Average test loss: 0.0016547520557004545\n",
      "Epoch 65/300\n",
      "Average training loss: 0.013619686727722485\n",
      "Average test loss: 0.0016556814168062475\n",
      "Epoch 66/300\n",
      "Average training loss: 0.013596100011633501\n",
      "Average test loss: 0.0016547931160570846\n",
      "Epoch 67/300\n",
      "Average training loss: 0.013542295292847686\n",
      "Average test loss: 0.0016479730991025767\n",
      "Epoch 68/300\n",
      "Average training loss: 0.013525098701318105\n",
      "Average test loss: 0.0016828699291994175\n",
      "Epoch 69/300\n",
      "Average training loss: 0.013531509127881793\n",
      "Average test loss: 0.001671827534524103\n",
      "Epoch 70/300\n",
      "Average training loss: 0.013422790409790145\n",
      "Average test loss: 0.001711572728637192\n",
      "Epoch 71/300\n",
      "Average training loss: 0.01340446130765809\n",
      "Average test loss: 0.0016295979908770985\n",
      "Epoch 72/300\n",
      "Average training loss: 0.013384499939779441\n",
      "Average test loss: 0.001631330748192138\n",
      "Epoch 73/300\n",
      "Average training loss: 0.013374323253830273\n",
      "Average test loss: 0.0016778096227596203\n",
      "Epoch 74/300\n",
      "Average training loss: 0.01331215623435047\n",
      "Average test loss: 0.001635291900071833\n",
      "Epoch 75/300\n",
      "Average training loss: 0.013310603174070517\n",
      "Average test loss: 0.0016809845085566243\n",
      "Epoch 76/300\n",
      "Average training loss: 0.013275751291877694\n",
      "Average test loss: 0.0017393846021344264\n",
      "Epoch 77/300\n",
      "Average training loss: 0.013229042244454224\n",
      "Average test loss: 0.0016281073719470037\n",
      "Epoch 78/300\n",
      "Average training loss: 0.013206808971861998\n",
      "Average test loss: 0.0016732719600614575\n",
      "Epoch 79/300\n",
      "Average training loss: 0.01321641745004389\n",
      "Average test loss: 0.0016684654624097878\n",
      "Epoch 80/300\n",
      "Average training loss: 0.013133461970422002\n",
      "Average test loss: 0.0018336421638313267\n",
      "Epoch 81/300\n",
      "Average training loss: 0.013152398171524207\n",
      "Average test loss: 0.0017095768493082788\n",
      "Epoch 82/300\n",
      "Average training loss: 0.013109218178523912\n",
      "Average test loss: 0.001653827585486902\n",
      "Epoch 83/300\n",
      "Average training loss: 0.013089854886134466\n",
      "Average test loss: 0.0017813849875496493\n",
      "Epoch 84/300\n",
      "Average training loss: 0.013053257656594118\n",
      "Average test loss: 0.0017890289520016975\n",
      "Epoch 85/300\n",
      "Average training loss: 0.013007974379592472\n",
      "Average test loss: 0.0016345902973165115\n",
      "Epoch 86/300\n",
      "Average training loss: 0.012979392977224455\n",
      "Average test loss: 0.0016555006671696902\n",
      "Epoch 87/300\n",
      "Average training loss: 0.012993456128570769\n",
      "Average test loss: 0.0018850183565583495\n",
      "Epoch 88/300\n",
      "Average training loss: 0.012969577067428165\n",
      "Average test loss: 0.0018139969148776598\n",
      "Epoch 89/300\n",
      "Average training loss: 0.012950193824039564\n",
      "Average test loss: 0.001654858114508291\n",
      "Epoch 90/300\n",
      "Average training loss: 0.012939017771846719\n",
      "Average test loss: 0.001713969577052113\n",
      "Epoch 91/300\n",
      "Average training loss: 0.012879907858040598\n",
      "Average test loss: 0.0016437998552703195\n",
      "Epoch 92/300\n",
      "Average training loss: 0.012886020749807358\n",
      "Average test loss: 0.001643241887808674\n",
      "Epoch 93/300\n",
      "Average training loss: 0.012838250015344884\n",
      "Average test loss: 0.0016808700919565228\n",
      "Epoch 94/300\n",
      "Average training loss: 0.012821639811827077\n",
      "Average test loss: 0.0017569319951451488\n",
      "Epoch 95/300\n",
      "Average training loss: 0.012805117068191369\n",
      "Average test loss: 0.001714030415440599\n",
      "Epoch 96/300\n",
      "Average training loss: 0.013153912370403607\n",
      "Average test loss: 0.00183460733294487\n",
      "Epoch 97/300\n",
      "Average training loss: 0.012755030257834329\n",
      "Average test loss: 0.0017577508631058864\n",
      "Epoch 98/300\n",
      "Average training loss: 0.01273826217982504\n",
      "Average test loss: 0.0019196265082185467\n",
      "Epoch 99/300\n",
      "Average training loss: 0.01272077607942952\n",
      "Average test loss: 0.0017408444531675843\n",
      "Epoch 100/300\n",
      "Average training loss: 0.012729515375362502\n",
      "Average test loss: 0.0016465360984827082\n",
      "Epoch 101/300\n",
      "Average training loss: 0.012681854618092379\n",
      "Average test loss: 0.0016531446984865599\n",
      "Epoch 102/300\n",
      "Average training loss: 0.01267021902733379\n",
      "Average test loss: 0.0017733995529512563\n",
      "Epoch 103/300\n",
      "Average training loss: 0.012649361886084079\n",
      "Average test loss: 0.0016516977700715264\n",
      "Epoch 104/300\n",
      "Average training loss: 0.012660791776246495\n",
      "Average test loss: 0.0016406604074355629\n",
      "Epoch 105/300\n",
      "Average training loss: 0.012618631749517387\n",
      "Average test loss: 0.0019166793131993876\n",
      "Epoch 106/300\n",
      "Average training loss: 0.012587749884360367\n",
      "Average test loss: 0.0017502436882091894\n",
      "Epoch 107/300\n",
      "Average training loss: 0.012607116269568602\n",
      "Average test loss: 0.0023504886767930456\n",
      "Epoch 108/300\n",
      "Average training loss: 0.012568689112034109\n",
      "Average test loss: 0.0018829831019457845\n",
      "Epoch 109/300\n",
      "Average training loss: 0.012543981956938902\n",
      "Average test loss: 0.0016402780411558019\n",
      "Epoch 110/300\n",
      "Average training loss: 0.012541138917207718\n",
      "Average test loss: 0.0020711819734424352\n",
      "Epoch 111/300\n",
      "Average training loss: 0.01252246128105455\n",
      "Average test loss: 0.0017792790193731586\n",
      "Epoch 112/300\n",
      "Average training loss: 0.01252031809091568\n",
      "Average test loss: 0.0017019730057153436\n",
      "Epoch 113/300\n",
      "Average training loss: 0.01249560784548521\n",
      "Average test loss: 0.0016487959891350733\n",
      "Epoch 114/300\n",
      "Average training loss: 0.012510629913873143\n",
      "Average test loss: 0.0021526889733763203\n",
      "Epoch 115/300\n",
      "Average training loss: 0.012470553299619092\n",
      "Average test loss: 0.0017556439029673735\n",
      "Epoch 116/300\n",
      "Average training loss: 0.012461880409055286\n",
      "Average test loss: 0.0017254518549889325\n",
      "Epoch 117/300\n",
      "Average training loss: 0.012430989445911514\n",
      "Average test loss: 0.0017148450290163358\n",
      "Epoch 118/300\n",
      "Average training loss: 0.01241361513485511\n",
      "Average test loss: 0.0017659728086243073\n",
      "Epoch 119/300\n",
      "Average training loss: 0.01242136828849713\n",
      "Average test loss: 0.0017270818767655227\n",
      "Epoch 120/300\n",
      "Average training loss: 0.012398286668790712\n",
      "Average test loss: 0.0016595335299563077\n",
      "Epoch 121/300\n",
      "Average training loss: 0.01237742378645473\n",
      "Average test loss: 0.001789385513919923\n",
      "Epoch 122/300\n",
      "Average training loss: 0.012346293377379577\n",
      "Average test loss: 0.0016981850027417142\n",
      "Epoch 123/300\n",
      "Average training loss: 0.012378784522414208\n",
      "Average test loss: 0.001841594012040231\n",
      "Epoch 124/300\n",
      "Average training loss: 0.012333301449815433\n",
      "Average test loss: 0.0017773324170460303\n",
      "Epoch 125/300\n",
      "Average training loss: 0.012318493083947234\n",
      "Average test loss: 0.0018977945894002916\n",
      "Epoch 126/300\n",
      "Average training loss: 0.012298854458663199\n",
      "Average test loss: 0.0016763149886909459\n",
      "Epoch 127/300\n",
      "Average training loss: 0.012303877364430163\n",
      "Average test loss: 0.0018742345803313785\n",
      "Epoch 128/300\n",
      "Average training loss: 0.012263405392567317\n",
      "Average test loss: 0.001701932041077978\n",
      "Epoch 129/300\n",
      "Average training loss: 0.012283541248076492\n",
      "Average test loss: 0.001895504882145259\n",
      "Epoch 130/300\n",
      "Average training loss: 0.012265137446423371\n",
      "Average test loss: 0.0017644392214715482\n",
      "Epoch 131/300\n",
      "Average training loss: 0.012283513208230337\n",
      "Average test loss: 0.0016918213851749896\n",
      "Epoch 132/300\n",
      "Average training loss: 0.012233190970288382\n",
      "Average test loss: 0.001721989258709881\n",
      "Epoch 133/300\n",
      "Average training loss: 0.012226182493070762\n",
      "Average test loss: 0.0016728142361260123\n",
      "Epoch 134/300\n",
      "Average training loss: 0.012193275704979897\n",
      "Average test loss: 0.0020687862157614694\n",
      "Epoch 135/300\n",
      "Average training loss: 0.012224758745067649\n",
      "Average test loss: 0.0016795455931892826\n",
      "Epoch 136/300\n",
      "Average training loss: 0.012186457238263554\n",
      "Average test loss: 0.0017379490631735986\n",
      "Epoch 137/300\n",
      "Average training loss: 0.012176004798461993\n",
      "Average test loss: 0.0018228602628741\n",
      "Epoch 138/300\n",
      "Average training loss: 0.012172322932630777\n",
      "Average test loss: 0.0019028154611587524\n",
      "Epoch 139/300\n",
      "Average training loss: 0.01213519954846965\n",
      "Average test loss: 0.001740430081056224\n",
      "Epoch 140/300\n",
      "Average training loss: 0.012134731468227175\n",
      "Average test loss: 0.00263227201708489\n",
      "Epoch 141/300\n",
      "Average training loss: 0.01213567156261868\n",
      "Average test loss: 0.0017885507289320231\n",
      "Epoch 142/300\n",
      "Average training loss: 0.012127651416593128\n",
      "Average test loss: 0.0017531464944283168\n",
      "Epoch 143/300\n",
      "Average training loss: 0.01211226213640637\n",
      "Average test loss: 0.001782246727289425\n",
      "Epoch 144/300\n",
      "Average training loss: 0.01208676567673683\n",
      "Average test loss: 0.0017509724603344996\n",
      "Epoch 145/300\n",
      "Average training loss: 0.012103033733864624\n",
      "Average test loss: 0.0017014972505470117\n",
      "Epoch 146/300\n",
      "Average training loss: 0.012129250402251879\n",
      "Average test loss: 0.0017417479515489604\n",
      "Epoch 147/300\n",
      "Average training loss: 0.012062684487137529\n",
      "Average test loss: 0.0017155572792722118\n",
      "Epoch 148/300\n",
      "Average training loss: 0.012068560298118326\n",
      "Average test loss: 0.001751290078792307\n",
      "Epoch 149/300\n",
      "Average training loss: 0.01204671284639173\n",
      "Average test loss: 0.0017018576016028723\n",
      "Epoch 150/300\n",
      "Average training loss: 0.012054594096210267\n",
      "Average test loss: 0.0017324620974767538\n",
      "Epoch 151/300\n",
      "Average training loss: 0.012011014430059327\n",
      "Average test loss: 0.0018523763756578167\n",
      "Epoch 152/300\n",
      "Average training loss: 0.012016561220917437\n",
      "Average test loss: 0.0017316054552793502\n",
      "Epoch 153/300\n",
      "Average training loss: 0.012012599125504494\n",
      "Average test loss: 0.0018832865096628667\n",
      "Epoch 154/300\n",
      "Average training loss: 0.011998156098028024\n",
      "Average test loss: 0.0017408413624184\n",
      "Epoch 155/300\n",
      "Average training loss: 0.012006441372964118\n",
      "Average test loss: 0.001722064677729375\n",
      "Epoch 156/300\n",
      "Average training loss: 0.011982902858820227\n",
      "Average test loss: 0.0017915348594801294\n",
      "Epoch 157/300\n",
      "Average training loss: 0.011970218499501546\n",
      "Average test loss: 0.001807784176017675\n",
      "Epoch 158/300\n",
      "Average training loss: 0.011953769892454147\n",
      "Average test loss: 0.0018033182331257396\n",
      "Epoch 159/300\n",
      "Average training loss: 0.01192876110639837\n",
      "Average test loss: 0.001737953625412451\n",
      "Epoch 160/300\n",
      "Average training loss: 0.011959624802900685\n",
      "Average test loss: 0.0017818120279245906\n",
      "Epoch 161/300\n",
      "Average training loss: 0.011987080309126113\n",
      "Average test loss: 0.0017269376052750482\n",
      "Epoch 162/300\n",
      "Average training loss: 0.011928975719544623\n",
      "Average test loss: 0.001793596656837811\n",
      "Epoch 163/300\n",
      "Average training loss: 0.011933471376697223\n",
      "Average test loss: 0.0017794480670450463\n",
      "Epoch 164/300\n",
      "Average training loss: 0.011898401892019643\n",
      "Average test loss: 0.0017228016172432237\n",
      "Epoch 165/300\n",
      "Average training loss: 0.011896317383481396\n",
      "Average test loss: 0.0017689776498203476\n",
      "Epoch 166/300\n",
      "Average training loss: 0.011907790787518024\n",
      "Average test loss: 0.0017501951814111736\n",
      "Epoch 167/300\n",
      "Average training loss: 0.011870847635798983\n",
      "Average test loss: 0.0017032652467282282\n",
      "Epoch 168/300\n",
      "Average training loss: 0.011867783963680267\n",
      "Average test loss: 0.001889826201316383\n",
      "Epoch 169/300\n",
      "Average training loss: 0.011839276695417034\n",
      "Average test loss: 0.001724745833211475\n",
      "Epoch 170/300\n",
      "Average training loss: 0.011825656230250994\n",
      "Average test loss: 0.0018564503819992145\n",
      "Epoch 171/300\n",
      "Average training loss: 0.011868296270569165\n",
      "Average test loss: 0.0016977262584906485\n",
      "Epoch 172/300\n",
      "Average training loss: 0.011843316912651062\n",
      "Average test loss: 0.0018150990719182623\n",
      "Epoch 173/300\n",
      "Average training loss: 0.011866233051651054\n",
      "Average test loss: 0.0017574069464786186\n",
      "Epoch 174/300\n",
      "Average training loss: 0.011806297703749604\n",
      "Average test loss: 0.0017754750738127364\n",
      "Epoch 175/300\n",
      "Average training loss: 0.01182389148324728\n",
      "Average test loss: 0.001803165743437906\n",
      "Epoch 176/300\n",
      "Average training loss: 0.011821736570861604\n",
      "Average test loss: 0.0017185307105796205\n",
      "Epoch 177/300\n",
      "Average training loss: 0.011797051303916508\n",
      "Average test loss: 0.0018851192286238074\n",
      "Epoch 178/300\n",
      "Average training loss: 0.01181502646787299\n",
      "Average test loss: 0.0017163831827541193\n",
      "Epoch 179/300\n",
      "Average training loss: 0.011793037941058477\n",
      "Average test loss: 0.0017859007279492087\n",
      "Epoch 180/300\n",
      "Average training loss: 0.01178743358535899\n",
      "Average test loss: 0.0017402913915510808\n",
      "Epoch 181/300\n",
      "Average training loss: 0.011776152168711026\n",
      "Average test loss: 0.001793372611825665\n",
      "Epoch 182/300\n",
      "Average training loss: 0.011751882903277874\n",
      "Average test loss: 0.0020193423117614456\n",
      "Epoch 183/300\n",
      "Average training loss: 0.011799827722211678\n",
      "Average test loss: 0.0017225403800192806\n",
      "Epoch 184/300\n",
      "Average training loss: 0.011744180462426609\n",
      "Average test loss: 0.0017670212808168597\n",
      "Epoch 185/300\n",
      "Average training loss: 0.011750764239165518\n",
      "Average test loss: 0.001779594788638254\n",
      "Epoch 186/300\n",
      "Average training loss: 0.011745401593546072\n",
      "Average test loss: 0.0017531160170005428\n",
      "Epoch 187/300\n",
      "Average training loss: 0.01175655660447147\n",
      "Average test loss: 0.0017920498768281606\n",
      "Epoch 188/300\n",
      "Average training loss: 0.011728795671628581\n",
      "Average test loss: 0.0017152746898225612\n",
      "Epoch 189/300\n",
      "Average training loss: 0.011720309130847454\n",
      "Average test loss: 0.0018709974424499605\n",
      "Epoch 190/300\n",
      "Average training loss: 0.011753401193353864\n",
      "Average test loss: 0.0018156535072873035\n",
      "Epoch 191/300\n",
      "Average training loss: 0.011697007901966571\n",
      "Average test loss: 0.0017369440423531666\n",
      "Epoch 192/300\n",
      "Average training loss: 0.011732715296248595\n",
      "Average test loss: 0.0017413518221841917\n",
      "Epoch 193/300\n",
      "Average training loss: 0.011669610855480034\n",
      "Average test loss: 0.0018077613297435972\n",
      "Epoch 194/300\n",
      "Average training loss: 0.011692819455431568\n",
      "Average test loss: 0.001804580726246867\n",
      "Epoch 195/300\n",
      "Average training loss: 0.011686826749808258\n",
      "Average test loss: 0.001948506626403994\n",
      "Epoch 196/300\n",
      "Average training loss: 0.011694396487540669\n",
      "Average test loss: 0.0017727432940155268\n",
      "Epoch 197/300\n",
      "Average training loss: 0.011689759862919648\n",
      "Average test loss: 0.0017543732849881052\n",
      "Epoch 198/300\n",
      "Average training loss: 0.011692800566554069\n",
      "Average test loss: 0.0017487319032144216\n",
      "Epoch 199/300\n",
      "Average training loss: 0.011657246423264345\n",
      "Average test loss: 0.0017429378864665826\n",
      "Epoch 200/300\n",
      "Average training loss: 0.011661167525582843\n",
      "Average test loss: 0.001739408679927389\n",
      "Epoch 201/300\n",
      "Average training loss: 0.011635779686272144\n",
      "Average test loss: 0.0018880144543428389\n",
      "Epoch 202/300\n",
      "Average training loss: 0.011664665162149403\n",
      "Average test loss: 0.0018103625543622508\n",
      "Epoch 203/300\n",
      "Average training loss: 0.011648945045967897\n",
      "Average test loss: 0.0018011331365754208\n",
      "Epoch 204/300\n",
      "Average training loss: 0.011672136464880573\n",
      "Average test loss: 0.001709497402732571\n",
      "Epoch 205/300\n",
      "Average training loss: 0.011627521360913912\n",
      "Average test loss: 0.0017859100264807541\n",
      "Epoch 206/300\n",
      "Average training loss: 0.011597801896433035\n",
      "Average test loss: 0.0018023352600220178\n",
      "Epoch 207/300\n",
      "Average training loss: 0.011625239940153229\n",
      "Average test loss: 0.00177297992259264\n",
      "Epoch 208/300\n",
      "Average training loss: 0.011598479236993526\n",
      "Average test loss: 0.0017642190963961183\n",
      "Epoch 209/300\n",
      "Average training loss: 0.011589488752186298\n",
      "Average test loss: 0.0017600045286946826\n",
      "Epoch 210/300\n",
      "Average training loss: 0.011592890793250666\n",
      "Average test loss: 0.0017861323039978744\n",
      "Epoch 211/300\n",
      "Average training loss: 0.011589834285279115\n",
      "Average test loss: 0.0017661740226257178\n",
      "Epoch 212/300\n",
      "Average training loss: 0.011581722055872282\n",
      "Average test loss: 0.0018980240626260637\n",
      "Epoch 213/300\n",
      "Average training loss: 0.011582968253228399\n",
      "Average test loss: 0.0017283744017283123\n",
      "Epoch 214/300\n",
      "Average training loss: 0.011647479358646604\n",
      "Average test loss: 0.0017662784504807658\n",
      "Epoch 215/300\n",
      "Average training loss: 0.011583854709234502\n",
      "Average test loss: 0.0017506818319153454\n",
      "Epoch 216/300\n",
      "Average training loss: 0.011567545317941241\n",
      "Average test loss: 0.001764280460257497\n",
      "Epoch 217/300\n",
      "Average training loss: 0.011549055667387115\n",
      "Average test loss: 0.0017719967822647756\n",
      "Epoch 218/300\n",
      "Average training loss: 0.011542704278396236\n",
      "Average test loss: 0.001767838445595569\n",
      "Epoch 219/300\n",
      "Average training loss: 0.011557661930720012\n",
      "Average test loss: 0.0017333343357054723\n",
      "Epoch 220/300\n",
      "Average training loss: 0.011541980419721869\n",
      "Average test loss: 0.0017429617883430587\n",
      "Epoch 221/300\n",
      "Average training loss: 0.011508830818865034\n",
      "Average test loss: 0.0017314372654590343\n",
      "Epoch 222/300\n",
      "Average training loss: 0.011548713804119162\n",
      "Average test loss: 0.0017798269972619082\n",
      "Epoch 223/300\n",
      "Average training loss: 0.01155240029344956\n",
      "Average test loss: 0.0017407123706200058\n",
      "Epoch 224/300\n",
      "Average training loss: 0.011561926106611888\n",
      "Average test loss: 0.0017726618368178607\n",
      "Epoch 225/300\n",
      "Average training loss: 0.01153044005152252\n",
      "Average test loss: 0.0017574851666059758\n",
      "Epoch 226/300\n",
      "Average training loss: 0.011504510636958811\n",
      "Average test loss: 0.001798790528335505\n",
      "Epoch 227/300\n",
      "Average training loss: 0.011497536251114474\n",
      "Average test loss: 0.0017468653960774343\n",
      "Epoch 228/300\n",
      "Average training loss: 0.011501309115025731\n",
      "Average test loss: 0.0017388653825554583\n",
      "Epoch 229/300\n",
      "Average training loss: 0.011490530656443702\n",
      "Average test loss: 0.0018627178466154469\n",
      "Epoch 230/300\n",
      "Average training loss: 0.011503001958959633\n",
      "Average test loss: 0.0018145374548104074\n",
      "Epoch 231/300\n",
      "Average training loss: 0.011480822712182998\n",
      "Average test loss: 0.0017684702932213743\n",
      "Epoch 232/300\n",
      "Average training loss: 0.011512998488214281\n",
      "Average test loss: 0.001712163627251155\n",
      "Epoch 233/300\n",
      "Average training loss: 0.01150251485986842\n",
      "Average test loss: 0.0018544042879301641\n",
      "Epoch 234/300\n",
      "Average training loss: 0.011452806754244699\n",
      "Average test loss: 0.002148448636755347\n",
      "Epoch 235/300\n",
      "Average training loss: 0.011460302646789286\n",
      "Average test loss: 0.0018582639808042182\n",
      "Epoch 236/300\n",
      "Average training loss: 0.011453653203116523\n",
      "Average test loss: 0.0017900374718010426\n",
      "Epoch 237/300\n",
      "Average training loss: 0.011477930035856034\n",
      "Average test loss: 0.0018147454626030392\n",
      "Epoch 238/300\n",
      "Average training loss: 0.011477394541932476\n",
      "Average test loss: 0.0017641827340962159\n",
      "Epoch 239/300\n",
      "Average training loss: 0.011463175380395518\n",
      "Average test loss: 0.0018713313437377413\n",
      "Epoch 240/300\n",
      "Average training loss: 0.011438473174141513\n",
      "Average test loss: 0.001821128921583295\n",
      "Epoch 241/300\n",
      "Average training loss: 0.011486701957881451\n",
      "Average test loss: 0.0018259930906610357\n",
      "Epoch 242/300\n",
      "Average training loss: 0.011425419029262331\n",
      "Average test loss: 0.001757094920302431\n",
      "Epoch 243/300\n",
      "Average training loss: 0.011430557326724131\n",
      "Average test loss: 0.0018127978975988096\n",
      "Epoch 244/300\n",
      "Average training loss: 0.011444508374565178\n",
      "Average test loss: 0.001823670648348828\n",
      "Epoch 245/300\n",
      "Average training loss: 0.011420424037509494\n",
      "Average test loss: 0.0018007650694085492\n",
      "Epoch 246/300\n",
      "Average training loss: 0.011445788434810108\n",
      "Average test loss: 0.0018614924804617962\n",
      "Epoch 247/300\n",
      "Average training loss: 0.01142452115068833\n",
      "Average test loss: 0.001744824145299693\n",
      "Epoch 248/300\n",
      "Average training loss: 0.011426546057065327\n",
      "Average test loss: 0.0017602977365669278\n",
      "Epoch 249/300\n",
      "Average training loss: 0.011405469808313582\n",
      "Average test loss: 0.002050310036684904\n",
      "Epoch 250/300\n",
      "Average training loss: 0.011427820058332549\n",
      "Average test loss: 0.0017660074783489108\n",
      "Epoch 251/300\n",
      "Average training loss: 0.01139205864402983\n",
      "Average test loss: 0.0018219595702572001\n",
      "Epoch 252/300\n",
      "Average training loss: 0.01138715707179573\n",
      "Average test loss: 0.0017758324860284727\n",
      "Epoch 253/300\n",
      "Average training loss: 0.011377965264850192\n",
      "Average test loss: 0.001781870633467204\n",
      "Epoch 254/300\n",
      "Average training loss: 0.011378455996927288\n",
      "Average test loss: 0.0018198246830660437\n",
      "Epoch 255/300\n",
      "Average training loss: 0.011401061383386453\n",
      "Average test loss: 0.0018516860263836053\n",
      "Epoch 256/300\n",
      "Average training loss: 0.011352728366023964\n",
      "Average test loss: 0.00178080144058913\n",
      "Epoch 257/300\n",
      "Average training loss: 0.011397533213926687\n",
      "Average test loss: 0.0018951175393950607\n",
      "Epoch 258/300\n",
      "Average training loss: 0.01135068131900496\n",
      "Average test loss: 0.0017386339110218817\n",
      "Epoch 259/300\n",
      "Average training loss: 0.011380031666821904\n",
      "Average test loss: 0.0017669081906788052\n",
      "Epoch 260/300\n",
      "Average training loss: 0.011385960871146785\n",
      "Average test loss: 0.0017381224741952286\n",
      "Epoch 261/300\n",
      "Average training loss: 0.011340193515022596\n",
      "Average test loss: 0.0018630980495363473\n",
      "Epoch 262/300\n",
      "Average training loss: 0.011343766288624869\n",
      "Average test loss: 0.0017892077039513322\n",
      "Epoch 263/300\n",
      "Average training loss: 0.01134742724812693\n",
      "Average test loss: 0.0018277698532781668\n",
      "Epoch 264/300\n",
      "Average training loss: 0.011337610225710604\n",
      "Average test loss: 0.0017743237260729075\n",
      "Epoch 265/300\n",
      "Average training loss: 0.01136431331684192\n",
      "Average test loss: 0.0019463181104511023\n",
      "Epoch 266/300\n",
      "Average training loss: 0.011340498205688264\n",
      "Average test loss: 0.0019873492488016683\n",
      "Epoch 267/300\n",
      "Average training loss: 0.011342408916602532\n",
      "Average test loss: 0.0018433653432875872\n",
      "Epoch 268/300\n",
      "Average training loss: 0.011321596064501338\n",
      "Average test loss: 0.0018098467577559252\n",
      "Epoch 269/300\n",
      "Average training loss: 0.011337194339268738\n",
      "Average test loss: 0.0018476976562912266\n",
      "Epoch 270/300\n",
      "Average training loss: 0.011322490997612476\n",
      "Average test loss: 0.0017984563985632526\n",
      "Epoch 271/300\n",
      "Average training loss: 0.011333600762817595\n",
      "Average test loss: 0.001798358688544896\n",
      "Epoch 272/300\n",
      "Average training loss: 0.011309346314933565\n",
      "Average test loss: 0.0018157476187787122\n",
      "Epoch 273/300\n",
      "Average training loss: 0.011305312352048026\n",
      "Average test loss: 0.0017847326248884201\n",
      "Epoch 274/300\n",
      "Average training loss: 0.011293426610529423\n",
      "Average test loss: 0.0018723751486589512\n",
      "Epoch 275/300\n",
      "Average training loss: 0.011306184883746837\n",
      "Average test loss: 0.0018365819574230246\n",
      "Epoch 276/300\n",
      "Average training loss: 0.011315409288638167\n",
      "Average test loss: 0.0017694129129457805\n",
      "Epoch 277/300\n",
      "Average training loss: 0.011301707697411378\n",
      "Average test loss: 0.0017933490927745071\n",
      "Epoch 278/300\n",
      "Average training loss: 0.011297196956144439\n",
      "Average test loss: 0.0018109911967896753\n",
      "Epoch 279/300\n",
      "Average training loss: 0.011282523882057931\n",
      "Average test loss: 0.0017864375768436326\n",
      "Epoch 280/300\n",
      "Average training loss: 0.011292723276548915\n",
      "Average test loss: 0.0017994955968525675\n",
      "Epoch 281/300\n",
      "Average training loss: 0.01128489297338658\n",
      "Average test loss: 0.0019492221150754227\n",
      "Epoch 282/300\n",
      "Average training loss: 0.011279687257276641\n",
      "Average test loss: 0.0018379540922534134\n",
      "Epoch 283/300\n",
      "Average training loss: 0.01129502431386047\n",
      "Average test loss: 0.0018969596703019406\n",
      "Epoch 284/300\n",
      "Average training loss: 0.01128058959543705\n",
      "Average test loss: 0.0017594250714820293\n",
      "Epoch 285/300\n",
      "Average training loss: 0.011271764882736736\n",
      "Average test loss: 0.0018113060610161888\n",
      "Epoch 286/300\n",
      "Average training loss: 0.011286712139017052\n",
      "Average test loss: 0.0018338421450720893\n",
      "Epoch 287/300\n",
      "Average training loss: 0.011263585141135587\n",
      "Average test loss: 0.0018154708518543178\n",
      "Epoch 288/300\n",
      "Average training loss: 0.011251648337476784\n",
      "Average test loss: 0.0018257317882445123\n",
      "Epoch 289/300\n",
      "Average training loss: 0.011249966169397036\n",
      "Average test loss: 0.0017873562302233444\n",
      "Epoch 290/300\n",
      "Average training loss: 0.011250514319373501\n",
      "Average test loss: 0.00180081195389438\n",
      "Epoch 291/300\n",
      "Average training loss: 0.011277663915521569\n",
      "Average test loss: 0.001810310564107365\n",
      "Epoch 292/300\n",
      "Average training loss: 0.011262672850655185\n",
      "Average test loss: 0.0019133728857462604\n",
      "Epoch 293/300\n",
      "Average training loss: 0.011232292878131071\n",
      "Average test loss: 0.0017624254648884138\n",
      "Epoch 294/300\n",
      "Average training loss: 0.011238994837221171\n",
      "Average test loss: 0.0018713622456416488\n",
      "Epoch 295/300\n",
      "Average training loss: 0.011260567155149248\n",
      "Average test loss: 0.0018348077676362462\n",
      "Epoch 296/300\n",
      "Average training loss: 0.01123796669476562\n",
      "Average test loss: 0.001827168387464351\n",
      "Epoch 297/300\n",
      "Average training loss: 0.011219329658067888\n",
      "Average test loss: 0.0019404442490906352\n",
      "Epoch 298/300\n",
      "Average training loss: 0.011219686675402853\n",
      "Average test loss: 0.0018239507275736994\n",
      "Epoch 299/300\n",
      "Average training loss: 0.01122905268230372\n",
      "Average test loss: 0.001867533065378666\n",
      "Epoch 300/300\n",
      "Average training loss: 0.011223507313264741\n",
      "Average test loss: 0.0020729851296378506\n"
     ]
    }
   ],
   "source": [
    "# 5 Projections\n",
    "num_projections = 5\n",
    "\n",
    "# Folder Path for 5 Projections\n",
    "proj5_path = 'Memory_Residual-Additive_Depth10-.01/5 Projections'\n",
    "\n",
    "gauss_10_proj5_weights, gauss_10_proj5_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj5', display=True)\n",
    "\n",
    "gauss_20_proj5_weights, gauss_20_proj5_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj5', display=True)\n",
    "\n",
    "gauss_30_proj5_weights, gauss_30_proj5_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj5', display=True)\n",
    "\n",
    "gauss_40_proj5_weights, gauss_40_proj5_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj5', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj5_path, gauss_10_proj5_weights, gauss_10_proj5_hist, '10% Sampling')\n",
    "save_progress(proj5_path, gauss_20_proj5_weights, gauss_20_proj5_hist, '20% Sampling')\n",
    "save_progress(proj5_path, gauss_30_proj5_weights, gauss_30_proj5_hist, '30% Sampling')\n",
    "save_progress(proj5_path, gauss_40_proj5_weights, gauss_40_proj5_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef797e2-4551-42c1-a674-872b3a185adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 22.39\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 24.18\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.02\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 25.65\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 26.07\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.13\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.84\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.34\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.24\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.25\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.25\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.04\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.84\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.51\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 24.44\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 27.89\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 29.97\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 31.25\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 31.90\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj5_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj5_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj5_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj5_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj5_psnr = average_PSNR(gauss_10_proj5_model, gauss_10_proj5_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj5_psnr = average_PSNR(gauss_20_proj5_model, gauss_20_proj5_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj5_psnr = average_PSNR(gauss_30_proj5_model, gauss_30_proj5_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj5_psnr = average_PSNR(gauss_40_proj5_model, gauss_40_proj5_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj5_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj5_psnr, f)\n",
    "with open(os.path.join(proj5_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj5_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48d9173-6d15-4ef0-844b-2d0b641a299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.159134792539809\n",
      "Average test loss: 1.3152923698359065\n",
      "Epoch 2/300\n",
      "Average training loss: 2.477279816309611\n",
      "Average test loss: 0.01593133513132731\n",
      "Epoch 3/300\n",
      "Average training loss: 1.8754397872289021\n",
      "Average test loss: 0.012929846155146758\n",
      "Epoch 4/300\n",
      "Average training loss: 1.5318757762908937\n",
      "Average test loss: 0.05299553495645523\n",
      "Epoch 5/300\n",
      "Average training loss: 1.266101944287618\n",
      "Average test loss: 0.01793888136578931\n",
      "Epoch 6/300\n",
      "Average training loss: 1.0786390316221448\n",
      "Average test loss: 0.031747606333759094\n",
      "Epoch 7/300\n",
      "Average training loss: 0.9375770379172431\n",
      "Average test loss: 0.009311059937708907\n",
      "Epoch 8/300\n",
      "Average training loss: 0.8254615059428745\n",
      "Average test loss: 0.010059488225314352\n",
      "Epoch 9/300\n",
      "Average training loss: 0.7320084744029575\n",
      "Average test loss: 0.008659454209109147\n",
      "Epoch 10/300\n",
      "Average training loss: 0.6547607383198208\n",
      "Average test loss: 0.007772641412085957\n",
      "Epoch 11/300\n",
      "Average training loss: 0.5868594967524211\n",
      "Average test loss: 0.022948424274722736\n",
      "Epoch 12/300\n",
      "Average training loss: 0.5280456040435367\n",
      "Average test loss: 0.008837803287638559\n",
      "Epoch 13/300\n",
      "Average training loss: 0.4763466603755951\n",
      "Average test loss: 0.007850738824241692\n",
      "Epoch 14/300\n",
      "Average training loss: 0.43078245973587037\n",
      "Average test loss: 0.007906329574684302\n",
      "Epoch 15/300\n",
      "Average training loss: 0.3877925779819488\n",
      "Average test loss: 0.007238450864122973\n",
      "Epoch 16/300\n",
      "Average training loss: 0.3507989841302236\n",
      "Average test loss: 0.007833442572918204\n",
      "Epoch 17/300\n",
      "Average training loss: 0.31831667669614155\n",
      "Average test loss: 0.008668567657470703\n",
      "Epoch 18/300\n",
      "Average training loss: 0.28755005849732296\n",
      "Average test loss: 0.0063064384661200975\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2615192943149143\n",
      "Average test loss: 0.007498116397195392\n",
      "Epoch 20/300\n",
      "Average training loss: 0.2392576583756341\n",
      "Average test loss: 0.0076862927112314435\n",
      "Epoch 21/300\n",
      "Average training loss: 0.21981297442648146\n",
      "Average test loss: 0.006414002617200216\n",
      "Epoch 22/300\n",
      "Average training loss: 0.2051414920224084\n",
      "Average test loss: 0.007768212716612551\n",
      "Epoch 23/300\n",
      "Average training loss: 0.19111252917183771\n",
      "Average test loss: 0.014917071920302179\n",
      "Epoch 24/300\n",
      "Average training loss: 0.18129005701012083\n",
      "Average test loss: 0.10512115915285217\n",
      "Epoch 25/300\n",
      "Average training loss: 0.17150076683362325\n",
      "Average test loss: 0.005848910556899177\n",
      "Epoch 26/300\n",
      "Average training loss: 0.16296916109985776\n",
      "Average test loss: 0.005740849502798584\n",
      "Epoch 27/300\n",
      "Average training loss: 0.15550754500759972\n",
      "Average test loss: 0.06341035479307175\n",
      "Epoch 28/300\n",
      "Average training loss: 0.14883106296592288\n",
      "Average test loss: 0.025692014324996206\n",
      "Epoch 29/300\n",
      "Average training loss: 0.14429961819118925\n",
      "Average test loss: 0.0069338811962968774\n",
      "Epoch 30/300\n",
      "Average training loss: 0.14028450331423017\n",
      "Average test loss: 0.006635631306303872\n",
      "Epoch 31/300\n",
      "Average training loss: 0.13433142654101055\n",
      "Average test loss: 0.005707646953355935\n",
      "Epoch 32/300\n",
      "Average training loss: 0.1312710090279579\n",
      "Average test loss: 0.02674581502377987\n",
      "Epoch 33/300\n",
      "Average training loss: 0.1276443070438173\n",
      "Average test loss: 0.8416207483311494\n",
      "Epoch 34/300\n",
      "Average training loss: 0.12414115019639334\n",
      "Average test loss: 0.005525282695889473\n",
      "Epoch 35/300\n",
      "Average training loss: 0.12124197722143597\n",
      "Average test loss: 0.00614247096164359\n",
      "Epoch 36/300\n",
      "Average training loss: 0.11790561189254124\n",
      "Average test loss: 0.005627090791033374\n",
      "Epoch 37/300\n",
      "Average training loss: 0.11649083132214016\n",
      "Average test loss: 0.0059659876614395115\n",
      "Epoch 38/300\n",
      "Average training loss: 0.11274194703499477\n",
      "Average test loss: 0.008075791009598309\n",
      "Epoch 39/300\n",
      "Average training loss: 0.11204248644245995\n",
      "Average test loss: 0.005636198969764842\n",
      "Epoch 40/300\n",
      "Average training loss: 0.10947477659251954\n",
      "Average test loss: 0.08840816371970707\n",
      "Epoch 41/300\n",
      "Average training loss: 0.10648318307929569\n",
      "Average test loss: 0.005280817572441366\n",
      "Epoch 42/300\n",
      "Average training loss: 0.10457535215218862\n",
      "Average test loss: 0.006612178894380728\n",
      "Epoch 43/300\n",
      "Average training loss: 0.10235772026909722\n",
      "Average test loss: 0.0052998245192898644\n",
      "Epoch 44/300\n",
      "Average training loss: 0.1007008614341418\n",
      "Average test loss: 0.005404165953811672\n",
      "Epoch 45/300\n",
      "Average training loss: 0.09848960276444753\n",
      "Average test loss: 0.14373260827693674\n",
      "Epoch 46/300\n",
      "Average training loss: 0.10118753719329834\n",
      "Average test loss: 0.005893006507307291\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09622758203082614\n",
      "Average test loss: 0.010774896080295245\n",
      "Epoch 48/300\n",
      "Average training loss: 0.09476044712464014\n",
      "Average test loss: 0.026125960352934068\n",
      "Epoch 49/300\n",
      "Average training loss: 0.09385366458363004\n",
      "Average test loss: 18.94456188117133\n",
      "Epoch 50/300\n",
      "Average training loss: 0.09258032537168927\n",
      "Average test loss: 0.007822191615071562\n",
      "Epoch 51/300\n",
      "Average training loss: 0.09134822101063199\n",
      "Average test loss: 0.005889826730721527\n",
      "Epoch 52/300\n",
      "Average training loss: 0.09009584852059682\n",
      "Average test loss: 0.10950347040593623\n",
      "Epoch 53/300\n",
      "Average training loss: 0.08954338924090068\n",
      "Average test loss: 0.010466936078336504\n",
      "Epoch 54/300\n",
      "Average training loss: 0.2901616244978375\n",
      "Average test loss: 0.006736189001964198\n",
      "Epoch 55/300\n",
      "Average training loss: 0.15871868901782565\n",
      "Average test loss: 0.006296956419944763\n",
      "Epoch 56/300\n",
      "Average training loss: 0.13182517008648978\n",
      "Average test loss: 0.007845556622164117\n",
      "Epoch 57/300\n",
      "Average training loss: 0.12224156904220582\n",
      "Average test loss: 0.0069562911979026264\n",
      "Epoch 58/300\n",
      "Average training loss: 0.11595193392700619\n",
      "Average test loss: 0.005525714048494895\n",
      "Epoch 59/300\n",
      "Average training loss: 0.11134707863463296\n",
      "Average test loss: 0.005372580108129316\n",
      "Epoch 60/300\n",
      "Average training loss: 0.10811959873967701\n",
      "Average test loss: 0.00560472488651673\n",
      "Epoch 61/300\n",
      "Average training loss: 0.10488558037413491\n",
      "Average test loss: 0.00561256959165136\n",
      "Epoch 62/300\n",
      "Average training loss: 0.10165569255749385\n",
      "Average test loss: 0.005223107494413853\n",
      "Epoch 63/300\n",
      "Average training loss: 0.09909086048603058\n",
      "Average test loss: 0.005436237090991603\n",
      "Epoch 64/300\n",
      "Average training loss: 0.0970604055987464\n",
      "Average test loss: 0.005443360635389884\n",
      "Epoch 65/300\n",
      "Average training loss: 0.09575564040078058\n",
      "Average test loss: 0.005260362776203288\n",
      "Epoch 66/300\n",
      "Average training loss: 0.09428964717189471\n",
      "Average test loss: 0.014479263936479886\n",
      "Epoch 67/300\n",
      "Average training loss: 0.09329124845398797\n",
      "Average test loss: 0.005871165003213617\n",
      "Epoch 68/300\n",
      "Average training loss: 0.09238848648468653\n",
      "Average test loss: 0.005181223494311174\n",
      "Epoch 69/300\n",
      "Average training loss: 0.09129047858052783\n",
      "Average test loss: 0.005143312621447775\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09063865712616179\n",
      "Average test loss: 0.008674877470566167\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08972907377613916\n",
      "Average test loss: 0.005411220735973782\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08893286283148659\n",
      "Average test loss: 0.006481262185093429\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08824363773398929\n",
      "Average test loss: 0.12646560236811638\n",
      "Epoch 74/300\n",
      "Average training loss: 0.08758968071805107\n",
      "Average test loss: 50.11669721233845\n",
      "Epoch 75/300\n",
      "Average training loss: 0.08679659237464268\n",
      "Average test loss: 0.0052858115140762595\n",
      "Epoch 76/300\n",
      "Average training loss: 0.08610435113641951\n",
      "Average test loss: 0.0052101550975607504\n",
      "Epoch 77/300\n",
      "Average training loss: 0.08564596345027288\n",
      "Average test loss: 0.016941522788670327\n",
      "Epoch 78/300\n",
      "Average training loss: 0.08468921769989861\n",
      "Average test loss: 0.005738002969986862\n",
      "Epoch 79/300\n",
      "Average training loss: 0.08432923653390673\n",
      "Average test loss: 0.22551296655378408\n",
      "Epoch 80/300\n",
      "Average training loss: 0.08377433700693979\n",
      "Average test loss: 0.005558246483612392\n",
      "Epoch 81/300\n",
      "Average training loss: 0.08303701936536365\n",
      "Average test loss: 0.00598533984977338\n",
      "Epoch 82/300\n",
      "Average training loss: 0.08254388792647256\n",
      "Average test loss: 0.005261466990742419\n",
      "Epoch 83/300\n",
      "Average training loss: 0.08194807663228777\n",
      "Average test loss: 0.005300306484103203\n",
      "Epoch 84/300\n",
      "Average training loss: 0.08143598394261466\n",
      "Average test loss: 0.022548643179237843\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08087278894583384\n",
      "Average test loss: 0.008167625785701805\n",
      "Epoch 86/300\n",
      "Average training loss: 0.08018976456589169\n",
      "Average test loss: 0.013181072771549224\n",
      "Epoch 87/300\n",
      "Average training loss: 0.07988430365257793\n",
      "Average test loss: 0.005298088327878051\n",
      "Epoch 88/300\n",
      "Average training loss: 0.07920943077736431\n",
      "Average test loss: 0.013554857903677556\n",
      "Epoch 89/300\n",
      "Average training loss: 0.07891772456963857\n",
      "Average test loss: 0.0052945106153686845\n",
      "Epoch 90/300\n",
      "Average training loss: 0.07858702043029997\n",
      "Average test loss: 0.005529767062101099\n",
      "Epoch 91/300\n",
      "Average training loss: 0.07821210157208973\n",
      "Average test loss: 0.0052723381486203934\n",
      "Epoch 92/300\n",
      "Average training loss: 0.07758345749643114\n",
      "Average test loss: 0.005534690015431908\n",
      "Epoch 93/300\n",
      "Average training loss: 0.07702054710851776\n",
      "Average test loss: 0.005456598485302594\n",
      "Epoch 94/300\n",
      "Average training loss: 0.07687144406305418\n",
      "Average test loss: 0.005494902267224259\n",
      "Epoch 95/300\n",
      "Average training loss: 0.07647522852155897\n",
      "Average test loss: 0.00722662453353405\n",
      "Epoch 96/300\n",
      "Average training loss: 0.07603018688493304\n",
      "Average test loss: 0.005177526689238018\n",
      "Epoch 97/300\n",
      "Average training loss: 0.0757101975745625\n",
      "Average test loss: 0.0063185332893497415\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07544989768664043\n",
      "Average test loss: 0.005467314576109251\n",
      "Epoch 99/300\n",
      "Average training loss: 0.07519986452658971\n",
      "Average test loss: 0.005481105090429385\n",
      "Epoch 100/300\n",
      "Average training loss: 0.07500941758023368\n",
      "Average test loss: 0.005316427356666989\n",
      "Epoch 101/300\n",
      "Average training loss: 0.07437188002136018\n",
      "Average test loss: 0.00609624452682005\n",
      "Epoch 102/300\n",
      "Average training loss: 0.07425507956080966\n",
      "Average test loss: 0.005683012406445212\n",
      "Epoch 103/300\n",
      "Average training loss: 0.07388637441727851\n",
      "Average test loss: 0.00638810125986735\n",
      "Epoch 104/300\n",
      "Average training loss: 0.07394868633482192\n",
      "Average test loss: 0.005337173834857013\n",
      "Epoch 105/300\n",
      "Average training loss: 962.8718087981013\n",
      "Average test loss: 0.039975097783737715\n",
      "Epoch 106/300\n",
      "Average training loss: 3.2237016128963893\n",
      "Average test loss: 1.4811068861087164\n",
      "Epoch 107/300\n",
      "Average training loss: 2.203675363328722\n",
      "Average test loss: 0.022839083595408333\n",
      "Epoch 108/300\n",
      "Average training loss: 1.7567089306513468\n",
      "Average test loss: 83.60396817989482\n",
      "Epoch 109/300\n",
      "Average training loss: 1.4826676681306628\n",
      "Average test loss: 0.01458878391649988\n",
      "Epoch 110/300\n",
      "Average training loss: 1.282989550060696\n",
      "Average test loss: 0.46511097274058394\n",
      "Epoch 111/300\n",
      "Average training loss: 1.1346178890864054\n",
      "Average test loss: 0.008843156463570066\n",
      "Epoch 112/300\n",
      "Average training loss: 1.0143512627283733\n",
      "Average test loss: 0.019239119404306015\n",
      "Epoch 113/300\n",
      "Average training loss: 0.9096307151582506\n",
      "Average test loss: 0.007383537875695361\n",
      "Epoch 114/300\n",
      "Average training loss: 0.8155005209710863\n",
      "Average test loss: 0.00707277067999045\n",
      "Epoch 115/300\n",
      "Average training loss: 0.7367311147054036\n",
      "Average test loss: 13.018143556667699\n",
      "Epoch 116/300\n",
      "Average training loss: 0.6707283587985569\n",
      "Average test loss: 0.030781040982653697\n",
      "Epoch 117/300\n",
      "Average training loss: 0.6104954078992207\n",
      "Average test loss: 165.56310189940532\n",
      "Epoch 118/300\n",
      "Average training loss: 0.5491857915454441\n",
      "Average test loss: 0.037118315770394275\n",
      "Epoch 119/300\n",
      "Average training loss: 0.4893066898451911\n",
      "Average test loss: 1.3169408393369781\n",
      "Epoch 120/300\n",
      "Average training loss: 0.43647424308458965\n",
      "Average test loss: 0.006707387853827742\n",
      "Epoch 121/300\n",
      "Average training loss: 0.38907630933655635\n",
      "Average test loss: 0.007029630694952276\n",
      "Epoch 122/300\n",
      "Average training loss: 0.34817858809894986\n",
      "Average test loss: 0.020709880057308408\n",
      "Epoch 123/300\n",
      "Average training loss: 0.3108930847114987\n",
      "Average test loss: 0.00773816493484709\n",
      "Epoch 124/300\n",
      "Average training loss: 0.2726842452287674\n",
      "Average test loss: 0.005892125989413924\n",
      "Epoch 125/300\n",
      "Average training loss: 0.23347347033023835\n",
      "Average test loss: 0.005805611549566189\n",
      "Epoch 126/300\n",
      "Average training loss: 0.2028211410442988\n",
      "Average test loss: 0.0062611448644763895\n",
      "Epoch 127/300\n",
      "Average training loss: 0.1851923478576872\n",
      "Average test loss: 0.00570083300024271\n",
      "Epoch 128/300\n",
      "Average training loss: 0.17186309720410242\n",
      "Average test loss: 0.018862172842025758\n",
      "Epoch 129/300\n",
      "Average training loss: 0.16085381364822388\n",
      "Average test loss: 0.005492277676032649\n",
      "Epoch 130/300\n",
      "Average training loss: 0.1529094860818651\n",
      "Average test loss: 0.00547049485395352\n",
      "Epoch 131/300\n",
      "Average training loss: 0.143750459962421\n",
      "Average test loss: 0.29120464696155657\n",
      "Epoch 132/300\n",
      "Average training loss: 0.13756319052643246\n",
      "Average test loss: 0.005645297290964259\n",
      "Epoch 133/300\n",
      "Average training loss: 0.13198190830813514\n",
      "Average test loss: 0.005802179660234186\n",
      "Epoch 134/300\n",
      "Average training loss: 0.125778602540493\n",
      "Average test loss: 0.005424212404423289\n",
      "Epoch 135/300\n",
      "Average training loss: 0.1205412523481581\n",
      "Average test loss: 0.005264717201805777\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11597827156384787\n",
      "Average test loss: 0.005344998639076949\n",
      "Epoch 137/300\n",
      "Average training loss: 0.11065369549062518\n",
      "Average test loss: 0.005141747034672234\n",
      "Epoch 138/300\n",
      "Average training loss: 0.11094957425196965\n",
      "Average test loss: 0.005457116011944082\n",
      "Epoch 139/300\n",
      "Average training loss: 0.10454072933064566\n",
      "Average test loss: 0.005462269652634859\n",
      "Epoch 140/300\n",
      "Average training loss: 0.10102474300728904\n",
      "Average test loss: 0.005193229611135192\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09903115781148275\n",
      "Average test loss: 0.0054985158828397595\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09644905516836379\n",
      "Average test loss: 0.00532433178068863\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09425776463084751\n",
      "Average test loss: 0.008927906325707833\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09997305176655451\n",
      "Average test loss: 0.005284757665875885\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09353408778376049\n",
      "Average test loss: 0.005149058933059374\n",
      "Epoch 146/300\n",
      "Average training loss: 0.09014509328868654\n",
      "Average test loss: 0.005186149061967929\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08911290612485674\n",
      "Average test loss: 0.005759631497164568\n",
      "Epoch 148/300\n",
      "Average training loss: 0.087339248975118\n",
      "Average test loss: 0.005169159128020207\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08608262893888685\n",
      "Average test loss: 0.0057499778891603156\n",
      "Epoch 150/300\n",
      "Average training loss: 0.08476376420259475\n",
      "Average test loss: 0.005175202521185081\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08348388533459769\n",
      "Average test loss: 0.0055217118735114736\n",
      "Epoch 152/300\n",
      "Average training loss: 0.0829001507891549\n",
      "Average test loss: 0.006436991777684953\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08132840226093928\n",
      "Average test loss: 0.005192790805465646\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08063883995347552\n",
      "Average test loss: 0.23264391755395467\n",
      "Epoch 155/300\n",
      "Average training loss: 0.07927513442436854\n",
      "Average test loss: 6.257373132776883\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08067818471458223\n",
      "Average test loss: 0.005798822119418118\n",
      "Epoch 157/300\n",
      "Average training loss: 0.07825399976968765\n",
      "Average test loss: 0.00543871109560132\n",
      "Epoch 158/300\n",
      "Average training loss: 0.07726530845959981\n",
      "Average test loss: 0.005220890488475561\n",
      "Epoch 159/300\n",
      "Average training loss: 0.07673672489325205\n",
      "Average test loss: 0.005454118739813566\n",
      "Epoch 160/300\n",
      "Average training loss: 0.07641833389467663\n",
      "Average test loss: 0.005472090208695994\n",
      "Epoch 161/300\n",
      "Average training loss: 0.07540148418479496\n",
      "Average test loss: 0.10290438913305601\n",
      "Epoch 162/300\n",
      "Average training loss: 0.07516968179411358\n",
      "Average test loss: 0.10033926300373343\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07487831025653416\n",
      "Average test loss: 0.005448776272022062\n",
      "Epoch 164/300\n",
      "Average training loss: 0.07457614652315775\n",
      "Average test loss: 0.005372445752637254\n",
      "Epoch 165/300\n",
      "Average training loss: 0.07413360776503881\n",
      "Average test loss: 0.00539357868851059\n",
      "Epoch 166/300\n",
      "Average training loss: 0.07453834113809797\n",
      "Average test loss: 0.005376413887573613\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07322849974367354\n",
      "Average test loss: 0.00532917419117358\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07305784434411261\n",
      "Average test loss: 0.005390997178024715\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07259246020184623\n",
      "Average test loss: 0.0054127540489037835\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07228315065966712\n",
      "Average test loss: 0.005502020662029585\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07216980053650009\n",
      "Average test loss: 0.006289407428767946\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07182552860180537\n",
      "Average test loss: 0.01791924730771118\n",
      "Epoch 173/300\n",
      "Average training loss: 0.07174791611234348\n",
      "Average test loss: 0.050250617795520355\n",
      "Epoch 174/300\n",
      "Average training loss: 0.07139014575878778\n",
      "Average test loss: 0.0058302916002770265\n",
      "Epoch 175/300\n",
      "Average training loss: 0.07104161245955361\n",
      "Average test loss: 0.005512987441072861\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0707867371638616\n",
      "Average test loss: 0.005599748245129983\n",
      "Epoch 177/300\n",
      "Average training loss: 0.0709621520837148\n",
      "Average test loss: 0.005495296304424604\n",
      "Epoch 178/300\n",
      "Average training loss: 6.226961746096611\n",
      "Average test loss: 0.008748198010855251\n",
      "Epoch 179/300\n",
      "Average training loss: 1.0280594484541152\n",
      "Average test loss: 1.1496653742703298\n",
      "Epoch 180/300\n",
      "Average training loss: 0.6845879641638862\n",
      "Average test loss: 0.03819286107396086\n",
      "Epoch 181/300\n",
      "Average training loss: 0.5043668074607849\n",
      "Average test loss: 54.5661487126814\n",
      "Epoch 182/300\n",
      "Average training loss: 0.3851950467162662\n",
      "Average test loss: 37979.57124902344\n",
      "Epoch 183/300\n",
      "Average training loss: 0.3045181735621558\n",
      "Average test loss: 54.12204977486862\n",
      "Epoch 184/300\n",
      "Average training loss: 0.24768487045500012\n",
      "Average test loss: 24.977747026433548\n",
      "Epoch 185/300\n",
      "Average training loss: 0.20993226696385278\n",
      "Average test loss: 5360.338791002976\n",
      "Epoch 186/300\n",
      "Average training loss: 0.18340727468331655\n",
      "Average test loss: 12029.963091457483\n",
      "Epoch 187/300\n",
      "Average training loss: 0.16417030435138277\n",
      "Average test loss: 2742.5716890916046\n",
      "Epoch 188/300\n",
      "Average training loss: 0.150984243730704\n",
      "Average test loss: 2.143565295416448\n",
      "Epoch 189/300\n",
      "Average training loss: 0.13826565555731454\n",
      "Average test loss: 0.021849377781980567\n",
      "Epoch 190/300\n",
      "Average training loss: 0.12845777550670837\n",
      "Average test loss: 0.005779204696416855\n",
      "Epoch 191/300\n",
      "Average training loss: 0.12062051476372612\n",
      "Average test loss: 0.007517094345556365\n",
      "Epoch 192/300\n",
      "Average training loss: 0.11442519210444556\n",
      "Average test loss: 0.006054792802366945\n",
      "Epoch 193/300\n",
      "Average training loss: 0.10945332735776901\n",
      "Average test loss: 0.01925782786640856\n",
      "Epoch 194/300\n",
      "Average training loss: 0.10549532339307997\n",
      "Average test loss: 0.01007152352316512\n",
      "Epoch 195/300\n",
      "Average training loss: 0.1020086674756474\n",
      "Average test loss: 0.005373596468319496\n",
      "Epoch 196/300\n",
      "Average training loss: 0.10036615229977502\n",
      "Average test loss: 0.005172039346976413\n",
      "Epoch 197/300\n",
      "Average training loss: 0.09623188986380896\n",
      "Average test loss: 0.005224020180809829\n",
      "Epoch 198/300\n",
      "Average training loss: 0.09399756575955286\n",
      "Average test loss: 0.005545749846018023\n",
      "Epoch 199/300\n",
      "Average training loss: 0.09177375348409017\n",
      "Average test loss: 0.005198634579777718\n",
      "Epoch 200/300\n",
      "Average training loss: 0.09093357398774889\n",
      "Average test loss: 0.005280508057524761\n",
      "Epoch 201/300\n",
      "Average training loss: 0.08756027950843175\n",
      "Average test loss: 0.005219769902941254\n",
      "Epoch 202/300\n",
      "Average training loss: 0.08596156992846066\n",
      "Average test loss: 0.005357243674496809\n",
      "Epoch 203/300\n",
      "Average training loss: 0.08238344291183683\n",
      "Average test loss: 0.005307420446640915\n",
      "Epoch 205/300\n",
      "Average training loss: 0.09504998621013429\n",
      "Average test loss: 0.00522875040397048\n",
      "Epoch 206/300\n",
      "Average training loss: 0.08121749915017022\n",
      "Average test loss: 0.005437679715454578\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07563137796852323\n",
      "Average test loss: 0.005511759125524097\n",
      "Epoch 210/300\n",
      "Average training loss: 0.07439020276731916\n",
      "Average test loss: 0.007964083997739686\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07377212890651491\n",
      "Average test loss: 0.005710260690086418\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07292507518662347\n",
      "Average test loss: 0.005333592005487945\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07231044926577145\n",
      "Average test loss: 0.00536850600130856\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07183659264776442\n",
      "Average test loss: 0.005409430652442906\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07157972719934251\n",
      "Average test loss: 0.005323380465308825\n",
      "Epoch 216/300\n",
      "Average training loss: 0.07148323825995127\n",
      "Average test loss: 0.005405041825647156\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07069562994771533\n",
      "Average test loss: 0.006114820669508643\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07037221546967824\n",
      "Average test loss: 0.0056148792550795605\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07047558991776573\n",
      "Average test loss: 0.00794107454849614\n",
      "Epoch 220/300\n",
      "Average training loss: 0.11686980524990294\n",
      "Average test loss: 0.006215772980617152\n",
      "Epoch 222/300\n",
      "Average training loss: 0.1148880542450481\n",
      "Average test loss: 0.006010675301982297\n",
      "Epoch 223/300\n",
      "Average training loss: 0.09545515708790885\n",
      "Average test loss: 0.005221531384728021\n",
      "Epoch 224/300\n",
      "Average training loss: 0.08877756199571822\n",
      "Average test loss: 0.005177855451073911\n",
      "Epoch 225/300\n",
      "Average training loss: 0.0838152604367998\n",
      "Average test loss: 0.005375856260872549\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07997126881281535\n",
      "Average test loss: 0.0073823875395788085\n",
      "Epoch 227/300\n",
      "Average training loss: 0.0767493030362659\n",
      "Average test loss: 0.005202232266879744\n",
      "Epoch 228/300\n",
      "Average training loss: 0.07436680191424158\n",
      "Average test loss: 0.00704560139361355\n",
      "Epoch 229/300\n",
      "Average training loss: 0.0732765068312486\n",
      "Average test loss: 0.009611384089622232\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07208469481600656\n",
      "Average test loss: 0.005399254199117422\n",
      "Epoch 231/300\n",
      "Average training loss: 0.0711517793138822\n",
      "Average test loss: 0.005367939648321933\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07011545957459343\n",
      "Average test loss: 0.005467665631531013\n",
      "Epoch 233/300\n",
      "Average training loss: 0.070072433223327\n",
      "Average test loss: 0.0058261845625109145\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07012303872903188\n",
      "Average test loss: 0.0055299229195548425\n",
      "Epoch 235/300\n",
      "Average training loss: 0.06949411572681533\n",
      "Average test loss: 0.005727183123015695\n",
      "Epoch 236/300\n",
      "Average training loss: 0.0690672608282831\n",
      "Average test loss: 0.013004465576675204\n",
      "Epoch 237/300\n",
      "Average training loss: 0.06921689247422748\n",
      "Average test loss: 0.008824543238100079\n",
      "Epoch 238/300\n",
      "Average training loss: 0.06879519057273865\n",
      "Average test loss: 0.005583443346536821\n",
      "Epoch 239/300\n",
      "Average training loss: 0.06876743750108613\n",
      "Average test loss: 0.006179390160780814\n",
      "Epoch 240/300\n",
      "Average training loss: 0.06835116076469422\n",
      "Average test loss: 0.013683904740545485\n",
      "Epoch 241/300\n",
      "Average training loss: 0.06823914264970356\n",
      "Average test loss: 0.005457629877660009\n",
      "Epoch 242/300\n",
      "Average training loss: 0.06825517787535985\n",
      "Average test loss: 0.029629852121902837\n",
      "Epoch 243/300\n",
      "Average training loss: 0.07282505332099067\n",
      "Average test loss: 0.006088495756189028\n",
      "Epoch 244/300\n",
      "Average training loss: 19.76457721796963\n",
      "Average test loss: 695.1490764558348\n",
      "Epoch 245/300\n",
      "Average training loss: 1.8240170702404446\n",
      "Average test loss: 20.425933606543474\n",
      "Epoch 246/300\n",
      "Average training loss: 1.2806033104790582\n",
      "Average test loss: 4.772721550381018\n",
      "Epoch 247/300\n",
      "Average training loss: 1.0603745271894667\n",
      "Average test loss: 43.12990098737346\n",
      "Epoch 248/300\n",
      "Average training loss: 0.915069876882765\n",
      "Average test loss: 24326.22423478736\n",
      "Epoch 249/300\n",
      "Average training loss: 0.7955833682484097\n",
      "Average test loss: 0.006681827135384083\n",
      "Epoch 250/300\n",
      "Average training loss: 0.6922957797580295\n",
      "Average test loss: 32.185183656591505\n",
      "Epoch 251/300\n",
      "Average training loss: 0.5997283398840163\n",
      "Average test loss: 2.2897388513386248\n",
      "Epoch 252/300\n",
      "Average training loss: 0.5168497233920627\n",
      "Average test loss: 0.15779981356362502\n",
      "Epoch 253/300\n",
      "Average training loss: 0.44259037929111056\n",
      "Average test loss: 20.232251112417213\n",
      "Epoch 254/300\n",
      "Average training loss: 0.3758819622198741\n",
      "Average test loss: 858.4502556797729\n",
      "Epoch 255/300\n",
      "Average training loss: 0.3136825413439009\n",
      "Average test loss: 0.02341287293160955\n",
      "Epoch 256/300\n",
      "Average training loss: 0.2629002950191498\n",
      "Average test loss: 0.006278133231939541\n",
      "Epoch 257/300\n",
      "Average training loss: 0.22054693665769365\n",
      "Average test loss: 12.037585887951984\n",
      "Epoch 258/300\n",
      "Average training loss: 0.18816326716211107\n",
      "Average test loss: 1.2788779098590215\n",
      "Epoch 259/300\n",
      "Average training loss: 0.1641589621835285\n",
      "Average test loss: 92277.35106182183\n",
      "Epoch 260/300\n",
      "Average training loss: 0.1326809822983212\n",
      "Average test loss: 0.005543710637009806\n",
      "Epoch 262/300\n",
      "Average training loss: 0.12400066526730855\n",
      "Average test loss: 0.00637990871278776\n",
      "Epoch 263/300\n",
      "Average training loss: 0.11807894065645005\n",
      "Average test loss: 0.005272746721489562\n",
      "Epoch 264/300\n",
      "Average training loss: 0.11138829892873764\n",
      "Average test loss: 0.005662994134757254\n",
      "Epoch 265/300\n",
      "Average training loss: 0.10697442004415723\n",
      "Average test loss: 0.005286870616591639\n",
      "Epoch 266/300\n",
      "Average training loss: 0.10378865263859431\n",
      "Average test loss: 0.005942392985853884\n",
      "Epoch 267/300\n",
      "Average training loss: 0.10023579527272118\n",
      "Average test loss: 0.011113505767451392\n",
      "Epoch 268/300\n",
      "Average training loss: 0.09715481270021863\n",
      "Average test loss: 0.024784258322583304\n",
      "Epoch 269/300\n",
      "Average training loss: 0.10864887118339539\n",
      "Average test loss: 0.0052891134578320715\n",
      "Epoch 270/300\n",
      "Average training loss: 0.09384867617819044\n",
      "Average test loss: 0.005635286155674193\n",
      "Epoch 271/300\n",
      "Average training loss: 0.09106464529037475\n",
      "Average test loss: 0.005230127308103774\n",
      "Epoch 272/300\n",
      "Average training loss: 0.08882304994927512\n",
      "Average test loss: 0.006038581289764908\n",
      "Epoch 273/300\n",
      "Average training loss: 0.08698257768816418\n",
      "Average test loss: 0.00527862038794491\n",
      "Epoch 274/300\n",
      "Average training loss: 0.08507109108236101\n",
      "Average test loss: 0.005255765743967559\n",
      "Epoch 275/300\n",
      "Average training loss: 0.08345047407680088\n",
      "Average test loss: 0.005316506076190206\n",
      "Epoch 276/300\n",
      "Average training loss: 0.08023742194970448\n",
      "Average test loss: 0.00532523558412989\n",
      "Epoch 278/300\n",
      "Average training loss: 0.07895409717162451\n",
      "Average test loss: 0.009997648155523672\n",
      "Epoch 279/300\n",
      "Average training loss: 0.07864641435278787\n",
      "Average test loss: 0.0056481684181425304\n",
      "Epoch 280/300\n",
      "Average training loss: 0.07638476252555847\n",
      "Average test loss: 0.005442339573883348\n",
      "Epoch 281/300\n",
      "Average training loss: 0.07584590780072742\n",
      "Average test loss: 0.010490214697188801\n",
      "Epoch 282/300\n",
      "Average training loss: 0.07440627248419655\n",
      "Average test loss: 0.009893183319932884\n",
      "Epoch 283/300\n",
      "Average training loss: 0.07344735555516349\n",
      "Average test loss: 0.005356495525274012\n",
      "Epoch 284/300\n",
      "Average training loss: 0.07250441365771823\n",
      "Average test loss: 0.006350074760615825\n",
      "Epoch 285/300\n",
      "Average training loss: 0.0719756446149614\n",
      "Average test loss: 0.005902964652412468\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0710814199646314\n",
      "Average test loss: 0.005408165030595329\n",
      "Epoch 287/300\n",
      "Average training loss: 0.07133889873160257\n",
      "Average test loss: 0.005338922711296214\n",
      "Epoch 288/300\n",
      "Average training loss: 0.07017726216051313\n",
      "Average test loss: 0.005886578511860635\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06978295949432585\n",
      "Average test loss: 0.007270112084845702\n",
      "Epoch 291/300\n",
      "Average training loss: 0.06934402777089013\n",
      "Average test loss: 0.0054947330502586236\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06903936466574669\n",
      "Average test loss: 0.00552862709057\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06884633174207476\n",
      "Average test loss: 0.0060148036041193535\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06833317920234468\n",
      "Average test loss: 0.005628869541817241\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06789852813879649\n",
      "Average test loss: 0.006793608321083917\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06776023235585954\n",
      "Average test loss: 0.005499316436549028\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0676105507016182\n",
      "Average test loss: 0.005634755535672108\n",
      "Epoch 300/300\n",
      "Average training loss: 0.08274150443077087\n",
      "Average test loss: 0.0055220044760240445\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.897254934098985\n",
      "Average test loss: 1.9859398562908173\n",
      "Epoch 2/300\n",
      "Average training loss: 2.1468723966810437\n",
      "Average test loss: 0.01124879080719418\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4820193371242947\n",
      "Average test loss: 0.1523799809217453\n",
      "Epoch 4/300\n",
      "Average training loss: 1.1994216124216714\n",
      "Average test loss: 0.006644878663950496\n",
      "Epoch 5/300\n",
      "Average training loss: 1.009711018403371\n",
      "Average test loss: 0.006266959988408619\n",
      "Epoch 6/300\n",
      "Average training loss: 0.8686606451670329\n",
      "Average test loss: 0.005934807562993632\n",
      "Epoch 7/300\n",
      "Average training loss: 0.7627170371479458\n",
      "Average test loss: 0.006630417102326949\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6767976583374871\n",
      "Average test loss: 0.0053552776484025845\n",
      "Epoch 9/300\n",
      "Average training loss: 0.6036655739148458\n",
      "Average test loss: 0.006651477951142523\n",
      "Epoch 10/300\n",
      "Average training loss: 0.5396340423160129\n",
      "Average test loss: 0.0049551911569303934\n",
      "Epoch 11/300\n",
      "Average training loss: 0.48278698330455355\n",
      "Average test loss: 0.014434557490878635\n",
      "Epoch 12/300\n",
      "Average training loss: 0.34355851165453594\n",
      "Average test loss: 0.00433599905607601\n",
      "Epoch 15/300\n",
      "Average training loss: 0.30656160333421495\n",
      "Average test loss: 0.004323705578843752\n",
      "Epoch 16/300\n",
      "Average training loss: 0.273826488799519\n",
      "Average test loss: 0.00435671084250013\n",
      "Epoch 17/300\n",
      "Average training loss: 0.24502778204282125\n",
      "Average test loss: 0.004155809344930781\n",
      "Epoch 18/300\n",
      "Average training loss: 0.22154308376047346\n",
      "Average test loss: 0.004060416373320752\n",
      "Epoch 19/300\n",
      "Average training loss: 0.20097651008764902\n",
      "Average test loss: 0.00412883702541391\n",
      "Epoch 20/300\n",
      "Average training loss: 0.18280258462164137\n",
      "Average test loss: 0.0037466152529749604\n",
      "Epoch 21/300\n",
      "Average training loss: 0.1683259057071474\n",
      "Average test loss: 0.006580603650874562\n",
      "Epoch 22/300\n",
      "Average training loss: 0.15712180927064684\n",
      "Average test loss: 0.00356230384318365\n",
      "Epoch 23/300\n",
      "Average training loss: 0.14529610737164816\n",
      "Average test loss: 0.015549710190958447\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1364332590368059\n",
      "Average test loss: 0.007220681494308843\n",
      "Epoch 25/300\n",
      "Average training loss: 0.12808150237136418\n",
      "Average test loss: 0.003374455380253494\n",
      "Epoch 26/300\n",
      "Average training loss: 0.12140044376585218\n",
      "Average test loss: 0.011163316468811697\n",
      "Epoch 27/300\n",
      "Average training loss: 0.10316882727543512\n",
      "Average test loss: 0.003272027213540342\n",
      "Epoch 30/300\n",
      "Average training loss: 0.09881100373135673\n",
      "Average test loss: 0.0035229240196446576\n",
      "Epoch 31/300\n",
      "Average training loss: 0.09415353223350313\n",
      "Average test loss: 0.13359820449497137\n",
      "Epoch 32/300\n",
      "Average training loss: 0.09058037641975615\n",
      "Average test loss: 0.003566077949686183\n",
      "Epoch 33/300\n",
      "Average training loss: 0.08769156725539101\n",
      "Average test loss: 0.6296951125760873\n",
      "Epoch 34/300\n",
      "Average training loss: 0.08461273649003771\n",
      "Average test loss: 0.0033450651841445103\n",
      "Epoch 35/300\n",
      "Average training loss: 0.08137483145793278\n",
      "Average test loss: 0.003116477459255192\n",
      "Epoch 36/300\n",
      "Average training loss: 0.07927161761124929\n",
      "Average test loss: 0.003078221263984839\n",
      "Epoch 37/300\n",
      "Average training loss: 0.07676283687353135\n",
      "Average test loss: 0.003134576935528053\n",
      "Epoch 38/300\n",
      "Average training loss: 0.07430304194821252\n",
      "Average test loss: 0.003099846474826336\n",
      "Epoch 39/300\n",
      "Average training loss: 0.07211775547266007\n",
      "Average test loss: 0.003908405538648367\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06999865885906749\n",
      "Average test loss: 0.0031054932535108594\n",
      "Epoch 41/300\n",
      "Average training loss: 0.06831931785411305\n",
      "Average test loss: 0.16639789943065908\n",
      "Epoch 42/300\n",
      "Average training loss: 0.06674804651406076\n",
      "Average test loss: 0.003941802379157808\n",
      "Epoch 43/300\n",
      "Average training loss: 0.06671939704153274\n",
      "Average test loss: 0.0033956145983603265\n",
      "Epoch 46/300\n",
      "Average training loss: 0.19573565335406198\n",
      "Average test loss: 0.0037911671793295276\n",
      "Epoch 47/300\n",
      "Average training loss: 0.09390364071395663\n",
      "Average test loss: 0.003865712528841363\n",
      "Epoch 48/300\n",
      "Average training loss: 0.08062475734949112\n",
      "Average test loss: 0.0032171914550579257\n",
      "Epoch 49/300\n",
      "Average training loss: 0.07555270605948236\n",
      "Average test loss: 0.003128854098626309\n",
      "Epoch 50/300\n",
      "Average training loss: 0.07252032233609093\n",
      "Average test loss: 0.003354205296271377\n",
      "Epoch 51/300\n",
      "Average training loss: 0.06963460287120607\n",
      "Average test loss: 0.004501300886066423\n",
      "Epoch 52/300\n",
      "Average training loss: 0.06752315798732969\n",
      "Average test loss: 0.005536918654623958\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0662309515244431\n",
      "Average test loss: 0.003119808853086498\n",
      "Epoch 54/300\n",
      "Average training loss: 0.06474969779782826\n",
      "Average test loss: 0.005057825092640188\n",
      "Epoch 55/300\n",
      "Average training loss: 0.06344344572557344\n",
      "Average test loss: 0.003620382390709387\n",
      "Epoch 56/300\n",
      "Average training loss: 0.062474125279320614\n",
      "Average test loss: 0.003113621081949936\n",
      "Epoch 57/300\n",
      "Average training loss: 0.060768443600998985\n",
      "Average test loss: 0.0034030535125897992\n",
      "Epoch 59/300\n",
      "Average training loss: 0.060141664011610876\n",
      "Average test loss: 0.0030244224996616442\n",
      "Epoch 60/300\n",
      "Average training loss: 0.05936380812525749\n",
      "Average test loss: 0.006323012359440327\n",
      "Epoch 61/300\n",
      "Average training loss: 0.05887463657723533\n",
      "Average test loss: 0.003051094743733605\n",
      "Epoch 62/300\n",
      "Average training loss: 0.058175202475653755\n",
      "Average test loss: 0.00409689004222552\n",
      "Epoch 63/300\n",
      "Average training loss: 0.0577984132402473\n",
      "Average test loss: 0.0029065042034619386\n",
      "Epoch 64/300\n",
      "Average training loss: 0.1276756582889292\n",
      "Average test loss: 0.0034027525708079337\n",
      "Epoch 65/300\n",
      "Average training loss: 0.07942021030187607\n",
      "Average test loss: 0.003935734283592966\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07062239630354775\n",
      "Average test loss: 0.0030274584988753002\n",
      "Epoch 67/300\n",
      "Average training loss: 0.06642325490050846\n",
      "Average test loss: 0.0030205950723547073\n",
      "Epoch 68/300\n",
      "Average training loss: 0.06358495156301393\n",
      "Average test loss: 0.003101168250044187\n",
      "Epoch 69/300\n",
      "Average training loss: 0.062088038716051314\n",
      "Average test loss: 0.0030254460865010817\n",
      "Epoch 70/300\n",
      "Average training loss: 0.05987256235215399\n",
      "Average test loss: 0.0050922838168011774\n",
      "Epoch 72/300\n",
      "Average training loss: 0.05881616736451785\n",
      "Average test loss: 0.0031204468253999947\n",
      "Epoch 73/300\n",
      "Average training loss: 0.05828340498275227\n",
      "Average test loss: 0.003024027071479294\n",
      "Epoch 74/300\n",
      "Average training loss: 0.05810332629084587\n",
      "Average test loss: 0.0029111466389149425\n",
      "Epoch 75/300\n",
      "Average training loss: 0.057122970511515936\n",
      "Average test loss: 0.0030394482298029793\n",
      "Epoch 76/300\n",
      "Average training loss: 0.05650101602408621\n",
      "Average test loss: 0.424181899247898\n",
      "Epoch 77/300\n",
      "Average training loss: 0.05609217715925641\n",
      "Average test loss: 1.1560465886915723\n",
      "Epoch 78/300\n",
      "Average training loss: 0.05566752487421036\n",
      "Average test loss: 0.007271861131820414\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05516467249062326\n",
      "Average test loss: 0.07741755762033993\n",
      "Epoch 80/300\n",
      "Average training loss: 0.05476689154571957\n",
      "Average test loss: 0.22544408041487138\n",
      "Epoch 81/300\n",
      "Average training loss: 0.054219188806083465\n",
      "Average test loss: 0.0033919963565551573\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05401431439651383\n",
      "Average test loss: 0.003016200991968314\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05483104339904255\n",
      "Average test loss: 0.00437051357411676\n",
      "Epoch 84/300\n",
      "Average training loss: 0.17158303179343543\n",
      "Average test loss: 342983181.05981016\n",
      "Epoch 85/300\n",
      "Average training loss: 0.08926086378759808\n",
      "Average test loss: 7176431.9604195235\n",
      "Epoch 86/300\n",
      "Average training loss: 0.07484052798483107\n",
      "Average test loss: 259.33795123621405\n",
      "Epoch 87/300\n",
      "Average training loss: 0.061941628959443835\n",
      "Average test loss: 0.0078093935985946\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06010446899135907\n",
      "Average test loss: 0.0030524362785120805\n",
      "Epoch 91/300\n",
      "Average training loss: 0.058525749014483555\n",
      "Average test loss: 0.0029876460087382133\n",
      "Epoch 92/300\n",
      "Average training loss: 0.05754917753736178\n",
      "Average test loss: 4298807719029.3145\n",
      "Epoch 93/300\n",
      "Average training loss: 0.057815162277883955\n",
      "Average test loss: 0.0030266497263477907\n",
      "Epoch 94/300\n",
      "Average training loss: 0.05568660838405291\n",
      "Average test loss: 0.002893037458260854\n",
      "Epoch 95/300\n",
      "Average training loss: 0.05492721223831177\n",
      "Average test loss: 0.007971918068826198\n",
      "Epoch 96/300\n",
      "Average training loss: 0.055446672899855505\n",
      "Average test loss: 0.002990050221896834\n",
      "Epoch 97/300\n",
      "Average training loss: 0.05427016184065077\n",
      "Average test loss: 0.006563419291542636\n",
      "Epoch 98/300\n",
      "Average training loss: 0.05365468322899607\n",
      "Average test loss: 0.00292768666603499\n",
      "Epoch 99/300\n",
      "Average training loss: 0.053229724102550086\n",
      "Average test loss: 0.0029594202244447336\n",
      "Epoch 100/300\n",
      "Average test loss: 0.0035199459460046557\n",
      "Epoch 102/300\n",
      "Average training loss: 0.05226865321397781\n",
      "Average test loss: 0.0029241130051927433\n",
      "Epoch 103/300\n",
      "Average training loss: 0.052038487248950535\n",
      "Average test loss: 0.00362269028265857\n",
      "Epoch 104/300\n",
      "Average training loss: 0.051731166160768935\n",
      "Average test loss: 0.0035359424245026377\n",
      "Epoch 105/300\n",
      "Average training loss: 0.05158243772056368\n",
      "Average test loss: 0.008398782776461708\n",
      "Epoch 106/300\n",
      "Average training loss: 0.051170720828904045\n",
      "Average test loss: 0.002989263702183962\n",
      "Epoch 107/300\n",
      "Average training loss: 0.05087195829219288\n",
      "Average test loss: 0.021212010012732613\n",
      "Epoch 108/300\n",
      "Average training loss: 0.050664413899183275\n",
      "Average test loss: 0.005468043120784892\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05045117184519768\n",
      "Average test loss: 0.003214383605039782\n",
      "Epoch 110/300\n",
      "Average training loss: 0.050152365346749626\n",
      "Average test loss: 0.003078159560759862\n",
      "Epoch 111/300\n",
      "Average training loss: 0.05514058127005895\n",
      "Average test loss: 0.0031291708143221008\n",
      "Epoch 112/300\n",
      "Average training loss: 0.1421438895530171\n",
      "Average test loss: 327.4335170143892\n",
      "Epoch 113/300\n",
      "Average training loss: 0.08036650203996235\n",
      "Average test loss: 0.0029954674577133524\n",
      "Epoch 114/300\n",
      "Average training loss: 0.06796210642655691\n",
      "Average test loss: 0.002945554287483295\n",
      "Epoch 115/300\n",
      "Average training loss: 0.05869232874446445\n",
      "Average test loss: 0.002912924039694998\n",
      "Epoch 117/300\n",
      "Average training loss: 0.05639322730236583\n",
      "Average test loss: 0.003312662238255143\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05461335758037037\n",
      "Average test loss: 0.0030848614771126046\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05314346366127332\n",
      "Average test loss: 0.0031475956663489344\n",
      "Epoch 120/300\n",
      "Average training loss: 0.052109701613585156\n",
      "Average test loss: 0.0030003043487668037\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05118963556488355\n",
      "Average test loss: 0.0030616225144929356\n",
      "Epoch 122/300\n",
      "Average training loss: 0.05052801855074035\n",
      "Average test loss: 0.0029866409223112795\n",
      "Epoch 123/300\n",
      "Average training loss: 0.05040334678689639\n",
      "Average test loss: 0.002935143455862999\n",
      "Epoch 124/300\n",
      "Average training loss: 0.04971441193090545\n",
      "Average test loss: 0.0029634901189969647\n",
      "Epoch 125/300\n",
      "Average training loss: 0.049467227429151536\n",
      "Average test loss: 0.0033530323254979317\n",
      "Epoch 126/300\n",
      "Average training loss: 0.049138971633381316\n",
      "Average test loss: 0.0029390460501114526\n",
      "Epoch 127/300\n",
      "Average training loss: 0.049032025668356155\n",
      "Average test loss: 0.0029731453942755857\n",
      "Epoch 128/300\n",
      "Average training loss: 0.0487909422814846\n",
      "Average test loss: 0.0030021497851444616\n",
      "Epoch 129/300\n",
      "Average training loss: 0.04856545316179593\n",
      "Average test loss: 0.0030441338604109153\n",
      "Epoch 130/300\n",
      "Average training loss: 0.048338337073723474\n",
      "Average test loss: 0.0033621610113316112\n",
      "Epoch 131/300\n",
      "Average training loss: 0.04824186462826199\n",
      "Average test loss: 0.0030036410823878314\n",
      "Epoch 132/300\n",
      "Average training loss: 0.0478917553126812\n",
      "Average test loss: 0.007610942229628563\n",
      "Epoch 133/300\n",
      "Average training loss: 0.048828985744052464\n",
      "Average test loss: 0.002990512587957912\n",
      "Epoch 136/300\n",
      "Average training loss: 0.06234469305144416\n",
      "Average test loss: 0.0035018046274781227\n",
      "Epoch 137/300\n",
      "Average training loss: 0.06194843696223365\n",
      "Average test loss: 0.0031006452886180747\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05267872659696473\n",
      "Average test loss: 0.0030030803026424514\n",
      "Epoch 139/300\n",
      "Average training loss: 0.04958023417658276\n",
      "Average test loss: 0.0030456260374436776\n",
      "Epoch 140/300\n",
      "Average training loss: 0.048889807548787856\n",
      "Average test loss: 0.0030212531572000848\n",
      "Epoch 141/300\n",
      "Average training loss: 0.04765416153934267\n",
      "Average test loss: 0.0031190628620485464\n",
      "Epoch 142/300\n",
      "Average training loss: 0.04707499088512527\n",
      "Average test loss: 0.0030017985850572587\n",
      "Epoch 143/300\n",
      "Average training loss: 0.046765957282649144\n",
      "Average test loss: 0.0034380841048227417\n",
      "Epoch 144/300\n",
      "Average training loss: 0.04667906980713209\n",
      "Average test loss: 0.0031797286917765936\n",
      "Epoch 145/300\n",
      "Average training loss: 0.04689583094583617\n",
      "Average test loss: 0.0029965734862618977\n",
      "Epoch 146/300\n",
      "Average training loss: 0.04613370170858171\n",
      "Average test loss: 0.003053137414985233\n",
      "Epoch 148/300\n",
      "Average training loss: 0.04600376968913608\n",
      "Average test loss: 0.0031965249613341356\n",
      "Epoch 149/300\n",
      "Average training loss: 0.04584012201428413\n",
      "Average test loss: 0.026220996912982728\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04584905429350005\n",
      "Average test loss: 0.0031465109050687816\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04564111933774418\n",
      "Average test loss: 0.0032343288542081914\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04534014557798704\n",
      "Average test loss: 0.003036366783496406\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04539073485798306\n",
      "Average test loss: 0.34591232589549487\n",
      "Epoch 154/300\n",
      "Average training loss: 0.13854320377442572\n",
      "Average test loss: 0.003171443269898494\n",
      "Epoch 155/300\n",
      "Average training loss: 0.06770962029364375\n",
      "Average test loss: 0.003031781115465694\n",
      "Epoch 156/300\n",
      "Average training loss: 0.062174587501419916\n",
      "Average test loss: 0.0030257896963093015\n",
      "Epoch 157/300\n",
      "Average training loss: 0.05896412052710851\n",
      "Average test loss: 0.002948146257135603\n",
      "Epoch 158/300\n",
      "Average training loss: 0.0564642496407032\n",
      "Average test loss: 0.0030737328922583\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05445587800608741\n",
      "Average test loss: 0.0029967336139331262\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05265075937575764\n",
      "Average test loss: 0.0029689778520001304\n",
      "Epoch 161/300\n",
      "Average training loss: 0.05098945425285233\n",
      "Average test loss: 0.0030397448615274497\n",
      "Epoch 162/300\n",
      "Average training loss: 0.04950622237722079\n",
      "Average test loss: 0.0030325274582331378\n",
      "Epoch 163/300\n",
      "Average training loss: 0.048151394916905295\n",
      "Average test loss: 0.004227173767983913\n",
      "Epoch 164/300\n",
      "Average training loss: 0.04692839263545142\n",
      "Average test loss: 0.0031803674093551105\n",
      "Epoch 165/300\n",
      "Average training loss: 0.04608494288722674\n",
      "Average test loss: 0.0030170494767112863\n",
      "Epoch 166/300\n",
      "Average training loss: 0.04551214000582695\n",
      "Average test loss: 0.0031021152010394467\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04510472796691788\n",
      "Average test loss: 0.003051141191687849\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04483937987354067\n",
      "Average test loss: 0.005203312433221274\n",
      "Epoch 169/300\n",
      "Average training loss: 0.044727838622199166\n",
      "Average test loss: 0.003131056317852603\n",
      "Epoch 170/300\n",
      "Average training loss: 0.044864710837602614\n",
      "Average test loss: 0.0030711978840538196\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04451599735683865\n",
      "Average test loss: 0.0032166510025660195\n",
      "Epoch 172/300\n",
      "Average training loss: 0.04444631044069926\n",
      "Average test loss: 0.003181758180467619\n",
      "Epoch 173/300\n",
      "Average training loss: 0.0443616732623842\n",
      "Average test loss: 0.003193382012554341\n",
      "Epoch 174/300\n",
      "Average training loss: 0.04617164648572604\n",
      "Average test loss: 0.0032663708383010494\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04440258061885834\n",
      "Average test loss: 0.0031060518694834578\n",
      "Epoch 176/300\n",
      "Average training loss: 0.045312152446972\n",
      "Average test loss: 0.0031183031520081892\n",
      "Epoch 177/300\n",
      "Average training loss: 0.07625930425855848\n",
      "Average test loss: 5155.082717096923\n",
      "Epoch 178/300\n",
      "Average training loss: 0.07630678105685446\n",
      "Average test loss: 0.0030019117784168985\n",
      "Epoch 179/300\n",
      "Average training loss: 0.06361731253067653\n",
      "Average test loss: 0.0029768637167289854\n",
      "Epoch 180/300\n",
      "Average training loss: 0.059348126073678335\n",
      "Average test loss: 0.0029494292851951387\n",
      "Epoch 181/300\n",
      "Average training loss: 0.05532301651769214\n",
      "Average test loss: 0.0029983141751339037\n",
      "Epoch 182/300\n",
      "Average training loss: 0.052980582571691935\n",
      "Average test loss: 0.0029715350800090367\n",
      "Epoch 183/300\n",
      "Average training loss: 0.0511334366566605\n",
      "Average test loss: 0.0035010646395385266\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04958156030376752\n",
      "Average test loss: 0.0030332014626926846\n",
      "Epoch 185/300\n",
      "Average training loss: 0.048307757844527566\n",
      "Average test loss: 0.002947643785013093\n",
      "Epoch 186/300\n",
      "Average training loss: 0.047934669107198714\n",
      "Average test loss: 0.0031096910815685986\n",
      "Epoch 187/300\n",
      "Average training loss: 0.049138418820169234\n",
      "Average test loss: 0.011438431517117553\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04626908826496866\n",
      "Average test loss: 0.003075410729274154\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04556573836008708\n",
      "Average test loss: 0.0030773536529805925\n",
      "Epoch 190/300\n",
      "Average training loss: 0.045252267956733705\n",
      "Average test loss: 0.0031731222694118817\n",
      "Epoch 191/300\n",
      "Average training loss: 0.045262868387831584\n",
      "Average test loss: 0.003077114063211613\n",
      "Epoch 192/300\n",
      "Average training loss: 0.04477004216776954\n",
      "Average test loss: 0.003367057499786218\n",
      "Epoch 193/300\n",
      "Average training loss: 0.04426517024967406\n",
      "Average test loss: 0.0030600607738726668\n",
      "Epoch 194/300\n",
      "Average training loss: 0.04392605511016316\n",
      "Average test loss: 0.003120417847401566\n",
      "Epoch 195/300\n",
      "Average training loss: 0.04355967492196295\n",
      "Average test loss: 0.003564000093481607\n",
      "Epoch 196/300\n",
      "Average training loss: 0.043338954991764496\n",
      "Average test loss: 0.0030591815532081657\n",
      "Epoch 197/300\n",
      "Average training loss: 0.043765444229046505\n",
      "Average test loss: 0.003023413227457139\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04300325871507327\n",
      "Average test loss: 0.0032018019093407526\n",
      "Epoch 199/300\n",
      "Average training loss: 0.042944070554441875\n",
      "Average test loss: 0.0036408223184860415\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04289197332991494\n",
      "Average test loss: 0.003423173086096843\n",
      "Epoch 201/300\n",
      "Average training loss: 0.06128220052520434\n",
      "Average test loss: 0.0030833736745019755\n",
      "Epoch 202/300\n",
      "Average training loss: 0.05315346359875467\n",
      "Average test loss: 0.0032914069982038605\n",
      "Epoch 203/300\n",
      "Average training loss: 0.04658775253428353\n",
      "Average test loss: 0.0032047275211662053\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04400333029694027\n",
      "Average test loss: 0.0030862236053993304\n",
      "Epoch 205/300\n",
      "Average training loss: 0.043258582353591916\n",
      "Average test loss: 0.003188215927531322\n",
      "Epoch 206/300\n",
      "Average training loss: 0.042673985183238984\n",
      "Average test loss: 0.003204022806344761\n",
      "Epoch 207/300\n",
      "Average training loss: 0.042415547337796955\n",
      "Average test loss: 0.0031471670677678453\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04228522271249029\n",
      "Average test loss: 0.0030664524173157084\n",
      "Epoch 209/300\n",
      "Average training loss: 0.042478669295708336\n",
      "Average test loss: 0.004282221231195662\n",
      "Epoch 210/300\n",
      "Average training loss: 0.061193327956729465\n",
      "Average test loss: 0.0030883863042626115\n",
      "Epoch 211/300\n",
      "Average training loss: 0.05772617577181922\n",
      "Average test loss: 0.002939210594942172\n",
      "Epoch 212/300\n",
      "Average training loss: 0.050600602189699806\n",
      "Average test loss: 0.007786270824985371\n",
      "Epoch 213/300\n",
      "Average training loss: 0.046762978043821124\n",
      "Average test loss: 0.0030484191469020315\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04412167968683773\n",
      "Average test loss: 0.0033865815024409027\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04300508291191525\n",
      "Average test loss: 0.004718715100652642\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04236304136448436\n",
      "Average test loss: 0.003227104569060935\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04212066633833779\n",
      "Average test loss: 0.003112382298749354\n",
      "Epoch 218/300\n",
      "Average training loss: 0.04201029133796692\n",
      "Average test loss: 0.0031400437098410396\n",
      "Epoch 219/300\n",
      "Average training loss: 0.04192796580990155\n",
      "Average test loss: 0.004402484306858645\n",
      "Epoch 220/300\n",
      "Average training loss: 0.04187386425336202\n",
      "Average test loss: 0.003309692807495594\n",
      "Epoch 221/300\n",
      "Average training loss: 0.04183772982822524\n",
      "Average test loss: 0.005663837393952741\n",
      "Epoch 222/300\n",
      "Average training loss: 0.041757977836661866\n",
      "Average test loss: 0.0031605898146000173\n",
      "Epoch 223/300\n",
      "Average training loss: 0.04164631983306673\n",
      "Average test loss: 0.0032254964725838766\n",
      "Epoch 224/300\n",
      "Average training loss: 0.041766338523891236\n",
      "Average test loss: 0.0032153553280772434\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04163568908472856\n",
      "Average test loss: 0.0031562289309998355\n",
      "Epoch 226/300\n",
      "Average training loss: 0.04228567880392074\n",
      "Average test loss: 0.0031320175495412614\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04197065920962228\n",
      "Average test loss: 0.0033405619013226694\n",
      "Epoch 228/300\n",
      "Average training loss: 0.041192970275878905\n",
      "Average test loss: 0.003206403683871031\n",
      "Epoch 229/300\n",
      "Average training loss: 0.041092094739278155\n",
      "Average test loss: 0.003187983927213483\n",
      "Epoch 230/300\n",
      "Average training loss: 0.04113557397656971\n",
      "Average test loss: 0.0031047405786812305\n",
      "Epoch 231/300\n",
      "Average training loss: 0.04101686207122273\n",
      "Average test loss: 0.0032113685237450734\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04099628304110633\n",
      "Average test loss: 0.0032431545309308503\n",
      "Epoch 233/300\n",
      "Average training loss: 0.040909253898594115\n",
      "Average test loss: 0.003285904491113292\n",
      "Epoch 234/300\n",
      "Average training loss: 0.040849114542206126\n",
      "Average test loss: 0.0038988395449188022\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04095465858777364\n",
      "Average test loss: 0.003258238413474626\n",
      "Epoch 236/300\n",
      "Average training loss: 0.040627122352520625\n",
      "Average test loss: 0.0039579789605405594\n",
      "Epoch 237/300\n",
      "Average training loss: 0.040731245554155776\n",
      "Average test loss: 0.003228192640882399\n",
      "Epoch 238/300\n",
      "Average training loss: 0.040504112399286696\n",
      "Average test loss: 0.0034157935488555165\n",
      "Epoch 239/300\n",
      "Average training loss: 0.04069098304046525\n",
      "Average test loss: 0.003160212651722961\n",
      "Epoch 240/300\n",
      "Average training loss: 0.041661335027880136\n",
      "Average test loss: 0.003123059110302064\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04063575481043922\n",
      "Average test loss: 0.003456016811852654\n",
      "Epoch 242/300\n",
      "Average training loss: 0.04038231136732631\n",
      "Average test loss: 0.0033984752104928096\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04022918114397261\n",
      "Average test loss: 0.0032913576426605385\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04031492328643799\n",
      "Average test loss: 0.003232249419722292\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04010632340775596\n",
      "Average test loss: 0.003183558450390895\n",
      "Epoch 246/300\n",
      "Average training loss: 0.040009395226836206\n",
      "Average test loss: 0.0037576541304588317\n",
      "Epoch 247/300\n",
      "Average training loss: 0.04019497952196333\n",
      "Average test loss: 0.003125660843319363\n",
      "Epoch 248/300\n",
      "Average training loss: 0.0400331847137875\n",
      "Average test loss: 0.0032224754012293287\n",
      "Epoch 249/300\n",
      "Average training loss: 0.0398098707165983\n",
      "Average test loss: 0.003191460919669933\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03995214450359345\n",
      "Average test loss: 0.0032675579766639406\n",
      "Epoch 251/300\n",
      "Average training loss: 0.039830027371644976\n",
      "Average test loss: 0.003223042669809527\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03980659452411864\n",
      "Average test loss: 0.003375894567825728\n",
      "Epoch 253/300\n",
      "Average training loss: 0.040265810834036936\n",
      "Average test loss: 0.0033456425136990015\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03983463076088164\n",
      "Average test loss: 0.003209020811236567\n",
      "Epoch 255/300\n",
      "Average training loss: 0.03957818004488945\n",
      "Average test loss: 0.003350096244364977\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03954513529936472\n",
      "Average test loss: 0.0032445488741828336\n",
      "Epoch 257/300\n",
      "Average training loss: 0.039499093802438844\n",
      "Average test loss: 0.00348838562063045\n",
      "Epoch 258/300\n",
      "Average training loss: 0.03951100615329212\n",
      "Average test loss: 0.0032603615524454247\n",
      "Epoch 259/300\n",
      "Average training loss: 0.039496644414133496\n",
      "Average test loss: 0.00353187512109677\n",
      "Epoch 260/300\n",
      "Average training loss: 0.039441991796096164\n",
      "Average test loss: 0.0035398484321518078\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03940701610512204\n",
      "Average test loss: 0.0035273842683268917\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03943741381830639\n",
      "Average test loss: 0.0033182457510588896\n",
      "Epoch 263/300\n",
      "Average training loss: 0.039325426008966234\n",
      "Average test loss: 0.0032954895682632923\n",
      "Epoch 264/300\n",
      "Average training loss: 0.03964215340548091\n",
      "Average test loss: 0.004371035225896372\n",
      "Epoch 265/300\n",
      "Average training loss: 0.039306569304731157\n",
      "Average test loss: 0.014548994157049392\n",
      "Epoch 266/300\n",
      "Average training loss: 0.03932314732339647\n",
      "Average test loss: 0.01604078305926588\n",
      "Epoch 267/300\n",
      "Average training loss: 0.03912403384844462\n",
      "Average test loss: 0.003244080843196975\n",
      "Epoch 268/300\n",
      "Average training loss: 0.03906812659568257\n",
      "Average test loss: 0.0033102670196029876\n",
      "Epoch 269/300\n",
      "Average training loss: 0.03909279890275664\n",
      "Average test loss: 0.0034453346530596415\n",
      "Epoch 270/300\n",
      "Average training loss: 0.039064101778798634\n",
      "Average test loss: 0.004938648727205065\n",
      "Epoch 271/300\n",
      "Average training loss: 0.03907796187202136\n",
      "Average test loss: 0.0032740476325982147\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03904305158389939\n",
      "Average test loss: 0.0034803767384340364\n",
      "Epoch 273/300\n",
      "Average training loss: 0.038872117075655195\n",
      "Average test loss: 0.0032926383045398526\n",
      "Epoch 274/300\n",
      "Average training loss: 0.038931889173057344\n",
      "Average test loss: 0.0033034207564261223\n",
      "Epoch 275/300\n",
      "Average training loss: 0.03884575273924404\n",
      "Average test loss: 0.0032248693180994856\n",
      "Epoch 276/300\n",
      "Average training loss: 0.039401048534446294\n",
      "Average test loss: 0.003185014833178785\n",
      "Epoch 277/300\n",
      "Average training loss: 0.038729331182108986\n",
      "Average test loss: 0.0035277051735255454\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0387304729157024\n",
      "Average test loss: 0.0033237897983441752\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0389106923143069\n",
      "Average test loss: 0.0033395669019470613\n",
      "Epoch 280/300\n",
      "Average training loss: 0.038619383262263406\n",
      "Average test loss: 0.003814076829287741\n",
      "Epoch 281/300\n",
      "Average training loss: 0.03857387069861094\n",
      "Average test loss: 0.003283191481605172\n",
      "Epoch 282/300\n",
      "Average training loss: 0.03858467018935415\n",
      "Average test loss: 0.0032389456164091824\n",
      "Epoch 283/300\n",
      "Average training loss: 0.03856147988968425\n",
      "Average test loss: 0.003308133435746034\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03845973931749662\n",
      "Average test loss: 0.003233784035158654\n",
      "Epoch 285/300\n",
      "Average training loss: 0.038531073096725675\n",
      "Average test loss: 0.0033616987098422316\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0383577015813854\n",
      "Average test loss: 0.003547665120826827\n",
      "Epoch 287/300\n",
      "Average training loss: 0.03854555426372422\n",
      "Average test loss: 0.00392972094586326\n",
      "Epoch 288/300\n",
      "Average training loss: 0.03833646994166904\n",
      "Average test loss: 0.003792211438425713\n",
      "Epoch 289/300\n",
      "Average training loss: 0.03836894149250454\n",
      "Average test loss: 0.0032566270526084634\n",
      "Epoch 290/300\n",
      "Average training loss: 0.03840851226449013\n",
      "Average test loss: 0.0035264690899186662\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03831492322021061\n",
      "Average test loss: 0.003450462505221367\n",
      "Epoch 292/300\n",
      "Average training loss: 0.03833578576975399\n",
      "Average test loss: 0.0034421579527358216\n",
      "Epoch 293/300\n",
      "Average training loss: 0.038527201268408036\n",
      "Average test loss: 0.003253779551635186\n",
      "Epoch 294/300\n",
      "Average training loss: 0.03817260175281101\n",
      "Average test loss: 0.0032904544503738484\n",
      "Epoch 295/300\n",
      "Average training loss: 0.03879219510820177\n",
      "Average test loss: 0.0034563996626271144\n",
      "Epoch 296/300\n",
      "Average training loss: 0.037986131868428655\n",
      "Average test loss: 0.003273366834140486\n",
      "Epoch 297/300\n",
      "Average training loss: 0.03802919160988596\n",
      "Average test loss: 0.0035804060254659917\n",
      "Epoch 298/300\n",
      "Average training loss: 0.03815413968761762\n",
      "Average test loss: 0.0031879807049408557\n",
      "Epoch 299/300\n",
      "Average training loss: 0.03805632696549098\n",
      "Average test loss: 0.003414522507124477\n",
      "Epoch 300/300\n",
      "Average training loss: 0.038212758474879795\n",
      "Average test loss: 0.0033066318343496987\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 5.239294918484158\n",
      "Average test loss: 0.025030296077330906\n",
      "Epoch 2/300\n",
      "Average training loss: 2.094307614326477\n",
      "Average test loss: 0.006547318776034646\n",
      "Epoch 3/300\n",
      "Average training loss: 1.4007641554938421\n",
      "Average test loss: 0.008100778388894267\n",
      "Epoch 4/300\n",
      "Average training loss: 1.1043250348303053\n",
      "Average test loss: 0.004707438067015675\n",
      "Epoch 5/300\n",
      "Average training loss: 0.9231571939256457\n",
      "Average test loss: 0.004597688297016753\n",
      "Epoch 6/300\n",
      "Average training loss: 0.790548183494144\n",
      "Average test loss: 0.004278897483729654\n",
      "Epoch 7/300\n",
      "Average training loss: 0.6854134253925748\n",
      "Average test loss: 0.004132587519784768\n",
      "Epoch 8/300\n",
      "Average training loss: 0.6023475413322449\n",
      "Average test loss: 0.004795189568979873\n",
      "Epoch 9/300\n",
      "Average training loss: 0.4716577343410916\n",
      "Average test loss: 0.0034036370284027525\n",
      "Epoch 11/300\n",
      "Average training loss: 0.41911363230811227\n",
      "Average test loss: 0.0034041358166270784\n",
      "Epoch 12/300\n",
      "Average training loss: 0.3716224941412608\n",
      "Average test loss: 0.0033327878941264415\n",
      "Epoch 13/300\n",
      "Average training loss: 0.33126075095600555\n",
      "Average test loss: 0.003238676437901126\n",
      "Epoch 14/300\n",
      "Average training loss: 0.2944897381199731\n",
      "Average test loss: 0.0037484202149013677\n",
      "Epoch 15/300\n",
      "Average training loss: 0.26271963556607564\n",
      "Average test loss: 0.003008976131470667\n",
      "Epoch 16/300\n",
      "Average training loss: 0.23401874584621854\n",
      "Average test loss: 0.002929006101977494\n",
      "Epoch 17/300\n",
      "Average training loss: 0.21056248055564034\n",
      "Average test loss: 0.003466473621626695\n",
      "Epoch 18/300\n",
      "Average training loss: 0.18954609648386636\n",
      "Average test loss: 0.0026899998450858726\n",
      "Epoch 19/300\n",
      "Average training loss: 0.17163648676872253\n",
      "Average test loss: 0.0025998148595293364\n",
      "Epoch 20/300\n",
      "Average training loss: 0.15672120496961806\n",
      "Average test loss: 0.002508520605456498\n",
      "Epoch 21/300\n",
      "Average training loss: 0.14417880217234294\n",
      "Average test loss: 0.0027158316783607007\n",
      "Epoch 22/300\n",
      "Average training loss: 0.1325024126238293\n",
      "Average test loss: 0.0025178153255126542\n",
      "Epoch 23/300\n",
      "Average training loss: 0.12193402525451448\n",
      "Average test loss: 0.003093564498341746\n",
      "Epoch 24/300\n",
      "Average training loss: 0.1131444646583663\n",
      "Average test loss: 0.0022823138630224597\n",
      "Epoch 25/300\n",
      "Average training loss: 0.1047822511990865\n",
      "Average test loss: 0.004740169050378932\n",
      "Epoch 26/300\n",
      "Average training loss: 0.09812707914908728\n",
      "Average test loss: 0.0023634699792083767\n",
      "Epoch 27/300\n",
      "Average training loss: 0.0911030509074529\n",
      "Average test loss: 0.00246309543804576\n",
      "Epoch 28/300\n",
      "Average training loss: 0.08699361321661207\n",
      "Average test loss: 0.002195943910525077\n",
      "Epoch 29/300\n",
      "Average training loss: 0.0816215290096071\n",
      "Average test loss: 0.0022146928167591493\n",
      "Epoch 30/300\n",
      "Average training loss: 0.07656569976276821\n",
      "Average test loss: 0.055783469533754716\n",
      "Epoch 31/300\n",
      "Average training loss: 0.07289787324269613\n",
      "Average test loss: 0.05184743842523959\n",
      "Epoch 32/300\n",
      "Average training loss: 0.06940091254976061\n",
      "Average test loss: 0.002373598240522875\n",
      "Epoch 33/300\n",
      "Average training loss: 0.0659548244410091\n",
      "Average test loss: 0.0021826843114362824\n",
      "Epoch 34/300\n",
      "Average training loss: 0.06421119440926445\n",
      "Average test loss: 0.012501388885080815\n",
      "Epoch 35/300\n",
      "Average training loss: 0.061403441184096866\n",
      "Average test loss: 0.014746742086278068\n",
      "Epoch 36/300\n",
      "Average training loss: 0.05894031525320477\n",
      "Average test loss: 0.003627382184896204\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05632463016774919\n",
      "Average test loss: 0.0020171380701164403\n",
      "Epoch 38/300\n",
      "Average training loss: 0.05658519232935376\n",
      "Average test loss: 0.0020415041183845864\n",
      "Epoch 39/300\n",
      "Average training loss: 0.06695131638314988\n",
      "Average test loss: 0.002652774579409096\n",
      "Epoch 40/300\n",
      "Average training loss: 0.06154086400734054\n",
      "Average test loss: 0.002183576495593621\n",
      "Epoch 41/300\n",
      "Average training loss: 0.055953658004601795\n",
      "Average test loss: 0.009098948743608263\n",
      "Epoch 42/300\n",
      "Average training loss: 0.05174817068709268\n",
      "Average test loss: 0.0021322758690350586\n",
      "Epoch 43/300\n",
      "Average training loss: 0.050009583512942\n",
      "Average test loss: 0.0020167071460228825\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04833373608191808\n",
      "Average test loss: 0.002024035540616347\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04741513709227244\n",
      "Average test loss: 0.005626383392243749\n",
      "Epoch 46/300\n",
      "Average training loss: 0.046388345748186115\n",
      "Average test loss: 0.0019763241330575613\n",
      "Epoch 47/300\n",
      "Average training loss: 0.045605488416221404\n",
      "Average test loss: 0.0019977216139539246\n",
      "Epoch 48/300\n",
      "Average training loss: 0.044803862945901024\n",
      "Average test loss: 0.0025975573588576583\n",
      "Epoch 49/300\n",
      "Average training loss: 0.04415058482686678\n",
      "Average test loss: 0.004146598613096608\n",
      "Epoch 50/300\n",
      "Average training loss: 0.04432063291801347\n",
      "Average test loss: 0.0020793272267199225\n",
      "Epoch 51/300\n",
      "Average training loss: 0.05743087547686365\n",
      "Average test loss: 0.0024146270491182805\n",
      "Epoch 52/300\n",
      "Average training loss: 0.051057033664650385\n",
      "Average test loss: 0.0020689881472951838\n",
      "Epoch 53/300\n",
      "Average training loss: 0.04603014276425044\n",
      "Average test loss: 0.001986020682172643\n",
      "Epoch 54/300\n",
      "Average training loss: 0.044724683943721984\n",
      "Average test loss: 0.0020115974681037996\n",
      "Epoch 55/300\n",
      "Average training loss: 0.04380743172764778\n",
      "Average test loss: 0.006311255566568838\n",
      "Epoch 56/300\n",
      "Average training loss: 0.04313884609275394\n",
      "Average test loss: 0.0023540521868401104\n",
      "Epoch 57/300\n",
      "Average training loss: 0.0426862863070435\n",
      "Average test loss: 0.005738024254226023\n",
      "Epoch 58/300\n",
      "Average training loss: 0.04229149115416739\n",
      "Average test loss: 0.0019859746139910487\n",
      "Epoch 59/300\n",
      "Average training loss: 0.041572865542438296\n",
      "Average test loss: 0.0020304901222180987\n",
      "Epoch 60/300\n",
      "Average training loss: 0.041439947525660194\n",
      "Average test loss: 0.002625487502664328\n",
      "Epoch 61/300\n",
      "Average training loss: 0.04165394733349482\n",
      "Average test loss: 0.0020401751664984556\n",
      "Epoch 62/300\n",
      "Average training loss: 0.040653940679298506\n",
      "Average test loss: 0.0020269893509232335\n",
      "Epoch 63/300\n",
      "Average training loss: 0.04014840094579591\n",
      "Average test loss: 0.0019691582597378228\n",
      "Epoch 64/300\n",
      "Average training loss: 0.03980503853989972\n",
      "Average test loss: 0.002122172561267184\n",
      "Epoch 65/300\n",
      "Average training loss: 0.039550891621245275\n",
      "Average test loss: 0.004900174993193812\n",
      "Epoch 66/300\n",
      "Average training loss: 0.039075726062059404\n",
      "Average test loss: 0.002352469422130121\n",
      "Epoch 67/300\n",
      "Average training loss: 0.03883893015649584\n",
      "Average test loss: 0.0029908132495151626\n",
      "Epoch 68/300\n",
      "Average training loss: 0.041325809397631225\n",
      "Average test loss: 0.0020896535464045075\n",
      "Epoch 69/300\n",
      "Average training loss: 0.04074379416637951\n",
      "Average test loss: 0.06141458886509968\n",
      "Epoch 70/300\n",
      "Average training loss: 0.03879469864898258\n",
      "Average test loss: 0.0021610707105654808\n",
      "Epoch 71/300\n",
      "Average training loss: 0.038145454304085835\n",
      "Average test loss: 0.002695850770610074\n",
      "Epoch 72/300\n",
      "Average training loss: 0.03787842133641243\n",
      "Average test loss: 0.0020299590933654044\n",
      "Epoch 73/300\n",
      "Average training loss: 0.03758469936748346\n",
      "Average test loss: 0.002091413640520639\n",
      "Epoch 74/300\n",
      "Average training loss: 0.0372588668399387\n",
      "Average test loss: 0.0020354030936335526\n",
      "Epoch 75/300\n",
      "Average training loss: 0.037002892875009116\n",
      "Average test loss: 0.0020904773347493674\n",
      "Epoch 76/300\n",
      "Average training loss: 0.03693656667073568\n",
      "Average test loss: 0.0019187222323897811\n",
      "Epoch 77/300\n",
      "Average training loss: 0.03661210966275798\n",
      "Average test loss: 0.002044586066984468\n",
      "Epoch 78/300\n",
      "Average training loss: 0.03653557738992903\n",
      "Average test loss: 0.002058432196163469\n",
      "Epoch 79/300\n",
      "Average training loss: 0.05027635939584838\n",
      "Average test loss: 0.002214353134234746\n",
      "Epoch 80/300\n",
      "Average training loss: 0.043508278525537916\n",
      "Average test loss: 0.002348158948433896\n",
      "Epoch 81/300\n",
      "Average training loss: 0.044497333923975625\n",
      "Average test loss: 0.0019717615671041937\n",
      "Epoch 82/300\n",
      "Average training loss: 0.03946751030948427\n",
      "Average test loss: 0.002033711857628077\n",
      "Epoch 83/300\n",
      "Average training loss: 0.03781434147556623\n",
      "Average test loss: 0.0020175000112503767\n",
      "Epoch 84/300\n",
      "Average training loss: 0.037237524777650834\n",
      "Average test loss: 0.002193135329625673\n",
      "Epoch 85/300\n",
      "Average training loss: 0.03650094294713603\n",
      "Average test loss: 0.0021727672417958576\n",
      "Epoch 86/300\n",
      "Average training loss: 0.036448222077555124\n",
      "Average test loss: 0.002044018144822783\n",
      "Epoch 87/300\n",
      "Average training loss: 0.03598193735546536\n",
      "Average test loss: 0.0019626850871783163\n",
      "Epoch 88/300\n",
      "Average training loss: 0.03582536823881997\n",
      "Average test loss: 0.001970433322816259\n",
      "Epoch 89/300\n",
      "Average training loss: 0.03554054911269082\n",
      "Average test loss: 0.001986165865013997\n",
      "Epoch 90/300\n",
      "Average training loss: 0.03543597176339891\n",
      "Average test loss: 0.0020449666138738394\n",
      "Epoch 91/300\n",
      "Average training loss: 0.03525026920106676\n",
      "Average test loss: 8.250618061510226\n",
      "Epoch 92/300\n",
      "Average training loss: 0.03512964187231329\n",
      "Average test loss: 0.002049469513611661\n",
      "Epoch 93/300\n",
      "Average training loss: 0.03489518048365911\n",
      "Average test loss: 0.002004119576472375\n",
      "Epoch 94/300\n",
      "Average training loss: 0.034664383972684544\n",
      "Average test loss: 0.0038985743665446836\n",
      "Epoch 95/300\n",
      "Average training loss: 0.03450205996301439\n",
      "Average test loss: 0.0020634116027504206\n",
      "Epoch 96/300\n",
      "Average training loss: 0.03432553979423311\n",
      "Average test loss: 0.0019746928960084915\n",
      "Epoch 97/300\n",
      "Average training loss: 0.03414531273477607\n",
      "Average test loss: 0.0020882884804159403\n",
      "Epoch 98/300\n",
      "Average training loss: 0.034475941078530414\n",
      "Average test loss: 0.007106606394880348\n",
      "Epoch 99/300\n",
      "Average training loss: 0.08358127048611641\n",
      "Average test loss: 0.004063854143437412\n",
      "Epoch 100/300\n",
      "Average training loss: 0.04655733382701874\n",
      "Average test loss: 0.002078463698219922\n",
      "Epoch 101/300\n",
      "Average training loss: 0.041636254413260354\n",
      "Average test loss: 0.0019817568903995886\n",
      "Epoch 102/300\n",
      "Average training loss: 0.03921391045716074\n",
      "Average test loss: 0.0019735347383345167\n",
      "Epoch 103/300\n",
      "Average training loss: 0.037464045739836166\n",
      "Average test loss: 0.052824697146192194\n",
      "Epoch 104/300\n",
      "Average training loss: 0.03624191121425894\n",
      "Average test loss: 1.2329350581963856\n",
      "Epoch 105/300\n",
      "Average training loss: 0.03548877130614387\n",
      "Average test loss: 0.00202995135800706\n",
      "Epoch 106/300\n",
      "Average training loss: 0.03482556649545829\n",
      "Average test loss: 0.0021467789113521576\n",
      "Epoch 107/300\n",
      "Average training loss: 0.034471788631545174\n",
      "Average test loss: 0.001944787429852618\n",
      "Epoch 108/300\n",
      "Average training loss: 0.03414813004102972\n",
      "Average test loss: 0.001962548796915346\n",
      "Epoch 109/300\n",
      "Average training loss: 0.033918950214982035\n",
      "Average test loss: 0.0019461882066809468\n",
      "Epoch 110/300\n",
      "Average training loss: 0.033786043776406184\n",
      "Average test loss: 0.001998115352768865\n",
      "Epoch 111/300\n",
      "Average training loss: 0.033672324122654064\n",
      "Average test loss: 0.09046522750643392\n",
      "Epoch 112/300\n",
      "Average training loss: 0.033428842412100895\n",
      "Average test loss: 0.0028530029712451827\n",
      "Epoch 113/300\n",
      "Average training loss: 0.03359308929906951\n",
      "Average test loss: 0.0020596545562148096\n",
      "Epoch 114/300\n",
      "Average training loss: 0.03778600005308787\n",
      "Average test loss: 0.002749903113270799\n",
      "Epoch 115/300\n",
      "Average training loss: 0.03606124639842245\n",
      "Average test loss: 0.0020695168969945776\n",
      "Epoch 116/300\n",
      "Average training loss: 0.034053769141435625\n",
      "Average test loss: 0.003026701047602627\n",
      "Epoch 117/300\n",
      "Average training loss: 0.0334614212943448\n",
      "Average test loss: 0.002632608216152423\n",
      "Epoch 118/300\n",
      "Average training loss: 0.03311552114122444\n",
      "Average test loss: 0.002194690863705344\n",
      "Epoch 119/300\n",
      "Average training loss: 0.03297187260455555\n",
      "Average test loss: 0.0020017741618471013\n",
      "Epoch 120/300\n",
      "Average training loss: 0.032834148006306754\n",
      "Average test loss: 0.001987627383114563\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03275070477690962\n",
      "Average test loss: 0.004272041361365053\n",
      "Epoch 122/300\n",
      "Average training loss: 0.03275177802973323\n",
      "Average test loss: 0.0019287995943385694\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03240620528161526\n",
      "Average test loss: 0.0029462978082398575\n",
      "Epoch 124/300\n",
      "Average training loss: 0.03234113762113783\n",
      "Average test loss: 0.0020681512980825372\n",
      "Epoch 125/300\n",
      "Average training loss: 0.032181208507882225\n",
      "Average test loss: 0.0020691102732800777\n",
      "Epoch 126/300\n",
      "Average training loss: 0.032180463077293504\n",
      "Average test loss: 0.0021135066588305763\n",
      "Epoch 127/300\n",
      "Average training loss: 0.03209660714533594\n",
      "Average test loss: 0.00209527942393389\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03274336615204811\n",
      "Average test loss: 0.002070278178486559\n",
      "Epoch 129/300\n",
      "Average training loss: 0.03251853859424591\n",
      "Average test loss: 0.002006913449201319\n",
      "Epoch 130/300\n",
      "Average training loss: 0.0318850444836749\n",
      "Average test loss: 0.0021078323550108406\n",
      "Epoch 131/300\n",
      "Average training loss: 0.031575632726152734\n",
      "Average test loss: 0.0023394557496325837\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03159368727604548\n",
      "Average test loss: 0.002163464363457428\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03141629891594251\n",
      "Average test loss: 0.0019763031088643602\n",
      "Epoch 134/300\n",
      "Average training loss: 0.0313419505175617\n",
      "Average test loss: 0.0022847557249996396\n",
      "Epoch 135/300\n",
      "Average training loss: 0.031613732043239806\n",
      "Average test loss: 0.0020441556410450076\n",
      "Epoch 136/300\n",
      "Average training loss: 0.031182719762126606\n",
      "Average test loss: 0.00208758684910006\n",
      "Epoch 137/300\n",
      "Average training loss: 0.031240281058682335\n",
      "Average test loss: 0.003917643328921663\n",
      "Epoch 138/300\n",
      "Average training loss: 0.030905665669176314\n",
      "Average test loss: 0.0019922923613339663\n",
      "Epoch 139/300\n",
      "Average training loss: 0.0309457960791058\n",
      "Average test loss: 0.0020776243893843556\n",
      "Epoch 140/300\n",
      "Average training loss: 0.030811744840608703\n",
      "Average test loss: 0.002259472436582049\n",
      "Epoch 141/300\n",
      "Average training loss: 0.030786913080347908\n",
      "Average test loss: 0.004552992416752709\n",
      "Epoch 142/300\n",
      "Average training loss: 0.055707110424836476\n",
      "Average test loss: 0.002302762906791435\n",
      "Epoch 143/300\n",
      "Average training loss: 0.040979572763045626\n",
      "Average test loss: 0.00201862953003082\n",
      "Epoch 144/300\n",
      "Average training loss: 0.036866541370749475\n",
      "Average test loss: 0.0019804069142167766\n",
      "Epoch 145/300\n",
      "Average training loss: 0.03457316931254334\n",
      "Average test loss: 0.0020972994767750304\n",
      "Epoch 146/300\n",
      "Average training loss: 0.032961742733915646\n",
      "Average test loss: 0.0020365577903058794\n",
      "Epoch 147/300\n",
      "Average training loss: 0.031918800393740336\n",
      "Average test loss: 0.002063948999469479\n",
      "Epoch 148/300\n",
      "Average training loss: 0.03132120788594087\n",
      "Average test loss: 0.002006477347264687\n",
      "Epoch 149/300\n",
      "Average training loss: 0.030922160673472615\n",
      "Average test loss: 0.002050486219012075\n",
      "Epoch 150/300\n",
      "Average training loss: 0.030697035032841893\n",
      "Average test loss: 0.002209251037488381\n",
      "Epoch 151/300\n",
      "Average training loss: 0.03063284313182036\n",
      "Average test loss: 0.006929168148173227\n",
      "Epoch 152/300\n",
      "Average training loss: 0.03080133203499847\n",
      "Average test loss: 0.0021585491765290497\n",
      "Epoch 153/300\n",
      "Average training loss: 0.030471043336722587\n",
      "Average test loss: 0.002109582578245964\n",
      "Epoch 154/300\n",
      "Average training loss: 0.030531675706307094\n",
      "Average test loss: 0.007000818633370929\n",
      "Epoch 155/300\n",
      "Average training loss: 0.03079418812857734\n",
      "Average test loss: 0.002038422368777295\n",
      "Epoch 156/300\n",
      "Average training loss: 0.030420702199141184\n",
      "Average test loss: 0.0020304449174760116\n",
      "Epoch 157/300\n",
      "Average training loss: 0.03031274985273679\n",
      "Average test loss: 0.0020912359514170223\n",
      "Epoch 158/300\n",
      "Average training loss: 0.030153492238786484\n",
      "Average test loss: 0.0021088511148053737\n",
      "Epoch 159/300\n",
      "Average training loss: 0.03013188134961658\n",
      "Average test loss: 0.00262050878846397\n",
      "Epoch 160/300\n",
      "Average training loss: 0.030170784537990887\n",
      "Average test loss: 0.0020630899495962594\n",
      "Epoch 161/300\n",
      "Average training loss: 0.03004696233570576\n",
      "Average test loss: 0.0021615280215111043\n",
      "Epoch 162/300\n",
      "Average training loss: 0.030072258589996233\n",
      "Average test loss: 0.002113836870011356\n",
      "Epoch 163/300\n",
      "Average training loss: 0.029961641942461332\n",
      "Average test loss: 0.004598788905992276\n",
      "Epoch 164/300\n",
      "Average training loss: 0.029912194379501874\n",
      "Average test loss: 0.002222277708351612\n",
      "Epoch 165/300\n",
      "Average training loss: 0.0298442991177241\n",
      "Average test loss: 0.0021191038058863748\n",
      "Epoch 166/300\n",
      "Average training loss: 0.029758298196726374\n",
      "Average test loss: 0.0020592494135101636\n",
      "Epoch 167/300\n",
      "Average training loss: 0.029666654816932148\n",
      "Average test loss: 0.0021392677545340527\n",
      "Epoch 168/300\n",
      "Average training loss: 0.029708732813596726\n",
      "Average test loss: 0.002158427065083136\n",
      "Epoch 169/300\n",
      "Average training loss: 0.029645411474837197\n",
      "Average test loss: 0.002943015240339769\n",
      "Epoch 170/300\n",
      "Average training loss: 0.029594953808519574\n",
      "Average test loss: 0.003162922066118982\n",
      "Epoch 171/300\n",
      "Average training loss: 0.029534749993019634\n",
      "Average test loss: 0.0022683559772041107\n",
      "Epoch 172/300\n",
      "Average training loss: 0.02950095419751273\n",
      "Average test loss: 0.002971883226186037\n",
      "Epoch 173/300\n",
      "Average training loss: 0.029459500559502177\n",
      "Average test loss: 0.002223369055427611\n",
      "Epoch 174/300\n",
      "Average training loss: 0.029261200873388186\n",
      "Average test loss: 0.0021649513157705465\n",
      "Epoch 175/300\n",
      "Average training loss: 0.02949723998374409\n",
      "Average test loss: 0.0022333306384583313\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0293395536839962\n",
      "Average test loss: 0.0022893673131863276\n",
      "Epoch 177/300\n",
      "Average training loss: 0.029309637380970848\n",
      "Average test loss: 0.002301046181263195\n",
      "Epoch 178/300\n",
      "Average training loss: 0.029279458666841188\n",
      "Average test loss: 0.002272980021312833\n",
      "Epoch 179/300\n",
      "Average training loss: 0.029192341801193025\n",
      "Average test loss: 0.0020431472280373177\n",
      "Epoch 180/300\n",
      "Average training loss: 0.0291116280886862\n",
      "Average test loss: 0.0021853786304386127\n",
      "Epoch 181/300\n",
      "Average training loss: 0.029039830540617307\n",
      "Average test loss: 0.0023604673433841932\n",
      "Epoch 182/300\n",
      "Average training loss: 0.02906728752122985\n",
      "Average test loss: 0.06202977531010078\n",
      "Epoch 183/300\n",
      "Average training loss: 0.029041045013401243\n",
      "Average test loss: 0.01729941255930397\n",
      "Epoch 184/300\n",
      "Average training loss: 0.029070536113447613\n",
      "Average test loss: 0.004489570704392261\n",
      "Epoch 185/300\n",
      "Average training loss: 0.02924924092491468\n",
      "Average test loss: 0.002128583485467566\n",
      "Epoch 186/300\n",
      "Average training loss: 0.028934658567110697\n",
      "Average test loss: 0.003452093171576659\n",
      "Epoch 187/300\n",
      "Average training loss: 0.02888717363609208\n",
      "Average test loss: 0.0022892937457395924\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02882825413842996\n",
      "Average test loss: 0.0022197671594719093\n",
      "Epoch 189/300\n",
      "Average training loss: 0.028833432419432533\n",
      "Average test loss: 0.0023770260852244165\n",
      "Epoch 190/300\n",
      "Average training loss: 0.02892279436190923\n",
      "Average test loss: 0.002425241476442251\n",
      "Epoch 191/300\n",
      "Average training loss: 0.028832031246688632\n",
      "Average test loss: 0.002702788990922272\n",
      "Epoch 192/300\n",
      "Average training loss: 0.028794354655676417\n",
      "Average test loss: 0.005500453758570883\n",
      "Epoch 193/300\n",
      "Average training loss: 0.028838187287251155\n",
      "Average test loss: 0.0021996187826411593\n",
      "Epoch 194/300\n",
      "Average training loss: 0.028668004691600798\n",
      "Average test loss: 0.0025566041427147056\n",
      "Epoch 195/300\n",
      "Average training loss: 0.028739024442103175\n",
      "Average test loss: 0.002222927579656243\n",
      "Epoch 196/300\n",
      "Average training loss: 0.028661184867223105\n",
      "Average test loss: 0.0021845408626314667\n",
      "Epoch 197/300\n",
      "Average training loss: 0.028573112320568826\n",
      "Average test loss: 0.0025035843638082345\n",
      "Epoch 198/300\n",
      "Average training loss: 0.028615561245216265\n",
      "Average test loss: 0.03263828455263542\n",
      "Epoch 199/300\n",
      "Average training loss: 0.02865228960580296\n",
      "Average test loss: 0.0021255674997551575\n",
      "Epoch 200/300\n",
      "Average training loss: 0.02853455995519956\n",
      "Average test loss: 0.0028642135109338495\n",
      "Epoch 201/300\n",
      "Average training loss: 0.028555549759003852\n",
      "Average test loss: 0.002471009504567418\n",
      "Epoch 202/300\n",
      "Average training loss: 0.0285680370926857\n",
      "Average test loss: 0.002684727568800251\n",
      "Epoch 203/300\n",
      "Average training loss: 0.02857418750060929\n",
      "Average test loss: 0.005903701867080397\n",
      "Epoch 204/300\n",
      "Average training loss: 0.028345452263951302\n",
      "Average test loss: 0.00559472187070383\n",
      "Epoch 205/300\n",
      "Average training loss: 0.028309083089232443\n",
      "Average test loss: 0.002600562225199408\n",
      "Epoch 206/300\n",
      "Average training loss: 0.02836459512511889\n",
      "Average test loss: 0.004412272472555438\n",
      "Epoch 207/300\n",
      "Average training loss: 0.028463153648707603\n",
      "Average test loss: 0.002163706704456773\n",
      "Epoch 208/300\n",
      "Average training loss: 0.028276728649934132\n",
      "Average test loss: 0.0024094529474774995\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02842840954164664\n",
      "Average test loss: 0.0024930951663603386\n",
      "Epoch 210/300\n",
      "Average training loss: 0.028271519227160348\n",
      "Average test loss: 0.0021422025852112308\n",
      "Epoch 211/300\n",
      "Average training loss: 0.028228744553195104\n",
      "Average test loss: 0.002158632257125444\n",
      "Epoch 212/300\n",
      "Average training loss: 0.028221732391251458\n",
      "Average test loss: 0.0022881680335849522\n",
      "Epoch 213/300\n",
      "Average training loss: 0.0282719000213676\n",
      "Average test loss: 0.0023347415145900515\n",
      "Epoch 214/300\n",
      "Average training loss: 0.02811331706576877\n",
      "Average test loss: 0.0036622775153567393\n",
      "Epoch 215/300\n",
      "Average training loss: 0.028104927278227276\n",
      "Average test loss: 0.004434172393133243\n",
      "Epoch 216/300\n",
      "Average training loss: 0.028170834581057232\n",
      "Average test loss: 0.0021923114363518025\n",
      "Epoch 217/300\n",
      "Average training loss: 0.028369522116250462\n",
      "Average test loss: 0.002118428455458747\n",
      "Epoch 218/300\n",
      "Average training loss: 0.028087667687071695\n",
      "Average test loss: 0.002343239550375276\n",
      "Epoch 219/300\n",
      "Average training loss: 0.027999403614136907\n",
      "Average test loss: 0.002385397147387266\n",
      "Epoch 220/300\n",
      "Average training loss: 0.027958004703124366\n",
      "Average test loss: 0.0023035831705977517\n",
      "Epoch 221/300\n",
      "Average training loss: 0.027958460473352008\n",
      "Average test loss: 0.0024163766310860714\n",
      "Epoch 222/300\n",
      "Average training loss: 0.028048569970660738\n",
      "Average test loss: 3.5591846589412954\n",
      "Epoch 223/300\n",
      "Average training loss: 0.02804569105969535\n",
      "Average test loss: 0.00913850933396154\n",
      "Epoch 224/300\n",
      "Average training loss: 0.028000876252849898\n",
      "Average test loss: 0.002860159279157718\n",
      "Epoch 225/300\n",
      "Average training loss: 0.027890894310341943\n",
      "Average test loss: 0.002319430659421616\n",
      "Epoch 226/300\n",
      "Average training loss: 0.027957993696133297\n",
      "Average test loss: 0.06765493551724487\n",
      "Epoch 227/300\n",
      "Average training loss: 0.02793071039683289\n",
      "Average test loss: 0.004785396884712908\n",
      "Epoch 228/300\n",
      "Average training loss: 0.028207773538099393\n",
      "Average test loss: 0.0022203359795320366\n",
      "Epoch 229/300\n",
      "Average training loss: 0.02775567361878024\n",
      "Average test loss: 0.0022898468180663056\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02782990968061818\n",
      "Average test loss: 0.002316358105176025\n",
      "Epoch 231/300\n",
      "Average training loss: 0.027738705373472637\n",
      "Average test loss: 0.003599630654272106\n",
      "Epoch 232/300\n",
      "Average training loss: 0.027782675799396302\n",
      "Average test loss: 0.030352615788578986\n",
      "Epoch 233/300\n",
      "Average training loss: 0.02774944469001558\n",
      "Average test loss: 0.002926629634367095\n",
      "Epoch 234/300\n",
      "Average training loss: 0.028161187254720263\n",
      "Average test loss: 0.002732018221790592\n",
      "Epoch 235/300\n",
      "Average training loss: 0.027705834529466098\n",
      "Average test loss: 0.003816064092848036\n",
      "Epoch 236/300\n",
      "Average training loss: 0.027613020105494393\n",
      "Average test loss: 0.01408477904751069\n",
      "Epoch 237/300\n",
      "Average training loss: 0.02760054000880983\n",
      "Average test loss: 0.0021563516314037972\n",
      "Epoch 238/300\n",
      "Average training loss: 0.027714752112825713\n",
      "Average test loss: 0.0022574077223738035\n",
      "Epoch 239/300\n",
      "Average training loss: 0.02772959984673394\n",
      "Average test loss: 0.002252649188041687\n",
      "Epoch 240/300\n",
      "Average training loss: 0.027633976186315218\n",
      "Average test loss: 0.0028072739276621076\n",
      "Epoch 241/300\n",
      "Average training loss: 0.027622641344865164\n",
      "Average test loss: 0.002835606352943513\n",
      "Epoch 242/300\n",
      "Average training loss: 0.027770613739887873\n",
      "Average test loss: 1.0245985011723306\n",
      "Epoch 243/300\n",
      "Average training loss: 0.027573430624273088\n",
      "Average test loss: 0.002482549632899463\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0275578013451563\n",
      "Average test loss: 0.002277346071166297\n",
      "Epoch 245/300\n",
      "Average training loss: 0.02754754530886809\n",
      "Average test loss: 0.0036438393375525873\n",
      "Epoch 246/300\n",
      "Average training loss: 0.027558448452088567\n",
      "Average test loss: 0.0022885371481792794\n",
      "Epoch 247/300\n",
      "Average training loss: 0.027544471389717527\n",
      "Average test loss: 0.0030541322177482975\n",
      "Epoch 248/300\n",
      "Average training loss: 0.027586832015050782\n",
      "Average test loss: 0.009027544463674228\n",
      "Epoch 249/300\n",
      "Average training loss: 0.027418481730752522\n",
      "Average test loss: 0.014336876779380772\n",
      "Epoch 250/300\n",
      "Average training loss: 0.027464511627952257\n",
      "Average test loss: 0.002208263396492435\n",
      "Epoch 251/300\n",
      "Average training loss: 0.027536425459716057\n",
      "Average test loss: 0.013877703136660986\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02735275567902459\n",
      "Average test loss: 0.002166900873184204\n",
      "Epoch 253/300\n",
      "Average training loss: 0.02736229581468635\n",
      "Average test loss: 0.003583665421543022\n",
      "Epoch 254/300\n",
      "Average training loss: 0.027386428044901955\n",
      "Average test loss: 0.0021829408111257685\n",
      "Epoch 255/300\n",
      "Average training loss: 0.027722979951235984\n",
      "Average test loss: 0.002321674708690908\n",
      "Epoch 256/300\n",
      "Average training loss: 0.027268662040432295\n",
      "Average test loss: 0.0025802447484392257\n",
      "Epoch 257/300\n",
      "Average training loss: 0.02727065986394882\n",
      "Average test loss: 0.008140749791430103\n",
      "Epoch 258/300\n",
      "Average training loss: 0.027353673971361583\n",
      "Average test loss: 0.7058995854895976\n",
      "Epoch 259/300\n",
      "Average training loss: 0.027449993977944055\n",
      "Average test loss: 0.002316701188062628\n",
      "Epoch 260/300\n",
      "Average training loss: 0.02725122191177474\n",
      "Average test loss: 0.01558052249542541\n",
      "Epoch 261/300\n",
      "Average training loss: 0.027280751981669002\n",
      "Average test loss: 0.004056104041222069\n",
      "Epoch 262/300\n",
      "Average training loss: 0.027231562068065006\n",
      "Average test loss: 0.0031408658760289352\n",
      "Epoch 263/300\n",
      "Average training loss: 0.027314099359843465\n",
      "Average test loss: 0.002259960488881916\n",
      "Epoch 264/300\n",
      "Average training loss: 0.027261257376935747\n",
      "Average test loss: 0.004744910644160377\n",
      "Epoch 265/300\n",
      "Average training loss: 0.027273168707887332\n",
      "Average test loss: 0.002242241949877805\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02717271883620156\n",
      "Average test loss: 0.002241797077210827\n",
      "Epoch 267/300\n",
      "Average training loss: 0.027277052068048053\n",
      "Average test loss: 0.0022465083396269216\n",
      "Epoch 268/300\n",
      "Average training loss: 0.027194189495510526\n",
      "Average test loss: 0.0021820468962606455\n",
      "Epoch 269/300\n",
      "Average training loss: 0.027088660668995644\n",
      "Average test loss: 0.002599401137067212\n",
      "Epoch 270/300\n",
      "Average training loss: 0.027554044442044363\n",
      "Average test loss: 0.00230037562838859\n",
      "Epoch 271/300\n",
      "Average training loss: 0.027071855636106597\n",
      "Average test loss: 0.0033863515531023345\n",
      "Epoch 272/300\n",
      "Average training loss: 0.027092019740078185\n",
      "Average test loss: 0.00280416880071991\n",
      "Epoch 273/300\n",
      "Average training loss: 0.027070992577407096\n",
      "Average test loss: 0.0022459561417086256\n",
      "Epoch 274/300\n",
      "Average training loss: 0.02699092823929257\n",
      "Average test loss: 0.0024374162221534384\n",
      "Epoch 275/300\n",
      "Average training loss: 0.027327762650118933\n",
      "Average test loss: 0.002140121007959048\n",
      "Epoch 276/300\n",
      "Average training loss: 0.026976503355635537\n",
      "Average test loss: 0.0027214242385493383\n",
      "Epoch 277/300\n",
      "Average training loss: 0.027040736325913004\n",
      "Average test loss: 0.0022189344755477376\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0269778884086344\n",
      "Average test loss: 0.0022432518671784136\n",
      "Epoch 279/300\n",
      "Average training loss: 0.027675781938764785\n",
      "Average test loss: 0.005940441253284613\n",
      "Epoch 280/300\n",
      "Average training loss: 0.026909310155444677\n",
      "Average test loss: 0.002211235063150525\n",
      "Epoch 281/300\n",
      "Average training loss: 0.027002714556124477\n",
      "Average test loss: 0.0024217206264535588\n",
      "Epoch 282/300\n",
      "Average training loss: 0.027016664394074016\n",
      "Average test loss: 0.002263183525453011\n",
      "Epoch 283/300\n",
      "Average training loss: 0.02690592642294036\n",
      "Average test loss: 0.007102450744145446\n",
      "Epoch 284/300\n",
      "Average training loss: 0.026970853433012962\n",
      "Average test loss: 0.07259226729679438\n",
      "Epoch 285/300\n",
      "Average training loss: 0.027096373644140033\n",
      "Average test loss: 0.0027301969904866483\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02683654790620009\n",
      "Average test loss: 0.002905791273133622\n",
      "Epoch 287/300\n",
      "Average training loss: 0.02684983400338226\n",
      "Average test loss: 0.0022440685476693843\n",
      "Epoch 288/300\n",
      "Average training loss: 0.026873180175820986\n",
      "Average test loss: 0.002269900100512637\n",
      "Epoch 289/300\n",
      "Average training loss: 0.02697771717939112\n",
      "Average test loss: 0.0022235620183249315\n",
      "Epoch 290/300\n",
      "Average training loss: 0.02707183960907989\n",
      "Average test loss: 0.0021723628157956734\n",
      "Epoch 291/300\n",
      "Average training loss: 0.026764752502242722\n",
      "Average test loss: 0.0023385098195738262\n",
      "Epoch 292/300\n",
      "Average training loss: 0.026898035870658026\n",
      "Average test loss: 0.0032218489783505597\n",
      "Epoch 293/300\n",
      "Average training loss: 0.026913106630245844\n",
      "Average test loss: 0.00454794819590946\n",
      "Epoch 294/300\n",
      "Average training loss: 0.02672842249770959\n",
      "Average test loss: 0.009586083855893877\n",
      "Epoch 295/300\n",
      "Average training loss: 0.026765341548456088\n",
      "Average test loss: 0.0022221923549142148\n",
      "Epoch 296/300\n",
      "Average training loss: 0.026802565864390795\n",
      "Average test loss: 0.002362672089288632\n",
      "Epoch 297/300\n",
      "Average training loss: 0.02687410670850012\n",
      "Average test loss: 0.0023693144419747922\n",
      "Epoch 298/300\n",
      "Average training loss: 0.02681997026503086\n",
      "Average test loss: 0.002334836601590117\n",
      "Epoch 299/300\n",
      "Average training loss: 0.02669448748230934\n",
      "Average test loss: 0.0022437806938671403\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02684432350926929\n",
      "Average test loss: 0.0022975975372311144\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 4.79528061082628\n",
      "Average test loss: 0.10514723245633974\n",
      "Epoch 2/300\n",
      "Average training loss: 2.5155654430389403\n",
      "Average test loss: 0.004992746433864037\n",
      "Epoch 3/300\n",
      "Average training loss: 1.8656329541736179\n",
      "Average test loss: 0.006198921970609161\n",
      "Epoch 4/300\n",
      "Average training loss: 1.506287184715271\n",
      "Average test loss: 0.00383863321526183\n",
      "Epoch 5/300\n",
      "Average training loss: 1.2795446665022108\n",
      "Average test loss: 0.003635387077720629\n",
      "Epoch 6/300\n",
      "Average training loss: 1.1242053332858615\n",
      "Average test loss: 0.023419325459334586\n",
      "Epoch 7/300\n",
      "Average training loss: 1.0032090509732565\n",
      "Average test loss: 0.0042808598606950705\n",
      "Epoch 8/300\n",
      "Average training loss: 0.9020448817676968\n",
      "Average test loss: 0.0031743171916653714\n",
      "Epoch 9/300\n",
      "Average training loss: 0.8132978930473328\n",
      "Average test loss: 0.0029221783253467744\n",
      "Epoch 10/300\n",
      "Average training loss: 0.7348444559309217\n",
      "Average test loss: 0.002873233352891273\n",
      "Epoch 11/300\n",
      "Average training loss: 0.6663937406539917\n",
      "Average test loss: 0.0028706251171727974\n",
      "Epoch 12/300\n",
      "Average training loss: 0.6018825578689575\n",
      "Average test loss: 0.00267160926759243\n",
      "Epoch 13/300\n",
      "Average training loss: 0.5440512577692668\n",
      "Average test loss: 0.0029955428457922404\n",
      "Epoch 14/300\n",
      "Average training loss: 0.4910229128731622\n",
      "Average test loss: 0.0026647927034646273\n",
      "Epoch 15/300\n",
      "Average training loss: 0.44312721959749857\n",
      "Average test loss: 0.002334555626122488\n",
      "Epoch 16/300\n",
      "Average training loss: 0.39877066538068984\n",
      "Average test loss: 0.004263020233561596\n",
      "Epoch 17/300\n",
      "Average training loss: 0.3578014019595252\n",
      "Average test loss: 0.0022596374756346147\n",
      "Epoch 18/300\n",
      "Average training loss: 0.3195989969041612\n",
      "Average test loss: 0.002159592638206151\n",
      "Epoch 19/300\n",
      "Average training loss: 0.2852496952613195\n",
      "Average test loss: 0.002075404672159089\n",
      "Epoch 20/300\n",
      "Average training loss: 0.25460781587494746\n",
      "Average test loss: 0.0025458023639188874\n",
      "Epoch 21/300\n",
      "Average training loss: 0.22799465896023643\n",
      "Average test loss: 0.002069235708978441\n",
      "Epoch 22/300\n",
      "Average training loss: 0.20487950016392603\n",
      "Average test loss: 0.0032270557458202046\n",
      "Epoch 23/300\n",
      "Average training loss: 0.1844590837160746\n",
      "Average test loss: 0.00272738293144438\n",
      "Epoch 24/300\n",
      "Average training loss: 0.16705740208095973\n",
      "Average test loss: 0.008702768748626112\n",
      "Epoch 25/300\n",
      "Average training loss: 0.15146265982256996\n",
      "Average test loss: 0.0020768993145061864\n",
      "Epoch 26/300\n",
      "Average training loss: 0.13787345441182455\n",
      "Average test loss: 0.0019684032135539585\n",
      "Epoch 27/300\n",
      "Average training loss: 0.12569197029537624\n",
      "Average test loss: 0.0018092696218647891\n",
      "Epoch 28/300\n",
      "Average training loss: 0.11476002967357636\n",
      "Average test loss: 0.0017173145167115663\n",
      "Epoch 29/300\n",
      "Average training loss: 0.1051151928835445\n",
      "Average test loss: 0.0018212022460583183\n",
      "Epoch 30/300\n",
      "Average training loss: 0.0970235961344507\n",
      "Average test loss: 0.0017462012101378706\n",
      "Epoch 31/300\n",
      "Average training loss: 0.08946317162116368\n",
      "Average test loss: 0.0018407426923513412\n",
      "Epoch 32/300\n",
      "Average training loss: 0.08224418775902854\n",
      "Average test loss: 0.0022245385682003365\n",
      "Epoch 33/300\n",
      "Average training loss: 0.07642917102575301\n",
      "Average test loss: 0.0021523059310598505\n",
      "Epoch 34/300\n",
      "Average training loss: 0.07164409298366971\n",
      "Average test loss: 0.0016678548056839242\n",
      "Epoch 35/300\n",
      "Average training loss: 0.0670057806107733\n",
      "Average test loss: 0.00607897897488955\n",
      "Epoch 36/300\n",
      "Average training loss: 0.06281549892160627\n",
      "Average test loss: 0.0015322148950977458\n",
      "Epoch 37/300\n",
      "Average training loss: 0.05916126820776198\n",
      "Average test loss: 0.05120642691436741\n",
      "Epoch 38/300\n",
      "Average training loss: 0.055656962394714356\n",
      "Average test loss: 0.0015037179632733265\n",
      "Epoch 39/300\n",
      "Average training loss: 0.05280951754252116\n",
      "Average test loss: 0.017114006214568184\n",
      "Epoch 40/300\n",
      "Average training loss: 0.05017686613400777\n",
      "Average test loss: 0.004636933795279926\n",
      "Epoch 41/300\n",
      "Average training loss: 0.05104275197784106\n",
      "Average test loss: 0.0016022454408100909\n",
      "Epoch 42/300\n",
      "Average training loss: 0.04602172237303522\n",
      "Average test loss: 0.0015443769408803848\n",
      "Epoch 43/300\n",
      "Average training loss: 0.044213609635829924\n",
      "Average test loss: 0.0014305155743948288\n",
      "Epoch 44/300\n",
      "Average training loss: 0.04259294584393501\n",
      "Average test loss: 0.001544780928020676\n",
      "Epoch 45/300\n",
      "Average training loss: 0.04106433865759108\n",
      "Average test loss: 0.001399186305080851\n",
      "Epoch 46/300\n",
      "Average training loss: 0.03954620256026586\n",
      "Average test loss: 0.008177986538451578\n",
      "Epoch 47/300\n",
      "Average training loss: 0.038094365295436645\n",
      "Average test loss: 0.0015466315825987194\n",
      "Epoch 48/300\n",
      "Average training loss: 0.03719238675302929\n",
      "Average test loss: 0.0013565148101705644\n",
      "Epoch 49/300\n",
      "Average training loss: 0.03468150351776017\n",
      "Average test loss: 0.00137665498494688\n",
      "Epoch 52/300\n",
      "Average training loss: 0.03376724635064602\n",
      "Average test loss: 0.05305572021400763\n",
      "Epoch 53/300\n",
      "Average training loss: 0.0330438971751266\n",
      "Average test loss: 0.0014023429173976184\n",
      "Epoch 54/300\n",
      "Average training loss: 0.032564009832011326\n",
      "Average test loss: 0.001908361956735866\n",
      "Epoch 55/300\n",
      "Average training loss: 0.03211122218436665\n",
      "Average test loss: 0.0013838470034922162\n",
      "Epoch 56/300\n",
      "Average training loss: 0.031620006483462124\n",
      "Average test loss: 0.0014084918905670444\n",
      "Epoch 57/300\n",
      "Average training loss: 0.03130891346434752\n",
      "Average test loss: 0.022624773991604646\n",
      "Epoch 58/300\n",
      "Average training loss: 0.03084781452185578\n",
      "Average test loss: 0.001579459104169574\n",
      "Epoch 59/300\n",
      "Average training loss: 0.030704536822107102\n",
      "Average test loss: 0.001338270122754491\n",
      "Epoch 60/300\n",
      "Average training loss: 0.030837797448039054\n",
      "Average test loss: 0.0015048922817740176\n",
      "Epoch 61/300\n",
      "Average training loss: 0.030250962974296677\n",
      "Average test loss: 0.0013956666958207886\n",
      "Epoch 62/300\n",
      "Average training loss: 0.029539884757664468\n",
      "Average test loss: 0.0013666050634864304\n",
      "Epoch 63/300\n",
      "Average training loss: 0.029115672659542825\n",
      "Average test loss: 0.002032573838614755\n",
      "Epoch 65/300\n",
      "Average training loss: 0.028853491945399178\n",
      "Average test loss: 0.007352267645920316\n",
      "Epoch 66/300\n",
      "Average training loss: 0.028614043608307837\n",
      "Average test loss: 0.0020758168019561303\n",
      "Epoch 67/300\n",
      "Average training loss: 0.028476013554467095\n",
      "Average test loss: 0.001382821205796467\n",
      "Epoch 68/300\n",
      "Average training loss: 0.028322932990060914\n",
      "Average test loss: 0.001476945699399544\n",
      "Epoch 69/300\n",
      "Average training loss: 0.027972299582428403\n",
      "Average test loss: 0.0012985274886919392\n",
      "Epoch 70/300\n",
      "Average training loss: 0.027978497818112374\n",
      "Average test loss: 0.08282275499362085\n",
      "Epoch 71/300\n",
      "Average training loss: 0.027607753300004534\n",
      "Average test loss: 0.2561672695974509\n",
      "Epoch 72/300\n",
      "Average training loss: 0.027542039167549874\n",
      "Average test loss: 0.002944448815865649\n",
      "Epoch 73/300\n",
      "Average training loss: 0.027322632701860534\n",
      "Average test loss: 0.0018072869269591238\n",
      "Epoch 74/300\n",
      "Average training loss: 0.027385471095641455\n",
      "Average test loss: 0.03280035012629297\n",
      "Epoch 75/300\n",
      "Average training loss: 0.028166134433613883\n",
      "Average test loss: 0.0014011344083895286\n",
      "Epoch 77/300\n",
      "Average training loss: 0.027245225661330752\n",
      "Average test loss: 3.327193609127154\n",
      "Epoch 78/300\n",
      "Average training loss: 0.026922433645361\n",
      "Average test loss: 0.0022038168830590117\n",
      "Epoch 79/300\n",
      "Average training loss: 0.026817818178070916\n",
      "Average test loss: 0.0017623419061096178\n",
      "Epoch 80/300\n",
      "Average training loss: 0.026553856020172437\n",
      "Average test loss: 0.0013646052525275283\n",
      "Epoch 81/300\n",
      "Average training loss: 0.02638797308007876\n",
      "Average test loss: 0.0031888454912437332\n",
      "Epoch 82/300\n",
      "Average training loss: 0.026378492027521135\n",
      "Average test loss: 0.0021268481386618483\n",
      "Epoch 83/300\n",
      "Average training loss: 0.026223786521289084\n",
      "Average test loss: 0.001478061397973862\n",
      "Epoch 84/300\n",
      "Average training loss: 0.02617294315993786\n",
      "Average test loss: 1.3934791858552231\n",
      "Epoch 85/300\n",
      "Average training loss: 0.02591411251657539\n",
      "Average test loss: 0.008992529463436868\n",
      "Epoch 87/300\n",
      "Average training loss: 0.026055027921994527\n",
      "Average test loss: 0.005647092686759101\n",
      "Epoch 88/300\n",
      "Average training loss: 0.025610222576393022\n",
      "Average test loss: 0.001469861477613449\n",
      "Epoch 89/300\n",
      "Average training loss: 0.02559208783507347\n",
      "Average test loss: 0.6794934456095927\n",
      "Epoch 90/300\n",
      "Average training loss: 0.025498525985413127\n",
      "Average test loss: 0.0022455796365522676\n",
      "Epoch 91/300\n",
      "Average training loss: 0.025399117117126783\n",
      "Average test loss: 0.001672487985653182\n",
      "Epoch 92/300\n",
      "Average training loss: 0.025273443010118274\n",
      "Average test loss: 0.008947146150800916\n",
      "Epoch 93/300\n",
      "Average training loss: 0.02522845308813784\n",
      "Average test loss: 0.0016188070847549373\n",
      "Epoch 94/300\n",
      "Average training loss: 0.025125103938910695\n",
      "Average test loss: 0.003146526442012853\n",
      "Epoch 95/300\n",
      "Average training loss: 0.02498619303603967\n",
      "Average test loss: 0.0013975010015484359\n",
      "Epoch 96/300\n",
      "Average training loss: 0.024981547532810105\n",
      "Average test loss: 0.011571573637011978\n",
      "Epoch 97/300\n",
      "Average training loss: 0.02485744844708178\n",
      "Average test loss: 0.1764404001554681\n",
      "Epoch 98/300\n",
      "Average training loss: 0.024765047119723427\n",
      "Average test loss: 0.001409709309020804\n",
      "Epoch 99/300\n",
      "Average training loss: 0.024745750397443773\n",
      "Average test loss: 24.87363459279802\n",
      "Epoch 100/300\n",
      "Average training loss: 0.024591169100668695\n",
      "Average test loss: 0.005638382496726182\n",
      "Epoch 102/300\n",
      "Average training loss: 0.024532587514155442\n",
      "Average test loss: 0.2423407019392277\n",
      "Epoch 103/300\n",
      "Average training loss: 0.024514190708597502\n",
      "Average test loss: 0.5257525055938297\n",
      "Epoch 104/300\n",
      "Average training loss: 0.024346475718749892\n",
      "Average test loss: 0.056982136919266645\n",
      "Epoch 105/300\n",
      "Average training loss: 0.024284490409824583\n",
      "Average test loss: 0.17996315392769047\n",
      "Epoch 106/300\n",
      "Average training loss: 0.025063887662357755\n",
      "Average test loss: 0.0014458497582624356\n",
      "Epoch 107/300\n",
      "Average training loss: 0.024292329692178302\n",
      "Average test loss: 0.0047668638344233235\n",
      "Epoch 108/300\n",
      "Average training loss: 0.024049795236852432\n",
      "Average test loss: 0.008339983937227064\n",
      "Epoch 109/300\n",
      "Average training loss: 0.02404705937869019\n",
      "Average test loss: 0.003385694642447763\n",
      "Epoch 110/300\n",
      "Average training loss: 0.024038072789708775\n",
      "Average test loss: 1.8724852770384814\n",
      "Epoch 111/300\n",
      "Average training loss: 0.02394561121530003\n",
      "Average test loss: 0.7585257167783048\n",
      "Epoch 112/300\n",
      "Average training loss: 0.023953432788451513\n",
      "Average test loss: 0.0014060380106998814\n",
      "Epoch 113/300\n",
      "Average training loss: 0.023710217045413123\n",
      "Average test loss: 0.001432293814400004\n",
      "Epoch 116/300\n",
      "Average training loss: 0.023710990086197852\n",
      "Average test loss: 0.0020395390953247745\n",
      "Epoch 117/300\n",
      "Average training loss: 0.02368999778230985\n",
      "Average test loss: 0.005565670032882028\n",
      "Epoch 118/300\n",
      "Average training loss: 0.023609590580066044\n",
      "Average test loss: 0.0015189032564974494\n",
      "Epoch 119/300\n",
      "Average training loss: 0.023609048462576335\n",
      "Average test loss: 0.010016380796519418\n",
      "Epoch 120/300\n",
      "Average training loss: 0.02422167649699582\n",
      "Average test loss: 0.1303948816396296\n",
      "Epoch 121/300\n",
      "Average training loss: 0.023522095693482292\n",
      "Average test loss: 0.0020946818826099237\n",
      "Epoch 122/300\n",
      "Average training loss: 0.02338761699365245\n",
      "Average test loss: 0.0013914797053568894\n",
      "Epoch 123/300\n",
      "Average training loss: 0.023412267876995935\n",
      "Average test loss: 0.07871307007720073\n",
      "Epoch 124/300\n",
      "Average training loss: 0.023316695620616276\n",
      "Average test loss: 0.011033115948239962\n",
      "Epoch 125/300\n",
      "Average training loss: 0.023368709145320788\n",
      "Average test loss: 460.9491839284607\n",
      "Epoch 126/300\n",
      "Average training loss: 0.023278291998638047\n",
      "Average test loss: 0.0014206956133453383\n",
      "Epoch 128/300\n",
      "Average training loss: 0.023194256621930334\n",
      "Average test loss: 198.91833759652243\n",
      "Epoch 129/300\n",
      "Average training loss: 0.023133328571915628\n",
      "Average test loss: 0.002104525913277434\n",
      "Epoch 130/300\n",
      "Average training loss: 0.023146179790298146\n",
      "Average test loss: 1002.2319997297161\n",
      "Epoch 131/300\n",
      "Average training loss: 0.023115300079186756\n",
      "Average test loss: 0.4288014344503689\n",
      "Epoch 132/300\n",
      "Average training loss: 0.023037123794356983\n",
      "Average test loss: 0.0014902693196717236\n",
      "Epoch 133/300\n",
      "Average training loss: 0.023057231416304905\n",
      "Average test loss: 0.010797270780636204\n",
      "Epoch 134/300\n",
      "Average training loss: 0.023008638666735755\n",
      "Average test loss: 0.001753120943903923\n",
      "Epoch 135/300\n",
      "Average training loss: 0.02302424021065235\n",
      "Average test loss: 0.0019757999026527007\n",
      "Epoch 136/300\n",
      "Average training loss: 0.022932208446992768\n",
      "Average test loss: 0.018972895788391016\n",
      "Epoch 137/300\n",
      "Average training loss: 0.022953358886970414\n",
      "Average test loss: 0.22083060404027088\n",
      "Epoch 138/300\n",
      "Average training loss: 0.0235218164531721\n",
      "Average test loss: 0.004608450874893201\n",
      "Epoch 139/300\n",
      "Average training loss: 0.02270793229341507\n",
      "Average test loss: 0.0014234898334058623\n",
      "Epoch 141/300\n",
      "Average training loss: 0.022720850519008108\n",
      "Average test loss: 0.001481636011403882\n",
      "Epoch 142/300\n",
      "Average training loss: 0.022762787515918415\n",
      "Average test loss: 0.0015069441513882744\n",
      "Epoch 143/300\n",
      "Average training loss: 0.022869819096393056\n",
      "Average test loss: 0.09423749278651343\n",
      "Epoch 144/300\n",
      "Average training loss: 0.02273830705549982\n",
      "Average test loss: 0.0022608069634685916\n",
      "Epoch 145/300\n",
      "Average training loss: 0.022537452190286585\n",
      "Average test loss: 3361.0577419314914\n",
      "Epoch 146/300\n",
      "Average training loss: 0.022555092439055444\n",
      "Average test loss: 0.002517449806806528\n",
      "Epoch 147/300\n",
      "Average training loss: 0.024289379499024814\n",
      "Average test loss: 0.0014870629061220421\n",
      "Epoch 148/300\n",
      "Average training loss: 0.022417115754551357\n",
      "Average test loss: 38.80948324926115\n",
      "Epoch 149/300\n",
      "Average training loss: 0.022413862554563416\n",
      "Average test loss: 0.3457880163826048\n",
      "Epoch 150/300\n",
      "Average training loss: 0.022431402560737397\n",
      "Average test loss: 0.007123019362385902\n",
      "Epoch 151/300\n",
      "Average training loss: 0.022556492576996486\n",
      "Average test loss: 0.0032185762946804366\n",
      "Epoch 152/300\n",
      "Average training loss: 0.02236330567797025\n",
      "Average test loss: 0.1895379243410296\n",
      "Epoch 153/300\n",
      "Average training loss: 0.022393799068199265\n",
      "Average test loss: 0.0018971214779756136\n",
      "Epoch 154/300\n",
      "Average training loss: 0.022397049196892314\n",
      "Average test loss: 10.30105070714073\n",
      "Epoch 155/300\n",
      "Average training loss: 0.022236365406049622\n",
      "Average test loss: 0.012749434968042705\n",
      "Epoch 156/300\n",
      "Average training loss: 0.022393187480668226\n",
      "Average test loss: 0.0014752151976443\n",
      "Epoch 157/300\n",
      "Average training loss: 0.022278331269820532\n",
      "Average test loss: 0.1697769970074296\n",
      "Epoch 158/300\n",
      "Average training loss: 0.02227556352979607\n",
      "Average test loss: 1.501619155312164\n",
      "Epoch 159/300\n",
      "Average training loss: 0.02283720549609926\n",
      "Average test loss: 312015.86834125774\n",
      "Epoch 160/300\n",
      "Average training loss: 0.02282639508611626\n",
      "Average training loss: 0.022155817359685897\n",
      "Average test loss: 0.0035311092534619903\n",
      "Epoch 163/300\n",
      "Average training loss: 0.022118912676970165\n",
      "Average test loss: 0.0014521456490167314\n",
      "Epoch 164/300\n",
      "Average training loss: 0.02207400958902306\n",
      "Average test loss: 0.001454058791614241\n",
      "Epoch 165/300\n",
      "Average training loss: 0.022135980405741267\n",
      "Average test loss: 21994.554241776677\n",
      "Epoch 166/300\n",
      "Average training loss: 0.022104247523678674\n",
      "Average test loss: 0.018026435166390405\n",
      "Epoch 167/300\n",
      "Average training loss: 0.022028159136573473\n",
      "Average test loss: 25.311499116284566\n",
      "Epoch 168/300\n",
      "Average training loss: 0.022095966700050564\n",
      "Average test loss: 0.0733624382097688\n",
      "Epoch 169/300\n",
      "Average training loss: 0.021923498093254036\n",
      "Average test loss: 0.04718345465718044\n",
      "Epoch 170/300\n",
      "Average training loss: 0.022115587227874333\n",
      "Average test loss: 0.014602539360109303\n",
      "Epoch 171/300\n",
      "Average training loss: 0.022001118825541604\n",
      "Average test loss: 33346.767851874574\n",
      "Epoch 172/300\n",
      "Average training loss: 0.021946560103032323\n",
      "Average test loss: 89906.25440234375\n",
      "Epoch 173/300\n",
      "Average training loss: 0.021877198421292834\n",
      "Average test loss: 0.010292684477546978\n",
      "Epoch 174/300\n",
      "Average training loss: 0.021825467555059326\n",
      "Average test loss: 94.71544164250842\n",
      "Epoch 177/300\n",
      "Average training loss: 0.026450207471847533\n",
      "Average test loss: 0.0021764308077593646\n",
      "Epoch 178/300\n",
      "Average training loss: 0.022308204844594\n",
      "Average test loss: 0.008342482337728142\n",
      "Epoch 179/300\n",
      "Average training loss: 0.021780223942465253\n",
      "Average test loss: 0.0016400022107279962\n",
      "Epoch 180/300\n",
      "Average training loss: 0.02170704853700267\n",
      "Average test loss: 0.0014984763748943806\n",
      "Epoch 181/300\n",
      "Average training loss: 0.021665229115221236\n",
      "Average test loss: 0.004832001423049305\n",
      "Epoch 182/300\n",
      "Average training loss: 0.021617978268199497\n",
      "Average test loss: 0.0025875186659395696\n",
      "Epoch 183/300\n",
      "Average training loss: 0.02169499146938324\n",
      "Average test loss: 0.018107196798134183\n",
      "Epoch 184/300\n",
      "Average training loss: 0.02166056988471084\n",
      "Average test loss: 0.07737383734352059\n",
      "Epoch 185/300\n",
      "Average training loss: 0.021718511258562407\n",
      "Average test loss: 0.0016139723897601167\n",
      "Epoch 186/300\n",
      "Average training loss: 0.021604115770922767\n",
      "Average test loss: 0.06754065241664647\n",
      "Epoch 188/300\n",
      "Average training loss: 0.02163336308962769\n",
      "Average test loss: 0.002949512771072073\n",
      "Epoch 189/300\n",
      "Average training loss: 0.02167274429069625\n",
      "Average test loss: 0.3989640165153477\n",
      "Epoch 190/300\n",
      "Average training loss: 0.021637346741225985\n",
      "Average test loss: 0.0027175687334189814\n",
      "Epoch 191/300\n",
      "Average training loss: 0.02156376842326588\n",
      "Average test loss: 4121.026321687668\n",
      "Epoch 192/300\n",
      "Average training loss: 0.021508430171344014\n",
      "Average test loss: 41.78278997269604\n",
      "Epoch 193/300\n",
      "Average training loss: 0.021517922366658847\n",
      "Average test loss: 0.002024684024353822\n",
      "Epoch 194/300\n",
      "Average training loss: 0.021574919335544108\n",
      "Average test loss: 0.001445964417937729\n",
      "Epoch 195/300\n",
      "Average training loss: 0.021452320966455673\n",
      "Average test loss: 0.37678545534114044\n",
      "Epoch 196/300\n",
      "Average training loss: 0.02174755994313293\n",
      "Average test loss: 0.016089452183081044\n",
      "Epoch 197/300\n",
      "Average training loss: 0.02149652949637837\n",
      "Average test loss: 0.0023488479627089367\n",
      "Epoch 198/300\n",
      "Average training loss: 0.021431173166467084\n",
      "Average test loss: 0.0015194090517858664\n",
      "Epoch 199/300\n",
      "Average training loss: 0.021377610988087124\n",
      "Average test loss: 0.0016049540429893467\n",
      "Epoch 202/300\n",
      "Average training loss: 0.021451834993229973\n",
      "Average test loss: 0.0021801043359769715\n",
      "Epoch 203/300\n",
      "Average training loss: 0.021323247565163506\n",
      "Average test loss: 0.001811164844988121\n",
      "Epoch 204/300\n",
      "Average training loss: 0.021409874276982415\n",
      "Average test loss: 0.0034716493568072715\n",
      "Epoch 205/300\n",
      "Average training loss: 0.021367058341701824\n",
      "Average test loss: 0.0015081648744332294\n",
      "Epoch 206/300\n",
      "Average training loss: 0.021383305369979805\n",
      "Average test loss: 0.001722543484220902\n",
      "Epoch 207/300\n",
      "Average training loss: 0.02128520713084274\n",
      "Average test loss: 0.2690325156354035\n",
      "Epoch 208/300\n",
      "Average training loss: 0.022413539515601263\n",
      "Average test loss: 0.0016190769515103765\n",
      "Epoch 209/300\n",
      "Average training loss: 0.02114724083079232\n",
      "Average test loss: 0.002571421974959473\n",
      "Epoch 210/300\n",
      "Average training loss: 0.021224788856175212\n",
      "Average test loss: 0.04343373081998693\n",
      "Epoch 211/300\n",
      "Average training loss: 0.02119696051047908\n",
      "Average test loss: 0.0014958226825628016\n",
      "Epoch 212/300\n",
      "Average training loss: 0.02130429268380006\n",
      "Average test loss: 0.0015529498531379634\n",
      "Epoch 213/300\n",
      "Average training loss: 0.02131807397140397\n",
      "Average test loss: 0.0014864340572514469\n",
      "Epoch 214/300\n",
      "Average training loss: 0.021118929730521308\n",
      "Average test loss: 0.0054967393652639454\n",
      "Epoch 217/300\n",
      "Average training loss: 0.02110895041624705\n",
      "Average test loss: 356.2168303259487\n",
      "Epoch 218/300\n",
      "Average training loss: 0.02170553519162867\n",
      "Average test loss: 1974.480330173303\n",
      "Epoch 219/300\n",
      "Average training loss: 0.021120082181360987\n",
      "Average test loss: 0.00396153970538742\n",
      "Epoch 220/300\n",
      "Average training loss: 0.021531958982348443\n",
      "Average test loss: 0.0018469740717361371\n",
      "Epoch 221/300\n",
      "Average training loss: 0.0210880437526438\n",
      "Average test loss: 0.0021657749525167878\n",
      "Epoch 222/300\n",
      "Average training loss: 0.021055520253049003\n",
      "Average test loss: 0.002660495223891404\n",
      "Epoch 223/300\n",
      "Average training loss: 0.021090733705295458\n",
      "Average test loss: 0.3815706241356416\n",
      "Epoch 224/300\n",
      "Average training loss: 0.021141820541686483\n",
      "Average test loss: 0.39211297289240693\n",
      "Epoch 225/300\n",
      "Average training loss: 0.021083242371678353\n",
      "Average test loss: 0.0015473572571451466\n",
      "Epoch 226/300\n",
      "Average training loss: 0.021012629977530905\n",
      "Average test loss: 0.0014806867754086851\n",
      "Epoch 227/300\n",
      "Average training loss: 0.021879088691539234\n",
      "Average test loss: 0.004472530346777704\n",
      "Epoch 228/300\n",
      "Average training loss: 0.021115129412876237\n",
      "Average test loss: 0.0015049230782315135\n",
      "Epoch 229/300\n",
      "Average training loss: 0.021026012198792562\n",
      "Average test loss: 0.0016887193080037833\n",
      "Epoch 230/300\n",
      "Average training loss: 0.02091011643740866\n",
      "Average test loss: 0.1613038136077424\n",
      "Epoch 231/300\n",
      "Average training loss: 0.020921355523996883\n",
      "Average test loss: 1.2743553727294008\n",
      "Epoch 232/300\n",
      "Average training loss: 0.020935656507809958\n",
      "Average test loss: 0.0024430267388621967\n",
      "Epoch 233/300\n",
      "Average training loss: 0.020950055761469737\n",
      "Average test loss: 0.5757328539503117\n",
      "Epoch 234/300\n",
      "Average training loss: 0.020944277505079906\n",
      "Average test loss: 0.0021461427637065452\n",
      "Epoch 235/300\n",
      "Average training loss: 0.02088255143331157\n",
      "Average test loss: 1.594564851547281\n",
      "Epoch 236/300\n",
      "Average training loss: 0.020901060968637468\n",
      "Average test loss: 0.005839411465451121\n",
      "Epoch 237/300\n",
      "Average training loss: 0.020888807025220658\n",
      "Average test loss: 137.67409428137128\n",
      "Epoch 238/300\n",
      "Average training loss: 0.020928221883045304\n",
      "Average test loss: 0.7935231783927108\n",
      "Epoch 239/300\n",
      "Average training loss: 0.020830468078454335\n",
      "Average test loss: 135.85979196278504\n",
      "Epoch 240/300\n",
      "Average training loss: 0.020869315958685346\n",
      "Average test loss: 0.0015553279235545133\n",
      "Epoch 241/300\n",
      "Average training loss: 0.02088627646366755\n",
      "Average test loss: 3.7736128150175015\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0209093702054686\n",
      "Average test loss: 0.8535000449857778\n",
      "Epoch 243/300\n",
      "Average training loss: 0.020784917338026895\n",
      "Average test loss: 0.0024673310845262472\n",
      "Epoch 244/300\n",
      "Average training loss: 0.0207724220007658\n",
      "Average test loss: 0.002973465937707159\n",
      "Epoch 245/300\n",
      "Average training loss: 0.020875396014915573\n",
      "Average test loss: 0.0028832270941800543\n",
      "Epoch 246/300\n",
      "Average training loss: 0.020788046214315625\n",
      "Average test loss: 0.002568133041262627\n",
      "Epoch 247/300\n",
      "Average training loss: 0.020801436442467902\n",
      "Average test loss: 0.0061941940946918395\n",
      "Epoch 248/300\n",
      "Average training loss: 0.020770545214414595\n",
      "Average test loss: 0.0014671211494132876\n",
      "Epoch 249/300\n",
      "Average training loss: 0.021293630338377424\n",
      "Average test loss: 50.99602635475538\n",
      "Epoch 250/300\n",
      "Average training loss: 0.020725134698881043\n",
      "Average test loss: 0.01823553682449791\n",
      "Epoch 251/300\n",
      "Average training loss: 0.02070618001288838\n",
      "Average test loss: 0.002064251345064905\n",
      "Epoch 252/300\n",
      "Average training loss: 0.02068646691242854\n",
      "Average test loss: 0.0015576493495868312\n",
      "Epoch 253/300\n",
      "Average training loss: 0.020712120819422933\n",
      "Average test loss: 3221.5451296166316\n",
      "Epoch 254/300\n",
      "Average training loss: 0.02212885330286291\n",
      "Average test loss: 0.006612674654771884\n",
      "Epoch 255/300\n",
      "Average training loss: 0.020663733446763623\n",
      "Average test loss: 0.04623760093179428\n",
      "Epoch 256/300\n",
      "Average training loss: 0.02064849386612574\n",
      "Average test loss: 0.00318485805992451\n",
      "Epoch 257/300\n",
      "Average training loss: 0.020608160658015146\n",
      "Average test loss: 0.0025919454311951994\n",
      "Epoch 258/300\n",
      "Average training loss: 0.0206069948706362\n",
      "Average test loss: 4.699544157069591\n",
      "Epoch 259/300\n",
      "Average training loss: 0.020655009554492103\n",
      "Average test loss: 0.08786315327282582\n",
      "Epoch 260/300\n",
      "Average training loss: 0.020690314539604716\n",
      "Average test loss: 0.005488056747035848\n",
      "Epoch 261/300\n",
      "Average training loss: 0.02057539170814885\n",
      "Average test loss: 0.01632385867782351\n",
      "Epoch 262/300\n",
      "Average training loss: 0.020675934323006205\n",
      "Average test loss: 2.3876583813130856\n",
      "Epoch 263/300\n",
      "Average training loss: 0.020637306776311662\n",
      "Average test loss: 0.2106033206826283\n",
      "Epoch 264/300\n",
      "Average training loss: 0.0205632424917486\n",
      "Average test loss: 0.0026082982422990933\n",
      "Epoch 265/300\n",
      "Average training loss: 0.021050090587801402\n",
      "Average test loss: 0.0022016828465792867\n",
      "Epoch 266/300\n",
      "Average training loss: 0.02054679213795397\n",
      "Average test loss: 0.0018598735689496\n",
      "Epoch 267/300\n",
      "Average training loss: 0.02058589248193635\n",
      "Average test loss: 0.001533293397993677\n",
      "Epoch 268/300\n",
      "Average training loss: 0.020515048566791746\n",
      "Average test loss: 0.7933781674127612\n",
      "Epoch 269/300\n",
      "Average training loss: 0.020529554789264996\n",
      "Average test loss: 0.09252209973004129\n",
      "Epoch 270/300\n",
      "Average training loss: 0.020510520805915197\n",
      "Average test loss: 0.0015347876953375008\n",
      "Epoch 271/300\n",
      "Average training loss: 0.020574005249473785\n",
      "Average test loss: 0.0022277561384253203\n",
      "Epoch 272/300\n",
      "Average training loss: 0.02045667280836238\n",
      "Average test loss: 0.002554517904917399\n",
      "Epoch 273/300\n",
      "Average training loss: 0.020487229430841074\n",
      "Average test loss: 0.0021775778761754434\n",
      "Epoch 274/300\n",
      "Average training loss: 0.020511021117369334\n",
      "Average test loss: 0.0036601902805268763\n",
      "Epoch 275/300\n",
      "Average training loss: 0.02048090255757173\n",
      "Average test loss: 0.9941238449729152\n",
      "Epoch 276/300\n",
      "Average training loss: 0.02047076538370715\n",
      "Average test loss: 4.36560833082762\n",
      "Epoch 277/300\n",
      "Average training loss: 0.020480285609761874\n",
      "Average test loss: 0.0015221618698495957\n",
      "Epoch 278/300\n",
      "Average training loss: 0.021320694574051433\n",
      "Average test loss: 0.1750746814393335\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0204049956202507\n",
      "Average test loss: 9779190.69870319\n",
      "Epoch 280/300\n",
      "Average training loss: 0.020425559166404934\n",
      "Average test loss: 0.0061169091792156296\n",
      "Epoch 281/300\n",
      "Average training loss: 0.0203286984331078\n",
      "Average test loss: 0.002311890930765205\n",
      "Epoch 282/300\n",
      "Average training loss: 0.020435710963275698\n",
      "Average test loss: 0.09136921893060207\n",
      "Epoch 283/300\n",
      "Average training loss: 0.0204009799675809\n",
      "Average test loss: 0.0015100422602974707\n",
      "Epoch 284/300\n",
      "Average training loss: 0.020640738940901227\n",
      "Average test loss: 0.0026423954196895163\n",
      "Epoch 285/300\n",
      "Average training loss: 0.02093849231302738\n",
      "Average test loss: 0.0015191755317565468\n",
      "Epoch 286/300\n",
      "Average training loss: 0.02035557546797726\n",
      "Average test loss: 0.001644441802572045\n",
      "Epoch 287/300\n",
      "Average training loss: 0.020358663549025855\n",
      "Average test loss: 0.0016864876257669595\n",
      "Epoch 288/300\n",
      "Average training loss: 0.020346107468836836\n",
      "Average test loss: 0.0018229931017590894\n",
      "Epoch 289/300\n",
      "Average training loss: 0.020544052125679124\n",
      "Average test loss: 0.0017314902918620242\n",
      "Epoch 290/300\n",
      "Average training loss: 0.030818574857380654\n",
      "Average test loss: 0.003003995658415887\n",
      "Epoch 291/300\n",
      "Average training loss: 0.03167632098330392\n",
      "Average test loss: 0.0014309772793720994\n",
      "Epoch 292/300\n",
      "Average training loss: 0.02463910473552015\n",
      "Average test loss: 0.0015287092741992738\n",
      "Epoch 293/300\n",
      "Average training loss: 0.022195481985807418\n",
      "Average test loss: 0.0017944380758951107\n",
      "Epoch 294/300\n",
      "Average training loss: 0.021127337646153238\n",
      "Average test loss: 0.0017424350712034438\n",
      "Epoch 295/300\n",
      "Average training loss: 0.02064607178999318\n",
      "Average test loss: 0.005774532285415464\n",
      "Epoch 296/300\n",
      "Average training loss: 0.020377941376633114\n",
      "Average test loss: 0.0016392914446898632\n",
      "Epoch 297/300\n",
      "Average training loss: 0.020351770902673403\n",
      "Average test loss: 0.11399558336981055\n",
      "Epoch 298/300\n",
      "Average training loss: 0.020323335877723163\n",
      "Average test loss: 0.0017600159665776623\n",
      "Epoch 299/300\n",
      "Average training loss: 0.0202476111965047\n",
      "Average test loss: 0.00945596564312776\n",
      "Epoch 300/300\n",
      "Average training loss: 0.02047683955066734\n",
      "Average test loss: 0.00541002648199598\n"
     ]
    }
   ],
   "source": [
    "# 15 Projections\n",
    "num_projections = 15\n",
    "\n",
    "# Folder Path for 15 Projections\n",
    "proj15_path = 'Memory_Residual-Additive_Depth10-.01/15 Projections'\n",
    "\n",
    "gauss_10_proj15_weights, gauss_10_proj15_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj15', display=True)\n",
    "\n",
    "gauss_20_proj15_weights, gauss_20_proj15_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj15', display=True)\n",
    "\n",
    "gauss_30_proj15_weights, gauss_30_proj15_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj15', display=True)\n",
    "\n",
    "gauss_40_proj15_weights, gauss_40_proj15_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj15', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj15_path, gauss_10_proj15_weights, gauss_10_proj15_hist, '10% Sampling')\n",
    "save_progress(proj15_path, gauss_20_proj15_weights, gauss_20_proj15_hist, '20% Sampling')\n",
    "save_progress(proj15_path, gauss_30_proj15_weights, gauss_30_proj15_hist, '30% Sampling')\n",
    "save_progress(proj15_path, gauss_40_proj15_weights, gauss_40_proj15_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a23e5-a890-4149-86ee-5c0b3641cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 18.95\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.43\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.64\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.45\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 23.91\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.79\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 25.22\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.48\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.83\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 26.03\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 26.23\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 26.69\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 26.88\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 26.98\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.15\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.28\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.07\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.67\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.23\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.75\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.17\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.73\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.89\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 29.13\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.57\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.79\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 23.46\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.35\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 27.87\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 28.80\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 29.95\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 30.36\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 30.67\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.85\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 31.00\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.66\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 31.02\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 31.30\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.48\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.57\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.93\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 26.27\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 29.90\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 30.86\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 31.34\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 31.86\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 32.06\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 32.26\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 32.50\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 32.66\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 32.75\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 32.92\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 33.02\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 33.10\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj15_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj15_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj15_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj15_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj15_psnr = average_PSNR(gauss_10_proj15_model, gauss_10_proj15_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj15_psnr = average_PSNR(gauss_20_proj15_model, gauss_20_proj15_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj15_psnr = average_PSNR(gauss_30_proj15_model, gauss_30_proj15_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj15_psnr = average_PSNR(gauss_40_proj15_model, gauss_40_proj15_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj15_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj15_psnr, f)\n",
    "with open(os.path.join(proj15_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj15_psnr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59306cd4-c86e-4d10-84d3-3e1f00c84f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 41.80465545993381\n",
      "Average test loss: 0.027380966463022762\n",
      "Epoch 2/300\n",
      "Average training loss: 18.38431677585178\n",
      "Average test loss: 0.03248022295203474\n",
      "Epoch 3/300\n",
      "Average training loss: 12.70553933207194\n",
      "Average test loss: 14.102252251916461\n",
      "Epoch 4/300\n",
      "Average training loss: 8.915941240098741\n",
      "Average test loss: 0.04537667570677068\n",
      "Epoch 5/300\n",
      "Average training loss: 7.112725441826714\n",
      "Average test loss: 2.816195452047719\n",
      "Epoch 6/300\n",
      "Average training loss: 6.1548004578484425\n",
      "Average test loss: 0.01214336854716142\n",
      "Epoch 7/300\n",
      "Average training loss: 4.956765889909533\n",
      "Average test loss: 1.427789199716515\n",
      "Epoch 8/300\n",
      "Average training loss: 4.542490794711643\n",
      "Average test loss: 138.58228542594114\n",
      "Epoch 9/300\n",
      "Average training loss: 4.10451442888048\n",
      "Average test loss: 0.03119078525652488\n",
      "Epoch 10/300\n",
      "Average training loss: 3.9308577325608995\n",
      "Average test loss: 200998.91211582502\n",
      "Epoch 11/300\n",
      "Average training loss: 3.398588973151313\n",
      "Average test loss: 0.008474527017937767\n",
      "Epoch 12/300\n",
      "Average training loss: 2.8081039492289226\n",
      "Average test loss: 0.020749957309828863\n",
      "Epoch 13/300\n",
      "Average training loss: 1.9653302345275878\n",
      "Average test loss: 20.729371283676063\n",
      "Epoch 15/300\n",
      "Average training loss: 1.7274334513346354\n",
      "Average test loss: 1.5277613276437754\n",
      "Epoch 16/300\n",
      "Average training loss: 1.5076805312898425\n",
      "Average test loss: 160.90912680398506\n",
      "Epoch 17/300\n",
      "Average training loss: 1.2995482145945232\n",
      "Average test loss: 0.011340229358938005\n",
      "Epoch 18/300\n",
      "Average training loss: 1.1430637016296388\n",
      "Average test loss: 0.017147684829930464\n",
      "Epoch 19/300\n",
      "Average training loss: 1.0192609928978813\n",
      "Average test loss: 0.007731941656106048\n",
      "Epoch 20/300\n",
      "Average training loss: 0.9147217483520508\n",
      "Average test loss: 0.008019970531264941\n",
      "Epoch 21/300\n",
      "Average training loss: 0.8117950921588474\n",
      "Average test loss: 0.007418462831940916\n",
      "Epoch 22/300\n",
      "Average training loss: 0.726931807200114\n",
      "Average test loss: 0.006539723733647002\n",
      "Epoch 23/300\n",
      "Average training loss: 0.6516561522483826\n",
      "Average test loss: 0.006113911223080424\n",
      "Epoch 24/300\n",
      "Average training loss: 0.5887960324287415\n",
      "Average test loss: 0.006533570502781205\n",
      "Epoch 25/300\n",
      "Average training loss: 0.5341743084589641\n",
      "Average test loss: 0.005837496782342593\n",
      "Epoch 26/300\n",
      "Average training loss: 0.48683750269148085\n",
      "Average test loss: 0.006555705005509986\n",
      "Epoch 27/300\n",
      "Average training loss: 0.41713773565822176\n",
      "Average test loss: 4.986771086202728\n",
      "Epoch 29/300\n",
      "Average training loss: 0.3856903469297621\n",
      "Average test loss: 0.006469966932303376\n",
      "Epoch 30/300\n",
      "Average training loss: 0.3605176810423533\n",
      "Average test loss: 0.005840031712419457\n",
      "Epoch 31/300\n",
      "Average training loss: 0.33982910556263396\n",
      "Average test loss: 0.00567774315054218\n",
      "Epoch 32/300\n",
      "Average training loss: 0.3219265927208794\n",
      "Average test loss: 1.6612914633386664\n",
      "Epoch 33/300\n",
      "Average training loss: 0.3013551369773017\n",
      "Average test loss: 0.006187896284378237\n",
      "Epoch 34/300\n",
      "Average training loss: 0.2891774560213089\n",
      "Average test loss: 0.0055695260953572065\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2770132457945082\n",
      "Average test loss: 0.005591205768287182\n",
      "Epoch 36/300\n",
      "Average training loss: 0.2663644891050127\n",
      "Average test loss: 0.007334826631678475\n",
      "Epoch 37/300\n",
      "Average training loss: 0.25618919282489355\n",
      "Average test loss: 0.005695456040816175\n",
      "Epoch 38/300\n",
      "Average training loss: 0.24788743646939596\n",
      "Average test loss: 0.005271592709753249\n",
      "Epoch 39/300\n",
      "Average training loss: 0.24280210496319665\n",
      "Average test loss: 0.00559038505413466\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2283977964454227\n",
      "Average test loss: 0.025482870028250747\n",
      "Epoch 42/300\n",
      "Average training loss: 0.22397424246205225\n",
      "Average test loss: 0.006290238814221488\n",
      "Epoch 43/300\n",
      "Average training loss: 0.21782869642310673\n",
      "Average test loss: 0.005857489270054632\n",
      "Epoch 44/300\n",
      "Average training loss: 0.21420543564690483\n",
      "Average test loss: 0.00579613320570853\n",
      "Epoch 45/300\n",
      "Average training loss: 0.20848208341333602\n",
      "Average test loss: 0.5484597669409381\n",
      "Epoch 46/300\n",
      "Average training loss: 0.20546924817562104\n",
      "Average test loss: 0.006847109284251928\n",
      "Epoch 47/300\n",
      "Average training loss: 0.20918621016873254\n",
      "Average test loss: 0.008530661279128658\n",
      "Epoch 48/300\n",
      "Average training loss: 0.19698496834437051\n",
      "Average test loss: 0.0056836331979268125\n",
      "Epoch 49/300\n",
      "Average training loss: 0.1941328757206599\n",
      "Average test loss: 0.06661845609380139\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1920121786461936\n",
      "Average test loss: 181.22702602101367\n",
      "Epoch 51/300\n",
      "Average training loss: 0.19048145690229204\n",
      "Average test loss: 0.005504529760529598\n",
      "Epoch 52/300\n",
      "Average training loss: 0.18411229549513922\n",
      "Average test loss: 0.0050545595400035385\n",
      "Epoch 53/300\n",
      "Average training loss: 0.17831887518035042\n",
      "Average test loss: 0.023255690614382426\n",
      "Epoch 55/300\n",
      "Average training loss: 0.17795554224650065\n",
      "Average test loss: 0.006832055781036616\n",
      "Epoch 56/300\n",
      "Average training loss: 0.17517508198155296\n",
      "Average test loss: 0.005130537327378989\n",
      "Epoch 57/300\n",
      "Average training loss: 0.17218494676219093\n",
      "Average test loss: 0.0060906378566804855\n",
      "Epoch 58/300\n",
      "Average training loss: 0.17095797273847793\n",
      "Average test loss: 0.1646323036601146\n",
      "Epoch 59/300\n",
      "Average test loss: 0.007446531133105358\n",
      "Epoch 60/300\n",
      "Average training loss: 0.26992397220929465\n",
      "Average test loss: 0.0051636704119543235\n",
      "Epoch 61/300\n",
      "Average training loss: 0.23333745437198214\n",
      "Average test loss: 0.005106803492125537\n",
      "Epoch 62/300\n",
      "Average training loss: 0.21746031637324229\n",
      "Average test loss: 0.008456536788079474\n",
      "Epoch 63/300\n",
      "Average training loss: 0.20880132557286157\n",
      "Average test loss: 0.005790693163457844\n",
      "Epoch 64/300\n",
      "Average training loss: 0.19961879436175028\n",
      "Average test loss: 0.005392531772454579\n",
      "Epoch 65/300\n",
      "Average training loss: 0.19524944758415222\n",
      "Average test loss: 0.009479892791145379\n",
      "Epoch 66/300\n",
      "Average training loss: 0.18848233399126266\n",
      "Average test loss: 0.20337182648976643\n",
      "Epoch 67/300\n",
      "Average training loss: 0.18555345653163063\n",
      "Average test loss: 0.005589533478849464\n",
      "Epoch 68/300\n",
      "Average training loss: 0.18304537494977316\n",
      "Average test loss: 0.005081561950138874\n",
      "Epoch 69/300\n",
      "Average training loss: 0.18150591962867313\n",
      "Average test loss: 0.005707207198875646\n",
      "Epoch 70/300\n",
      "Average training loss: 0.17803324629200828\n",
      "Average test loss: 0.0049723374721490675\n",
      "Epoch 71/300\n",
      "Average training loss: 0.17466249227523803\n",
      "Average test loss: 0.0050906213081131375\n",
      "Epoch 72/300\n",
      "Average training loss: 0.16974849806891548\n",
      "Average test loss: 0.005014271023372809\n",
      "Epoch 74/300\n",
      "Average training loss: 0.1685875479910109\n",
      "Average test loss: 0.005522853470096985\n",
      "Epoch 75/300\n",
      "Average training loss: 0.16745079351796044\n",
      "Average test loss: 0.12460338148143556\n",
      "Epoch 76/300\n",
      "Average training loss: 0.16642394351296955\n",
      "Average test loss: 0.008356545394907395\n",
      "Epoch 77/300\n",
      "Average training loss: 0.16484193053510454\n",
      "Average test loss: 0.005819943581604295\n",
      "Epoch 78/300\n",
      "Average training loss: 0.16279493509398565\n",
      "Average test loss: 0.00510280165862706\n",
      "Epoch 79/300\n",
      "Average training loss: 0.16146690213680268\n",
      "Average test loss: 0.00524921984370384\n",
      "Epoch 80/300\n",
      "Average training loss: 0.15955022037029265\n",
      "Average test loss: 0.005742184489551517\n",
      "Epoch 81/300\n",
      "Average training loss: 0.15842744370301565\n",
      "Average test loss: 0.005182904918160703\n",
      "Epoch 82/300\n",
      "Average training loss: 0.15788196109400854\n",
      "Average test loss: 0.004984128148605427\n",
      "Epoch 83/300\n",
      "Average training loss: 0.156044955306583\n",
      "Average test loss: 0.005076024789777067\n",
      "Epoch 84/300\n",
      "Average training loss: 0.1559620947043101\n",
      "Average test loss: 0.04073247539831532\n",
      "Epoch 85/300\n",
      "Average training loss: 0.15380610092480979\n",
      "Average test loss: 0.005044557959669166\n",
      "Epoch 86/300\n",
      "Average training loss: 0.15282016633616555\n",
      "Average test loss: 0.009497335368146499\n",
      "Epoch 87/300\n",
      "Average training loss: 0.15411279275019962\n",
      "Average test loss: 0.005089203276154068\n",
      "Epoch 88/300\n",
      "Average training loss: 0.15066514270835452\n",
      "Average test loss: 0.005104028572431869\n",
      "Epoch 89/300\n",
      "Average training loss: 0.1496034283041954\n",
      "Average test loss: 0.006134781975713041\n",
      "Epoch 90/300\n",
      "Average training loss: 0.14902089789178635\n",
      "Average test loss: 0.005046687632385228\n",
      "Epoch 91/300\n",
      "Average training loss: 0.14726289754443697\n",
      "Average test loss: 0.005111914338750972\n",
      "Epoch 92/300\n",
      "Average training loss: 0.14743210877312554\n",
      "Average test loss: 0.06980721149179671\n",
      "Epoch 93/300\n",
      "Average training loss: 0.14636172976758746\n",
      "Average test loss: 0.006094403535955482\n",
      "Epoch 94/300\n",
      "Average training loss: 5.875737701641189\n",
      "Average test loss: 1005645816676978.2\n",
      "Epoch 96/300\n",
      "Average training loss: 2.6372762627071804\n",
      "Average test loss: 52881890387893.36\n",
      "Epoch 98/300\n",
      "Average training loss: 1.912373355653551\n",
      "Average test loss: 5760569.074181705\n",
      "Epoch 99/300\n",
      "Average training loss: 1.5004330223931206\n",
      "Average test loss: 3.3340275482533537e+22\n",
      "Epoch 100/300\n",
      "Average training loss: 1.1628694391250611\n",
      "Average test loss: 2096269661739.152\n",
      "Epoch 101/300\n",
      "Average training loss: 0.8173472293747795\n",
      "Average test loss: 2567809.4459557673\n",
      "Epoch 102/300\n",
      "Average training loss: 0.6023826215532091\n",
      "Average test loss: 913579067.1015929\n",
      "Epoch 103/300\n",
      "Average training loss: 0.472134573009279\n",
      "Average test loss: 54150.00225317307\n",
      "Epoch 104/300\n",
      "Average training loss: 0.34153751582569547\n",
      "Average test loss: 8209542219.20913\n",
      "Epoch 106/300\n",
      "Average training loss: 0.30344130052460566\n",
      "Average test loss: 190305559649.48627\n",
      "Epoch 107/300\n",
      "Average training loss: 0.282521482653088\n",
      "Average test loss: 105685994.62005751\n",
      "Epoch 108/300\n",
      "Average training loss: 0.25722895153363545\n",
      "Average test loss: 2049.7817542145285\n",
      "Epoch 109/300\n",
      "Average training loss: 0.2425149998135037\n",
      "Average test loss: 1020.5125356334754\n",
      "Epoch 110/300\n",
      "Average training loss: 0.23409202002154456\n",
      "Average test loss: 45223.776241244545\n",
      "Epoch 111/300\n",
      "Average training loss: 22.08891876220703\n",
      "Average test loss: 116843.71784922961\n",
      "Epoch 113/300\n",
      "Average training loss: 16.706690056694878\n",
      "Average test loss: 918.7057829807202\n",
      "Epoch 114/300\n",
      "Average training loss: 13.168258149041069\n",
      "Average test loss: 54.300236449135674\n",
      "Epoch 115/300\n",
      "Average training loss: 10.518862931993272\n",
      "Average test loss: 0.05556535782251093\n",
      "Epoch 116/300\n",
      "Average training loss: 8.941996626112196\n",
      "Average test loss: 0.37347029673556487\n",
      "Epoch 117/300\n",
      "Average training loss: 7.584661783006456\n",
      "Average test loss: 17.839205765373176\n",
      "Epoch 118/300\n",
      "Average training loss: 6.3013083551194935\n",
      "Average training loss: 5.083293450249566\n",
      "Average test loss: 0.009576066926949555\n",
      "Epoch 120/300\n",
      "Average training loss: 4.075730299207899\n",
      "Average test loss: 0.005954108436695403\n",
      "Epoch 121/300\n",
      "Average training loss: 3.255019873301188\n",
      "Average test loss: 0.7872312534385257\n",
      "Epoch 122/300\n",
      "Average training loss: 2.6147161784701876\n",
      "Average test loss: 0.007067612702647845\n",
      "Epoch 123/300\n",
      "Average training loss: 2.100202640745375\n",
      "Average test loss: 0.009123477074007194\n",
      "Epoch 124/300\n",
      "Average training loss: 1.6794777608447604\n",
      "Average test loss: 0.24389781780623726\n",
      "Epoch 125/300\n",
      "Average test loss: 0.005509744216170576\n",
      "Epoch 126/300\n",
      "Average training loss: 1.118624184926351\n",
      "Average test loss: 0.019017898021058903\n",
      "Epoch 127/300\n",
      "Average training loss: 0.9101908116870456\n",
      "Average test loss: 7.533437098693517\n",
      "Epoch 128/300\n",
      "Average training loss: 0.7456864462428623\n",
      "Average test loss: 1.113016788566692\n",
      "Epoch 129/300\n",
      "Average training loss: 0.6169771304130555\n",
      "Average test loss: 42.12605399734444\n",
      "Epoch 130/300\n",
      "Average training loss: 0.5252762649854025\n",
      "Average test loss: 785.1259399082975\n",
      "Epoch 131/300\n",
      "Average training loss: 0.3952493635018667\n",
      "Average test loss: 18162.86475788566\n",
      "Epoch 133/300\n",
      "Average training loss: 0.36602228567335343\n",
      "Average test loss: 4.904907868907269\n",
      "Epoch 134/300\n",
      "Average training loss: 0.33482712933752273\n",
      "Average test loss: 3.4776134248992636\n",
      "Epoch 135/300\n",
      "Average training loss: 0.2936999985906813\n",
      "Average test loss: 4.309163756498032\n",
      "Epoch 136/300\n",
      "Average training loss: 0.27346794743008085\n",
      "Average test loss: 2.736150285963797\n",
      "Epoch 137/300\n",
      "Average training loss: 0.2588271918826633\n",
      "Average test loss: 50215.755214921024\n",
      "Epoch 138/300\n",
      "Average training loss: 0.2430770826207267\n",
      "Average test loss: 0.005944183453089661\n",
      "Epoch 139/300\n",
      "Average training loss: 0.22378851885265774\n",
      "Average test loss: 0.005091168889568912\n",
      "Epoch 141/300\n",
      "Average training loss: 0.21549123311042787\n",
      "Average test loss: 0.005573183746180601\n",
      "Epoch 142/300\n",
      "Average training loss: 0.20268088274531895\n",
      "Average test loss: 0.005117580315305127\n",
      "Epoch 144/300\n",
      "Average training loss: 0.1971694449186325\n",
      "Average test loss: 0.005537266314443615\n",
      "Epoch 145/300\n",
      "Average training loss: 0.19054592465029824\n",
      "Average test loss: 0.0052635599544478785\n",
      "Epoch 146/300\n",
      "Average training loss: 0.18593859648704528\n",
      "Average test loss: 0.0050908993029346065\n",
      "Epoch 147/300\n",
      "Average training loss: 0.18181179469161562\n",
      "Average test loss: 0.011597329948304428\n",
      "Epoch 148/300\n",
      "Average training loss: 0.1772418997420205\n",
      "Average test loss: 0.0050203318951858415\n",
      "Epoch 149/300\n",
      "Average training loss: 0.17567091801431445\n",
      "Average test loss: 0.005317114774137735\n",
      "Epoch 151/300\n",
      "Average training loss: 0.16690647417969173\n",
      "Average test loss: 0.0050167793561187056\n",
      "Epoch 152/300\n",
      "Average training loss: 0.16549633697668711\n",
      "Average test loss: 0.00519437920426329\n",
      "Epoch 153/300\n",
      "Average training loss: 0.16346664955880907\n",
      "Average test loss: 0.0050566863264474605\n",
      "Epoch 154/300\n",
      "Average training loss: 0.16169525695509382\n",
      "Average test loss: 0.016726539957854482\n",
      "Epoch 155/300\n",
      "Average training loss: 0.15934226961930592\n",
      "Average test loss: 0.0057622455395758155\n",
      "Epoch 156/300\n",
      "Average training loss: 0.16242364980777105\n",
      "Average test loss: 0.005135265056043863\n",
      "Epoch 157/300\n",
      "Average training loss: 0.1562646549012926\n",
      "Average test loss: 0.005079267583787441\n",
      "Epoch 158/300\n",
      "Average training loss: 0.15443417833911047\n",
      "Average test loss: 0.0053697205819189546\n",
      "Epoch 159/300\n",
      "Average training loss: 0.1538012139664756\n",
      "Average test loss: 0.005266375111829903\n",
      "Epoch 160/300\n",
      "Average training loss: 0.15481645945707956\n",
      "Average test loss: 0.07503747287723753\n",
      "Epoch 161/300\n",
      "Average training loss: 0.15224467080169254\n",
      "Average test loss: 299.12856844423544\n",
      "Epoch 162/300\n",
      "Average training loss: 0.15307134828302596\n",
      "Average test loss: 0.07245574183927642\n",
      "Epoch 163/300\n",
      "Average training loss: 0.1496131538550059\n",
      "Average test loss: 0.005277637035068538\n",
      "Epoch 164/300\n",
      "Average training loss: 0.14786207710372076\n",
      "Average test loss: 2.265210281514459\n",
      "Epoch 165/300\n",
      "Average training loss: 0.14625898636711968\n",
      "Average test loss: 2.4088282555118203\n",
      "Epoch 167/300\n",
      "Average training loss: 0.1453671558962928\n",
      "Average test loss: 0.0052768859321044555\n",
      "Epoch 168/300\n",
      "Average training loss: 0.14507174829641978\n",
      "Average test loss: 0.008134447874294386\n",
      "Epoch 169/300\n",
      "Average training loss: 0.1452073044379552\n",
      "Average test loss: 0.005435988427864181\n",
      "Epoch 170/300\n",
      "Average training loss: 0.1438803979423311\n",
      "Average test loss: 0.005649422067321009\n",
      "Epoch 171/300\n",
      "Average training loss: 93.27018761886491\n",
      "Average test loss: 4474718263029.976\n",
      "Epoch 172/300\n",
      "Average training loss: 4.757620059967041\n",
      "Average test loss: 4100557458171.0605\n",
      "Epoch 174/300\n",
      "Average training loss: 3.78587867503696\n",
      "Average test loss: 68867244904.9031\n",
      "Epoch 175/300\n",
      "Average training loss: 3.2353243435753716\n",
      "Average test loss: 28700220452523.51\n",
      "Epoch 176/300\n",
      "Average training loss: 2.8595443856981064\n",
      "Average test loss: 31625861388308.53\n",
      "Epoch 177/300\n",
      "Average training loss: 2.4315598176320394\n",
      "Average training loss: 1.7199462728500365\n",
      "Average test loss: 251.34911060008386\n",
      "Epoch 180/300\n",
      "Average training loss: 1.4539265784157647\n",
      "Average test loss: 708855.1471286933\n",
      "Epoch 181/300\n",
      "Average training loss: 1.2528111207750108\n",
      "Average test loss: 1528.761340017981\n",
      "Epoch 182/300\n",
      "Average training loss: 1.0774284245173136\n",
      "Average test loss: 0.008004629921168089\n",
      "Epoch 183/300\n",
      "Average training loss: 0.9381045388645596\n",
      "Average test loss: 0.005433787990361452\n",
      "Epoch 184/300\n",
      "Average training loss: 0.8271807889938354\n",
      "Average test loss: 8049306224639.153\n",
      "Epoch 185/300\n",
      "Average training loss: 0.7843143408033583\n",
      "Average test loss: 335337.71070122364\n",
      "Epoch 187/300\n",
      "Average training loss: 0.6531909875869751\n",
      "Average test loss: 122416.43100628699\n",
      "Epoch 188/300\n",
      "Average training loss: 0.559706339041392\n",
      "Average test loss: 0.009076343860063289\n",
      "Epoch 189/300\n",
      "Average training loss: 0.3815328110853831\n",
      "Average test loss: 0.005115249085757468\n",
      "Epoch 191/300\n",
      "Average training loss: 559.6504330440628\n",
      "Average test loss: 9623214663106.34\n",
      "Epoch 192/300\n",
      "Average training loss: 9.962911253187391\n",
      "Average test loss: 3939.9244470070735\n",
      "Epoch 193/300\n",
      "Average training loss: 7.464142503102621\n",
      "Average test loss: 0.11409358110734158\n",
      "Epoch 194/300\n",
      "Average training loss: 6.219829006195068\n",
      "Average test loss: 0.006986971878343158\n",
      "Epoch 195/300\n",
      "Average training loss: 5.414488753848605\n",
      "Average test loss: 1.3378284936249256\n",
      "Epoch 196/300\n",
      "Average training loss: 4.319058120727539\n",
      "Average test loss: 0.011726530121846332\n",
      "Epoch 198/300\n",
      "Average training loss: 3.8846054117414686\n",
      "Average test loss: 0.013496685058706336\n",
      "Epoch 199/300\n",
      "Average training loss: 3.48760192489624\n",
      "Average test loss: 0.006219067373623451\n",
      "Epoch 200/300\n",
      "Average training loss: 3.121225618362427\n",
      "Average test loss: 0.005920167543821865\n",
      "Epoch 201/300\n",
      "Average training loss: 2.785434190750122\n",
      "Average test loss: 112722138.87007532\n",
      "Epoch 202/300\n",
      "Average training loss: 2.468057183795505\n",
      "Average test loss: 0.0056618995563023624\n",
      "Epoch 203/300\n",
      "Average training loss: 2.1776424740685356\n",
      "Average test loss: 0.006349572875433498\n",
      "Epoch 204/300\n",
      "Average training loss: 1.8889373905393811\n",
      "Average test loss: 1.0863096211320824\n",
      "Epoch 205/300\n",
      "Average training loss: 1.4361536119249132\n",
      "Average test loss: 0.005905702489945624\n",
      "Epoch 207/300\n",
      "Average training loss: 1.2385543100568983\n",
      "Average test loss: 153.21580333169965\n",
      "Epoch 208/300\n",
      "Average training loss: 1.0708254920641582\n",
      "Average test loss: 0.00536721138159434\n",
      "Epoch 209/300\n",
      "Average training loss: 0.931587508254581\n",
      "Average test loss: 0.03017749018677407\n",
      "Epoch 210/300\n",
      "Average training loss: 0.8101786909633213\n",
      "Average test loss: 0.01896406400534842\n",
      "Epoch 211/300\n",
      "Average training loss: 0.7057803841167026\n",
      "Average test loss: 0.00523909335086743\n",
      "Epoch 212/300\n",
      "Average training loss: 0.6163559720251295\n",
      "Average test loss: 0.008396903878284826\n",
      "Epoch 213/300\n",
      "Average training loss: 0.5309413864877489\n",
      "Average test loss: 0.005356328897592094\n",
      "Epoch 214/300\n",
      "Average training loss: 0.4582001985973782\n",
      "Average test loss: 0.0053203248298830455\n",
      "Epoch 215/300\n",
      "Average training loss: 0.3929748179912567\n",
      "Average test loss: 0.11926058825684918\n",
      "Epoch 216/300\n",
      "Average training loss: 0.3379448000854916\n",
      "Average test loss: 0.005077544190610448\n",
      "Epoch 217/300\n",
      "Average training loss: 0.35174216254552204\n",
      "Average test loss: 0.010693113282736805\n",
      "Epoch 218/300\n",
      "Average training loss: 0.28640150512589346\n",
      "Average test loss: 0.03210089488989777\n",
      "Epoch 219/300\n",
      "Average training loss: 0.26420634447203745\n",
      "Average test loss: 0.005067952448295223\n",
      "Epoch 220/300\n",
      "Average training loss: 0.24662198201815289\n",
      "Average test loss: 0.005204780283901427\n",
      "Epoch 221/300\n",
      "Average training loss: 0.2325564081536399\n",
      "Average test loss: 0.005272148996591568\n",
      "Epoch 222/300\n",
      "Average training loss: 0.22156934016280705\n",
      "Average test loss: 0.005222320854249928\n",
      "Epoch 223/300\n",
      "Average training loss: 0.23806104675928752\n",
      "Average test loss: 0.005138744342658255\n",
      "Epoch 224/300\n",
      "Average training loss: 0.21004706105921003\n",
      "Average test loss: 0.005199321193413602\n",
      "Epoch 225/300\n",
      "Average training loss: 0.1945645523733563\n",
      "Average test loss: 0.005497437917109993\n",
      "Epoch 226/300\n",
      "Average training loss: 0.18642776662773555\n",
      "Average test loss: 0.005083254080679681\n",
      "Epoch 227/300\n",
      "Average training loss: 0.1813517051405377\n",
      "Average test loss: 0.005197109742297067\n",
      "Epoch 228/300\n",
      "Average training loss: 0.1770982475148307\n",
      "Average test loss: 0.0062253594117032155\n",
      "Epoch 229/300\n",
      "Average training loss: 0.18639108811484442\n",
      "Average test loss: 0.00508908811956644\n",
      "Epoch 230/300\n",
      "Average training loss: 0.17197252402040694\n",
      "Average test loss: 0.0051614148459500735\n",
      "Epoch 231/300\n",
      "Average training loss: 0.16594408334626093\n",
      "Average test loss: 0.01457694452504317\n",
      "Epoch 232/300\n",
      "Average training loss: 0.163359722243415\n",
      "Average test loss: 0.00517334175730745\n",
      "Epoch 233/300\n",
      "Average training loss: 0.1640950593021181\n",
      "Average test loss: 0.006639345708820555\n",
      "Epoch 234/300\n",
      "Average training loss: 0.15855968305799698\n",
      "Average test loss: 0.005233956790839632\n",
      "Epoch 235/300\n",
      "Average training loss: 0.15454868369632296\n",
      "Average test loss: 0.005250852261152532\n",
      "Epoch 236/300\n",
      "Average training loss: 0.1548317900498708\n",
      "Average test loss: 0.01056700822421246\n",
      "Epoch 237/300\n",
      "Average training loss: 0.15131951529449886\n",
      "Average test loss: 0.005187643819799026\n",
      "Epoch 238/300\n",
      "Average training loss: 0.14940840617815654\n",
      "Average test loss: 0.2932255405055152\n",
      "Epoch 239/300\n",
      "Average training loss: 0.14906186729007298\n",
      "Average test loss: 0.005170725168453323\n",
      "Epoch 240/300\n",
      "Average training loss: 0.148967937707901\n",
      "Average test loss: 0.005371031807528601\n",
      "Epoch 241/300\n",
      "Average training loss: 0.15734694033198887\n",
      "Average test loss: 0.030500461736487017\n",
      "Epoch 242/300\n",
      "Average training loss: 0.1485603479411867\n",
      "Average test loss: 0.005232279823886024\n",
      "Epoch 243/300\n",
      "Average training loss: 1317.3690796808005\n",
      "Average test loss: 14.964952191056478\n",
      "Epoch 244/300\n",
      "Average training loss: 17.205220303005643\n",
      "Average test loss: 33.69461365041716\n",
      "Epoch 245/300\n",
      "Average training loss: 13.728576295640734\n",
      "Average test loss: 0.01664809471865495\n",
      "Epoch 246/300\n",
      "Average training loss: 11.530755514356825\n",
      "Average test loss: 0.03536234805650181\n",
      "Epoch 247/300\n",
      "Average training loss: 9.796093171013727\n",
      "Average test loss: 0.018834361222883067\n",
      "Epoch 248/300\n",
      "Average training loss: 8.439460202111139\n",
      "Average test loss: 0.0068380312017268605\n",
      "Epoch 249/300\n",
      "Average training loss: 7.365965670267741\n",
      "Average test loss: 0.32266318884160783\n",
      "Epoch 250/300\n",
      "Average training loss: 6.550018716600206\n",
      "Average test loss: 1.387435539662838\n",
      "Epoch 251/300\n",
      "Average training loss: 5.838396568298339\n",
      "Average test loss: 10.590979432491793\n",
      "Epoch 252/300\n",
      "Average training loss: 5.194316714392768\n",
      "Average test loss: 0.006241132777184248\n",
      "Epoch 253/300\n",
      "Average training loss: 4.596160758548312\n",
      "Average test loss: 0.423738864434676\n",
      "Epoch 254/300\n",
      "Average training loss: 4.014586914698283\n",
      "Average test loss: 2.859439682504369\n",
      "Epoch 255/300\n",
      "Average training loss: 3.4864973572625053\n",
      "Average test loss: 8.38127964392263\n",
      "Epoch 256/300\n",
      "Average training loss: 3.008562866422865\n",
      "Average test loss: 75.04747461822505\n",
      "Epoch 257/300\n",
      "Average training loss: 2.573615844514635\n",
      "Average test loss: 22.185078086071545\n",
      "Epoch 258/300\n",
      "Average training loss: 2.155901575512356\n",
      "Average test loss: 60402.28718118625\n",
      "Epoch 259/300\n",
      "Average training loss: 1.7735613334443834\n",
      "Average test loss: 2.224607272134887\n",
      "Epoch 260/300\n",
      "Average training loss: 1.461545768207974\n",
      "Average test loss: 8945.881635723035\n",
      "Epoch 261/300\n",
      "Average training loss: 1.209266169336107\n",
      "Average test loss: 13.525952803981387\n",
      "Epoch 262/300\n",
      "Average training loss: 0.9957831248177422\n",
      "Average test loss: 0.07669493631190724\n",
      "Epoch 263/300\n",
      "Average training loss: 0.8043664686944749\n",
      "Average test loss: 0.005358342698878712\n",
      "Epoch 264/300\n",
      "Average training loss: 0.6361052028867934\n",
      "Average test loss: 0.16852169548802906\n",
      "Epoch 265/300\n",
      "Average training loss: 0.5192003995312585\n",
      "Average test loss: 30.236015719883973\n",
      "Epoch 266/300\n",
      "Average training loss: 0.44216328610314265\n",
      "Average test loss: 0.00522226022142503\n",
      "Epoch 267/300\n",
      "Average training loss: 0.38073312611050075\n",
      "Average test loss: 9652.828259209919\n",
      "Epoch 268/300\n",
      "Average training loss: 0.336205776002672\n",
      "Average test loss: 6.009447591260075\n",
      "Epoch 269/300\n",
      "Average training loss: 0.368465951455964\n",
      "Average test loss: 262919452.04726827\n",
      "Epoch 270/300\n",
      "Average training loss: 0.28141636771625944\n",
      "Average test loss: 0.005194068138384157\n",
      "Epoch 271/300\n",
      "Average training loss: 0.2595725570254856\n",
      "Average test loss: 0.00988587925169203\n",
      "Epoch 272/300\n",
      "Average training loss: 0.2419158426920573\n",
      "Average test loss: 0.005168104660593801\n",
      "Epoch 273/300\n",
      "Average training loss: 0.2267033569018046\n",
      "Average test loss: 320.38167712253\n",
      "Epoch 274/300\n",
      "Average training loss: 0.23684105571111044\n",
      "Average test loss: 0.5416148372623656\n",
      "Epoch 275/300\n",
      "Average training loss: 0.20232637482219273\n",
      "Average test loss: 0.005192462382631169\n",
      "Epoch 276/300\n",
      "Average training loss: 0.195249302983284\n",
      "Average test loss: 0.005409771995411979\n",
      "Epoch 277/300\n",
      "Average training loss: 0.6305546913279427\n",
      "Average test loss: 2777814226.0168014\n",
      "Epoch 278/300\n",
      "Average training loss: 0.4570668207539452\n",
      "Average test loss: 848.5993598858495\n",
      "Epoch 279/300\n",
      "Average training loss: 0.29563407190640767\n",
      "Average test loss: 38069.96774747698\n",
      "Epoch 280/300\n",
      "Average training loss: 0.25150898310873243\n",
      "Average test loss: 2529.002451670592\n",
      "Epoch 281/300\n",
      "Average training loss: 0.22483502876758577\n",
      "Average test loss: 0.013859597817684213\n",
      "Epoch 282/300\n",
      "Average training loss: 0.21104583823680878\n",
      "Average test loss: 0.034318002943777376\n",
      "Epoch 283/300\n",
      "Average training loss: 0.3053605040576723\n",
      "Average test loss: 0.20049682961404325\n",
      "Epoch 284/300\n",
      "Average training loss: 0.21824266052246094\n",
      "Average test loss: 0.00532126441018449\n",
      "Epoch 285/300\n",
      "Average training loss: 0.19004067952103085\n",
      "Average test loss: 0.005066907862408294\n",
      "Epoch 286/300\n",
      "Average training loss: 0.18201329400804309\n",
      "Average test loss: 0.005079232538946801\n",
      "Epoch 287/300\n",
      "Average training loss: 0.17779067099094392\n",
      "Average test loss: 0.005232999061544737\n",
      "Epoch 288/300\n",
      "Average training loss: 0.1838904211388694\n",
      "Average test loss: 0.007838911251889335\n",
      "Epoch 289/300\n",
      "Average training loss: 0.16855097479290432\n",
      "Average test loss: 0.005812136656294266\n",
      "Epoch 290/300\n",
      "Average training loss: 0.1732937340868844\n",
      "Average test loss: 0.005845508663190736\n",
      "Epoch 291/300\n",
      "Average training loss: 247.58810088740455\n",
      "Average test loss: 0.2124396929430465\n",
      "Epoch 292/300\n",
      "Average training loss: 7.272951939053006\n",
      "Average test loss: 0.0074225216139521865\n",
      "Epoch 293/300\n",
      "Average training loss: 5.012920068105061\n",
      "Average test loss: 0.006283002484175894\n",
      "Epoch 294/300\n",
      "Average training loss: 3.94536028819614\n",
      "Average test loss: 0.006531259613318576\n",
      "Epoch 295/300\n",
      "Average training loss: 3.236132872051663\n",
      "Average test loss: 0.005939106716050042\n",
      "Epoch 296/300\n",
      "Average training loss: 2.6706779634687634\n",
      "Average test loss: 0.007264223945223623\n",
      "Epoch 297/300\n",
      "Average training loss: 2.1522714360555013\n",
      "Average test loss: 0.005488080439468225\n",
      "Epoch 298/300\n",
      "Average training loss: 1.6678890443378025\n",
      "Average test loss: 0.009876593951963716\n",
      "Epoch 299/300\n",
      "Average training loss: 1.3144183481004503\n",
      "Average test loss: 0.020460611153807904\n",
      "Epoch 300/300\n",
      "Average training loss: 1.0552915771272446\n",
      "Average test loss: 66.66725532027085\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 31.268100802951388\n",
      "Average test loss: 74.09959327735504\n",
      "Epoch 2/300\n",
      "Average training loss: 16.55118756612142\n",
      "Average test loss: 9.602992474813428\n",
      "Epoch 3/300\n",
      "Average training loss: 13.331851975335015\n",
      "Average test loss: 0.610785134765837\n",
      "Epoch 4/300\n",
      "Average training loss: 11.312976921081543\n",
      "Average test loss: 0.007503950813578235\n",
      "Epoch 5/300\n",
      "Average training loss: 9.979387068854438\n",
      "Average test loss: 8.176486671255695\n",
      "Epoch 6/300\n",
      "Average training loss: 8.360680275387233\n",
      "Average test loss: 5.859594660989113\n",
      "Epoch 7/300\n",
      "Average training loss: 7.014325047387017\n",
      "Average test loss: 465.3088607274393\n",
      "Epoch 8/300\n",
      "Average training loss: 5.932051482306586\n",
      "Average test loss: 0.006551235632763969\n",
      "Epoch 9/300\n",
      "Average training loss: 5.335569073571099\n",
      "Average test loss: 0.16603006895838512\n",
      "Epoch 10/300\n",
      "Average training loss: 4.667930411020914\n",
      "Average test loss: 0.5646918142504163\n",
      "Epoch 11/300\n",
      "Average training loss: 4.143821426179674\n",
      "Average test loss: 50.17413069811794\n",
      "Epoch 12/300\n",
      "Average training loss: 3.5285687113867867\n",
      "Average test loss: 0.006478735373665889\n",
      "Epoch 13/300\n",
      "Average training loss: 3.0477956216600206\n",
      "Average test loss: 0.09236734451767471\n",
      "Epoch 14/300\n",
      "Average training loss: 2.5959638527764213\n",
      "Average test loss: 0.004602188263916307\n",
      "Epoch 15/300\n",
      "Average training loss: 2.3740364004770913\n",
      "Average test loss: 0.021802915517654686\n",
      "Epoch 16/300\n",
      "Average training loss: 2.067172122531467\n",
      "Average test loss: 4.25235792416003\n",
      "Epoch 17/300\n",
      "Average training loss: 1.8311413208643594\n",
      "Average test loss: 0.007540844050960408\n",
      "Epoch 18/300\n",
      "Average training loss: 1.6069417341020371\n",
      "Average test loss: 0.05140068688036667\n",
      "Epoch 19/300\n",
      "Average training loss: 1.4340893081029256\n",
      "Average test loss: 0.08510311768783463\n",
      "Epoch 20/300\n",
      "Average training loss: 1.269741313510471\n",
      "Average test loss: 0.00380544550716877\n",
      "Epoch 21/300\n",
      "Average training loss: 1.1541699760225084\n",
      "Average test loss: 0.004359437439590693\n",
      "Epoch 22/300\n",
      "Average training loss: 1.04028164768219\n",
      "Average test loss: 0.004130137382282151\n",
      "Epoch 23/300\n",
      "Average training loss: 0.9430413109461466\n",
      "Average test loss: 0.0036438170979834266\n",
      "Epoch 24/300\n",
      "Average training loss: 0.8529395671950446\n",
      "Average test loss: 0.003474579265755084\n",
      "Epoch 25/300\n",
      "Average training loss: 0.7673218125237359\n",
      "Average test loss: 0.003919963035939469\n",
      "Epoch 26/300\n",
      "Average training loss: 0.694108513408237\n",
      "Average test loss: 0.003573727195875512\n",
      "Epoch 27/300\n",
      "Average training loss: 0.6262078462176853\n",
      "Average test loss: 0.0037066709817283683\n",
      "Epoch 28/300\n",
      "Average training loss: 0.5659529996977912\n",
      "Average test loss: 0.0032158897382517657\n",
      "Epoch 29/300\n",
      "Average training loss: 0.5099774864514669\n",
      "Average test loss: 0.003234844460669491\n",
      "Epoch 30/300\n",
      "Average training loss: 0.46038249603907266\n",
      "Average test loss: 0.003922746442672279\n",
      "Epoch 31/300\n",
      "Average training loss: 0.41607654560936824\n",
      "Average test loss: 0.0033300321700258386\n",
      "Epoch 32/300\n",
      "Average training loss: 0.3760331161022186\n",
      "Average test loss: 0.0031685935920104384\n",
      "Epoch 33/300\n",
      "Average training loss: 0.341147092183431\n",
      "Average test loss: 0.0031970266554918556\n",
      "Epoch 34/300\n",
      "Average training loss: 0.31092742196718853\n",
      "Average test loss: 0.0038589289825823572\n",
      "Epoch 35/300\n",
      "Average training loss: 0.2847519698937734\n",
      "Average test loss: 0.003143443116918206\n",
      "Epoch 36/300\n",
      "Average training loss: 0.26448314657476213\n",
      "Average test loss: 0.004470216566489802\n",
      "Epoch 37/300\n",
      "Average training loss: 0.24503407657146453\n",
      "Average test loss: 0.0030474678575992584\n",
      "Epoch 38/300\n",
      "Average training loss: 0.22794529712200165\n",
      "Average test loss: 0.005174503419962194\n",
      "Epoch 39/300\n",
      "Average training loss: 0.21509010881847807\n",
      "Average test loss: 0.00337674867009951\n",
      "Epoch 40/300\n",
      "Average training loss: 0.2032709049516254\n",
      "Average test loss: 0.0031497095569761263\n",
      "Epoch 41/300\n",
      "Average training loss: 0.19148250042067633\n",
      "Average test loss: 3690.666299675103\n",
      "Epoch 42/300\n",
      "Average training loss: 0.18666182822651334\n",
      "Average test loss: 0.02308579148352146\n",
      "Epoch 43/300\n",
      "Average training loss: 0.1738304369714525\n",
      "Average test loss: 0.0030125589420398075\n",
      "Epoch 44/300\n",
      "Average training loss: 0.16710118491782083\n",
      "Average test loss: 0.0031061950545344086\n",
      "Epoch 45/300\n",
      "Average training loss: 0.16038144997093412\n",
      "Average test loss: 0.003499619891660081\n",
      "Epoch 46/300\n",
      "Average training loss: 0.15324419586526022\n",
      "Average test loss: 0.30055853002932337\n",
      "Epoch 47/300\n",
      "Average training loss: 0.14789832635058298\n",
      "Average test loss: 0.0031177251492109565\n",
      "Epoch 48/300\n",
      "Average training loss: 0.1427520775463846\n",
      "Average test loss: 0.6790652242263572\n",
      "Epoch 49/300\n",
      "Average training loss: 0.13949505793386036\n",
      "Average test loss: 0.0028838158916268085\n",
      "Epoch 50/300\n",
      "Average training loss: 0.1356908585164282\n",
      "Average test loss: 0.0029520052172657516\n",
      "Epoch 51/300\n",
      "Average training loss: 0.13096809183888966\n",
      "Average test loss: 0.00517158653255966\n",
      "Epoch 52/300\n",
      "Average training loss: 0.12674824334515467\n",
      "Average test loss: 0.004589682995652159\n",
      "Epoch 53/300\n",
      "Average training loss: 0.12538958852158652\n",
      "Average test loss: 0.00426587390444345\n",
      "Epoch 54/300\n",
      "Average training loss: 0.1209548029171096\n",
      "Average test loss: 0.0028558470441235437\n",
      "Epoch 55/300\n",
      "Average training loss: 0.26609055059485964\n",
      "Average test loss: 0.003273537953487701\n",
      "Epoch 56/300\n",
      "Average training loss: 0.15338310417864057\n",
      "Average test loss: 0.0031348322298791675\n",
      "Epoch 57/300\n",
      "Average training loss: 0.13757641622755262\n",
      "Average test loss: 0.0039745680731203825\n",
      "Epoch 58/300\n",
      "Average training loss: 0.13017820853657192\n",
      "Average test loss: 0.0030422734564377204\n",
      "Epoch 59/300\n",
      "Average training loss: 0.12450159157647027\n",
      "Average test loss: 0.002901962664185299\n",
      "Epoch 60/300\n",
      "Average training loss: 0.12023772586054272\n",
      "Average test loss: 0.007261064915193452\n",
      "Epoch 61/300\n",
      "Average training loss: 0.11719661904705896\n",
      "Average test loss: 0.002948667275408904\n",
      "Epoch 62/300\n",
      "Average training loss: 0.11430333529578315\n",
      "Average test loss: 0.0028596866174290577\n",
      "Epoch 63/300\n",
      "Average training loss: 0.11211594115363227\n",
      "Average test loss: 0.009491432057072719\n",
      "Epoch 64/300\n",
      "Average training loss: 0.11015454773108165\n",
      "Average test loss: 0.002908110281659497\n",
      "Epoch 65/300\n",
      "Average training loss: 0.11016672570837868\n",
      "Average test loss: 0.0028916549413568445\n",
      "Epoch 66/300\n",
      "Average training loss: 0.10672925743791792\n",
      "Average test loss: 0.002862379376673036\n",
      "Epoch 67/300\n",
      "Average training loss: 0.10567989871899287\n",
      "Average test loss: 0.0029264140799641607\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10425092762708664\n",
      "Average test loss: 0.0030093607294062776\n",
      "Epoch 69/300\n",
      "Average training loss: 0.11413648341761695\n",
      "Average test loss: 0.006627325053844187\n",
      "Epoch 70/300\n",
      "Average training loss: 0.10526062482595444\n",
      "Average test loss: 0.0029860188751998874\n",
      "Epoch 71/300\n",
      "Average training loss: 0.10234790068202548\n",
      "Average test loss: 0.002895047683475746\n",
      "Epoch 72/300\n",
      "Average training loss: 0.10062243369552824\n",
      "Average test loss: 0.09440590911772516\n",
      "Epoch 73/300\n",
      "Average training loss: 0.0991036462717586\n",
      "Average test loss: 0.00296095352826847\n",
      "Epoch 74/300\n",
      "Average training loss: 0.12258604006634818\n",
      "Average test loss: 0.0031116194884396263\n",
      "Epoch 75/300\n",
      "Average training loss: 0.11585183833042781\n",
      "Average test loss: 0.005693801377796464\n",
      "Epoch 76/300\n",
      "Average training loss: 0.27167219265302023\n",
      "Average test loss: 0.05843099843959013\n",
      "Epoch 77/300\n",
      "Average training loss: 0.1487548643483056\n",
      "Average test loss: 0.0029777720775455234\n",
      "Epoch 78/300\n",
      "Average training loss: 0.12989816049072478\n",
      "Average test loss: 0.0030501194360355536\n",
      "Epoch 79/300\n",
      "Average training loss: 0.12078592175907558\n",
      "Average test loss: 0.0029101717660410535\n",
      "Epoch 80/300\n",
      "Average training loss: 0.11453131820758183\n",
      "Average test loss: 0.0031470526160879268\n",
      "Epoch 81/300\n",
      "Average training loss: 0.11083793485164642\n",
      "Average test loss: 0.0064253997521268\n",
      "Epoch 82/300\n",
      "Average training loss: 0.10810588766468895\n",
      "Average test loss: 0.002835308571656545\n",
      "Epoch 83/300\n",
      "Average training loss: 0.10605009338590835\n",
      "Average test loss: 0.002889656689431932\n",
      "Epoch 84/300\n",
      "Average training loss: 0.10465869182348252\n",
      "Average test loss: 0.0028236571161283385\n",
      "Epoch 85/300\n",
      "Average training loss: 0.1031285160349475\n",
      "Average test loss: 0.0029465612603558435\n",
      "Epoch 86/300\n",
      "Average training loss: 0.10210637169414097\n",
      "Average test loss: 0.17813544877701334\n",
      "Epoch 87/300\n",
      "Average training loss: 0.10088439407613542\n",
      "Average test loss: 0.002996098585633768\n",
      "Epoch 88/300\n",
      "Average training loss: 0.10006265621052848\n",
      "Average test loss: 0.0030324564407476122\n",
      "Epoch 89/300\n",
      "Average training loss: 0.31294681396749285\n",
      "Average test loss: 7044782.247777778\n",
      "Epoch 90/300\n",
      "Average training loss: 0.16612263382805717\n",
      "Average test loss: 0.003079929601815012\n",
      "Epoch 91/300\n",
      "Average training loss: 0.13298120619853337\n",
      "Average test loss: 0.002992770300143295\n",
      "Epoch 92/300\n",
      "Average training loss: 0.12180214883883794\n",
      "Average test loss: 0.002926181168605884\n",
      "Epoch 93/300\n",
      "Average training loss: 0.11592853537533018\n",
      "Average test loss: 0.0029185294926994377\n",
      "Epoch 94/300\n",
      "Average training loss: 0.1124062223566903\n",
      "Average test loss: 0.0029003744303352304\n",
      "Epoch 95/300\n",
      "Average training loss: 0.10948808725012674\n",
      "Average test loss: 0.0029788553304970263\n",
      "Epoch 96/300\n",
      "Average training loss: 0.10641709399885602\n",
      "Average test loss: 0.0028620765070534416\n",
      "Epoch 97/300\n",
      "Average training loss: 0.10487606650590897\n",
      "Average test loss: 0.0028387024499889877\n",
      "Epoch 98/300\n",
      "Average training loss: 0.10265791255235672\n",
      "Average test loss: 0.002882408380922344\n",
      "Epoch 99/300\n",
      "Average training loss: 0.10141493522127469\n",
      "Average test loss: 0.002930001757418116\n",
      "Epoch 100/300\n",
      "Average training loss: 0.10047008170684178\n",
      "Average test loss: 0.002829727722124921\n",
      "Epoch 101/300\n",
      "Average training loss: 0.09956756583187315\n",
      "Average test loss: 0.002834144453000691\n",
      "Epoch 102/300\n",
      "Average training loss: 0.09863290880786048\n",
      "Average test loss: 0.002964645154774189\n",
      "Epoch 103/300\n",
      "Average training loss: 0.09765596855680148\n",
      "Average test loss: 0.0028831463009119035\n",
      "Epoch 104/300\n",
      "Average training loss: 0.09732656428549025\n",
      "Average test loss: 0.0028165983721199963\n",
      "Epoch 105/300\n",
      "Average training loss: 0.09693750220537185\n",
      "Average test loss: 0.0029143674976916775\n",
      "Epoch 106/300\n",
      "Average training loss: 0.09699875903791852\n",
      "Average test loss: 0.003047259462169475\n",
      "Epoch 107/300\n",
      "Average training loss: 0.09549459958738751\n",
      "Average test loss: 0.0030177742911295757\n",
      "Epoch 108/300\n",
      "Average training loss: 0.09425562914212544\n",
      "Average test loss: 0.0031439683141393795\n",
      "Epoch 109/300\n",
      "Average training loss: 0.0937216243478987\n",
      "Average test loss: 0.003134983526542783\n",
      "Epoch 110/300\n",
      "Average training loss: 0.09262585147221883\n",
      "Average test loss: 0.0029495576299313044\n",
      "Epoch 111/300\n",
      "Average training loss: 0.09222025337815284\n",
      "Average test loss: 0.007703104422531194\n",
      "Epoch 112/300\n",
      "Average training loss: 1.9057975164055825\n",
      "Average test loss: 71533798012289.03\n",
      "Epoch 113/300\n",
      "Average training loss: 0.36365249037742614\n",
      "Average test loss: 324339002931.84216\n",
      "Epoch 114/300\n",
      "Average training loss: 0.24595573992199368\n",
      "Average test loss: 2690.6421455465365\n",
      "Epoch 115/300\n",
      "Average training loss: 0.20324799427721235\n",
      "Average test loss: 90692.18182499259\n",
      "Epoch 116/300\n",
      "Average training loss: 0.179897887772984\n",
      "Average test loss: 5744.897327058499\n",
      "Epoch 117/300\n",
      "Average training loss: 0.15276427163018122\n",
      "Average test loss: 0.00921370023012989\n",
      "Epoch 119/300\n",
      "Average training loss: 0.14305694770812988\n",
      "Average test loss: 0.002928581417331265\n",
      "Epoch 120/300\n",
      "Average training loss: 0.1351029353936513\n",
      "Average test loss: 9.248911300437317\n",
      "Epoch 121/300\n",
      "Average training loss: 0.12814399029148948\n",
      "Average test loss: 0.0030265670420808926\n",
      "Epoch 122/300\n",
      "Average training loss: 0.12515185123019748\n",
      "Average test loss: 0.002906317189335823\n",
      "Epoch 123/300\n",
      "Average training loss: 0.11691369922955831\n",
      "Average test loss: 0.0030291711993308532\n",
      "Epoch 124/300\n",
      "Average training loss: 0.11321275418334537\n",
      "Average test loss: 0.0028924187479747667\n",
      "Epoch 125/300\n",
      "Average training loss: 0.10756859838962556\n",
      "Average test loss: 0.0029005271864848007\n",
      "Epoch 127/300\n",
      "Average training loss: 0.10508308064275318\n",
      "Average test loss: 0.008244864012218184\n",
      "Epoch 128/300\n",
      "Average training loss: 0.10317464856968986\n",
      "Average test loss: 0.004210414423296849\n",
      "Epoch 129/300\n",
      "Average training loss: 0.10123825952741834\n",
      "Average test loss: 0.003084404511998097\n",
      "Epoch 130/300\n",
      "Average training loss: 0.09944372556606929\n",
      "Average test loss: 0.002938541665052374\n",
      "Epoch 131/300\n",
      "Average training loss: 0.09807952775557836\n",
      "Average test loss: 0.002839645464387205\n",
      "Epoch 132/300\n",
      "Average training loss: 0.10215280986494488\n",
      "Average test loss: 0.0028495725043531923\n",
      "Epoch 133/300\n",
      "Average training loss: 0.09648431157403523\n",
      "Average test loss: 0.0029213756658136845\n",
      "Epoch 134/300\n",
      "Average training loss: 0.09442618530326419\n",
      "Average test loss: 0.0029043798852297995\n",
      "Epoch 135/300\n",
      "Average training loss: 0.12192983136574427\n",
      "Average test loss: 0.0057892391002840465\n",
      "Epoch 136/300\n",
      "Average training loss: 0.11061670876873864\n",
      "Average test loss: 0.0028359783408749436\n",
      "Epoch 137/300\n",
      "Average training loss: 0.09837039040194617\n",
      "Average test loss: 0.0028884267854607766\n",
      "Epoch 138/300\n",
      "Average training loss: 0.09483502001894845\n",
      "Average test loss: 0.002865724886043204\n",
      "Epoch 139/300\n",
      "Average training loss: 0.09321730538871553\n",
      "Average test loss: 0.0028538193326029513\n",
      "Epoch 140/300\n",
      "Average training loss: 0.09226578702529271\n",
      "Average test loss: 0.002862081239724325\n",
      "Epoch 141/300\n",
      "Average training loss: 0.09161861228280598\n",
      "Average test loss: 0.0028770701485789484\n",
      "Epoch 142/300\n",
      "Average training loss: 0.09225416338443757\n",
      "Average test loss: 0.004732693217280838\n",
      "Epoch 143/300\n",
      "Average training loss: 0.09056329147020976\n",
      "Average test loss: 0.00288478211644623\n",
      "Epoch 144/300\n",
      "Average training loss: 0.09985431959231694\n",
      "Average test loss: 0.0076454286144839395\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09050340321328905\n",
      "Average test loss: 0.0028721227395451733\n",
      "Epoch 146/300\n",
      "Average training loss: 0.08926740508609347\n",
      "Average test loss: 0.0074716890934440824\n",
      "Epoch 147/300\n",
      "Average training loss: 0.1003357896010081\n",
      "Average test loss: 0.002800433890066213\n",
      "Epoch 148/300\n",
      "Average training loss: 0.09048396614525053\n",
      "Average test loss: 0.0028456283828450575\n",
      "Epoch 149/300\n",
      "Average training loss: 0.08844681787490845\n",
      "Average test loss: 0.0032478307320011985\n",
      "Epoch 150/300\n",
      "Average training loss: 0.0876038513382276\n",
      "Average test loss: 0.0043089848392539555\n",
      "Epoch 151/300\n",
      "Average training loss: 0.08728293602002991\n",
      "Average test loss: 0.002959260095324781\n",
      "Epoch 152/300\n",
      "Average training loss: 0.08672377909554375\n",
      "Average test loss: 0.0031159997733516825\n",
      "Epoch 153/300\n",
      "Average training loss: 0.08611998185846541\n",
      "Average test loss: 0.003094033315984739\n",
      "Epoch 154/300\n",
      "Average training loss: 0.08596894007921219\n",
      "Average test loss: 0.002917042921607693\n",
      "Epoch 155/300\n",
      "Average training loss: 0.08528918185498979\n",
      "Average test loss: 0.0029497624774359995\n",
      "Epoch 156/300\n",
      "Average training loss: 0.08574262132909563\n",
      "Average test loss: 0.003512501302692625\n",
      "Epoch 157/300\n",
      "Average training loss: 0.08466201210021973\n",
      "Average test loss: 0.0029058619799713294\n",
      "Epoch 158/300\n",
      "Average training loss: 0.08365249934130245\n",
      "Average test loss: 0.0029977810931288533\n",
      "Epoch 159/300\n",
      "Average training loss: 0.08354827978875902\n",
      "Average test loss: 0.00287180482596159\n",
      "Epoch 160/300\n",
      "Average training loss: 0.08308558047480054\n",
      "Average test loss: 0.002911976863733596\n",
      "Epoch 161/300\n",
      "Average training loss: 0.08272201192378997\n",
      "Average test loss: 0.0032760056538714303\n",
      "Epoch 162/300\n",
      "Average training loss: 0.08189526318179237\n",
      "Average test loss: 0.002966753442047371\n",
      "Epoch 163/300\n",
      "Average training loss: 0.08173802528116438\n",
      "Average test loss: 0.0028848130516707896\n",
      "Epoch 164/300\n",
      "Average training loss: 0.08158894943528705\n",
      "Average test loss: 0.0029368658947447938\n",
      "Epoch 165/300\n",
      "Average training loss: 0.08183224216434691\n",
      "Average test loss: 0.002976147620835238\n",
      "Epoch 166/300\n",
      "Average training loss: 0.08050955653521749\n",
      "Average test loss: 0.008763916302886273\n",
      "Epoch 167/300\n",
      "Average training loss: 0.07995481498705016\n",
      "Average test loss: 0.0030729226333399613\n",
      "Epoch 168/300\n",
      "Average training loss: 0.07987879741854138\n",
      "Average test loss: 0.0030823082296798626\n",
      "Epoch 169/300\n",
      "Average training loss: 0.07946297909816107\n",
      "Average test loss: 0.003118093850918942\n",
      "Epoch 170/300\n",
      "Average training loss: 0.07929186555412081\n",
      "Average test loss: 0.0064873722717165945\n",
      "Epoch 171/300\n",
      "Average training loss: 0.07873736631207996\n",
      "Average test loss: 0.012920976681013902\n",
      "Epoch 172/300\n",
      "Average training loss: 0.07856760556168027\n",
      "Average test loss: 0.0031370470540391074\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08588093598021401\n",
      "Average test loss: 0.004778776188277536\n",
      "Epoch 174/300\n",
      "Average training loss: 0.19972805431816312\n",
      "Average test loss: 0.003226883793042766\n",
      "Epoch 175/300\n",
      "Average training loss: 0.13438272850381003\n",
      "Average test loss: 0.0031161499780913192\n",
      "Epoch 176/300\n",
      "Average training loss: 0.11090416189697054\n",
      "Average test loss: 0.003048810897808936\n",
      "Epoch 177/300\n",
      "Average training loss: 0.10244385637839636\n",
      "Average test loss: 0.0028880893178284167\n",
      "Epoch 178/300\n",
      "Average training loss: 0.09749672528107961\n",
      "Average test loss: 0.002880095496359799\n",
      "Epoch 179/300\n",
      "Average training loss: 0.09386836689048343\n",
      "Average test loss: 0.0032015919973038963\n",
      "Epoch 180/300\n",
      "Average training loss: 0.09060422170824475\n",
      "Average test loss: 0.0029757931669139198\n",
      "Epoch 181/300\n",
      "Average training loss: 0.08798096452818976\n",
      "Average test loss: 0.004617437457872762\n",
      "Epoch 182/300\n",
      "Average training loss: 0.08533762574195862\n",
      "Average test loss: 0.0029824386379785008\n",
      "Epoch 183/300\n",
      "Average training loss: 0.08322285761435827\n",
      "Average test loss: 0.0036289713693161804\n",
      "Epoch 184/300\n",
      "Average training loss: 0.081876427034537\n",
      "Average test loss: 0.0029947755686524843\n",
      "Epoch 185/300\n",
      "Average training loss: 0.08048392707771725\n",
      "Average test loss: 0.003062927630212572\n",
      "Epoch 186/300\n",
      "Average training loss: 0.07964088801542918\n",
      "Average test loss: 0.003108941402286291\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07932812000645531\n",
      "Average test loss: 0.0030350829193161594\n",
      "Epoch 188/300\n",
      "Average training loss: 0.0782229890955819\n",
      "Average test loss: 0.003064308094067706\n",
      "Epoch 189/300\n",
      "Average training loss: 0.07824014545811547\n",
      "Average test loss: 0.0030448553113059866\n",
      "Epoch 190/300\n",
      "Average training loss: 0.07824338878525627\n",
      "Average test loss: 0.0037490867695046796\n",
      "Epoch 191/300\n",
      "Average training loss: 0.07781332221296099\n",
      "Average test loss: 0.003111645894125104\n",
      "Epoch 192/300\n",
      "Average training loss: 0.0771753230591615\n",
      "Average test loss: 0.003090173304080963\n",
      "Epoch 193/300\n",
      "Average training loss: 0.07725512643655141\n",
      "Average test loss: 0.004316491241152916\n",
      "Epoch 194/300\n",
      "Average training loss: 0.07735853401819864\n",
      "Average test loss: 0.003552982451601161\n",
      "Epoch 195/300\n",
      "Average training loss: 0.07691016430987252\n",
      "Average test loss: 0.0030460677391125094\n",
      "Epoch 196/300\n",
      "Average training loss: 0.07660230316056145\n",
      "Average test loss: 0.0030458104078554447\n",
      "Epoch 197/300\n",
      "Average training loss: 0.07576544576883316\n",
      "Average test loss: 0.0030343249245650236\n",
      "Epoch 198/300\n",
      "Average training loss: 0.0756041771935092\n",
      "Average test loss: 0.0032323128148499464\n",
      "Epoch 199/300\n",
      "Average training loss: 0.07559923493199878\n",
      "Average test loss: 0.006042326162258784\n",
      "Epoch 200/300\n",
      "Average training loss: 0.07521175726254781\n",
      "Average test loss: 0.005217685961681935\n",
      "Epoch 201/300\n",
      "Average training loss: 0.07518940927584966\n",
      "Average test loss: 0.0029787314741147887\n",
      "Epoch 202/300\n",
      "Average training loss: 0.07720347206460106\n",
      "Average test loss: 0.22579274801413218\n",
      "Epoch 203/300\n",
      "Average training loss: 0.07595434665679932\n",
      "Average test loss: 0.003160517426621583\n",
      "Epoch 204/300\n",
      "Average training loss: 0.07800654318597582\n",
      "Average test loss: 0.00319988363991595\n",
      "Epoch 205/300\n",
      "Average training loss: 0.07385765312446488\n",
      "Average test loss: 0.003227698263608747\n",
      "Epoch 206/300\n",
      "Average training loss: 0.07364607557985518\n",
      "Average test loss: 0.0030947897243830892\n",
      "Epoch 207/300\n",
      "Average training loss: 0.07467561159531276\n",
      "Average test loss: 0.0038459810161342225\n",
      "Epoch 208/300\n",
      "Average training loss: 0.07309511872794892\n",
      "Average test loss: 0.0029964025664246745\n",
      "Epoch 209/300\n",
      "Average training loss: 0.07335595501131481\n",
      "Average test loss: 0.0030961398647891152\n",
      "Epoch 210/300\n",
      "Average training loss: 0.08365830269124773\n",
      "Average test loss: 0.003195485346019268\n",
      "Epoch 211/300\n",
      "Average training loss: 0.07372094542119238\n",
      "Average test loss: 0.0037204460627916786\n",
      "Epoch 212/300\n",
      "Average training loss: 0.07843124699592591\n",
      "Average test loss: 0.0030729200012154048\n",
      "Epoch 213/300\n",
      "Average training loss: 0.07347535566488901\n",
      "Average test loss: 0.009239440031349659\n",
      "Epoch 214/300\n",
      "Average training loss: 0.07253911344210306\n",
      "Average test loss: 0.0030764773270736136\n",
      "Epoch 215/300\n",
      "Average training loss: 0.07303789457678794\n",
      "Average test loss: 0.0031472486110611093\n",
      "Epoch 216/300\n",
      "Average training loss: 0.08700781064232191\n",
      "Average test loss: 0.0030743695532696115\n",
      "Epoch 217/300\n",
      "Average training loss: 0.07308868935373095\n",
      "Average test loss: 0.0030985660950342813\n",
      "Epoch 218/300\n",
      "Average training loss: 0.07173980065849092\n",
      "Average test loss: 0.007339936766359541\n",
      "Epoch 219/300\n",
      "Average training loss: 0.07425760629442003\n",
      "Average test loss: 0.003180192725112041\n",
      "Epoch 220/300\n",
      "Average training loss: 0.07156996227635277\n",
      "Average test loss: 0.008115072065757381\n",
      "Epoch 221/300\n",
      "Average training loss: 0.07169148506720861\n",
      "Average test loss: 0.0032361558901353015\n",
      "Epoch 222/300\n",
      "Average training loss: 0.11066777894894282\n",
      "Average test loss: 0.0030345159860120877\n",
      "Epoch 223/300\n",
      "Average training loss: 0.08390500144826041\n",
      "Average test loss: 0.003066343303459386\n",
      "Epoch 224/300\n",
      "Average training loss: 0.07650253523720635\n",
      "Average test loss: 0.020635740113755068\n",
      "Epoch 225/300\n",
      "Average training loss: 0.07357169225480821\n",
      "Average test loss: 0.0031130362985034785\n",
      "Epoch 226/300\n",
      "Average training loss: 0.07254604789945815\n",
      "Average test loss: 0.0029639374162587855\n",
      "Epoch 227/300\n",
      "Average training loss: 0.07153033226728439\n",
      "Average test loss: 0.004112391449718012\n",
      "Epoch 228/300\n",
      "Average training loss: 0.08443421267800862\n",
      "Average test loss: 0.003399191235502561\n",
      "Epoch 229/300\n",
      "Average training loss: 0.07284726010428534\n",
      "Average test loss: 0.0030768019784655834\n",
      "Epoch 230/300\n",
      "Average training loss: 0.07121500484479798\n",
      "Average test loss: 0.0031182319550878473\n",
      "Epoch 231/300\n",
      "Average training loss: 0.07181728672981262\n",
      "Average test loss: 0.004640379181338681\n",
      "Epoch 232/300\n",
      "Average training loss: 0.07089787987205717\n",
      "Average test loss: 0.009296332386218839\n",
      "Epoch 233/300\n",
      "Average training loss: 0.075750637822681\n",
      "Average test loss: 0.07399958078066508\n",
      "Epoch 234/300\n",
      "Average training loss: 0.07058025710450279\n",
      "Average test loss: 0.003189991321414709\n",
      "Epoch 235/300\n",
      "Average training loss: 0.07098561302489705\n",
      "Average test loss: 0.003066623736690316\n",
      "Epoch 236/300\n",
      "Average training loss: 0.072491988254918\n",
      "Average test loss: 0.0031593319428049856\n",
      "Epoch 237/300\n",
      "Average training loss: 0.07080413314037853\n",
      "Average test loss: 0.003233234065481358\n",
      "Epoch 238/300\n",
      "Average training loss: 0.0705623572534985\n",
      "Average test loss: 0.004052876833205422\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07026093203491635\n",
      "Average test loss: 0.0038825752277755073\n",
      "Epoch 240/300\n",
      "Average training loss: 0.08270304693778356\n",
      "Average test loss: 0.007042554166581895\n",
      "Epoch 241/300\n",
      "Average training loss: 0.07158790013856357\n",
      "Average test loss: 0.003154557765358024\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0717826015651226\n",
      "Average test loss: 0.0030790153207878273\n",
      "Epoch 243/300\n",
      "Average training loss: 0.08077766329712338\n",
      "Average test loss: 0.00321413222555485\n",
      "Epoch 244/300\n",
      "Average training loss: 0.08072637506988313\n",
      "Average test loss: 0.0030987598662161166\n",
      "Epoch 245/300\n",
      "Average training loss: 0.0713357276585367\n",
      "Average test loss: 0.0032073070475210747\n",
      "Epoch 246/300\n",
      "Average training loss: 0.06970837675200568\n",
      "Average test loss: 0.0031358472891151903\n",
      "Epoch 247/300\n",
      "Average training loss: 0.06936445921990607\n",
      "Average test loss: 0.009962367887298266\n",
      "Epoch 248/300\n",
      "Average training loss: 0.07187408299247423\n",
      "Average test loss: 0.003147943763062358\n",
      "Epoch 249/300\n",
      "Average training loss: 0.06962452609671486\n",
      "Average test loss: 0.0030629302118387485\n",
      "Epoch 250/300\n",
      "Average training loss: 0.06903590669896867\n",
      "Average test loss: 0.003146835713336865\n",
      "Epoch 251/300\n",
      "Average training loss: 0.06897915417121517\n",
      "Average test loss: 0.003726197849545214\n",
      "Epoch 252/300\n",
      "Average training loss: 0.09611406383911769\n",
      "Average test loss: 0.0031606702134013174\n",
      "Epoch 253/300\n",
      "Average training loss: 0.07598066803481844\n",
      "Average test loss: 0.00305351486760709\n",
      "Epoch 254/300\n",
      "Average training loss: 0.07621686443355348\n",
      "Average test loss: 0.0030758951298064654\n",
      "Epoch 256/300\n",
      "Average training loss: 0.07156736665964127\n",
      "Average test loss: 0.004223710152010123\n",
      "Epoch 259/300\n",
      "Average training loss: 0.06882634858621492\n",
      "Average test loss: 0.0031245803833007814\n",
      "Epoch 260/300\n",
      "Average training loss: 0.06845298682318793\n",
      "Average test loss: 0.02047972455455197\n",
      "Epoch 261/300\n",
      "Average training loss: 0.12148728825979763\n",
      "Average test loss: 0.003043835981438557\n",
      "Epoch 262/300\n",
      "Average training loss: 0.07804666078752942\n",
      "Average test loss: 0.0031062626995974115\n",
      "Epoch 263/300\n",
      "Average training loss: 0.07163425351513757\n",
      "Average test loss: 0.003030227313439051\n",
      "Epoch 264/300\n",
      "Average training loss: 0.06848371891180674\n",
      "Average test loss: 0.003214438656758931\n",
      "Epoch 266/300\n",
      "Average training loss: 0.06807443556520675\n",
      "Average test loss: 0.0049381090104579925\n",
      "Epoch 268/300\n",
      "Average training loss: 0.0680560691025522\n",
      "Average test loss: 0.003144780925992462\n",
      "Epoch 269/300\n",
      "Average training loss: 0.07140566918585035\n",
      "Average test loss: 467619.68881194235\n",
      "Epoch 270/300\n",
      "Average training loss: 0.07106122932169172\n",
      "Average test loss: 0.0031299252609411875\n",
      "Epoch 271/300\n",
      "Average training loss: 0.07238984852366977\n",
      "Average test loss: 0.0032923683592428763\n",
      "Epoch 272/300\n",
      "Average training loss: 0.06784579867124557\n",
      "Average test loss: 0.004305644316391812\n",
      "Epoch 273/300\n",
      "Average training loss: 0.07099717611736721\n",
      "Average test loss: 0.003108284353175097\n",
      "Epoch 274/300\n",
      "Average training loss: 0.0673439501259062\n",
      "Average test loss: 0.0032444321411765286\n",
      "Epoch 275/300\n",
      "Average training loss: 0.06760263512863053\n",
      "Average test loss: 0.006610327092723714\n",
      "Epoch 276/300\n",
      "Average training loss: 0.06875394807259241\n",
      "Average test loss: 0.0048000352912478976\n",
      "Epoch 277/300\n",
      "Average training loss: 0.06758459436562327\n",
      "Average test loss: 0.0033452227573013966\n",
      "Epoch 279/300\n",
      "Average training loss: 0.0674411032795906\n",
      "Average test loss: 0.0030608221563614077\n",
      "Epoch 280/300\n",
      "Average training loss: 0.0680138277610143\n",
      "Average test loss: 0.00312655683234334\n",
      "Epoch 281/300\n",
      "Average training loss: 0.06808439334895876\n",
      "Average test loss: 0.0030675860337085192\n",
      "Epoch 282/300\n",
      "Average training loss: 0.06697505076064005\n",
      "Average test loss: 0.009123302883572049\n",
      "Epoch 283/300\n",
      "Average training loss: 0.06998255630334219\n",
      "Average test loss: 0.003220914036035538\n",
      "Epoch 284/300\n",
      "Average training loss: 0.06687584502167172\n",
      "Average test loss: 0.004270904473546479\n",
      "Epoch 285/300\n",
      "Average training loss: 0.06684946407212151\n",
      "Average test loss: 0.003619010099934207\n",
      "Epoch 286/300\n",
      "Average training loss: 0.0704481544163492\n",
      "Average test loss: 0.0032846229117777614\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06679150338967642\n",
      "Average test loss: 0.0031477291993796826\n",
      "Epoch 289/300\n",
      "Average training loss: 0.06619249931308958\n",
      "Average test loss: 0.003185885192412469\n",
      "Epoch 290/300\n",
      "Average training loss: 0.08357066760460535\n",
      "Average test loss: 0.003098344008334809\n",
      "Epoch 291/300\n",
      "Average training loss: 0.0671373581190904\n",
      "Average test loss: 0.0031007162481546404\n",
      "Epoch 292/300\n",
      "Average training loss: 0.06625932191477882\n",
      "Average test loss: 0.003165783775349458\n",
      "Epoch 293/300\n",
      "Average training loss: 0.06608983234564464\n",
      "Average test loss: 0.003078795815507571\n",
      "Epoch 294/300\n",
      "Average training loss: 0.06639177992277676\n",
      "Average test loss: 0.005413779330750306\n",
      "Epoch 295/300\n",
      "Average training loss: 0.07388376627365748\n",
      "Average test loss: 0.0030540937141825754\n",
      "Epoch 296/300\n",
      "Average training loss: 0.06593030505710178\n",
      "Average test loss: 0.0030952195059508083\n",
      "Epoch 297/300\n",
      "Average training loss: 0.06593345977862676\n",
      "Average test loss: 0.003248858187554611\n",
      "Epoch 298/300\n",
      "Average training loss: 0.06579820011721717\n",
      "Average test loss: 0.0031371364316178695\n",
      "Epoch 299/300\n",
      "Average training loss: 0.06620402185122172\n",
      "Average test loss: 0.003233782909396622\n",
      "Epoch 300/300\n",
      "Average training loss: 0.07337471641434563\n",
      "Average test loss: 0.003038597298372123\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 35.46857024298774\n",
      "Average test loss: 0.3359880153760314\n",
      "Epoch 2/300\n",
      "Average training loss: 19.222470504760743\n",
      "Average test loss: 0.007333942340893878\n",
      "Epoch 3/300\n",
      "Average training loss: 14.433571567959255\n",
      "Average test loss: 45.318896636727786\n",
      "Epoch 4/300\n",
      "Average training loss: 12.405891092936198\n",
      "Average test loss: 0.014143777143210173\n",
      "Epoch 5/300\n",
      "Average training loss: 11.807825602213542\n",
      "Average test loss: 3.524621678481913\n",
      "Epoch 6/300\n",
      "Average training loss: 9.610784018622505\n",
      "Average test loss: 0.6593465703138047\n",
      "Epoch 7/300\n",
      "Average training loss: 7.909699982113309\n",
      "Average test loss: 17.502130124578873\n",
      "Epoch 8/300\n",
      "Average training loss: 7.116345450931125\n",
      "Average test loss: 394.0883156571372\n",
      "Epoch 9/300\n",
      "Average training loss: 5.960702458275689\n",
      "Average test loss: 73.23371862997115\n",
      "Epoch 10/300\n",
      "Average training loss: 5.245845205518934\n",
      "Average test loss: 2.24630256200023\n",
      "Epoch 11/300\n",
      "Average training loss: 4.75963255988227\n",
      "Average test loss: 26.21385058111242\n",
      "Epoch 12/300\n",
      "Average training loss: 3.5281430030398897\n",
      "Average test loss: 0.0054196449911428825\n",
      "Epoch 14/300\n",
      "Average training loss: 3.027186284383138\n",
      "Average test loss: 0.0032416798904952074\n",
      "Epoch 15/300\n",
      "Average training loss: 2.684359068552653\n",
      "Average test loss: 0.006761970080642237\n",
      "Epoch 16/300\n",
      "Average training loss: 2.423111621008979\n",
      "Average test loss: 10.868974844012824\n",
      "Epoch 17/300\n",
      "Average training loss: 2.1598932950761585\n",
      "Average test loss: 0.0053697412560383476\n",
      "Epoch 18/300\n",
      "Average training loss: 1.8969305166668362\n",
      "Average test loss: 0.38527756109957895\n",
      "Epoch 19/300\n",
      "Average training loss: 1.6878728210661147\n",
      "Average test loss: 0.006094090062710974\n",
      "Epoch 20/300\n",
      "Average training loss: 1.4959522682825723\n",
      "Average test loss: 0.23203000047223435\n",
      "Epoch 21/300\n",
      "Average training loss: 1.336128446367052\n",
      "Average test loss: 0.00338782226625416\n",
      "Epoch 22/300\n",
      "Average training loss: 1.211080916404724\n",
      "Average test loss: 0.002622318360540602\n",
      "Epoch 23/300\n",
      "Average training loss: 1.0945476374096341\n",
      "Average test loss: 0.003815575479840239\n",
      "Epoch 24/300\n",
      "Average training loss: 0.9869756945504082\n",
      "Average test loss: 69.85672836007012\n",
      "Epoch 25/300\n",
      "Average training loss: 0.8930008662541707\n",
      "Average test loss: 0.0025478243266956672\n",
      "Epoch 26/300\n",
      "Average training loss: 0.7295722177293565\n",
      "Average test loss: 3.176358307480812\n",
      "Epoch 28/300\n",
      "Average training loss: 0.6578778541353014\n",
      "Average test loss: 0.26717185379399194\n",
      "Epoch 29/300\n",
      "Average training loss: 0.5953054376178317\n",
      "Average test loss: 0.003227847149802579\n",
      "Epoch 30/300\n",
      "Average training loss: 0.5371291018591987\n",
      "Average test loss: 0.0024040315953186816\n",
      "Epoch 31/300\n",
      "Average training loss: 0.4845977784792582\n",
      "Average test loss: 0.002310892165121105\n",
      "Epoch 32/300\n",
      "Average training loss: 0.4359546500576867\n",
      "Average test loss: 0.002340538225861059\n",
      "Epoch 33/300\n",
      "Average training loss: 0.393427470419142\n",
      "Average test loss: 0.002554213735585411\n",
      "Epoch 34/300\n",
      "Average training loss: 0.35332695566283334\n",
      "Average test loss: 0.013112411114697655\n",
      "Epoch 35/300\n",
      "Average training loss: 0.3180910451412201\n",
      "Average test loss: 0.016434916231677765\n",
      "Epoch 36/300\n",
      "Average training loss: 0.286758494509591\n",
      "Average test loss: 0.1307105007585552\n",
      "Epoch 37/300\n",
      "Average training loss: 0.25945375612046984\n",
      "Average test loss: 0.002014706341135833\n",
      "Epoch 38/300\n",
      "Average training loss: 0.23484094880686865\n",
      "Average test loss: 0.0020424429369676443\n",
      "Epoch 39/300\n",
      "Average training loss: 0.21467925216092004\n",
      "Average test loss: 0.002037524762459927\n",
      "Epoch 40/300\n",
      "Average training loss: 0.19746481158998277\n",
      "Average test loss: 0.0033154648064325255\n",
      "Epoch 41/300\n",
      "Average training loss: 0.18190959566169315\n",
      "Average test loss: 0.0023441435435993803\n",
      "Epoch 42/300\n",
      "Average training loss: 0.16747323464022743\n",
      "Average test loss: 0.529607705126206\n",
      "Epoch 43/300\n",
      "Average training loss: 0.15654976920286814\n",
      "Average test loss: 0.0019978698360630207\n",
      "Epoch 44/300\n",
      "Average training loss: 0.14687224605348376\n",
      "Average test loss: 0.0019078872927154103\n",
      "Epoch 45/300\n",
      "Average training loss: 0.13864388577143352\n",
      "Average test loss: 0.002405209970453547\n",
      "Epoch 46/300\n",
      "Average training loss: 0.129631951921516\n",
      "Average test loss: 0.02366560215337409\n",
      "Epoch 47/300\n",
      "Average training loss: 0.11646257657474941\n",
      "Average test loss: 0.002539464834249682\n",
      "Epoch 49/300\n",
      "Average training loss: 0.11066726536883248\n",
      "Average test loss: 0.0019266782137047913\n",
      "Epoch 50/300\n",
      "Average training loss: 0.10697685004605187\n",
      "Average test loss: 0.001957028079467515\n",
      "Epoch 51/300\n",
      "Average training loss: 0.10237508175108168\n",
      "Average test loss: 0.006377270470787254\n",
      "Epoch 52/300\n",
      "Average training loss: 0.10026497146818374\n",
      "Average test loss: 0.0018698020621927248\n",
      "Epoch 53/300\n",
      "Average training loss: 0.09581864009300868\n",
      "Average test loss: 0.0018724130066111683\n",
      "Epoch 54/300\n",
      "Average training loss: 0.09266786069340176\n",
      "Average test loss: 0.002211781981608106\n",
      "Epoch 55/300\n",
      "Average training loss: 0.1644519123501248\n",
      "Average test loss: 0.0027301872887959084\n",
      "Epoch 56/300\n",
      "Average training loss: 0.10419540849659178\n",
      "Average test loss: 0.0023137237190579375\n",
      "Epoch 57/300\n",
      "Average training loss: 0.09572270586755541\n",
      "Average test loss: 0.0032325429415537253\n",
      "Epoch 58/300\n",
      "Average training loss: 0.0909894986351331\n",
      "Average test loss: 0.002068138201514052\n",
      "Epoch 59/300\n",
      "Average training loss: 0.08826398706436157\n",
      "Average test loss: 0.001942691248220702\n",
      "Epoch 60/300\n",
      "Average training loss: 0.0859713788860374\n",
      "Average test loss: 0.001889184380053646\n",
      "Epoch 61/300\n",
      "Average training loss: 0.0829374409019947\n",
      "Average test loss: 0.0037330690717531576\n",
      "Epoch 62/300\n",
      "Average training loss: 0.0816707595454322\n",
      "Average test loss: 0.001868899770701925\n",
      "Epoch 63/300\n",
      "Average training loss: 0.08013338330056932\n",
      "Average test loss: 0.01968599484364192\n",
      "Epoch 64/300\n",
      "Average training loss: 0.07876678528388341\n",
      "Average test loss: 0.0021118853110820056\n",
      "Epoch 66/300\n",
      "Average training loss: 0.07529081645276811\n",
      "Average test loss: 0.010548482620674702\n",
      "Epoch 67/300\n",
      "Average training loss: 0.07416340234213405\n",
      "Average test loss: 0.0023317931172334486\n",
      "Epoch 68/300\n",
      "Average training loss: 0.21416149481799868\n",
      "Average test loss: 0.0020487171979621053\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10167608771059249\n",
      "Average test loss: 0.001968044580788248\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09120829253064261\n",
      "Average test loss: 0.001941888956560029\n",
      "Epoch 71/300\n",
      "Average training loss: 0.08275075450870725\n",
      "Average test loss: 0.0031009515604625147\n",
      "Epoch 73/300\n",
      "Average training loss: 0.07944954135682848\n",
      "Average test loss: 0.001950419247771303\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07769799665609996\n",
      "Average test loss: 0.0018652505077835586\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07598938801222377\n",
      "Average test loss: 0.0025934381710572374\n",
      "Epoch 76/300\n",
      "Average training loss: 0.07512944720188777\n",
      "Average test loss: 0.13095690773924193\n",
      "Epoch 77/300\n",
      "Average training loss: 0.07385984261168374\n",
      "Average test loss: 0.02277673943589131\n",
      "Epoch 78/300\n",
      "Average training loss: 0.07218619539340337\n",
      "Average test loss: 0.004176042831192414\n",
      "Epoch 80/300\n",
      "Average training loss: 0.0716471421983507\n",
      "Average test loss: 0.0018838785524583526\n",
      "Epoch 81/300\n",
      "Average training loss: 0.07098497868908776\n",
      "Average test loss: 0.0018886151434853672\n",
      "Epoch 82/300\n",
      "Average training loss: 0.0701051730910937\n",
      "Average test loss: 1.6154307918250561\n",
      "Epoch 83/300\n",
      "Average training loss: 0.06960367888874477\n",
      "Average test loss: 36.540425668822394\n",
      "Epoch 84/300\n",
      "Average training loss: 0.07343469425704745\n",
      "Average test loss: 0.024439526393181747\n",
      "Epoch 85/300\n",
      "Average training loss: 0.06922768641180463\n",
      "Average test loss: 0.0019778248547679847\n",
      "Epoch 86/300\n",
      "Average training loss: 0.06779930963781144\n",
      "Average test loss: 0.0019636993871794805\n",
      "Epoch 87/300\n",
      "Average training loss: 0.0670953538219134\n",
      "Average test loss: 0.0018254822053843075\n",
      "Epoch 88/300\n",
      "Average training loss: 0.06707982066935964\n",
      "Average test loss: 0.0018653119852145514\n",
      "Epoch 90/300\n",
      "Average training loss: 0.06563541956080331\n",
      "Average test loss: 0.0019033589573163126\n",
      "Epoch 91/300\n",
      "Average training loss: 0.06546420307623016\n",
      "Average test loss: 0.001961386307970517\n",
      "Epoch 92/300\n",
      "Average training loss: 0.06415940770506859\n",
      "Average test loss: 1.6149946490559313\n",
      "Epoch 93/300\n",
      "Average training loss: 0.06356325225366487\n",
      "Average test loss: 0.4385979913034373\n",
      "Epoch 94/300\n",
      "Average training loss: 0.0631201180782583\n",
      "Average test loss: 0.007081393872077266\n",
      "Epoch 95/300\n",
      "Average training loss: 0.06674925753805372\n",
      "Average test loss: 0.001919364314008918\n",
      "Epoch 97/300\n",
      "Average training loss: 0.09438386279675695\n",
      "Average test loss: 0.0029909556330078176\n",
      "Epoch 98/300\n",
      "Average training loss: 0.07644480076101091\n",
      "Average test loss: 0.0018484450173046855\n",
      "Epoch 99/300\n",
      "Average training loss: 0.06964451890852716\n",
      "Average test loss: 0.004095302268862724\n",
      "Epoch 100/300\n",
      "Average training loss: 0.06709973734617233\n",
      "Average test loss: 0.0018537360165889064\n",
      "Epoch 101/300\n",
      "Average training loss: 0.06538360570867856\n",
      "Average test loss: 0.0025552675823370618\n",
      "Epoch 102/300\n",
      "Average training loss: 0.0640938706960943\n",
      "Average test loss: 0.012749943700205121\n",
      "Epoch 103/300\n",
      "Average training loss: 0.0632066377600034\n",
      "Average test loss: 0.0020318651385605336\n",
      "Epoch 104/300\n",
      "Average training loss: 0.062492058740721806\n",
      "Average test loss: 0.002128844296766652\n",
      "Epoch 105/300\n",
      "Average training loss: 0.06179204409983423\n",
      "Average test loss: 0.002256390454971956\n",
      "Epoch 106/300\n",
      "Average training loss: 0.06115385334690412\n",
      "Average test loss: 0.004901475116403566\n",
      "Epoch 107/300\n",
      "Average training loss: 0.060482581744591396\n",
      "Average test loss: 0.002136712192764713\n",
      "Epoch 108/300\n",
      "Average training loss: 0.060266905426979066\n",
      "Average test loss: 2.137053923826665\n",
      "Epoch 109/300\n",
      "Average training loss: 0.05974540990259912\n",
      "Average test loss: 0.002050596789249943\n",
      "Epoch 110/300\n",
      "Average training loss: 0.0590032055510415\n",
      "Average test loss: 0.0021605374415715538\n",
      "Epoch 112/300\n",
      "Average training loss: 0.05861468102534612\n",
      "Average test loss: 0.0021527278661313985\n",
      "Epoch 113/300\n",
      "Average training loss: 0.05881367054912779\n",
      "Average test loss: 0.01113569742337697\n",
      "Epoch 114/300\n",
      "Average training loss: 0.057849524156914814\n",
      "Average test loss: 0.004483339485194948\n",
      "Epoch 115/300\n",
      "Average training loss: 0.057397661815087\n",
      "Average test loss: 0.0024267851043906475\n",
      "Epoch 116/300\n",
      "Average training loss: 0.05751552311910523\n",
      "Average test loss: 3.7663601171573005\n",
      "Epoch 117/300\n",
      "Average training loss: 0.056820173611243564\n",
      "Average test loss: 0.001980506758412553\n",
      "Epoch 118/300\n",
      "Average training loss: 0.05701419955823157\n",
      "Average test loss: 0.00301462963099281\n",
      "Epoch 119/300\n",
      "Average training loss: 0.05612699263294538\n",
      "Average test loss: 0.0027639087365112372\n",
      "Epoch 120/300\n",
      "Average training loss: 0.057131567569242585\n",
      "Average test loss: 0.002057370364665985\n",
      "Epoch 121/300\n",
      "Average training loss: 0.05564830528696378\n",
      "Average test loss: 0.0019069684379630619\n",
      "Epoch 122/300\n",
      "Average training loss: 0.055257100694709356\n",
      "Average test loss: 0.02052964168300645\n",
      "Epoch 123/300\n",
      "Average training loss: 0.055086221201552286\n",
      "Average test loss: 0.0020448792063527638\n",
      "Epoch 124/300\n",
      "Average training loss: 0.05534803436862098\n",
      "Average test loss: 0.0021730753185434473\n",
      "Epoch 125/300\n",
      "Average training loss: 0.05657936038242446\n",
      "Average test loss: 0.002155056536818544\n",
      "Epoch 127/300\n",
      "Average training loss: 0.05493127523528205\n",
      "Average test loss: 0.0020281883652011556\n",
      "Epoch 128/300\n",
      "Average training loss: 0.05474649304482672\n",
      "Average test loss: 0.0022816826043029624\n",
      "Epoch 129/300\n",
      "Average training loss: 0.05403909745150142\n",
      "Average test loss: 0.0019611304520318904\n",
      "Epoch 130/300\n",
      "Average training loss: 0.053761535776986016\n",
      "Average test loss: 0.0020791561775323417\n",
      "Epoch 131/300\n",
      "Average training loss: 0.05357006868388918\n",
      "Average test loss: 0.0020424089722542297\n",
      "Epoch 132/300\n",
      "Average training loss: 0.05354832393593258\n",
      "Average test loss: 0.002340496574838956\n",
      "Epoch 134/300\n",
      "Average training loss: 0.05310739693376753\n",
      "Average test loss: 0.0019501615666473904\n",
      "Epoch 135/300\n",
      "Average training loss: 0.05284227645728323\n",
      "Average test loss: 0.002072994576767087\n",
      "Epoch 136/300\n",
      "Average training loss: 0.052730710138877236\n",
      "Average test loss: 0.008489342772298389\n",
      "Epoch 137/300\n",
      "Average training loss: 0.05443430369098981\n",
      "Average test loss: 0.005587219062778685\n",
      "Epoch 138/300\n",
      "Average training loss: 0.05473678430914879\n",
      "Average test loss: 0.002298569051341878\n",
      "Epoch 139/300\n",
      "Average training loss: 0.05242821204000049\n",
      "Average test loss: 0.0022880198841707575\n",
      "Epoch 140/300\n",
      "Average training loss: 0.05204011200533973\n",
      "Average test loss: 0.0020648618919981853\n",
      "Epoch 141/300\n",
      "Average training loss: 0.052238406038946576\n",
      "Average test loss: 0.006288302288287216\n",
      "Epoch 142/300\n",
      "Average training loss: 0.051674437671899796\n",
      "Average training loss: 0.05204641240172916\n",
      "Average test loss: 0.0032908384849627812\n",
      "Epoch 144/300\n",
      "Average training loss: 0.05140058958199289\n",
      "Average test loss: 0.005424847372704082\n",
      "Epoch 145/300\n",
      "Average training loss: 0.09508709190951453\n",
      "Average test loss: 0.019132759340935283\n",
      "Epoch 147/300\n",
      "Average training loss: 0.10000858954257436\n",
      "Average test loss: 0.0021197136158330574\n",
      "Epoch 148/300\n",
      "Average test loss: 0.0027545843532101977\n",
      "Epoch 149/300\n",
      "Average training loss: 0.06588176528943909\n",
      "Average test loss: 18.97213947402603\n",
      "Epoch 150/300\n",
      "Average training loss: 0.060978663901487984\n",
      "Average test loss: 0.006409376121229595\n",
      "Epoch 151/300\n",
      "Average training loss: 0.05741091459658411\n",
      "Average test loss: 0.0019864077754318714\n",
      "Epoch 152/300\n",
      "Average training loss: 0.055166616764333515\n",
      "Average test loss: 0.032774942819443015\n",
      "Epoch 153/300\n",
      "Average training loss: 0.05306714775827196\n",
      "Average test loss: 0.002170676745267378\n",
      "Epoch 154/300\n",
      "Average training loss: 0.05202169924808873\n",
      "Average test loss: 0.002035215861991876\n",
      "Epoch 155/300\n",
      "Average training loss: 0.05136151454514927\n",
      "Average test loss: 0.001982281139741341\n",
      "Epoch 156/300\n",
      "Average training loss: 0.05178419707549943\n",
      "Average test loss: 0.0025264857829444937\n",
      "Epoch 157/300\n",
      "Average training loss: 0.051040960944361155\n",
      "Average test loss: 0.057835912505785625\n",
      "Epoch 158/300\n",
      "Average training loss: 0.05058046246899499\n",
      "Average test loss: 0.0021078880799727307\n",
      "Epoch 159/300\n",
      "Average training loss: 0.05214063980182012\n",
      "Average test loss: 376.0243143225776\n",
      "Epoch 160/300\n",
      "Average training loss: 0.05094479369951619\n",
      "Average test loss: 0.0020310623488492435\n",
      "Epoch 161/300\n",
      "Average training loss: 0.0504599995844894\n",
      "Average test loss: 0.0021818272550072934\n",
      "Epoch 162/300\n",
      "Average training loss: 0.05041246639026536\n",
      "Average test loss: 0.0022045336092511815\n",
      "Epoch 163/300\n",
      "Average training loss: 0.05024696036510997\n",
      "Average test loss: 0.0020918751783255076\n",
      "Epoch 164/300\n",
      "Average training loss: 0.051113270001278986\n",
      "Average test loss: 0.0020353223108169104\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05006710145539708\n",
      "Average test loss: 0.0027744098130820525\n",
      "Epoch 166/300\n",
      "Average training loss: 0.05010717186000612\n",
      "Average test loss: 0.002268556284407775\n",
      "Epoch 167/300\n",
      "Average training loss: 0.04976195439365175\n",
      "Average test loss: 0.0028077920786001615\n",
      "Epoch 168/300\n",
      "Average training loss: 0.05084797940320439\n",
      "Average test loss: 0.37864025995300876\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04947757626242108\n",
      "Average test loss: 0.0033218231128735675\n",
      "Epoch 170/300\n",
      "Average training loss: 0.049394192530049216\n",
      "Average test loss: 0.0023029175620112156\n",
      "Epoch 171/300\n",
      "Average training loss: 0.04959560171100828\n",
      "Average test loss: 279.2215329232746\n",
      "Epoch 172/300\n",
      "Average training loss: 0.049381468070877926\n",
      "Average test loss: 0.0020788216737823353\n",
      "Epoch 173/300\n",
      "Average training loss: 0.04982586674226655\n",
      "Average test loss: 0.003012908492030369\n",
      "Epoch 174/300\n",
      "Average training loss: 0.049154726362890665\n",
      "Average test loss: 0.0039347757755054365\n",
      "Epoch 175/300\n",
      "Average training loss: 0.04902248888545566\n",
      "Average test loss: 0.0022274440142015617\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0525767774383227\n",
      "Average test loss: 0.009551584970619944\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04899264395568106\n",
      "Average test loss: 0.004327221965624226\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04864222621586588\n",
      "Average test loss: 0.008495052302256226\n",
      "Epoch 179/300\n",
      "Average training loss: 0.04858898509542147\n",
      "Average test loss: 0.0027732821045857335\n",
      "Epoch 180/300\n",
      "Average training loss: 0.048982075737582315\n",
      "Average test loss: 0.002085859566099114\n",
      "Epoch 181/300\n",
      "Average training loss: 0.0485018782251411\n",
      "Average test loss: 0.00210450412477884\n",
      "Epoch 182/300\n",
      "Average training loss: 0.05631195106771257\n",
      "Average test loss: 0.002673330697748396\n",
      "Epoch 183/300\n",
      "Average training loss: 0.04877139690518379\n",
      "Average test loss: 0.0020648660820184483\n",
      "Epoch 184/300\n",
      "Average training loss: 0.04812423950764868\n",
      "Average test loss: 0.0100580545152641\n",
      "Epoch 185/300\n",
      "Average training loss: 0.047921928018331526\n",
      "Average test loss: 0.0020481668557557793\n",
      "Epoch 186/300\n",
      "Average training loss: 0.15102736875745984\n",
      "Average test loss: 0.002375450514463915\n",
      "Epoch 187/300\n",
      "Average training loss: 0.07966624153322643\n",
      "Average test loss: 0.0018709196628381808\n",
      "Epoch 188/300\n",
      "Average training loss: 0.07122635914882024\n",
      "Average test loss: 0.001894852773596843\n",
      "Epoch 189/300\n",
      "Average training loss: 0.06537402056985431\n",
      "Average test loss: 0.0021285419141252836\n",
      "Epoch 190/300\n",
      "Average training loss: 0.061945910053120716\n",
      "Average test loss: 0.0031127933450043202\n",
      "Epoch 191/300\n",
      "Average training loss: 0.05947026516662703\n",
      "Average test loss: 0.002051890065169169\n",
      "Epoch 192/300\n",
      "Average training loss: 0.05742759684059355\n",
      "Average test loss: 0.0022864710608911185\n",
      "Epoch 193/300\n",
      "Average training loss: 0.055640869458516436\n",
      "Average test loss: 0.003364366035287579\n",
      "Epoch 194/300\n",
      "Average training loss: 0.05418033731314871\n",
      "Average test loss: 0.002069148498897751\n",
      "Epoch 195/300\n",
      "Average training loss: 0.05279252876838048\n",
      "Average test loss: 0.0021730619945252937\n",
      "Epoch 196/300\n",
      "Average training loss: 0.052203040113051734\n",
      "Average test loss: 0.0025014830902218817\n",
      "Epoch 197/300\n",
      "Average training loss: 0.05103595792253812\n",
      "Average test loss: 0.0039924228183097306\n",
      "Epoch 198/300\n",
      "Average training loss: 0.05058082454403241\n",
      "Average test loss: 0.0021933888586031067\n",
      "Epoch 199/300\n",
      "Average training loss: 0.0503656695385774\n",
      "Average test loss: 0.003271113177140554\n",
      "Epoch 200/300\n",
      "Average training loss: 0.04950123595197996\n",
      "Average test loss: 0.0020350297153409985\n",
      "Epoch 201/300\n",
      "Average training loss: 0.049501622375514774\n",
      "Average test loss: 0.002408596307142741\n",
      "Epoch 202/300\n",
      "Average training loss: 0.04907257336378097\n",
      "Average test loss: 32470.562594899475\n",
      "Epoch 203/300\n",
      "Average training loss: 0.0641206111907959\n",
      "Average test loss: 0.03727254381527503\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04890603927440113\n",
      "Average test loss: 0.002037337097753253\n",
      "Epoch 205/300\n",
      "Average training loss: 0.04791305623120732\n",
      "Average test loss: 0.004553684683309661\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04770700605710348\n",
      "Average test loss: 0.002700554827435149\n",
      "Epoch 207/300\n",
      "Average training loss: 0.04749799190296067\n",
      "Average test loss: 0.004116840987362796\n",
      "Epoch 208/300\n",
      "Average training loss: 0.04775638885630502\n",
      "Average test loss: 0.0020548443004695907\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04745050841238763\n",
      "Average test loss: 0.002097494492100345\n",
      "Epoch 210/300\n",
      "Average training loss: 0.04793168525894483\n",
      "Average test loss: 0.0024978319101242557\n",
      "Epoch 211/300\n",
      "Average training loss: 0.047606293982929655\n",
      "Average test loss: 1.6133724294073053\n",
      "Epoch 212/300\n",
      "Average training loss: 0.04754195342461268\n",
      "Average test loss: 0.002028946244882213\n",
      "Epoch 213/300\n",
      "Average training loss: 0.04768011251423094\n",
      "Average test loss: 0.0023925691253195206\n",
      "Epoch 214/300\n",
      "Average training loss: 0.04824938625097275\n",
      "Average test loss: 0.007827263970031506\n",
      "Epoch 215/300\n",
      "Average training loss: 0.04677564907736249\n",
      "Average test loss: 0.0020912474625640444\n",
      "Epoch 216/300\n",
      "Average training loss: 0.04691813292768267\n",
      "Average test loss: 0.002520215988987022\n",
      "Epoch 217/300\n",
      "Average training loss: 0.04876113684309853\n",
      "Average test loss: 0.0022189629001335967\n",
      "Epoch 218/300\n",
      "Average training loss: 0.046817734920316274\n",
      "Average test loss: 0.0034605584177705975\n",
      "Epoch 219/300\n",
      "Average training loss: 0.046657439950439666\n",
      "Average test loss: 0.002152026447157065\n",
      "Epoch 220/300\n",
      "Average training loss: 0.06405560124251578\n",
      "Average test loss: 0.001986486302378277\n",
      "Epoch 221/300\n",
      "Average training loss: 0.05568987199001842\n",
      "Average test loss: 0.0021120140821569494\n",
      "Epoch 222/300\n",
      "Average training loss: 0.04817320943872134\n",
      "Average test loss: 0.0030905165689893896\n",
      "Epoch 223/300\n",
      "Average training loss: 0.046671848469310336\n",
      "Average test loss: 0.002109393225879305\n",
      "Epoch 224/300\n",
      "Average training loss: 0.0461041268673208\n",
      "Average test loss: 0.005287323067585627\n",
      "Epoch 225/300\n",
      "Average training loss: 0.04612654552194807\n",
      "Average test loss: 0.042835652196572885\n",
      "Epoch 226/300\n",
      "Average training loss: 0.046413804978132245\n",
      "Average test loss: 0.06918094680375524\n",
      "Epoch 227/300\n",
      "Average training loss: 0.04615025595492787\n",
      "Average test loss: 0.002600952464983695\n",
      "Epoch 228/300\n",
      "Average training loss: 0.045981539643473096\n",
      "Average test loss: 0.0026044328310009506\n",
      "Epoch 229/300\n",
      "Average training loss: 0.08884000804026922\n",
      "Average test loss: 0.002100415369288789\n",
      "Epoch 230/300\n",
      "Average training loss: 0.05933134177658293\n",
      "Average test loss: 0.012609881897767385\n",
      "Epoch 231/300\n",
      "Average training loss: 0.05589453948868645\n",
      "Average test loss: 0.0020835827922241557\n",
      "Epoch 232/300\n",
      "Average training loss: 0.04884230985244115\n",
      "Average test loss: 0.0027687056058396894\n",
      "Epoch 233/300\n",
      "Average training loss: 0.047289117366075516\n",
      "Average test loss: 0.0020755568500608204\n",
      "Epoch 234/300\n",
      "Average training loss: 0.04635378661751747\n",
      "Average test loss: 0.004762495295662019\n",
      "Epoch 235/300\n",
      "Average training loss: 0.04605747703380055\n",
      "Average test loss: 0.0022553432879762517\n",
      "Epoch 236/300\n",
      "Average training loss: 0.04618210684425301\n",
      "Average test loss: 0.006575392779790693\n",
      "Epoch 237/300\n",
      "Average training loss: 0.04581823092699051\n",
      "Average test loss: 0.002156202249125474\n",
      "Epoch 238/300\n",
      "Average training loss: 0.04672868871026569\n",
      "Average test loss: 0.004844627083175712\n",
      "Epoch 239/300\n",
      "Average training loss: 0.046097705566220816\n",
      "Average test loss: 0.0020750814409305654\n",
      "Epoch 240/300\n",
      "Average training loss: 0.04758410814404487\n",
      "Average test loss: 0.0020673546995967624\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04587231265174018\n",
      "Average test loss: 0.002062876864646872\n",
      "Epoch 242/300\n",
      "Average training loss: 0.0461913637916247\n",
      "Average test loss: 33226789.39288889\n",
      "Epoch 243/300\n",
      "Average training loss: 0.04591623905963368\n",
      "Average test loss: 0.0020480990573349925\n",
      "Epoch 244/300\n",
      "Average training loss: 0.04791408697101805\n",
      "Average test loss: 0.0022495935716562803\n",
      "Epoch 245/300\n",
      "Average training loss: 0.04551862984564569\n",
      "Average test loss: 0.0030673788082268504\n",
      "Epoch 246/300\n",
      "Average training loss: 0.045598378148343825\n",
      "Average test loss: 0.012763548916412725\n",
      "Epoch 247/300\n",
      "Average training loss: 0.05013091167807579\n",
      "Average test loss: 0.002039461795654562\n",
      "Epoch 248/300\n",
      "Average training loss: 0.04622361710998747\n",
      "Average test loss: 0.0021669233658661445\n",
      "Epoch 249/300\n",
      "Average training loss: 0.04582791648308436\n",
      "Average test loss: 0.0021318206309030455\n",
      "Epoch 250/300\n",
      "Average training loss: 0.045072234027915534\n",
      "Average test loss: 0.0082781513908671\n",
      "Epoch 251/300\n",
      "Average training loss: 0.04546547414859136\n",
      "Average test loss: 0.0020807361826300622\n",
      "Epoch 252/300\n",
      "Average training loss: 0.04524548000097275\n",
      "Average test loss: 0.00522910347332557\n",
      "Epoch 253/300\n",
      "Average training loss: 0.05196175027887027\n",
      "Average test loss: 0.005620711405451099\n",
      "Epoch 254/300\n",
      "Average training loss: 0.04588159778383043\n",
      "Average test loss: 0.0036989451338433556\n",
      "Epoch 255/300\n",
      "Average training loss: 0.04498298097981347\n",
      "Average test loss: 0.0025870783397306998\n",
      "Epoch 256/300\n",
      "Average training loss: 0.0448813632329305\n",
      "Average test loss: 0.08562295966967941\n",
      "Epoch 257/300\n",
      "Average training loss: 0.04542724483874109\n",
      "Average test loss: 1.7661318174930702\n",
      "Epoch 258/300\n",
      "Average training loss: 0.04505868271489938\n",
      "Average test loss: 0.002198201431789332\n",
      "Epoch 259/300\n",
      "Average training loss: 0.045914245853821437\n",
      "Average test loss: 0.0021758307286848626\n",
      "Epoch 260/300\n",
      "Average training loss: 0.04504722600181898\n",
      "Average test loss: 0.00458135139486856\n",
      "Epoch 261/300\n",
      "Average training loss: 0.04503162924448649\n",
      "Average test loss: 0.0052692699635194405\n",
      "Epoch 262/300\n",
      "Average training loss: 0.06022005743450588\n",
      "Average test loss: 0.0032353696533375317\n",
      "Epoch 263/300\n",
      "Average training loss: 0.04527859260969692\n",
      "Average test loss: 0.0020880193662726216\n",
      "Epoch 264/300\n",
      "Average training loss: 0.04442306078804864\n",
      "Average test loss: 0.00324992966113819\n",
      "Epoch 265/300\n",
      "Average training loss: 0.044749167740345\n",
      "Average test loss: 0.002656657019837035\n",
      "Epoch 266/300\n",
      "Average training loss: 0.04540044004387326\n",
      "Average test loss: 0.0022701823733126125\n",
      "Epoch 267/300\n",
      "Average training loss: 0.04504203971558147\n",
      "Average test loss: 0.0020659250445250006\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04574510345194075\n",
      "Average test loss: 0.0021423229293690785\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04437886261608866\n",
      "Average test loss: 0.0021398757729265425\n",
      "Epoch 270/300\n",
      "Average training loss: 0.04447411945462227\n",
      "Average training loss: 0.04427099390162362\n",
      "Average test loss: 0.0021771143842488527\n",
      "Epoch 274/300\n",
      "Average training loss: 0.04605334750480122\n",
      "Average test loss: 0.0021633905385517412\n",
      "Epoch 275/300\n",
      "Average training loss: 0.044337340394655866\n",
      "Average test loss: 0.0021339080336814126\n",
      "Epoch 276/300\n",
      "Average training loss: 0.04401819181442261\n",
      "Average test loss: 0.002061295866345366\n",
      "Epoch 277/300\n",
      "Average training loss: 0.04442957607573933\n",
      "Average test loss: 0.00209529994138413\n",
      "Epoch 278/300\n",
      "Average training loss: 0.0441945272816552\n",
      "Average test loss: 0.0039782370349599255\n",
      "Epoch 279/300\n",
      "Average training loss: 0.043833566533194646\n",
      "Average test loss: 0.376169044473105\n",
      "Epoch 281/300\n",
      "Average training loss: 0.044265409015946916\n",
      "Average test loss: 0.003249830358972152\n",
      "Epoch 282/300\n",
      "Average training loss: 0.052611755874421864\n",
      "Average test loss: 26770102109.489777\n",
      "Epoch 283/300\n",
      "Average training loss: 0.21303988492488862\n",
      "Average test loss: 0.0021138811346350446\n",
      "Epoch 284/300\n",
      "Average training loss: 0.09029519002305136\n",
      "Average test loss: 0.0019659711632670626\n",
      "Epoch 285/300\n",
      "Average training loss: 0.07966575388444795\n",
      "Average test loss: 0.0705512056817404\n",
      "Epoch 286/300\n",
      "Average training loss: 0.06825885550181071\n",
      "Average test loss: 0.00503843463708957\n",
      "Epoch 288/300\n",
      "Average training loss: 0.06354525733656353\n",
      "Average test loss: 0.0019873304749942486\n",
      "Epoch 289/300\n",
      "Average training loss: 0.059602473553684025\n",
      "Average test loss: 0.26586646303161976\n",
      "Epoch 290/300\n",
      "Average training loss: 0.05651992999182807\n",
      "Average test loss: 0.028898146332552034\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05376806438962618\n",
      "Average test loss: 0.0020827591771052944\n",
      "Epoch 292/300\n",
      "Average training loss: 0.05133441579341889\n",
      "Average test loss: 0.0025685432317356267\n",
      "Epoch 293/300\n",
      "Average training loss: 0.049381770600875216\n",
      "Average training loss: 0.048424773385127386\n",
      "Average test loss: 0.029895066557659042\n",
      "Epoch 295/300\n",
      "Average training loss: 0.047602326224247614\n",
      "Average test loss: 0.0020648934061949452\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04625139523214764\n",
      "Average test loss: 0.0028843485729561913\n",
      "Epoch 297/300\n",
      "Average training loss: 0.04586264704995685\n",
      "Average test loss: 0.002940504723125034\n",
      "Epoch 299/300\n",
      "Average training loss: 0.04512370091014438\n",
      "Average test loss: 0.002322070200617115\n",
      "Epoch 300/300\n",
      "Average training loss: 0.045177509999937485\n",
      "Average test loss: 0.0020907204047673276\n",
      "Validation Size: 4500, Train Size: 18000\n",
      "Epoch 1/300\n",
      "Average training loss: 2776.8760469529893\n",
      "Average test loss: 15838859372.09681\n",
      "Epoch 2/300\n",
      "Average training loss: 38.79979586283366\n",
      "Average test loss: 64091463369.52436\n",
      "Epoch 3/300\n",
      "Average training loss: 34.86390037367079\n",
      "Average test loss: 164914.4257309511\n",
      "Epoch 4/300\n",
      "Average training loss: 31.40487478129069\n",
      "Average test loss: 75602667.84423956\n",
      "Epoch 5/300\n",
      "Average training loss: 26.088722313774955\n",
      "Average test loss: 27509.93995657703\n",
      "Epoch 6/300\n",
      "Average training loss: 23.096170194837782\n",
      "Average test loss: 126581320519.68317\n",
      "Epoch 7/300\n",
      "Average training loss: 23.512182523939344\n",
      "Average test loss: 73828277672.47125\n",
      "Epoch 8/300\n",
      "Average training loss: 21.81103325229221\n",
      "Average test loss: 72640318062.15378\n",
      "Epoch 9/300\n",
      "Average training loss: 19.42657600063748\n",
      "Average test loss: 2771460.9793807734\n",
      "Epoch 10/300\n",
      "Average training loss: 17.556190695868597\n",
      "Average training loss: 15.858620869954427\n",
      "Average test loss: 15278454.516756877\n",
      "Epoch 12/300\n",
      "Average training loss: 14.796966470506456\n",
      "Average test loss: 0.0046987257450819015\n",
      "Epoch 13/300\n",
      "Average training loss: 13.811107226901584\n",
      "Average test loss: 8087.02200467533\n",
      "Epoch 14/300\n",
      "Average training loss: 12.925141717698839\n",
      "Average test loss: 0.0947024330397447\n",
      "Epoch 15/300\n",
      "Average training loss: 11.964481897142198\n",
      "Average test loss: 33776.20801899679\n",
      "Epoch 16/300\n",
      "Average training loss: 11.016886150783963\n",
      "Average test loss: 0.0034716067121674616\n",
      "Epoch 17/300\n",
      "Average training loss: 10.186296278211806\n",
      "Average test loss: 158.00842673002805\n",
      "Epoch 18/300\n",
      "Average training loss: 8.941496512518988\n",
      "Average test loss: 4660.245903611825\n",
      "Epoch 20/300\n",
      "Average training loss: 8.357701018439398\n",
      "Average test loss: 0.08575169323695203\n",
      "Epoch 21/300\n",
      "Average training loss: 7.518324376424154\n",
      "Average test loss: 0.0028875703007603683\n",
      "Epoch 22/300\n",
      "Average training loss: 7.15527029715644\n",
      "Average test loss: 0.03151965446666711\n",
      "Epoch 23/300\n",
      "Average training loss: 6.7183351275126135\n",
      "Average test loss: 5.236593195535242\n",
      "Epoch 24/300\n",
      "Average training loss: 6.222367539723714\n",
      "Average test loss: 0.25626317910249863\n",
      "Epoch 25/300\n",
      "Average training loss: 5.522927513970269\n",
      "Average test loss: 102.37135091835736\n",
      "Epoch 26/300\n",
      "Average training loss: 5.352363817426894\n",
      "Average test loss: 0.00815465769937469\n",
      "Epoch 27/300\n",
      "Average training loss: 4.7405936571757\n",
      "Average test loss: 0.008911684307373232\n",
      "Epoch 28/300\n",
      "Average training loss: 4.38065180545383\n",
      "Average test loss: 0.002376413636530439\n",
      "Epoch 29/300\n",
      "Average training loss: 3.96489435450236\n",
      "Average test loss: 0.0021705125286761258\n",
      "Epoch 30/300\n",
      "Average training loss: 3.4600076866149903\n",
      "Average test loss: 0.12766128906607627\n",
      "Epoch 32/300\n",
      "Average training loss: 3.631502566231622\n",
      "Average test loss: 0.0027547896491984525\n",
      "Epoch 33/300\n",
      "Average training loss: 3.6098226278093124\n",
      "Average test loss: 4.7604508612313206\n",
      "Epoch 34/300\n",
      "Average training loss: 3.1679047094980874\n",
      "Average test loss: 0.0020102803697809577\n",
      "Epoch 35/300\n",
      "Average training loss: 2.982606679280599\n",
      "Average test loss: 0.417215378026271\n",
      "Epoch 36/300\n",
      "Average training loss: 2.7105377498202854\n",
      "Average test loss: 0.0018296694873521726\n",
      "Epoch 37/300\n",
      "Average training loss: 2.482211293114556\n",
      "Average training loss: 2.2646564371320936\n",
      "Average test loss: 0.0020780239169382385\n",
      "Epoch 39/300\n",
      "Average training loss: 2.0063522129058837\n",
      "Average test loss: 0.0021489992541157536\n",
      "Epoch 40/300\n",
      "Average training loss: 1.7568581514358521\n",
      "Average test loss: 0.013361556369604335\n",
      "Epoch 41/300\n",
      "Average training loss: 1.5693039880328707\n",
      "Average test loss: 0.0023061067399879295\n",
      "Epoch 42/300\n",
      "Average training loss: 1.3941794147491455\n",
      "Average test loss: 0.001762757732739879\n",
      "Epoch 43/300\n",
      "Average training loss: 1.2502386774486967\n",
      "Average test loss: 0.007772993415801062\n",
      "Epoch 44/300\n",
      "Average training loss: 1.1192708683013917\n",
      "Average test loss: 0.0016899329756593538\n",
      "Epoch 45/300\n",
      "Average training loss: 0.9918528672854106\n",
      "Average test loss: 0.0016880622662396895\n",
      "Epoch 46/300\n",
      "Average training loss: 0.8848760955598619\n",
      "Average test loss: 0.00161385264651229\n",
      "Epoch 47/300\n",
      "Average training loss: 0.792414359357622\n",
      "Average test loss: 0.0050763036786682075\n",
      "Epoch 48/300\n",
      "Average training loss: 0.7077707995838589\n",
      "Average test loss: 0.0019186577608601915\n",
      "Epoch 49/300\n",
      "Average training loss: 0.6365287763277689\n",
      "Average test loss: 0.009053374197644492\n",
      "Epoch 50/300\n",
      "Average training loss: 0.5715446544753181\n",
      "Average test loss: 0.001541160416478912\n",
      "Epoch 51/300\n",
      "Average training loss: 0.5120481550958421\n",
      "Average test loss: 0.002138565259675185\n",
      "Epoch 52/300\n",
      "Average training loss: 0.45890245347552827\n",
      "Average test loss: 9.119383415309919\n",
      "Epoch 53/300\n",
      "Average training loss: 0.4120524635050032\n",
      "Average test loss: 0.0017737270171443622\n",
      "Epoch 54/300\n",
      "Average training loss: 0.3673792694409688\n",
      "Average test loss: 0.01649023487449934\n",
      "Epoch 55/300\n",
      "Average training loss: 0.3296830375459459\n",
      "Average test loss: 0.006188286779655351\n",
      "Epoch 56/300\n",
      "Average training loss: 0.2962397439214918\n",
      "Average test loss: 0.013114784998612272\n",
      "Epoch 57/300\n",
      "Average training loss: 0.2676746354765362\n",
      "Average test loss: 14.190369390019216\n",
      "Epoch 58/300\n",
      "Average training loss: 0.24223366689682008\n",
      "Average test loss: 0.0016803829375033577\n",
      "Epoch 59/300\n",
      "Average training loss: 0.22140341181225248\n",
      "Average test loss: 0.006945323599502444\n",
      "Epoch 60/300\n",
      "Average training loss: 0.20440090001953973\n",
      "Average test loss: 0.0014776336358239253\n",
      "Epoch 61/300\n",
      "Average training loss: 0.18569736976093715\n",
      "Average test loss: 0.0014948828952490456\n",
      "Epoch 62/300\n",
      "Average training loss: 0.17131478575865428\n",
      "Average test loss: 0.001614590823236439\n",
      "Epoch 63/300\n",
      "Average training loss: 0.1577588348057535\n",
      "Average test loss: 2.9041668485451697\n",
      "Epoch 64/300\n",
      "Average training loss: 0.14576208137141333\n",
      "Average test loss: 0.011720148209482431\n",
      "Epoch 65/300\n",
      "Average training loss: 0.13466274891297023\n",
      "Average test loss: 0.0013853362893892658\n",
      "Epoch 66/300\n",
      "Average training loss: 0.1257144653134876\n",
      "Average test loss: 28775.951543093335\n",
      "Epoch 67/300\n",
      "Average training loss: 0.1160240880979432\n",
      "Average test loss: 0.0013749233183140556\n",
      "Epoch 68/300\n",
      "Average training loss: 0.10841795675622093\n",
      "Average test loss: 0.595390607236988\n",
      "Epoch 69/300\n",
      "Average training loss: 0.10208547661701838\n",
      "Average test loss: 0.0015685338544555835\n",
      "Epoch 70/300\n",
      "Average training loss: 0.09620084602302975\n",
      "Average test loss: 0.0012988669944720135\n",
      "Epoch 71/300\n",
      "Average training loss: 0.09038441687822342\n",
      "Average test loss: 0.0013517997989224062\n",
      "Epoch 72/300\n",
      "Average training loss: 0.08595328818427192\n",
      "Average test loss: 0.001478657129738066\n",
      "Epoch 73/300\n",
      "Average training loss: 0.08196709795130623\n",
      "Average test loss: 0.0034244044890834224\n",
      "Epoch 74/300\n",
      "Average training loss: 0.07723618925280042\n",
      "Average test loss: 0.005795102519190146\n",
      "Epoch 75/300\n",
      "Average training loss: 0.07522993173201879\n",
      "Average test loss: 0.1799978373547395\n",
      "Epoch 76/300\n",
      "Average training loss: 0.0716939081715213\n",
      "Average test loss: 0.0013300450184485979\n",
      "Epoch 77/300\n",
      "Average training loss: 0.06839973859654533\n",
      "Average test loss: 0.0012799891239653031\n",
      "Epoch 78/300\n",
      "Average training loss: 0.06589204168319703\n",
      "Average test loss: 0.0015555453151464462\n",
      "Epoch 79/300\n",
      "Average training loss: 0.06365379949741894\n",
      "Average test loss: 0.001713298695989781\n",
      "Epoch 80/300\n",
      "Average training loss: 0.061552111284600364\n",
      "Average test loss: 0.014793104194104671\n",
      "Epoch 81/300\n",
      "Average training loss: 0.06105981703599294\n",
      "Average test loss: 0.0014399200787560808\n",
      "Epoch 82/300\n",
      "Average training loss: 0.05849480318029721\n",
      "Average test loss: 0.0013574279670914013\n",
      "Epoch 83/300\n",
      "Average training loss: 0.05644893894592921\n",
      "Average test loss: 0.002581976641797357\n",
      "Epoch 84/300\n",
      "Average training loss: 0.055032626105679404\n",
      "Average test loss: 0.016473014983865948\n",
      "Epoch 85/300\n",
      "Average training loss: 0.053622422609064316\n",
      "Average test loss: 0.0026411632457748056\n",
      "Epoch 86/300\n",
      "Average training loss: 0.05241262541876899\n",
      "Average test loss: 0.0013204660286299057\n",
      "Epoch 87/300\n",
      "Average training loss: 0.05139041035042869\n",
      "Average test loss: 9.76244668823491\n",
      "Epoch 88/300\n",
      "Average training loss: 0.05087185122238265\n",
      "Average test loss: 0.8116282243016694\n",
      "Epoch 89/300\n",
      "Average training loss: 0.049813007245461144\n",
      "Average test loss: 0.001546265611735483\n",
      "Epoch 90/300\n",
      "Average training loss: 0.049257265935341514\n",
      "Average test loss: 0.002704432936385274\n",
      "Epoch 91/300\n",
      "Average training loss: 0.04828363283806377\n",
      "Average test loss: 0.002413955439089073\n",
      "Epoch 92/300\n",
      "Average training loss: 0.04781860909197066\n",
      "Average test loss: 24155.301721272786\n",
      "Epoch 93/300\n",
      "Average training loss: 0.047373177740308976\n",
      "Average test loss: 0.00397624570524527\n",
      "Epoch 94/300\n",
      "Average training loss: 0.04714114283190833\n",
      "Average test loss: 0.0035385439599760703\n",
      "Epoch 95/300\n",
      "Average training loss: 0.04699739336305194\n",
      "Average test loss: 0.003354050355239047\n",
      "Epoch 96/300\n",
      "Average training loss: 0.045890266398588814\n",
      "Average test loss: 0.3070522983877195\n",
      "Epoch 97/300\n",
      "Average training loss: 0.055753484699461196\n",
      "Average test loss: 0.08882728533364005\n",
      "Epoch 98/300\n",
      "Average training loss: 0.04785434583822886\n",
      "Average test loss: 1630.5267538139224\n",
      "Epoch 99/300\n",
      "Average training loss: 0.04585048814614614\n",
      "Average test loss: 1.39048706375673\n",
      "Epoch 100/300\n",
      "Average training loss: 0.044955809576643836\n",
      "Average test loss: 0.0041219673264357775\n",
      "Epoch 101/300\n",
      "Average training loss: 0.04458126732375887\n",
      "Average test loss: 0.0015038530347454878\n",
      "Epoch 102/300\n",
      "Average training loss: 0.044332020804286006\n",
      "Average test loss: 0.002183599122696453\n",
      "Epoch 103/300\n",
      "Average training loss: 0.044174767394860585\n",
      "Average test loss: 0.001698737036022875\n",
      "Epoch 104/300\n",
      "Average training loss: 0.043792432592974766\n",
      "Average test loss: 0.4437456808372711\n",
      "Epoch 105/300\n",
      "Average training loss: 0.04314201046029727\n",
      "Average test loss: 0.0013439183851910962\n",
      "Epoch 106/300\n",
      "Average training loss: 0.043158834291829005\n",
      "Average test loss: 0.09200270353754361\n",
      "Epoch 107/300\n",
      "Average training loss: 0.0427962820397483\n",
      "Average test loss: 0.001696120352587766\n",
      "Epoch 108/300\n",
      "Average training loss: 0.042404418402247956\n",
      "Average test loss: 0.0022212365611145894\n",
      "Epoch 109/300\n",
      "Average training loss: 0.04227305896745788\n",
      "Average test loss: 0.0025186078006194696\n",
      "Epoch 110/300\n",
      "Average training loss: 0.042116054501798415\n",
      "Average test loss: 0.0014877352623475922\n",
      "Epoch 111/300\n",
      "Average training loss: 0.04078470867872238\n",
      "Average test loss: 0.008431912577400604\n",
      "Epoch 115/300\n",
      "Average training loss: 0.04076242829196983\n",
      "Average test loss: 0.5631769611462951\n",
      "Epoch 116/300\n",
      "Average training loss: 0.04063020208809111\n",
      "Average test loss: 0.0018095648700578344\n",
      "Epoch 117/300\n",
      "Average training loss: 0.04066821713580025\n",
      "Average test loss: 0.08048371518734429\n",
      "Epoch 118/300\n",
      "Average training loss: 0.0401489192545414\n",
      "Average test loss: 2.2221961530910597\n",
      "Epoch 119/300\n",
      "Average training loss: 0.0410434651904636\n",
      "Average test loss: 0.002897983768540952\n",
      "Epoch 120/300\n",
      "Average training loss: 0.039651037242677475\n",
      "Average test loss: 0.0015877983342442248\n",
      "Epoch 121/300\n",
      "Average training loss: 0.03960247552394867\n",
      "Average test loss: 0.003142600765037868\n",
      "Epoch 123/300\n",
      "Average training loss: 0.03975581420792473\n",
      "Average test loss: 0.0026009316058415504\n",
      "Epoch 124/300\n",
      "Average training loss: 0.039120445503128896\n",
      "Average test loss: 0.001500576831607355\n",
      "Epoch 125/300\n",
      "Average training loss: 0.03881005473269357\n",
      "Average test loss: 0.001430823128360013\n",
      "Epoch 126/300\n",
      "Average training loss: 0.038858642829789056\n",
      "Average test loss: 0.0019513592266788085\n",
      "Epoch 127/300\n",
      "Average training loss: 0.038750782188442016\n",
      "Average test loss: 0.0033992627503143416\n",
      "Epoch 128/300\n",
      "Average training loss: 0.03879627054929733\n",
      "Average test loss: 0.001517088166438043\n",
      "Epoch 130/300\n",
      "Average training loss: 0.03831020563840866\n",
      "Average test loss: 0.029617517289188173\n",
      "Epoch 131/300\n",
      "Average training loss: 0.038159083031945755\n",
      "Average test loss: 0.0017339770526935657\n",
      "Epoch 132/300\n",
      "Average training loss: 0.03814313640197118\n",
      "Average test loss: 2.1312506532420716\n",
      "Epoch 133/300\n",
      "Average training loss: 0.03817984329495165\n",
      "Average test loss: 0.001473457464068714\n",
      "Epoch 134/300\n",
      "Average training loss: 0.03789183017280367\n",
      "Average test loss: 0.0014801455803422465\n",
      "Epoch 135/300\n",
      "Average training loss: 0.037514168116781446\n",
      "Average test loss: 0.0031771310534742143\n",
      "Epoch 137/300\n",
      "Average training loss: 0.037577105939388275\n",
      "Average test loss: 0.7902862580501371\n",
      "Epoch 138/300\n",
      "Average training loss: 0.03726875906189283\n",
      "Average test loss: 0.0018551997018771038\n",
      "Epoch 139/300\n",
      "Average training loss: 0.040424860460890666\n",
      "Average test loss: 0.0016028734605448942\n",
      "Epoch 140/300\n",
      "Average training loss: 0.036961487601200736\n",
      "Average test loss: 4.30443988887469\n",
      "Epoch 141/300\n",
      "Average training loss: 0.036891019092665775\n",
      "Average test loss: 0.0014127321796905663\n",
      "Epoch 142/300\n",
      "Average training loss: 0.0370165122879876\n",
      "Average test loss: 0.0023225873630079957\n",
      "Epoch 143/300\n",
      "Average training loss: 0.03685388648841116\n",
      "Average test loss: 0.0017121355453920033\n",
      "Epoch 144/300\n",
      "Average training loss: 0.03693808423479398\n",
      "Average test loss: 0.0016281849209012257\n",
      "Epoch 145/300\n",
      "Average training loss: 0.036631722340981165\n",
      "Average test loss: 0.016906689698083534\n",
      "Epoch 146/300\n",
      "Average training loss: 0.03663113228811158\n",
      "Average test loss: 0.0015338305810259447\n",
      "Epoch 147/300\n",
      "Average training loss: 0.08322313383221626\n",
      "Average test loss: 0.0016671969627754556\n",
      "Epoch 148/300\n",
      "Average training loss: 0.06397185153100225\n",
      "Average test loss: 0.0056767694801092144\n",
      "Epoch 149/300\n",
      "Average training loss: 0.05369098098079363\n",
      "Average test loss: 0.001375982707987229\n",
      "Epoch 150/300\n",
      "Average training loss: 0.04888559983505143\n",
      "Average test loss: 0.0013090779952601426\n",
      "Epoch 151/300\n",
      "Average training loss: 0.04545977691147063\n",
      "Average test loss: 1.193256237110744\n",
      "Epoch 152/300\n",
      "Average training loss: 0.04301729200614823\n",
      "Average test loss: 0.001362305007254084\n",
      "Epoch 153/300\n",
      "Average training loss: 0.04083725869324472\n",
      "Average test loss: 0.0018313471330329775\n",
      "Epoch 154/300\n",
      "Average training loss: 0.039421094725529356\n",
      "Average test loss: 0.04747040429297421\n",
      "Epoch 155/300\n",
      "Average training loss: 0.037769437371028794\n",
      "Average test loss: 1.3281717558784618\n",
      "Epoch 157/300\n",
      "Average training loss: 0.036766227159235214\n",
      "Average test loss: 0.001387734745318691\n",
      "Epoch 158/300\n",
      "Average training loss: 0.03627015951938099\n",
      "Average test loss: 0.001629954817601376\n",
      "Epoch 160/300\n",
      "Average training loss: 0.036097664074765314\n",
      "Average test loss: 8.052819699970385\n",
      "Epoch 161/300\n",
      "Average training loss: 0.3662847718331549\n",
      "Average test loss: 0.05239461040165689\n",
      "Epoch 162/300\n",
      "Average training loss: 0.09001122591230605\n",
      "Average test loss: 0.0017380299977958202\n",
      "Epoch 163/300\n",
      "Average training loss: 0.07431923819250531\n",
      "Average test loss: 0.0028772221700184876\n",
      "Epoch 164/300\n",
      "Average training loss: 0.06393602051999833\n",
      "Average test loss: 0.19982346369326115\n",
      "Epoch 165/300\n",
      "Average training loss: 0.05704281422826979\n",
      "Average test loss: 0.0013839021324076586\n",
      "Epoch 166/300\n",
      "Average training loss: 0.053165669427977666\n",
      "Average test loss: 0.0014079756627066269\n",
      "Epoch 167/300\n",
      "Average training loss: 0.050244429849916034\n",
      "Average test loss: 0.0013874166009740696\n",
      "Epoch 168/300\n",
      "Average training loss: 0.04764697058664428\n",
      "Average test loss: 0.0037022515177312825\n",
      "Epoch 169/300\n",
      "Average training loss: 0.04539191937446594\n",
      "Average test loss: 0.0015764269528703557\n",
      "Epoch 170/300\n",
      "Average training loss: 0.043528581834501694\n",
      "Average test loss: 0.0014211235023621055\n",
      "Epoch 171/300\n",
      "Average training loss: 0.041904335392846\n",
      "Average test loss: 0.0013651330491734875\n",
      "Epoch 172/300\n",
      "Average training loss: 0.040652532024516\n",
      "Average test loss: 0.0013331116610724066\n",
      "Epoch 173/300\n",
      "Average training loss: 0.08724831748008728\n",
      "Average test loss: 0.0013683021496981383\n",
      "Epoch 174/300\n",
      "Average training loss: 0.05207792759272787\n",
      "Average test loss: 0.010521386491341723\n",
      "Epoch 175/300\n",
      "Average training loss: 0.046552885111835265\n",
      "Average test loss: 0.001373366330853767\n",
      "Epoch 176/300\n",
      "Average training loss: 0.0435791121688154\n",
      "Average test loss: 0.001439636257580585\n",
      "Epoch 177/300\n",
      "Average training loss: 0.04159147865242428\n",
      "Average test loss: 0.001441293304061724\n",
      "Epoch 178/300\n",
      "Average training loss: 0.04022960828741391\n",
      "Average test loss: 0.0014991507482611471\n",
      "Epoch 179/300\n",
      "Average training loss: 0.03931738500793775\n",
      "Average test loss: 0.001380212187870509\n",
      "Epoch 180/300\n",
      "Average training loss: 0.03833072351581521\n",
      "Average test loss: 0.0013788780234754085\n",
      "Epoch 181/300\n",
      "Average training loss: 0.038042485535144804\n",
      "Average test loss: 0.0014613497761181659\n",
      "Epoch 182/300\n",
      "Average training loss: 0.037358999970886445\n",
      "Average test loss: 0.010018657604853312\n",
      "Epoch 183/300\n",
      "Average training loss: 0.037073921369181737\n",
      "Average test loss: 0.001550403395978113\n",
      "Epoch 184/300\n",
      "Average training loss: 0.036985943410131665\n",
      "Average test loss: 332085056.90577775\n",
      "Epoch 185/300\n",
      "Average training loss: 0.10698867835932308\n",
      "Average test loss: 0.0016609314870503213\n",
      "Epoch 186/300\n",
      "Average training loss: 0.05498948543932703\n",
      "Average test loss: 0.012513697730066876\n",
      "Epoch 187/300\n",
      "Average training loss: 0.04908710582057635\n",
      "Average test loss: 0.02857458796021011\n",
      "Epoch 188/300\n",
      "Average training loss: 0.04509668238792155\n",
      "Average test loss: 1452.898727420383\n",
      "Epoch 189/300\n",
      "Average training loss: 0.04216170616944631\n",
      "Average test loss: 0.0014294913742277358\n",
      "Epoch 190/300\n",
      "Average training loss: 0.04024945674008793\n",
      "Average test loss: 0.0018012751524026196\n",
      "Epoch 191/300\n",
      "Average training loss: 0.03899910535249445\n",
      "Average test loss: 0.0015504890642542807\n",
      "Epoch 192/300\n",
      "Average training loss: 0.038005261396368346\n",
      "Average test loss: 0.001378552722123762\n",
      "Epoch 193/300\n",
      "Average training loss: 0.03734010075529416\n",
      "Average test loss: 0.0032360302929042113\n",
      "Epoch 194/300\n",
      "Average training loss: 0.036856866886218385\n",
      "Average test loss: 0.1248218464296725\n",
      "Epoch 195/300\n",
      "Average training loss: 0.08074652001923985\n",
      "Average test loss: 0.006774450076123079\n",
      "Epoch 196/300\n",
      "Average training loss: 0.055984598944584525\n",
      "Average test loss: 0.023673510343871183\n",
      "Epoch 197/300\n",
      "Average training loss: 0.04857983093129264\n",
      "Average test loss: 0.0014121610796492962\n",
      "Epoch 198/300\n",
      "Average training loss: 0.04378043980730904\n",
      "Average test loss: 36754.30086979167\n",
      "Epoch 199/300\n",
      "Average training loss: 0.041984943757454554\n",
      "Average test loss: 0.08000103044841024\n",
      "Epoch 200/300\n",
      "Average training loss: 0.1199368751347065\n",
      "Average test loss: 0.0024617932722386386\n",
      "Epoch 202/300\n",
      "Average training loss: 0.058578218244844014\n",
      "Average test loss: 0.010935629908926785\n",
      "Epoch 203/300\n",
      "Average training loss: 0.05255223358008597\n",
      "Average test loss: 0.0033007083974985613\n",
      "Epoch 204/300\n",
      "Average training loss: 0.04870375734898779\n",
      "Average test loss: 0.001631482641523083\n",
      "Epoch 205/300\n",
      "Average training loss: 0.045535419868098365\n",
      "Average test loss: 0.0023229570749940145\n",
      "Epoch 206/300\n",
      "Average training loss: 0.04334195139010747\n",
      "Average test loss: 0.016207819439056848\n",
      "Epoch 207/300\n",
      "Average training loss: 0.041702842563390734\n",
      "Average test loss: 0.006235321843582723\n",
      "Epoch 208/300\n",
      "Average training loss: 0.042255928243199986\n",
      "Average test loss: 0.0213851691811449\n",
      "Epoch 209/300\n",
      "Average training loss: 0.04026967564887471\n",
      "Average test loss: 0.0016131771590767636\n",
      "Epoch 210/300\n",
      "Average training loss: 0.03906885399089919\n",
      "Average training loss: 0.047178346633911135\n",
      "Average test loss: 0.003171187458559871\n",
      "Epoch 212/300\n",
      "Average training loss: 0.041788969056473835\n",
      "Average test loss: 0.0013768177949306038\n",
      "Epoch 213/300\n",
      "Average training loss: 0.03853510857621829\n",
      "Average test loss: 0.0013947460361652905\n",
      "Epoch 214/300\n",
      "Average training loss: 0.03803404954738087\n",
      "Average test loss: 0.0015302863412847121\n",
      "Epoch 215/300\n",
      "Average training loss: 0.0374393345117569\n",
      "Average test loss: 0.009294267226010561\n",
      "Epoch 216/300\n",
      "Average training loss: 0.037465946955813303\n",
      "Average test loss: 0.5303451170515683\n",
      "Epoch 217/300\n",
      "Average training loss: 0.03747140448623233\n",
      "Average test loss: 238.27482370743488\n",
      "Epoch 218/300\n",
      "Average training loss: 0.03742426103519069\n",
      "Average test loss: 0.002825634972088867\n",
      "Epoch 219/300\n",
      "Average training loss: 0.03847695040040546\n",
      "Average test loss: 58.50327904647258\n",
      "Epoch 220/300\n",
      "Average training loss: 0.03639617299371296\n",
      "Average test loss: 0.061656205885940125\n",
      "Epoch 222/300\n",
      "Average training loss: 0.03675165359179179\n",
      "Average test loss: 0.0024098504171706737\n",
      "Epoch 223/300\n",
      "Average training loss: 0.036417119870583214\n",
      "Average test loss: 0.005115194388561778\n",
      "Epoch 224/300\n",
      "Average training loss: 0.03582228277458085\n",
      "Average test loss: 0.001702486059938868\n",
      "Epoch 225/300\n",
      "Average training loss: 0.03629043074448903\n",
      "Average test loss: 1586877.3246526693\n",
      "Epoch 226/300\n",
      "Average training loss: 0.035802493300702835\n",
      "Average test loss: 0.011824761489199268\n",
      "Epoch 227/300\n",
      "Average training loss: 0.03566246946321593\n",
      "Average test loss: 0.001957708099339571\n",
      "Epoch 229/300\n",
      "Average training loss: 0.03535651798711883\n",
      "Average test loss: 0.0015077947178441617\n",
      "Epoch 230/300\n",
      "Average training loss: 0.035146161903937656\n",
      "Average test loss: 0.0023129297188586658\n",
      "Epoch 231/300\n",
      "Average training loss: 0.03573019535011716\n",
      "Average test loss: 0.002306540494577752\n",
      "Epoch 232/300\n",
      "Average training loss: 0.034925160802072946\n",
      "Average test loss: 0.11418952866850628\n",
      "Epoch 233/300\n",
      "Average training loss: 0.03569945959250132\n",
      "Average test loss: 0.008348256361981233\n",
      "Epoch 234/300\n",
      "Average training loss: 0.03583353048563004\n",
      "Average test loss: 0.6943198369832503\n",
      "Epoch 235/300\n",
      "Average training loss: 0.034748224804798764\n",
      "Average test loss: 0.001415187657189866\n",
      "Epoch 237/300\n",
      "Average training loss: 0.034639673988024394\n",
      "Average test loss: 0.001496379335100452\n",
      "Epoch 238/300\n",
      "Average training loss: 0.057471643689605924\n",
      "Average test loss: 5343.337066946344\n",
      "Epoch 239/300\n",
      "Average training loss: 0.07451110296779209\n",
      "Average test loss: 0.0019291815602531035\n",
      "Epoch 240/300\n",
      "Average training loss: 0.05020302623841497\n",
      "Average test loss: 0.0013215926771776544\n",
      "Epoch 241/300\n",
      "Average training loss: 0.04047104457683033\n",
      "Average test loss: 0.0016142054182580775\n",
      "Epoch 243/300\n",
      "Average training loss: 0.03810745710962349\n",
      "Average test loss: 1.8461615852990911\n",
      "Epoch 244/300\n",
      "Average training loss: 0.03670282537738482\n",
      "Average test loss: 0.011705132235669427\n",
      "Epoch 245/300\n",
      "Average training loss: 0.03562173974017302\n",
      "Average test loss: 0.0015965759755215711\n",
      "Epoch 246/300\n",
      "Average training loss: 0.03630663585662842\n",
      "Average test loss: 0.0014937587541838486\n",
      "Epoch 247/300\n",
      "Average training loss: 0.03477549667656422\n",
      "Average test loss: 0.001429828078382545\n",
      "Epoch 248/300\n",
      "Average training loss: 0.034140321804417506\n",
      "Average test loss: 0.0015215204970704184\n",
      "Epoch 249/300\n",
      "Average training loss: 0.034114434265428116\n",
      "Average test loss: 0.0017948303504122629\n",
      "Epoch 250/300\n",
      "Average training loss: 0.03416198086076313\n",
      "Average test loss: 0.005933945671655238\n",
      "Epoch 251/300\n",
      "Average training loss: 0.03437950848208533\n",
      "Average test loss: 0.10982290419998268\n",
      "Epoch 252/300\n",
      "Average training loss: 0.03469540989564525\n",
      "Average test loss: 0.031249641594373517\n",
      "Epoch 253/300\n",
      "Average training loss: 0.03402010578744941\n",
      "Average test loss: 0.0014821297529463966\n",
      "Epoch 254/300\n",
      "Average training loss: 0.03435146289401584\n",
      "Average test loss: 0.0014772211174584097\n",
      "Epoch 256/300\n",
      "Average training loss: 0.03519478737314542\n",
      "Average test loss: 0.009230863681270017\n",
      "Epoch 257/300\n",
      "Average training loss: 0.03663083093033897\n",
      "Average test loss: 0.0014856969916986094\n",
      "Epoch 258/300\n",
      "Average training loss: 0.033793492358591824\n",
      "Average test loss: 0.002567402993225389\n",
      "Epoch 259/300\n",
      "Average training loss: 0.03363594049215317\n",
      "Average test loss: 0.0014150008936930033\n",
      "Epoch 260/300\n",
      "Average training loss: 0.033619476136234075\n",
      "Average test loss: 0.9280467174665795\n",
      "Epoch 261/300\n",
      "Average training loss: 0.03360093098051018\n",
      "Average test loss: 0.0015345342073382602\n",
      "Epoch 262/300\n",
      "Average training loss: 0.03367230835888121\n",
      "Average test loss: 0.6325353197913823\n",
      "Epoch 263/300\n",
      "Average test loss: 0.004227476716662446\n",
      "Epoch 264/300\n",
      "Average training loss: 0.07000686355431875\n",
      "Average test loss: 699.7041583930121\n",
      "Epoch 265/300\n",
      "Average training loss: 0.06148134458065033\n",
      "Average test loss: 0.00724742571823299\n",
      "Epoch 266/300\n",
      "Average training loss: 0.055612474819024406\n",
      "Average test loss: 0.0013914707305116786\n",
      "Epoch 267/300\n",
      "Average training loss: 0.05147813354598151\n",
      "Average test loss: 0.0013485602765447564\n",
      "Epoch 268/300\n",
      "Average training loss: 0.04831026283899943\n",
      "Average test loss: 0.002154237355209059\n",
      "Epoch 269/300\n",
      "Average training loss: 0.04298079181048605\n",
      "Average test loss: 6.621784686668051\n",
      "Epoch 271/300\n",
      "Average training loss: 0.04065289560953776\n",
      "Average test loss: 0.003754408417476548\n",
      "Epoch 272/300\n",
      "Average training loss: 0.03891623564892345\n",
      "Average test loss: 0.0448943533135785\n",
      "Epoch 273/300\n",
      "Average training loss: 0.03756547410620584\n",
      "Average test loss: 0.0018802582633992036\n",
      "Epoch 274/300\n",
      "Average training loss: 0.036285199996497895\n",
      "Average test loss: 0.001493289527328064\n",
      "Epoch 275/300\n",
      "Average training loss: 0.035505132569207086\n",
      "Average test loss: 0.0014801232496069537\n",
      "Epoch 276/300\n",
      "Average training loss: 0.034778555830319725\n",
      "Average test loss: 0.0719176950926582\n",
      "Epoch 278/300\n",
      "Average training loss: 0.034285539193285836\n",
      "Average test loss: 34.80080190520816\n",
      "Epoch 279/300\n",
      "Average test loss: 0.0013965689146684275\n",
      "Epoch 280/300\n",
      "Average training loss: 0.03629114374849531\n",
      "Average test loss: 54.48809120517307\n",
      "Epoch 281/300\n",
      "Average training loss: 0.04383410809437434\n",
      "Average test loss: 0.007310242387569613\n",
      "Epoch 282/300\n",
      "Average training loss: 0.034816369977262285\n",
      "Average test loss: 0.0014212341891187761\n",
      "Epoch 283/300\n",
      "Average training loss: 0.033942056904236476\n",
      "Average test loss: 0.0013885876805418067\n",
      "Epoch 284/300\n",
      "Average training loss: 0.03373424849245284\n",
      "Average test loss: 0.0025068779674669106\n",
      "Epoch 285/300\n",
      "Average training loss: 0.03344795945286751\n",
      "Average test loss: 0.001417768057435751\n",
      "Epoch 286/300\n",
      "Average training loss: 0.03361262153420183\n",
      "Average test loss: 0.0017036103173676464\n",
      "Epoch 287/300\n",
      "Average training loss: 0.0338890923311313\n",
      "Average test loss: 0.007016982298965255\n",
      "Epoch 288/300\n",
      "Average training loss: 0.034115639160076774\n",
      "Average test loss: 0.0014089448078432017\n",
      "Epoch 289/300\n",
      "Average training loss: 0.14503041837281652\n",
      "Average test loss: 0.006013753931969405\n",
      "Epoch 290/300\n",
      "Average training loss: 0.06985026450620757\n",
      "Average test loss: 0.009494914082189401\n",
      "Epoch 291/300\n",
      "Average training loss: 0.05924216232697169\n",
      "Average test loss: 0.001345291172568169\n",
      "Epoch 292/300\n",
      "Average training loss: 0.053406278885073134\n",
      "Average test loss: 0.0014076658645644784\n",
      "Epoch 293/300\n",
      "Average training loss: 0.049192622916566\n",
      "Average test loss: 0.0031550865154713393\n",
      "Epoch 294/300\n",
      "Average training loss: 0.045944726013475\n",
      "Average test loss: 0.001783484754876958\n",
      "Epoch 295/300\n",
      "Average training loss: 0.04324853766957919\n",
      "Average test loss: 72136380.34783357\n",
      "Epoch 296/300\n",
      "Average training loss: 0.04116646419962247\n",
      "Average test loss: 0.001536440816293988\n",
      "Epoch 297/300\n",
      "Average training loss: 0.039116892943779624\n",
      "Average test loss: 0.0014372568887968857\n",
      "Epoch 298/300\n",
      "Average training loss: 0.04307379915813605\n",
      "Average test loss: 0.0024721159877048597\n",
      "Epoch 299/300\n",
      "Average training loss: 0.036907262245814004\n",
      "Average test loss: 94.45094109162854\n",
      "Epoch 300/300\n",
      "Average training loss: 0.035439066602124106\n",
      "Average test loss: 0.0019383971409665213\n"
     ]
    }
   ],
   "source": [
    "# 30 Projections\n",
    "num_projections = 30\n",
    "\n",
    "# Folder Path for 30 Projections\n",
    "proj30_path = 'Memory_Residual-Additive_Depth10-.01/30 Projections'\n",
    "\n",
    "gauss_10_proj30_weights, gauss_10_proj30_hist = train_main(gaussian_10_normalized, [1] * num_projections, gaussian_10_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_10_proj30', display=True)\n",
    "\n",
    "gauss_20_proj30_weights, gauss_20_proj30_hist = train_main(gaussian_20_normalized, [1] * num_projections, gaussian_20_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_20_proj30', display=True)\n",
    "\n",
    "gauss_30_proj30_weights, gauss_30_proj30_hist = train_main(gaussian_30_normalized, [1] * num_projections, gaussian_30_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_30_proj30', display=True)\n",
    "\n",
    "gauss_40_proj30_weights, gauss_40_proj30_hist = train_main(gaussian_40_normalized, [1] * num_projections, gaussian_40_train, model=projector, device=device, model_type='Memory', split=0.2, \n",
    "                                                     numProjections=num_projections, residual=True, loss_function=loss_function, model_dir='Model_Paths/Gaussian_40_proj30', display=True)\n",
    "\n",
    "# Store Best Model Weights and History\n",
    "\n",
    "save_progress(proj30_path, gauss_10_proj30_weights, gauss_10_proj30_hist, '10% Sampling')\n",
    "save_progress(proj30_path, gauss_20_proj30_weights, gauss_20_proj30_hist, '20% Sampling')\n",
    "save_progress(proj30_path, gauss_30_proj30_weights, gauss_30_proj30_hist, '30% Sampling')\n",
    "save_progress(proj30_path, gauss_40_proj30_weights, gauss_40_proj30_hist, '40% Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7872a-ff1c-45c9-9e18-325cc3e8c525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for Projection Layer 0 across 2500 images: 20.08\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 21.86\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 22.60\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 23.43\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 24.11\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 24.39\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 24.77\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 25.11\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 25.38\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 25.37\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 25.40\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 25.55\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 25.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 25.96\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 25.89\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 26.15\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 26.00\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 26.19\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 26.30\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 26.43\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 26.45\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 26.61\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 26.72\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 26.80\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 26.50\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 26.66\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 26.75\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 26.85\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 26.99\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 27.03\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 22.70\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 25.08\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 26.25\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.97\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 27.46\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 27.71\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 28.06\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 28.21\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 28.39\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 28.37\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 28.72\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 28.84\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 29.06\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 29.07\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 29.15\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 29.23\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 29.28\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 29.33\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 29.11\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 29.31\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 29.44\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 29.41\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 29.53\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 29.73\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 29.89\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 30.06\n",
      "Average PSNR for Projection Layer 0 across 2500 images: 19.67\n",
      "Average PSNR for Projection Layer 1 across 2500 images: 23.05\n",
      "Average PSNR for Projection Layer 2 across 2500 images: 25.42\n",
      "Average PSNR for Projection Layer 3 across 2500 images: 26.52\n",
      "Average PSNR for Projection Layer 4 across 2500 images: 28.05\n",
      "Average PSNR for Projection Layer 5 across 2500 images: 28.96\n",
      "Average PSNR for Projection Layer 6 across 2500 images: 29.50\n",
      "Average PSNR for Projection Layer 7 across 2500 images: 29.94\n",
      "Average PSNR for Projection Layer 8 across 2500 images: 30.28\n",
      "Average PSNR for Projection Layer 9 across 2500 images: 30.32\n",
      "Average PSNR for Projection Layer 10 across 2500 images: 30.27\n",
      "Average PSNR for Projection Layer 11 across 2500 images: 30.61\n",
      "Average PSNR for Projection Layer 12 across 2500 images: 30.87\n",
      "Average PSNR for Projection Layer 13 across 2500 images: 31.18\n",
      "Average PSNR for Projection Layer 14 across 2500 images: 31.24\n",
      "Average PSNR for Projection Layer 15 across 2500 images: 31.39\n",
      "Average PSNR for Projection Layer 16 across 2500 images: 31.42\n",
      "Average PSNR for Projection Layer 17 across 2500 images: 31.76\n",
      "Average PSNR for Projection Layer 18 across 2500 images: 31.83\n",
      "Average PSNR for Projection Layer 19 across 2500 images: 32.05\n",
      "Average PSNR for Projection Layer 20 across 2500 images: 32.11\n",
      "Average PSNR for Projection Layer 21 across 2500 images: 32.13\n",
      "Average PSNR for Projection Layer 22 across 2500 images: 32.22\n",
      "Average PSNR for Projection Layer 23 across 2500 images: 32.33\n",
      "Average PSNR for Projection Layer 24 across 2500 images: 32.57\n",
      "Average PSNR for Projection Layer 25 across 2500 images: 32.69\n",
      "Average PSNR for Projection Layer 26 across 2500 images: 32.87\n",
      "Average PSNR for Projection Layer 27 across 2500 images: 32.99\n",
      "Average PSNR for Projection Layer 28 across 2500 images: 32.96\n",
      "Average PSNR for Projection Layer 29 across 2500 images: 33.06\n"
     ]
    }
   ],
   "source": [
    "gauss_10_proj30_model = MemoryNetwork(gaussian_10_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_20_proj30_model = MemoryNetwork(gaussian_20_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_30_proj30_model = MemoryNetwork(gaussian_30_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "gauss_40_proj30_model = MemoryNetwork(gaussian_40_normalized, [1] * num_projections, num_projections, projector, device, residual=True).to(device)\n",
    "\n",
    "gauss_10_proj30_psnr = average_PSNR(gauss_10_proj30_model, gauss_10_proj30_weights, num_projections, gaussian_10_test, print_psnr=True)\n",
    "gauss_20_proj30_psnr = average_PSNR(gauss_20_proj30_model, gauss_20_proj30_weights, num_projections, gaussian_20_test, print_psnr=True)\n",
    "gauss_30_proj30_psnr = average_PSNR(gauss_30_proj30_model, gauss_30_proj30_weights, num_projections, gaussian_30_test, print_psnr=True)\n",
    "gauss_40_proj30_psnr = average_PSNR(gauss_40_proj30_model, gauss_40_proj30_weights, num_projections, gaussian_40_test, print_psnr=True)\n",
    "\n",
    "with open(os.path.join(proj30_path, '10% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_10_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '20% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_20_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '30% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_30_proj30_psnr, f)\n",
    "with open(os.path.join(proj30_path, '40% Sampling', 'test_PSNR.pkl'), 'wb') as f:\n",
    "    pickle.dump(gauss_40_proj30_psnr, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
